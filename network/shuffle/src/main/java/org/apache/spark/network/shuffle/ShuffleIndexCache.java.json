[{
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "need license header\n",
    "commit": "3485e0f6680732bd055789d68f15fc3189ab54ad",
    "createdAt": "2015-12-14T22:13:59Z",
    "diffHunk": "@@ -0,0 +1,251 @@\n+package org.apache.spark.network.shuffle;"
  }],
  "prId": 10277
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "style:\n\n```\npublic ShuffleIndexRecord readIndexFile(\n    ExecutorShuffleInfo executor,\n    int shuffleId,\n    int mapId,\n    int reduceId) throws IOException {\n  ...\n}\n```\n",
    "commit": "3485e0f6680732bd055789d68f15fc3189ab54ad",
    "createdAt": "2015-12-14T22:14:44Z",
    "diffHunk": "@@ -0,0 +1,251 @@\n+package org.apache.spark.network.shuffle;\n+\n+import java.io.DataInputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.LongBuffer;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentMap;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Objects;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.network.shuffle.protocol.ExecutorShuffleInfo;\n+import org.apache.spark.network.util.JavaUtils;\n+\n+/**\n+ * Store the offsets of the data blocks in cache.\n+ * When index cache is not enough, remove firstly used index information.\n+ */\n+public class ShuffleIndexCache {\n+  private static final Logger logger = LoggerFactory.getLogger(ShuffleIndexCache.class);\n+\n+  private final ConcurrentMap<ShuffleMapId, IndexInformation> indexCache;\n+  private final LinkedBlockingQueue<ShuffleMapId> queue = new LinkedBlockingQueue<ShuffleMapId>();\n+  private final int totalMemoryAllowed;\n+  private AtomicInteger totalMemoryUsed = new AtomicInteger();\n+\n+  public ShuffleIndexCache(int totalMemoryAllowed) {\n+    this.indexCache = new ConcurrentHashMap<ShuffleMapId, IndexInformation>();\n+    this.totalMemoryAllowed = totalMemoryAllowed;\n+    logger.info(\"IndexCache created with max memory = {}\", totalMemoryAllowed);\n+  }\n+\n+  /**\n+   * Get the index information for the given shuffleId, mapId and reduceId.\n+   * It reads the index file into cache if it is not already present.\n+   */\n+  public ShuffleIndexRecord getIndexInformation(\n+    ExecutorShuffleInfo executor, int shuffleId, int mapId, int reduceId) throws IOException {\n+    if (totalMemoryAllowed > 0) {\n+      ShuffleMapId shuffleMapId = new ShuffleMapId(shuffleId, mapId);\n+      IndexInformation info = indexCache.get(shuffleMapId);\n+\n+      if (info == null) {\n+        info = readIndexFileToCache(executor, shuffleMapId);\n+      } else {\n+        synchronized(info) {\n+          while (isUnderConstruction(info)) {\n+            try {\n+              info.wait();\n+            } catch (InterruptedException e) {\n+              throw new IOException(\"Interrupted waiting for construction\", e);\n+            }\n+          }\n+        }\n+      }\n+\n+      if(info.getLength() == 0 || info.getLength() <= reduceId + 1) {\n+        throw new IOException(\"Invalid request \" + \" shuffleMapId = \" + shuffleMapId +\n+          \" reduceId = \" + reduceId + \" Index Info Length = \" + info.getLength() +\n+            \" index file = \" + getIndexFile(executor, mapId, reduceId));\n+      }\n+\n+      return info.getIndex(reduceId);\n+    } else {\n+      return this.readIndexFile(executor, shuffleId, mapId, reduceId);\n+    }\n+  }\n+\n+  public ShuffleIndexRecord readIndexFile(\n+    ExecutorShuffleInfo executor, int shuffleId, int mapId, int reduceId) throws IOException {"
  }],
  "prId": 10277
}]