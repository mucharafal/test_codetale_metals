[{
  "comments": [{
    "author": {
      "login": "aarondav"
    },
    "body": "should we advance msg by the readableBytes?\n",
    "commit": "ff019662055edeaf8d709087e0279afa37011353",
    "createdAt": "2015-04-27T05:17:52Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.network.sasl;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.WritableByteChannel;\n+import java.util.List;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.netty.buffer.ByteBuf;\n+import io.netty.buffer.Unpooled;\n+import io.netty.channel.Channel;\n+import io.netty.channel.ChannelHandlerContext;\n+import io.netty.channel.ChannelOutboundHandlerAdapter;\n+import io.netty.channel.ChannelPromise;\n+import io.netty.channel.FileRegion;\n+import io.netty.handler.codec.MessageToMessageDecoder;\n+import io.netty.util.AbstractReferenceCounted;\n+import io.netty.util.ReferenceCountUtil;\n+\n+import org.apache.spark.network.util.ByteArrayWritableChannel;\n+import org.apache.spark.network.util.NettyUtils;\n+\n+class SaslEncryption {\n+\n+  @VisibleForTesting\n+  static final String ENCRYPTION_HANDLER_NAME = \"saslEncryption\";\n+\n+  /**\n+   * Adds channel handlers that perform encryption / decryption of data using SASL.\n+   *\n+   * @param channel The channel.\n+   * @param backend The SASL backend.\n+   * @param maxOutboundBlockSize Max size in bytes of outgoing encrypted blocks, to control\n+   *                             memory usage.\n+   */\n+  static void addToChannel(\n+      Channel channel,\n+      SaslEncryptionBackend backend,\n+      int maxOutboundBlockSize) {\n+    channel.pipeline()\n+      .addFirst(ENCRYPTION_HANDLER_NAME, new EncryptionHandler(backend, maxOutboundBlockSize))\n+      .addFirst(\"saslDecryption\", new DecryptionHandler(backend))\n+      .addFirst(\"saslFrameDecoder\", NettyUtils.createFrameDecoder());\n+  }\n+\n+  private static class EncryptionHandler extends ChannelOutboundHandlerAdapter {\n+\n+    private final int maxOutboundBlockSize;\n+    private final SaslEncryptionBackend backend;\n+\n+    EncryptionHandler(SaslEncryptionBackend backend, int maxOutboundBlockSize) {\n+      this.backend = backend;\n+      this.maxOutboundBlockSize = maxOutboundBlockSize;\n+    }\n+\n+    /**\n+     * Wrap the incoming message in an implementation that will perform encryption lazily. This is\n+     * needed to guarantee ordering of the outgoing encrypted packets - they need to be decrypted in\n+     * the same order, and netty doesn't have an atomic ChannelHandlerContext.write() API, so it\n+     * does not guarantee any ordering.\n+     */\n+    @Override\n+    public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise)\n+      throws Exception {\n+\n+      ctx.write(new EncryptedMessage(backend, msg, maxOutboundBlockSize), promise);\n+    }\n+\n+    @Override\n+    public void handlerRemoved(ChannelHandlerContext ctx) throws Exception {\n+      try {\n+        backend.dispose();\n+      } finally {\n+        super.handlerRemoved(ctx);\n+      }\n+    }\n+\n+  }\n+\n+  private static class DecryptionHandler extends MessageToMessageDecoder<ByteBuf> {\n+\n+    private final SaslEncryptionBackend backend;\n+\n+    DecryptionHandler(SaslEncryptionBackend backend) {\n+      this.backend = backend;\n+    }\n+\n+    @Override\n+    protected void decode(ChannelHandlerContext ctx, ByteBuf msg, List<Object> out)\n+      throws Exception {\n+\n+      byte[] data;\n+      int offset;\n+      int length = msg.readableBytes();\n+      if (msg.hasArray()) {\n+        data = msg.array();\n+        offset = msg.arrayOffset();",
    "line": 119
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "It's unnecessary since `MessageToMessageDecoder` will release the input message when this method returns.\n",
    "commit": "ff019662055edeaf8d709087e0279afa37011353",
    "createdAt": "2015-04-27T20:39:31Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.network.sasl;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.WritableByteChannel;\n+import java.util.List;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.netty.buffer.ByteBuf;\n+import io.netty.buffer.Unpooled;\n+import io.netty.channel.Channel;\n+import io.netty.channel.ChannelHandlerContext;\n+import io.netty.channel.ChannelOutboundHandlerAdapter;\n+import io.netty.channel.ChannelPromise;\n+import io.netty.channel.FileRegion;\n+import io.netty.handler.codec.MessageToMessageDecoder;\n+import io.netty.util.AbstractReferenceCounted;\n+import io.netty.util.ReferenceCountUtil;\n+\n+import org.apache.spark.network.util.ByteArrayWritableChannel;\n+import org.apache.spark.network.util.NettyUtils;\n+\n+class SaslEncryption {\n+\n+  @VisibleForTesting\n+  static final String ENCRYPTION_HANDLER_NAME = \"saslEncryption\";\n+\n+  /**\n+   * Adds channel handlers that perform encryption / decryption of data using SASL.\n+   *\n+   * @param channel The channel.\n+   * @param backend The SASL backend.\n+   * @param maxOutboundBlockSize Max size in bytes of outgoing encrypted blocks, to control\n+   *                             memory usage.\n+   */\n+  static void addToChannel(\n+      Channel channel,\n+      SaslEncryptionBackend backend,\n+      int maxOutboundBlockSize) {\n+    channel.pipeline()\n+      .addFirst(ENCRYPTION_HANDLER_NAME, new EncryptionHandler(backend, maxOutboundBlockSize))\n+      .addFirst(\"saslDecryption\", new DecryptionHandler(backend))\n+      .addFirst(\"saslFrameDecoder\", NettyUtils.createFrameDecoder());\n+  }\n+\n+  private static class EncryptionHandler extends ChannelOutboundHandlerAdapter {\n+\n+    private final int maxOutboundBlockSize;\n+    private final SaslEncryptionBackend backend;\n+\n+    EncryptionHandler(SaslEncryptionBackend backend, int maxOutboundBlockSize) {\n+      this.backend = backend;\n+      this.maxOutboundBlockSize = maxOutboundBlockSize;\n+    }\n+\n+    /**\n+     * Wrap the incoming message in an implementation that will perform encryption lazily. This is\n+     * needed to guarantee ordering of the outgoing encrypted packets - they need to be decrypted in\n+     * the same order, and netty doesn't have an atomic ChannelHandlerContext.write() API, so it\n+     * does not guarantee any ordering.\n+     */\n+    @Override\n+    public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise)\n+      throws Exception {\n+\n+      ctx.write(new EncryptedMessage(backend, msg, maxOutboundBlockSize), promise);\n+    }\n+\n+    @Override\n+    public void handlerRemoved(ChannelHandlerContext ctx) throws Exception {\n+      try {\n+        backend.dispose();\n+      } finally {\n+        super.handlerRemoved(ctx);\n+      }\n+    }\n+\n+  }\n+\n+  private static class DecryptionHandler extends MessageToMessageDecoder<ByteBuf> {\n+\n+    private final SaslEncryptionBackend backend;\n+\n+    DecryptionHandler(SaslEncryptionBackend backend) {\n+      this.backend = backend;\n+    }\n+\n+    @Override\n+    protected void decode(ChannelHandlerContext ctx, ByteBuf msg, List<Object> out)\n+      throws Exception {\n+\n+      byte[] data;\n+      int offset;\n+      int length = msg.readableBytes();\n+      if (msg.hasArray()) {\n+        data = msg.array();\n+        offset = msg.arrayOffset();",
    "line": 119
  }, {
    "author": {
      "login": "aarondav"
    },
    "body": "I see, it's just slightly odd that only one of the two cases moves msg's reader index. In the current implementation, you're right that it looks correct, but a seemingly unrelated change could cause issues down the line due to the discrepancy, and only in a very weird circumstance (off-heap vs on-heap messages). Consider using `msg.getBytes(msg.readerIndx(), data)` below.\n",
    "commit": "ff019662055edeaf8d709087e0279afa37011353",
    "createdAt": "2015-04-27T21:49:25Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.network.sasl;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.WritableByteChannel;\n+import java.util.List;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.netty.buffer.ByteBuf;\n+import io.netty.buffer.Unpooled;\n+import io.netty.channel.Channel;\n+import io.netty.channel.ChannelHandlerContext;\n+import io.netty.channel.ChannelOutboundHandlerAdapter;\n+import io.netty.channel.ChannelPromise;\n+import io.netty.channel.FileRegion;\n+import io.netty.handler.codec.MessageToMessageDecoder;\n+import io.netty.util.AbstractReferenceCounted;\n+import io.netty.util.ReferenceCountUtil;\n+\n+import org.apache.spark.network.util.ByteArrayWritableChannel;\n+import org.apache.spark.network.util.NettyUtils;\n+\n+class SaslEncryption {\n+\n+  @VisibleForTesting\n+  static final String ENCRYPTION_HANDLER_NAME = \"saslEncryption\";\n+\n+  /**\n+   * Adds channel handlers that perform encryption / decryption of data using SASL.\n+   *\n+   * @param channel The channel.\n+   * @param backend The SASL backend.\n+   * @param maxOutboundBlockSize Max size in bytes of outgoing encrypted blocks, to control\n+   *                             memory usage.\n+   */\n+  static void addToChannel(\n+      Channel channel,\n+      SaslEncryptionBackend backend,\n+      int maxOutboundBlockSize) {\n+    channel.pipeline()\n+      .addFirst(ENCRYPTION_HANDLER_NAME, new EncryptionHandler(backend, maxOutboundBlockSize))\n+      .addFirst(\"saslDecryption\", new DecryptionHandler(backend))\n+      .addFirst(\"saslFrameDecoder\", NettyUtils.createFrameDecoder());\n+  }\n+\n+  private static class EncryptionHandler extends ChannelOutboundHandlerAdapter {\n+\n+    private final int maxOutboundBlockSize;\n+    private final SaslEncryptionBackend backend;\n+\n+    EncryptionHandler(SaslEncryptionBackend backend, int maxOutboundBlockSize) {\n+      this.backend = backend;\n+      this.maxOutboundBlockSize = maxOutboundBlockSize;\n+    }\n+\n+    /**\n+     * Wrap the incoming message in an implementation that will perform encryption lazily. This is\n+     * needed to guarantee ordering of the outgoing encrypted packets - they need to be decrypted in\n+     * the same order, and netty doesn't have an atomic ChannelHandlerContext.write() API, so it\n+     * does not guarantee any ordering.\n+     */\n+    @Override\n+    public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise)\n+      throws Exception {\n+\n+      ctx.write(new EncryptedMessage(backend, msg, maxOutboundBlockSize), promise);\n+    }\n+\n+    @Override\n+    public void handlerRemoved(ChannelHandlerContext ctx) throws Exception {\n+      try {\n+        backend.dispose();\n+      } finally {\n+        super.handlerRemoved(ctx);\n+      }\n+    }\n+\n+  }\n+\n+  private static class DecryptionHandler extends MessageToMessageDecoder<ByteBuf> {\n+\n+    private final SaslEncryptionBackend backend;\n+\n+    DecryptionHandler(SaslEncryptionBackend backend) {\n+      this.backend = backend;\n+    }\n+\n+    @Override\n+    protected void decode(ChannelHandlerContext ctx, ByteBuf msg, List<Object> out)\n+      throws Exception {\n+\n+      byte[] data;\n+      int offset;\n+      int length = msg.readableBytes();\n+      if (msg.hasArray()) {\n+        data = msg.array();\n+        offset = msg.arrayOffset();",
    "line": 119
  }],
  "prId": 5377
}, {
  "comments": [{
    "author": {
      "login": "aarondav"
    },
    "body": "Could this be a ByteArrayOutputStream? If it were, I think you could use buf.readBytes(byteStream) or something of that sort, and avoid the skip.\n",
    "commit": "ff019662055edeaf8d709087e0279afa37011353",
    "createdAt": "2015-04-27T05:21:45Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.network.sasl;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.WritableByteChannel;\n+import java.util.List;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.netty.buffer.ByteBuf;\n+import io.netty.buffer.Unpooled;\n+import io.netty.channel.Channel;\n+import io.netty.channel.ChannelHandlerContext;\n+import io.netty.channel.ChannelOutboundHandlerAdapter;\n+import io.netty.channel.ChannelPromise;\n+import io.netty.channel.FileRegion;\n+import io.netty.handler.codec.MessageToMessageDecoder;\n+import io.netty.util.AbstractReferenceCounted;\n+import io.netty.util.ReferenceCountUtil;\n+\n+import org.apache.spark.network.util.ByteArrayWritableChannel;\n+import org.apache.spark.network.util.NettyUtils;\n+\n+class SaslEncryption {\n+\n+  @VisibleForTesting\n+  static final String ENCRYPTION_HANDLER_NAME = \"saslEncryption\";\n+\n+  /**\n+   * Adds channel handlers that perform encryption / decryption of data using SASL.\n+   *\n+   * @param channel The channel.\n+   * @param backend The SASL backend.\n+   * @param maxOutboundBlockSize Max size in bytes of outgoing encrypted blocks, to control\n+   *                             memory usage.\n+   */\n+  static void addToChannel(\n+      Channel channel,\n+      SaslEncryptionBackend backend,\n+      int maxOutboundBlockSize) {\n+    channel.pipeline()\n+      .addFirst(ENCRYPTION_HANDLER_NAME, new EncryptionHandler(backend, maxOutboundBlockSize))\n+      .addFirst(\"saslDecryption\", new DecryptionHandler(backend))\n+      .addFirst(\"saslFrameDecoder\", NettyUtils.createFrameDecoder());\n+  }\n+\n+  private static class EncryptionHandler extends ChannelOutboundHandlerAdapter {\n+\n+    private final int maxOutboundBlockSize;\n+    private final SaslEncryptionBackend backend;\n+\n+    EncryptionHandler(SaslEncryptionBackend backend, int maxOutboundBlockSize) {\n+      this.backend = backend;\n+      this.maxOutboundBlockSize = maxOutboundBlockSize;\n+    }\n+\n+    /**\n+     * Wrap the incoming message in an implementation that will perform encryption lazily. This is\n+     * needed to guarantee ordering of the outgoing encrypted packets - they need to be decrypted in\n+     * the same order, and netty doesn't have an atomic ChannelHandlerContext.write() API, so it\n+     * does not guarantee any ordering.\n+     */\n+    @Override\n+    public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise)\n+      throws Exception {\n+\n+      ctx.write(new EncryptedMessage(backend, msg, maxOutboundBlockSize), promise);\n+    }\n+\n+    @Override\n+    public void handlerRemoved(ChannelHandlerContext ctx) throws Exception {\n+      try {\n+        backend.dispose();\n+      } finally {\n+        super.handlerRemoved(ctx);\n+      }\n+    }\n+\n+  }\n+\n+  private static class DecryptionHandler extends MessageToMessageDecoder<ByteBuf> {\n+\n+    private final SaslEncryptionBackend backend;\n+\n+    DecryptionHandler(SaslEncryptionBackend backend) {\n+      this.backend = backend;\n+    }\n+\n+    @Override\n+    protected void decode(ChannelHandlerContext ctx, ByteBuf msg, List<Object> out)\n+      throws Exception {\n+\n+      byte[] data;\n+      int offset;\n+      int length = msg.readableBytes();\n+      if (msg.hasArray()) {\n+        data = msg.array();\n+        offset = msg.arrayOffset();\n+      } else {\n+        data = new byte[length];\n+        msg.readBytes(data);\n+        offset = 0;\n+      }\n+\n+      out.add(Unpooled.wrappedBuffer(backend.unwrap(data, offset, length)));\n+    }\n+\n+  }\n+\n+  private static class EncryptedMessage extends AbstractReferenceCounted implements FileRegion {\n+\n+    private final SaslEncryptionBackend backend;\n+    private final boolean isByteBuf;\n+    private final ByteBuf buf;\n+    private final FileRegion region;\n+    private final ByteArrayWritableChannel byteChannel;\n+\n+    private ByteBuf currentHeader;\n+    private ByteBuffer currentChunk;\n+    private long currentChunkSize;\n+    private long unencryptedChunkSize;\n+    private long transferred;\n+\n+    EncryptedMessage(SaslEncryptionBackend backend, Object msg, int maxOutboundBlockSize) {\n+      Preconditions.checkArgument(msg instanceof ByteBuf || msg instanceof FileRegion,\n+        \"Unrecognized message type: %s\", msg.getClass().getName());\n+      this.backend = backend;\n+      this.isByteBuf = msg instanceof ByteBuf;\n+      this.buf = isByteBuf ? (ByteBuf) msg : null;\n+      this.region = isByteBuf ? null : (FileRegion) msg;\n+      this.byteChannel = new ByteArrayWritableChannel(maxOutboundBlockSize);\n+    }\n+\n+    /**\n+     * Returns the size of the original (unencrypted) message.\n+     *\n+     * This makes assumptions about how netty treats FileRegion instances, because there's no way\n+     * to know beforehand what will be the size of the encrypted message. Namely, it assumes\n+     * that netty will try to transfer data from this message while\n+     * <code>transfered() < count()</code>. So these two methods return, technically, wrong data,\n+     * but netty doesn't know better.\n+     */\n+    @Override\n+    public long count() {\n+      return isByteBuf ? buf.readableBytes() : region.count();\n+    }\n+\n+    @Override\n+    public long position() {\n+      return 0;\n+    }\n+\n+    /**\n+     * Returns an approximation of the amount of data transferred. See {@link #count()}.\n+     */\n+    @Override\n+    public long transfered() {\n+      return transferred;\n+    }\n+\n+    /**\n+     * Transfers data from the original message to the channel, encrypting it in the process.\n+     *\n+     * This method also breaks down the original message into smaller chunks when needed. This\n+     * is done to keep memory usage under control. This avoids having to copy the whole message\n+     * data into memory at once, and can avoid ballooning memory usage when transferring large\n+     * messages such as shuffle blocks.\n+     *\n+     * The {@link #transfered()} counter also behaves a little funny, in that it won't go forward\n+     * until a whole chunk has been written. This is done because the code can't use the actual\n+     * number of bytes written to the channel as the transferred count (see {@link #count()}).\n+     * Instead, once an encrypted chunk is written to the output (including its header), the\n+     * size of the original block will be added to the {@link #transfered()} amount.\n+     */\n+    @Override\n+    public long transferTo(final WritableByteChannel target, final long position)\n+      throws IOException {\n+\n+      Preconditions.checkArgument(position == transfered(), \"Invalid position.\");\n+\n+      long written = 0;\n+      do {\n+        if (currentChunk == null) {\n+          nextChunk();\n+        }\n+\n+        if (currentHeader.readableBytes() > 0) {\n+          int bytesWritten = target.write(currentHeader.nioBuffer());\n+          currentHeader.skipBytes(bytesWritten);\n+          if (currentHeader.readableBytes() > 0) {\n+            // Break out of loop if there are still header bytes left to write.\n+            break;\n+          }\n+        }\n+\n+        target.write(currentChunk);\n+        if (!currentChunk.hasRemaining()) {\n+          // Only update the count of written bytes once a full chunk has been written.\n+          // See method javadoc.\n+          written += unencryptedChunkSize;\n+          currentHeader.release();\n+          currentHeader = null;\n+          currentChunk = null;\n+          currentChunkSize = 0;\n+        }\n+      } while (currentChunk == null && transfered() + written < count());\n+\n+      transferred += written;\n+      return written;\n+    }\n+\n+    private void nextChunk() throws IOException {\n+      byteChannel.reset();\n+      if (isByteBuf) {\n+        int copied = byteChannel.write(buf.nioBuffer());",
    "line": 263
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "Hmm... I could, but then I'd need different logic for the `FileRegion` case (since it needs a `WritableByteChannel`).\n\nIt's not a lot of code, but is more than just leaving the `skipBytes` call there.\n\n(I could also implement `GatheringByteChannel` instead of `WritableByteChannel`, but, again, that would be more code than just the one skip call...)\n",
    "commit": "ff019662055edeaf8d709087e0279afa37011353",
    "createdAt": "2015-04-27T20:53:33Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.network.sasl;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.WritableByteChannel;\n+import java.util.List;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.netty.buffer.ByteBuf;\n+import io.netty.buffer.Unpooled;\n+import io.netty.channel.Channel;\n+import io.netty.channel.ChannelHandlerContext;\n+import io.netty.channel.ChannelOutboundHandlerAdapter;\n+import io.netty.channel.ChannelPromise;\n+import io.netty.channel.FileRegion;\n+import io.netty.handler.codec.MessageToMessageDecoder;\n+import io.netty.util.AbstractReferenceCounted;\n+import io.netty.util.ReferenceCountUtil;\n+\n+import org.apache.spark.network.util.ByteArrayWritableChannel;\n+import org.apache.spark.network.util.NettyUtils;\n+\n+class SaslEncryption {\n+\n+  @VisibleForTesting\n+  static final String ENCRYPTION_HANDLER_NAME = \"saslEncryption\";\n+\n+  /**\n+   * Adds channel handlers that perform encryption / decryption of data using SASL.\n+   *\n+   * @param channel The channel.\n+   * @param backend The SASL backend.\n+   * @param maxOutboundBlockSize Max size in bytes of outgoing encrypted blocks, to control\n+   *                             memory usage.\n+   */\n+  static void addToChannel(\n+      Channel channel,\n+      SaslEncryptionBackend backend,\n+      int maxOutboundBlockSize) {\n+    channel.pipeline()\n+      .addFirst(ENCRYPTION_HANDLER_NAME, new EncryptionHandler(backend, maxOutboundBlockSize))\n+      .addFirst(\"saslDecryption\", new DecryptionHandler(backend))\n+      .addFirst(\"saslFrameDecoder\", NettyUtils.createFrameDecoder());\n+  }\n+\n+  private static class EncryptionHandler extends ChannelOutboundHandlerAdapter {\n+\n+    private final int maxOutboundBlockSize;\n+    private final SaslEncryptionBackend backend;\n+\n+    EncryptionHandler(SaslEncryptionBackend backend, int maxOutboundBlockSize) {\n+      this.backend = backend;\n+      this.maxOutboundBlockSize = maxOutboundBlockSize;\n+    }\n+\n+    /**\n+     * Wrap the incoming message in an implementation that will perform encryption lazily. This is\n+     * needed to guarantee ordering of the outgoing encrypted packets - they need to be decrypted in\n+     * the same order, and netty doesn't have an atomic ChannelHandlerContext.write() API, so it\n+     * does not guarantee any ordering.\n+     */\n+    @Override\n+    public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise)\n+      throws Exception {\n+\n+      ctx.write(new EncryptedMessage(backend, msg, maxOutboundBlockSize), promise);\n+    }\n+\n+    @Override\n+    public void handlerRemoved(ChannelHandlerContext ctx) throws Exception {\n+      try {\n+        backend.dispose();\n+      } finally {\n+        super.handlerRemoved(ctx);\n+      }\n+    }\n+\n+  }\n+\n+  private static class DecryptionHandler extends MessageToMessageDecoder<ByteBuf> {\n+\n+    private final SaslEncryptionBackend backend;\n+\n+    DecryptionHandler(SaslEncryptionBackend backend) {\n+      this.backend = backend;\n+    }\n+\n+    @Override\n+    protected void decode(ChannelHandlerContext ctx, ByteBuf msg, List<Object> out)\n+      throws Exception {\n+\n+      byte[] data;\n+      int offset;\n+      int length = msg.readableBytes();\n+      if (msg.hasArray()) {\n+        data = msg.array();\n+        offset = msg.arrayOffset();\n+      } else {\n+        data = new byte[length];\n+        msg.readBytes(data);\n+        offset = 0;\n+      }\n+\n+      out.add(Unpooled.wrappedBuffer(backend.unwrap(data, offset, length)));\n+    }\n+\n+  }\n+\n+  private static class EncryptedMessage extends AbstractReferenceCounted implements FileRegion {\n+\n+    private final SaslEncryptionBackend backend;\n+    private final boolean isByteBuf;\n+    private final ByteBuf buf;\n+    private final FileRegion region;\n+    private final ByteArrayWritableChannel byteChannel;\n+\n+    private ByteBuf currentHeader;\n+    private ByteBuffer currentChunk;\n+    private long currentChunkSize;\n+    private long unencryptedChunkSize;\n+    private long transferred;\n+\n+    EncryptedMessage(SaslEncryptionBackend backend, Object msg, int maxOutboundBlockSize) {\n+      Preconditions.checkArgument(msg instanceof ByteBuf || msg instanceof FileRegion,\n+        \"Unrecognized message type: %s\", msg.getClass().getName());\n+      this.backend = backend;\n+      this.isByteBuf = msg instanceof ByteBuf;\n+      this.buf = isByteBuf ? (ByteBuf) msg : null;\n+      this.region = isByteBuf ? null : (FileRegion) msg;\n+      this.byteChannel = new ByteArrayWritableChannel(maxOutboundBlockSize);\n+    }\n+\n+    /**\n+     * Returns the size of the original (unencrypted) message.\n+     *\n+     * This makes assumptions about how netty treats FileRegion instances, because there's no way\n+     * to know beforehand what will be the size of the encrypted message. Namely, it assumes\n+     * that netty will try to transfer data from this message while\n+     * <code>transfered() < count()</code>. So these two methods return, technically, wrong data,\n+     * but netty doesn't know better.\n+     */\n+    @Override\n+    public long count() {\n+      return isByteBuf ? buf.readableBytes() : region.count();\n+    }\n+\n+    @Override\n+    public long position() {\n+      return 0;\n+    }\n+\n+    /**\n+     * Returns an approximation of the amount of data transferred. See {@link #count()}.\n+     */\n+    @Override\n+    public long transfered() {\n+      return transferred;\n+    }\n+\n+    /**\n+     * Transfers data from the original message to the channel, encrypting it in the process.\n+     *\n+     * This method also breaks down the original message into smaller chunks when needed. This\n+     * is done to keep memory usage under control. This avoids having to copy the whole message\n+     * data into memory at once, and can avoid ballooning memory usage when transferring large\n+     * messages such as shuffle blocks.\n+     *\n+     * The {@link #transfered()} counter also behaves a little funny, in that it won't go forward\n+     * until a whole chunk has been written. This is done because the code can't use the actual\n+     * number of bytes written to the channel as the transferred count (see {@link #count()}).\n+     * Instead, once an encrypted chunk is written to the output (including its header), the\n+     * size of the original block will be added to the {@link #transfered()} amount.\n+     */\n+    @Override\n+    public long transferTo(final WritableByteChannel target, final long position)\n+      throws IOException {\n+\n+      Preconditions.checkArgument(position == transfered(), \"Invalid position.\");\n+\n+      long written = 0;\n+      do {\n+        if (currentChunk == null) {\n+          nextChunk();\n+        }\n+\n+        if (currentHeader.readableBytes() > 0) {\n+          int bytesWritten = target.write(currentHeader.nioBuffer());\n+          currentHeader.skipBytes(bytesWritten);\n+          if (currentHeader.readableBytes() > 0) {\n+            // Break out of loop if there are still header bytes left to write.\n+            break;\n+          }\n+        }\n+\n+        target.write(currentChunk);\n+        if (!currentChunk.hasRemaining()) {\n+          // Only update the count of written bytes once a full chunk has been written.\n+          // See method javadoc.\n+          written += unencryptedChunkSize;\n+          currentHeader.release();\n+          currentHeader = null;\n+          currentChunk = null;\n+          currentChunkSize = 0;\n+        }\n+      } while (currentChunk == null && transfered() + written < count());\n+\n+      transferred += written;\n+      return written;\n+    }\n+\n+    private void nextChunk() throws IOException {\n+      byteChannel.reset();\n+      if (isByteBuf) {\n+        int copied = byteChannel.write(buf.nioBuffer());",
    "line": 263
  }, {
    "author": {
      "login": "aarondav"
    },
    "body": "I think `Channels.newChannel(outputStream)` might work.\n\n(The reason I'd especially like this solution is that we could keep ByteArrayWritableChannel in src/test if we could just resolve this use-case without it.)\n",
    "commit": "ff019662055edeaf8d709087e0279afa37011353",
    "createdAt": "2015-04-27T22:03:09Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.network.sasl;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.WritableByteChannel;\n+import java.util.List;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.netty.buffer.ByteBuf;\n+import io.netty.buffer.Unpooled;\n+import io.netty.channel.Channel;\n+import io.netty.channel.ChannelHandlerContext;\n+import io.netty.channel.ChannelOutboundHandlerAdapter;\n+import io.netty.channel.ChannelPromise;\n+import io.netty.channel.FileRegion;\n+import io.netty.handler.codec.MessageToMessageDecoder;\n+import io.netty.util.AbstractReferenceCounted;\n+import io.netty.util.ReferenceCountUtil;\n+\n+import org.apache.spark.network.util.ByteArrayWritableChannel;\n+import org.apache.spark.network.util.NettyUtils;\n+\n+class SaslEncryption {\n+\n+  @VisibleForTesting\n+  static final String ENCRYPTION_HANDLER_NAME = \"saslEncryption\";\n+\n+  /**\n+   * Adds channel handlers that perform encryption / decryption of data using SASL.\n+   *\n+   * @param channel The channel.\n+   * @param backend The SASL backend.\n+   * @param maxOutboundBlockSize Max size in bytes of outgoing encrypted blocks, to control\n+   *                             memory usage.\n+   */\n+  static void addToChannel(\n+      Channel channel,\n+      SaslEncryptionBackend backend,\n+      int maxOutboundBlockSize) {\n+    channel.pipeline()\n+      .addFirst(ENCRYPTION_HANDLER_NAME, new EncryptionHandler(backend, maxOutboundBlockSize))\n+      .addFirst(\"saslDecryption\", new DecryptionHandler(backend))\n+      .addFirst(\"saslFrameDecoder\", NettyUtils.createFrameDecoder());\n+  }\n+\n+  private static class EncryptionHandler extends ChannelOutboundHandlerAdapter {\n+\n+    private final int maxOutboundBlockSize;\n+    private final SaslEncryptionBackend backend;\n+\n+    EncryptionHandler(SaslEncryptionBackend backend, int maxOutboundBlockSize) {\n+      this.backend = backend;\n+      this.maxOutboundBlockSize = maxOutboundBlockSize;\n+    }\n+\n+    /**\n+     * Wrap the incoming message in an implementation that will perform encryption lazily. This is\n+     * needed to guarantee ordering of the outgoing encrypted packets - they need to be decrypted in\n+     * the same order, and netty doesn't have an atomic ChannelHandlerContext.write() API, so it\n+     * does not guarantee any ordering.\n+     */\n+    @Override\n+    public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise)\n+      throws Exception {\n+\n+      ctx.write(new EncryptedMessage(backend, msg, maxOutboundBlockSize), promise);\n+    }\n+\n+    @Override\n+    public void handlerRemoved(ChannelHandlerContext ctx) throws Exception {\n+      try {\n+        backend.dispose();\n+      } finally {\n+        super.handlerRemoved(ctx);\n+      }\n+    }\n+\n+  }\n+\n+  private static class DecryptionHandler extends MessageToMessageDecoder<ByteBuf> {\n+\n+    private final SaslEncryptionBackend backend;\n+\n+    DecryptionHandler(SaslEncryptionBackend backend) {\n+      this.backend = backend;\n+    }\n+\n+    @Override\n+    protected void decode(ChannelHandlerContext ctx, ByteBuf msg, List<Object> out)\n+      throws Exception {\n+\n+      byte[] data;\n+      int offset;\n+      int length = msg.readableBytes();\n+      if (msg.hasArray()) {\n+        data = msg.array();\n+        offset = msg.arrayOffset();\n+      } else {\n+        data = new byte[length];\n+        msg.readBytes(data);\n+        offset = 0;\n+      }\n+\n+      out.add(Unpooled.wrappedBuffer(backend.unwrap(data, offset, length)));\n+    }\n+\n+  }\n+\n+  private static class EncryptedMessage extends AbstractReferenceCounted implements FileRegion {\n+\n+    private final SaslEncryptionBackend backend;\n+    private final boolean isByteBuf;\n+    private final ByteBuf buf;\n+    private final FileRegion region;\n+    private final ByteArrayWritableChannel byteChannel;\n+\n+    private ByteBuf currentHeader;\n+    private ByteBuffer currentChunk;\n+    private long currentChunkSize;\n+    private long unencryptedChunkSize;\n+    private long transferred;\n+\n+    EncryptedMessage(SaslEncryptionBackend backend, Object msg, int maxOutboundBlockSize) {\n+      Preconditions.checkArgument(msg instanceof ByteBuf || msg instanceof FileRegion,\n+        \"Unrecognized message type: %s\", msg.getClass().getName());\n+      this.backend = backend;\n+      this.isByteBuf = msg instanceof ByteBuf;\n+      this.buf = isByteBuf ? (ByteBuf) msg : null;\n+      this.region = isByteBuf ? null : (FileRegion) msg;\n+      this.byteChannel = new ByteArrayWritableChannel(maxOutboundBlockSize);\n+    }\n+\n+    /**\n+     * Returns the size of the original (unencrypted) message.\n+     *\n+     * This makes assumptions about how netty treats FileRegion instances, because there's no way\n+     * to know beforehand what will be the size of the encrypted message. Namely, it assumes\n+     * that netty will try to transfer data from this message while\n+     * <code>transfered() < count()</code>. So these two methods return, technically, wrong data,\n+     * but netty doesn't know better.\n+     */\n+    @Override\n+    public long count() {\n+      return isByteBuf ? buf.readableBytes() : region.count();\n+    }\n+\n+    @Override\n+    public long position() {\n+      return 0;\n+    }\n+\n+    /**\n+     * Returns an approximation of the amount of data transferred. See {@link #count()}.\n+     */\n+    @Override\n+    public long transfered() {\n+      return transferred;\n+    }\n+\n+    /**\n+     * Transfers data from the original message to the channel, encrypting it in the process.\n+     *\n+     * This method also breaks down the original message into smaller chunks when needed. This\n+     * is done to keep memory usage under control. This avoids having to copy the whole message\n+     * data into memory at once, and can avoid ballooning memory usage when transferring large\n+     * messages such as shuffle blocks.\n+     *\n+     * The {@link #transfered()} counter also behaves a little funny, in that it won't go forward\n+     * until a whole chunk has been written. This is done because the code can't use the actual\n+     * number of bytes written to the channel as the transferred count (see {@link #count()}).\n+     * Instead, once an encrypted chunk is written to the output (including its header), the\n+     * size of the original block will be added to the {@link #transfered()} amount.\n+     */\n+    @Override\n+    public long transferTo(final WritableByteChannel target, final long position)\n+      throws IOException {\n+\n+      Preconditions.checkArgument(position == transfered(), \"Invalid position.\");\n+\n+      long written = 0;\n+      do {\n+        if (currentChunk == null) {\n+          nextChunk();\n+        }\n+\n+        if (currentHeader.readableBytes() > 0) {\n+          int bytesWritten = target.write(currentHeader.nioBuffer());\n+          currentHeader.skipBytes(bytesWritten);\n+          if (currentHeader.readableBytes() > 0) {\n+            // Break out of loop if there are still header bytes left to write.\n+            break;\n+          }\n+        }\n+\n+        target.write(currentChunk);\n+        if (!currentChunk.hasRemaining()) {\n+          // Only update the count of written bytes once a full chunk has been written.\n+          // See method javadoc.\n+          written += unencryptedChunkSize;\n+          currentHeader.release();\n+          currentHeader = null;\n+          currentChunk = null;\n+          currentChunkSize = 0;\n+        }\n+      } while (currentChunk == null && transfered() + written < count());\n+\n+      transferred += written;\n+      return written;\n+    }\n+\n+    private void nextChunk() throws IOException {\n+      byteChannel.reset();\n+      if (isByteBuf) {\n+        int copied = byteChannel.write(buf.nioBuffer());",
    "line": 263
  }],
  "prId": 5377
}, {
  "comments": [{
    "author": {
      "login": "aarondav"
    },
    "body": "This mechanism may add even more performance burden, as I believe there is special logic which checks for zero-sized writes and backs off from trying to send until the channel appears writable again. However, I can't think of a good way to avoid this that isn't significantly hackier, so it's probably fine.\n",
    "commit": "ff019662055edeaf8d709087e0279afa37011353",
    "createdAt": "2015-04-27T05:27:54Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.network.sasl;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.WritableByteChannel;\n+import java.util.List;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.netty.buffer.ByteBuf;\n+import io.netty.buffer.Unpooled;\n+import io.netty.channel.Channel;\n+import io.netty.channel.ChannelHandlerContext;\n+import io.netty.channel.ChannelOutboundHandlerAdapter;\n+import io.netty.channel.ChannelPromise;\n+import io.netty.channel.FileRegion;\n+import io.netty.handler.codec.MessageToMessageDecoder;\n+import io.netty.util.AbstractReferenceCounted;\n+import io.netty.util.ReferenceCountUtil;\n+\n+import org.apache.spark.network.util.ByteArrayWritableChannel;\n+import org.apache.spark.network.util.NettyUtils;\n+\n+class SaslEncryption {\n+\n+  @VisibleForTesting\n+  static final String ENCRYPTION_HANDLER_NAME = \"saslEncryption\";\n+\n+  /**\n+   * Adds channel handlers that perform encryption / decryption of data using SASL.\n+   *\n+   * @param channel The channel.\n+   * @param backend The SASL backend.\n+   * @param maxOutboundBlockSize Max size in bytes of outgoing encrypted blocks, to control\n+   *                             memory usage.\n+   */\n+  static void addToChannel(\n+      Channel channel,\n+      SaslEncryptionBackend backend,\n+      int maxOutboundBlockSize) {\n+    channel.pipeline()\n+      .addFirst(ENCRYPTION_HANDLER_NAME, new EncryptionHandler(backend, maxOutboundBlockSize))\n+      .addFirst(\"saslDecryption\", new DecryptionHandler(backend))\n+      .addFirst(\"saslFrameDecoder\", NettyUtils.createFrameDecoder());\n+  }\n+\n+  private static class EncryptionHandler extends ChannelOutboundHandlerAdapter {\n+\n+    private final int maxOutboundBlockSize;\n+    private final SaslEncryptionBackend backend;\n+\n+    EncryptionHandler(SaslEncryptionBackend backend, int maxOutboundBlockSize) {\n+      this.backend = backend;\n+      this.maxOutboundBlockSize = maxOutboundBlockSize;\n+    }\n+\n+    /**\n+     * Wrap the incoming message in an implementation that will perform encryption lazily. This is\n+     * needed to guarantee ordering of the outgoing encrypted packets - they need to be decrypted in\n+     * the same order, and netty doesn't have an atomic ChannelHandlerContext.write() API, so it\n+     * does not guarantee any ordering.\n+     */\n+    @Override\n+    public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise)\n+      throws Exception {\n+\n+      ctx.write(new EncryptedMessage(backend, msg, maxOutboundBlockSize), promise);\n+    }\n+\n+    @Override\n+    public void handlerRemoved(ChannelHandlerContext ctx) throws Exception {\n+      try {\n+        backend.dispose();\n+      } finally {\n+        super.handlerRemoved(ctx);\n+      }\n+    }\n+\n+  }\n+\n+  private static class DecryptionHandler extends MessageToMessageDecoder<ByteBuf> {\n+\n+    private final SaslEncryptionBackend backend;\n+\n+    DecryptionHandler(SaslEncryptionBackend backend) {\n+      this.backend = backend;\n+    }\n+\n+    @Override\n+    protected void decode(ChannelHandlerContext ctx, ByteBuf msg, List<Object> out)\n+      throws Exception {\n+\n+      byte[] data;\n+      int offset;\n+      int length = msg.readableBytes();\n+      if (msg.hasArray()) {\n+        data = msg.array();\n+        offset = msg.arrayOffset();\n+      } else {\n+        data = new byte[length];\n+        msg.readBytes(data);\n+        offset = 0;\n+      }\n+\n+      out.add(Unpooled.wrappedBuffer(backend.unwrap(data, offset, length)));\n+    }\n+\n+  }\n+\n+  private static class EncryptedMessage extends AbstractReferenceCounted implements FileRegion {\n+\n+    private final SaslEncryptionBackend backend;\n+    private final boolean isByteBuf;\n+    private final ByteBuf buf;\n+    private final FileRegion region;\n+    private final ByteArrayWritableChannel byteChannel;\n+\n+    private ByteBuf currentHeader;\n+    private ByteBuffer currentChunk;\n+    private long currentChunkSize;\n+    private long unencryptedChunkSize;\n+    private long transferred;\n+\n+    EncryptedMessage(SaslEncryptionBackend backend, Object msg, int maxOutboundBlockSize) {\n+      Preconditions.checkArgument(msg instanceof ByteBuf || msg instanceof FileRegion,\n+        \"Unrecognized message type: %s\", msg.getClass().getName());\n+      this.backend = backend;\n+      this.isByteBuf = msg instanceof ByteBuf;\n+      this.buf = isByteBuf ? (ByteBuf) msg : null;\n+      this.region = isByteBuf ? null : (FileRegion) msg;\n+      this.byteChannel = new ByteArrayWritableChannel(maxOutboundBlockSize);\n+    }\n+\n+    /**\n+     * Returns the size of the original (unencrypted) message.\n+     *\n+     * This makes assumptions about how netty treats FileRegion instances, because there's no way\n+     * to know beforehand what will be the size of the encrypted message. Namely, it assumes\n+     * that netty will try to transfer data from this message while\n+     * <code>transfered() < count()</code>. So these two methods return, technically, wrong data,\n+     * but netty doesn't know better.\n+     */\n+    @Override\n+    public long count() {\n+      return isByteBuf ? buf.readableBytes() : region.count();\n+    }\n+\n+    @Override\n+    public long position() {\n+      return 0;\n+    }\n+\n+    /**\n+     * Returns an approximation of the amount of data transferred. See {@link #count()}.\n+     */\n+    @Override\n+    public long transfered() {\n+      return transferred;\n+    }\n+\n+    /**\n+     * Transfers data from the original message to the channel, encrypting it in the process.\n+     *\n+     * This method also breaks down the original message into smaller chunks when needed. This\n+     * is done to keep memory usage under control. This avoids having to copy the whole message\n+     * data into memory at once, and can avoid ballooning memory usage when transferring large\n+     * messages such as shuffle blocks.\n+     *\n+     * The {@link #transfered()} counter also behaves a little funny, in that it won't go forward\n+     * until a whole chunk has been written. This is done because the code can't use the actual\n+     * number of bytes written to the channel as the transferred count (see {@link #count()}).\n+     * Instead, once an encrypted chunk is written to the output (including its header), the\n+     * size of the original block will be added to the {@link #transfered()} amount.\n+     */\n+    @Override\n+    public long transferTo(final WritableByteChannel target, final long position)\n+      throws IOException {\n+\n+      Preconditions.checkArgument(position == transfered(), \"Invalid position.\");\n+\n+      long written = 0;\n+      do {\n+        if (currentChunk == null) {\n+          nextChunk();\n+        }\n+\n+        if (currentHeader.readableBytes() > 0) {\n+          int bytesWritten = target.write(currentHeader.nioBuffer());\n+          currentHeader.skipBytes(bytesWritten);\n+          if (currentHeader.readableBytes() > 0) {\n+            // Break out of loop if there are still header bytes left to write.\n+            break;\n+          }\n+        }\n+\n+        target.write(currentChunk);\n+        if (!currentChunk.hasRemaining()) {\n+          // Only update the count of written bytes once a full chunk has been written."
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "I see. I could return `1` instead, and keep the counts synchronized, but some pathological case might still require returning `0`. That should be rare, though, so let me do this to avoid the performance hit in the common case.\n",
    "commit": "ff019662055edeaf8d709087e0279afa37011353",
    "createdAt": "2015-04-27T20:56:30Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.network.sasl;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.WritableByteChannel;\n+import java.util.List;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.netty.buffer.ByteBuf;\n+import io.netty.buffer.Unpooled;\n+import io.netty.channel.Channel;\n+import io.netty.channel.ChannelHandlerContext;\n+import io.netty.channel.ChannelOutboundHandlerAdapter;\n+import io.netty.channel.ChannelPromise;\n+import io.netty.channel.FileRegion;\n+import io.netty.handler.codec.MessageToMessageDecoder;\n+import io.netty.util.AbstractReferenceCounted;\n+import io.netty.util.ReferenceCountUtil;\n+\n+import org.apache.spark.network.util.ByteArrayWritableChannel;\n+import org.apache.spark.network.util.NettyUtils;\n+\n+class SaslEncryption {\n+\n+  @VisibleForTesting\n+  static final String ENCRYPTION_HANDLER_NAME = \"saslEncryption\";\n+\n+  /**\n+   * Adds channel handlers that perform encryption / decryption of data using SASL.\n+   *\n+   * @param channel The channel.\n+   * @param backend The SASL backend.\n+   * @param maxOutboundBlockSize Max size in bytes of outgoing encrypted blocks, to control\n+   *                             memory usage.\n+   */\n+  static void addToChannel(\n+      Channel channel,\n+      SaslEncryptionBackend backend,\n+      int maxOutboundBlockSize) {\n+    channel.pipeline()\n+      .addFirst(ENCRYPTION_HANDLER_NAME, new EncryptionHandler(backend, maxOutboundBlockSize))\n+      .addFirst(\"saslDecryption\", new DecryptionHandler(backend))\n+      .addFirst(\"saslFrameDecoder\", NettyUtils.createFrameDecoder());\n+  }\n+\n+  private static class EncryptionHandler extends ChannelOutboundHandlerAdapter {\n+\n+    private final int maxOutboundBlockSize;\n+    private final SaslEncryptionBackend backend;\n+\n+    EncryptionHandler(SaslEncryptionBackend backend, int maxOutboundBlockSize) {\n+      this.backend = backend;\n+      this.maxOutboundBlockSize = maxOutboundBlockSize;\n+    }\n+\n+    /**\n+     * Wrap the incoming message in an implementation that will perform encryption lazily. This is\n+     * needed to guarantee ordering of the outgoing encrypted packets - they need to be decrypted in\n+     * the same order, and netty doesn't have an atomic ChannelHandlerContext.write() API, so it\n+     * does not guarantee any ordering.\n+     */\n+    @Override\n+    public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise)\n+      throws Exception {\n+\n+      ctx.write(new EncryptedMessage(backend, msg, maxOutboundBlockSize), promise);\n+    }\n+\n+    @Override\n+    public void handlerRemoved(ChannelHandlerContext ctx) throws Exception {\n+      try {\n+        backend.dispose();\n+      } finally {\n+        super.handlerRemoved(ctx);\n+      }\n+    }\n+\n+  }\n+\n+  private static class DecryptionHandler extends MessageToMessageDecoder<ByteBuf> {\n+\n+    private final SaslEncryptionBackend backend;\n+\n+    DecryptionHandler(SaslEncryptionBackend backend) {\n+      this.backend = backend;\n+    }\n+\n+    @Override\n+    protected void decode(ChannelHandlerContext ctx, ByteBuf msg, List<Object> out)\n+      throws Exception {\n+\n+      byte[] data;\n+      int offset;\n+      int length = msg.readableBytes();\n+      if (msg.hasArray()) {\n+        data = msg.array();\n+        offset = msg.arrayOffset();\n+      } else {\n+        data = new byte[length];\n+        msg.readBytes(data);\n+        offset = 0;\n+      }\n+\n+      out.add(Unpooled.wrappedBuffer(backend.unwrap(data, offset, length)));\n+    }\n+\n+  }\n+\n+  private static class EncryptedMessage extends AbstractReferenceCounted implements FileRegion {\n+\n+    private final SaslEncryptionBackend backend;\n+    private final boolean isByteBuf;\n+    private final ByteBuf buf;\n+    private final FileRegion region;\n+    private final ByteArrayWritableChannel byteChannel;\n+\n+    private ByteBuf currentHeader;\n+    private ByteBuffer currentChunk;\n+    private long currentChunkSize;\n+    private long unencryptedChunkSize;\n+    private long transferred;\n+\n+    EncryptedMessage(SaslEncryptionBackend backend, Object msg, int maxOutboundBlockSize) {\n+      Preconditions.checkArgument(msg instanceof ByteBuf || msg instanceof FileRegion,\n+        \"Unrecognized message type: %s\", msg.getClass().getName());\n+      this.backend = backend;\n+      this.isByteBuf = msg instanceof ByteBuf;\n+      this.buf = isByteBuf ? (ByteBuf) msg : null;\n+      this.region = isByteBuf ? null : (FileRegion) msg;\n+      this.byteChannel = new ByteArrayWritableChannel(maxOutboundBlockSize);\n+    }\n+\n+    /**\n+     * Returns the size of the original (unencrypted) message.\n+     *\n+     * This makes assumptions about how netty treats FileRegion instances, because there's no way\n+     * to know beforehand what will be the size of the encrypted message. Namely, it assumes\n+     * that netty will try to transfer data from this message while\n+     * <code>transfered() < count()</code>. So these two methods return, technically, wrong data,\n+     * but netty doesn't know better.\n+     */\n+    @Override\n+    public long count() {\n+      return isByteBuf ? buf.readableBytes() : region.count();\n+    }\n+\n+    @Override\n+    public long position() {\n+      return 0;\n+    }\n+\n+    /**\n+     * Returns an approximation of the amount of data transferred. See {@link #count()}.\n+     */\n+    @Override\n+    public long transfered() {\n+      return transferred;\n+    }\n+\n+    /**\n+     * Transfers data from the original message to the channel, encrypting it in the process.\n+     *\n+     * This method also breaks down the original message into smaller chunks when needed. This\n+     * is done to keep memory usage under control. This avoids having to copy the whole message\n+     * data into memory at once, and can avoid ballooning memory usage when transferring large\n+     * messages such as shuffle blocks.\n+     *\n+     * The {@link #transfered()} counter also behaves a little funny, in that it won't go forward\n+     * until a whole chunk has been written. This is done because the code can't use the actual\n+     * number of bytes written to the channel as the transferred count (see {@link #count()}).\n+     * Instead, once an encrypted chunk is written to the output (including its header), the\n+     * size of the original block will be added to the {@link #transfered()} amount.\n+     */\n+    @Override\n+    public long transferTo(final WritableByteChannel target, final long position)\n+      throws IOException {\n+\n+      Preconditions.checkArgument(position == transfered(), \"Invalid position.\");\n+\n+      long written = 0;\n+      do {\n+        if (currentChunk == null) {\n+          nextChunk();\n+        }\n+\n+        if (currentHeader.readableBytes() > 0) {\n+          int bytesWritten = target.write(currentHeader.nioBuffer());\n+          currentHeader.skipBytes(bytesWritten);\n+          if (currentHeader.readableBytes() > 0) {\n+            // Break out of loop if there are still header bytes left to write.\n+            break;\n+          }\n+        }\n+\n+        target.write(currentChunk);\n+        if (!currentChunk.hasRemaining()) {\n+          // Only update the count of written bytes once a full chunk has been written."
  }],
  "prId": 5377
}, {
  "comments": [{
    "author": {
      "login": "aarondav"
    },
    "body": "Please add a class comment\n",
    "commit": "ff019662055edeaf8d709087e0279afa37011353",
    "createdAt": "2015-04-27T05:41:53Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.network.sasl;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.WritableByteChannel;\n+import java.util.List;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.netty.buffer.ByteBuf;\n+import io.netty.buffer.Unpooled;\n+import io.netty.channel.Channel;\n+import io.netty.channel.ChannelHandlerContext;\n+import io.netty.channel.ChannelOutboundHandlerAdapter;\n+import io.netty.channel.ChannelPromise;\n+import io.netty.channel.FileRegion;\n+import io.netty.handler.codec.MessageToMessageDecoder;\n+import io.netty.util.AbstractReferenceCounted;\n+import io.netty.util.ReferenceCountUtil;\n+\n+import org.apache.spark.network.util.ByteArrayWritableChannel;\n+import org.apache.spark.network.util.NettyUtils;\n+\n+class SaslEncryption {"
  }],
  "prId": 5377
}, {
  "comments": [{
    "author": {
      "login": "aarondav"
    },
    "body": "won't this count() change as we read bytes?\n",
    "commit": "ff019662055edeaf8d709087e0279afa37011353",
    "createdAt": "2015-04-28T00:03:45Z",
    "diffHunk": "@@ -0,0 +1,302 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.network.sasl;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.Channels;\n+import java.nio.channels.WritableByteChannel;\n+import java.util.List;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.netty.buffer.ByteBuf;\n+import io.netty.buffer.Unpooled;\n+import io.netty.channel.Channel;\n+import io.netty.channel.ChannelHandlerContext;\n+import io.netty.channel.ChannelOutboundHandlerAdapter;\n+import io.netty.channel.ChannelPromise;\n+import io.netty.channel.FileRegion;\n+import io.netty.handler.codec.MessageToMessageDecoder;\n+import io.netty.util.AbstractReferenceCounted;\n+import io.netty.util.ReferenceCountUtil;\n+\n+import org.apache.spark.network.util.NettyUtils;\n+\n+/**\n+ * Provides SASL-based encription for transport channels. The single method exposed by this\n+ * class installs the needed channel handlers on a connected channel.\n+ */\n+class SaslEncryption {\n+\n+  @VisibleForTesting\n+  static final String ENCRYPTION_HANDLER_NAME = \"saslEncryption\";\n+\n+  /**\n+   * Adds channel handlers that perform encryption / decryption of data using SASL.\n+   *\n+   * @param channel The channel.\n+   * @param backend The SASL backend.\n+   * @param maxOutboundBlockSize Max size in bytes of outgoing encrypted blocks, to control\n+   *                             memory usage.\n+   */\n+  static void addToChannel(\n+      Channel channel,\n+      SaslEncryptionBackend backend,\n+      int maxOutboundBlockSize) {\n+    channel.pipeline()\n+      .addFirst(ENCRYPTION_HANDLER_NAME, new EncryptionHandler(backend, maxOutboundBlockSize))\n+      .addFirst(\"saslDecryption\", new DecryptionHandler(backend))\n+      .addFirst(\"saslFrameDecoder\", NettyUtils.createFrameDecoder());\n+  }\n+\n+  private static class EncryptionHandler extends ChannelOutboundHandlerAdapter {\n+\n+    private final int maxOutboundBlockSize;\n+    private final SaslEncryptionBackend backend;\n+\n+    EncryptionHandler(SaslEncryptionBackend backend, int maxOutboundBlockSize) {\n+      this.backend = backend;\n+      this.maxOutboundBlockSize = maxOutboundBlockSize;\n+    }\n+\n+    /**\n+     * Wrap the incoming message in an implementation that will perform encryption lazily. This is\n+     * needed to guarantee ordering of the outgoing encrypted packets - they need to be decrypted in\n+     * the same order, and netty doesn't have an atomic ChannelHandlerContext.write() API, so it\n+     * does not guarantee any ordering.\n+     */\n+    @Override\n+    public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise)\n+      throws Exception {\n+\n+      ctx.write(new EncryptedMessage(backend, msg, maxOutboundBlockSize), promise);\n+    }\n+\n+    @Override\n+    public void handlerRemoved(ChannelHandlerContext ctx) throws Exception {\n+      try {\n+        backend.dispose();\n+      } finally {\n+        super.handlerRemoved(ctx);\n+      }\n+    }\n+\n+  }\n+\n+  private static class DecryptionHandler extends MessageToMessageDecoder<ByteBuf> {\n+\n+    private final SaslEncryptionBackend backend;\n+\n+    DecryptionHandler(SaslEncryptionBackend backend) {\n+      this.backend = backend;\n+    }\n+\n+    @Override\n+    protected void decode(ChannelHandlerContext ctx, ByteBuf msg, List<Object> out)\n+      throws Exception {\n+\n+      byte[] data;\n+      int offset;\n+      int length = msg.readableBytes();\n+      if (msg.hasArray()) {\n+        data = msg.array();\n+        offset = msg.arrayOffset();\n+        msg.skipBytes(length);\n+      } else {\n+        data = new byte[length];\n+        msg.readBytes(data);\n+        offset = 0;\n+      }\n+\n+      out.add(Unpooled.wrappedBuffer(backend.unwrap(data, offset, length)));\n+    }\n+\n+  }\n+\n+  @VisibleForTesting\n+  static class EncryptedMessage extends AbstractReferenceCounted implements FileRegion {\n+\n+    private final SaslEncryptionBackend backend;\n+    private final boolean isByteBuf;\n+    private final ByteBuf buf;\n+    private final FileRegion region;\n+    private final int maxOutboundBlockSize;\n+    private final ExposedByteArrayOutputStream byteStream;\n+\n+    private ByteBuf currentHeader;\n+    private ByteBuffer currentChunk;\n+    private long currentChunkSize;\n+    private long currentReportedBytes;\n+    private long unencryptedChunkSize;\n+    private long transferred;\n+\n+    EncryptedMessage(SaslEncryptionBackend backend, Object msg, int maxOutboundBlockSize) {\n+      Preconditions.checkArgument(msg instanceof ByteBuf || msg instanceof FileRegion,\n+        \"Unrecognized message type: %s\", msg.getClass().getName());\n+      this.backend = backend;\n+      this.isByteBuf = msg instanceof ByteBuf;\n+      this.buf = isByteBuf ? (ByteBuf) msg : null;\n+      this.region = isByteBuf ? null : (FileRegion) msg;\n+      this.maxOutboundBlockSize = maxOutboundBlockSize;\n+      this.byteStream = new ExposedByteArrayOutputStream(maxOutboundBlockSize);\n+    }\n+\n+    /**\n+     * Returns the size of the original (unencrypted) message.\n+     *\n+     * This makes assumptions about how netty treats FileRegion instances, because there's no way\n+     * to know beforehand what will be the size of the encrypted message. Namely, it assumes\n+     * that netty will try to transfer data from this message while\n+     * <code>transfered() < count()</code>. So these two methods return, technically, wrong data,\n+     * but netty doesn't know better.\n+     */\n+    @Override\n+    public long count() {\n+      return isByteBuf ? buf.readableBytes() : region.count();",
    "line": 175
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "It's unclear from the docs, but in the existing implementations (e.g. LazyFileRegion), `count()` always returns the same value. Also, netty does [this](https://github.com/netty/netty/blob/33f75d374091946058fc334c3cdbcd0f0a59d9b3/transport/src/main/java/io/netty/channel/nio/AbstractNioByteChannel.java#L189):\n\n```\nboolean done = region.transfered() >= region.count();\n```\n\nWhich implies that `count()` should not change as bytes are transferred.\n",
    "commit": "ff019662055edeaf8d709087e0279afa37011353",
    "createdAt": "2015-04-28T00:21:28Z",
    "diffHunk": "@@ -0,0 +1,302 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.network.sasl;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.Channels;\n+import java.nio.channels.WritableByteChannel;\n+import java.util.List;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.netty.buffer.ByteBuf;\n+import io.netty.buffer.Unpooled;\n+import io.netty.channel.Channel;\n+import io.netty.channel.ChannelHandlerContext;\n+import io.netty.channel.ChannelOutboundHandlerAdapter;\n+import io.netty.channel.ChannelPromise;\n+import io.netty.channel.FileRegion;\n+import io.netty.handler.codec.MessageToMessageDecoder;\n+import io.netty.util.AbstractReferenceCounted;\n+import io.netty.util.ReferenceCountUtil;\n+\n+import org.apache.spark.network.util.NettyUtils;\n+\n+/**\n+ * Provides SASL-based encription for transport channels. The single method exposed by this\n+ * class installs the needed channel handlers on a connected channel.\n+ */\n+class SaslEncryption {\n+\n+  @VisibleForTesting\n+  static final String ENCRYPTION_HANDLER_NAME = \"saslEncryption\";\n+\n+  /**\n+   * Adds channel handlers that perform encryption / decryption of data using SASL.\n+   *\n+   * @param channel The channel.\n+   * @param backend The SASL backend.\n+   * @param maxOutboundBlockSize Max size in bytes of outgoing encrypted blocks, to control\n+   *                             memory usage.\n+   */\n+  static void addToChannel(\n+      Channel channel,\n+      SaslEncryptionBackend backend,\n+      int maxOutboundBlockSize) {\n+    channel.pipeline()\n+      .addFirst(ENCRYPTION_HANDLER_NAME, new EncryptionHandler(backend, maxOutboundBlockSize))\n+      .addFirst(\"saslDecryption\", new DecryptionHandler(backend))\n+      .addFirst(\"saslFrameDecoder\", NettyUtils.createFrameDecoder());\n+  }\n+\n+  private static class EncryptionHandler extends ChannelOutboundHandlerAdapter {\n+\n+    private final int maxOutboundBlockSize;\n+    private final SaslEncryptionBackend backend;\n+\n+    EncryptionHandler(SaslEncryptionBackend backend, int maxOutboundBlockSize) {\n+      this.backend = backend;\n+      this.maxOutboundBlockSize = maxOutboundBlockSize;\n+    }\n+\n+    /**\n+     * Wrap the incoming message in an implementation that will perform encryption lazily. This is\n+     * needed to guarantee ordering of the outgoing encrypted packets - they need to be decrypted in\n+     * the same order, and netty doesn't have an atomic ChannelHandlerContext.write() API, so it\n+     * does not guarantee any ordering.\n+     */\n+    @Override\n+    public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise)\n+      throws Exception {\n+\n+      ctx.write(new EncryptedMessage(backend, msg, maxOutboundBlockSize), promise);\n+    }\n+\n+    @Override\n+    public void handlerRemoved(ChannelHandlerContext ctx) throws Exception {\n+      try {\n+        backend.dispose();\n+      } finally {\n+        super.handlerRemoved(ctx);\n+      }\n+    }\n+\n+  }\n+\n+  private static class DecryptionHandler extends MessageToMessageDecoder<ByteBuf> {\n+\n+    private final SaslEncryptionBackend backend;\n+\n+    DecryptionHandler(SaslEncryptionBackend backend) {\n+      this.backend = backend;\n+    }\n+\n+    @Override\n+    protected void decode(ChannelHandlerContext ctx, ByteBuf msg, List<Object> out)\n+      throws Exception {\n+\n+      byte[] data;\n+      int offset;\n+      int length = msg.readableBytes();\n+      if (msg.hasArray()) {\n+        data = msg.array();\n+        offset = msg.arrayOffset();\n+        msg.skipBytes(length);\n+      } else {\n+        data = new byte[length];\n+        msg.readBytes(data);\n+        offset = 0;\n+      }\n+\n+      out.add(Unpooled.wrappedBuffer(backend.unwrap(data, offset, length)));\n+    }\n+\n+  }\n+\n+  @VisibleForTesting\n+  static class EncryptedMessage extends AbstractReferenceCounted implements FileRegion {\n+\n+    private final SaslEncryptionBackend backend;\n+    private final boolean isByteBuf;\n+    private final ByteBuf buf;\n+    private final FileRegion region;\n+    private final int maxOutboundBlockSize;\n+    private final ExposedByteArrayOutputStream byteStream;\n+\n+    private ByteBuf currentHeader;\n+    private ByteBuffer currentChunk;\n+    private long currentChunkSize;\n+    private long currentReportedBytes;\n+    private long unencryptedChunkSize;\n+    private long transferred;\n+\n+    EncryptedMessage(SaslEncryptionBackend backend, Object msg, int maxOutboundBlockSize) {\n+      Preconditions.checkArgument(msg instanceof ByteBuf || msg instanceof FileRegion,\n+        \"Unrecognized message type: %s\", msg.getClass().getName());\n+      this.backend = backend;\n+      this.isByteBuf = msg instanceof ByteBuf;\n+      this.buf = isByteBuf ? (ByteBuf) msg : null;\n+      this.region = isByteBuf ? null : (FileRegion) msg;\n+      this.maxOutboundBlockSize = maxOutboundBlockSize;\n+      this.byteStream = new ExposedByteArrayOutputStream(maxOutboundBlockSize);\n+    }\n+\n+    /**\n+     * Returns the size of the original (unencrypted) message.\n+     *\n+     * This makes assumptions about how netty treats FileRegion instances, because there's no way\n+     * to know beforehand what will be the size of the encrypted message. Namely, it assumes\n+     * that netty will try to transfer data from this message while\n+     * <code>transfered() < count()</code>. So these two methods return, technically, wrong data,\n+     * but netty doesn't know better.\n+     */\n+    @Override\n+    public long count() {\n+      return isByteBuf ? buf.readableBytes() : region.count();",
    "line": 175
  }],
  "prId": 5377
}]