[{
  "comments": [{
    "author": {
      "login": "felixcheung"
    },
    "body": "this is added to createExternalTable (won't work with json source otherwise)\r\neverything else is simply moved as-is from SQLContext.R",
    "commit": "aff13a860282375a650f6323987c73364ed439cd",
    "createdAt": "2017-03-30T16:33:09Z",
    "diffHunk": "@@ -0,0 +1,478 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# catalog.R: SparkSession catalog functions\n+\n+#' Create an external table\n+#'\n+#' Creates an external table based on the dataset in a data source,\n+#' Returns a SparkDataFrame associated with the external table.\n+#'\n+#' The data source is specified by the \\code{source} and a set of options(...).\n+#' If \\code{source} is not specified, the default data source configured by\n+#' \"spark.sql.sources.default\" will be used.\n+#'\n+#' @param tableName a name of the table.\n+#' @param path the path of files to load.\n+#' @param source the name of external data source.\n+#' @param schema the schema of the data for certain data source.",
    "line": 32
  }],
  "prId": 17483
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "Just FYI, `createExternalTable ` is deprecated. See the PR: https://github.com/apache/spark/pull/16528\r\n\r\nLet me make the corresponding changes in SQLContext too. ",
    "commit": "aff13a860282375a650f6323987c73364ed439cd",
    "createdAt": "2017-03-30T21:44:20Z",
    "diffHunk": "@@ -0,0 +1,478 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# catalog.R: SparkSession catalog functions\n+\n+#' Create an external table\n+#'\n+#' Creates an external table based on the dataset in a data source,\n+#' Returns a SparkDataFrame associated with the external table.\n+#'\n+#' The data source is specified by the \\code{source} and a set of options(...).\n+#' If \\code{source} is not specified, the default data source configured by\n+#' \"spark.sql.sources.default\" will be used.\n+#'\n+#' @param tableName a name of the table.\n+#' @param path the path of files to load.\n+#' @param source the name of external data source.\n+#' @param schema the schema of the data for certain data source.\n+#' @param ... additional argument(s) passed to the method.\n+#' @return A SparkDataFrame.\n+#' @rdname createExternalTable\n+#' @export\n+#' @examples\n+#'\\dontrun{\n+#' sparkR.session()\n+#' df <- createExternalTable(\"myjson\", path=\"path/to/json\", source=\"json\", schema)\n+#' }\n+#' @name createExternalTable\n+#' @method createExternalTable default\n+#' @note createExternalTable since 1.4.0\n+createExternalTable.default <- function(tableName, path = NULL, source = NULL, schema = NULL, ...) {\n+  sparkSession <- getSparkSession()\n+  options <- varargsToStrEnv(...)\n+  if (!is.null(path)) {\n+    options[[\"path\"]] <- path\n+  }\n+  catalog <- callJMethod(sparkSession, \"catalog\")\n+  if (!is.null(schema)) {\n+    sdf <- callJMethod(catalog, \"createExternalTable\", tableName, source, options)\n+  } else {\n+    sdf <- callJMethod(catalog, \"createExternalTable\", tableName, source, schema$jobj, options)\n+  }\n+  dataFrame(sdf)\n+}\n+\n+createExternalTable <- function(x, ...) {",
    "line": 60
  }, {
    "author": {
      "login": "felixcheung"
    },
    "body": "yes, I'm aware. I'm not sure if we need to make changes to SQLContext - schema is required for json source but it's ok in Scala to use createTable instead.\r\n\r\nIt makes sense to add in R here though because\r\n- there is no createTable method (hmm, reviewing that PR you reference, maybe we should add it)\r\n- createTable just sounds too generic and too much like many existing R methods (in R, table is everywhere!), that I wasn't sure it's a good idea to add in R\r\n- createExternalTable since 2.0 is decoupled from SQLContext or SparkSession - it doesn't take either as parameter and it's calling on catalog\r\n",
    "commit": "aff13a860282375a650f6323987c73364ed439cd",
    "createdAt": "2017-03-31T03:32:25Z",
    "diffHunk": "@@ -0,0 +1,478 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# catalog.R: SparkSession catalog functions\n+\n+#' Create an external table\n+#'\n+#' Creates an external table based on the dataset in a data source,\n+#' Returns a SparkDataFrame associated with the external table.\n+#'\n+#' The data source is specified by the \\code{source} and a set of options(...).\n+#' If \\code{source} is not specified, the default data source configured by\n+#' \"spark.sql.sources.default\" will be used.\n+#'\n+#' @param tableName a name of the table.\n+#' @param path the path of files to load.\n+#' @param source the name of external data source.\n+#' @param schema the schema of the data for certain data source.\n+#' @param ... additional argument(s) passed to the method.\n+#' @return A SparkDataFrame.\n+#' @rdname createExternalTable\n+#' @export\n+#' @examples\n+#'\\dontrun{\n+#' sparkR.session()\n+#' df <- createExternalTable(\"myjson\", path=\"path/to/json\", source=\"json\", schema)\n+#' }\n+#' @name createExternalTable\n+#' @method createExternalTable default\n+#' @note createExternalTable since 1.4.0\n+createExternalTable.default <- function(tableName, path = NULL, source = NULL, schema = NULL, ...) {\n+  sparkSession <- getSparkSession()\n+  options <- varargsToStrEnv(...)\n+  if (!is.null(path)) {\n+    options[[\"path\"]] <- path\n+  }\n+  catalog <- callJMethod(sparkSession, \"catalog\")\n+  if (!is.null(schema)) {\n+    sdf <- callJMethod(catalog, \"createExternalTable\", tableName, source, options)\n+  } else {\n+    sdf <- callJMethod(catalog, \"createExternalTable\", tableName, source, schema$jobj, options)\n+  }\n+  dataFrame(sdf)\n+}\n+\n+createExternalTable <- function(x, ...) {",
    "line": 60
  }, {
    "author": {
      "login": "shivaram"
    },
    "body": "I agree that `createTable` sounds very general, but I dont think its used by base R or any popular R package ?",
    "commit": "aff13a860282375a650f6323987c73364ed439cd",
    "createdAt": "2017-03-31T22:01:34Z",
    "diffHunk": "@@ -0,0 +1,478 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# catalog.R: SparkSession catalog functions\n+\n+#' Create an external table\n+#'\n+#' Creates an external table based on the dataset in a data source,\n+#' Returns a SparkDataFrame associated with the external table.\n+#'\n+#' The data source is specified by the \\code{source} and a set of options(...).\n+#' If \\code{source} is not specified, the default data source configured by\n+#' \"spark.sql.sources.default\" will be used.\n+#'\n+#' @param tableName a name of the table.\n+#' @param path the path of files to load.\n+#' @param source the name of external data source.\n+#' @param schema the schema of the data for certain data source.\n+#' @param ... additional argument(s) passed to the method.\n+#' @return A SparkDataFrame.\n+#' @rdname createExternalTable\n+#' @export\n+#' @examples\n+#'\\dontrun{\n+#' sparkR.session()\n+#' df <- createExternalTable(\"myjson\", path=\"path/to/json\", source=\"json\", schema)\n+#' }\n+#' @name createExternalTable\n+#' @method createExternalTable default\n+#' @note createExternalTable since 1.4.0\n+createExternalTable.default <- function(tableName, path = NULL, source = NULL, schema = NULL, ...) {\n+  sparkSession <- getSparkSession()\n+  options <- varargsToStrEnv(...)\n+  if (!is.null(path)) {\n+    options[[\"path\"]] <- path\n+  }\n+  catalog <- callJMethod(sparkSession, \"catalog\")\n+  if (!is.null(schema)) {\n+    sdf <- callJMethod(catalog, \"createExternalTable\", tableName, source, options)\n+  } else {\n+    sdf <- callJMethod(catalog, \"createExternalTable\", tableName, source, schema$jobj, options)\n+  }\n+  dataFrame(sdf)\n+}\n+\n+createExternalTable <- function(x, ...) {",
    "line": 60
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "`createExternalTable` is misleading now, because the table could be `managed` if users did not provide the value of `path`. Thus, we decided to rename it. ",
    "commit": "aff13a860282375a650f6323987c73364ed439cd",
    "createdAt": "2017-04-01T05:54:55Z",
    "diffHunk": "@@ -0,0 +1,478 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# catalog.R: SparkSession catalog functions\n+\n+#' Create an external table\n+#'\n+#' Creates an external table based on the dataset in a data source,\n+#' Returns a SparkDataFrame associated with the external table.\n+#'\n+#' The data source is specified by the \\code{source} and a set of options(...).\n+#' If \\code{source} is not specified, the default data source configured by\n+#' \"spark.sql.sources.default\" will be used.\n+#'\n+#' @param tableName a name of the table.\n+#' @param path the path of files to load.\n+#' @param source the name of external data source.\n+#' @param schema the schema of the data for certain data source.\n+#' @param ... additional argument(s) passed to the method.\n+#' @return A SparkDataFrame.\n+#' @rdname createExternalTable\n+#' @export\n+#' @examples\n+#'\\dontrun{\n+#' sparkR.session()\n+#' df <- createExternalTable(\"myjson\", path=\"path/to/json\", source=\"json\", schema)\n+#' }\n+#' @name createExternalTable\n+#' @method createExternalTable default\n+#' @note createExternalTable since 1.4.0\n+createExternalTable.default <- function(tableName, path = NULL, source = NULL, schema = NULL, ...) {\n+  sparkSession <- getSparkSession()\n+  options <- varargsToStrEnv(...)\n+  if (!is.null(path)) {\n+    options[[\"path\"]] <- path\n+  }\n+  catalog <- callJMethod(sparkSession, \"catalog\")\n+  if (!is.null(schema)) {\n+    sdf <- callJMethod(catalog, \"createExternalTable\", tableName, source, options)\n+  } else {\n+    sdf <- callJMethod(catalog, \"createExternalTable\", tableName, source, schema$jobj, options)\n+  }\n+  dataFrame(sdf)\n+}\n+\n+createExternalTable <- function(x, ...) {",
    "line": 60
  }, {
    "author": {
      "login": "felixcheung"
    },
    "body": "right, I was just concerned that with `data.table`, `read.table` etc, table == data.frame in R as supposed to `hive table` or `managed table`, which could be fairly confusing.\r\nanyway, I think I'll follow up with a PR for `createTable` but as of now `path` is optional for `createExternalTable`, even though it's potentially misleading, it does work now.\r\n",
    "commit": "aff13a860282375a650f6323987c73364ed439cd",
    "createdAt": "2017-04-01T18:12:59Z",
    "diffHunk": "@@ -0,0 +1,478 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# catalog.R: SparkSession catalog functions\n+\n+#' Create an external table\n+#'\n+#' Creates an external table based on the dataset in a data source,\n+#' Returns a SparkDataFrame associated with the external table.\n+#'\n+#' The data source is specified by the \\code{source} and a set of options(...).\n+#' If \\code{source} is not specified, the default data source configured by\n+#' \"spark.sql.sources.default\" will be used.\n+#'\n+#' @param tableName a name of the table.\n+#' @param path the path of files to load.\n+#' @param source the name of external data source.\n+#' @param schema the schema of the data for certain data source.\n+#' @param ... additional argument(s) passed to the method.\n+#' @return A SparkDataFrame.\n+#' @rdname createExternalTable\n+#' @export\n+#' @examples\n+#'\\dontrun{\n+#' sparkR.session()\n+#' df <- createExternalTable(\"myjson\", path=\"path/to/json\", source=\"json\", schema)\n+#' }\n+#' @name createExternalTable\n+#' @method createExternalTable default\n+#' @note createExternalTable since 1.4.0\n+createExternalTable.default <- function(tableName, path = NULL, source = NULL, schema = NULL, ...) {\n+  sparkSession <- getSparkSession()\n+  options <- varargsToStrEnv(...)\n+  if (!is.null(path)) {\n+    options[[\"path\"]] <- path\n+  }\n+  catalog <- callJMethod(sparkSession, \"catalog\")\n+  if (!is.null(schema)) {\n+    sdf <- callJMethod(catalog, \"createExternalTable\", tableName, source, options)\n+  } else {\n+    sdf <- callJMethod(catalog, \"createExternalTable\", tableName, source, schema$jobj, options)\n+  }\n+  dataFrame(sdf)\n+}\n+\n+createExternalTable <- function(x, ...) {",
    "line": 60
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "> If you drop a managed table both data and meta data will be deleted if you drop an external table only metadata is deleted, external table is a way to protect data against accidental drop commands.\r\n\r\nThus, it is a pretty important concept. It could be either Hive or Spark native one. ",
    "commit": "aff13a860282375a650f6323987c73364ed439cd",
    "createdAt": "2017-04-03T00:40:26Z",
    "diffHunk": "@@ -0,0 +1,478 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# catalog.R: SparkSession catalog functions\n+\n+#' Create an external table\n+#'\n+#' Creates an external table based on the dataset in a data source,\n+#' Returns a SparkDataFrame associated with the external table.\n+#'\n+#' The data source is specified by the \\code{source} and a set of options(...).\n+#' If \\code{source} is not specified, the default data source configured by\n+#' \"spark.sql.sources.default\" will be used.\n+#'\n+#' @param tableName a name of the table.\n+#' @param path the path of files to load.\n+#' @param source the name of external data source.\n+#' @param schema the schema of the data for certain data source.\n+#' @param ... additional argument(s) passed to the method.\n+#' @return A SparkDataFrame.\n+#' @rdname createExternalTable\n+#' @export\n+#' @examples\n+#'\\dontrun{\n+#' sparkR.session()\n+#' df <- createExternalTable(\"myjson\", path=\"path/to/json\", source=\"json\", schema)\n+#' }\n+#' @name createExternalTable\n+#' @method createExternalTable default\n+#' @note createExternalTable since 1.4.0\n+createExternalTable.default <- function(tableName, path = NULL, source = NULL, schema = NULL, ...) {\n+  sparkSession <- getSparkSession()\n+  options <- varargsToStrEnv(...)\n+  if (!is.null(path)) {\n+    options[[\"path\"]] <- path\n+  }\n+  catalog <- callJMethod(sparkSession, \"catalog\")\n+  if (!is.null(schema)) {\n+    sdf <- callJMethod(catalog, \"createExternalTable\", tableName, source, options)\n+  } else {\n+    sdf <- callJMethod(catalog, \"createExternalTable\", tableName, source, schema$jobj, options)\n+  }\n+  dataFrame(sdf)\n+}\n+\n+createExternalTable <- function(x, ...) {",
    "line": 60
  }],
  "prId": 17483
}]