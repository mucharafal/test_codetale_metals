[{
  "comments": [{
    "author": {
      "login": "felixcheung"
    },
    "body": "this seems to be `colname` below. should this be `colnames`?\n",
    "commit": "f2b8d97ba80ebd424aeab7d14c09c8225f3daa69",
    "createdAt": "2016-05-27T00:02:09Z",
    "diffHunk": "@@ -126,6 +126,47 @@ methods <- c(\"avg\", \"max\", \"mean\", \"min\", \"sum\")\n # These are not exposed on GroupedData: \"kurtosis\", \"skewness\", \"stddev\", \"stddev_samp\", \"stddev_pop\",\n # \"variance\", \"var_samp\", \"var_pop\"\n \n+#' Pivot\n+#' \n+#' Pivot GroupedData column to expand unique values of this column into set of column named accordingly and perform\n+#' aggregation. Same purpose as dcast in reshape2 or pivot table in Excel\n+#' \n+#' @param x: a GroupedData object\n+#' @param by: name of column to pivot data by"
  }],
  "prId": 13295
}, {
  "comments": [{
    "author": {
      "login": "felixcheung"
    },
    "body": "please add @family, @name\n",
    "commit": "f2b8d97ba80ebd424aeab7d14c09c8225f3daa69",
    "createdAt": "2016-05-27T00:03:44Z",
    "diffHunk": "@@ -126,6 +126,47 @@ methods <- c(\"avg\", \"max\", \"mean\", \"min\", \"sum\")\n # These are not exposed on GroupedData: \"kurtosis\", \"skewness\", \"stddev\", \"stddev_samp\", \"stddev_pop\",\n # \"variance\", \"var_samp\", \"var_pop\"\n \n+#' Pivot\n+#' \n+#' Pivot GroupedData column to expand unique values of this column into set of column named accordingly and perform\n+#' aggregation. Same purpose as dcast in reshape2 or pivot table in Excel\n+#' \n+#' @param x: a GroupedData object\n+#' @param by: name of column to pivot data by\n+#' @param values: a unique list of values that will be translated to columns in the output SparkDataFrame. \n+#' If not given it will be computed on the fly which will influence performance\n+#' @return GroupedData object\n+#' @rdname pivot\n+#' @export \n+#' @examples "
  }],
  "prId": 13295
}, {
  "comments": [{
    "author": {
      "login": "felixcheung"
    },
    "body": "should this be `values=list()`? I think it encourages the use by showing it is a list\n",
    "commit": "f2b8d97ba80ebd424aeab7d14c09c8225f3daa69",
    "createdAt": "2016-05-27T00:04:23Z",
    "diffHunk": "@@ -126,6 +126,47 @@ methods <- c(\"avg\", \"max\", \"mean\", \"min\", \"sum\")\n # These are not exposed on GroupedData: \"kurtosis\", \"skewness\", \"stddev\", \"stddev_samp\", \"stddev_pop\",\n # \"variance\", \"var_samp\", \"var_pop\"\n \n+#' Pivot\n+#' \n+#' Pivot GroupedData column to expand unique values of this column into set of column named accordingly and perform\n+#' aggregation. Same purpose as dcast in reshape2 or pivot table in Excel\n+#' \n+#' @param x: a GroupedData object\n+#' @param by: name of column to pivot data by\n+#' @param values: a unique list of values that will be translated to columns in the output SparkDataFrame. \n+#' If not given it will be computed on the fly which will influence performance\n+#' @return GroupedData object\n+#' @rdname pivot\n+#' @export \n+#' @examples \n+#' \\dontrun{\n+#' library(magrittr)\n+#' df <- data.frame(\n+#'     earnings = c(10000, 10000, 11000, 15000, 12000, 20000, 21000, 22000),\n+#'     course = c(\"R\", \"Python\", \"R\", \"Python\", \"R\", \"Python\", \"R\", \"Python\"),\n+#'     year = c(2013, 2013, 2014, 2014, 2015, 2015, 2016, 2016)\n+#' )\n+#' SparkRdf <- createDataFrame(sqlContext, df)\n+#' values <- list(\"R\", \"Python\")\n+#' sums <- groupBy(SparkRdf, \"year\") %>% \n+#'     pivot(\"course\", values) %>% \n+#'     SparkR::summarize(sumOfEarnings = sum(SparkRdf$earnings) ) %>%\n+#'     collect()\n+#' }\n+\n+setMethod(\"pivot\",\n+          signature(x = \"GroupedData\"),\n+          function(x, colname, values=NULL){"
  }, {
    "author": {
      "login": "mhnatiuk"
    },
    "body": "OK, so I'll add a test to verify that list ain't empty, right? Or does the underlaying scala method accept empty lists?\n",
    "commit": "f2b8d97ba80ebd424aeab7d14c09c8225f3daa69",
    "createdAt": "2016-05-30T12:13:58Z",
    "diffHunk": "@@ -126,6 +126,47 @@ methods <- c(\"avg\", \"max\", \"mean\", \"min\", \"sum\")\n # These are not exposed on GroupedData: \"kurtosis\", \"skewness\", \"stddev\", \"stddev_samp\", \"stddev_pop\",\n # \"variance\", \"var_samp\", \"var_pop\"\n \n+#' Pivot\n+#' \n+#' Pivot GroupedData column to expand unique values of this column into set of column named accordingly and perform\n+#' aggregation. Same purpose as dcast in reshape2 or pivot table in Excel\n+#' \n+#' @param x: a GroupedData object\n+#' @param by: name of column to pivot data by\n+#' @param values: a unique list of values that will be translated to columns in the output SparkDataFrame. \n+#' If not given it will be computed on the fly which will influence performance\n+#' @return GroupedData object\n+#' @rdname pivot\n+#' @export \n+#' @examples \n+#' \\dontrun{\n+#' library(magrittr)\n+#' df <- data.frame(\n+#'     earnings = c(10000, 10000, 11000, 15000, 12000, 20000, 21000, 22000),\n+#'     course = c(\"R\", \"Python\", \"R\", \"Python\", \"R\", \"Python\", \"R\", \"Python\"),\n+#'     year = c(2013, 2013, 2014, 2014, 2015, 2015, 2016, 2016)\n+#' )\n+#' SparkRdf <- createDataFrame(sqlContext, df)\n+#' values <- list(\"R\", \"Python\")\n+#' sums <- groupBy(SparkRdf, \"year\") %>% \n+#'     pivot(\"course\", values) %>% \n+#'     SparkR::summarize(sumOfEarnings = sum(SparkRdf$earnings) ) %>%\n+#'     collect()\n+#' }\n+\n+setMethod(\"pivot\",\n+          signature(x = \"GroupedData\"),\n+          function(x, colname, values=NULL){"
  }],
  "prId": 13295
}, {
  "comments": [{
    "author": {
      "login": "shivaram"
    },
    "body": "style nit: space after `}` here \n",
    "commit": "f2b8d97ba80ebd424aeab7d14c09c8225f3daa69",
    "createdAt": "2016-05-30T17:12:23Z",
    "diffHunk": "@@ -126,6 +126,49 @@ methods <- c(\"avg\", \"max\", \"mean\", \"min\", \"sum\")\n # These are not exposed on GroupedData: \"kurtosis\", \"skewness\", \"stddev\", \"stddev_samp\", \"stddev_pop\",\n # \"variance\", \"var_samp\", \"var_pop\"\n \n+#' Pivot\n+#' \n+#' Pivot GroupedData column to expand unique values of this column into set of column named accordingly and perform\n+#' aggregation. Same purpose as dcast in reshape2 or pivot table in Excel\n+#' \n+#' @param x a GroupedData object\n+#' @param by name of column to pivot data by\n+#' @param values a unique list of values that will be translated to columns in the output SparkDataFrame. \n+#' If not given it will be computed on the fly which will influence performance\n+#' @return GroupedData object\n+#' @rdname pivot\n+#' @family agg\n+#' @name pivot\n+#' @export \n+#' @examples \n+#' \\dontrun{\n+#' library(magrittr)\n+#' df <- data.frame(\n+#'     earnings = c(10000, 10000, 11000, 15000, 12000, 20000, 21000, 22000),\n+#'     course = c(\"R\", \"Python\", \"R\", \"Python\", \"R\", \"Python\", \"R\", \"Python\"),\n+#'     year = c(2013, 2013, 2014, 2014, 2015, 2015, 2016, 2016)\n+#' )\n+#' SparkRdf <- createDataFrame(sqlContext, df)\n+#' values <- list(\"R\", \"Python\")\n+#' sums <- groupBy(SparkRdf, \"year\") %>% \n+#'     pivot(\"course\", values) %>% \n+#'     SparkR::summarize(sumOfEarnings = sum(SparkRdf$earnings) ) %>%\n+#'     collect()\n+#' }\n+\n+setMethod(\"pivot\",\n+          signature(x = \"GroupedData\"),\n+          function(x, colname, values=list()){\n+              if (length(values) == 0) {\n+                  result <- SparkR:::callJMethod(x@sgd, \"pivot\", colname)\n+              }else {"
  }],
  "prId": 13295
}, {
  "comments": [{
    "author": {
      "login": "felixcheung"
    },
    "body": "would be good to add a message - use `if () { stop() }` instead?\n",
    "commit": "f2b8d97ba80ebd424aeab7d14c09c8225f3daa69",
    "createdAt": "2016-05-30T18:18:38Z",
    "diffHunk": "@@ -126,6 +126,49 @@ methods <- c(\"avg\", \"max\", \"mean\", \"min\", \"sum\")\n # These are not exposed on GroupedData: \"kurtosis\", \"skewness\", \"stddev\", \"stddev_samp\", \"stddev_pop\",\n # \"variance\", \"var_samp\", \"var_pop\"\n \n+#' Pivot\n+#' \n+#' Pivot GroupedData column to expand unique values of this column into set of column named accordingly and perform\n+#' aggregation. Same purpose as dcast in reshape2 or pivot table in Excel\n+#' \n+#' @param x a GroupedData object\n+#' @param by name of column to pivot data by\n+#' @param values a unique list of values that will be translated to columns in the output SparkDataFrame. \n+#' If not given it will be computed on the fly which will influence performance\n+#' @return GroupedData object\n+#' @rdname pivot\n+#' @family agg\n+#' @name pivot\n+#' @export \n+#' @examples \n+#' \\dontrun{\n+#' library(magrittr)\n+#' df <- data.frame(\n+#'     earnings = c(10000, 10000, 11000, 15000, 12000, 20000, 21000, 22000),\n+#'     course = c(\"R\", \"Python\", \"R\", \"Python\", \"R\", \"Python\", \"R\", \"Python\"),\n+#'     year = c(2013, 2013, 2014, 2014, 2015, 2015, 2016, 2016)\n+#' )\n+#' SparkRdf <- createDataFrame(sqlContext, df)\n+#' values <- list(\"R\", \"Python\")\n+#' sums <- groupBy(SparkRdf, \"year\") %>% \n+#'     pivot(\"course\", values) %>% \n+#'     SparkR::summarize(sumOfEarnings = sum(SparkRdf$earnings) ) %>%\n+#'     collect()\n+#' }\n+\n+setMethod(\"pivot\",\n+          signature(x = \"GroupedData\"),\n+          function(x, colname, values=list()){\n+              if (length(values) == 0) {\n+                  result <- SparkR:::callJMethod(x@sgd, \"pivot\", colname)\n+              }else {\n+                  stopifnot(length(values) == length(unique(values)))"
  }],
  "prId": 13295
}, {
  "comments": [{
    "author": {
      "login": "felixcheung"
    },
    "body": "why do we need \"SparkR:::\" here and line 168?\n",
    "commit": "f2b8d97ba80ebd424aeab7d14c09c8225f3daa69",
    "createdAt": "2016-05-30T18:19:08Z",
    "diffHunk": "@@ -126,6 +126,49 @@ methods <- c(\"avg\", \"max\", \"mean\", \"min\", \"sum\")\n # These are not exposed on GroupedData: \"kurtosis\", \"skewness\", \"stddev\", \"stddev_samp\", \"stddev_pop\",\n # \"variance\", \"var_samp\", \"var_pop\"\n \n+#' Pivot\n+#' \n+#' Pivot GroupedData column to expand unique values of this column into set of column named accordingly and perform\n+#' aggregation. Same purpose as dcast in reshape2 or pivot table in Excel\n+#' \n+#' @param x a GroupedData object\n+#' @param by name of column to pivot data by\n+#' @param values a unique list of values that will be translated to columns in the output SparkDataFrame. \n+#' If not given it will be computed on the fly which will influence performance\n+#' @return GroupedData object\n+#' @rdname pivot\n+#' @family agg\n+#' @name pivot\n+#' @export \n+#' @examples \n+#' \\dontrun{\n+#' library(magrittr)\n+#' df <- data.frame(\n+#'     earnings = c(10000, 10000, 11000, 15000, 12000, 20000, 21000, 22000),\n+#'     course = c(\"R\", \"Python\", \"R\", \"Python\", \"R\", \"Python\", \"R\", \"Python\"),\n+#'     year = c(2013, 2013, 2014, 2014, 2015, 2015, 2016, 2016)\n+#' )\n+#' SparkRdf <- createDataFrame(sqlContext, df)\n+#' values <- list(\"R\", \"Python\")\n+#' sums <- groupBy(SparkRdf, \"year\") %>% \n+#'     pivot(\"course\", values) %>% \n+#'     SparkR::summarize(sumOfEarnings = sum(SparkRdf$earnings) ) %>%\n+#'     collect()\n+#' }\n+\n+setMethod(\"pivot\",\n+          signature(x = \"GroupedData\"),\n+          function(x, colname, values=list()){\n+              if (length(values) == 0) {\n+                  result <- SparkR:::callJMethod(x@sgd, \"pivot\", colname)\n+              }else {\n+                  stopifnot(length(values) == length(unique(values)))\n+                  result <- SparkR:::callJMethod(x@sgd, \"pivot\", colname, values)"
  }, {
    "author": {
      "login": "felixcheung"
    },
    "body": "and 163\n",
    "commit": "f2b8d97ba80ebd424aeab7d14c09c8225f3daa69",
    "createdAt": "2016-05-30T18:19:21Z",
    "diffHunk": "@@ -126,6 +126,49 @@ methods <- c(\"avg\", \"max\", \"mean\", \"min\", \"sum\")\n # These are not exposed on GroupedData: \"kurtosis\", \"skewness\", \"stddev\", \"stddev_samp\", \"stddev_pop\",\n # \"variance\", \"var_samp\", \"var_pop\"\n \n+#' Pivot\n+#' \n+#' Pivot GroupedData column to expand unique values of this column into set of column named accordingly and perform\n+#' aggregation. Same purpose as dcast in reshape2 or pivot table in Excel\n+#' \n+#' @param x a GroupedData object\n+#' @param by name of column to pivot data by\n+#' @param values a unique list of values that will be translated to columns in the output SparkDataFrame. \n+#' If not given it will be computed on the fly which will influence performance\n+#' @return GroupedData object\n+#' @rdname pivot\n+#' @family agg\n+#' @name pivot\n+#' @export \n+#' @examples \n+#' \\dontrun{\n+#' library(magrittr)\n+#' df <- data.frame(\n+#'     earnings = c(10000, 10000, 11000, 15000, 12000, 20000, 21000, 22000),\n+#'     course = c(\"R\", \"Python\", \"R\", \"Python\", \"R\", \"Python\", \"R\", \"Python\"),\n+#'     year = c(2013, 2013, 2014, 2014, 2015, 2015, 2016, 2016)\n+#' )\n+#' SparkRdf <- createDataFrame(sqlContext, df)\n+#' values <- list(\"R\", \"Python\")\n+#' sums <- groupBy(SparkRdf, \"year\") %>% \n+#'     pivot(\"course\", values) %>% \n+#'     SparkR::summarize(sumOfEarnings = sum(SparkRdf$earnings) ) %>%\n+#'     collect()\n+#' }\n+\n+setMethod(\"pivot\",\n+          signature(x = \"GroupedData\"),\n+          function(x, colname, values=list()){\n+              if (length(values) == 0) {\n+                  result <- SparkR:::callJMethod(x@sgd, \"pivot\", colname)\n+              }else {\n+                  stopifnot(length(values) == length(unique(values)))\n+                  result <- SparkR:::callJMethod(x@sgd, \"pivot\", colname, values)"
  }],
  "prId": 13295
}]