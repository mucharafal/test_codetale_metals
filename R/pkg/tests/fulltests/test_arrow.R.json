[{
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Nothing changed from `test_sparkSQL.R` except this new test.",
    "commit": "a58c86dfeff23a0c0f333b205ad182a4bf5af332",
    "createdAt": "2019-03-05T05:46:55Z",
    "diffHunk": "@@ -0,0 +1,314 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+library(testthat)\n+\n+context(\"SparkSQL Arrow optimization\")\n+\n+sparkSession <- sparkR.session(master = sparkRTestMaster, enableHiveSupport = FALSE)\n+\n+test_that(\"createDataFrame/collect Arrow optimization\", {\n+  skip_if_not_installed(\"arrow\")\n+\n+  conf <- callJMethod(sparkSession, \"conf\")\n+  arrowEnabled <- sparkR.conf(\"spark.sql.execution.arrow.enabled\")[[1]]\n+\n+  callJMethod(conf, \"set\", \"spark.sql.execution.arrow.enabled\", \"false\")\n+  tryCatch({\n+    expected <- collect(createDataFrame(mtcars))\n+  },\n+  finally = {\n+    callJMethod(conf, \"set\", \"spark.sql.execution.arrow.enabled\", arrowEnabled)\n+  })\n+\n+  callJMethod(conf, \"set\", \"spark.sql.execution.arrow.enabled\", \"true\")\n+  tryCatch({\n+    expect_equal(collect(createDataFrame(mtcars)), expected)\n+  },\n+  finally = {\n+    callJMethod(conf, \"set\", \"spark.sql.execution.arrow.enabled\", arrowEnabled)\n+  })\n+})\n+\n+test_that(\"createDataFrame/collect Arrow optimization - many partitions (partition order test)\", {\n+  skip_if_not_installed(\"arrow\")\n+\n+  conf <- callJMethod(sparkSession, \"conf\")\n+  arrowEnabled <- sparkR.conf(\"spark.sql.execution.arrow.enabled\")[[1]]\n+\n+  callJMethod(conf, \"set\", \"spark.sql.execution.arrow.enabled\", \"true\")\n+  tryCatch({\n+    expect_equal(collect(createDataFrame(mtcars, numPartitions = 32)),\n+                 collect(createDataFrame(mtcars, numPartitions = 1)))\n+  },\n+  finally = {\n+    callJMethod(conf, \"set\", \"spark.sql.execution.arrow.enabled\", arrowEnabled)\n+  })\n+})\n+\n+test_that(\"createDataFrame/collect Arrow optimization - type specification\", {\n+  skip_if_not_installed(\"arrow\")\n+  rdf <- data.frame(list(list(a = 1,\n+                              b = \"a\",\n+                              c = TRUE,\n+                              d = 1.1,\n+                              e = 1L,\n+                              f = as.Date(\"1990-02-24\"),\n+                              g = as.POSIXct(\"1990-02-24 12:34:56\"))))\n+\n+  arrowEnabled <- sparkR.conf(\"spark.sql.execution.arrow.enabled\")[[1]]\n+  conf <- callJMethod(sparkSession, \"conf\")\n+\n+  callJMethod(conf, \"set\", \"spark.sql.execution.arrow.enabled\", \"false\")\n+  tryCatch({\n+    expected <- collect(createDataFrame(rdf))\n+  },\n+  finally = {\n+    callJMethod(conf, \"set\", \"spark.sql.execution.arrow.enabled\", arrowEnabled)\n+  })\n+\n+  callJMethod(conf, \"set\", \"spark.sql.execution.arrow.enabled\", \"true\")\n+  tryCatch({\n+    expect_equal(collect(createDataFrame(rdf)), expected)\n+  },\n+  finally = {\n+    callJMethod(conf, \"set\", \"spark.sql.execution.arrow.enabled\", arrowEnabled)\n+  })\n+})\n+\n+test_that(\"dapply() Arrow optimization\", {\n+  skip_if_not_installed(\"arrow\")\n+  df <- createDataFrame(mtcars)\n+\n+  conf <- callJMethod(sparkSession, \"conf\")\n+  arrowEnabled <- sparkR.conf(\"spark.sql.execution.arrow.enabled\")[[1]]\n+\n+  callJMethod(conf, \"set\", \"spark.sql.execution.arrow.enabled\", \"false\")\n+  tryCatch({\n+    ret <- dapply(df,\n+    function(rdf) {\n+      stopifnot(class(rdf) == \"data.frame\")\n+      rdf\n+    },\n+    schema(df))\n+    expected <- collect(ret)\n+  },\n+  finally = {\n+    callJMethod(conf, \"set\", \"spark.sql.execution.arrow.enabled\", arrowEnabled)\n+  })\n+\n+  callJMethod(conf, \"set\", \"spark.sql.execution.arrow.enabled\", \"true\")\n+  tryCatch({\n+    ret <- dapply(df,\n+                  function(rdf) {\n+                    stopifnot(class(rdf) == \"data.frame\")\n+                    # mtcars' hp is more then 50.\n+                    stopifnot(all(rdf$hp > 50))\n+                    rdf\n+                  },\n+                  schema(df))\n+    actual <- collect(ret)\n+    expect_equal(actual, expected)\n+    expect_equal(count(ret), nrow(mtcars))\n+  },\n+  finally = {\n+    callJMethod(conf, \"set\", \"spark.sql.execution.arrow.enabled\", arrowEnabled)\n+  })\n+})\n+\n+test_that(\"dapply() Arrow optimization - type specification\", {\n+  skip_if_not_installed(\"arrow\")\n+  # Note that regular dapply() seems not supporting date and timestamps\n+  # whereas Arrow-optimized dapply() does.\n+  rdf <- data.frame(list(list(a = 1,\n+                              b = \"a\",\n+                              c = TRUE,\n+                              d = 1.1,\n+                              e = 1L)))\n+  # numPartitions are set to 8 intentionally to test empty partitions as well.\n+  df <- createDataFrame(rdf, numPartitions = 8)\n+\n+  conf <- callJMethod(sparkSession, \"conf\")\n+  arrowEnabled <- sparkR.conf(\"spark.sql.execution.arrow.enabled\")[[1]]\n+\n+  callJMethod(conf, \"set\", \"spark.sql.execution.arrow.enabled\", \"false\")\n+  tryCatch({\n+    ret <- dapply(df, function(rdf) { rdf }, schema(df))\n+    expected <- collect(ret)\n+  },\n+  finally = {\n+    callJMethod(conf, \"set\", \"spark.sql.execution.arrow.enabled\", arrowEnabled)\n+  })\n+\n+  callJMethod(conf, \"set\", \"spark.sql.execution.arrow.enabled\", \"true\")\n+  tryCatch({\n+    ret <- dapply(df, function(rdf) { rdf }, schema(df))\n+    actual <- collect(ret)\n+    expect_equal(actual, expected)\n+  },\n+  finally = {\n+    callJMethod(conf, \"set\", \"spark.sql.execution.arrow.enabled\", arrowEnabled)\n+  })\n+})\n+\n+test_that(\"dapply() Arrow optimization - type specification (date and timestamp)\", {\n+  skip_if_not_installed(\"arrow\")\n+  rdf <- data.frame(list(list(a = as.Date(\"1990-02-24\"),\n+                              b = as.POSIXct(\"1990-02-24 12:34:56\"))))\n+  df <- createDataFrame(rdf)\n+\n+  conf <- callJMethod(sparkSession, \"conf\")\n+  arrowEnabled <- sparkR.conf(\"spark.sql.execution.arrow.enabled\")[[1]]\n+\n+  callJMethod(conf, \"set\", \"spark.sql.execution.arrow.enabled\", \"true\")\n+  tryCatch({\n+    ret <- dapply(df, function(rdf) { rdf }, schema(df))\n+    expect_equal(collect(ret), rdf)\n+  },\n+  finally = {\n+    callJMethod(conf, \"set\", \"spark.sql.execution.arrow.enabled\", arrowEnabled)\n+  })\n+})\n+\n+test_that(\"gapply() Arrow optimization\", {\n+  skip_if_not_installed(\"arrow\")\n+  df <- createDataFrame(mtcars)\n+\n+  conf <- callJMethod(sparkSession, \"conf\")\n+  arrowEnabled <- sparkR.conf(\"spark.sql.execution.arrow.enabled\")[[1]]\n+\n+  callJMethod(conf, \"set\", \"spark.sql.execution.arrow.enabled\", \"false\")\n+  tryCatch({\n+    ret <- gapply(df,\n+                 \"gear\",\n+                 function(key, grouped) {\n+                   if (length(key) > 0) {\n+                     stopifnot(is.numeric(key[[1]]))\n+                   }\n+                   stopifnot(class(grouped) == \"data.frame\")\n+                   grouped\n+                 },\n+                 schema(df))\n+    expected <- collect(ret)\n+  },\n+  finally = {\n+    callJMethod(conf, \"set\", \"spark.sql.execution.arrow.enabled\", arrowEnabled)\n+  })\n+\n+  callJMethod(conf, \"set\", \"spark.sql.execution.arrow.enabled\", \"true\")\n+  tryCatch({\n+    ret <- gapply(df,\n+                 \"gear\",\n+                 function(key, grouped) {\n+                   if (length(key) > 0) {\n+                     stopifnot(is.numeric(key[[1]]))\n+                   }\n+                   stopifnot(class(grouped) == \"data.frame\")\n+                   stopifnot(length(colnames(grouped)) == 11)\n+                   # mtcars' hp is more then 50.\n+                   stopifnot(all(grouped$hp > 50))\n+                   grouped\n+                 },\n+                 schema(df))\n+    actual <- collect(ret)\n+    expect_equal(actual, expected)\n+    expect_equal(count(ret), nrow(mtcars))\n+  },\n+  finally = {\n+    callJMethod(conf, \"set\", \"spark.sql.execution.arrow.enabled\", arrowEnabled)\n+  })\n+})\n+\n+test_that(\"gapply() Arrow optimization - type specification\", {\n+  skip_if_not_installed(\"arrow\")\n+  # Note that regular gapply() seems not supporting date and timestamps\n+  # whereas Arrow-optimized gapply() does.\n+  rdf <- data.frame(list(list(a = 1,\n+                              b = \"a\",\n+                              c = TRUE,\n+                              d = 1.1,\n+                              e = 1L)))\n+  df <- createDataFrame(rdf)\n+\n+  conf <- callJMethod(sparkSession, \"conf\")\n+  arrowEnabled <- sparkR.conf(\"spark.sql.execution.arrow.enabled\")[[1]]\n+\n+  callJMethod(conf, \"set\", \"spark.sql.execution.arrow.enabled\", \"false\")\n+  tryCatch({\n+    ret <- gapply(df,\n+                 \"a\",\n+                 function(key, grouped) { grouped }, schema(df))\n+    expected <- collect(ret)\n+  },\n+  finally = {\n+    callJMethod(conf, \"set\", \"spark.sql.execution.arrow.enabled\", arrowEnabled)\n+  })\n+\n+\n+  callJMethod(conf, \"set\", \"spark.sql.execution.arrow.enabled\", \"true\")\n+  tryCatch({\n+    ret <- gapply(df,\n+                 \"a\",\n+                 function(key, grouped) { grouped }, schema(df))\n+    actual <- collect(ret)\n+    expect_equal(actual, expected)\n+  },\n+  finally = {\n+    callJMethod(conf, \"set\", \"spark.sql.execution.arrow.enabled\", arrowEnabled)\n+  })\n+})\n+\n+test_that(\"gapply() Arrow optimization - type specification (date and timestamp)\", {\n+  skip_if_not_installed(\"arrow\")\n+  rdf <- data.frame(list(list(a = as.Date(\"1990-02-24\"),\n+                              b = as.POSIXct(\"1990-02-24 12:34:56\"))))\n+  df <- createDataFrame(rdf)\n+\n+  conf <- callJMethod(sparkSession, \"conf\")\n+  arrowEnabled <- sparkR.conf(\"spark.sql.execution.arrow.enabled\")[[1]]\n+\n+  callJMethod(conf, \"set\", \"spark.sql.execution.arrow.enabled\", \"true\")\n+  tryCatch({\n+    ret <- gapply(df,\n+                  \"a\",\n+                  function(key, grouped) { grouped }, schema(df))\n+    expect_equal(collect(ret), rdf)\n+  },\n+  finally = {\n+    callJMethod(conf, \"set\", \"spark.sql.execution.arrow.enabled\", arrowEnabled)\n+  })\n+})\n+\n+test_that(\"Arrow optimization - unsupported types\", {"
  }],
  "prId": 23969
}]