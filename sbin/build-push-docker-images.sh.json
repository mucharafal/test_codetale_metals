[{
  "comments": [{
    "author": {
      "login": "foxish"
    },
    "body": "I think building docker images right into the minikube VM's docker daemon is uncommon and not something we'd want to recommend. Users on minikube should also use a proper registry - (for example, there is a [registry addon](https://github.com/kubernetes/minikube/blob/master/docs/addons.md#add-ons)) that could be used. \r\n\r\nWhile this might be good to document as a local developer workflow, I'm apprehensive about adding a new flag just for this particular mode. Also one could invoke `eval $(minikube docker-env)` and then use the `build` command to get the same effect.\r\n  ",
    "commit": "ef992145426b82be104127d78fa215f48ee54ea1",
    "createdAt": "2018-01-04T18:11:26Z",
    "diffHunk": "@@ -19,51 +19,118 @@\n # This script builds and pushes docker images when run from a release of Spark\n # with Kubernetes support.\n \n-declare -A path=( [spark-driver]=kubernetes/dockerfiles/driver/Dockerfile \\\n-                  [spark-executor]=kubernetes/dockerfiles/executor/Dockerfile \\\n-                  [spark-init]=kubernetes/dockerfiles/init-container/Dockerfile )\n+function error {\n+  echo \"$@\" 1>&2\n+  exit 1\n+}\n+\n+# Detect whether this is a git clone or a Spark distribution and adjust paths\n+# accordingly.\n+if [ -z \"${SPARK_HOME}\" ]; then\n+  SPARK_HOME=\"$(cd \"`dirname \"$0\"`\"/..; pwd)\"\n+fi\n+. \"${SPARK_HOME}/bin/load-spark-env.sh\"\n+\n+if [ -f \"$SPARK_HOME/RELEASE\" ]; then\n+  IMG_PATH=\"kubernetes/dockerfiles\"\n+  SPARK_JARS=\"jars\"\n+else\n+  IMG_PATH=\"resource-managers/kubernetes/docker/src/main/dockerfiles\"\n+  SPARK_JARS=\"assembly/target/scala-$SPARK_SCALA_VERSION/jars\"\n+fi\n+\n+if [ ! -d \"$IMG_PATH\" ]; then\n+  error \"Cannot find docker images. This script must be run from a runnable distribution of Apache Spark.\"\n+fi\n+\n+declare -A path=( [spark-driver]=\"$IMG_PATH/driver/Dockerfile\" \\\n+                  [spark-executor]=\"$IMG_PATH/executor/Dockerfile\" \\\n+                  [spark-init]=\"$IMG_PATH/init-container/Dockerfile\" )\n+\n+function image_ref {\n+  local image=\"$1\"\n+  local add_repo=\"${2:-1}\"\n+  if [ $add_repo = 1 ] && [ -n \"$REPO\" ]; then\n+    image=\"$REPO/$image\"\n+  fi\n+  if [ -n \"$TAG\" ]; then\n+    image=\"$image:$TAG\"\n+  fi\n+  echo \"$image\"\n+}\n \n function build {\n-  docker build -t spark-base -f kubernetes/dockerfiles/spark-base/Dockerfile .\n+  local base_image=\"$(image_ref spark-base 0)\"\n+  docker build --build-arg \"spark_jars=$SPARK_JARS\" \\\n+    --build-arg \"img_path=$IMG_PATH\" \\\n+    -t \"$base_image\" \\\n+    -f \"$IMG_PATH/spark-base/Dockerfile\" .\n   for image in \"${!path[@]}\"; do\n-    docker build -t ${REPO}/$image:${TAG} -f ${path[$image]} .\n+    docker build --build-arg \"base_image=$base_image\" -t \"$(image_ref $image)\" -f ${path[$image]} .\n   done\n }\n \n-\n function push {\n   for image in \"${!path[@]}\"; do\n-    docker push ${REPO}/$image:${TAG}\n+    docker push \"$(image_ref $image)\"\n   done\n }\n \n function usage {\n-  echo \"This script must be run from a runnable distribution of Apache Spark.\"\n-  echo \"Usage: ./sbin/build-push-docker-images.sh -r <repo> -t <tag> build\"\n-  echo \"       ./sbin/build-push-docker-images.sh -r <repo> -t <tag> push\"\n-  echo \"for example: ./sbin/build-push-docker-images.sh -r docker.io/myrepo -t v2.3.0 push\"\n+  cat <<EOF\n+Usage: $0 [options] [command]\n+Builds or pushes the built-in Spark docker images.\n+\n+Commands:\n+  build       Build docker images.\n+  push        Push images to a registry. Requires a repository address to be provided, both\n+              when building and when pushing the images.\n+\n+Options:\n+  -r repo     Repository address.\n+  -t tag      Tag to apply to built images, or to identify images to be pushed.\n+  -m          Use minikube environment when invoking docker.\n+\n+Example:\n+  $0 -r docker.io/myrepo -t v2.3.0 push\n+EOF\n }\n \n if [[ \"$@\" = *--help ]] || [[ \"$@\" = *-h ]]; then\n   usage\n   exit 0\n fi\n \n-while getopts r:t: option\n+REPO=\n+TAG=\n+while getopts mr:t: option\n do\n  case \"${option}\"\n  in\n  r) REPO=${OPTARG};;\n  t) TAG=${OPTARG};;\n+ m)\n+   if ! which minikube 1>/dev/null; then\n+     error \"Cannot find minikube.\"\n+   fi\n+   eval $(minikube docker-env)",
    "line": 123
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "I started calling that command separately, but it's really annoying. This option is useful not just for Spark devs, but for people who want to try their own apps on minikube before trying them on a larger cluster, for example.\r\n\r\n> building docker images right into the minikube VM's docker daemon is uncommon\r\n\r\nWhat's the alternative? Deploying your own registry? I struggled with that for hours and it's nearly impossible to get docker to talk to an insecure registry (or one with a self signed cert like minikube's). This approach just worked (tm).\r\n  \r\n  ",
    "commit": "ef992145426b82be104127d78fa215f48ee54ea1",
    "createdAt": "2018-01-04T18:22:12Z",
    "diffHunk": "@@ -19,51 +19,118 @@\n # This script builds and pushes docker images when run from a release of Spark\n # with Kubernetes support.\n \n-declare -A path=( [spark-driver]=kubernetes/dockerfiles/driver/Dockerfile \\\n-                  [spark-executor]=kubernetes/dockerfiles/executor/Dockerfile \\\n-                  [spark-init]=kubernetes/dockerfiles/init-container/Dockerfile )\n+function error {\n+  echo \"$@\" 1>&2\n+  exit 1\n+}\n+\n+# Detect whether this is a git clone or a Spark distribution and adjust paths\n+# accordingly.\n+if [ -z \"${SPARK_HOME}\" ]; then\n+  SPARK_HOME=\"$(cd \"`dirname \"$0\"`\"/..; pwd)\"\n+fi\n+. \"${SPARK_HOME}/bin/load-spark-env.sh\"\n+\n+if [ -f \"$SPARK_HOME/RELEASE\" ]; then\n+  IMG_PATH=\"kubernetes/dockerfiles\"\n+  SPARK_JARS=\"jars\"\n+else\n+  IMG_PATH=\"resource-managers/kubernetes/docker/src/main/dockerfiles\"\n+  SPARK_JARS=\"assembly/target/scala-$SPARK_SCALA_VERSION/jars\"\n+fi\n+\n+if [ ! -d \"$IMG_PATH\" ]; then\n+  error \"Cannot find docker images. This script must be run from a runnable distribution of Apache Spark.\"\n+fi\n+\n+declare -A path=( [spark-driver]=\"$IMG_PATH/driver/Dockerfile\" \\\n+                  [spark-executor]=\"$IMG_PATH/executor/Dockerfile\" \\\n+                  [spark-init]=\"$IMG_PATH/init-container/Dockerfile\" )\n+\n+function image_ref {\n+  local image=\"$1\"\n+  local add_repo=\"${2:-1}\"\n+  if [ $add_repo = 1 ] && [ -n \"$REPO\" ]; then\n+    image=\"$REPO/$image\"\n+  fi\n+  if [ -n \"$TAG\" ]; then\n+    image=\"$image:$TAG\"\n+  fi\n+  echo \"$image\"\n+}\n \n function build {\n-  docker build -t spark-base -f kubernetes/dockerfiles/spark-base/Dockerfile .\n+  local base_image=\"$(image_ref spark-base 0)\"\n+  docker build --build-arg \"spark_jars=$SPARK_JARS\" \\\n+    --build-arg \"img_path=$IMG_PATH\" \\\n+    -t \"$base_image\" \\\n+    -f \"$IMG_PATH/spark-base/Dockerfile\" .\n   for image in \"${!path[@]}\"; do\n-    docker build -t ${REPO}/$image:${TAG} -f ${path[$image]} .\n+    docker build --build-arg \"base_image=$base_image\" -t \"$(image_ref $image)\" -f ${path[$image]} .\n   done\n }\n \n-\n function push {\n   for image in \"${!path[@]}\"; do\n-    docker push ${REPO}/$image:${TAG}\n+    docker push \"$(image_ref $image)\"\n   done\n }\n \n function usage {\n-  echo \"This script must be run from a runnable distribution of Apache Spark.\"\n-  echo \"Usage: ./sbin/build-push-docker-images.sh -r <repo> -t <tag> build\"\n-  echo \"       ./sbin/build-push-docker-images.sh -r <repo> -t <tag> push\"\n-  echo \"for example: ./sbin/build-push-docker-images.sh -r docker.io/myrepo -t v2.3.0 push\"\n+  cat <<EOF\n+Usage: $0 [options] [command]\n+Builds or pushes the built-in Spark docker images.\n+\n+Commands:\n+  build       Build docker images.\n+  push        Push images to a registry. Requires a repository address to be provided, both\n+              when building and when pushing the images.\n+\n+Options:\n+  -r repo     Repository address.\n+  -t tag      Tag to apply to built images, or to identify images to be pushed.\n+  -m          Use minikube environment when invoking docker.\n+\n+Example:\n+  $0 -r docker.io/myrepo -t v2.3.0 push\n+EOF\n }\n \n if [[ \"$@\" = *--help ]] || [[ \"$@\" = *-h ]]; then\n   usage\n   exit 0\n fi\n \n-while getopts r:t: option\n+REPO=\n+TAG=\n+while getopts mr:t: option\n do\n  case \"${option}\"\n  in\n  r) REPO=${OPTARG};;\n  t) TAG=${OPTARG};;\n+ m)\n+   if ! which minikube 1>/dev/null; then\n+     error \"Cannot find minikube.\"\n+   fi\n+   eval $(minikube docker-env)",
    "line": 123
  }, {
    "author": {
      "login": "foxish"
    },
    "body": "I see your point - this is considerably easier. I spoke with a minikube maintainer and it seems this is not as uncommon as I initially thought. So, this change looks good, but I'd prefer that we add some more explanation to the usage section, that this will build an image within the minikube environment - and also linking to https://kubernetes.io/docs/getting-started-guides/minikube/#reusing-the-docker-daemon. \r\n\r\ncc/ @aaron-prindle",
    "commit": "ef992145426b82be104127d78fa215f48ee54ea1",
    "createdAt": "2018-01-04T19:27:29Z",
    "diffHunk": "@@ -19,51 +19,118 @@\n # This script builds and pushes docker images when run from a release of Spark\n # with Kubernetes support.\n \n-declare -A path=( [spark-driver]=kubernetes/dockerfiles/driver/Dockerfile \\\n-                  [spark-executor]=kubernetes/dockerfiles/executor/Dockerfile \\\n-                  [spark-init]=kubernetes/dockerfiles/init-container/Dockerfile )\n+function error {\n+  echo \"$@\" 1>&2\n+  exit 1\n+}\n+\n+# Detect whether this is a git clone or a Spark distribution and adjust paths\n+# accordingly.\n+if [ -z \"${SPARK_HOME}\" ]; then\n+  SPARK_HOME=\"$(cd \"`dirname \"$0\"`\"/..; pwd)\"\n+fi\n+. \"${SPARK_HOME}/bin/load-spark-env.sh\"\n+\n+if [ -f \"$SPARK_HOME/RELEASE\" ]; then\n+  IMG_PATH=\"kubernetes/dockerfiles\"\n+  SPARK_JARS=\"jars\"\n+else\n+  IMG_PATH=\"resource-managers/kubernetes/docker/src/main/dockerfiles\"\n+  SPARK_JARS=\"assembly/target/scala-$SPARK_SCALA_VERSION/jars\"\n+fi\n+\n+if [ ! -d \"$IMG_PATH\" ]; then\n+  error \"Cannot find docker images. This script must be run from a runnable distribution of Apache Spark.\"\n+fi\n+\n+declare -A path=( [spark-driver]=\"$IMG_PATH/driver/Dockerfile\" \\\n+                  [spark-executor]=\"$IMG_PATH/executor/Dockerfile\" \\\n+                  [spark-init]=\"$IMG_PATH/init-container/Dockerfile\" )\n+\n+function image_ref {\n+  local image=\"$1\"\n+  local add_repo=\"${2:-1}\"\n+  if [ $add_repo = 1 ] && [ -n \"$REPO\" ]; then\n+    image=\"$REPO/$image\"\n+  fi\n+  if [ -n \"$TAG\" ]; then\n+    image=\"$image:$TAG\"\n+  fi\n+  echo \"$image\"\n+}\n \n function build {\n-  docker build -t spark-base -f kubernetes/dockerfiles/spark-base/Dockerfile .\n+  local base_image=\"$(image_ref spark-base 0)\"\n+  docker build --build-arg \"spark_jars=$SPARK_JARS\" \\\n+    --build-arg \"img_path=$IMG_PATH\" \\\n+    -t \"$base_image\" \\\n+    -f \"$IMG_PATH/spark-base/Dockerfile\" .\n   for image in \"${!path[@]}\"; do\n-    docker build -t ${REPO}/$image:${TAG} -f ${path[$image]} .\n+    docker build --build-arg \"base_image=$base_image\" -t \"$(image_ref $image)\" -f ${path[$image]} .\n   done\n }\n \n-\n function push {\n   for image in \"${!path[@]}\"; do\n-    docker push ${REPO}/$image:${TAG}\n+    docker push \"$(image_ref $image)\"\n   done\n }\n \n function usage {\n-  echo \"This script must be run from a runnable distribution of Apache Spark.\"\n-  echo \"Usage: ./sbin/build-push-docker-images.sh -r <repo> -t <tag> build\"\n-  echo \"       ./sbin/build-push-docker-images.sh -r <repo> -t <tag> push\"\n-  echo \"for example: ./sbin/build-push-docker-images.sh -r docker.io/myrepo -t v2.3.0 push\"\n+  cat <<EOF\n+Usage: $0 [options] [command]\n+Builds or pushes the built-in Spark docker images.\n+\n+Commands:\n+  build       Build docker images.\n+  push        Push images to a registry. Requires a repository address to be provided, both\n+              when building and when pushing the images.\n+\n+Options:\n+  -r repo     Repository address.\n+  -t tag      Tag to apply to built images, or to identify images to be pushed.\n+  -m          Use minikube environment when invoking docker.\n+\n+Example:\n+  $0 -r docker.io/myrepo -t v2.3.0 push\n+EOF\n }\n \n if [[ \"$@\" = *--help ]] || [[ \"$@\" = *-h ]]; then\n   usage\n   exit 0\n fi\n \n-while getopts r:t: option\n+REPO=\n+TAG=\n+while getopts mr:t: option\n do\n  case \"${option}\"\n  in\n  r) REPO=${OPTARG};;\n  t) TAG=${OPTARG};;\n+ m)\n+   if ! which minikube 1>/dev/null; then\n+     error \"Cannot find minikube.\"\n+   fi\n+   eval $(minikube docker-env)",
    "line": 123
  }],
  "prId": 20154
}, {
  "comments": [{
    "author": {
      "login": "foxish"
    },
    "body": "Update this comment? I presume now it should say runnable distribution, or from source.",
    "commit": "ef992145426b82be104127d78fa215f48ee54ea1",
    "createdAt": "2018-01-04T18:16:25Z",
    "diffHunk": "@@ -19,51 +19,118 @@\n # This script builds and pushes docker images when run from a release of Spark\n # with Kubernetes support.\n \n-declare -A path=( [spark-driver]=kubernetes/dockerfiles/driver/Dockerfile \\\n-                  [spark-executor]=kubernetes/dockerfiles/executor/Dockerfile \\\n-                  [spark-init]=kubernetes/dockerfiles/init-container/Dockerfile )\n+function error {\n+  echo \"$@\" 1>&2\n+  exit 1\n+}\n+\n+# Detect whether this is a git clone or a Spark distribution and adjust paths\n+# accordingly.\n+if [ -z \"${SPARK_HOME}\" ]; then\n+  SPARK_HOME=\"$(cd \"`dirname \"$0\"`\"/..; pwd)\"\n+fi\n+. \"${SPARK_HOME}/bin/load-spark-env.sh\"\n+\n+if [ -f \"$SPARK_HOME/RELEASE\" ]; then\n+  IMG_PATH=\"kubernetes/dockerfiles\"\n+  SPARK_JARS=\"jars\"\n+else\n+  IMG_PATH=\"resource-managers/kubernetes/docker/src/main/dockerfiles\"\n+  SPARK_JARS=\"assembly/target/scala-$SPARK_SCALA_VERSION/jars\"\n+fi\n+\n+if [ ! -d \"$IMG_PATH\" ]; then\n+  error \"Cannot find docker images. This script must be run from a runnable distribution of Apache Spark.\"",
    "line": 28
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "The source directory is sort of a \"runnable distribution\" if Spark is built. I'd rather keep the message simple since it's mostly targeted at end users (not devs).",
    "commit": "ef992145426b82be104127d78fa215f48ee54ea1",
    "createdAt": "2018-01-04T18:27:59Z",
    "diffHunk": "@@ -19,51 +19,118 @@\n # This script builds and pushes docker images when run from a release of Spark\n # with Kubernetes support.\n \n-declare -A path=( [spark-driver]=kubernetes/dockerfiles/driver/Dockerfile \\\n-                  [spark-executor]=kubernetes/dockerfiles/executor/Dockerfile \\\n-                  [spark-init]=kubernetes/dockerfiles/init-container/Dockerfile )\n+function error {\n+  echo \"$@\" 1>&2\n+  exit 1\n+}\n+\n+# Detect whether this is a git clone or a Spark distribution and adjust paths\n+# accordingly.\n+if [ -z \"${SPARK_HOME}\" ]; then\n+  SPARK_HOME=\"$(cd \"`dirname \"$0\"`\"/..; pwd)\"\n+fi\n+. \"${SPARK_HOME}/bin/load-spark-env.sh\"\n+\n+if [ -f \"$SPARK_HOME/RELEASE\" ]; then\n+  IMG_PATH=\"kubernetes/dockerfiles\"\n+  SPARK_JARS=\"jars\"\n+else\n+  IMG_PATH=\"resource-managers/kubernetes/docker/src/main/dockerfiles\"\n+  SPARK_JARS=\"assembly/target/scala-$SPARK_SCALA_VERSION/jars\"\n+fi\n+\n+if [ ! -d \"$IMG_PATH\" ]; then\n+  error \"Cannot find docker images. This script must be run from a runnable distribution of Apache Spark.\"",
    "line": 28
  }, {
    "author": {
      "login": "foxish"
    },
    "body": "SGTM",
    "commit": "ef992145426b82be104127d78fa215f48ee54ea1",
    "createdAt": "2018-01-04T18:35:51Z",
    "diffHunk": "@@ -19,51 +19,118 @@\n # This script builds and pushes docker images when run from a release of Spark\n # with Kubernetes support.\n \n-declare -A path=( [spark-driver]=kubernetes/dockerfiles/driver/Dockerfile \\\n-                  [spark-executor]=kubernetes/dockerfiles/executor/Dockerfile \\\n-                  [spark-init]=kubernetes/dockerfiles/init-container/Dockerfile )\n+function error {\n+  echo \"$@\" 1>&2\n+  exit 1\n+}\n+\n+# Detect whether this is a git clone or a Spark distribution and adjust paths\n+# accordingly.\n+if [ -z \"${SPARK_HOME}\" ]; then\n+  SPARK_HOME=\"$(cd \"`dirname \"$0\"`\"/..; pwd)\"\n+fi\n+. \"${SPARK_HOME}/bin/load-spark-env.sh\"\n+\n+if [ -f \"$SPARK_HOME/RELEASE\" ]; then\n+  IMG_PATH=\"kubernetes/dockerfiles\"\n+  SPARK_JARS=\"jars\"\n+else\n+  IMG_PATH=\"resource-managers/kubernetes/docker/src/main/dockerfiles\"\n+  SPARK_JARS=\"assembly/target/scala-$SPARK_SCALA_VERSION/jars\"\n+fi\n+\n+if [ ! -d \"$IMG_PATH\" ]; then\n+  error \"Cannot find docker images. This script must be run from a runnable distribution of Apache Spark.\"",
    "line": 28
  }],
  "prId": 20154
}, {
  "comments": [{
    "author": {
      "login": "foxish"
    },
    "body": "typo int -> in.",
    "commit": "ef992145426b82be104127d78fa215f48ee54ea1",
    "createdAt": "2018-01-04T21:00:57Z",
    "diffHunk": "@@ -19,51 +19,131 @@\n # This script builds and pushes docker images when run from a release of Spark\n # with Kubernetes support.\n \n-declare -A path=( [spark-driver]=kubernetes/dockerfiles/driver/Dockerfile \\\n-                  [spark-executor]=kubernetes/dockerfiles/executor/Dockerfile \\\n-                  [spark-init]=kubernetes/dockerfiles/init-container/Dockerfile )\n+function error {\n+  echo \"$@\" 1>&2\n+  exit 1\n+}\n+\n+# Detect whether this is a git clone or a Spark distribution and adjust paths\n+# accordingly.\n+if [ -z \"${SPARK_HOME}\" ]; then\n+  SPARK_HOME=\"$(cd \"`dirname \"$0\"`\"/..; pwd)\"\n+fi\n+. \"${SPARK_HOME}/bin/load-spark-env.sh\"\n+\n+if [ -f \"$SPARK_HOME/RELEASE\" ]; then\n+  IMG_PATH=\"kubernetes/dockerfiles\"\n+  SPARK_JARS=\"jars\"\n+else\n+  IMG_PATH=\"resource-managers/kubernetes/docker/src/main/dockerfiles\"\n+  SPARK_JARS=\"assembly/target/scala-$SPARK_SCALA_VERSION/jars\"\n+fi\n+\n+if [ ! -d \"$IMG_PATH\" ]; then\n+  error \"Cannot find docker images. This script must be run from a runnable distribution of Apache Spark.\"\n+fi\n+\n+declare -A path=( [spark-driver]=\"$IMG_PATH/driver/Dockerfile\" \\\n+                  [spark-executor]=\"$IMG_PATH/executor/Dockerfile\" \\\n+                  [spark-init]=\"$IMG_PATH/init-container/Dockerfile\" )\n+\n+function image_ref {\n+  local image=\"$1\"\n+  local add_repo=\"${2:-1}\"\n+  if [ $add_repo = 1 ] && [ -n \"$REPO\" ]; then\n+    image=\"$REPO/$image\"\n+  fi\n+  if [ -n \"$TAG\" ]; then\n+    image=\"$image:$TAG\"\n+  fi\n+  echo \"$image\"\n+}\n \n function build {\n-  docker build -t spark-base -f kubernetes/dockerfiles/spark-base/Dockerfile .\n+  local base_image=\"$(image_ref spark-base 0)\"\n+  docker build --build-arg \"spark_jars=$SPARK_JARS\" \\\n+    --build-arg \"img_path=$IMG_PATH\" \\\n+    -t \"$base_image\" \\\n+    -f \"$IMG_PATH/spark-base/Dockerfile\" .\n   for image in \"${!path[@]}\"; do\n-    docker build -t ${REPO}/$image:${TAG} -f ${path[$image]} .\n+    docker build --build-arg \"base_image=$base_image\" -t \"$(image_ref $image)\" -f ${path[$image]} .\n   done\n }\n \n-\n function push {\n   for image in \"${!path[@]}\"; do\n-    docker push ${REPO}/$image:${TAG}\n+    docker push \"$(image_ref $image)\"\n   done\n }\n \n function usage {\n-  echo \"This script must be run from a runnable distribution of Apache Spark.\"\n-  echo \"Usage: ./sbin/build-push-docker-images.sh -r <repo> -t <tag> build\"\n-  echo \"       ./sbin/build-push-docker-images.sh -r <repo> -t <tag> push\"\n-  echo \"for example: ./sbin/build-push-docker-images.sh -r docker.io/myrepo -t v2.3.0 push\"\n+  cat <<EOF\n+Usage: $0 [options] [command]\n+Builds or pushes the built-in Spark Docker images.\n+\n+Commands:\n+  build       Build images.\n+  push        Push images to a registry. Requires a repository address to be provided, both\n+              when building and when pushing the images.\n+\n+Options:\n+  -r repo     Repository address.\n+  -t tag      Tag to apply to built images, or to identify images to be pushed.\n+  -m          Use minikube's Docker daemon.\n+\n+Using minikube when building images will do so directly into minikube's Docker daemon.\n+There is no need to push the images into minikube int that case, they'll be automatically"
  }],
  "prId": 20154
}]