[{
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "You can just re-use $FWDIR from above\n",
    "commit": "b2f12954c695fb3e24512a1b2fa9e9ac9cea8cbc",
    "createdAt": "2014-05-08T20:22:14Z",
    "diffHunk": "@@ -19,5 +19,10 @@\n \n FWDIR=\"$(cd `dirname $0`/..; pwd)\"\n \n+sbin=`dirname \"$0\"`"
  }, {
    "author": {
      "login": "pwendell"
    },
    "body": "I.e. `$FWDIR/sbin/spark-config.sh`.\n",
    "commit": "b2f12954c695fb3e24512a1b2fa9e9ac9cea8cbc",
    "createdAt": "2014-05-08T20:22:38Z",
    "diffHunk": "@@ -19,5 +19,10 @@\n \n FWDIR=\"$(cd `dirname $0`/..; pwd)\"\n \n+sbin=`dirname \"$0\"`"
  }],
  "prId": 651
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "Hey actually - instead of doing this, do you mind just \"inlining\" the setting up of the PYTHONPATH into this file? `spark-config.sh` is intended to configure Spark's standalone daemons, it's not meant to be used at all in mesos mode. I realize the current approach fixes the problem, but it would be cleaner to just have the mesos executor setup $PYTHONPATH directly.\n\n```\nexport PYTHONPATH=$FWDIR/python:$PYTHONPATH\nexport PYTHONPATH=$FWDIR/python/lib/py4j-0.8.1-src.zip:$PYTHONPATH\n```\n",
    "commit": "b2f12954c695fb3e24512a1b2fa9e9ac9cea8cbc",
    "createdAt": "2014-05-08T20:25:43Z",
    "diffHunk": "@@ -19,5 +19,10 @@\n \n FWDIR=\"$(cd `dirname $0`/..; pwd)\"\n \n+sbin=`dirname \"$0\"`\n+sbin=`cd \"$sbin\"; pwd`\n+\n+. \"$sbin/spark-config.sh\""
  }, {
    "author": {
      "login": "bouk"
    },
    "body": "Done\n",
    "commit": "b2f12954c695fb3e24512a1b2fa9e9ac9cea8cbc",
    "createdAt": "2014-05-08T21:26:10Z",
    "diffHunk": "@@ -19,5 +19,10 @@\n \n FWDIR=\"$(cd `dirname $0`/..; pwd)\"\n \n+sbin=`dirname \"$0\"`\n+sbin=`cd \"$sbin\"; pwd`\n+\n+. \"$sbin/spark-config.sh\""
  }],
  "prId": 651
}]