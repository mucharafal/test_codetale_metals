[{
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "In general I don't think this is a good place for documentation. This file would be embedded in a jar file and not one would ever look at it. Documentation should go in `docs/*`.\n\nOn top of that, this particular option doesn't make a lot of sense. It wasn't an option before, and it was and should continue to be a required command line argument of spark-submit. The \"primary resource\" is just the main file of your application (e.g. the jar or python file). It doesn't make a lot sense to put this in a config file.\n",
    "commit": "7410f2c69f58aab6f74d04232d4255ba19891c23",
    "createdAt": "2014-09-24T20:35:33Z",
    "diffHunk": "@@ -0,0 +1,90 @@\n+# The master URL for the cluster egspark://host:port, mesos://host:port, yarn, or local.\n+# legacy env variable MASTER\n+spark.master = local[*]\n+\n+# Should spark submit run in verbose mode: default is false\n+spark.verbose = false\n+\n+# Comma-separated list of files to be placed in the working directory of each executor\n+# spark.files =\n+\n+# Comma-separated list of .zip, .egg, or .py files to place on the PYTHONPATH for Python apps.\n+# spark.submit.pyfiles =\n+\n+# Comma-separated list of local jars to include on the driver and executor classpaths.\n+# spark.jars =\n+\n+\n+# Path to a bundled jar including your application and all dependencies."
  }],
  "prId": 2516
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "SparkSubmit will come up with a proper default name for the application if it's not provided - a name that is based on other command line parameters (such as the class to run). So I don't see much sense in having a default.\n",
    "commit": "7410f2c69f58aab6f74d04232d4255ba19891c23",
    "createdAt": "2014-09-24T20:36:35Z",
    "diffHunk": "@@ -0,0 +1,90 @@\n+# The master URL for the cluster egspark://host:port, mesos://host:port, yarn, or local.\n+# legacy env variable MASTER\n+spark.master = local[*]\n+\n+# Should spark submit run in verbose mode: default is false\n+spark.verbose = false\n+\n+# Comma-separated list of files to be placed in the working directory of each executor\n+# spark.files =\n+\n+# Comma-separated list of .zip, .egg, or .py files to place on the PYTHONPATH for Python apps.\n+# spark.submit.pyfiles =\n+\n+# Comma-separated list of local jars to include on the driver and executor classpaths.\n+# spark.jars =\n+\n+\n+# Path to a bundled jar including your application and all dependencies.\n+# The URL must be globally visible inside of your cluster, for instance,\n+# an hdfs:// path or a file:// path that is present on all nodes.\n+# spark.app.primaryResource =\n+\n+# A name of your application.\n+spark.app.name = Unknown Application"
  }, {
    "author": {
      "login": "tigerquoll"
    },
    "body": "ahh.. Looking at the code it was hard to determine if primaryResource was a required configuration item at all, which is why was I defined it as an Option[String]. I'll remove the default value, change it back to a String and add an explicit test to make sure it is defined after all new config items have been derived\n",
    "commit": "7410f2c69f58aab6f74d04232d4255ba19891c23",
    "createdAt": "2014-09-24T23:51:28Z",
    "diffHunk": "@@ -0,0 +1,90 @@\n+# The master URL for the cluster egspark://host:port, mesos://host:port, yarn, or local.\n+# legacy env variable MASTER\n+spark.master = local[*]\n+\n+# Should spark submit run in verbose mode: default is false\n+spark.verbose = false\n+\n+# Comma-separated list of files to be placed in the working directory of each executor\n+# spark.files =\n+\n+# Comma-separated list of .zip, .egg, or .py files to place on the PYTHONPATH for Python apps.\n+# spark.submit.pyfiles =\n+\n+# Comma-separated list of local jars to include on the driver and executor classpaths.\n+# spark.jars =\n+\n+\n+# Path to a bundled jar including your application and all dependencies.\n+# The URL must be globally visible inside of your cluster, for instance,\n+# an hdfs:// path or a file:// path that is present on all nodes.\n+# spark.app.primaryResource =\n+\n+# A name of your application.\n+spark.app.name = Unknown Application"
  }],
  "prId": 2516
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Another option that didn't exist and doesn't make much sense in a config file.\n",
    "commit": "7410f2c69f58aab6f74d04232d4255ba19891c23",
    "createdAt": "2014-09-24T20:37:10Z",
    "diffHunk": "@@ -0,0 +1,90 @@\n+# The master URL for the cluster egspark://host:port, mesos://host:port, yarn, or local.\n+# legacy env variable MASTER\n+spark.master = local[*]\n+\n+# Should spark submit run in verbose mode: default is false\n+spark.verbose = false\n+\n+# Comma-separated list of files to be placed in the working directory of each executor\n+# spark.files =\n+\n+# Comma-separated list of .zip, .egg, or .py files to place on the PYTHONPATH for Python apps.\n+# spark.submit.pyfiles =\n+\n+# Comma-separated list of local jars to include on the driver and executor classpaths.\n+# spark.jars =\n+\n+\n+# Path to a bundled jar including your application and all dependencies.\n+# The URL must be globally visible inside of your cluster, for instance,\n+# an hdfs:// path or a file:// path that is present on all nodes.\n+# spark.app.primaryResource =\n+\n+# A name of your application.\n+spark.app.name = Unknown Application\n+\n+# Your application's main class (for Java / Scala apps).\n+# spark.app.class ="
  }, {
    "author": {
      "login": "tigerquoll"
    },
    "body": "primaryResource, spark.app.name and spark.app.class have been removed from the file. I note in sparkSubmit:  \nif (mainClass == null && !isPython && primaryResource != null) {\n.. attempt to derive class from primaryResource \n}\nDoes that mean it is valid to not have a main class defined at this stage if we are running python? or should we error out if we still do not have a main class after this code, regardless if we are running python or not?\n",
    "commit": "7410f2c69f58aab6f74d04232d4255ba19891c23",
    "createdAt": "2014-09-25T00:03:39Z",
    "diffHunk": "@@ -0,0 +1,90 @@\n+# The master URL for the cluster egspark://host:port, mesos://host:port, yarn, or local.\n+# legacy env variable MASTER\n+spark.master = local[*]\n+\n+# Should spark submit run in verbose mode: default is false\n+spark.verbose = false\n+\n+# Comma-separated list of files to be placed in the working directory of each executor\n+# spark.files =\n+\n+# Comma-separated list of .zip, .egg, or .py files to place on the PYTHONPATH for Python apps.\n+# spark.submit.pyfiles =\n+\n+# Comma-separated list of local jars to include on the driver and executor classpaths.\n+# spark.jars =\n+\n+\n+# Path to a bundled jar including your application and all dependencies.\n+# The URL must be globally visible inside of your cluster, for instance,\n+# an hdfs:// path or a file:// path that is present on all nodes.\n+# spark.app.primaryResource =\n+\n+# A name of your application.\n+spark.app.name = Unknown Application\n+\n+# Your application's main class (for Java / Scala apps).\n+# spark.app.class ="
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "What this means is that if you haven't defined `mainClass`, and it's not running pyspark, and you have a primary resource, then SparkSubmit will try to figure out `mainClass` from the jar's metadata.\n\n`mainClass` doesn't make sense when you're running pyspark, but I'm pretty sure there's somewhere else where that particular check is performed.\n",
    "commit": "7410f2c69f58aab6f74d04232d4255ba19891c23",
    "createdAt": "2014-09-25T00:07:22Z",
    "diffHunk": "@@ -0,0 +1,90 @@\n+# The master URL for the cluster egspark://host:port, mesos://host:port, yarn, or local.\n+# legacy env variable MASTER\n+spark.master = local[*]\n+\n+# Should spark submit run in verbose mode: default is false\n+spark.verbose = false\n+\n+# Comma-separated list of files to be placed in the working directory of each executor\n+# spark.files =\n+\n+# Comma-separated list of .zip, .egg, or .py files to place on the PYTHONPATH for Python apps.\n+# spark.submit.pyfiles =\n+\n+# Comma-separated list of local jars to include on the driver and executor classpaths.\n+# spark.jars =\n+\n+\n+# Path to a bundled jar including your application and all dependencies.\n+# The URL must be globally visible inside of your cluster, for instance,\n+# an hdfs:// path or a file:// path that is present on all nodes.\n+# spark.app.primaryResource =\n+\n+# A name of your application.\n+spark.app.name = Unknown Application\n+\n+# Your application's main class (for Java / Scala apps).\n+# spark.app.class ="
  }],
  "prId": 2516
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Ditto.\n",
    "commit": "7410f2c69f58aab6f74d04232d4255ba19891c23",
    "createdAt": "2014-09-24T20:37:21Z",
    "diffHunk": "@@ -0,0 +1,90 @@\n+# The master URL for the cluster egspark://host:port, mesos://host:port, yarn, or local.\n+# legacy env variable MASTER\n+spark.master = local[*]\n+\n+# Should spark submit run in verbose mode: default is false\n+spark.verbose = false\n+\n+# Comma-separated list of files to be placed in the working directory of each executor\n+# spark.files =\n+\n+# Comma-separated list of .zip, .egg, or .py files to place on the PYTHONPATH for Python apps.\n+# spark.submit.pyfiles =\n+\n+# Comma-separated list of local jars to include on the driver and executor classpaths.\n+# spark.jars =\n+\n+\n+# Path to a bundled jar including your application and all dependencies.\n+# The URL must be globally visible inside of your cluster, for instance,\n+# an hdfs:// path or a file:// path that is present on all nodes.\n+# spark.app.primaryResource =\n+\n+# A name of your application.\n+spark.app.name = Unknown Application\n+\n+# Your application's main class (for Java / Scala apps).\n+# spark.app.class =\n+\n+# Additional arguments to pass to the driver program\n+# spark.app.arguments ="
  }],
  "prId": 2516
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Hmmm... now that this file has been cleaned up, it looks a lot smaller than it did before. I'm not sure it justifies the extra code to load it. It could very well live in a `val DEFAULTS = Map(...)` where it's used.\n",
    "commit": "7410f2c69f58aab6f74d04232d4255ba19891c23",
    "createdAt": "2014-09-25T21:09:49Z",
    "diffHunk": "@@ -0,0 +1,18 @@\n+\n+spark.master = local[*]"
  }, {
    "author": {
      "login": "tigerquoll"
    },
    "body": "It could, but if this PR gets accepted I'm thinking of extending the concept to other configuration properties as well, many of which have default values sitting buried in other parts of the code\n",
    "commit": "7410f2c69f58aab6f74d04232d4255ba19891c23",
    "createdAt": "2014-09-25T23:18:09Z",
    "diffHunk": "@@ -0,0 +1,18 @@\n+\n+spark.master = local[*]"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "I'd cross that bridge when you get there. I've had push back before when trying to consolidate options into a single location like this.\n\nFor now I feel it's cleaner to just keep this in `SparkSubmitArguments`.\n",
    "commit": "7410f2c69f58aab6f74d04232d4255ba19891c23",
    "createdAt": "2014-09-25T23:22:38Z",
    "diffHunk": "@@ -0,0 +1,18 @@\n+\n+spark.master = local[*]"
  }, {
    "author": {
      "login": "tigerquoll"
    },
    "body": "Ok, will do\n",
    "commit": "7410f2c69f58aab6f74d04232d4255ba19891c23",
    "createdAt": "2014-09-29T11:31:20Z",
    "diffHunk": "@@ -0,0 +1,18 @@\n+\n+spark.master = local[*]"
  }],
  "prId": 2516
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "BTW this is also something that probably just makes sense in the command line, and probably shouldn't be turned into a `spark.*` conf.\n",
    "commit": "7410f2c69f58aab6f74d04232d4255ba19891c23",
    "createdAt": "2014-09-25T21:49:18Z",
    "diffHunk": "@@ -0,0 +1,18 @@\n+\n+spark.master = local[*]\n+\n+spark.verbose = false"
  }],
  "prId": 2516
}]