[{
  "comments": [{
    "author": {
      "login": "davies"
    },
    "body": "Is it possible that the `recordLength` could be zero?\n",
    "commit": "6beb4674999820126861fdac99fb84f8ea5d57ff",
    "createdAt": "2015-07-08T00:11:05Z",
    "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.collection.unsafe.sort;\n+\n+import java.io.File;\n+import java.io.IOException;\n+\n+import scala.Tuple2;\n+\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.serializer.DummySerializerInstance;\n+import org.apache.spark.storage.BlockId;\n+import org.apache.spark.storage.BlockManager;\n+import org.apache.spark.storage.BlockObjectWriter;\n+import org.apache.spark.storage.TempLocalBlockId;\n+import org.apache.spark.unsafe.PlatformDependent;\n+\n+/**\n+ * Spills a list of sorted records to disk. Spill files have the following format:\n+ *\n+ *   [# of records (int)] [[len (int)][prefix (long)][data (bytes)]...]\n+ */\n+final class UnsafeSorterSpillWriter {\n+\n+  static final int DISK_WRITE_BUFFER_SIZE = 1024 * 1024;\n+\n+  // Small writes to DiskBlockObjectWriter will be fairly inefficient. Since there doesn't seem to\n+  // be an API to directly transfer bytes from managed memory to the disk writer, we buffer\n+  // data through a byte array.\n+  private byte[] writeBuffer = new byte[DISK_WRITE_BUFFER_SIZE];\n+\n+  private final File file;\n+  private final BlockId blockId;\n+  private final int numRecordsToWrite;\n+  private BlockObjectWriter writer;\n+  private int numRecordsSpilled = 0;\n+\n+  public UnsafeSorterSpillWriter(\n+      BlockManager blockManager,\n+      int fileBufferSize,\n+      ShuffleWriteMetrics writeMetrics,\n+      int numRecordsToWrite) throws IOException {\n+    final Tuple2<TempLocalBlockId, File> spilledFileInfo =\n+      blockManager.diskBlockManager().createTempLocalBlock();\n+    this.file = spilledFileInfo._2();\n+    this.blockId = spilledFileInfo._1();\n+    this.numRecordsToWrite = numRecordsToWrite;\n+    // Unfortunately, we need a serializer instance in order to construct a DiskBlockObjectWriter.\n+    // Our write path doesn't actually use this serializer (since we end up calling the `write()`\n+    // OutputStream methods), but DiskBlockObjectWriter still calls some methods on it. To work\n+    // around this, we pass a dummy no-op serializer.\n+    writer = blockManager.getDiskWriter(\n+      blockId, file, DummySerializerInstance.INSTANCE, fileBufferSize, writeMetrics);\n+    // Write the number of records\n+    writeIntToBuffer(numRecordsToWrite, 0);\n+    writer.write(writeBuffer, 0, 4);\n+  }\n+\n+  // Based on DataOutputStream.writeLong.\n+  private void writeLongToBuffer(long v, int offset) throws IOException {\n+    writeBuffer[offset + 0] = (byte)(v >>> 56);\n+    writeBuffer[offset + 1] = (byte)(v >>> 48);\n+    writeBuffer[offset + 2] = (byte)(v >>> 40);\n+    writeBuffer[offset + 3] = (byte)(v >>> 32);\n+    writeBuffer[offset + 4] = (byte)(v >>> 24);\n+    writeBuffer[offset + 5] = (byte)(v >>> 16);\n+    writeBuffer[offset + 6] = (byte)(v >>>  8);\n+    writeBuffer[offset + 7] = (byte)(v >>>  0);\n+  }\n+\n+  // Based on DataOutputStream.writeInt.\n+  private void writeIntToBuffer(int v, int offset) throws IOException {\n+    writeBuffer[offset + 0] = (byte)(v >>> 24);\n+    writeBuffer[offset + 1] = (byte)(v >>> 16);\n+    writeBuffer[offset + 2] = (byte)(v >>>  8);\n+    writeBuffer[offset + 3] = (byte)(v >>>  0);\n+  }\n+\n+  /**\n+   * Write a record to a spill file.\n+   *\n+   * @param baseObject the base object / memory page containing the record\n+   * @param baseOffset the base offset which points directly to the record data.\n+   * @param recordLength the length of the record.\n+   * @param keyPrefix a sort key prefix\n+   */\n+  public void write(\n+      Object baseObject,\n+      long baseOffset,\n+      int recordLength,\n+      long keyPrefix) throws IOException {\n+    if (numRecordsSpilled == numRecordsToWrite) {\n+      throw new IllegalStateException(\n+        \"Number of records written exceeded numRecordsToWrite = \" + numRecordsToWrite);\n+    } else {\n+      numRecordsSpilled++;\n+    }\n+    writeIntToBuffer(recordLength, 0);\n+    writeLongToBuffer(keyPrefix, 4);\n+    int dataRemaining = recordLength;",
    "line": 115
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "When we're sorting rows in Spark SQL, we'll never have an UnsfafeRow whose length is zero because that row will still have an 8-byte null tracking bitmap.  In the more general case where this operator is used outside of SQL, an empty array would be a valid input.  I'll see if I can add a test for this and will fix any bugs that I find.\n",
    "commit": "6beb4674999820126861fdac99fb84f8ea5d57ff",
    "createdAt": "2015-07-10T00:38:14Z",
    "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.collection.unsafe.sort;\n+\n+import java.io.File;\n+import java.io.IOException;\n+\n+import scala.Tuple2;\n+\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.serializer.DummySerializerInstance;\n+import org.apache.spark.storage.BlockId;\n+import org.apache.spark.storage.BlockManager;\n+import org.apache.spark.storage.BlockObjectWriter;\n+import org.apache.spark.storage.TempLocalBlockId;\n+import org.apache.spark.unsafe.PlatformDependent;\n+\n+/**\n+ * Spills a list of sorted records to disk. Spill files have the following format:\n+ *\n+ *   [# of records (int)] [[len (int)][prefix (long)][data (bytes)]...]\n+ */\n+final class UnsafeSorterSpillWriter {\n+\n+  static final int DISK_WRITE_BUFFER_SIZE = 1024 * 1024;\n+\n+  // Small writes to DiskBlockObjectWriter will be fairly inefficient. Since there doesn't seem to\n+  // be an API to directly transfer bytes from managed memory to the disk writer, we buffer\n+  // data through a byte array.\n+  private byte[] writeBuffer = new byte[DISK_WRITE_BUFFER_SIZE];\n+\n+  private final File file;\n+  private final BlockId blockId;\n+  private final int numRecordsToWrite;\n+  private BlockObjectWriter writer;\n+  private int numRecordsSpilled = 0;\n+\n+  public UnsafeSorterSpillWriter(\n+      BlockManager blockManager,\n+      int fileBufferSize,\n+      ShuffleWriteMetrics writeMetrics,\n+      int numRecordsToWrite) throws IOException {\n+    final Tuple2<TempLocalBlockId, File> spilledFileInfo =\n+      blockManager.diskBlockManager().createTempLocalBlock();\n+    this.file = spilledFileInfo._2();\n+    this.blockId = spilledFileInfo._1();\n+    this.numRecordsToWrite = numRecordsToWrite;\n+    // Unfortunately, we need a serializer instance in order to construct a DiskBlockObjectWriter.\n+    // Our write path doesn't actually use this serializer (since we end up calling the `write()`\n+    // OutputStream methods), but DiskBlockObjectWriter still calls some methods on it. To work\n+    // around this, we pass a dummy no-op serializer.\n+    writer = blockManager.getDiskWriter(\n+      blockId, file, DummySerializerInstance.INSTANCE, fileBufferSize, writeMetrics);\n+    // Write the number of records\n+    writeIntToBuffer(numRecordsToWrite, 0);\n+    writer.write(writeBuffer, 0, 4);\n+  }\n+\n+  // Based on DataOutputStream.writeLong.\n+  private void writeLongToBuffer(long v, int offset) throws IOException {\n+    writeBuffer[offset + 0] = (byte)(v >>> 56);\n+    writeBuffer[offset + 1] = (byte)(v >>> 48);\n+    writeBuffer[offset + 2] = (byte)(v >>> 40);\n+    writeBuffer[offset + 3] = (byte)(v >>> 32);\n+    writeBuffer[offset + 4] = (byte)(v >>> 24);\n+    writeBuffer[offset + 5] = (byte)(v >>> 16);\n+    writeBuffer[offset + 6] = (byte)(v >>>  8);\n+    writeBuffer[offset + 7] = (byte)(v >>>  0);\n+  }\n+\n+  // Based on DataOutputStream.writeInt.\n+  private void writeIntToBuffer(int v, int offset) throws IOException {\n+    writeBuffer[offset + 0] = (byte)(v >>> 24);\n+    writeBuffer[offset + 1] = (byte)(v >>> 16);\n+    writeBuffer[offset + 2] = (byte)(v >>>  8);\n+    writeBuffer[offset + 3] = (byte)(v >>>  0);\n+  }\n+\n+  /**\n+   * Write a record to a spill file.\n+   *\n+   * @param baseObject the base object / memory page containing the record\n+   * @param baseOffset the base offset which points directly to the record data.\n+   * @param recordLength the length of the record.\n+   * @param keyPrefix a sort key prefix\n+   */\n+  public void write(\n+      Object baseObject,\n+      long baseOffset,\n+      int recordLength,\n+      long keyPrefix) throws IOException {\n+    if (numRecordsSpilled == numRecordsToWrite) {\n+      throw new IllegalStateException(\n+        \"Number of records written exceeded numRecordsToWrite = \" + numRecordsToWrite);\n+    } else {\n+      numRecordsSpilled++;\n+    }\n+    writeIntToBuffer(recordLength, 0);\n+    writeLongToBuffer(keyPrefix, 4);\n+    int dataRemaining = recordLength;",
    "line": 115
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "I added a test for this and fixed the bug that it caught.\n",
    "commit": "6beb4674999820126861fdac99fb84f8ea5d57ff",
    "createdAt": "2015-07-10T05:58:09Z",
    "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.collection.unsafe.sort;\n+\n+import java.io.File;\n+import java.io.IOException;\n+\n+import scala.Tuple2;\n+\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.serializer.DummySerializerInstance;\n+import org.apache.spark.storage.BlockId;\n+import org.apache.spark.storage.BlockManager;\n+import org.apache.spark.storage.BlockObjectWriter;\n+import org.apache.spark.storage.TempLocalBlockId;\n+import org.apache.spark.unsafe.PlatformDependent;\n+\n+/**\n+ * Spills a list of sorted records to disk. Spill files have the following format:\n+ *\n+ *   [# of records (int)] [[len (int)][prefix (long)][data (bytes)]...]\n+ */\n+final class UnsafeSorterSpillWriter {\n+\n+  static final int DISK_WRITE_BUFFER_SIZE = 1024 * 1024;\n+\n+  // Small writes to DiskBlockObjectWriter will be fairly inefficient. Since there doesn't seem to\n+  // be an API to directly transfer bytes from managed memory to the disk writer, we buffer\n+  // data through a byte array.\n+  private byte[] writeBuffer = new byte[DISK_WRITE_BUFFER_SIZE];\n+\n+  private final File file;\n+  private final BlockId blockId;\n+  private final int numRecordsToWrite;\n+  private BlockObjectWriter writer;\n+  private int numRecordsSpilled = 0;\n+\n+  public UnsafeSorterSpillWriter(\n+      BlockManager blockManager,\n+      int fileBufferSize,\n+      ShuffleWriteMetrics writeMetrics,\n+      int numRecordsToWrite) throws IOException {\n+    final Tuple2<TempLocalBlockId, File> spilledFileInfo =\n+      blockManager.diskBlockManager().createTempLocalBlock();\n+    this.file = spilledFileInfo._2();\n+    this.blockId = spilledFileInfo._1();\n+    this.numRecordsToWrite = numRecordsToWrite;\n+    // Unfortunately, we need a serializer instance in order to construct a DiskBlockObjectWriter.\n+    // Our write path doesn't actually use this serializer (since we end up calling the `write()`\n+    // OutputStream methods), but DiskBlockObjectWriter still calls some methods on it. To work\n+    // around this, we pass a dummy no-op serializer.\n+    writer = blockManager.getDiskWriter(\n+      blockId, file, DummySerializerInstance.INSTANCE, fileBufferSize, writeMetrics);\n+    // Write the number of records\n+    writeIntToBuffer(numRecordsToWrite, 0);\n+    writer.write(writeBuffer, 0, 4);\n+  }\n+\n+  // Based on DataOutputStream.writeLong.\n+  private void writeLongToBuffer(long v, int offset) throws IOException {\n+    writeBuffer[offset + 0] = (byte)(v >>> 56);\n+    writeBuffer[offset + 1] = (byte)(v >>> 48);\n+    writeBuffer[offset + 2] = (byte)(v >>> 40);\n+    writeBuffer[offset + 3] = (byte)(v >>> 32);\n+    writeBuffer[offset + 4] = (byte)(v >>> 24);\n+    writeBuffer[offset + 5] = (byte)(v >>> 16);\n+    writeBuffer[offset + 6] = (byte)(v >>>  8);\n+    writeBuffer[offset + 7] = (byte)(v >>>  0);\n+  }\n+\n+  // Based on DataOutputStream.writeInt.\n+  private void writeIntToBuffer(int v, int offset) throws IOException {\n+    writeBuffer[offset + 0] = (byte)(v >>> 24);\n+    writeBuffer[offset + 1] = (byte)(v >>> 16);\n+    writeBuffer[offset + 2] = (byte)(v >>>  8);\n+    writeBuffer[offset + 3] = (byte)(v >>>  0);\n+  }\n+\n+  /**\n+   * Write a record to a spill file.\n+   *\n+   * @param baseObject the base object / memory page containing the record\n+   * @param baseOffset the base offset which points directly to the record data.\n+   * @param recordLength the length of the record.\n+   * @param keyPrefix a sort key prefix\n+   */\n+  public void write(\n+      Object baseObject,\n+      long baseOffset,\n+      int recordLength,\n+      long keyPrefix) throws IOException {\n+    if (numRecordsSpilled == numRecordsToWrite) {\n+      throw new IllegalStateException(\n+        \"Number of records written exceeded numRecordsToWrite = \" + numRecordsToWrite);\n+    } else {\n+      numRecordsSpilled++;\n+    }\n+    writeIntToBuffer(recordLength, 0);\n+    writeLongToBuffer(keyPrefix, 4);\n+    int dataRemaining = recordLength;",
    "line": 115
  }],
  "prId": 6444
}, {
  "comments": [{
    "author": {
      "login": "tedyu"
    },
    "body": "Should this class declare 'implement Closeable' ?\nThis way, its caller would be able to use try-with-resources construct\n",
    "commit": "6beb4674999820126861fdac99fb84f8ea5d57ff",
    "createdAt": "2015-07-16T22:59:48Z",
    "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.collection.unsafe.sort;\n+\n+import java.io.File;\n+import java.io.IOException;\n+\n+import scala.Tuple2;\n+\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.serializer.DummySerializerInstance;\n+import org.apache.spark.storage.BlockId;\n+import org.apache.spark.storage.BlockManager;\n+import org.apache.spark.storage.BlockObjectWriter;\n+import org.apache.spark.storage.TempLocalBlockId;\n+import org.apache.spark.unsafe.PlatformDependent;\n+\n+/**\n+ * Spills a list of sorted records to disk. Spill files have the following format:\n+ *\n+ *   [# of records (int)] [[len (int)][prefix (long)][data (bytes)]...]\n+ */\n+final class UnsafeSorterSpillWriter {",
    "line": 38
  }],
  "prId": 6444
}, {
  "comments": [{
    "author": {
      "login": "tedyu"
    },
    "body": "If I understand correctly, this would be true if the above loop is skipped.\nIn that case we write 12 bytes. Is this intended ?\n",
    "commit": "6beb4674999820126861fdac99fb84f8ea5d57ff",
    "createdAt": "2015-07-16T23:49:46Z",
    "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.collection.unsafe.sort;\n+\n+import java.io.File;\n+import java.io.IOException;\n+\n+import scala.Tuple2;\n+\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.serializer.DummySerializerInstance;\n+import org.apache.spark.storage.BlockId;\n+import org.apache.spark.storage.BlockManager;\n+import org.apache.spark.storage.BlockObjectWriter;\n+import org.apache.spark.storage.TempLocalBlockId;\n+import org.apache.spark.unsafe.PlatformDependent;\n+\n+/**\n+ * Spills a list of sorted records to disk. Spill files have the following format:\n+ *\n+ *   [# of records (int)] [[len (int)][prefix (long)][data (bytes)]...]\n+ */\n+final class UnsafeSorterSpillWriter {\n+\n+  static final int DISK_WRITE_BUFFER_SIZE = 1024 * 1024;\n+\n+  // Small writes to DiskBlockObjectWriter will be fairly inefficient. Since there doesn't seem to\n+  // be an API to directly transfer bytes from managed memory to the disk writer, we buffer\n+  // data through a byte array.\n+  private byte[] writeBuffer = new byte[DISK_WRITE_BUFFER_SIZE];\n+\n+  private final File file;\n+  private final BlockId blockId;\n+  private final int numRecordsToWrite;\n+  private BlockObjectWriter writer;\n+  private int numRecordsSpilled = 0;\n+\n+  public UnsafeSorterSpillWriter(\n+      BlockManager blockManager,\n+      int fileBufferSize,\n+      ShuffleWriteMetrics writeMetrics,\n+      int numRecordsToWrite) throws IOException {\n+    final Tuple2<TempLocalBlockId, File> spilledFileInfo =\n+      blockManager.diskBlockManager().createTempLocalBlock();\n+    this.file = spilledFileInfo._2();\n+    this.blockId = spilledFileInfo._1();\n+    this.numRecordsToWrite = numRecordsToWrite;\n+    // Unfortunately, we need a serializer instance in order to construct a DiskBlockObjectWriter.\n+    // Our write path doesn't actually use this serializer (since we end up calling the `write()`\n+    // OutputStream methods), but DiskBlockObjectWriter still calls some methods on it. To work\n+    // around this, we pass a dummy no-op serializer.\n+    writer = blockManager.getDiskWriter(\n+      blockId, file, DummySerializerInstance.INSTANCE, fileBufferSize, writeMetrics);\n+    // Write the number of records\n+    writeIntToBuffer(numRecordsToWrite, 0);\n+    writer.write(writeBuffer, 0, 4);\n+  }\n+\n+  // Based on DataOutputStream.writeLong.\n+  private void writeLongToBuffer(long v, int offset) throws IOException {\n+    writeBuffer[offset + 0] = (byte)(v >>> 56);\n+    writeBuffer[offset + 1] = (byte)(v >>> 48);\n+    writeBuffer[offset + 2] = (byte)(v >>> 40);\n+    writeBuffer[offset + 3] = (byte)(v >>> 32);\n+    writeBuffer[offset + 4] = (byte)(v >>> 24);\n+    writeBuffer[offset + 5] = (byte)(v >>> 16);\n+    writeBuffer[offset + 6] = (byte)(v >>>  8);\n+    writeBuffer[offset + 7] = (byte)(v >>>  0);\n+  }\n+\n+  // Based on DataOutputStream.writeInt.\n+  private void writeIntToBuffer(int v, int offset) throws IOException {\n+    writeBuffer[offset + 0] = (byte)(v >>> 24);\n+    writeBuffer[offset + 1] = (byte)(v >>> 16);\n+    writeBuffer[offset + 2] = (byte)(v >>>  8);\n+    writeBuffer[offset + 3] = (byte)(v >>>  0);\n+  }\n+\n+  /**\n+   * Write a record to a spill file.\n+   *\n+   * @param baseObject the base object / memory page containing the record\n+   * @param baseOffset the base offset which points directly to the record data.\n+   * @param recordLength the length of the record.\n+   * @param keyPrefix a sort key prefix\n+   */\n+  public void write(\n+      Object baseObject,\n+      long baseOffset,\n+      int recordLength,\n+      long keyPrefix) throws IOException {\n+    if (numRecordsSpilled == numRecordsToWrite) {\n+      throw new IllegalStateException(\n+        \"Number of records written exceeded numRecordsToWrite = \" + numRecordsToWrite);\n+    } else {\n+      numRecordsSpilled++;\n+    }\n+    writeIntToBuffer(recordLength, 0);\n+    writeLongToBuffer(keyPrefix, 4);\n+    int dataRemaining = recordLength;\n+    int freeSpaceInWriteBuffer = DISK_WRITE_BUFFER_SIZE - 4 - 8; // space used by prefix + len\n+    long recordReadPosition = baseOffset;\n+    while (dataRemaining > 0) {\n+      final int toTransfer = Math.min(freeSpaceInWriteBuffer, dataRemaining);\n+      PlatformDependent.copyMemory(\n+        baseObject,\n+        recordReadPosition,\n+        writeBuffer,\n+        PlatformDependent.BYTE_ARRAY_OFFSET + (DISK_WRITE_BUFFER_SIZE - freeSpaceInWriteBuffer),\n+        toTransfer);\n+      writer.write(writeBuffer, 0, (DISK_WRITE_BUFFER_SIZE - freeSpaceInWriteBuffer) + toTransfer);\n+      recordReadPosition += toTransfer;\n+      dataRemaining -= toTransfer;\n+      freeSpaceInWriteBuffer = DISK_WRITE_BUFFER_SIZE;\n+    }\n+    if (freeSpaceInWriteBuffer < DISK_WRITE_BUFFER_SIZE) {",
    "line": 131
  }],
  "prId": 6444
}]