[{
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "This comment is inaccurate; the resources are freed in a separate call.  I'll update the comment to reflect this.\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-06T03:14:44Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+\n+import org.apache.spark.storage.*;\n+import scala.Tuple2;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.MemoryBlock;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+/**\n+ * An external sorter that is specialized for sort-based shuffle.\n+ * <p>\n+ * Incoming records are appended to data pages. When all records have been inserted (or when the\n+ * current thread's shuffle memory limit is reached), the in-memory records are sorted according to\n+ * their partition ids (using a {@link UnsafeShuffleSorter}). The sorted records are then written\n+ * to a single output file (or multiple files, if we've spilled). The format of the output files is\n+ * the same as the format of the final output file written by\n+ * {@link org.apache.spark.shuffle.sort.SortShuffleWriter}: each output partition's records are\n+ * written as a single serialized, compressed stream that can be read with a new decompression and\n+ * deserialization stream.\n+ * <p>\n+ * Unlike {@link org.apache.spark.util.collection.ExternalSorter}, this sorter does not merge its\n+ * spill files. Instead, this merging is performed in {@link UnsafeShuffleWriter}, which uses a\n+ * specialized merge procedure that avoids extra serialization/deserialization.\n+ */\n+public final class UnsafeShuffleSpillWriter {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleSpillWriter.class);\n+\n+  private static final int SER_BUFFER_SIZE = 1024 * 1024;  // TODO: tune this / don't duplicate\n+  private static final int PAGE_SIZE = 1024 * 1024;  // TODO: tune this\n+\n+  private final int initialSize;\n+  private final int numPartitions;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final BlockManager blockManager;\n+  private final TaskContext taskContext;\n+  private final boolean spillingEnabled;\n+  private ShuffleWriteMetrics writeMetrics;\n+\n+  /** The buffer size to use when writing spills using DiskBlockObjectWriter */\n+  private final int fileBufferSize;\n+\n+  /**\n+   * Memory pages that hold the records being sorted. The pages in this list are freed when\n+   * spilling, although in principle we could recycle these pages across spills (on the other hand,\n+   * this might not be necessary if we maintained a pool of re-usable pages in the TaskMemoryManager\n+   * itself).\n+   */\n+  private final LinkedList<MemoryBlock> allocatedPages = new LinkedList<MemoryBlock>();\n+\n+  private final LinkedList<SpillInfo> spills = new LinkedList<SpillInfo>();\n+\n+  // All three of these variables are reset after spilling:\n+  private UnsafeShuffleSorter sorter;\n+  private MemoryBlock currentPage = null;\n+  private long currentPagePosition = -1;\n+\n+  public UnsafeShuffleSpillWriter(\n+      TaskMemoryManager memoryManager,\n+      ShuffleMemoryManager shuffleMemoryManager,\n+      BlockManager blockManager,\n+      TaskContext taskContext,\n+      int initialSize,\n+      int numPartitions,\n+      SparkConf conf) throws IOException {\n+    this.memoryManager = memoryManager;\n+    this.shuffleMemoryManager = shuffleMemoryManager;\n+    this.blockManager = blockManager;\n+    this.taskContext = taskContext;\n+    this.initialSize = initialSize;\n+    this.numPartitions = numPartitions;\n+    this.spillingEnabled = conf.getBoolean(\"spark.shuffle.spill\", true);\n+    // Use getSizeAsKb (not bytes) to maintain backwards compatibility for units\n+    this.fileBufferSize = (int) conf.getSizeAsKb(\"spark.shuffle.file.buffer\", \"32k\") * 1024;\n+    openSorter();\n+  }\n+\n+  // TODO: metrics tracking + integration with shuffle write metrics\n+\n+  /**\n+   * Allocates a new sorter. Called when opening the spill writer for the first time and after\n+   * each spill.\n+   */\n+  private void openSorter() throws IOException {\n+    this.writeMetrics = new ShuffleWriteMetrics();\n+    // TODO: connect write metrics to task metrics?\n+    // TODO: move this sizing calculation logic into a static method of sorter:\n+    final long memoryRequested = initialSize * 8L;\n+    if (spillingEnabled) {\n+      final long memoryAcquired = shuffleMemoryManager.tryToAcquire(memoryRequested);\n+      if (memoryAcquired != memoryRequested) {\n+        shuffleMemoryManager.release(memoryAcquired);\n+        throw new IOException(\"Could not acquire memory!\");\n+      }\n+    }\n+\n+    this.sorter = new UnsafeShuffleSorter(initialSize);\n+  }\n+\n+  /**\n+   * Sorts the in-memory records, writes the sorted records to a spill file, and frees the in-memory"
  }],
  "prId": 5868
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "i'd name this UnsafeShuffleSorter\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-07T00:40:05Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+\n+import org.apache.spark.storage.*;\n+import scala.Tuple2;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.MemoryBlock;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+/**\n+ * An external sorter that is specialized for sort-based shuffle.\n+ * <p>\n+ * Incoming records are appended to data pages. When all records have been inserted (or when the\n+ * current thread's shuffle memory limit is reached), the in-memory records are sorted according to\n+ * their partition ids (using a {@link UnsafeShuffleSorter}). The sorted records are then written\n+ * to a single output file (or multiple files, if we've spilled). The format of the output files is\n+ * the same as the format of the final output file written by\n+ * {@link org.apache.spark.shuffle.sort.SortShuffleWriter}: each output partition's records are\n+ * written as a single serialized, compressed stream that can be read with a new decompression and\n+ * deserialization stream.\n+ * <p>\n+ * Unlike {@link org.apache.spark.util.collection.ExternalSorter}, this sorter does not merge its\n+ * spill files. Instead, this merging is performed in {@link UnsafeShuffleWriter}, which uses a\n+ * specialized merge procedure that avoids extra serialization/deserialization.\n+ */\n+public final class UnsafeShuffleSpillWriter {"
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "The actual underlying sorter is already named `UnsafeShuffleSorter`, so I'll rename this to `UnsafeShuffleExternalSorter` to clarify.\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-07T00:41:16Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+\n+import org.apache.spark.storage.*;\n+import scala.Tuple2;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.MemoryBlock;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+/**\n+ * An external sorter that is specialized for sort-based shuffle.\n+ * <p>\n+ * Incoming records are appended to data pages. When all records have been inserted (or when the\n+ * current thread's shuffle memory limit is reached), the in-memory records are sorted according to\n+ * their partition ids (using a {@link UnsafeShuffleSorter}). The sorted records are then written\n+ * to a single output file (or multiple files, if we've spilled). The format of the output files is\n+ * the same as the format of the final output file written by\n+ * {@link org.apache.spark.shuffle.sort.SortShuffleWriter}: each output partition's records are\n+ * written as a single serialized, compressed stream that can be read with a new decompression and\n+ * deserialization stream.\n+ * <p>\n+ * Unlike {@link org.apache.spark.util.collection.ExternalSorter}, this sorter does not merge its\n+ * spill files. Instead, this merging is performed in {@link UnsafeShuffleWriter}, which uses a\n+ * specialized merge procedure that avoids extra serialization/deserialization.\n+ */\n+public final class UnsafeShuffleSpillWriter {"
  }],
  "prId": 5868
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "debug level\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-07T00:40:31Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+\n+import org.apache.spark.storage.*;\n+import scala.Tuple2;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.MemoryBlock;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+/**\n+ * An external sorter that is specialized for sort-based shuffle.\n+ * <p>\n+ * Incoming records are appended to data pages. When all records have been inserted (or when the\n+ * current thread's shuffle memory limit is reached), the in-memory records are sorted according to\n+ * their partition ids (using a {@link UnsafeShuffleSorter}). The sorted records are then written\n+ * to a single output file (or multiple files, if we've spilled). The format of the output files is\n+ * the same as the format of the final output file written by\n+ * {@link org.apache.spark.shuffle.sort.SortShuffleWriter}: each output partition's records are\n+ * written as a single serialized, compressed stream that can be read with a new decompression and\n+ * deserialization stream.\n+ * <p>\n+ * Unlike {@link org.apache.spark.util.collection.ExternalSorter}, this sorter does not merge its\n+ * spill files. Instead, this merging is performed in {@link UnsafeShuffleWriter}, which uses a\n+ * specialized merge procedure that avoids extra serialization/deserialization.\n+ */\n+public final class UnsafeShuffleSpillWriter {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleSpillWriter.class);\n+\n+  private static final int SER_BUFFER_SIZE = 1024 * 1024;  // TODO: tune this / don't duplicate\n+  private static final int PAGE_SIZE = 1024 * 1024;  // TODO: tune this\n+\n+  private final int initialSize;\n+  private final int numPartitions;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final BlockManager blockManager;\n+  private final TaskContext taskContext;\n+  private final boolean spillingEnabled;\n+  private ShuffleWriteMetrics writeMetrics;\n+\n+  /** The buffer size to use when writing spills using DiskBlockObjectWriter */\n+  private final int fileBufferSize;\n+\n+  /**\n+   * Memory pages that hold the records being sorted. The pages in this list are freed when\n+   * spilling, although in principle we could recycle these pages across spills (on the other hand,\n+   * this might not be necessary if we maintained a pool of re-usable pages in the TaskMemoryManager\n+   * itself).\n+   */\n+  private final LinkedList<MemoryBlock> allocatedPages = new LinkedList<MemoryBlock>();\n+\n+  private final LinkedList<SpillInfo> spills = new LinkedList<SpillInfo>();\n+\n+  // All three of these variables are reset after spilling:\n+  private UnsafeShuffleSorter sorter;\n+  private MemoryBlock currentPage = null;\n+  private long currentPagePosition = -1;\n+\n+  public UnsafeShuffleSpillWriter(\n+      TaskMemoryManager memoryManager,\n+      ShuffleMemoryManager shuffleMemoryManager,\n+      BlockManager blockManager,\n+      TaskContext taskContext,\n+      int initialSize,\n+      int numPartitions,\n+      SparkConf conf) throws IOException {\n+    this.memoryManager = memoryManager;\n+    this.shuffleMemoryManager = shuffleMemoryManager;\n+    this.blockManager = blockManager;\n+    this.taskContext = taskContext;\n+    this.initialSize = initialSize;\n+    this.numPartitions = numPartitions;\n+    this.spillingEnabled = conf.getBoolean(\"spark.shuffle.spill\", true);\n+    // Use getSizeAsKb (not bytes) to maintain backwards compatibility for units\n+    this.fileBufferSize = (int) conf.getSizeAsKb(\"spark.shuffle.file.buffer\", \"32k\") * 1024;\n+    openSorter();\n+  }\n+\n+  // TODO: metrics tracking + integration with shuffle write metrics\n+\n+  /**\n+   * Allocates a new sorter. Called when opening the spill writer for the first time and after\n+   * each spill.\n+   */\n+  private void openSorter() throws IOException {\n+    this.writeMetrics = new ShuffleWriteMetrics();\n+    // TODO: connect write metrics to task metrics?\n+    // TODO: move this sizing calculation logic into a static method of sorter:\n+    final long memoryRequested = initialSize * 8L;\n+    if (spillingEnabled) {\n+      final long memoryAcquired = shuffleMemoryManager.tryToAcquire(memoryRequested);\n+      if (memoryAcquired != memoryRequested) {\n+        shuffleMemoryManager.release(memoryAcquired);\n+        throw new IOException(\"Could not acquire memory!\");\n+      }\n+    }\n+\n+    this.sorter = new UnsafeShuffleSorter(initialSize);\n+  }\n+\n+  /**\n+   * Sorts the in-memory records, writes the sorted records to a spill file, and frees the in-memory\n+   * data structures associated with this sort. New data structures are not automatically allocated.\n+   */\n+  private SpillInfo writeSpillFile() throws IOException {\n+    // This call performs the actual sort.\n+    final UnsafeShuffleSorter.UnsafeShuffleSorterIterator sortedRecords =\n+      sorter.getSortedIterator();\n+\n+    // Currently, we need to open a new DiskBlockObjectWriter for each partition; we can avoid this\n+    // after SPARK-5581 is fixed.\n+    BlockObjectWriter writer = null;\n+\n+    // Small writes to DiskBlockObjectWriter will be fairly inefficient. Since there doesn't seem to\n+    // be an API to directly transfer bytes from managed memory to the disk writer, we buffer\n+    // records in a byte array. This array only needs to be big enough to hold a single record.\n+    final byte[] arr = new byte[SER_BUFFER_SIZE];\n+\n+    // Because this output will be read during shuffle, its compression codec must be controlled by\n+    // spark.shuffle.compress instead of spark.shuffle.spill.compress, so we need to use\n+    // createTempShuffleBlock here; see SPARK-3426 for more details.\n+    final Tuple2<TempShuffleBlockId, File> spilledFileInfo =\n+      blockManager.diskBlockManager().createTempShuffleBlock();\n+    final File file = spilledFileInfo._2();\n+    final BlockId blockId = spilledFileInfo._1();\n+    final SpillInfo spillInfo = new SpillInfo(numPartitions, file, blockId);\n+\n+    // Unfortunately, we need a serializer instance in order to construct a DiskBlockObjectWriter.\n+    // Our write path doesn't actually use this serializer (since we end up calling the `write()`\n+    // OutputStream methods), but DiskBlockObjectWriter still calls some methods on it. To work\n+    // around this, we pass a dummy no-op serializer.\n+    final SerializerInstance ser = new DummySerializerInstance();\n+    writer = blockManager.getDiskWriter(blockId, file, ser, fileBufferSize, writeMetrics);\n+\n+    int currentPartition = -1;\n+    while (sortedRecords.hasNext()) {\n+      sortedRecords.loadNext();\n+      final int partition = sortedRecords.packedRecordPointer.getPartitionId();\n+      assert (partition >= currentPartition);\n+      if (partition != currentPartition) {\n+        // Switch to the new partition\n+        if (currentPartition != -1) {\n+          writer.commitAndClose();\n+          spillInfo.partitionLengths[currentPartition] = writer.fileSegment().length();\n+        }\n+        currentPartition = partition;\n+        writer = blockManager.getDiskWriter(blockId, file, ser, fileBufferSize, writeMetrics);\n+      }\n+\n+      final long recordPointer = sortedRecords.packedRecordPointer.getRecordPointer();\n+      final int recordLength = PlatformDependent.UNSAFE.getInt(\n+        memoryManager.getPage(recordPointer), memoryManager.getOffsetInPage(recordPointer));\n+      PlatformDependent.copyMemory(\n+        memoryManager.getPage(recordPointer),\n+        memoryManager.getOffsetInPage(recordPointer) + 4, // skip over record length\n+        arr,\n+        PlatformDependent.BYTE_ARRAY_OFFSET,\n+        recordLength);\n+      assert (writer != null);  // To suppress an IntelliJ warning\n+      writer.write(arr, 0, recordLength);\n+      // TODO: add a test that detects whether we leave this call out:\n+      writer.recordWritten();\n+    }\n+\n+    if (writer != null) {\n+      writer.commitAndClose();\n+      // If `writeSpillFile()` was called from `closeAndGetSpills()` and no records were inserted,\n+      // then the spill file might be empty. Note that it might be better to avoid calling\n+      // writeSpillFile() in that case.\n+      if (currentPartition != -1) {\n+        spillInfo.partitionLengths[currentPartition] = writer.fileSegment().length();\n+        spills.add(spillInfo);\n+      }\n+    }\n+    return spillInfo;\n+  }\n+\n+  /**\n+   * Sort and spill the current records in response to memory pressure.\n+   */\n+  private void spill() throws IOException {\n+    final long threadId = Thread.currentThread().getId();\n+    logger.info(\"Thread \" + threadId + \" spilling sort data of \" +"
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "This was modeled after `Spillable`, which uses info-level logging: https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/collection/Spillable.scala#L116\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-07T00:42:37Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+\n+import org.apache.spark.storage.*;\n+import scala.Tuple2;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.MemoryBlock;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+/**\n+ * An external sorter that is specialized for sort-based shuffle.\n+ * <p>\n+ * Incoming records are appended to data pages. When all records have been inserted (or when the\n+ * current thread's shuffle memory limit is reached), the in-memory records are sorted according to\n+ * their partition ids (using a {@link UnsafeShuffleSorter}). The sorted records are then written\n+ * to a single output file (or multiple files, if we've spilled). The format of the output files is\n+ * the same as the format of the final output file written by\n+ * {@link org.apache.spark.shuffle.sort.SortShuffleWriter}: each output partition's records are\n+ * written as a single serialized, compressed stream that can be read with a new decompression and\n+ * deserialization stream.\n+ * <p>\n+ * Unlike {@link org.apache.spark.util.collection.ExternalSorter}, this sorter does not merge its\n+ * spill files. Instead, this merging is performed in {@link UnsafeShuffleWriter}, which uses a\n+ * specialized merge procedure that avoids extra serialization/deserialization.\n+ */\n+public final class UnsafeShuffleSpillWriter {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleSpillWriter.class);\n+\n+  private static final int SER_BUFFER_SIZE = 1024 * 1024;  // TODO: tune this / don't duplicate\n+  private static final int PAGE_SIZE = 1024 * 1024;  // TODO: tune this\n+\n+  private final int initialSize;\n+  private final int numPartitions;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final BlockManager blockManager;\n+  private final TaskContext taskContext;\n+  private final boolean spillingEnabled;\n+  private ShuffleWriteMetrics writeMetrics;\n+\n+  /** The buffer size to use when writing spills using DiskBlockObjectWriter */\n+  private final int fileBufferSize;\n+\n+  /**\n+   * Memory pages that hold the records being sorted. The pages in this list are freed when\n+   * spilling, although in principle we could recycle these pages across spills (on the other hand,\n+   * this might not be necessary if we maintained a pool of re-usable pages in the TaskMemoryManager\n+   * itself).\n+   */\n+  private final LinkedList<MemoryBlock> allocatedPages = new LinkedList<MemoryBlock>();\n+\n+  private final LinkedList<SpillInfo> spills = new LinkedList<SpillInfo>();\n+\n+  // All three of these variables are reset after spilling:\n+  private UnsafeShuffleSorter sorter;\n+  private MemoryBlock currentPage = null;\n+  private long currentPagePosition = -1;\n+\n+  public UnsafeShuffleSpillWriter(\n+      TaskMemoryManager memoryManager,\n+      ShuffleMemoryManager shuffleMemoryManager,\n+      BlockManager blockManager,\n+      TaskContext taskContext,\n+      int initialSize,\n+      int numPartitions,\n+      SparkConf conf) throws IOException {\n+    this.memoryManager = memoryManager;\n+    this.shuffleMemoryManager = shuffleMemoryManager;\n+    this.blockManager = blockManager;\n+    this.taskContext = taskContext;\n+    this.initialSize = initialSize;\n+    this.numPartitions = numPartitions;\n+    this.spillingEnabled = conf.getBoolean(\"spark.shuffle.spill\", true);\n+    // Use getSizeAsKb (not bytes) to maintain backwards compatibility for units\n+    this.fileBufferSize = (int) conf.getSizeAsKb(\"spark.shuffle.file.buffer\", \"32k\") * 1024;\n+    openSorter();\n+  }\n+\n+  // TODO: metrics tracking + integration with shuffle write metrics\n+\n+  /**\n+   * Allocates a new sorter. Called when opening the spill writer for the first time and after\n+   * each spill.\n+   */\n+  private void openSorter() throws IOException {\n+    this.writeMetrics = new ShuffleWriteMetrics();\n+    // TODO: connect write metrics to task metrics?\n+    // TODO: move this sizing calculation logic into a static method of sorter:\n+    final long memoryRequested = initialSize * 8L;\n+    if (spillingEnabled) {\n+      final long memoryAcquired = shuffleMemoryManager.tryToAcquire(memoryRequested);\n+      if (memoryAcquired != memoryRequested) {\n+        shuffleMemoryManager.release(memoryAcquired);\n+        throw new IOException(\"Could not acquire memory!\");\n+      }\n+    }\n+\n+    this.sorter = new UnsafeShuffleSorter(initialSize);\n+  }\n+\n+  /**\n+   * Sorts the in-memory records, writes the sorted records to a spill file, and frees the in-memory\n+   * data structures associated with this sort. New data structures are not automatically allocated.\n+   */\n+  private SpillInfo writeSpillFile() throws IOException {\n+    // This call performs the actual sort.\n+    final UnsafeShuffleSorter.UnsafeShuffleSorterIterator sortedRecords =\n+      sorter.getSortedIterator();\n+\n+    // Currently, we need to open a new DiskBlockObjectWriter for each partition; we can avoid this\n+    // after SPARK-5581 is fixed.\n+    BlockObjectWriter writer = null;\n+\n+    // Small writes to DiskBlockObjectWriter will be fairly inefficient. Since there doesn't seem to\n+    // be an API to directly transfer bytes from managed memory to the disk writer, we buffer\n+    // records in a byte array. This array only needs to be big enough to hold a single record.\n+    final byte[] arr = new byte[SER_BUFFER_SIZE];\n+\n+    // Because this output will be read during shuffle, its compression codec must be controlled by\n+    // spark.shuffle.compress instead of spark.shuffle.spill.compress, so we need to use\n+    // createTempShuffleBlock here; see SPARK-3426 for more details.\n+    final Tuple2<TempShuffleBlockId, File> spilledFileInfo =\n+      blockManager.diskBlockManager().createTempShuffleBlock();\n+    final File file = spilledFileInfo._2();\n+    final BlockId blockId = spilledFileInfo._1();\n+    final SpillInfo spillInfo = new SpillInfo(numPartitions, file, blockId);\n+\n+    // Unfortunately, we need a serializer instance in order to construct a DiskBlockObjectWriter.\n+    // Our write path doesn't actually use this serializer (since we end up calling the `write()`\n+    // OutputStream methods), but DiskBlockObjectWriter still calls some methods on it. To work\n+    // around this, we pass a dummy no-op serializer.\n+    final SerializerInstance ser = new DummySerializerInstance();\n+    writer = blockManager.getDiskWriter(blockId, file, ser, fileBufferSize, writeMetrics);\n+\n+    int currentPartition = -1;\n+    while (sortedRecords.hasNext()) {\n+      sortedRecords.loadNext();\n+      final int partition = sortedRecords.packedRecordPointer.getPartitionId();\n+      assert (partition >= currentPartition);\n+      if (partition != currentPartition) {\n+        // Switch to the new partition\n+        if (currentPartition != -1) {\n+          writer.commitAndClose();\n+          spillInfo.partitionLengths[currentPartition] = writer.fileSegment().length();\n+        }\n+        currentPartition = partition;\n+        writer = blockManager.getDiskWriter(blockId, file, ser, fileBufferSize, writeMetrics);\n+      }\n+\n+      final long recordPointer = sortedRecords.packedRecordPointer.getRecordPointer();\n+      final int recordLength = PlatformDependent.UNSAFE.getInt(\n+        memoryManager.getPage(recordPointer), memoryManager.getOffsetInPage(recordPointer));\n+      PlatformDependent.copyMemory(\n+        memoryManager.getPage(recordPointer),\n+        memoryManager.getOffsetInPage(recordPointer) + 4, // skip over record length\n+        arr,\n+        PlatformDependent.BYTE_ARRAY_OFFSET,\n+        recordLength);\n+      assert (writer != null);  // To suppress an IntelliJ warning\n+      writer.write(arr, 0, recordLength);\n+      // TODO: add a test that detects whether we leave this call out:\n+      writer.recordWritten();\n+    }\n+\n+    if (writer != null) {\n+      writer.commitAndClose();\n+      // If `writeSpillFile()` was called from `closeAndGetSpills()` and no records were inserted,\n+      // then the spill file might be empty. Note that it might be better to avoid calling\n+      // writeSpillFile() in that case.\n+      if (currentPartition != -1) {\n+        spillInfo.partitionLengths[currentPartition] = writer.fileSegment().length();\n+        spills.add(spillInfo);\n+      }\n+    }\n+    return spillInfo;\n+  }\n+\n+  /**\n+   * Sort and spill the current records in response to memory pressure.\n+   */\n+  private void spill() throws IOException {\n+    final long threadId = Thread.currentThread().getId();\n+    logger.info(\"Thread \" + threadId + \" spilling sort data of \" +"
  }],
  "prId": 5868
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "you can use \n\n``` java\nfor (MemoryBlock block: allocatedPages) {\n\n}\n```\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-07T00:41:53Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+\n+import org.apache.spark.storage.*;\n+import scala.Tuple2;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.MemoryBlock;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+/**\n+ * An external sorter that is specialized for sort-based shuffle.\n+ * <p>\n+ * Incoming records are appended to data pages. When all records have been inserted (or when the\n+ * current thread's shuffle memory limit is reached), the in-memory records are sorted according to\n+ * their partition ids (using a {@link UnsafeShuffleSorter}). The sorted records are then written\n+ * to a single output file (or multiple files, if we've spilled). The format of the output files is\n+ * the same as the format of the final output file written by\n+ * {@link org.apache.spark.shuffle.sort.SortShuffleWriter}: each output partition's records are\n+ * written as a single serialized, compressed stream that can be read with a new decompression and\n+ * deserialization stream.\n+ * <p>\n+ * Unlike {@link org.apache.spark.util.collection.ExternalSorter}, this sorter does not merge its\n+ * spill files. Instead, this merging is performed in {@link UnsafeShuffleWriter}, which uses a\n+ * specialized merge procedure that avoids extra serialization/deserialization.\n+ */\n+public final class UnsafeShuffleSpillWriter {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleSpillWriter.class);\n+\n+  private static final int SER_BUFFER_SIZE = 1024 * 1024;  // TODO: tune this / don't duplicate\n+  private static final int PAGE_SIZE = 1024 * 1024;  // TODO: tune this\n+\n+  private final int initialSize;\n+  private final int numPartitions;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final BlockManager blockManager;\n+  private final TaskContext taskContext;\n+  private final boolean spillingEnabled;\n+  private ShuffleWriteMetrics writeMetrics;\n+\n+  /** The buffer size to use when writing spills using DiskBlockObjectWriter */\n+  private final int fileBufferSize;\n+\n+  /**\n+   * Memory pages that hold the records being sorted. The pages in this list are freed when\n+   * spilling, although in principle we could recycle these pages across spills (on the other hand,\n+   * this might not be necessary if we maintained a pool of re-usable pages in the TaskMemoryManager\n+   * itself).\n+   */\n+  private final LinkedList<MemoryBlock> allocatedPages = new LinkedList<MemoryBlock>();\n+\n+  private final LinkedList<SpillInfo> spills = new LinkedList<SpillInfo>();\n+\n+  // All three of these variables are reset after spilling:\n+  private UnsafeShuffleSorter sorter;\n+  private MemoryBlock currentPage = null;\n+  private long currentPagePosition = -1;\n+\n+  public UnsafeShuffleSpillWriter(\n+      TaskMemoryManager memoryManager,\n+      ShuffleMemoryManager shuffleMemoryManager,\n+      BlockManager blockManager,\n+      TaskContext taskContext,\n+      int initialSize,\n+      int numPartitions,\n+      SparkConf conf) throws IOException {\n+    this.memoryManager = memoryManager;\n+    this.shuffleMemoryManager = shuffleMemoryManager;\n+    this.blockManager = blockManager;\n+    this.taskContext = taskContext;\n+    this.initialSize = initialSize;\n+    this.numPartitions = numPartitions;\n+    this.spillingEnabled = conf.getBoolean(\"spark.shuffle.spill\", true);\n+    // Use getSizeAsKb (not bytes) to maintain backwards compatibility for units\n+    this.fileBufferSize = (int) conf.getSizeAsKb(\"spark.shuffle.file.buffer\", \"32k\") * 1024;\n+    openSorter();\n+  }\n+\n+  // TODO: metrics tracking + integration with shuffle write metrics\n+\n+  /**\n+   * Allocates a new sorter. Called when opening the spill writer for the first time and after\n+   * each spill.\n+   */\n+  private void openSorter() throws IOException {\n+    this.writeMetrics = new ShuffleWriteMetrics();\n+    // TODO: connect write metrics to task metrics?\n+    // TODO: move this sizing calculation logic into a static method of sorter:\n+    final long memoryRequested = initialSize * 8L;\n+    if (spillingEnabled) {\n+      final long memoryAcquired = shuffleMemoryManager.tryToAcquire(memoryRequested);\n+      if (memoryAcquired != memoryRequested) {\n+        shuffleMemoryManager.release(memoryAcquired);\n+        throw new IOException(\"Could not acquire memory!\");\n+      }\n+    }\n+\n+    this.sorter = new UnsafeShuffleSorter(initialSize);\n+  }\n+\n+  /**\n+   * Sorts the in-memory records, writes the sorted records to a spill file, and frees the in-memory\n+   * data structures associated with this sort. New data structures are not automatically allocated.\n+   */\n+  private SpillInfo writeSpillFile() throws IOException {\n+    // This call performs the actual sort.\n+    final UnsafeShuffleSorter.UnsafeShuffleSorterIterator sortedRecords =\n+      sorter.getSortedIterator();\n+\n+    // Currently, we need to open a new DiskBlockObjectWriter for each partition; we can avoid this\n+    // after SPARK-5581 is fixed.\n+    BlockObjectWriter writer = null;\n+\n+    // Small writes to DiskBlockObjectWriter will be fairly inefficient. Since there doesn't seem to\n+    // be an API to directly transfer bytes from managed memory to the disk writer, we buffer\n+    // records in a byte array. This array only needs to be big enough to hold a single record.\n+    final byte[] arr = new byte[SER_BUFFER_SIZE];\n+\n+    // Because this output will be read during shuffle, its compression codec must be controlled by\n+    // spark.shuffle.compress instead of spark.shuffle.spill.compress, so we need to use\n+    // createTempShuffleBlock here; see SPARK-3426 for more details.\n+    final Tuple2<TempShuffleBlockId, File> spilledFileInfo =\n+      blockManager.diskBlockManager().createTempShuffleBlock();\n+    final File file = spilledFileInfo._2();\n+    final BlockId blockId = spilledFileInfo._1();\n+    final SpillInfo spillInfo = new SpillInfo(numPartitions, file, blockId);\n+\n+    // Unfortunately, we need a serializer instance in order to construct a DiskBlockObjectWriter.\n+    // Our write path doesn't actually use this serializer (since we end up calling the `write()`\n+    // OutputStream methods), but DiskBlockObjectWriter still calls some methods on it. To work\n+    // around this, we pass a dummy no-op serializer.\n+    final SerializerInstance ser = new DummySerializerInstance();\n+    writer = blockManager.getDiskWriter(blockId, file, ser, fileBufferSize, writeMetrics);\n+\n+    int currentPartition = -1;\n+    while (sortedRecords.hasNext()) {\n+      sortedRecords.loadNext();\n+      final int partition = sortedRecords.packedRecordPointer.getPartitionId();\n+      assert (partition >= currentPartition);\n+      if (partition != currentPartition) {\n+        // Switch to the new partition\n+        if (currentPartition != -1) {\n+          writer.commitAndClose();\n+          spillInfo.partitionLengths[currentPartition] = writer.fileSegment().length();\n+        }\n+        currentPartition = partition;\n+        writer = blockManager.getDiskWriter(blockId, file, ser, fileBufferSize, writeMetrics);\n+      }\n+\n+      final long recordPointer = sortedRecords.packedRecordPointer.getRecordPointer();\n+      final int recordLength = PlatformDependent.UNSAFE.getInt(\n+        memoryManager.getPage(recordPointer), memoryManager.getOffsetInPage(recordPointer));\n+      PlatformDependent.copyMemory(\n+        memoryManager.getPage(recordPointer),\n+        memoryManager.getOffsetInPage(recordPointer) + 4, // skip over record length\n+        arr,\n+        PlatformDependent.BYTE_ARRAY_OFFSET,\n+        recordLength);\n+      assert (writer != null);  // To suppress an IntelliJ warning\n+      writer.write(arr, 0, recordLength);\n+      // TODO: add a test that detects whether we leave this call out:\n+      writer.recordWritten();\n+    }\n+\n+    if (writer != null) {\n+      writer.commitAndClose();\n+      // If `writeSpillFile()` was called from `closeAndGetSpills()` and no records were inserted,\n+      // then the spill file might be empty. Note that it might be better to avoid calling\n+      // writeSpillFile() in that case.\n+      if (currentPartition != -1) {\n+        spillInfo.partitionLengths[currentPartition] = writer.fileSegment().length();\n+        spills.add(spillInfo);\n+      }\n+    }\n+    return spillInfo;\n+  }\n+\n+  /**\n+   * Sort and spill the current records in response to memory pressure.\n+   */\n+  private void spill() throws IOException {\n+    final long threadId = Thread.currentThread().getId();\n+    logger.info(\"Thread \" + threadId + \" spilling sort data of \" +\n+      org.apache.spark.util.Utils.bytesToString(getMemoryUsage()) + \" to disk (\" +\n+      (spills.size() + (spills.size() > 1 ? \" times\" : \" time\")) + \" so far)\");\n+\n+    final SpillInfo spillInfo = writeSpillFile();\n+    final long sorterMemoryUsage = sorter.getMemoryUsage();\n+    sorter = null;\n+    shuffleMemoryManager.release(sorterMemoryUsage);\n+    final long spillSize = freeMemory();\n+    taskContext.taskMetrics().incMemoryBytesSpilled(spillSize);\n+    taskContext.taskMetrics().incDiskBytesSpilled(spillInfo.file.length());\n+\n+    openSorter();\n+  }\n+\n+  private long getMemoryUsage() {\n+    return sorter.getMemoryUsage() + (allocatedPages.size() * PAGE_SIZE);\n+  }\n+\n+  private long freeMemory() {\n+    long memoryFreed = 0;\n+    final Iterator<MemoryBlock> iter = allocatedPages.iterator();\n+    while (iter.hasNext()) {"
  }],
  "prId": 5868
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "i'm somewhat confused; maybe more comemnts would help. do we always spill as soon as we reach the size of a page?\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-07T00:51:53Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+\n+import org.apache.spark.storage.*;\n+import scala.Tuple2;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.MemoryBlock;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+/**\n+ * An external sorter that is specialized for sort-based shuffle.\n+ * <p>\n+ * Incoming records are appended to data pages. When all records have been inserted (or when the\n+ * current thread's shuffle memory limit is reached), the in-memory records are sorted according to\n+ * their partition ids (using a {@link UnsafeShuffleSorter}). The sorted records are then written\n+ * to a single output file (or multiple files, if we've spilled). The format of the output files is\n+ * the same as the format of the final output file written by\n+ * {@link org.apache.spark.shuffle.sort.SortShuffleWriter}: each output partition's records are\n+ * written as a single serialized, compressed stream that can be read with a new decompression and\n+ * deserialization stream.\n+ * <p>\n+ * Unlike {@link org.apache.spark.util.collection.ExternalSorter}, this sorter does not merge its\n+ * spill files. Instead, this merging is performed in {@link UnsafeShuffleWriter}, which uses a\n+ * specialized merge procedure that avoids extra serialization/deserialization.\n+ */\n+public final class UnsafeShuffleSpillWriter {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleSpillWriter.class);\n+\n+  private static final int SER_BUFFER_SIZE = 1024 * 1024;  // TODO: tune this / don't duplicate\n+  private static final int PAGE_SIZE = 1024 * 1024;  // TODO: tune this\n+\n+  private final int initialSize;\n+  private final int numPartitions;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final BlockManager blockManager;\n+  private final TaskContext taskContext;\n+  private final boolean spillingEnabled;\n+  private ShuffleWriteMetrics writeMetrics;\n+\n+  /** The buffer size to use when writing spills using DiskBlockObjectWriter */\n+  private final int fileBufferSize;\n+\n+  /**\n+   * Memory pages that hold the records being sorted. The pages in this list are freed when\n+   * spilling, although in principle we could recycle these pages across spills (on the other hand,\n+   * this might not be necessary if we maintained a pool of re-usable pages in the TaskMemoryManager\n+   * itself).\n+   */\n+  private final LinkedList<MemoryBlock> allocatedPages = new LinkedList<MemoryBlock>();\n+\n+  private final LinkedList<SpillInfo> spills = new LinkedList<SpillInfo>();\n+\n+  // All three of these variables are reset after spilling:\n+  private UnsafeShuffleSorter sorter;\n+  private MemoryBlock currentPage = null;\n+  private long currentPagePosition = -1;\n+\n+  public UnsafeShuffleSpillWriter(\n+      TaskMemoryManager memoryManager,\n+      ShuffleMemoryManager shuffleMemoryManager,\n+      BlockManager blockManager,\n+      TaskContext taskContext,\n+      int initialSize,\n+      int numPartitions,\n+      SparkConf conf) throws IOException {\n+    this.memoryManager = memoryManager;\n+    this.shuffleMemoryManager = shuffleMemoryManager;\n+    this.blockManager = blockManager;\n+    this.taskContext = taskContext;\n+    this.initialSize = initialSize;\n+    this.numPartitions = numPartitions;\n+    this.spillingEnabled = conf.getBoolean(\"spark.shuffle.spill\", true);\n+    // Use getSizeAsKb (not bytes) to maintain backwards compatibility for units\n+    this.fileBufferSize = (int) conf.getSizeAsKb(\"spark.shuffle.file.buffer\", \"32k\") * 1024;\n+    openSorter();\n+  }\n+\n+  // TODO: metrics tracking + integration with shuffle write metrics\n+\n+  /**\n+   * Allocates a new sorter. Called when opening the spill writer for the first time and after\n+   * each spill.\n+   */\n+  private void openSorter() throws IOException {\n+    this.writeMetrics = new ShuffleWriteMetrics();\n+    // TODO: connect write metrics to task metrics?\n+    // TODO: move this sizing calculation logic into a static method of sorter:\n+    final long memoryRequested = initialSize * 8L;\n+    if (spillingEnabled) {\n+      final long memoryAcquired = shuffleMemoryManager.tryToAcquire(memoryRequested);\n+      if (memoryAcquired != memoryRequested) {\n+        shuffleMemoryManager.release(memoryAcquired);\n+        throw new IOException(\"Could not acquire memory!\");\n+      }\n+    }\n+\n+    this.sorter = new UnsafeShuffleSorter(initialSize);\n+  }\n+\n+  /**\n+   * Sorts the in-memory records, writes the sorted records to a spill file, and frees the in-memory\n+   * data structures associated with this sort. New data structures are not automatically allocated.\n+   */\n+  private SpillInfo writeSpillFile() throws IOException {\n+    // This call performs the actual sort.\n+    final UnsafeShuffleSorter.UnsafeShuffleSorterIterator sortedRecords =\n+      sorter.getSortedIterator();\n+\n+    // Currently, we need to open a new DiskBlockObjectWriter for each partition; we can avoid this\n+    // after SPARK-5581 is fixed.\n+    BlockObjectWriter writer = null;\n+\n+    // Small writes to DiskBlockObjectWriter will be fairly inefficient. Since there doesn't seem to\n+    // be an API to directly transfer bytes from managed memory to the disk writer, we buffer\n+    // records in a byte array. This array only needs to be big enough to hold a single record.\n+    final byte[] arr = new byte[SER_BUFFER_SIZE];\n+\n+    // Because this output will be read during shuffle, its compression codec must be controlled by\n+    // spark.shuffle.compress instead of spark.shuffle.spill.compress, so we need to use\n+    // createTempShuffleBlock here; see SPARK-3426 for more details.\n+    final Tuple2<TempShuffleBlockId, File> spilledFileInfo =\n+      blockManager.diskBlockManager().createTempShuffleBlock();\n+    final File file = spilledFileInfo._2();\n+    final BlockId blockId = spilledFileInfo._1();\n+    final SpillInfo spillInfo = new SpillInfo(numPartitions, file, blockId);\n+\n+    // Unfortunately, we need a serializer instance in order to construct a DiskBlockObjectWriter.\n+    // Our write path doesn't actually use this serializer (since we end up calling the `write()`\n+    // OutputStream methods), but DiskBlockObjectWriter still calls some methods on it. To work\n+    // around this, we pass a dummy no-op serializer.\n+    final SerializerInstance ser = new DummySerializerInstance();\n+    writer = blockManager.getDiskWriter(blockId, file, ser, fileBufferSize, writeMetrics);\n+\n+    int currentPartition = -1;\n+    while (sortedRecords.hasNext()) {\n+      sortedRecords.loadNext();\n+      final int partition = sortedRecords.packedRecordPointer.getPartitionId();\n+      assert (partition >= currentPartition);\n+      if (partition != currentPartition) {\n+        // Switch to the new partition\n+        if (currentPartition != -1) {\n+          writer.commitAndClose();\n+          spillInfo.partitionLengths[currentPartition] = writer.fileSegment().length();\n+        }\n+        currentPartition = partition;\n+        writer = blockManager.getDiskWriter(blockId, file, ser, fileBufferSize, writeMetrics);\n+      }\n+\n+      final long recordPointer = sortedRecords.packedRecordPointer.getRecordPointer();\n+      final int recordLength = PlatformDependent.UNSAFE.getInt(\n+        memoryManager.getPage(recordPointer), memoryManager.getOffsetInPage(recordPointer));\n+      PlatformDependent.copyMemory(\n+        memoryManager.getPage(recordPointer),\n+        memoryManager.getOffsetInPage(recordPointer) + 4, // skip over record length\n+        arr,\n+        PlatformDependent.BYTE_ARRAY_OFFSET,\n+        recordLength);\n+      assert (writer != null);  // To suppress an IntelliJ warning\n+      writer.write(arr, 0, recordLength);\n+      // TODO: add a test that detects whether we leave this call out:\n+      writer.recordWritten();\n+    }\n+\n+    if (writer != null) {\n+      writer.commitAndClose();\n+      // If `writeSpillFile()` was called from `closeAndGetSpills()` and no records were inserted,\n+      // then the spill file might be empty. Note that it might be better to avoid calling\n+      // writeSpillFile() in that case.\n+      if (currentPartition != -1) {\n+        spillInfo.partitionLengths[currentPartition] = writer.fileSegment().length();\n+        spills.add(spillInfo);\n+      }\n+    }\n+    return spillInfo;\n+  }\n+\n+  /**\n+   * Sort and spill the current records in response to memory pressure.\n+   */\n+  private void spill() throws IOException {\n+    final long threadId = Thread.currentThread().getId();\n+    logger.info(\"Thread \" + threadId + \" spilling sort data of \" +\n+      org.apache.spark.util.Utils.bytesToString(getMemoryUsage()) + \" to disk (\" +\n+      (spills.size() + (spills.size() > 1 ? \" times\" : \" time\")) + \" so far)\");\n+\n+    final SpillInfo spillInfo = writeSpillFile();\n+    final long sorterMemoryUsage = sorter.getMemoryUsage();\n+    sorter = null;\n+    shuffleMemoryManager.release(sorterMemoryUsage);\n+    final long spillSize = freeMemory();\n+    taskContext.taskMetrics().incMemoryBytesSpilled(spillSize);\n+    taskContext.taskMetrics().incDiskBytesSpilled(spillInfo.file.length());\n+\n+    openSorter();\n+  }\n+\n+  private long getMemoryUsage() {\n+    return sorter.getMemoryUsage() + (allocatedPages.size() * PAGE_SIZE);\n+  }\n+\n+  private long freeMemory() {\n+    long memoryFreed = 0;\n+    final Iterator<MemoryBlock> iter = allocatedPages.iterator();\n+    while (iter.hasNext()) {\n+      memoryManager.freePage(iter.next());\n+      shuffleMemoryManager.release(PAGE_SIZE);\n+      memoryFreed += PAGE_SIZE;\n+      iter.remove();\n+    }\n+    currentPage = null;\n+    currentPagePosition = -1;\n+    return memoryFreed;\n+  }\n+\n+  /**\n+   * Checks whether there is enough space to insert a new record into the sorter. If there is\n+   * insufficient space, either allocate more memory or spill the current sort data (if spilling\n+   * is enabled), then insert the record.\n+   */\n+  private void ensureSpaceInDataPage(int requiredSpace) throws IOException {\n+    // TODO: we should re-order the `if` cases in this function so that the most common case (there\n+    // is enough space) appears first.\n+\n+    // TODO: merge these steps to first calculate total memory requirements for this insert,\n+    // then try to acquire; no point in acquiring sort buffer only to spill due to no space in the\n+    // data page.\n+    if (!sorter.hasSpaceForAnotherRecord() && spillingEnabled) {\n+      final long oldSortBufferMemoryUsage = sorter.getMemoryUsage();\n+      final long memoryToGrowSortBuffer = oldSortBufferMemoryUsage * 2;\n+      final long memoryAcquired = shuffleMemoryManager.tryToAcquire(memoryToGrowSortBuffer);\n+      if (memoryAcquired < memoryToGrowSortBuffer) {\n+        shuffleMemoryManager.release(memoryAcquired);\n+        spill();\n+      } else {\n+        sorter.expandSortBuffer();\n+        shuffleMemoryManager.release(oldSortBufferMemoryUsage);\n+      }\n+    }\n+\n+    final long spaceInCurrentPage;\n+    if (currentPage != null) {\n+      spaceInCurrentPage = PAGE_SIZE - (currentPagePosition - currentPage.getBaseOffset());\n+    } else {\n+      spaceInCurrentPage = 0;\n+    }\n+    if (requiredSpace > PAGE_SIZE) {\n+      // TODO: throw a more specific exception?\n+      throw new IOException(\"Required space \" + requiredSpace + \" is greater than page size (\" +\n+        PAGE_SIZE + \")\");\n+    } else if (requiredSpace > spaceInCurrentPage) {\n+      if (spillingEnabled) {"
  }],
  "prId": 5868
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "we should look at the potential exception paths, and handle them to make sure we can clean up the tmp files in the case of exceptions\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-07T00:56:19Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+\n+import org.apache.spark.storage.*;\n+import scala.Tuple2;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.MemoryBlock;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+/**\n+ * An external sorter that is specialized for sort-based shuffle.\n+ * <p>\n+ * Incoming records are appended to data pages. When all records have been inserted (or when the\n+ * current thread's shuffle memory limit is reached), the in-memory records are sorted according to\n+ * their partition ids (using a {@link UnsafeShuffleSorter}). The sorted records are then written\n+ * to a single output file (or multiple files, if we've spilled). The format of the output files is\n+ * the same as the format of the final output file written by\n+ * {@link org.apache.spark.shuffle.sort.SortShuffleWriter}: each output partition's records are\n+ * written as a single serialized, compressed stream that can be read with a new decompression and\n+ * deserialization stream.\n+ * <p>\n+ * Unlike {@link org.apache.spark.util.collection.ExternalSorter}, this sorter does not merge its\n+ * spill files. Instead, this merging is performed in {@link UnsafeShuffleWriter}, which uses a\n+ * specialized merge procedure that avoids extra serialization/deserialization.\n+ */\n+public final class UnsafeShuffleSpillWriter {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleSpillWriter.class);\n+\n+  private static final int SER_BUFFER_SIZE = 1024 * 1024;  // TODO: tune this / don't duplicate\n+  private static final int PAGE_SIZE = 1024 * 1024;  // TODO: tune this\n+\n+  private final int initialSize;\n+  private final int numPartitions;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final BlockManager blockManager;\n+  private final TaskContext taskContext;\n+  private final boolean spillingEnabled;\n+  private ShuffleWriteMetrics writeMetrics;\n+\n+  /** The buffer size to use when writing spills using DiskBlockObjectWriter */\n+  private final int fileBufferSize;\n+\n+  /**\n+   * Memory pages that hold the records being sorted. The pages in this list are freed when\n+   * spilling, although in principle we could recycle these pages across spills (on the other hand,\n+   * this might not be necessary if we maintained a pool of re-usable pages in the TaskMemoryManager\n+   * itself).\n+   */\n+  private final LinkedList<MemoryBlock> allocatedPages = new LinkedList<MemoryBlock>();\n+\n+  private final LinkedList<SpillInfo> spills = new LinkedList<SpillInfo>();\n+\n+  // All three of these variables are reset after spilling:\n+  private UnsafeShuffleSorter sorter;\n+  private MemoryBlock currentPage = null;\n+  private long currentPagePosition = -1;\n+\n+  public UnsafeShuffleSpillWriter(\n+      TaskMemoryManager memoryManager,\n+      ShuffleMemoryManager shuffleMemoryManager,\n+      BlockManager blockManager,\n+      TaskContext taskContext,\n+      int initialSize,\n+      int numPartitions,\n+      SparkConf conf) throws IOException {\n+    this.memoryManager = memoryManager;\n+    this.shuffleMemoryManager = shuffleMemoryManager;\n+    this.blockManager = blockManager;\n+    this.taskContext = taskContext;\n+    this.initialSize = initialSize;\n+    this.numPartitions = numPartitions;\n+    this.spillingEnabled = conf.getBoolean(\"spark.shuffle.spill\", true);\n+    // Use getSizeAsKb (not bytes) to maintain backwards compatibility for units\n+    this.fileBufferSize = (int) conf.getSizeAsKb(\"spark.shuffle.file.buffer\", \"32k\") * 1024;\n+    openSorter();\n+  }\n+\n+  // TODO: metrics tracking + integration with shuffle write metrics\n+\n+  /**\n+   * Allocates a new sorter. Called when opening the spill writer for the first time and after\n+   * each spill.\n+   */\n+  private void openSorter() throws IOException {\n+    this.writeMetrics = new ShuffleWriteMetrics();\n+    // TODO: connect write metrics to task metrics?\n+    // TODO: move this sizing calculation logic into a static method of sorter:\n+    final long memoryRequested = initialSize * 8L;\n+    if (spillingEnabled) {\n+      final long memoryAcquired = shuffleMemoryManager.tryToAcquire(memoryRequested);\n+      if (memoryAcquired != memoryRequested) {\n+        shuffleMemoryManager.release(memoryAcquired);\n+        throw new IOException(\"Could not acquire memory!\");\n+      }\n+    }\n+\n+    this.sorter = new UnsafeShuffleSorter(initialSize);\n+  }\n+\n+  /**\n+   * Sorts the in-memory records, writes the sorted records to a spill file, and frees the in-memory\n+   * data structures associated with this sort. New data structures are not automatically allocated.\n+   */\n+  private SpillInfo writeSpillFile() throws IOException {\n+    // This call performs the actual sort.\n+    final UnsafeShuffleSorter.UnsafeShuffleSorterIterator sortedRecords =\n+      sorter.getSortedIterator();\n+\n+    // Currently, we need to open a new DiskBlockObjectWriter for each partition; we can avoid this\n+    // after SPARK-5581 is fixed.\n+    BlockObjectWriter writer = null;\n+\n+    // Small writes to DiskBlockObjectWriter will be fairly inefficient. Since there doesn't seem to\n+    // be an API to directly transfer bytes from managed memory to the disk writer, we buffer\n+    // records in a byte array. This array only needs to be big enough to hold a single record.\n+    final byte[] arr = new byte[SER_BUFFER_SIZE];\n+\n+    // Because this output will be read during shuffle, its compression codec must be controlled by\n+    // spark.shuffle.compress instead of spark.shuffle.spill.compress, so we need to use\n+    // createTempShuffleBlock here; see SPARK-3426 for more details.\n+    final Tuple2<TempShuffleBlockId, File> spilledFileInfo =\n+      blockManager.diskBlockManager().createTempShuffleBlock();\n+    final File file = spilledFileInfo._2();\n+    final BlockId blockId = spilledFileInfo._1();\n+    final SpillInfo spillInfo = new SpillInfo(numPartitions, file, blockId);\n+\n+    // Unfortunately, we need a serializer instance in order to construct a DiskBlockObjectWriter.\n+    // Our write path doesn't actually use this serializer (since we end up calling the `write()`\n+    // OutputStream methods), but DiskBlockObjectWriter still calls some methods on it. To work\n+    // around this, we pass a dummy no-op serializer.\n+    final SerializerInstance ser = new DummySerializerInstance();\n+    writer = blockManager.getDiskWriter(blockId, file, ser, fileBufferSize, writeMetrics);\n+\n+    int currentPartition = -1;\n+    while (sortedRecords.hasNext()) {\n+      sortedRecords.loadNext();\n+      final int partition = sortedRecords.packedRecordPointer.getPartitionId();\n+      assert (partition >= currentPartition);\n+      if (partition != currentPartition) {\n+        // Switch to the new partition\n+        if (currentPartition != -1) {\n+          writer.commitAndClose();\n+          spillInfo.partitionLengths[currentPartition] = writer.fileSegment().length();\n+        }\n+        currentPartition = partition;\n+        writer = blockManager.getDiskWriter(blockId, file, ser, fileBufferSize, writeMetrics);\n+      }\n+\n+      final long recordPointer = sortedRecords.packedRecordPointer.getRecordPointer();\n+      final int recordLength = PlatformDependent.UNSAFE.getInt(\n+        memoryManager.getPage(recordPointer), memoryManager.getOffsetInPage(recordPointer));\n+      PlatformDependent.copyMemory(\n+        memoryManager.getPage(recordPointer),\n+        memoryManager.getOffsetInPage(recordPointer) + 4, // skip over record length\n+        arr,\n+        PlatformDependent.BYTE_ARRAY_OFFSET,\n+        recordLength);\n+      assert (writer != null);  // To suppress an IntelliJ warning\n+      writer.write(arr, 0, recordLength);\n+      // TODO: add a test that detects whether we leave this call out:\n+      writer.recordWritten();\n+    }\n+\n+    if (writer != null) {\n+      writer.commitAndClose();\n+      // If `writeSpillFile()` was called from `closeAndGetSpills()` and no records were inserted,\n+      // then the spill file might be empty. Note that it might be better to avoid calling\n+      // writeSpillFile() in that case.\n+      if (currentPartition != -1) {\n+        spillInfo.partitionLengths[currentPartition] = writer.fileSegment().length();\n+        spills.add(spillInfo);\n+      }\n+    }\n+    return spillInfo;\n+  }\n+\n+  /**\n+   * Sort and spill the current records in response to memory pressure.\n+   */\n+  private void spill() throws IOException {\n+    final long threadId = Thread.currentThread().getId();\n+    logger.info(\"Thread \" + threadId + \" spilling sort data of \" +\n+      org.apache.spark.util.Utils.bytesToString(getMemoryUsage()) + \" to disk (\" +\n+      (spills.size() + (spills.size() > 1 ? \" times\" : \" time\")) + \" so far)\");\n+\n+    final SpillInfo spillInfo = writeSpillFile();\n+    final long sorterMemoryUsage = sorter.getMemoryUsage();\n+    sorter = null;\n+    shuffleMemoryManager.release(sorterMemoryUsage);\n+    final long spillSize = freeMemory();\n+    taskContext.taskMetrics().incMemoryBytesSpilled(spillSize);\n+    taskContext.taskMetrics().incDiskBytesSpilled(spillInfo.file.length());\n+\n+    openSorter();\n+  }\n+\n+  private long getMemoryUsage() {\n+    return sorter.getMemoryUsage() + (allocatedPages.size() * PAGE_SIZE);\n+  }\n+\n+  private long freeMemory() {\n+    long memoryFreed = 0;\n+    final Iterator<MemoryBlock> iter = allocatedPages.iterator();\n+    while (iter.hasNext()) {\n+      memoryManager.freePage(iter.next());\n+      shuffleMemoryManager.release(PAGE_SIZE);\n+      memoryFreed += PAGE_SIZE;\n+      iter.remove();\n+    }\n+    currentPage = null;\n+    currentPagePosition = -1;\n+    return memoryFreed;\n+  }\n+\n+  /**\n+   * Checks whether there is enough space to insert a new record into the sorter. If there is\n+   * insufficient space, either allocate more memory or spill the current sort data (if spilling\n+   * is enabled), then insert the record.\n+   */\n+  private void ensureSpaceInDataPage(int requiredSpace) throws IOException {\n+    // TODO: we should re-order the `if` cases in this function so that the most common case (there\n+    // is enough space) appears first.\n+\n+    // TODO: merge these steps to first calculate total memory requirements for this insert,\n+    // then try to acquire; no point in acquiring sort buffer only to spill due to no space in the\n+    // data page.\n+    if (!sorter.hasSpaceForAnotherRecord() && spillingEnabled) {\n+      final long oldSortBufferMemoryUsage = sorter.getMemoryUsage();\n+      final long memoryToGrowSortBuffer = oldSortBufferMemoryUsage * 2;\n+      final long memoryAcquired = shuffleMemoryManager.tryToAcquire(memoryToGrowSortBuffer);\n+      if (memoryAcquired < memoryToGrowSortBuffer) {\n+        shuffleMemoryManager.release(memoryAcquired);\n+        spill();\n+      } else {\n+        sorter.expandSortBuffer();\n+        shuffleMemoryManager.release(oldSortBufferMemoryUsage);\n+      }\n+    }\n+\n+    final long spaceInCurrentPage;\n+    if (currentPage != null) {\n+      spaceInCurrentPage = PAGE_SIZE - (currentPagePosition - currentPage.getBaseOffset());\n+    } else {\n+      spaceInCurrentPage = 0;\n+    }\n+    if (requiredSpace > PAGE_SIZE) {\n+      // TODO: throw a more specific exception?\n+      throw new IOException(\"Required space \" + requiredSpace + \" is greater than page size (\" +\n+        PAGE_SIZE + \")\");\n+    } else if (requiredSpace > spaceInCurrentPage) {\n+      if (spillingEnabled) {\n+        final long memoryAcquired = shuffleMemoryManager.tryToAcquire(PAGE_SIZE);\n+        if (memoryAcquired < PAGE_SIZE) {\n+          shuffleMemoryManager.release(memoryAcquired);\n+          spill();\n+          final long memoryAcquiredAfterSpill = shuffleMemoryManager.tryToAcquire(PAGE_SIZE);\n+          if (memoryAcquiredAfterSpill != PAGE_SIZE) {\n+            shuffleMemoryManager.release(memoryAcquiredAfterSpill);\n+            throw new IOException(\"Can't allocate memory!\");\n+          }\n+        }\n+      }\n+      currentPage = memoryManager.allocatePage(PAGE_SIZE);\n+      currentPagePosition = currentPage.getBaseOffset();\n+      allocatedPages.add(currentPage);\n+      logger.info(\"Acquired new page! \" + allocatedPages.size() * PAGE_SIZE);\n+    }\n+  }\n+\n+  /**\n+   * Write a record to the shuffle sorter.\n+   */\n+  public void insertRecord(\n+      Object recordBaseObject,\n+      long recordBaseOffset,\n+      int lengthInBytes,\n+      int partitionId) throws IOException {\n+    // Need 4 bytes to store the record length.\n+    ensureSpaceInDataPage(lengthInBytes + 4);\n+\n+    final long recordAddress =\n+      memoryManager.encodePageNumberAndOffset(currentPage, currentPagePosition);\n+    final Object dataPageBaseObject = currentPage.getBaseObject();\n+    PlatformDependent.UNSAFE.putInt(dataPageBaseObject, currentPagePosition, lengthInBytes);\n+    currentPagePosition += 4;\n+    PlatformDependent.copyMemory(\n+      recordBaseObject,\n+      recordBaseOffset,\n+      dataPageBaseObject,\n+      currentPagePosition,\n+      lengthInBytes);\n+    currentPagePosition += lengthInBytes;\n+\n+    sorter.insertRecord(recordAddress, partitionId);\n+  }\n+\n+  /**\n+   * Close the sorter, causing any buffered data to be sorted and written out to disk.\n+   *\n+   * @return metadata for the spill files written by this sorter. If no records were ever inserted\n+   *         into this sorter, then this will return an empty array.\n+   * @throws IOException\n+   */\n+  public SpillInfo[] closeAndGetSpills() throws IOException {\n+    if (sorter != null) {\n+      writeSpillFile();"
  }],
  "prId": 5868
}]