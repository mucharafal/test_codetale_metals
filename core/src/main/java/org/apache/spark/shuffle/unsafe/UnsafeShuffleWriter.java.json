[{
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "bytesRemain?\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-07T01:04:07Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.FileChannel;\n+import java.util.Iterator;\n+\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import scala.Option;\n+import scala.Product2;\n+import scala.collection.JavaConversions;\n+import scala.reflect.ClassTag;\n+import scala.reflect.ClassTag$;\n+\n+import com.esotericsoftware.kryo.io.ByteBufferOutputStream;\n+\n+import org.apache.spark.*;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.scheduler.MapStatus;\n+import org.apache.spark.scheduler.MapStatus$;\n+import org.apache.spark.serializer.SerializationStream;\n+import org.apache.spark.serializer.Serializer;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.IndexShuffleBlockManager;\n+import org.apache.spark.shuffle.ShuffleWriter;\n+import org.apache.spark.storage.BlockManager;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+// IntelliJ gets confused and claims that this class should be abstract, but this actually compiles\n+public class UnsafeShuffleWriter<K, V> implements ShuffleWriter<K, V> {\n+\n+  private static final int SER_BUFFER_SIZE = 1024 * 1024;  // TODO: tune this\n+  private static final ClassTag<Object> OBJECT_CLASS_TAG = ClassTag$.MODULE$.Object();\n+\n+  private final BlockManager blockManager;\n+  private final IndexShuffleBlockManager shuffleBlockManager;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final SerializerInstance serializer;\n+  private final Partitioner partitioner;\n+  private final ShuffleWriteMetrics writeMetrics;\n+  private final int shuffleId;\n+  private final int mapId;\n+  private final TaskContext taskContext;\n+  private final SparkConf sparkConf;\n+\n+  private MapStatus mapStatus = null;\n+\n+  /**\n+   * Are we in the process of stopping? Because map tasks can call stop() with success = true\n+   * and then call stop() with success = false if they get an exception, we want to make sure\n+   * we don't try deleting files, etc twice.\n+   */\n+  private boolean stopping = false;\n+\n+  public UnsafeShuffleWriter(\n+      BlockManager blockManager,\n+      IndexShuffleBlockManager shuffleBlockManager,\n+      TaskMemoryManager memoryManager,\n+      ShuffleMemoryManager shuffleMemoryManager,\n+      UnsafeShuffleHandle<K, V> handle,\n+      int mapId,\n+      TaskContext taskContext,\n+      SparkConf sparkConf) {\n+    this.blockManager = blockManager;\n+    this.shuffleBlockManager = shuffleBlockManager;\n+    this.memoryManager = memoryManager;\n+    this.shuffleMemoryManager = shuffleMemoryManager;\n+    this.mapId = mapId;\n+    final ShuffleDependency<K, V, V> dep = handle.dependency();\n+    this.shuffleId = dep.shuffleId();\n+    this.serializer = Serializer.getSerializer(dep.serializer()).newInstance();\n+    this.partitioner = dep.partitioner();\n+    this.writeMetrics = new ShuffleWriteMetrics();\n+    taskContext.taskMetrics().shuffleWriteMetrics_$eq(Option.apply(writeMetrics));\n+    this.taskContext = taskContext;\n+    this.sparkConf = sparkConf;\n+  }\n+\n+  public void write(Iterator<Product2<K, V>> records) {\n+    write(JavaConversions.asScalaIterator(records));\n+  }\n+\n+  public void write(scala.collection.Iterator<Product2<K, V>> records) {\n+    try {\n+      final long[] partitionLengths = mergeSpills(insertRecordsIntoSorter(records));\n+      shuffleBlockManager.writeIndexFile(shuffleId, mapId, partitionLengths);\n+      mapStatus = MapStatus$.MODULE$.apply(blockManager.shuffleServerId(), partitionLengths);\n+    } catch (Exception e) {\n+      PlatformDependent.throwException(e);\n+    }\n+  }\n+\n+  private void freeMemory() {\n+    // TODO\n+  }\n+\n+  private SpillInfo[] insertRecordsIntoSorter(\n+      scala.collection.Iterator<? extends Product2<K, V>> records) throws Exception {\n+    final UnsafeShuffleSpillWriter sorter = new UnsafeShuffleSpillWriter(\n+      memoryManager,\n+      shuffleMemoryManager,\n+      blockManager,\n+      taskContext,\n+      4096, // Initial size (TODO: tune this!)\n+      partitioner.numPartitions(),\n+      sparkConf\n+    );\n+\n+    final byte[] serArray = new byte[SER_BUFFER_SIZE];\n+    final ByteBuffer serByteBuffer = ByteBuffer.wrap(serArray);\n+    // TODO: we should not depend on this class from Kryo; copy its source or find an alternative\n+    final SerializationStream serOutputStream =\n+      serializer.serializeStream(new ByteBufferOutputStream(serByteBuffer));\n+\n+    while (records.hasNext()) {\n+      final Product2<K, V> record = records.next();\n+      final K key = record._1();\n+      final int partitionId = partitioner.getPartition(key);\n+      serByteBuffer.position(0);\n+      serOutputStream.writeKey(key, OBJECT_CLASS_TAG);\n+      serOutputStream.writeValue(record._2(), OBJECT_CLASS_TAG);\n+      serOutputStream.flush();\n+\n+      final int serializedRecordSize = serByteBuffer.position();\n+      assert (serializedRecordSize > 0);\n+\n+      sorter.insertRecord(\n+        serArray, PlatformDependent.BYTE_ARRAY_OFFSET, serializedRecordSize, partitionId);\n+    }\n+\n+    return sorter.closeAndGetSpills();\n+  }\n+\n+  private long[] mergeSpills(SpillInfo[] spills) throws IOException {\n+    final File outputFile = shuffleBlockManager.getDataFile(shuffleId, mapId);\n+    final int numPartitions = partitioner.numPartitions();\n+    final long[] partitionLengths = new long[numPartitions];\n+\n+    if (spills.length == 0) {\n+      new FileOutputStream(outputFile).close();\n+      return partitionLengths;\n+    }\n+\n+    final FileChannel[] spillInputChannels = new FileChannel[spills.length];\n+    final long[] spillInputChannelPositions = new long[spills.length];\n+\n+    // TODO: We need to add an option to bypass transferTo here since older Linux kernels are\n+    // affected by a bug here that can lead to data truncation; see the comments Utils.scala,\n+    // in the copyStream() method. I didn't use copyStream() here because we only want to copy\n+    // a limited number of bytes from the stream and I didn't want to modify / extend that method\n+    // to accept a length.\n+\n+    // TODO: special case optimization for case where we only write one file (non-spill case).\n+\n+    for (int i = 0; i < spills.length; i++) {\n+      spillInputChannels[i] = new FileInputStream(spills[i].file).getChannel();\n+    }\n+\n+    final FileChannel mergedFileOutputChannel = new FileOutputStream(outputFile).getChannel();\n+\n+    for (int partition = 0; partition < numPartitions; partition++) {\n+      for (int i = 0; i < spills.length; i++) {\n+        System.out.println(\"In partition \" + partition + \" and spill \" + i );\n+        final long partitionLengthInSpill = spills[i].partitionLengths[partition];\n+        System.out.println(\"Partition length in spill is \" + partitionLengthInSpill);\n+        System.out.println(\"input channel position is \" + spillInputChannels[i].position());\n+        long bytesRemainingToBeTransferred = partitionLengthInSpill;"
  }],
  "prId": 5868
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "Do you have any hints for the initial sizing of our sort buffers?\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-07T01:13:24Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.FileChannel;\n+import java.util.Iterator;\n+\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import scala.Option;\n+import scala.Product2;\n+import scala.collection.JavaConversions;\n+import scala.reflect.ClassTag;\n+import scala.reflect.ClassTag$;\n+\n+import com.esotericsoftware.kryo.io.ByteBufferOutputStream;\n+\n+import org.apache.spark.*;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.scheduler.MapStatus;\n+import org.apache.spark.scheduler.MapStatus$;\n+import org.apache.spark.serializer.SerializationStream;\n+import org.apache.spark.serializer.Serializer;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.IndexShuffleBlockManager;\n+import org.apache.spark.shuffle.ShuffleWriter;\n+import org.apache.spark.storage.BlockManager;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+// IntelliJ gets confused and claims that this class should be abstract, but this actually compiles\n+public class UnsafeShuffleWriter<K, V> implements ShuffleWriter<K, V> {\n+\n+  private static final int SER_BUFFER_SIZE = 1024 * 1024;  // TODO: tune this\n+  private static final ClassTag<Object> OBJECT_CLASS_TAG = ClassTag$.MODULE$.Object();\n+\n+  private final BlockManager blockManager;\n+  private final IndexShuffleBlockManager shuffleBlockManager;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final SerializerInstance serializer;\n+  private final Partitioner partitioner;\n+  private final ShuffleWriteMetrics writeMetrics;\n+  private final int shuffleId;\n+  private final int mapId;\n+  private final TaskContext taskContext;\n+  private final SparkConf sparkConf;\n+\n+  private MapStatus mapStatus = null;\n+\n+  /**\n+   * Are we in the process of stopping? Because map tasks can call stop() with success = true\n+   * and then call stop() with success = false if they get an exception, we want to make sure\n+   * we don't try deleting files, etc twice.\n+   */\n+  private boolean stopping = false;\n+\n+  public UnsafeShuffleWriter(\n+      BlockManager blockManager,\n+      IndexShuffleBlockManager shuffleBlockManager,\n+      TaskMemoryManager memoryManager,\n+      ShuffleMemoryManager shuffleMemoryManager,\n+      UnsafeShuffleHandle<K, V> handle,\n+      int mapId,\n+      TaskContext taskContext,\n+      SparkConf sparkConf) {\n+    this.blockManager = blockManager;\n+    this.shuffleBlockManager = shuffleBlockManager;\n+    this.memoryManager = memoryManager;\n+    this.shuffleMemoryManager = shuffleMemoryManager;\n+    this.mapId = mapId;\n+    final ShuffleDependency<K, V, V> dep = handle.dependency();\n+    this.shuffleId = dep.shuffleId();\n+    this.serializer = Serializer.getSerializer(dep.serializer()).newInstance();\n+    this.partitioner = dep.partitioner();\n+    this.writeMetrics = new ShuffleWriteMetrics();\n+    taskContext.taskMetrics().shuffleWriteMetrics_$eq(Option.apply(writeMetrics));\n+    this.taskContext = taskContext;\n+    this.sparkConf = sparkConf;\n+  }\n+\n+  public void write(Iterator<Product2<K, V>> records) {\n+    write(JavaConversions.asScalaIterator(records));\n+  }\n+\n+  public void write(scala.collection.Iterator<Product2<K, V>> records) {\n+    try {\n+      final long[] partitionLengths = mergeSpills(insertRecordsIntoSorter(records));\n+      shuffleBlockManager.writeIndexFile(shuffleId, mapId, partitionLengths);\n+      mapStatus = MapStatus$.MODULE$.apply(blockManager.shuffleServerId(), partitionLengths);\n+    } catch (Exception e) {\n+      PlatformDependent.throwException(e);\n+    }\n+  }\n+\n+  private void freeMemory() {\n+    // TODO\n+  }\n+\n+  private SpillInfo[] insertRecordsIntoSorter(\n+      scala.collection.Iterator<? extends Product2<K, V>> records) throws Exception {\n+    final UnsafeShuffleSpillWriter sorter = new UnsafeShuffleSpillWriter(\n+      memoryManager,\n+      shuffleMemoryManager,\n+      blockManager,\n+      taskContext,\n+      4096, // Initial size (TODO: tune this!)"
  }],
  "prId": 5868
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "nit: becuase -> because\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-12T00:04:49Z",
    "diffHunk": "@@ -0,0 +1,382 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.*;\n+import java.nio.channels.FileChannel;\n+import java.util.Iterator;\n+import javax.annotation.Nullable;\n+\n+import scala.Option;\n+import scala.Product2;\n+import scala.collection.JavaConversions;\n+import scala.reflect.ClassTag;\n+import scala.reflect.ClassTag$;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.io.ByteStreams;\n+import com.google.common.io.Closeables;\n+import com.google.common.io.Files;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.*;\n+import org.apache.spark.io.CompressionCodec;\n+import org.apache.spark.io.CompressionCodec$;\n+import org.apache.spark.io.LZFCompressionCodec;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.network.util.LimitedInputStream;\n+import org.apache.spark.scheduler.MapStatus;\n+import org.apache.spark.scheduler.MapStatus$;\n+import org.apache.spark.serializer.SerializationStream;\n+import org.apache.spark.serializer.Serializer;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.IndexShuffleBlockResolver;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.shuffle.ShuffleWriter;\n+import org.apache.spark.storage.BlockManager;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+public class UnsafeShuffleWriter<K, V> extends ShuffleWriter<K, V> {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleWriter.class);\n+\n+  private static final ClassTag<Object> OBJECT_CLASS_TAG = ClassTag$.MODULE$.Object();\n+\n+  private final BlockManager blockManager;\n+  private final IndexShuffleBlockResolver shuffleBlockResolver;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final SerializerInstance serializer;\n+  private final Partitioner partitioner;\n+  private final ShuffleWriteMetrics writeMetrics;\n+  private final int shuffleId;\n+  private final int mapId;\n+  private final TaskContext taskContext;\n+  private final SparkConf sparkConf;\n+  private final boolean transferToEnabled;\n+\n+  private MapStatus mapStatus = null;\n+  private UnsafeShuffleExternalSorter sorter = null;\n+\n+  /** Subclass of ByteArrayOutputStream that exposes `buf` directly. */\n+  private static final class MyByteArrayOutputStream extends ByteArrayOutputStream {\n+    public MyByteArrayOutputStream(int size) { super(size); }\n+    public byte[] getBuf() { return buf; }\n+  }\n+\n+  private MyByteArrayOutputStream serBuffer;\n+  private SerializationStream serOutputStream;\n+\n+  /**\n+   * Are we in the process of stopping? Because map tasks can call stop() with success = true\n+   * and then call stop() with success = false if they get an exception, we want to make sure\n+   * we don't try deleting files, etc twice.\n+   */\n+  private boolean stopping = false;\n+\n+  public UnsafeShuffleWriter(\n+      BlockManager blockManager,\n+      IndexShuffleBlockResolver shuffleBlockResolver,\n+      TaskMemoryManager memoryManager,\n+      ShuffleMemoryManager shuffleMemoryManager,\n+      UnsafeShuffleHandle<K, V> handle,\n+      int mapId,\n+      TaskContext taskContext,\n+      SparkConf sparkConf) {\n+    final int numPartitions = handle.dependency().partitioner().numPartitions();\n+    if (numPartitions > PackedRecordPointer.MAXIMUM_PARTITION_ID) {\n+      throw new IllegalArgumentException(\n+        \"UnsafeShuffleWriter can only be used for shuffles with at most \" +\n+          PackedRecordPointer.MAXIMUM_PARTITION_ID + \" reduce partitions\");\n+    }\n+    this.blockManager = blockManager;\n+    this.shuffleBlockResolver = shuffleBlockResolver;\n+    this.memoryManager = memoryManager;\n+    this.shuffleMemoryManager = shuffleMemoryManager;\n+    this.mapId = mapId;\n+    final ShuffleDependency<K, V, V> dep = handle.dependency();\n+    this.shuffleId = dep.shuffleId();\n+    this.serializer = Serializer.getSerializer(dep.serializer()).newInstance();\n+    this.partitioner = dep.partitioner();\n+    this.writeMetrics = new ShuffleWriteMetrics();\n+    taskContext.taskMetrics().shuffleWriteMetrics_$eq(Option.apply(writeMetrics));\n+    this.taskContext = taskContext;\n+    this.sparkConf = sparkConf;\n+    this.transferToEnabled = sparkConf.getBoolean(\"spark.file.transferTo\", true);\n+  }\n+\n+  public void write(Iterator<Product2<K, V>> records) throws IOException {\n+    write(JavaConversions.asScalaIterator(records));\n+  }\n+\n+  @Override\n+  public void write(scala.collection.Iterator<Product2<K, V>> records) throws IOException {\n+    try {\n+      while (records.hasNext()) {\n+        insertRecordIntoSorter(records.next());\n+      }\n+      closeAndWriteOutput();\n+    } catch (Exception e) {\n+      // Unfortunately, we have to catch Exception here in order to ensure proper cleanup after\n+      // errors becuase Spark's Scala code, or users' custom Serializers, might throw arbitrary"
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "Haha, this had already been fixed by the time you commented on it :smile: \n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-12T00:05:36Z",
    "diffHunk": "@@ -0,0 +1,382 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.*;\n+import java.nio.channels.FileChannel;\n+import java.util.Iterator;\n+import javax.annotation.Nullable;\n+\n+import scala.Option;\n+import scala.Product2;\n+import scala.collection.JavaConversions;\n+import scala.reflect.ClassTag;\n+import scala.reflect.ClassTag$;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.io.ByteStreams;\n+import com.google.common.io.Closeables;\n+import com.google.common.io.Files;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.*;\n+import org.apache.spark.io.CompressionCodec;\n+import org.apache.spark.io.CompressionCodec$;\n+import org.apache.spark.io.LZFCompressionCodec;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.network.util.LimitedInputStream;\n+import org.apache.spark.scheduler.MapStatus;\n+import org.apache.spark.scheduler.MapStatus$;\n+import org.apache.spark.serializer.SerializationStream;\n+import org.apache.spark.serializer.Serializer;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.IndexShuffleBlockResolver;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.shuffle.ShuffleWriter;\n+import org.apache.spark.storage.BlockManager;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+public class UnsafeShuffleWriter<K, V> extends ShuffleWriter<K, V> {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleWriter.class);\n+\n+  private static final ClassTag<Object> OBJECT_CLASS_TAG = ClassTag$.MODULE$.Object();\n+\n+  private final BlockManager blockManager;\n+  private final IndexShuffleBlockResolver shuffleBlockResolver;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final SerializerInstance serializer;\n+  private final Partitioner partitioner;\n+  private final ShuffleWriteMetrics writeMetrics;\n+  private final int shuffleId;\n+  private final int mapId;\n+  private final TaskContext taskContext;\n+  private final SparkConf sparkConf;\n+  private final boolean transferToEnabled;\n+\n+  private MapStatus mapStatus = null;\n+  private UnsafeShuffleExternalSorter sorter = null;\n+\n+  /** Subclass of ByteArrayOutputStream that exposes `buf` directly. */\n+  private static final class MyByteArrayOutputStream extends ByteArrayOutputStream {\n+    public MyByteArrayOutputStream(int size) { super(size); }\n+    public byte[] getBuf() { return buf; }\n+  }\n+\n+  private MyByteArrayOutputStream serBuffer;\n+  private SerializationStream serOutputStream;\n+\n+  /**\n+   * Are we in the process of stopping? Because map tasks can call stop() with success = true\n+   * and then call stop() with success = false if they get an exception, we want to make sure\n+   * we don't try deleting files, etc twice.\n+   */\n+  private boolean stopping = false;\n+\n+  public UnsafeShuffleWriter(\n+      BlockManager blockManager,\n+      IndexShuffleBlockResolver shuffleBlockResolver,\n+      TaskMemoryManager memoryManager,\n+      ShuffleMemoryManager shuffleMemoryManager,\n+      UnsafeShuffleHandle<K, V> handle,\n+      int mapId,\n+      TaskContext taskContext,\n+      SparkConf sparkConf) {\n+    final int numPartitions = handle.dependency().partitioner().numPartitions();\n+    if (numPartitions > PackedRecordPointer.MAXIMUM_PARTITION_ID) {\n+      throw new IllegalArgumentException(\n+        \"UnsafeShuffleWriter can only be used for shuffles with at most \" +\n+          PackedRecordPointer.MAXIMUM_PARTITION_ID + \" reduce partitions\");\n+    }\n+    this.blockManager = blockManager;\n+    this.shuffleBlockResolver = shuffleBlockResolver;\n+    this.memoryManager = memoryManager;\n+    this.shuffleMemoryManager = shuffleMemoryManager;\n+    this.mapId = mapId;\n+    final ShuffleDependency<K, V, V> dep = handle.dependency();\n+    this.shuffleId = dep.shuffleId();\n+    this.serializer = Serializer.getSerializer(dep.serializer()).newInstance();\n+    this.partitioner = dep.partitioner();\n+    this.writeMetrics = new ShuffleWriteMetrics();\n+    taskContext.taskMetrics().shuffleWriteMetrics_$eq(Option.apply(writeMetrics));\n+    this.taskContext = taskContext;\n+    this.sparkConf = sparkConf;\n+    this.transferToEnabled = sparkConf.getBoolean(\"spark.file.transferTo\", true);\n+  }\n+\n+  public void write(Iterator<Product2<K, V>> records) throws IOException {\n+    write(JavaConversions.asScalaIterator(records));\n+  }\n+\n+  @Override\n+  public void write(scala.collection.Iterator<Product2<K, V>> records) throws IOException {\n+    try {\n+      while (records.hasNext()) {\n+        insertRecordIntoSorter(records.next());\n+      }\n+      closeAndWriteOutput();\n+    } catch (Exception e) {\n+      // Unfortunately, we have to catch Exception here in order to ensure proper cleanup after\n+      // errors becuase Spark's Scala code, or users' custom Serializers, might throw arbitrary"
  }],
  "prId": 5868
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "Here should be updated to MAXIMUM_PARTITION_ID + 1\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-12T05:51:33Z",
    "diffHunk": "@@ -0,0 +1,407 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.*;\n+import java.nio.channels.FileChannel;\n+import java.util.Iterator;\n+import javax.annotation.Nullable;\n+\n+import scala.Option;\n+import scala.Product2;\n+import scala.collection.JavaConversions;\n+import scala.reflect.ClassTag;\n+import scala.reflect.ClassTag$;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.io.ByteStreams;\n+import com.google.common.io.Closeables;\n+import com.google.common.io.Files;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.*;\n+import org.apache.spark.io.CompressionCodec;\n+import org.apache.spark.io.CompressionCodec$;\n+import org.apache.spark.io.LZFCompressionCodec;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.network.util.LimitedInputStream;\n+import org.apache.spark.scheduler.MapStatus;\n+import org.apache.spark.scheduler.MapStatus$;\n+import org.apache.spark.serializer.SerializationStream;\n+import org.apache.spark.serializer.Serializer;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.IndexShuffleBlockResolver;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.shuffle.ShuffleWriter;\n+import org.apache.spark.storage.BlockManager;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+public class UnsafeShuffleWriter<K, V> extends ShuffleWriter<K, V> {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleWriter.class);\n+\n+  private static final ClassTag<Object> OBJECT_CLASS_TAG = ClassTag$.MODULE$.Object();\n+\n+  @VisibleForTesting\n+  static final int INITIAL_SORT_BUFFER_SIZE = 4096;\n+\n+  private final BlockManager blockManager;\n+  private final IndexShuffleBlockResolver shuffleBlockResolver;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final SerializerInstance serializer;\n+  private final Partitioner partitioner;\n+  private final ShuffleWriteMetrics writeMetrics;\n+  private final int shuffleId;\n+  private final int mapId;\n+  private final TaskContext taskContext;\n+  private final SparkConf sparkConf;\n+  private final boolean transferToEnabled;\n+\n+  private MapStatus mapStatus = null;\n+  private UnsafeShuffleExternalSorter sorter = null;\n+\n+  /** Subclass of ByteArrayOutputStream that exposes `buf` directly. */\n+  private static final class MyByteArrayOutputStream extends ByteArrayOutputStream {\n+    public MyByteArrayOutputStream(int size) { super(size); }\n+    public byte[] getBuf() { return buf; }\n+  }\n+\n+  private MyByteArrayOutputStream serBuffer;\n+  private SerializationStream serOutputStream;\n+\n+  /**\n+   * Are we in the process of stopping? Because map tasks can call stop() with success = true\n+   * and then call stop() with success = false if they get an exception, we want to make sure\n+   * we don't try deleting files, etc twice.\n+   */\n+  private boolean stopping = false;\n+\n+  public UnsafeShuffleWriter(\n+      BlockManager blockManager,\n+      IndexShuffleBlockResolver shuffleBlockResolver,\n+      TaskMemoryManager memoryManager,\n+      ShuffleMemoryManager shuffleMemoryManager,\n+      UnsafeShuffleHandle<K, V> handle,\n+      int mapId,\n+      TaskContext taskContext,\n+      SparkConf sparkConf) {\n+    final int numPartitions = handle.dependency().partitioner().numPartitions();\n+    if (numPartitions > PackedRecordPointer.MAXIMUM_PARTITION_ID) {"
  }],
  "prId": 5868
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "Here should be updated to MAXIMUM_PARTITION_ID + 1\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-12T05:51:37Z",
    "diffHunk": "@@ -0,0 +1,407 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.*;\n+import java.nio.channels.FileChannel;\n+import java.util.Iterator;\n+import javax.annotation.Nullable;\n+\n+import scala.Option;\n+import scala.Product2;\n+import scala.collection.JavaConversions;\n+import scala.reflect.ClassTag;\n+import scala.reflect.ClassTag$;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.io.ByteStreams;\n+import com.google.common.io.Closeables;\n+import com.google.common.io.Files;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.*;\n+import org.apache.spark.io.CompressionCodec;\n+import org.apache.spark.io.CompressionCodec$;\n+import org.apache.spark.io.LZFCompressionCodec;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.network.util.LimitedInputStream;\n+import org.apache.spark.scheduler.MapStatus;\n+import org.apache.spark.scheduler.MapStatus$;\n+import org.apache.spark.serializer.SerializationStream;\n+import org.apache.spark.serializer.Serializer;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.IndexShuffleBlockResolver;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.shuffle.ShuffleWriter;\n+import org.apache.spark.storage.BlockManager;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+public class UnsafeShuffleWriter<K, V> extends ShuffleWriter<K, V> {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleWriter.class);\n+\n+  private static final ClassTag<Object> OBJECT_CLASS_TAG = ClassTag$.MODULE$.Object();\n+\n+  @VisibleForTesting\n+  static final int INITIAL_SORT_BUFFER_SIZE = 4096;\n+\n+  private final BlockManager blockManager;\n+  private final IndexShuffleBlockResolver shuffleBlockResolver;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final SerializerInstance serializer;\n+  private final Partitioner partitioner;\n+  private final ShuffleWriteMetrics writeMetrics;\n+  private final int shuffleId;\n+  private final int mapId;\n+  private final TaskContext taskContext;\n+  private final SparkConf sparkConf;\n+  private final boolean transferToEnabled;\n+\n+  private MapStatus mapStatus = null;\n+  private UnsafeShuffleExternalSorter sorter = null;\n+\n+  /** Subclass of ByteArrayOutputStream that exposes `buf` directly. */\n+  private static final class MyByteArrayOutputStream extends ByteArrayOutputStream {\n+    public MyByteArrayOutputStream(int size) { super(size); }\n+    public byte[] getBuf() { return buf; }\n+  }\n+\n+  private MyByteArrayOutputStream serBuffer;\n+  private SerializationStream serOutputStream;\n+\n+  /**\n+   * Are we in the process of stopping? Because map tasks can call stop() with success = true\n+   * and then call stop() with success = false if they get an exception, we want to make sure\n+   * we don't try deleting files, etc twice.\n+   */\n+  private boolean stopping = false;\n+\n+  public UnsafeShuffleWriter(\n+      BlockManager blockManager,\n+      IndexShuffleBlockResolver shuffleBlockResolver,\n+      TaskMemoryManager memoryManager,\n+      ShuffleMemoryManager shuffleMemoryManager,\n+      UnsafeShuffleHandle<K, V> handle,\n+      int mapId,\n+      TaskContext taskContext,\n+      SparkConf sparkConf) {\n+    final int numPartitions = handle.dependency().partitioner().numPartitions();\n+    if (numPartitions > PackedRecordPointer.MAXIMUM_PARTITION_ID) {\n+      throw new IllegalArgumentException(\n+        \"UnsafeShuffleWriter can only be used for shuffles with at most \" +\n+          PackedRecordPointer.MAXIMUM_PARTITION_ID + \" reduce partitions\");"
  }],
  "prId": 5868
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "is this only used in testing? maybe document that.\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-12T07:28:28Z",
    "diffHunk": "@@ -0,0 +1,407 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.*;\n+import java.nio.channels.FileChannel;\n+import java.util.Iterator;\n+import javax.annotation.Nullable;\n+\n+import scala.Option;\n+import scala.Product2;\n+import scala.collection.JavaConversions;\n+import scala.reflect.ClassTag;\n+import scala.reflect.ClassTag$;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.io.ByteStreams;\n+import com.google.common.io.Closeables;\n+import com.google.common.io.Files;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.*;\n+import org.apache.spark.io.CompressionCodec;\n+import org.apache.spark.io.CompressionCodec$;\n+import org.apache.spark.io.LZFCompressionCodec;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.network.util.LimitedInputStream;\n+import org.apache.spark.scheduler.MapStatus;\n+import org.apache.spark.scheduler.MapStatus$;\n+import org.apache.spark.serializer.SerializationStream;\n+import org.apache.spark.serializer.Serializer;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.IndexShuffleBlockResolver;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.shuffle.ShuffleWriter;\n+import org.apache.spark.storage.BlockManager;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+public class UnsafeShuffleWriter<K, V> extends ShuffleWriter<K, V> {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleWriter.class);\n+\n+  private static final ClassTag<Object> OBJECT_CLASS_TAG = ClassTag$.MODULE$.Object();\n+\n+  @VisibleForTesting\n+  static final int INITIAL_SORT_BUFFER_SIZE = 4096;\n+\n+  private final BlockManager blockManager;\n+  private final IndexShuffleBlockResolver shuffleBlockResolver;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final SerializerInstance serializer;\n+  private final Partitioner partitioner;\n+  private final ShuffleWriteMetrics writeMetrics;\n+  private final int shuffleId;\n+  private final int mapId;\n+  private final TaskContext taskContext;\n+  private final SparkConf sparkConf;\n+  private final boolean transferToEnabled;\n+\n+  private MapStatus mapStatus = null;\n+  private UnsafeShuffleExternalSorter sorter = null;\n+\n+  /** Subclass of ByteArrayOutputStream that exposes `buf` directly. */\n+  private static final class MyByteArrayOutputStream extends ByteArrayOutputStream {\n+    public MyByteArrayOutputStream(int size) { super(size); }\n+    public byte[] getBuf() { return buf; }\n+  }\n+\n+  private MyByteArrayOutputStream serBuffer;\n+  private SerializationStream serOutputStream;\n+\n+  /**\n+   * Are we in the process of stopping? Because map tasks can call stop() with success = true\n+   * and then call stop() with success = false if they get an exception, we want to make sure\n+   * we don't try deleting files, etc twice.\n+   */\n+  private boolean stopping = false;\n+\n+  public UnsafeShuffleWriter(\n+      BlockManager blockManager,\n+      IndexShuffleBlockResolver shuffleBlockResolver,\n+      TaskMemoryManager memoryManager,\n+      ShuffleMemoryManager shuffleMemoryManager,\n+      UnsafeShuffleHandle<K, V> handle,\n+      int mapId,\n+      TaskContext taskContext,\n+      SparkConf sparkConf) {\n+    final int numPartitions = handle.dependency().partitioner().numPartitions();\n+    if (numPartitions > PackedRecordPointer.MAXIMUM_PARTITION_ID) {\n+      throw new IllegalArgumentException(\n+        \"UnsafeShuffleWriter can only be used for shuffles with at most \" +\n+          PackedRecordPointer.MAXIMUM_PARTITION_ID + \" reduce partitions\");\n+    }\n+    this.blockManager = blockManager;\n+    this.shuffleBlockResolver = shuffleBlockResolver;\n+    this.memoryManager = memoryManager;\n+    this.shuffleMemoryManager = shuffleMemoryManager;\n+    this.mapId = mapId;\n+    final ShuffleDependency<K, V, V> dep = handle.dependency();\n+    this.shuffleId = dep.shuffleId();\n+    this.serializer = Serializer.getSerializer(dep.serializer()).newInstance();\n+    this.partitioner = dep.partitioner();\n+    this.writeMetrics = new ShuffleWriteMetrics();\n+    taskContext.taskMetrics().shuffleWriteMetrics_$eq(Option.apply(writeMetrics));\n+    this.taskContext = taskContext;\n+    this.sparkConf = sparkConf;\n+    this.transferToEnabled = sparkConf.getBoolean(\"spark.file.transferTo\", true);\n+  }\n+\n+  public void write(Iterator<Product2<K, V>> records) throws IOException {"
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "Yep, only for testing.  I could remove this method and sprinkle JavaConversions calls throughout the test suite, but that seemed messy.  I think that there might be an \"only for testing\" annotation that you can use in cases like this which enables certain IDE inspections / errors if you ever call these methods outside of test code.\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-12T18:11:11Z",
    "diffHunk": "@@ -0,0 +1,407 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.*;\n+import java.nio.channels.FileChannel;\n+import java.util.Iterator;\n+import javax.annotation.Nullable;\n+\n+import scala.Option;\n+import scala.Product2;\n+import scala.collection.JavaConversions;\n+import scala.reflect.ClassTag;\n+import scala.reflect.ClassTag$;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.io.ByteStreams;\n+import com.google.common.io.Closeables;\n+import com.google.common.io.Files;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.*;\n+import org.apache.spark.io.CompressionCodec;\n+import org.apache.spark.io.CompressionCodec$;\n+import org.apache.spark.io.LZFCompressionCodec;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.network.util.LimitedInputStream;\n+import org.apache.spark.scheduler.MapStatus;\n+import org.apache.spark.scheduler.MapStatus$;\n+import org.apache.spark.serializer.SerializationStream;\n+import org.apache.spark.serializer.Serializer;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.IndexShuffleBlockResolver;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.shuffle.ShuffleWriter;\n+import org.apache.spark.storage.BlockManager;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+public class UnsafeShuffleWriter<K, V> extends ShuffleWriter<K, V> {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleWriter.class);\n+\n+  private static final ClassTag<Object> OBJECT_CLASS_TAG = ClassTag$.MODULE$.Object();\n+\n+  @VisibleForTesting\n+  static final int INITIAL_SORT_BUFFER_SIZE = 4096;\n+\n+  private final BlockManager blockManager;\n+  private final IndexShuffleBlockResolver shuffleBlockResolver;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final SerializerInstance serializer;\n+  private final Partitioner partitioner;\n+  private final ShuffleWriteMetrics writeMetrics;\n+  private final int shuffleId;\n+  private final int mapId;\n+  private final TaskContext taskContext;\n+  private final SparkConf sparkConf;\n+  private final boolean transferToEnabled;\n+\n+  private MapStatus mapStatus = null;\n+  private UnsafeShuffleExternalSorter sorter = null;\n+\n+  /** Subclass of ByteArrayOutputStream that exposes `buf` directly. */\n+  private static final class MyByteArrayOutputStream extends ByteArrayOutputStream {\n+    public MyByteArrayOutputStream(int size) { super(size); }\n+    public byte[] getBuf() { return buf; }\n+  }\n+\n+  private MyByteArrayOutputStream serBuffer;\n+  private SerializationStream serOutputStream;\n+\n+  /**\n+   * Are we in the process of stopping? Because map tasks can call stop() with success = true\n+   * and then call stop() with success = false if they get an exception, we want to make sure\n+   * we don't try deleting files, etc twice.\n+   */\n+  private boolean stopping = false;\n+\n+  public UnsafeShuffleWriter(\n+      BlockManager blockManager,\n+      IndexShuffleBlockResolver shuffleBlockResolver,\n+      TaskMemoryManager memoryManager,\n+      ShuffleMemoryManager shuffleMemoryManager,\n+      UnsafeShuffleHandle<K, V> handle,\n+      int mapId,\n+      TaskContext taskContext,\n+      SparkConf sparkConf) {\n+    final int numPartitions = handle.dependency().partitioner().numPartitions();\n+    if (numPartitions > PackedRecordPointer.MAXIMUM_PARTITION_ID) {\n+      throw new IllegalArgumentException(\n+        \"UnsafeShuffleWriter can only be used for shuffles with at most \" +\n+          PackedRecordPointer.MAXIMUM_PARTITION_ID + \" reduce partitions\");\n+    }\n+    this.blockManager = blockManager;\n+    this.shuffleBlockResolver = shuffleBlockResolver;\n+    this.memoryManager = memoryManager;\n+    this.shuffleMemoryManager = shuffleMemoryManager;\n+    this.mapId = mapId;\n+    final ShuffleDependency<K, V, V> dep = handle.dependency();\n+    this.shuffleId = dep.shuffleId();\n+    this.serializer = Serializer.getSerializer(dep.serializer()).newInstance();\n+    this.partitioner = dep.partitioner();\n+    this.writeMetrics = new ShuffleWriteMetrics();\n+    taskContext.taskMetrics().shuffleWriteMetrics_$eq(Option.apply(writeMetrics));\n+    this.taskContext = taskContext;\n+    this.sparkConf = sparkConf;\n+    this.transferToEnabled = sparkConf.getBoolean(\"spark.file.transferTo\", true);\n+  }\n+\n+  public void write(Iterator<Product2<K, V>> records) throws IOException {"
  }],
  "prId": 5868
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "do u want to catch throwable or exception?\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-12T07:28:51Z",
    "diffHunk": "@@ -0,0 +1,407 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.*;\n+import java.nio.channels.FileChannel;\n+import java.util.Iterator;\n+import javax.annotation.Nullable;\n+\n+import scala.Option;\n+import scala.Product2;\n+import scala.collection.JavaConversions;\n+import scala.reflect.ClassTag;\n+import scala.reflect.ClassTag$;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.io.ByteStreams;\n+import com.google.common.io.Closeables;\n+import com.google.common.io.Files;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.*;\n+import org.apache.spark.io.CompressionCodec;\n+import org.apache.spark.io.CompressionCodec$;\n+import org.apache.spark.io.LZFCompressionCodec;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.network.util.LimitedInputStream;\n+import org.apache.spark.scheduler.MapStatus;\n+import org.apache.spark.scheduler.MapStatus$;\n+import org.apache.spark.serializer.SerializationStream;\n+import org.apache.spark.serializer.Serializer;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.IndexShuffleBlockResolver;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.shuffle.ShuffleWriter;\n+import org.apache.spark.storage.BlockManager;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+public class UnsafeShuffleWriter<K, V> extends ShuffleWriter<K, V> {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleWriter.class);\n+\n+  private static final ClassTag<Object> OBJECT_CLASS_TAG = ClassTag$.MODULE$.Object();\n+\n+  @VisibleForTesting\n+  static final int INITIAL_SORT_BUFFER_SIZE = 4096;\n+\n+  private final BlockManager blockManager;\n+  private final IndexShuffleBlockResolver shuffleBlockResolver;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final SerializerInstance serializer;\n+  private final Partitioner partitioner;\n+  private final ShuffleWriteMetrics writeMetrics;\n+  private final int shuffleId;\n+  private final int mapId;\n+  private final TaskContext taskContext;\n+  private final SparkConf sparkConf;\n+  private final boolean transferToEnabled;\n+\n+  private MapStatus mapStatus = null;\n+  private UnsafeShuffleExternalSorter sorter = null;\n+\n+  /** Subclass of ByteArrayOutputStream that exposes `buf` directly. */\n+  private static final class MyByteArrayOutputStream extends ByteArrayOutputStream {\n+    public MyByteArrayOutputStream(int size) { super(size); }\n+    public byte[] getBuf() { return buf; }\n+  }\n+\n+  private MyByteArrayOutputStream serBuffer;\n+  private SerializationStream serOutputStream;\n+\n+  /**\n+   * Are we in the process of stopping? Because map tasks can call stop() with success = true\n+   * and then call stop() with success = false if they get an exception, we want to make sure\n+   * we don't try deleting files, etc twice.\n+   */\n+  private boolean stopping = false;\n+\n+  public UnsafeShuffleWriter(\n+      BlockManager blockManager,\n+      IndexShuffleBlockResolver shuffleBlockResolver,\n+      TaskMemoryManager memoryManager,\n+      ShuffleMemoryManager shuffleMemoryManager,\n+      UnsafeShuffleHandle<K, V> handle,\n+      int mapId,\n+      TaskContext taskContext,\n+      SparkConf sparkConf) {\n+    final int numPartitions = handle.dependency().partitioner().numPartitions();\n+    if (numPartitions > PackedRecordPointer.MAXIMUM_PARTITION_ID) {\n+      throw new IllegalArgumentException(\n+        \"UnsafeShuffleWriter can only be used for shuffles with at most \" +\n+          PackedRecordPointer.MAXIMUM_PARTITION_ID + \" reduce partitions\");\n+    }\n+    this.blockManager = blockManager;\n+    this.shuffleBlockResolver = shuffleBlockResolver;\n+    this.memoryManager = memoryManager;\n+    this.shuffleMemoryManager = shuffleMemoryManager;\n+    this.mapId = mapId;\n+    final ShuffleDependency<K, V, V> dep = handle.dependency();\n+    this.shuffleId = dep.shuffleId();\n+    this.serializer = Serializer.getSerializer(dep.serializer()).newInstance();\n+    this.partitioner = dep.partitioner();\n+    this.writeMetrics = new ShuffleWriteMetrics();\n+    taskContext.taskMetrics().shuffleWriteMetrics_$eq(Option.apply(writeMetrics));\n+    this.taskContext = taskContext;\n+    this.sparkConf = sparkConf;\n+    this.transferToEnabled = sparkConf.getBoolean(\"spark.file.transferTo\", true);\n+  }\n+\n+  public void write(Iterator<Product2<K, V>> records) throws IOException {\n+    write(JavaConversions.asScalaIterator(records));\n+  }\n+\n+  @Override\n+  public void write(scala.collection.Iterator<Product2<K, V>> records) throws IOException {\n+    try {\n+      while (records.hasNext()) {\n+        insertRecordIntoSorter(records.next());\n+      }\n+      closeAndWriteOutput();\n+    } catch (Exception e) {"
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "Catching `Throwable` is risky if we can't re-throw it, since that might cause us to drop an `OutOfMemoryError`.  Instead, I think that we can use a `finally` block that checks a boolean flag to see whether cleanup needs to be performed.  This avoids the bad practice in the current code of throwing from a finally block.\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-12T21:04:29Z",
    "diffHunk": "@@ -0,0 +1,407 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.*;\n+import java.nio.channels.FileChannel;\n+import java.util.Iterator;\n+import javax.annotation.Nullable;\n+\n+import scala.Option;\n+import scala.Product2;\n+import scala.collection.JavaConversions;\n+import scala.reflect.ClassTag;\n+import scala.reflect.ClassTag$;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.io.ByteStreams;\n+import com.google.common.io.Closeables;\n+import com.google.common.io.Files;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.*;\n+import org.apache.spark.io.CompressionCodec;\n+import org.apache.spark.io.CompressionCodec$;\n+import org.apache.spark.io.LZFCompressionCodec;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.network.util.LimitedInputStream;\n+import org.apache.spark.scheduler.MapStatus;\n+import org.apache.spark.scheduler.MapStatus$;\n+import org.apache.spark.serializer.SerializationStream;\n+import org.apache.spark.serializer.Serializer;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.IndexShuffleBlockResolver;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.shuffle.ShuffleWriter;\n+import org.apache.spark.storage.BlockManager;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+public class UnsafeShuffleWriter<K, V> extends ShuffleWriter<K, V> {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleWriter.class);\n+\n+  private static final ClassTag<Object> OBJECT_CLASS_TAG = ClassTag$.MODULE$.Object();\n+\n+  @VisibleForTesting\n+  static final int INITIAL_SORT_BUFFER_SIZE = 4096;\n+\n+  private final BlockManager blockManager;\n+  private final IndexShuffleBlockResolver shuffleBlockResolver;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final SerializerInstance serializer;\n+  private final Partitioner partitioner;\n+  private final ShuffleWriteMetrics writeMetrics;\n+  private final int shuffleId;\n+  private final int mapId;\n+  private final TaskContext taskContext;\n+  private final SparkConf sparkConf;\n+  private final boolean transferToEnabled;\n+\n+  private MapStatus mapStatus = null;\n+  private UnsafeShuffleExternalSorter sorter = null;\n+\n+  /** Subclass of ByteArrayOutputStream that exposes `buf` directly. */\n+  private static final class MyByteArrayOutputStream extends ByteArrayOutputStream {\n+    public MyByteArrayOutputStream(int size) { super(size); }\n+    public byte[] getBuf() { return buf; }\n+  }\n+\n+  private MyByteArrayOutputStream serBuffer;\n+  private SerializationStream serOutputStream;\n+\n+  /**\n+   * Are we in the process of stopping? Because map tasks can call stop() with success = true\n+   * and then call stop() with success = false if they get an exception, we want to make sure\n+   * we don't try deleting files, etc twice.\n+   */\n+  private boolean stopping = false;\n+\n+  public UnsafeShuffleWriter(\n+      BlockManager blockManager,\n+      IndexShuffleBlockResolver shuffleBlockResolver,\n+      TaskMemoryManager memoryManager,\n+      ShuffleMemoryManager shuffleMemoryManager,\n+      UnsafeShuffleHandle<K, V> handle,\n+      int mapId,\n+      TaskContext taskContext,\n+      SparkConf sparkConf) {\n+    final int numPartitions = handle.dependency().partitioner().numPartitions();\n+    if (numPartitions > PackedRecordPointer.MAXIMUM_PARTITION_ID) {\n+      throw new IllegalArgumentException(\n+        \"UnsafeShuffleWriter can only be used for shuffles with at most \" +\n+          PackedRecordPointer.MAXIMUM_PARTITION_ID + \" reduce partitions\");\n+    }\n+    this.blockManager = blockManager;\n+    this.shuffleBlockResolver = shuffleBlockResolver;\n+    this.memoryManager = memoryManager;\n+    this.shuffleMemoryManager = shuffleMemoryManager;\n+    this.mapId = mapId;\n+    final ShuffleDependency<K, V, V> dep = handle.dependency();\n+    this.shuffleId = dep.shuffleId();\n+    this.serializer = Serializer.getSerializer(dep.serializer()).newInstance();\n+    this.partitioner = dep.partitioner();\n+    this.writeMetrics = new ShuffleWriteMetrics();\n+    taskContext.taskMetrics().shuffleWriteMetrics_$eq(Option.apply(writeMetrics));\n+    this.taskContext = taskContext;\n+    this.sparkConf = sparkConf;\n+    this.transferToEnabled = sparkConf.getBoolean(\"spark.file.transferTo\", true);\n+  }\n+\n+  public void write(Iterator<Product2<K, V>> records) throws IOException {\n+    write(JavaConversions.asScalaIterator(records));\n+  }\n+\n+  @Override\n+  public void write(scala.collection.Iterator<Product2<K, V>> records) throws IOException {\n+    try {\n+      while (records.hasNext()) {\n+        insertRecordIntoSorter(records.next());\n+      }\n+      closeAndWriteOutput();\n+    } catch (Exception e) {"
  }],
  "prId": 5868
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "nit: space after IOException\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-12T07:31:57Z",
    "diffHunk": "@@ -0,0 +1,407 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.*;\n+import java.nio.channels.FileChannel;\n+import java.util.Iterator;\n+import javax.annotation.Nullable;\n+\n+import scala.Option;\n+import scala.Product2;\n+import scala.collection.JavaConversions;\n+import scala.reflect.ClassTag;\n+import scala.reflect.ClassTag$;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.io.ByteStreams;\n+import com.google.common.io.Closeables;\n+import com.google.common.io.Files;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.*;\n+import org.apache.spark.io.CompressionCodec;\n+import org.apache.spark.io.CompressionCodec$;\n+import org.apache.spark.io.LZFCompressionCodec;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.network.util.LimitedInputStream;\n+import org.apache.spark.scheduler.MapStatus;\n+import org.apache.spark.scheduler.MapStatus$;\n+import org.apache.spark.serializer.SerializationStream;\n+import org.apache.spark.serializer.Serializer;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.IndexShuffleBlockResolver;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.shuffle.ShuffleWriter;\n+import org.apache.spark.storage.BlockManager;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+public class UnsafeShuffleWriter<K, V> extends ShuffleWriter<K, V> {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleWriter.class);\n+\n+  private static final ClassTag<Object> OBJECT_CLASS_TAG = ClassTag$.MODULE$.Object();\n+\n+  @VisibleForTesting\n+  static final int INITIAL_SORT_BUFFER_SIZE = 4096;\n+\n+  private final BlockManager blockManager;\n+  private final IndexShuffleBlockResolver shuffleBlockResolver;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final SerializerInstance serializer;\n+  private final Partitioner partitioner;\n+  private final ShuffleWriteMetrics writeMetrics;\n+  private final int shuffleId;\n+  private final int mapId;\n+  private final TaskContext taskContext;\n+  private final SparkConf sparkConf;\n+  private final boolean transferToEnabled;\n+\n+  private MapStatus mapStatus = null;\n+  private UnsafeShuffleExternalSorter sorter = null;\n+\n+  /** Subclass of ByteArrayOutputStream that exposes `buf` directly. */\n+  private static final class MyByteArrayOutputStream extends ByteArrayOutputStream {\n+    public MyByteArrayOutputStream(int size) { super(size); }\n+    public byte[] getBuf() { return buf; }\n+  }\n+\n+  private MyByteArrayOutputStream serBuffer;\n+  private SerializationStream serOutputStream;\n+\n+  /**\n+   * Are we in the process of stopping? Because map tasks can call stop() with success = true\n+   * and then call stop() with success = false if they get an exception, we want to make sure\n+   * we don't try deleting files, etc twice.\n+   */\n+  private boolean stopping = false;\n+\n+  public UnsafeShuffleWriter(\n+      BlockManager blockManager,\n+      IndexShuffleBlockResolver shuffleBlockResolver,\n+      TaskMemoryManager memoryManager,\n+      ShuffleMemoryManager shuffleMemoryManager,\n+      UnsafeShuffleHandle<K, V> handle,\n+      int mapId,\n+      TaskContext taskContext,\n+      SparkConf sparkConf) {\n+    final int numPartitions = handle.dependency().partitioner().numPartitions();\n+    if (numPartitions > PackedRecordPointer.MAXIMUM_PARTITION_ID) {\n+      throw new IllegalArgumentException(\n+        \"UnsafeShuffleWriter can only be used for shuffles with at most \" +\n+          PackedRecordPointer.MAXIMUM_PARTITION_ID + \" reduce partitions\");\n+    }\n+    this.blockManager = blockManager;\n+    this.shuffleBlockResolver = shuffleBlockResolver;\n+    this.memoryManager = memoryManager;\n+    this.shuffleMemoryManager = shuffleMemoryManager;\n+    this.mapId = mapId;\n+    final ShuffleDependency<K, V, V> dep = handle.dependency();\n+    this.shuffleId = dep.shuffleId();\n+    this.serializer = Serializer.getSerializer(dep.serializer()).newInstance();\n+    this.partitioner = dep.partitioner();\n+    this.writeMetrics = new ShuffleWriteMetrics();\n+    taskContext.taskMetrics().shuffleWriteMetrics_$eq(Option.apply(writeMetrics));\n+    this.taskContext = taskContext;\n+    this.sparkConf = sparkConf;\n+    this.transferToEnabled = sparkConf.getBoolean(\"spark.file.transferTo\", true);\n+  }\n+\n+  public void write(Iterator<Product2<K, V>> records) throws IOException {\n+    write(JavaConversions.asScalaIterator(records));\n+  }\n+\n+  @Override\n+  public void write(scala.collection.Iterator<Product2<K, V>> records) throws IOException {\n+    try {\n+      while (records.hasNext()) {\n+        insertRecordIntoSorter(records.next());\n+      }\n+      closeAndWriteOutput();\n+    } catch (Exception e) {\n+      // Unfortunately, we have to catch Exception here in order to ensure proper cleanup after\n+      // errors because Spark's Scala code, or users' custom Serializers, might throw arbitrary\n+      // unchecked exceptions.\n+      try {\n+        sorter.cleanupAfterError();\n+      } finally {\n+        throw new IOException(\"Error during shuffle write\", e);\n+      }\n+    }\n+  }\n+\n+  private void open() throws IOException {\n+    assert (sorter == null);\n+    sorter = new UnsafeShuffleExternalSorter(\n+      memoryManager,\n+      shuffleMemoryManager,\n+      blockManager,\n+      taskContext,\n+      INITIAL_SORT_BUFFER_SIZE,\n+      partitioner.numPartitions(),\n+      sparkConf,\n+      writeMetrics);\n+    serBuffer = new MyByteArrayOutputStream(1024 * 1024);\n+    serOutputStream = serializer.serializeStream(serBuffer);\n+  }\n+\n+  @VisibleForTesting\n+  void closeAndWriteOutput() throws IOException {\n+    if (sorter == null) {\n+      open();\n+    }\n+    serBuffer = null;\n+    serOutputStream = null;\n+    final SpillInfo[] spills = sorter.closeAndGetSpills();\n+    sorter = null;\n+    final long[] partitionLengths;\n+    try {\n+      partitionLengths = mergeSpills(spills);\n+    } finally {\n+      for (SpillInfo spill : spills) {\n+        if (spill.file.exists() && ! spill.file.delete()) {\n+          logger.error(\"Error while deleting spill file {}\", spill.file.getPath());\n+        }\n+      }\n+    }\n+    shuffleBlockResolver.writeIndexFile(shuffleId, mapId, partitionLengths);\n+    mapStatus = MapStatus$.MODULE$.apply(blockManager.shuffleServerId(), partitionLengths);\n+  }\n+\n+  @VisibleForTesting\n+  void insertRecordIntoSorter(Product2<K, V> record) throws IOException{"
  }],
  "prId": 5868
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "why ain't we explicitly calling open in the beginning, rather than lazily doing it?\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-12T07:33:03Z",
    "diffHunk": "@@ -0,0 +1,407 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.*;\n+import java.nio.channels.FileChannel;\n+import java.util.Iterator;\n+import javax.annotation.Nullable;\n+\n+import scala.Option;\n+import scala.Product2;\n+import scala.collection.JavaConversions;\n+import scala.reflect.ClassTag;\n+import scala.reflect.ClassTag$;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.io.ByteStreams;\n+import com.google.common.io.Closeables;\n+import com.google.common.io.Files;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.*;\n+import org.apache.spark.io.CompressionCodec;\n+import org.apache.spark.io.CompressionCodec$;\n+import org.apache.spark.io.LZFCompressionCodec;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.network.util.LimitedInputStream;\n+import org.apache.spark.scheduler.MapStatus;\n+import org.apache.spark.scheduler.MapStatus$;\n+import org.apache.spark.serializer.SerializationStream;\n+import org.apache.spark.serializer.Serializer;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.IndexShuffleBlockResolver;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.shuffle.ShuffleWriter;\n+import org.apache.spark.storage.BlockManager;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+public class UnsafeShuffleWriter<K, V> extends ShuffleWriter<K, V> {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleWriter.class);\n+\n+  private static final ClassTag<Object> OBJECT_CLASS_TAG = ClassTag$.MODULE$.Object();\n+\n+  @VisibleForTesting\n+  static final int INITIAL_SORT_BUFFER_SIZE = 4096;\n+\n+  private final BlockManager blockManager;\n+  private final IndexShuffleBlockResolver shuffleBlockResolver;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final SerializerInstance serializer;\n+  private final Partitioner partitioner;\n+  private final ShuffleWriteMetrics writeMetrics;\n+  private final int shuffleId;\n+  private final int mapId;\n+  private final TaskContext taskContext;\n+  private final SparkConf sparkConf;\n+  private final boolean transferToEnabled;\n+\n+  private MapStatus mapStatus = null;\n+  private UnsafeShuffleExternalSorter sorter = null;\n+\n+  /** Subclass of ByteArrayOutputStream that exposes `buf` directly. */\n+  private static final class MyByteArrayOutputStream extends ByteArrayOutputStream {\n+    public MyByteArrayOutputStream(int size) { super(size); }\n+    public byte[] getBuf() { return buf; }\n+  }\n+\n+  private MyByteArrayOutputStream serBuffer;\n+  private SerializationStream serOutputStream;\n+\n+  /**\n+   * Are we in the process of stopping? Because map tasks can call stop() with success = true\n+   * and then call stop() with success = false if they get an exception, we want to make sure\n+   * we don't try deleting files, etc twice.\n+   */\n+  private boolean stopping = false;\n+\n+  public UnsafeShuffleWriter(\n+      BlockManager blockManager,\n+      IndexShuffleBlockResolver shuffleBlockResolver,\n+      TaskMemoryManager memoryManager,\n+      ShuffleMemoryManager shuffleMemoryManager,\n+      UnsafeShuffleHandle<K, V> handle,\n+      int mapId,\n+      TaskContext taskContext,\n+      SparkConf sparkConf) {\n+    final int numPartitions = handle.dependency().partitioner().numPartitions();\n+    if (numPartitions > PackedRecordPointer.MAXIMUM_PARTITION_ID) {\n+      throw new IllegalArgumentException(\n+        \"UnsafeShuffleWriter can only be used for shuffles with at most \" +\n+          PackedRecordPointer.MAXIMUM_PARTITION_ID + \" reduce partitions\");\n+    }\n+    this.blockManager = blockManager;\n+    this.shuffleBlockResolver = shuffleBlockResolver;\n+    this.memoryManager = memoryManager;\n+    this.shuffleMemoryManager = shuffleMemoryManager;\n+    this.mapId = mapId;\n+    final ShuffleDependency<K, V, V> dep = handle.dependency();\n+    this.shuffleId = dep.shuffleId();\n+    this.serializer = Serializer.getSerializer(dep.serializer()).newInstance();\n+    this.partitioner = dep.partitioner();\n+    this.writeMetrics = new ShuffleWriteMetrics();\n+    taskContext.taskMetrics().shuffleWriteMetrics_$eq(Option.apply(writeMetrics));\n+    this.taskContext = taskContext;\n+    this.sparkConf = sparkConf;\n+    this.transferToEnabled = sparkConf.getBoolean(\"spark.file.transferTo\", true);\n+  }\n+\n+  public void write(Iterator<Product2<K, V>> records) throws IOException {\n+    write(JavaConversions.asScalaIterator(records));\n+  }\n+\n+  @Override\n+  public void write(scala.collection.Iterator<Product2<K, V>> records) throws IOException {\n+    try {\n+      while (records.hasNext()) {\n+        insertRecordIntoSorter(records.next());\n+      }\n+      closeAndWriteOutput();\n+    } catch (Exception e) {\n+      // Unfortunately, we have to catch Exception here in order to ensure proper cleanup after\n+      // errors because Spark's Scala code, or users' custom Serializers, might throw arbitrary\n+      // unchecked exceptions.\n+      try {\n+        sorter.cleanupAfterError();\n+      } finally {\n+        throw new IOException(\"Error during shuffle write\", e);\n+      }\n+    }\n+  }\n+\n+  private void open() throws IOException {",
    "line": 156
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "Fixed.\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-12T21:31:37Z",
    "diffHunk": "@@ -0,0 +1,407 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.*;\n+import java.nio.channels.FileChannel;\n+import java.util.Iterator;\n+import javax.annotation.Nullable;\n+\n+import scala.Option;\n+import scala.Product2;\n+import scala.collection.JavaConversions;\n+import scala.reflect.ClassTag;\n+import scala.reflect.ClassTag$;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.io.ByteStreams;\n+import com.google.common.io.Closeables;\n+import com.google.common.io.Files;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.*;\n+import org.apache.spark.io.CompressionCodec;\n+import org.apache.spark.io.CompressionCodec$;\n+import org.apache.spark.io.LZFCompressionCodec;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.network.util.LimitedInputStream;\n+import org.apache.spark.scheduler.MapStatus;\n+import org.apache.spark.scheduler.MapStatus$;\n+import org.apache.spark.serializer.SerializationStream;\n+import org.apache.spark.serializer.Serializer;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.IndexShuffleBlockResolver;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.shuffle.ShuffleWriter;\n+import org.apache.spark.storage.BlockManager;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+public class UnsafeShuffleWriter<K, V> extends ShuffleWriter<K, V> {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleWriter.class);\n+\n+  private static final ClassTag<Object> OBJECT_CLASS_TAG = ClassTag$.MODULE$.Object();\n+\n+  @VisibleForTesting\n+  static final int INITIAL_SORT_BUFFER_SIZE = 4096;\n+\n+  private final BlockManager blockManager;\n+  private final IndexShuffleBlockResolver shuffleBlockResolver;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final SerializerInstance serializer;\n+  private final Partitioner partitioner;\n+  private final ShuffleWriteMetrics writeMetrics;\n+  private final int shuffleId;\n+  private final int mapId;\n+  private final TaskContext taskContext;\n+  private final SparkConf sparkConf;\n+  private final boolean transferToEnabled;\n+\n+  private MapStatus mapStatus = null;\n+  private UnsafeShuffleExternalSorter sorter = null;\n+\n+  /** Subclass of ByteArrayOutputStream that exposes `buf` directly. */\n+  private static final class MyByteArrayOutputStream extends ByteArrayOutputStream {\n+    public MyByteArrayOutputStream(int size) { super(size); }\n+    public byte[] getBuf() { return buf; }\n+  }\n+\n+  private MyByteArrayOutputStream serBuffer;\n+  private SerializationStream serOutputStream;\n+\n+  /**\n+   * Are we in the process of stopping? Because map tasks can call stop() with success = true\n+   * and then call stop() with success = false if they get an exception, we want to make sure\n+   * we don't try deleting files, etc twice.\n+   */\n+  private boolean stopping = false;\n+\n+  public UnsafeShuffleWriter(\n+      BlockManager blockManager,\n+      IndexShuffleBlockResolver shuffleBlockResolver,\n+      TaskMemoryManager memoryManager,\n+      ShuffleMemoryManager shuffleMemoryManager,\n+      UnsafeShuffleHandle<K, V> handle,\n+      int mapId,\n+      TaskContext taskContext,\n+      SparkConf sparkConf) {\n+    final int numPartitions = handle.dependency().partitioner().numPartitions();\n+    if (numPartitions > PackedRecordPointer.MAXIMUM_PARTITION_ID) {\n+      throw new IllegalArgumentException(\n+        \"UnsafeShuffleWriter can only be used for shuffles with at most \" +\n+          PackedRecordPointer.MAXIMUM_PARTITION_ID + \" reduce partitions\");\n+    }\n+    this.blockManager = blockManager;\n+    this.shuffleBlockResolver = shuffleBlockResolver;\n+    this.memoryManager = memoryManager;\n+    this.shuffleMemoryManager = shuffleMemoryManager;\n+    this.mapId = mapId;\n+    final ShuffleDependency<K, V, V> dep = handle.dependency();\n+    this.shuffleId = dep.shuffleId();\n+    this.serializer = Serializer.getSerializer(dep.serializer()).newInstance();\n+    this.partitioner = dep.partitioner();\n+    this.writeMetrics = new ShuffleWriteMetrics();\n+    taskContext.taskMetrics().shuffleWriteMetrics_$eq(Option.apply(writeMetrics));\n+    this.taskContext = taskContext;\n+    this.sparkConf = sparkConf;\n+    this.transferToEnabled = sparkConf.getBoolean(\"spark.file.transferTo\", true);\n+  }\n+\n+  public void write(Iterator<Product2<K, V>> records) throws IOException {\n+    write(JavaConversions.asScalaIterator(records));\n+  }\n+\n+  @Override\n+  public void write(scala.collection.Iterator<Product2<K, V>> records) throws IOException {\n+    try {\n+      while (records.hasNext()) {\n+        insertRecordIntoSorter(records.next());\n+      }\n+      closeAndWriteOutput();\n+    } catch (Exception e) {\n+      // Unfortunately, we have to catch Exception here in order to ensure proper cleanup after\n+      // errors because Spark's Scala code, or users' custom Serializers, might throw arbitrary\n+      // unchecked exceptions.\n+      try {\n+        sorter.cleanupAfterError();\n+      } finally {\n+        throw new IOException(\"Error during shuffle write\", e);\n+      }\n+    }\n+  }\n+\n+  private void open() throws IOException {",
    "line": 156
  }],
  "prId": 5868
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "don't you want to swallow the ioexception here?\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-12T07:38:50Z",
    "diffHunk": "@@ -0,0 +1,407 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.*;\n+import java.nio.channels.FileChannel;\n+import java.util.Iterator;\n+import javax.annotation.Nullable;\n+\n+import scala.Option;\n+import scala.Product2;\n+import scala.collection.JavaConversions;\n+import scala.reflect.ClassTag;\n+import scala.reflect.ClassTag$;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.io.ByteStreams;\n+import com.google.common.io.Closeables;\n+import com.google.common.io.Files;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.*;\n+import org.apache.spark.io.CompressionCodec;\n+import org.apache.spark.io.CompressionCodec$;\n+import org.apache.spark.io.LZFCompressionCodec;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.network.util.LimitedInputStream;\n+import org.apache.spark.scheduler.MapStatus;\n+import org.apache.spark.scheduler.MapStatus$;\n+import org.apache.spark.serializer.SerializationStream;\n+import org.apache.spark.serializer.Serializer;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.IndexShuffleBlockResolver;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.shuffle.ShuffleWriter;\n+import org.apache.spark.storage.BlockManager;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+public class UnsafeShuffleWriter<K, V> extends ShuffleWriter<K, V> {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleWriter.class);\n+\n+  private static final ClassTag<Object> OBJECT_CLASS_TAG = ClassTag$.MODULE$.Object();\n+\n+  @VisibleForTesting\n+  static final int INITIAL_SORT_BUFFER_SIZE = 4096;\n+\n+  private final BlockManager blockManager;\n+  private final IndexShuffleBlockResolver shuffleBlockResolver;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final SerializerInstance serializer;\n+  private final Partitioner partitioner;\n+  private final ShuffleWriteMetrics writeMetrics;\n+  private final int shuffleId;\n+  private final int mapId;\n+  private final TaskContext taskContext;\n+  private final SparkConf sparkConf;\n+  private final boolean transferToEnabled;\n+\n+  private MapStatus mapStatus = null;\n+  private UnsafeShuffleExternalSorter sorter = null;\n+\n+  /** Subclass of ByteArrayOutputStream that exposes `buf` directly. */\n+  private static final class MyByteArrayOutputStream extends ByteArrayOutputStream {\n+    public MyByteArrayOutputStream(int size) { super(size); }\n+    public byte[] getBuf() { return buf; }\n+  }\n+\n+  private MyByteArrayOutputStream serBuffer;\n+  private SerializationStream serOutputStream;\n+\n+  /**\n+   * Are we in the process of stopping? Because map tasks can call stop() with success = true\n+   * and then call stop() with success = false if they get an exception, we want to make sure\n+   * we don't try deleting files, etc twice.\n+   */\n+  private boolean stopping = false;\n+\n+  public UnsafeShuffleWriter(\n+      BlockManager blockManager,\n+      IndexShuffleBlockResolver shuffleBlockResolver,\n+      TaskMemoryManager memoryManager,\n+      ShuffleMemoryManager shuffleMemoryManager,\n+      UnsafeShuffleHandle<K, V> handle,\n+      int mapId,\n+      TaskContext taskContext,\n+      SparkConf sparkConf) {\n+    final int numPartitions = handle.dependency().partitioner().numPartitions();\n+    if (numPartitions > PackedRecordPointer.MAXIMUM_PARTITION_ID) {\n+      throw new IllegalArgumentException(\n+        \"UnsafeShuffleWriter can only be used for shuffles with at most \" +\n+          PackedRecordPointer.MAXIMUM_PARTITION_ID + \" reduce partitions\");\n+    }\n+    this.blockManager = blockManager;\n+    this.shuffleBlockResolver = shuffleBlockResolver;\n+    this.memoryManager = memoryManager;\n+    this.shuffleMemoryManager = shuffleMemoryManager;\n+    this.mapId = mapId;\n+    final ShuffleDependency<K, V, V> dep = handle.dependency();\n+    this.shuffleId = dep.shuffleId();\n+    this.serializer = Serializer.getSerializer(dep.serializer()).newInstance();\n+    this.partitioner = dep.partitioner();\n+    this.writeMetrics = new ShuffleWriteMetrics();\n+    taskContext.taskMetrics().shuffleWriteMetrics_$eq(Option.apply(writeMetrics));\n+    this.taskContext = taskContext;\n+    this.sparkConf = sparkConf;\n+    this.transferToEnabled = sparkConf.getBoolean(\"spark.file.transferTo\", true);\n+  }\n+\n+  public void write(Iterator<Product2<K, V>> records) throws IOException {\n+    write(JavaConversions.asScalaIterator(records));\n+  }\n+\n+  @Override\n+  public void write(scala.collection.Iterator<Product2<K, V>> records) throws IOException {\n+    try {\n+      while (records.hasNext()) {\n+        insertRecordIntoSorter(records.next());\n+      }\n+      closeAndWriteOutput();\n+    } catch (Exception e) {\n+      // Unfortunately, we have to catch Exception here in order to ensure proper cleanup after\n+      // errors because Spark's Scala code, or users' custom Serializers, might throw arbitrary\n+      // unchecked exceptions.\n+      try {\n+        sorter.cleanupAfterError();\n+      } finally {\n+        throw new IOException(\"Error during shuffle write\", e);\n+      }\n+    }\n+  }\n+\n+  private void open() throws IOException {\n+    assert (sorter == null);\n+    sorter = new UnsafeShuffleExternalSorter(\n+      memoryManager,\n+      shuffleMemoryManager,\n+      blockManager,\n+      taskContext,\n+      INITIAL_SORT_BUFFER_SIZE,\n+      partitioner.numPartitions(),\n+      sparkConf,\n+      writeMetrics);\n+    serBuffer = new MyByteArrayOutputStream(1024 * 1024);\n+    serOutputStream = serializer.serializeStream(serBuffer);\n+  }\n+\n+  @VisibleForTesting\n+  void closeAndWriteOutput() throws IOException {\n+    if (sorter == null) {\n+      open();\n+    }\n+    serBuffer = null;\n+    serOutputStream = null;\n+    final SpillInfo[] spills = sorter.closeAndGetSpills();\n+    sorter = null;\n+    final long[] partitionLengths;\n+    try {\n+      partitionLengths = mergeSpills(spills);\n+    } finally {\n+      for (SpillInfo spill : spills) {\n+        if (spill.file.exists() && ! spill.file.delete()) {\n+          logger.error(\"Error while deleting spill file {}\", spill.file.getPath());\n+        }\n+      }\n+    }\n+    shuffleBlockResolver.writeIndexFile(shuffleId, mapId, partitionLengths);\n+    mapStatus = MapStatus$.MODULE$.apply(blockManager.shuffleServerId(), partitionLengths);\n+  }\n+\n+  @VisibleForTesting\n+  void insertRecordIntoSorter(Product2<K, V> record) throws IOException{\n+    if (sorter == null) {\n+      open();\n+    }\n+    final K key = record._1();\n+    final int partitionId = partitioner.getPartition(key);\n+    serBuffer.reset();\n+    serOutputStream.writeKey(key, OBJECT_CLASS_TAG);\n+    serOutputStream.writeValue(record._2(), OBJECT_CLASS_TAG);\n+    serOutputStream.flush();\n+\n+    final int serializedRecordSize = serBuffer.size();\n+    assert (serializedRecordSize > 0);\n+\n+    sorter.insertRecord(\n+      serBuffer.getBuf(), PlatformDependent.BYTE_ARRAY_OFFSET, serializedRecordSize, partitionId);\n+  }\n+\n+  @VisibleForTesting\n+  void forceSorterToSpill() throws IOException {\n+    assert (sorter != null);\n+    sorter.spill();\n+  }\n+\n+  /**\n+   * Merge zero or more spill files together, choosing the fastest merging strategy based on the\n+   * number of spills and the IO compression codec.\n+   *\n+   * @return the partition lengths in the merged file.\n+   */\n+  private long[] mergeSpills(SpillInfo[] spills) throws IOException {\n+    final File outputFile = shuffleBlockResolver.getDataFile(shuffleId, mapId);\n+    final boolean compressionEnabled = sparkConf.getBoolean(\"spark.shuffle.compress\", true);\n+    final CompressionCodec compressionCodec = CompressionCodec$.MODULE$.createCodec(sparkConf);\n+    final boolean fastMergeEnabled =\n+      sparkConf.getBoolean(\"spark.shuffle.unsafe.fastMergeEnabled\", true);\n+    final boolean fastMergeIsSupported =\n+      !compressionEnabled || compressionCodec instanceof LZFCompressionCodec;\n+    try {\n+      if (spills.length == 0) {\n+        new FileOutputStream(outputFile).close(); // Create an empty file\n+        return new long[partitioner.numPartitions()];\n+      } else if (spills.length == 1) {\n+        // Here, we don't need to perform any metrics updates because the bytes written to this\n+        // output file would have already been counted as shuffle bytes written.\n+        Files.move(spills[0].file, outputFile);\n+        return spills[0].partitionLengths;\n+      } else {\n+        final long[] partitionLengths;\n+        // There are multiple spills to merge, so none of these spill files' lengths were counted\n+        // towards our shuffle write count or shuffle write time. If we use the slow merge path,\n+        // then the final output file's size won't necessarily be equal to the sum of the spill\n+        // files' sizes. To guard against this case, we look at the output file's actual size when\n+        // computing shuffle bytes written.\n+        //\n+        // We allow the individual merge methods to report their own IO times since different merge\n+        // strategies use different IO techniques.  We count IO during merge towards the shuffle\n+        // shuffle write time, which appears to be consistent with the \"not bypassing merge-sort\"\n+        // branch in ExternalSorter.\n+        if (fastMergeEnabled && fastMergeIsSupported) {\n+          // Compression is disabled or we are using an IO compression codec that supports\n+          // decompression of concatenated compressed streams, so we can perform a fast spill merge\n+          // that doesn't need to interpret the spilled bytes.\n+          if (transferToEnabled) {\n+            logger.debug(\"Using transferTo-based fast merge\");\n+            partitionLengths = mergeSpillsWithTransferTo(spills, outputFile);\n+          } else {\n+            logger.debug(\"Using fileStream-based fast merge\");\n+            partitionLengths = mergeSpillsWithFileStream(spills, outputFile, null);\n+          }\n+        } else {\n+          logger.debug(\"Using slow merge\");\n+          partitionLengths = mergeSpillsWithFileStream(spills, outputFile, compressionCodec);\n+        }\n+        // The final shuffle spill's write would have directly updated shuffleBytesWritten, so\n+        // we need to decrement to avoid double-counting this write.\n+        writeMetrics.decShuffleBytesWritten(spills[spills.length - 1].file.length());\n+        writeMetrics.incShuffleBytesWritten(outputFile.length());\n+        return partitionLengths;\n+      }\n+    } catch (IOException e) {\n+      if (outputFile.exists() && !outputFile.delete()) {\n+        logger.error(\"Unable to delete output file {}\", outputFile.getPath());\n+      }\n+      throw e;\n+    }\n+  }\n+\n+  private long[] mergeSpillsWithFileStream(\n+      SpillInfo[] spills,\n+      File outputFile,\n+      @Nullable CompressionCodec compressionCodec) throws IOException {\n+    final int numPartitions = partitioner.numPartitions();\n+    final long[] partitionLengths = new long[numPartitions];\n+    final InputStream[] spillInputStreams = new FileInputStream[spills.length];\n+    OutputStream mergedFileOutputStream = null;\n+\n+    try {\n+      for (int i = 0; i < spills.length; i++) {\n+        spillInputStreams[i] = new FileInputStream(spills[i].file);\n+      }\n+      for (int partition = 0; partition < numPartitions; partition++) {\n+        final long initialFileLength = outputFile.length();\n+        mergedFileOutputStream =\n+          new TimeTrackingFileOutputStream(writeMetrics, new FileOutputStream(outputFile, true));\n+        if (compressionCodec != null) {\n+          mergedFileOutputStream = compressionCodec.compressedOutputStream(mergedFileOutputStream);\n+        }\n+\n+        for (int i = 0; i < spills.length; i++) {\n+          final long partitionLengthInSpill = spills[i].partitionLengths[partition];\n+          if (partitionLengthInSpill > 0) {\n+            InputStream partitionInputStream =\n+              new LimitedInputStream(spillInputStreams[i], partitionLengthInSpill);\n+            if (compressionCodec != null) {\n+              partitionInputStream = compressionCodec.compressedInputStream(partitionInputStream);\n+            }\n+            ByteStreams.copy(partitionInputStream, mergedFileOutputStream);\n+          }\n+        }\n+        mergedFileOutputStream.flush();\n+        mergedFileOutputStream.close();\n+        partitionLengths[partition] = (outputFile.length() - initialFileLength);\n+      }\n+    } finally {\n+      for (InputStream stream : spillInputStreams) {\n+        Closeables.close(stream, false);\n+      }\n+      Closeables.close(mergedFileOutputStream, false);\n+    }\n+    return partitionLengths;\n+  }\n+\n+  private long[] mergeSpillsWithTransferTo(SpillInfo[] spills, File outputFile) throws IOException {\n+    final int numPartitions = partitioner.numPartitions();\n+    final long[] partitionLengths = new long[numPartitions];\n+    final FileChannel[] spillInputChannels = new FileChannel[spills.length];\n+    final long[] spillInputChannelPositions = new long[spills.length];\n+    FileChannel mergedFileOutputChannel = null;\n+\n+    try {\n+      for (int i = 0; i < spills.length; i++) {\n+        spillInputChannels[i] = new FileInputStream(spills[i].file).getChannel();\n+      }\n+      // This file needs to opened in append mode in order to work around a Linux kernel bug that\n+      // affects transferTo; see SPARK-3948 for more details.\n+      mergedFileOutputChannel = new FileOutputStream(outputFile, true).getChannel();\n+\n+      long bytesWrittenToMergedFile = 0;\n+      for (int partition = 0; partition < numPartitions; partition++) {\n+        for (int i = 0; i < spills.length; i++) {\n+          final long partitionLengthInSpill = spills[i].partitionLengths[partition];\n+          long bytesToTransfer = partitionLengthInSpill;\n+          final FileChannel spillInputChannel = spillInputChannels[i];\n+          final long writeStartTime = System.nanoTime();\n+          while (bytesToTransfer > 0) {\n+            final long actualBytesTransferred = spillInputChannel.transferTo(\n+              spillInputChannelPositions[i],\n+              bytesToTransfer,\n+              mergedFileOutputChannel);\n+            spillInputChannelPositions[i] += actualBytesTransferred;\n+            bytesToTransfer -= actualBytesTransferred;\n+          }\n+          writeMetrics.incShuffleWriteTime(System.nanoTime() - writeStartTime);\n+          bytesWrittenToMergedFile += partitionLengthInSpill;\n+          partitionLengths[partition] += partitionLengthInSpill;\n+        }\n+      }\n+      // Check the position after transferTo loop to see if it is in the right position and raise an\n+      // exception if it is incorrect. The position will not be increased to the expected length\n+      // after calling transferTo in kernel version 2.6.32. This issue is described at\n+      // https://bugs.openjdk.java.net/browse/JDK-7052359 and SPARK-3948.\n+      if (mergedFileOutputChannel.position() != bytesWrittenToMergedFile) {\n+        throw new IOException(\n+          \"Current position \" + mergedFileOutputChannel.position() + \" does not equal expected \" +\n+            \"position \" + bytesWrittenToMergedFile + \" after transferTo. Please check your kernel\" +\n+            \" version to see if it is 2.6.32, as there is a kernel bug which will lead to \" +\n+            \"unexpected behavior when using transferTo. You can set spark.file.transferTo=false \" +\n+            \"to disable this NIO feature.\"\n+        );\n+      }\n+    } finally {\n+      for (int i = 0; i < spills.length; i++) {\n+        assert(spillInputChannelPositions[i] == spills[i].file.length());\n+        Closeables.close(spillInputChannels[i], false);"
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "I think that my concern was that throwing exceptions from the finally block would mask other exceptions, but it looks like there's a nice idiom for handling this that's shown in the Closeables.close docs:\n\n```\n<p>Example: <pre>   {@code\n   *\n   *   public void useStreamNicely() throws IOException {\n   *     SomeStream stream = new SomeStream(\"foo\");\n   *     boolean threw = true;\n   *     try {\n   *       // ... code which does something with the stream ...\n   *       threw = false;\n   *     } finally {\n   *       // If an exception occurs, rethrow it only if threw==false:\n   *       Closeables.close(stream, threw);\n   *     }\n   *   }}</pre>\n```\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-12T21:09:22Z",
    "diffHunk": "@@ -0,0 +1,407 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.*;\n+import java.nio.channels.FileChannel;\n+import java.util.Iterator;\n+import javax.annotation.Nullable;\n+\n+import scala.Option;\n+import scala.Product2;\n+import scala.collection.JavaConversions;\n+import scala.reflect.ClassTag;\n+import scala.reflect.ClassTag$;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.io.ByteStreams;\n+import com.google.common.io.Closeables;\n+import com.google.common.io.Files;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.*;\n+import org.apache.spark.io.CompressionCodec;\n+import org.apache.spark.io.CompressionCodec$;\n+import org.apache.spark.io.LZFCompressionCodec;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.network.util.LimitedInputStream;\n+import org.apache.spark.scheduler.MapStatus;\n+import org.apache.spark.scheduler.MapStatus$;\n+import org.apache.spark.serializer.SerializationStream;\n+import org.apache.spark.serializer.Serializer;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.IndexShuffleBlockResolver;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.shuffle.ShuffleWriter;\n+import org.apache.spark.storage.BlockManager;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+public class UnsafeShuffleWriter<K, V> extends ShuffleWriter<K, V> {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleWriter.class);\n+\n+  private static final ClassTag<Object> OBJECT_CLASS_TAG = ClassTag$.MODULE$.Object();\n+\n+  @VisibleForTesting\n+  static final int INITIAL_SORT_BUFFER_SIZE = 4096;\n+\n+  private final BlockManager blockManager;\n+  private final IndexShuffleBlockResolver shuffleBlockResolver;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final SerializerInstance serializer;\n+  private final Partitioner partitioner;\n+  private final ShuffleWriteMetrics writeMetrics;\n+  private final int shuffleId;\n+  private final int mapId;\n+  private final TaskContext taskContext;\n+  private final SparkConf sparkConf;\n+  private final boolean transferToEnabled;\n+\n+  private MapStatus mapStatus = null;\n+  private UnsafeShuffleExternalSorter sorter = null;\n+\n+  /** Subclass of ByteArrayOutputStream that exposes `buf` directly. */\n+  private static final class MyByteArrayOutputStream extends ByteArrayOutputStream {\n+    public MyByteArrayOutputStream(int size) { super(size); }\n+    public byte[] getBuf() { return buf; }\n+  }\n+\n+  private MyByteArrayOutputStream serBuffer;\n+  private SerializationStream serOutputStream;\n+\n+  /**\n+   * Are we in the process of stopping? Because map tasks can call stop() with success = true\n+   * and then call stop() with success = false if they get an exception, we want to make sure\n+   * we don't try deleting files, etc twice.\n+   */\n+  private boolean stopping = false;\n+\n+  public UnsafeShuffleWriter(\n+      BlockManager blockManager,\n+      IndexShuffleBlockResolver shuffleBlockResolver,\n+      TaskMemoryManager memoryManager,\n+      ShuffleMemoryManager shuffleMemoryManager,\n+      UnsafeShuffleHandle<K, V> handle,\n+      int mapId,\n+      TaskContext taskContext,\n+      SparkConf sparkConf) {\n+    final int numPartitions = handle.dependency().partitioner().numPartitions();\n+    if (numPartitions > PackedRecordPointer.MAXIMUM_PARTITION_ID) {\n+      throw new IllegalArgumentException(\n+        \"UnsafeShuffleWriter can only be used for shuffles with at most \" +\n+          PackedRecordPointer.MAXIMUM_PARTITION_ID + \" reduce partitions\");\n+    }\n+    this.blockManager = blockManager;\n+    this.shuffleBlockResolver = shuffleBlockResolver;\n+    this.memoryManager = memoryManager;\n+    this.shuffleMemoryManager = shuffleMemoryManager;\n+    this.mapId = mapId;\n+    final ShuffleDependency<K, V, V> dep = handle.dependency();\n+    this.shuffleId = dep.shuffleId();\n+    this.serializer = Serializer.getSerializer(dep.serializer()).newInstance();\n+    this.partitioner = dep.partitioner();\n+    this.writeMetrics = new ShuffleWriteMetrics();\n+    taskContext.taskMetrics().shuffleWriteMetrics_$eq(Option.apply(writeMetrics));\n+    this.taskContext = taskContext;\n+    this.sparkConf = sparkConf;\n+    this.transferToEnabled = sparkConf.getBoolean(\"spark.file.transferTo\", true);\n+  }\n+\n+  public void write(Iterator<Product2<K, V>> records) throws IOException {\n+    write(JavaConversions.asScalaIterator(records));\n+  }\n+\n+  @Override\n+  public void write(scala.collection.Iterator<Product2<K, V>> records) throws IOException {\n+    try {\n+      while (records.hasNext()) {\n+        insertRecordIntoSorter(records.next());\n+      }\n+      closeAndWriteOutput();\n+    } catch (Exception e) {\n+      // Unfortunately, we have to catch Exception here in order to ensure proper cleanup after\n+      // errors because Spark's Scala code, or users' custom Serializers, might throw arbitrary\n+      // unchecked exceptions.\n+      try {\n+        sorter.cleanupAfterError();\n+      } finally {\n+        throw new IOException(\"Error during shuffle write\", e);\n+      }\n+    }\n+  }\n+\n+  private void open() throws IOException {\n+    assert (sorter == null);\n+    sorter = new UnsafeShuffleExternalSorter(\n+      memoryManager,\n+      shuffleMemoryManager,\n+      blockManager,\n+      taskContext,\n+      INITIAL_SORT_BUFFER_SIZE,\n+      partitioner.numPartitions(),\n+      sparkConf,\n+      writeMetrics);\n+    serBuffer = new MyByteArrayOutputStream(1024 * 1024);\n+    serOutputStream = serializer.serializeStream(serBuffer);\n+  }\n+\n+  @VisibleForTesting\n+  void closeAndWriteOutput() throws IOException {\n+    if (sorter == null) {\n+      open();\n+    }\n+    serBuffer = null;\n+    serOutputStream = null;\n+    final SpillInfo[] spills = sorter.closeAndGetSpills();\n+    sorter = null;\n+    final long[] partitionLengths;\n+    try {\n+      partitionLengths = mergeSpills(spills);\n+    } finally {\n+      for (SpillInfo spill : spills) {\n+        if (spill.file.exists() && ! spill.file.delete()) {\n+          logger.error(\"Error while deleting spill file {}\", spill.file.getPath());\n+        }\n+      }\n+    }\n+    shuffleBlockResolver.writeIndexFile(shuffleId, mapId, partitionLengths);\n+    mapStatus = MapStatus$.MODULE$.apply(blockManager.shuffleServerId(), partitionLengths);\n+  }\n+\n+  @VisibleForTesting\n+  void insertRecordIntoSorter(Product2<K, V> record) throws IOException{\n+    if (sorter == null) {\n+      open();\n+    }\n+    final K key = record._1();\n+    final int partitionId = partitioner.getPartition(key);\n+    serBuffer.reset();\n+    serOutputStream.writeKey(key, OBJECT_CLASS_TAG);\n+    serOutputStream.writeValue(record._2(), OBJECT_CLASS_TAG);\n+    serOutputStream.flush();\n+\n+    final int serializedRecordSize = serBuffer.size();\n+    assert (serializedRecordSize > 0);\n+\n+    sorter.insertRecord(\n+      serBuffer.getBuf(), PlatformDependent.BYTE_ARRAY_OFFSET, serializedRecordSize, partitionId);\n+  }\n+\n+  @VisibleForTesting\n+  void forceSorterToSpill() throws IOException {\n+    assert (sorter != null);\n+    sorter.spill();\n+  }\n+\n+  /**\n+   * Merge zero or more spill files together, choosing the fastest merging strategy based on the\n+   * number of spills and the IO compression codec.\n+   *\n+   * @return the partition lengths in the merged file.\n+   */\n+  private long[] mergeSpills(SpillInfo[] spills) throws IOException {\n+    final File outputFile = shuffleBlockResolver.getDataFile(shuffleId, mapId);\n+    final boolean compressionEnabled = sparkConf.getBoolean(\"spark.shuffle.compress\", true);\n+    final CompressionCodec compressionCodec = CompressionCodec$.MODULE$.createCodec(sparkConf);\n+    final boolean fastMergeEnabled =\n+      sparkConf.getBoolean(\"spark.shuffle.unsafe.fastMergeEnabled\", true);\n+    final boolean fastMergeIsSupported =\n+      !compressionEnabled || compressionCodec instanceof LZFCompressionCodec;\n+    try {\n+      if (spills.length == 0) {\n+        new FileOutputStream(outputFile).close(); // Create an empty file\n+        return new long[partitioner.numPartitions()];\n+      } else if (spills.length == 1) {\n+        // Here, we don't need to perform any metrics updates because the bytes written to this\n+        // output file would have already been counted as shuffle bytes written.\n+        Files.move(spills[0].file, outputFile);\n+        return spills[0].partitionLengths;\n+      } else {\n+        final long[] partitionLengths;\n+        // There are multiple spills to merge, so none of these spill files' lengths were counted\n+        // towards our shuffle write count or shuffle write time. If we use the slow merge path,\n+        // then the final output file's size won't necessarily be equal to the sum of the spill\n+        // files' sizes. To guard against this case, we look at the output file's actual size when\n+        // computing shuffle bytes written.\n+        //\n+        // We allow the individual merge methods to report their own IO times since different merge\n+        // strategies use different IO techniques.  We count IO during merge towards the shuffle\n+        // shuffle write time, which appears to be consistent with the \"not bypassing merge-sort\"\n+        // branch in ExternalSorter.\n+        if (fastMergeEnabled && fastMergeIsSupported) {\n+          // Compression is disabled or we are using an IO compression codec that supports\n+          // decompression of concatenated compressed streams, so we can perform a fast spill merge\n+          // that doesn't need to interpret the spilled bytes.\n+          if (transferToEnabled) {\n+            logger.debug(\"Using transferTo-based fast merge\");\n+            partitionLengths = mergeSpillsWithTransferTo(spills, outputFile);\n+          } else {\n+            logger.debug(\"Using fileStream-based fast merge\");\n+            partitionLengths = mergeSpillsWithFileStream(spills, outputFile, null);\n+          }\n+        } else {\n+          logger.debug(\"Using slow merge\");\n+          partitionLengths = mergeSpillsWithFileStream(spills, outputFile, compressionCodec);\n+        }\n+        // The final shuffle spill's write would have directly updated shuffleBytesWritten, so\n+        // we need to decrement to avoid double-counting this write.\n+        writeMetrics.decShuffleBytesWritten(spills[spills.length - 1].file.length());\n+        writeMetrics.incShuffleBytesWritten(outputFile.length());\n+        return partitionLengths;\n+      }\n+    } catch (IOException e) {\n+      if (outputFile.exists() && !outputFile.delete()) {\n+        logger.error(\"Unable to delete output file {}\", outputFile.getPath());\n+      }\n+      throw e;\n+    }\n+  }\n+\n+  private long[] mergeSpillsWithFileStream(\n+      SpillInfo[] spills,\n+      File outputFile,\n+      @Nullable CompressionCodec compressionCodec) throws IOException {\n+    final int numPartitions = partitioner.numPartitions();\n+    final long[] partitionLengths = new long[numPartitions];\n+    final InputStream[] spillInputStreams = new FileInputStream[spills.length];\n+    OutputStream mergedFileOutputStream = null;\n+\n+    try {\n+      for (int i = 0; i < spills.length; i++) {\n+        spillInputStreams[i] = new FileInputStream(spills[i].file);\n+      }\n+      for (int partition = 0; partition < numPartitions; partition++) {\n+        final long initialFileLength = outputFile.length();\n+        mergedFileOutputStream =\n+          new TimeTrackingFileOutputStream(writeMetrics, new FileOutputStream(outputFile, true));\n+        if (compressionCodec != null) {\n+          mergedFileOutputStream = compressionCodec.compressedOutputStream(mergedFileOutputStream);\n+        }\n+\n+        for (int i = 0; i < spills.length; i++) {\n+          final long partitionLengthInSpill = spills[i].partitionLengths[partition];\n+          if (partitionLengthInSpill > 0) {\n+            InputStream partitionInputStream =\n+              new LimitedInputStream(spillInputStreams[i], partitionLengthInSpill);\n+            if (compressionCodec != null) {\n+              partitionInputStream = compressionCodec.compressedInputStream(partitionInputStream);\n+            }\n+            ByteStreams.copy(partitionInputStream, mergedFileOutputStream);\n+          }\n+        }\n+        mergedFileOutputStream.flush();\n+        mergedFileOutputStream.close();\n+        partitionLengths[partition] = (outputFile.length() - initialFileLength);\n+      }\n+    } finally {\n+      for (InputStream stream : spillInputStreams) {\n+        Closeables.close(stream, false);\n+      }\n+      Closeables.close(mergedFileOutputStream, false);\n+    }\n+    return partitionLengths;\n+  }\n+\n+  private long[] mergeSpillsWithTransferTo(SpillInfo[] spills, File outputFile) throws IOException {\n+    final int numPartitions = partitioner.numPartitions();\n+    final long[] partitionLengths = new long[numPartitions];\n+    final FileChannel[] spillInputChannels = new FileChannel[spills.length];\n+    final long[] spillInputChannelPositions = new long[spills.length];\n+    FileChannel mergedFileOutputChannel = null;\n+\n+    try {\n+      for (int i = 0; i < spills.length; i++) {\n+        spillInputChannels[i] = new FileInputStream(spills[i].file).getChannel();\n+      }\n+      // This file needs to opened in append mode in order to work around a Linux kernel bug that\n+      // affects transferTo; see SPARK-3948 for more details.\n+      mergedFileOutputChannel = new FileOutputStream(outputFile, true).getChannel();\n+\n+      long bytesWrittenToMergedFile = 0;\n+      for (int partition = 0; partition < numPartitions; partition++) {\n+        for (int i = 0; i < spills.length; i++) {\n+          final long partitionLengthInSpill = spills[i].partitionLengths[partition];\n+          long bytesToTransfer = partitionLengthInSpill;\n+          final FileChannel spillInputChannel = spillInputChannels[i];\n+          final long writeStartTime = System.nanoTime();\n+          while (bytesToTransfer > 0) {\n+            final long actualBytesTransferred = spillInputChannel.transferTo(\n+              spillInputChannelPositions[i],\n+              bytesToTransfer,\n+              mergedFileOutputChannel);\n+            spillInputChannelPositions[i] += actualBytesTransferred;\n+            bytesToTransfer -= actualBytesTransferred;\n+          }\n+          writeMetrics.incShuffleWriteTime(System.nanoTime() - writeStartTime);\n+          bytesWrittenToMergedFile += partitionLengthInSpill;\n+          partitionLengths[partition] += partitionLengthInSpill;\n+        }\n+      }\n+      // Check the position after transferTo loop to see if it is in the right position and raise an\n+      // exception if it is incorrect. The position will not be increased to the expected length\n+      // after calling transferTo in kernel version 2.6.32. This issue is described at\n+      // https://bugs.openjdk.java.net/browse/JDK-7052359 and SPARK-3948.\n+      if (mergedFileOutputChannel.position() != bytesWrittenToMergedFile) {\n+        throw new IOException(\n+          \"Current position \" + mergedFileOutputChannel.position() + \" does not equal expected \" +\n+            \"position \" + bytesWrittenToMergedFile + \" after transferTo. Please check your kernel\" +\n+            \" version to see if it is 2.6.32, as there is a kernel bug which will lead to \" +\n+            \"unexpected behavior when using transferTo. You can set spark.file.transferTo=false \" +\n+            \"to disable this NIO feature.\"\n+        );\n+      }\n+    } finally {\n+      for (int i = 0; i < spills.length; i++) {\n+        assert(spillInputChannelPositions[i] == spills[i].file.length());\n+        Closeables.close(spillInputChannels[i], false);"
  }],
  "prId": 5868
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "adding a short comment to this would be great\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-12T07:39:25Z",
    "diffHunk": "@@ -0,0 +1,407 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.*;\n+import java.nio.channels.FileChannel;\n+import java.util.Iterator;\n+import javax.annotation.Nullable;\n+\n+import scala.Option;\n+import scala.Product2;\n+import scala.collection.JavaConversions;\n+import scala.reflect.ClassTag;\n+import scala.reflect.ClassTag$;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.io.ByteStreams;\n+import com.google.common.io.Closeables;\n+import com.google.common.io.Files;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.*;\n+import org.apache.spark.io.CompressionCodec;\n+import org.apache.spark.io.CompressionCodec$;\n+import org.apache.spark.io.LZFCompressionCodec;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.network.util.LimitedInputStream;\n+import org.apache.spark.scheduler.MapStatus;\n+import org.apache.spark.scheduler.MapStatus$;\n+import org.apache.spark.serializer.SerializationStream;\n+import org.apache.spark.serializer.Serializer;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.IndexShuffleBlockResolver;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.shuffle.ShuffleWriter;\n+import org.apache.spark.storage.BlockManager;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+public class UnsafeShuffleWriter<K, V> extends ShuffleWriter<K, V> {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleWriter.class);\n+\n+  private static final ClassTag<Object> OBJECT_CLASS_TAG = ClassTag$.MODULE$.Object();\n+\n+  @VisibleForTesting\n+  static final int INITIAL_SORT_BUFFER_SIZE = 4096;\n+\n+  private final BlockManager blockManager;\n+  private final IndexShuffleBlockResolver shuffleBlockResolver;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final SerializerInstance serializer;\n+  private final Partitioner partitioner;\n+  private final ShuffleWriteMetrics writeMetrics;\n+  private final int shuffleId;\n+  private final int mapId;\n+  private final TaskContext taskContext;\n+  private final SparkConf sparkConf;\n+  private final boolean transferToEnabled;\n+\n+  private MapStatus mapStatus = null;\n+  private UnsafeShuffleExternalSorter sorter = null;\n+\n+  /** Subclass of ByteArrayOutputStream that exposes `buf` directly. */\n+  private static final class MyByteArrayOutputStream extends ByteArrayOutputStream {\n+    public MyByteArrayOutputStream(int size) { super(size); }\n+    public byte[] getBuf() { return buf; }\n+  }\n+\n+  private MyByteArrayOutputStream serBuffer;\n+  private SerializationStream serOutputStream;\n+\n+  /**\n+   * Are we in the process of stopping? Because map tasks can call stop() with success = true\n+   * and then call stop() with success = false if they get an exception, we want to make sure\n+   * we don't try deleting files, etc twice.\n+   */\n+  private boolean stopping = false;\n+\n+  public UnsafeShuffleWriter(\n+      BlockManager blockManager,\n+      IndexShuffleBlockResolver shuffleBlockResolver,\n+      TaskMemoryManager memoryManager,\n+      ShuffleMemoryManager shuffleMemoryManager,\n+      UnsafeShuffleHandle<K, V> handle,\n+      int mapId,\n+      TaskContext taskContext,\n+      SparkConf sparkConf) {\n+    final int numPartitions = handle.dependency().partitioner().numPartitions();\n+    if (numPartitions > PackedRecordPointer.MAXIMUM_PARTITION_ID) {\n+      throw new IllegalArgumentException(\n+        \"UnsafeShuffleWriter can only be used for shuffles with at most \" +\n+          PackedRecordPointer.MAXIMUM_PARTITION_ID + \" reduce partitions\");\n+    }\n+    this.blockManager = blockManager;\n+    this.shuffleBlockResolver = shuffleBlockResolver;\n+    this.memoryManager = memoryManager;\n+    this.shuffleMemoryManager = shuffleMemoryManager;\n+    this.mapId = mapId;\n+    final ShuffleDependency<K, V, V> dep = handle.dependency();\n+    this.shuffleId = dep.shuffleId();\n+    this.serializer = Serializer.getSerializer(dep.serializer()).newInstance();\n+    this.partitioner = dep.partitioner();\n+    this.writeMetrics = new ShuffleWriteMetrics();\n+    taskContext.taskMetrics().shuffleWriteMetrics_$eq(Option.apply(writeMetrics));\n+    this.taskContext = taskContext;\n+    this.sparkConf = sparkConf;\n+    this.transferToEnabled = sparkConf.getBoolean(\"spark.file.transferTo\", true);\n+  }\n+\n+  public void write(Iterator<Product2<K, V>> records) throws IOException {\n+    write(JavaConversions.asScalaIterator(records));\n+  }\n+\n+  @Override\n+  public void write(scala.collection.Iterator<Product2<K, V>> records) throws IOException {\n+    try {\n+      while (records.hasNext()) {\n+        insertRecordIntoSorter(records.next());\n+      }\n+      closeAndWriteOutput();\n+    } catch (Exception e) {\n+      // Unfortunately, we have to catch Exception here in order to ensure proper cleanup after\n+      // errors because Spark's Scala code, or users' custom Serializers, might throw arbitrary\n+      // unchecked exceptions.\n+      try {\n+        sorter.cleanupAfterError();\n+      } finally {\n+        throw new IOException(\"Error during shuffle write\", e);\n+      }\n+    }\n+  }\n+\n+  private void open() throws IOException {\n+    assert (sorter == null);\n+    sorter = new UnsafeShuffleExternalSorter(\n+      memoryManager,\n+      shuffleMemoryManager,\n+      blockManager,\n+      taskContext,\n+      INITIAL_SORT_BUFFER_SIZE,\n+      partitioner.numPartitions(),\n+      sparkConf,\n+      writeMetrics);\n+    serBuffer = new MyByteArrayOutputStream(1024 * 1024);\n+    serOutputStream = serializer.serializeStream(serBuffer);\n+  }\n+\n+  @VisibleForTesting\n+  void closeAndWriteOutput() throws IOException {\n+    if (sorter == null) {\n+      open();\n+    }\n+    serBuffer = null;\n+    serOutputStream = null;\n+    final SpillInfo[] spills = sorter.closeAndGetSpills();\n+    sorter = null;\n+    final long[] partitionLengths;\n+    try {\n+      partitionLengths = mergeSpills(spills);\n+    } finally {\n+      for (SpillInfo spill : spills) {\n+        if (spill.file.exists() && ! spill.file.delete()) {\n+          logger.error(\"Error while deleting spill file {}\", spill.file.getPath());\n+        }\n+      }\n+    }\n+    shuffleBlockResolver.writeIndexFile(shuffleId, mapId, partitionLengths);\n+    mapStatus = MapStatus$.MODULE$.apply(blockManager.shuffleServerId(), partitionLengths);\n+  }\n+\n+  @VisibleForTesting\n+  void insertRecordIntoSorter(Product2<K, V> record) throws IOException{\n+    if (sorter == null) {\n+      open();\n+    }\n+    final K key = record._1();\n+    final int partitionId = partitioner.getPartition(key);\n+    serBuffer.reset();\n+    serOutputStream.writeKey(key, OBJECT_CLASS_TAG);\n+    serOutputStream.writeValue(record._2(), OBJECT_CLASS_TAG);\n+    serOutputStream.flush();\n+\n+    final int serializedRecordSize = serBuffer.size();\n+    assert (serializedRecordSize > 0);\n+\n+    sorter.insertRecord(\n+      serBuffer.getBuf(), PlatformDependent.BYTE_ARRAY_OFFSET, serializedRecordSize, partitionId);\n+  }\n+\n+  @VisibleForTesting\n+  void forceSorterToSpill() throws IOException {\n+    assert (sorter != null);\n+    sorter.spill();\n+  }\n+\n+  /**\n+   * Merge zero or more spill files together, choosing the fastest merging strategy based on the\n+   * number of spills and the IO compression codec.\n+   *\n+   * @return the partition lengths in the merged file.\n+   */\n+  private long[] mergeSpills(SpillInfo[] spills) throws IOException {\n+    final File outputFile = shuffleBlockResolver.getDataFile(shuffleId, mapId);\n+    final boolean compressionEnabled = sparkConf.getBoolean(\"spark.shuffle.compress\", true);\n+    final CompressionCodec compressionCodec = CompressionCodec$.MODULE$.createCodec(sparkConf);\n+    final boolean fastMergeEnabled =\n+      sparkConf.getBoolean(\"spark.shuffle.unsafe.fastMergeEnabled\", true);\n+    final boolean fastMergeIsSupported =\n+      !compressionEnabled || compressionCodec instanceof LZFCompressionCodec;\n+    try {\n+      if (spills.length == 0) {\n+        new FileOutputStream(outputFile).close(); // Create an empty file\n+        return new long[partitioner.numPartitions()];\n+      } else if (spills.length == 1) {\n+        // Here, we don't need to perform any metrics updates because the bytes written to this\n+        // output file would have already been counted as shuffle bytes written.\n+        Files.move(spills[0].file, outputFile);\n+        return spills[0].partitionLengths;\n+      } else {\n+        final long[] partitionLengths;\n+        // There are multiple spills to merge, so none of these spill files' lengths were counted\n+        // towards our shuffle write count or shuffle write time. If we use the slow merge path,\n+        // then the final output file's size won't necessarily be equal to the sum of the spill\n+        // files' sizes. To guard against this case, we look at the output file's actual size when\n+        // computing shuffle bytes written.\n+        //\n+        // We allow the individual merge methods to report their own IO times since different merge\n+        // strategies use different IO techniques.  We count IO during merge towards the shuffle\n+        // shuffle write time, which appears to be consistent with the \"not bypassing merge-sort\"\n+        // branch in ExternalSorter.\n+        if (fastMergeEnabled && fastMergeIsSupported) {\n+          // Compression is disabled or we are using an IO compression codec that supports\n+          // decompression of concatenated compressed streams, so we can perform a fast spill merge\n+          // that doesn't need to interpret the spilled bytes.\n+          if (transferToEnabled) {\n+            logger.debug(\"Using transferTo-based fast merge\");\n+            partitionLengths = mergeSpillsWithTransferTo(spills, outputFile);\n+          } else {\n+            logger.debug(\"Using fileStream-based fast merge\");\n+            partitionLengths = mergeSpillsWithFileStream(spills, outputFile, null);\n+          }\n+        } else {\n+          logger.debug(\"Using slow merge\");\n+          partitionLengths = mergeSpillsWithFileStream(spills, outputFile, compressionCodec);\n+        }\n+        // The final shuffle spill's write would have directly updated shuffleBytesWritten, so\n+        // we need to decrement to avoid double-counting this write.\n+        writeMetrics.decShuffleBytesWritten(spills[spills.length - 1].file.length());\n+        writeMetrics.incShuffleBytesWritten(outputFile.length());\n+        return partitionLengths;\n+      }\n+    } catch (IOException e) {\n+      if (outputFile.exists() && !outputFile.delete()) {\n+        logger.error(\"Unable to delete output file {}\", outputFile.getPath());\n+      }\n+      throw e;\n+    }\n+  }\n+\n+  private long[] mergeSpillsWithFileStream(\n+      SpillInfo[] spills,\n+      File outputFile,\n+      @Nullable CompressionCodec compressionCodec) throws IOException {\n+    final int numPartitions = partitioner.numPartitions();\n+    final long[] partitionLengths = new long[numPartitions];\n+    final InputStream[] spillInputStreams = new FileInputStream[spills.length];\n+    OutputStream mergedFileOutputStream = null;\n+\n+    try {\n+      for (int i = 0; i < spills.length; i++) {\n+        spillInputStreams[i] = new FileInputStream(spills[i].file);\n+      }\n+      for (int partition = 0; partition < numPartitions; partition++) {\n+        final long initialFileLength = outputFile.length();\n+        mergedFileOutputStream =\n+          new TimeTrackingFileOutputStream(writeMetrics, new FileOutputStream(outputFile, true));\n+        if (compressionCodec != null) {\n+          mergedFileOutputStream = compressionCodec.compressedOutputStream(mergedFileOutputStream);\n+        }\n+\n+        for (int i = 0; i < spills.length; i++) {\n+          final long partitionLengthInSpill = spills[i].partitionLengths[partition];\n+          if (partitionLengthInSpill > 0) {\n+            InputStream partitionInputStream =\n+              new LimitedInputStream(spillInputStreams[i], partitionLengthInSpill);\n+            if (compressionCodec != null) {\n+              partitionInputStream = compressionCodec.compressedInputStream(partitionInputStream);\n+            }\n+            ByteStreams.copy(partitionInputStream, mergedFileOutputStream);\n+          }\n+        }\n+        mergedFileOutputStream.flush();\n+        mergedFileOutputStream.close();\n+        partitionLengths[partition] = (outputFile.length() - initialFileLength);\n+      }\n+    } finally {\n+      for (InputStream stream : spillInputStreams) {\n+        Closeables.close(stream, false);\n+      }\n+      Closeables.close(mergedFileOutputStream, false);\n+    }\n+    return partitionLengths;\n+  }\n+\n+  private long[] mergeSpillsWithTransferTo(SpillInfo[] spills, File outputFile) throws IOException {"
  }],
  "prId": 5868
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "adding a short comment to this would be great\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-12T07:39:28Z",
    "diffHunk": "@@ -0,0 +1,407 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.*;\n+import java.nio.channels.FileChannel;\n+import java.util.Iterator;\n+import javax.annotation.Nullable;\n+\n+import scala.Option;\n+import scala.Product2;\n+import scala.collection.JavaConversions;\n+import scala.reflect.ClassTag;\n+import scala.reflect.ClassTag$;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.io.ByteStreams;\n+import com.google.common.io.Closeables;\n+import com.google.common.io.Files;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.*;\n+import org.apache.spark.io.CompressionCodec;\n+import org.apache.spark.io.CompressionCodec$;\n+import org.apache.spark.io.LZFCompressionCodec;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.network.util.LimitedInputStream;\n+import org.apache.spark.scheduler.MapStatus;\n+import org.apache.spark.scheduler.MapStatus$;\n+import org.apache.spark.serializer.SerializationStream;\n+import org.apache.spark.serializer.Serializer;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.IndexShuffleBlockResolver;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.shuffle.ShuffleWriter;\n+import org.apache.spark.storage.BlockManager;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+public class UnsafeShuffleWriter<K, V> extends ShuffleWriter<K, V> {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleWriter.class);\n+\n+  private static final ClassTag<Object> OBJECT_CLASS_TAG = ClassTag$.MODULE$.Object();\n+\n+  @VisibleForTesting\n+  static final int INITIAL_SORT_BUFFER_SIZE = 4096;\n+\n+  private final BlockManager blockManager;\n+  private final IndexShuffleBlockResolver shuffleBlockResolver;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final SerializerInstance serializer;\n+  private final Partitioner partitioner;\n+  private final ShuffleWriteMetrics writeMetrics;\n+  private final int shuffleId;\n+  private final int mapId;\n+  private final TaskContext taskContext;\n+  private final SparkConf sparkConf;\n+  private final boolean transferToEnabled;\n+\n+  private MapStatus mapStatus = null;\n+  private UnsafeShuffleExternalSorter sorter = null;\n+\n+  /** Subclass of ByteArrayOutputStream that exposes `buf` directly. */\n+  private static final class MyByteArrayOutputStream extends ByteArrayOutputStream {\n+    public MyByteArrayOutputStream(int size) { super(size); }\n+    public byte[] getBuf() { return buf; }\n+  }\n+\n+  private MyByteArrayOutputStream serBuffer;\n+  private SerializationStream serOutputStream;\n+\n+  /**\n+   * Are we in the process of stopping? Because map tasks can call stop() with success = true\n+   * and then call stop() with success = false if they get an exception, we want to make sure\n+   * we don't try deleting files, etc twice.\n+   */\n+  private boolean stopping = false;\n+\n+  public UnsafeShuffleWriter(\n+      BlockManager blockManager,\n+      IndexShuffleBlockResolver shuffleBlockResolver,\n+      TaskMemoryManager memoryManager,\n+      ShuffleMemoryManager shuffleMemoryManager,\n+      UnsafeShuffleHandle<K, V> handle,\n+      int mapId,\n+      TaskContext taskContext,\n+      SparkConf sparkConf) {\n+    final int numPartitions = handle.dependency().partitioner().numPartitions();\n+    if (numPartitions > PackedRecordPointer.MAXIMUM_PARTITION_ID) {\n+      throw new IllegalArgumentException(\n+        \"UnsafeShuffleWriter can only be used for shuffles with at most \" +\n+          PackedRecordPointer.MAXIMUM_PARTITION_ID + \" reduce partitions\");\n+    }\n+    this.blockManager = blockManager;\n+    this.shuffleBlockResolver = shuffleBlockResolver;\n+    this.memoryManager = memoryManager;\n+    this.shuffleMemoryManager = shuffleMemoryManager;\n+    this.mapId = mapId;\n+    final ShuffleDependency<K, V, V> dep = handle.dependency();\n+    this.shuffleId = dep.shuffleId();\n+    this.serializer = Serializer.getSerializer(dep.serializer()).newInstance();\n+    this.partitioner = dep.partitioner();\n+    this.writeMetrics = new ShuffleWriteMetrics();\n+    taskContext.taskMetrics().shuffleWriteMetrics_$eq(Option.apply(writeMetrics));\n+    this.taskContext = taskContext;\n+    this.sparkConf = sparkConf;\n+    this.transferToEnabled = sparkConf.getBoolean(\"spark.file.transferTo\", true);\n+  }\n+\n+  public void write(Iterator<Product2<K, V>> records) throws IOException {\n+    write(JavaConversions.asScalaIterator(records));\n+  }\n+\n+  @Override\n+  public void write(scala.collection.Iterator<Product2<K, V>> records) throws IOException {\n+    try {\n+      while (records.hasNext()) {\n+        insertRecordIntoSorter(records.next());\n+      }\n+      closeAndWriteOutput();\n+    } catch (Exception e) {\n+      // Unfortunately, we have to catch Exception here in order to ensure proper cleanup after\n+      // errors because Spark's Scala code, or users' custom Serializers, might throw arbitrary\n+      // unchecked exceptions.\n+      try {\n+        sorter.cleanupAfterError();\n+      } finally {\n+        throw new IOException(\"Error during shuffle write\", e);\n+      }\n+    }\n+  }\n+\n+  private void open() throws IOException {\n+    assert (sorter == null);\n+    sorter = new UnsafeShuffleExternalSorter(\n+      memoryManager,\n+      shuffleMemoryManager,\n+      blockManager,\n+      taskContext,\n+      INITIAL_SORT_BUFFER_SIZE,\n+      partitioner.numPartitions(),\n+      sparkConf,\n+      writeMetrics);\n+    serBuffer = new MyByteArrayOutputStream(1024 * 1024);\n+    serOutputStream = serializer.serializeStream(serBuffer);\n+  }\n+\n+  @VisibleForTesting\n+  void closeAndWriteOutput() throws IOException {\n+    if (sorter == null) {\n+      open();\n+    }\n+    serBuffer = null;\n+    serOutputStream = null;\n+    final SpillInfo[] spills = sorter.closeAndGetSpills();\n+    sorter = null;\n+    final long[] partitionLengths;\n+    try {\n+      partitionLengths = mergeSpills(spills);\n+    } finally {\n+      for (SpillInfo spill : spills) {\n+        if (spill.file.exists() && ! spill.file.delete()) {\n+          logger.error(\"Error while deleting spill file {}\", spill.file.getPath());\n+        }\n+      }\n+    }\n+    shuffleBlockResolver.writeIndexFile(shuffleId, mapId, partitionLengths);\n+    mapStatus = MapStatus$.MODULE$.apply(blockManager.shuffleServerId(), partitionLengths);\n+  }\n+\n+  @VisibleForTesting\n+  void insertRecordIntoSorter(Product2<K, V> record) throws IOException{\n+    if (sorter == null) {\n+      open();\n+    }\n+    final K key = record._1();\n+    final int partitionId = partitioner.getPartition(key);\n+    serBuffer.reset();\n+    serOutputStream.writeKey(key, OBJECT_CLASS_TAG);\n+    serOutputStream.writeValue(record._2(), OBJECT_CLASS_TAG);\n+    serOutputStream.flush();\n+\n+    final int serializedRecordSize = serBuffer.size();\n+    assert (serializedRecordSize > 0);\n+\n+    sorter.insertRecord(\n+      serBuffer.getBuf(), PlatformDependent.BYTE_ARRAY_OFFSET, serializedRecordSize, partitionId);\n+  }\n+\n+  @VisibleForTesting\n+  void forceSorterToSpill() throws IOException {\n+    assert (sorter != null);\n+    sorter.spill();\n+  }\n+\n+  /**\n+   * Merge zero or more spill files together, choosing the fastest merging strategy based on the\n+   * number of spills and the IO compression codec.\n+   *\n+   * @return the partition lengths in the merged file.\n+   */\n+  private long[] mergeSpills(SpillInfo[] spills) throws IOException {\n+    final File outputFile = shuffleBlockResolver.getDataFile(shuffleId, mapId);\n+    final boolean compressionEnabled = sparkConf.getBoolean(\"spark.shuffle.compress\", true);\n+    final CompressionCodec compressionCodec = CompressionCodec$.MODULE$.createCodec(sparkConf);\n+    final boolean fastMergeEnabled =\n+      sparkConf.getBoolean(\"spark.shuffle.unsafe.fastMergeEnabled\", true);\n+    final boolean fastMergeIsSupported =\n+      !compressionEnabled || compressionCodec instanceof LZFCompressionCodec;\n+    try {\n+      if (spills.length == 0) {\n+        new FileOutputStream(outputFile).close(); // Create an empty file\n+        return new long[partitioner.numPartitions()];\n+      } else if (spills.length == 1) {\n+        // Here, we don't need to perform any metrics updates because the bytes written to this\n+        // output file would have already been counted as shuffle bytes written.\n+        Files.move(spills[0].file, outputFile);\n+        return spills[0].partitionLengths;\n+      } else {\n+        final long[] partitionLengths;\n+        // There are multiple spills to merge, so none of these spill files' lengths were counted\n+        // towards our shuffle write count or shuffle write time. If we use the slow merge path,\n+        // then the final output file's size won't necessarily be equal to the sum of the spill\n+        // files' sizes. To guard against this case, we look at the output file's actual size when\n+        // computing shuffle bytes written.\n+        //\n+        // We allow the individual merge methods to report their own IO times since different merge\n+        // strategies use different IO techniques.  We count IO during merge towards the shuffle\n+        // shuffle write time, which appears to be consistent with the \"not bypassing merge-sort\"\n+        // branch in ExternalSorter.\n+        if (fastMergeEnabled && fastMergeIsSupported) {\n+          // Compression is disabled or we are using an IO compression codec that supports\n+          // decompression of concatenated compressed streams, so we can perform a fast spill merge\n+          // that doesn't need to interpret the spilled bytes.\n+          if (transferToEnabled) {\n+            logger.debug(\"Using transferTo-based fast merge\");\n+            partitionLengths = mergeSpillsWithTransferTo(spills, outputFile);\n+          } else {\n+            logger.debug(\"Using fileStream-based fast merge\");\n+            partitionLengths = mergeSpillsWithFileStream(spills, outputFile, null);\n+          }\n+        } else {\n+          logger.debug(\"Using slow merge\");\n+          partitionLengths = mergeSpillsWithFileStream(spills, outputFile, compressionCodec);\n+        }\n+        // The final shuffle spill's write would have directly updated shuffleBytesWritten, so\n+        // we need to decrement to avoid double-counting this write.\n+        writeMetrics.decShuffleBytesWritten(spills[spills.length - 1].file.length());\n+        writeMetrics.incShuffleBytesWritten(outputFile.length());\n+        return partitionLengths;\n+      }\n+    } catch (IOException e) {\n+      if (outputFile.exists() && !outputFile.delete()) {\n+        logger.error(\"Unable to delete output file {}\", outputFile.getPath());\n+      }\n+      throw e;\n+    }\n+  }\n+\n+  private long[] mergeSpillsWithFileStream("
  }],
  "prId": 5868
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "A bit confused. So the size of the last file won't be counted as DiskBytesSpilled nor ShuffleRecordsWritten? \n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-13T23:07:28Z",
    "diffHunk": "@@ -0,0 +1,438 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.*;\n+import java.nio.channels.FileChannel;\n+import java.util.Iterator;\n+import javax.annotation.Nullable;\n+\n+import scala.Option;\n+import scala.Product2;\n+import scala.collection.JavaConversions;\n+import scala.reflect.ClassTag;\n+import scala.reflect.ClassTag$;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.io.ByteStreams;\n+import com.google.common.io.Closeables;\n+import com.google.common.io.Files;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.*;\n+import org.apache.spark.annotation.Private;\n+import org.apache.spark.io.CompressionCodec;\n+import org.apache.spark.io.CompressionCodec$;\n+import org.apache.spark.io.LZFCompressionCodec;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.network.util.LimitedInputStream;\n+import org.apache.spark.scheduler.MapStatus;\n+import org.apache.spark.scheduler.MapStatus$;\n+import org.apache.spark.serializer.SerializationStream;\n+import org.apache.spark.serializer.Serializer;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.IndexShuffleBlockResolver;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.shuffle.ShuffleWriter;\n+import org.apache.spark.storage.BlockManager;\n+import org.apache.spark.storage.TimeTrackingOutputStream;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+@Private\n+public class UnsafeShuffleWriter<K, V> extends ShuffleWriter<K, V> {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleWriter.class);\n+\n+  private static final ClassTag<Object> OBJECT_CLASS_TAG = ClassTag$.MODULE$.Object();\n+\n+  @VisibleForTesting\n+  static final int INITIAL_SORT_BUFFER_SIZE = 4096;\n+\n+  private final BlockManager blockManager;\n+  private final IndexShuffleBlockResolver shuffleBlockResolver;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final SerializerInstance serializer;\n+  private final Partitioner partitioner;\n+  private final ShuffleWriteMetrics writeMetrics;\n+  private final int shuffleId;\n+  private final int mapId;\n+  private final TaskContext taskContext;\n+  private final SparkConf sparkConf;\n+  private final boolean transferToEnabled;\n+\n+  private MapStatus mapStatus = null;\n+  private UnsafeShuffleExternalSorter sorter = null;\n+\n+  /** Subclass of ByteArrayOutputStream that exposes `buf` directly. */\n+  private static final class MyByteArrayOutputStream extends ByteArrayOutputStream {\n+    public MyByteArrayOutputStream(int size) { super(size); }\n+    public byte[] getBuf() { return buf; }\n+  }\n+\n+  private MyByteArrayOutputStream serBuffer;\n+  private SerializationStream serOutputStream;\n+\n+  /**\n+   * Are we in the process of stopping? Because map tasks can call stop() with success = true\n+   * and then call stop() with success = false if they get an exception, we want to make sure\n+   * we don't try deleting files, etc twice.\n+   */\n+  private boolean stopping = false;\n+\n+  public UnsafeShuffleWriter(\n+      BlockManager blockManager,\n+      IndexShuffleBlockResolver shuffleBlockResolver,\n+      TaskMemoryManager memoryManager,\n+      ShuffleMemoryManager shuffleMemoryManager,\n+      UnsafeShuffleHandle<K, V> handle,\n+      int mapId,\n+      TaskContext taskContext,\n+      SparkConf sparkConf) throws IOException {\n+    final int numPartitions = handle.dependency().partitioner().numPartitions();\n+    if (numPartitions > UnsafeShuffleManager.MAX_SHUFFLE_OUTPUT_PARTITIONS()) {\n+      throw new IllegalArgumentException(\n+        \"UnsafeShuffleWriter can only be used for shuffles with at most \" +\n+          UnsafeShuffleManager.MAX_SHUFFLE_OUTPUT_PARTITIONS() + \" reduce partitions\");\n+    }\n+    this.blockManager = blockManager;\n+    this.shuffleBlockResolver = shuffleBlockResolver;\n+    this.memoryManager = memoryManager;\n+    this.shuffleMemoryManager = shuffleMemoryManager;\n+    this.mapId = mapId;\n+    final ShuffleDependency<K, V, V> dep = handle.dependency();\n+    this.shuffleId = dep.shuffleId();\n+    this.serializer = Serializer.getSerializer(dep.serializer()).newInstance();\n+    this.partitioner = dep.partitioner();\n+    this.writeMetrics = new ShuffleWriteMetrics();\n+    taskContext.taskMetrics().shuffleWriteMetrics_$eq(Option.apply(writeMetrics));\n+    this.taskContext = taskContext;\n+    this.sparkConf = sparkConf;\n+    this.transferToEnabled = sparkConf.getBoolean(\"spark.file.transferTo\", true);\n+    open();\n+  }\n+\n+  /**\n+   * This convenience method should only be called in test code.\n+   */\n+  @VisibleForTesting\n+  public void write(Iterator<Product2<K, V>> records) throws IOException {\n+    write(JavaConversions.asScalaIterator(records));\n+  }\n+\n+  @Override\n+  public void write(scala.collection.Iterator<Product2<K, V>> records) throws IOException {\n+    boolean success = false;\n+    try {\n+      while (records.hasNext()) {\n+        insertRecordIntoSorter(records.next());\n+      }\n+      closeAndWriteOutput();\n+      success = true;\n+    } finally {\n+      if (!success) {\n+        sorter.cleanupAfterError();\n+      }\n+    }\n+  }\n+\n+  private void open() throws IOException {\n+    assert (sorter == null);\n+    sorter = new UnsafeShuffleExternalSorter(\n+      memoryManager,\n+      shuffleMemoryManager,\n+      blockManager,\n+      taskContext,\n+      INITIAL_SORT_BUFFER_SIZE,\n+      partitioner.numPartitions(),\n+      sparkConf,\n+      writeMetrics);\n+    serBuffer = new MyByteArrayOutputStream(1024 * 1024);\n+    serOutputStream = serializer.serializeStream(serBuffer);\n+  }\n+\n+  @VisibleForTesting\n+  void closeAndWriteOutput() throws IOException {\n+    serBuffer = null;\n+    serOutputStream = null;\n+    final SpillInfo[] spills = sorter.closeAndGetSpills();\n+    sorter = null;\n+    final long[] partitionLengths;\n+    try {\n+      partitionLengths = mergeSpills(spills);\n+    } finally {\n+      for (SpillInfo spill : spills) {\n+        if (spill.file.exists() && ! spill.file.delete()) {\n+          logger.error(\"Error while deleting spill file {}\", spill.file.getPath());\n+        }\n+      }\n+    }\n+    shuffleBlockResolver.writeIndexFile(shuffleId, mapId, partitionLengths);\n+    mapStatus = MapStatus$.MODULE$.apply(blockManager.shuffleServerId(), partitionLengths);\n+  }\n+\n+  @VisibleForTesting\n+  void insertRecordIntoSorter(Product2<K, V> record) throws IOException {\n+    final K key = record._1();\n+    final int partitionId = partitioner.getPartition(key);\n+    serBuffer.reset();\n+    serOutputStream.writeKey(key, OBJECT_CLASS_TAG);\n+    serOutputStream.writeValue(record._2(), OBJECT_CLASS_TAG);\n+    serOutputStream.flush();\n+\n+    final int serializedRecordSize = serBuffer.size();\n+    assert (serializedRecordSize > 0);\n+\n+    sorter.insertRecord(\n+      serBuffer.getBuf(), PlatformDependent.BYTE_ARRAY_OFFSET, serializedRecordSize, partitionId);\n+  }\n+\n+  @VisibleForTesting\n+  void forceSorterToSpill() throws IOException {\n+    assert (sorter != null);\n+    sorter.spill();\n+  }\n+\n+  /**\n+   * Merge zero or more spill files together, choosing the fastest merging strategy based on the\n+   * number of spills and the IO compression codec.\n+   *\n+   * @return the partition lengths in the merged file.\n+   */\n+  private long[] mergeSpills(SpillInfo[] spills) throws IOException {\n+    final File outputFile = shuffleBlockResolver.getDataFile(shuffleId, mapId);\n+    final boolean compressionEnabled = sparkConf.getBoolean(\"spark.shuffle.compress\", true);\n+    final CompressionCodec compressionCodec = CompressionCodec$.MODULE$.createCodec(sparkConf);\n+    final boolean fastMergeEnabled =\n+      sparkConf.getBoolean(\"spark.shuffle.unsafe.fastMergeEnabled\", true);\n+    final boolean fastMergeIsSupported =\n+      !compressionEnabled || compressionCodec instanceof LZFCompressionCodec;\n+    try {\n+      if (spills.length == 0) {\n+        new FileOutputStream(outputFile).close(); // Create an empty file\n+        return new long[partitioner.numPartitions()];\n+      } else if (spills.length == 1) {\n+        // Here, we don't need to perform any metrics updates because the bytes written to this\n+        // output file would have already been counted as shuffle bytes written.\n+        Files.move(spills[0].file, outputFile);\n+        return spills[0].partitionLengths;\n+      } else {\n+        final long[] partitionLengths;\n+        // There are multiple spills to merge, so none of these spill files' lengths were counted\n+        // towards our shuffle write count or shuffle write time. If we use the slow merge path,\n+        // then the final output file's size won't necessarily be equal to the sum of the spill\n+        // files' sizes. To guard against this case, we look at the output file's actual size when\n+        // computing shuffle bytes written.\n+        //\n+        // We allow the individual merge methods to report their own IO times since different merge\n+        // strategies use different IO techniques.  We count IO during merge towards the shuffle\n+        // shuffle write time, which appears to be consistent with the \"not bypassing merge-sort\"\n+        // branch in ExternalSorter.\n+        if (fastMergeEnabled && fastMergeIsSupported) {\n+          // Compression is disabled or we are using an IO compression codec that supports\n+          // decompression of concatenated compressed streams, so we can perform a fast spill merge\n+          // that doesn't need to interpret the spilled bytes.\n+          if (transferToEnabled) {\n+            logger.debug(\"Using transferTo-based fast merge\");\n+            partitionLengths = mergeSpillsWithTransferTo(spills, outputFile);\n+          } else {\n+            logger.debug(\"Using fileStream-based fast merge\");\n+            partitionLengths = mergeSpillsWithFileStream(spills, outputFile, null);\n+          }\n+        } else {\n+          logger.debug(\"Using slow merge\");\n+          partitionLengths = mergeSpillsWithFileStream(spills, outputFile, compressionCodec);\n+        }\n+        // When closing an UnsafeShuffleExternalSorter that has already spilled once but also has\n+        // in-memory records, we write out the in-memory records to a file but do not count that\n+        // final write as bytes spilled (instead, it's accounted as shuffle write). The merge needs\n+        // to be counted as shuffle write, but this will lead to double-counting of the final\n+        // SpillInfo's bytes.\n+        writeMetrics.decShuffleBytesWritten(spills[spills.length - 1].file.length());",
    "line": 268
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "If control reaches this point, then we have written multiple files to disk in our external sorter.  The last file should not be counted as bytes spilled in order to try to remain consistent with other code.  In the existing sort-merge code, the final in-memory partition is merged against the on-disk partitions and thus does not get counted as extra disk write.  Here, we end up dumping the in-memory partition to disk, incurring a bit of extra write but massively simplifying the merge path code.  It's true that we're performing some disk writes that aren't accounted for anywhere in the final metrics, but I'm not sure how this should be reported / handled. I don't think that it's correct to treat these bytes as spilled bytes, but treating them as shuffle write is also confusing because then we'll end up reporting more shuffle bytes written than are transferred over the network.\n\nThe reason that I decrement the count here is so that the final count of shuffle bytes written is equal to the output file's size.  Alternatively, I could have avoided incrementing shuffleBytesWritten when writing the last file, but that would cause complications when updating record counts.\n\nI'm happy to work on fixes for the metrics, but I'd rather defer that to a separate followup patch that does a broader refactoring of our metrics code.\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-13T23:23:58Z",
    "diffHunk": "@@ -0,0 +1,438 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.*;\n+import java.nio.channels.FileChannel;\n+import java.util.Iterator;\n+import javax.annotation.Nullable;\n+\n+import scala.Option;\n+import scala.Product2;\n+import scala.collection.JavaConversions;\n+import scala.reflect.ClassTag;\n+import scala.reflect.ClassTag$;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.io.ByteStreams;\n+import com.google.common.io.Closeables;\n+import com.google.common.io.Files;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.*;\n+import org.apache.spark.annotation.Private;\n+import org.apache.spark.io.CompressionCodec;\n+import org.apache.spark.io.CompressionCodec$;\n+import org.apache.spark.io.LZFCompressionCodec;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.network.util.LimitedInputStream;\n+import org.apache.spark.scheduler.MapStatus;\n+import org.apache.spark.scheduler.MapStatus$;\n+import org.apache.spark.serializer.SerializationStream;\n+import org.apache.spark.serializer.Serializer;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.IndexShuffleBlockResolver;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.shuffle.ShuffleWriter;\n+import org.apache.spark.storage.BlockManager;\n+import org.apache.spark.storage.TimeTrackingOutputStream;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+@Private\n+public class UnsafeShuffleWriter<K, V> extends ShuffleWriter<K, V> {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleWriter.class);\n+\n+  private static final ClassTag<Object> OBJECT_CLASS_TAG = ClassTag$.MODULE$.Object();\n+\n+  @VisibleForTesting\n+  static final int INITIAL_SORT_BUFFER_SIZE = 4096;\n+\n+  private final BlockManager blockManager;\n+  private final IndexShuffleBlockResolver shuffleBlockResolver;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final SerializerInstance serializer;\n+  private final Partitioner partitioner;\n+  private final ShuffleWriteMetrics writeMetrics;\n+  private final int shuffleId;\n+  private final int mapId;\n+  private final TaskContext taskContext;\n+  private final SparkConf sparkConf;\n+  private final boolean transferToEnabled;\n+\n+  private MapStatus mapStatus = null;\n+  private UnsafeShuffleExternalSorter sorter = null;\n+\n+  /** Subclass of ByteArrayOutputStream that exposes `buf` directly. */\n+  private static final class MyByteArrayOutputStream extends ByteArrayOutputStream {\n+    public MyByteArrayOutputStream(int size) { super(size); }\n+    public byte[] getBuf() { return buf; }\n+  }\n+\n+  private MyByteArrayOutputStream serBuffer;\n+  private SerializationStream serOutputStream;\n+\n+  /**\n+   * Are we in the process of stopping? Because map tasks can call stop() with success = true\n+   * and then call stop() with success = false if they get an exception, we want to make sure\n+   * we don't try deleting files, etc twice.\n+   */\n+  private boolean stopping = false;\n+\n+  public UnsafeShuffleWriter(\n+      BlockManager blockManager,\n+      IndexShuffleBlockResolver shuffleBlockResolver,\n+      TaskMemoryManager memoryManager,\n+      ShuffleMemoryManager shuffleMemoryManager,\n+      UnsafeShuffleHandle<K, V> handle,\n+      int mapId,\n+      TaskContext taskContext,\n+      SparkConf sparkConf) throws IOException {\n+    final int numPartitions = handle.dependency().partitioner().numPartitions();\n+    if (numPartitions > UnsafeShuffleManager.MAX_SHUFFLE_OUTPUT_PARTITIONS()) {\n+      throw new IllegalArgumentException(\n+        \"UnsafeShuffleWriter can only be used for shuffles with at most \" +\n+          UnsafeShuffleManager.MAX_SHUFFLE_OUTPUT_PARTITIONS() + \" reduce partitions\");\n+    }\n+    this.blockManager = blockManager;\n+    this.shuffleBlockResolver = shuffleBlockResolver;\n+    this.memoryManager = memoryManager;\n+    this.shuffleMemoryManager = shuffleMemoryManager;\n+    this.mapId = mapId;\n+    final ShuffleDependency<K, V, V> dep = handle.dependency();\n+    this.shuffleId = dep.shuffleId();\n+    this.serializer = Serializer.getSerializer(dep.serializer()).newInstance();\n+    this.partitioner = dep.partitioner();\n+    this.writeMetrics = new ShuffleWriteMetrics();\n+    taskContext.taskMetrics().shuffleWriteMetrics_$eq(Option.apply(writeMetrics));\n+    this.taskContext = taskContext;\n+    this.sparkConf = sparkConf;\n+    this.transferToEnabled = sparkConf.getBoolean(\"spark.file.transferTo\", true);\n+    open();\n+  }\n+\n+  /**\n+   * This convenience method should only be called in test code.\n+   */\n+  @VisibleForTesting\n+  public void write(Iterator<Product2<K, V>> records) throws IOException {\n+    write(JavaConversions.asScalaIterator(records));\n+  }\n+\n+  @Override\n+  public void write(scala.collection.Iterator<Product2<K, V>> records) throws IOException {\n+    boolean success = false;\n+    try {\n+      while (records.hasNext()) {\n+        insertRecordIntoSorter(records.next());\n+      }\n+      closeAndWriteOutput();\n+      success = true;\n+    } finally {\n+      if (!success) {\n+        sorter.cleanupAfterError();\n+      }\n+    }\n+  }\n+\n+  private void open() throws IOException {\n+    assert (sorter == null);\n+    sorter = new UnsafeShuffleExternalSorter(\n+      memoryManager,\n+      shuffleMemoryManager,\n+      blockManager,\n+      taskContext,\n+      INITIAL_SORT_BUFFER_SIZE,\n+      partitioner.numPartitions(),\n+      sparkConf,\n+      writeMetrics);\n+    serBuffer = new MyByteArrayOutputStream(1024 * 1024);\n+    serOutputStream = serializer.serializeStream(serBuffer);\n+  }\n+\n+  @VisibleForTesting\n+  void closeAndWriteOutput() throws IOException {\n+    serBuffer = null;\n+    serOutputStream = null;\n+    final SpillInfo[] spills = sorter.closeAndGetSpills();\n+    sorter = null;\n+    final long[] partitionLengths;\n+    try {\n+      partitionLengths = mergeSpills(spills);\n+    } finally {\n+      for (SpillInfo spill : spills) {\n+        if (spill.file.exists() && ! spill.file.delete()) {\n+          logger.error(\"Error while deleting spill file {}\", spill.file.getPath());\n+        }\n+      }\n+    }\n+    shuffleBlockResolver.writeIndexFile(shuffleId, mapId, partitionLengths);\n+    mapStatus = MapStatus$.MODULE$.apply(blockManager.shuffleServerId(), partitionLengths);\n+  }\n+\n+  @VisibleForTesting\n+  void insertRecordIntoSorter(Product2<K, V> record) throws IOException {\n+    final K key = record._1();\n+    final int partitionId = partitioner.getPartition(key);\n+    serBuffer.reset();\n+    serOutputStream.writeKey(key, OBJECT_CLASS_TAG);\n+    serOutputStream.writeValue(record._2(), OBJECT_CLASS_TAG);\n+    serOutputStream.flush();\n+\n+    final int serializedRecordSize = serBuffer.size();\n+    assert (serializedRecordSize > 0);\n+\n+    sorter.insertRecord(\n+      serBuffer.getBuf(), PlatformDependent.BYTE_ARRAY_OFFSET, serializedRecordSize, partitionId);\n+  }\n+\n+  @VisibleForTesting\n+  void forceSorterToSpill() throws IOException {\n+    assert (sorter != null);\n+    sorter.spill();\n+  }\n+\n+  /**\n+   * Merge zero or more spill files together, choosing the fastest merging strategy based on the\n+   * number of spills and the IO compression codec.\n+   *\n+   * @return the partition lengths in the merged file.\n+   */\n+  private long[] mergeSpills(SpillInfo[] spills) throws IOException {\n+    final File outputFile = shuffleBlockResolver.getDataFile(shuffleId, mapId);\n+    final boolean compressionEnabled = sparkConf.getBoolean(\"spark.shuffle.compress\", true);\n+    final CompressionCodec compressionCodec = CompressionCodec$.MODULE$.createCodec(sparkConf);\n+    final boolean fastMergeEnabled =\n+      sparkConf.getBoolean(\"spark.shuffle.unsafe.fastMergeEnabled\", true);\n+    final boolean fastMergeIsSupported =\n+      !compressionEnabled || compressionCodec instanceof LZFCompressionCodec;\n+    try {\n+      if (spills.length == 0) {\n+        new FileOutputStream(outputFile).close(); // Create an empty file\n+        return new long[partitioner.numPartitions()];\n+      } else if (spills.length == 1) {\n+        // Here, we don't need to perform any metrics updates because the bytes written to this\n+        // output file would have already been counted as shuffle bytes written.\n+        Files.move(spills[0].file, outputFile);\n+        return spills[0].partitionLengths;\n+      } else {\n+        final long[] partitionLengths;\n+        // There are multiple spills to merge, so none of these spill files' lengths were counted\n+        // towards our shuffle write count or shuffle write time. If we use the slow merge path,\n+        // then the final output file's size won't necessarily be equal to the sum of the spill\n+        // files' sizes. To guard against this case, we look at the output file's actual size when\n+        // computing shuffle bytes written.\n+        //\n+        // We allow the individual merge methods to report their own IO times since different merge\n+        // strategies use different IO techniques.  We count IO during merge towards the shuffle\n+        // shuffle write time, which appears to be consistent with the \"not bypassing merge-sort\"\n+        // branch in ExternalSorter.\n+        if (fastMergeEnabled && fastMergeIsSupported) {\n+          // Compression is disabled or we are using an IO compression codec that supports\n+          // decompression of concatenated compressed streams, so we can perform a fast spill merge\n+          // that doesn't need to interpret the spilled bytes.\n+          if (transferToEnabled) {\n+            logger.debug(\"Using transferTo-based fast merge\");\n+            partitionLengths = mergeSpillsWithTransferTo(spills, outputFile);\n+          } else {\n+            logger.debug(\"Using fileStream-based fast merge\");\n+            partitionLengths = mergeSpillsWithFileStream(spills, outputFile, null);\n+          }\n+        } else {\n+          logger.debug(\"Using slow merge\");\n+          partitionLengths = mergeSpillsWithFileStream(spills, outputFile, compressionCodec);\n+        }\n+        // When closing an UnsafeShuffleExternalSorter that has already spilled once but also has\n+        // in-memory records, we write out the in-memory records to a file but do not count that\n+        // final write as bytes spilled (instead, it's accounted as shuffle write). The merge needs\n+        // to be counted as shuffle write, but this will lead to double-counting of the final\n+        // SpillInfo's bytes.\n+        writeMetrics.decShuffleBytesWritten(spills[spills.length - 1].file.length());",
    "line": 268
  }],
  "prId": 5868
}]