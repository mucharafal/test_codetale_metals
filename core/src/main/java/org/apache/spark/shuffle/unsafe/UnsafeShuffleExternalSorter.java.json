[{
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "According to IntelliJ's code coverage metrics, this branch is never taken in our unit tests.  I'll add a new test to cover this.\n\nThe problem here is that I've been relying too heavily on manual spill triggering, but we should also have tests that exercise automatic spills.\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-11T22:58:34Z",
    "diffHunk": "@@ -0,0 +1,403 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.LinkedList;\n+\n+import scala.Tuple2;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.storage.*;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.MemoryBlock;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+/**\n+ * An external sorter that is specialized for sort-based shuffle.\n+ * <p>\n+ * Incoming records are appended to data pages. When all records have been inserted (or when the\n+ * current thread's shuffle memory limit is reached), the in-memory records are sorted according to\n+ * their partition ids (using a {@link UnsafeShuffleSorter}). The sorted records are then written\n+ * to a single output file (or multiple files, if we've spilled). The format of the output files is\n+ * the same as the format of the final output file written by\n+ * {@link org.apache.spark.shuffle.sort.SortShuffleWriter}: each output partition's records are\n+ * written as a single serialized, compressed stream that can be read with a new decompression and\n+ * deserialization stream.\n+ * <p>\n+ * Unlike {@link org.apache.spark.util.collection.ExternalSorter}, this sorter does not merge its\n+ * spill files. Instead, this merging is performed in {@link UnsafeShuffleWriter}, which uses a\n+ * specialized merge procedure that avoids extra serialization/deserialization.\n+ */\n+final class UnsafeShuffleExternalSorter {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleExternalSorter.class);\n+\n+  private static final int PAGE_SIZE = PackedRecordPointer.MAXIMUM_PAGE_SIZE_BYTES;\n+  @VisibleForTesting\n+  static final int DISK_WRITE_BUFFER_SIZE = 1024 * 1024;\n+  @VisibleForTesting\n+  static final int MAX_RECORD_SIZE = PAGE_SIZE - 4;\n+\n+  private final int initialSize;\n+  private final int numPartitions;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final BlockManager blockManager;\n+  private final TaskContext taskContext;\n+  private final boolean spillingEnabled;\n+\n+  /** The buffer size to use when writing spills using DiskBlockObjectWriter */\n+  private final int fileBufferSize;\n+\n+  /**\n+   * Memory pages that hold the records being sorted. The pages in this list are freed when\n+   * spilling, although in principle we could recycle these pages across spills (on the other hand,\n+   * this might not be necessary if we maintained a pool of re-usable pages in the TaskMemoryManager\n+   * itself).\n+   */\n+  private final LinkedList<MemoryBlock> allocatedPages = new LinkedList<MemoryBlock>();\n+\n+  private final LinkedList<SpillInfo> spills = new LinkedList<SpillInfo>();\n+\n+  // All three of these variables are reset after spilling:\n+  private UnsafeShuffleSorter sorter;\n+  private MemoryBlock currentPage = null;\n+  private long currentPagePosition = -1;\n+  private long freeSpaceInCurrentPage = 0;\n+\n+  public UnsafeShuffleExternalSorter(\n+      TaskMemoryManager memoryManager,\n+      ShuffleMemoryManager shuffleMemoryManager,\n+      BlockManager blockManager,\n+      TaskContext taskContext,\n+      int initialSize,\n+      int numPartitions,\n+      SparkConf conf) throws IOException {\n+    this.memoryManager = memoryManager;\n+    this.shuffleMemoryManager = shuffleMemoryManager;\n+    this.blockManager = blockManager;\n+    this.taskContext = taskContext;\n+    this.initialSize = initialSize;\n+    this.numPartitions = numPartitions;\n+    this.spillingEnabled = conf.getBoolean(\"spark.shuffle.spill\", true);\n+    // Use getSizeAsKb (not bytes) to maintain backwards compatibility for units\n+    this.fileBufferSize = (int) conf.getSizeAsKb(\"spark.shuffle.file.buffer\", \"32k\") * 1024;\n+    openSorter();\n+  }\n+\n+  /**\n+   * Allocates a new sorter. Called when opening the spill writer for the first time and after\n+   * each spill.\n+   */\n+  private void openSorter() throws IOException {\n+    // TODO: move this sizing calculation logic into a static method of sorter:\n+    final long memoryRequested = initialSize * 8L;\n+    if (spillingEnabled) {\n+      final long memoryAcquired = shuffleMemoryManager.tryToAcquire(memoryRequested);\n+      if (memoryAcquired != memoryRequested) {\n+        shuffleMemoryManager.release(memoryAcquired);\n+        throw new IOException(\"Could not acquire memory!\");\n+      }\n+    }\n+\n+    this.sorter = new UnsafeShuffleSorter(initialSize);\n+  }\n+\n+  /**\n+   * Sorts the in-memory records and writes the sorted records to a spill file.\n+   * This method does not free the sort data structures.\n+   */\n+  private SpillInfo writeSpillFile() throws IOException {\n+    // This call performs the actual sort.\n+    final UnsafeShuffleSorter.UnsafeShuffleSorterIterator sortedRecords =\n+      sorter.getSortedIterator();\n+\n+    // Currently, we need to open a new DiskBlockObjectWriter for each partition; we can avoid this\n+    // after SPARK-5581 is fixed.\n+    BlockObjectWriter writer = null;\n+\n+    // Small writes to DiskBlockObjectWriter will be fairly inefficient. Since there doesn't seem to\n+    // be an API to directly transfer bytes from managed memory to the disk writer, we buffer\n+    // data through a byte array. This array does not need to be large enough to hold a single\n+    // record;\n+    final byte[] writeBuffer = new byte[DISK_WRITE_BUFFER_SIZE];\n+\n+    // Because this output will be read during shuffle, its compression codec must be controlled by\n+    // spark.shuffle.compress instead of spark.shuffle.spill.compress, so we need to use\n+    // createTempShuffleBlock here; see SPARK-3426 for more details.\n+    final Tuple2<TempShuffleBlockId, File> spilledFileInfo =\n+      blockManager.diskBlockManager().createTempShuffleBlock();\n+    final File file = spilledFileInfo._2();\n+    final TempShuffleBlockId blockId = spilledFileInfo._1();\n+    final SpillInfo spillInfo = new SpillInfo(numPartitions, file, blockId);\n+\n+    // Unfortunately, we need a serializer instance in order to construct a DiskBlockObjectWriter.\n+    // Our write path doesn't actually use this serializer (since we end up calling the `write()`\n+    // OutputStream methods), but DiskBlockObjectWriter still calls some methods on it. To work\n+    // around this, we pass a dummy no-op serializer.\n+    final SerializerInstance ser = DummySerializerInstance.INSTANCE;\n+    // TODO: audit the metrics-related code and ensure proper metrics integration:\n+    // It's not clear how we should handle shuffle write metrics for spill files; currently, Spark\n+    // doesn't report IO time spent writing spill files (see SPARK-7413). This method,\n+    // writeSpillFile(), is called both when writing spill files and when writing the single output\n+    // file in cases where we didn't spill. As a result, we don't necessarily know whether this\n+    // should be reported as bytes spilled or as shuffle bytes written. We could defer the updating\n+    // of these metrics until the end of the shuffle write, but that would mean that that users\n+    // wouldn't get useful metrics updates in the UI from long-running tasks. Given this complexity,\n+    // I'm deferring these decisions to a separate follow-up commit or patch.\n+    writer =\n+      blockManager.getDiskWriter(blockId, file, ser, fileBufferSize, new ShuffleWriteMetrics());\n+\n+    int currentPartition = -1;\n+    while (sortedRecords.hasNext()) {\n+      sortedRecords.loadNext();\n+      final int partition = sortedRecords.packedRecordPointer.getPartitionId();\n+      assert (partition >= currentPartition);\n+      if (partition != currentPartition) {\n+        // Switch to the new partition\n+        if (currentPartition != -1) {\n+          writer.commitAndClose();\n+          spillInfo.partitionLengths[currentPartition] = writer.fileSegment().length();\n+        }\n+        currentPartition = partition;\n+        writer =\n+          blockManager.getDiskWriter(blockId, file, ser, fileBufferSize, new ShuffleWriteMetrics());\n+      }\n+\n+      final long recordPointer = sortedRecords.packedRecordPointer.getRecordPointer();\n+      final Object recordPage = memoryManager.getPage(recordPointer);\n+      final long recordOffsetInPage = memoryManager.getOffsetInPage(recordPointer);\n+      int dataRemaining = PlatformDependent.UNSAFE.getInt(recordPage, recordOffsetInPage);\n+      long recordReadPosition = recordOffsetInPage + 4; // skip over record length\n+      while (dataRemaining > 0) {\n+        final int toTransfer = Math.min(DISK_WRITE_BUFFER_SIZE, dataRemaining);\n+        PlatformDependent.copyMemory(\n+          recordPage,\n+          recordReadPosition,\n+          writeBuffer,\n+          PlatformDependent.BYTE_ARRAY_OFFSET,\n+          toTransfer);\n+        assert (writer != null);  // To suppress an IntelliJ warning\n+        writer.write(writeBuffer, 0, toTransfer);\n+        recordReadPosition += toTransfer;\n+        dataRemaining -= toTransfer;\n+      }\n+      // TODO: add a test that detects whether we leave this call out:\n+      writer.recordWritten();\n+    }\n+\n+    if (writer != null) {\n+      writer.commitAndClose();\n+      // If `writeSpillFile()` was called from `closeAndGetSpills()` and no records were inserted,\n+      // then the spill file might be empty. Note that it might be better to avoid calling\n+      // writeSpillFile() in that case.\n+      if (currentPartition != -1) {\n+        spillInfo.partitionLengths[currentPartition] = writer.fileSegment().length();\n+        spills.add(spillInfo);\n+      }\n+    }\n+    return spillInfo;\n+  }\n+\n+  /**\n+   * Sort and spill the current records in response to memory pressure.\n+   */\n+  @VisibleForTesting\n+  void spill() throws IOException {\n+    final long threadId = Thread.currentThread().getId();\n+    logger.info(\"Thread \" + threadId + \" spilling sort data of \" +\n+      org.apache.spark.util.Utils.bytesToString(getMemoryUsage()) + \" to disk (\" +\n+      (spills.size() + (spills.size() > 1 ? \" times\" : \" time\")) + \" so far)\");\n+\n+    final SpillInfo spillInfo = writeSpillFile();\n+    final long sorterMemoryUsage = sorter.getMemoryUsage();\n+    sorter = null;\n+    shuffleMemoryManager.release(sorterMemoryUsage);\n+    final long spillSize = freeMemory();\n+    taskContext.taskMetrics().incMemoryBytesSpilled(spillSize);\n+    taskContext.taskMetrics().incDiskBytesSpilled(spillInfo.file.length());\n+\n+    openSorter();\n+  }\n+\n+  private long getMemoryUsage() {\n+    return sorter.getMemoryUsage() + (allocatedPages.size() * PAGE_SIZE);\n+  }\n+\n+  private long freeMemory() {\n+    long memoryFreed = 0;\n+    for (MemoryBlock block : allocatedPages) {\n+      memoryManager.freePage(block);\n+      shuffleMemoryManager.release(block.size());\n+      memoryFreed += block.size();\n+    }\n+    allocatedPages.clear();\n+    currentPage = null;\n+    currentPagePosition = -1;\n+    freeSpaceInCurrentPage = 0;\n+    return memoryFreed;\n+  }\n+\n+  /**\n+   * Force all memory and spill files to be deleted; called by shuffle error-handling code.\n+   */\n+  public void cleanupAfterError() {\n+    freeMemory();\n+    for (SpillInfo spill : spills) {\n+      if (spill.file.exists() && !spill.file.delete()) {\n+        logger.error(\"Unable to delete spill file {}\", spill.file.getPath());\n+      }\n+    }\n+    if (spillingEnabled && sorter != null) {\n+      shuffleMemoryManager.release(sorter.getMemoryUsage());\n+      sorter = null;\n+    }\n+  }\n+\n+  /**\n+   * Checks whether there is enough space to insert a new record into the sorter.\n+   *\n+   * @param requiredSpace the required space in the data page, in bytes, including space for storing\n+   *                      the record size.\n+\n+   * @return true if the record can be inserted without requiring more allocations, false otherwise.\n+   */\n+  private boolean haveSpaceForRecord(int requiredSpace) {\n+    assert (requiredSpace > 0);\n+    // The sort array will automatically expand when inserting a new record, so we only need to\n+    // worry about it having free space when spilling is enabled.\n+    final boolean sortBufferHasSpace = !spillingEnabled || sorter.hasSpaceForAnotherRecord();\n+    final boolean dataPageHasSpace = requiredSpace <= freeSpaceInCurrentPage;\n+    return (sortBufferHasSpace && dataPageHasSpace);\n+  }\n+\n+  /**\n+   * Allocates more memory in order to insert an additional record. If spilling is enabled, this\n+   * will request additional memory from the {@link ShuffleMemoryManager} and spill if the requested\n+   * memory can not be obtained. If spilling is disabled, then this will allocate memory without\n+   * coordinating with the ShuffleMemoryManager.\n+   *\n+   * @param requiredSpace the required space in the data page, in bytes, including space for storing\n+   *                      the record size.\n+   */\n+  private void allocateSpaceForRecord(int requiredSpace) throws IOException {\n+    if (spillingEnabled && !sorter.hasSpaceForAnotherRecord()) {"
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "This is now covered.  We're now at 93% coverage in this file and the only things that are missing appear to be error-handling catch blocks.\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-11T23:59:04Z",
    "diffHunk": "@@ -0,0 +1,403 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.LinkedList;\n+\n+import scala.Tuple2;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.storage.*;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.MemoryBlock;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+/**\n+ * An external sorter that is specialized for sort-based shuffle.\n+ * <p>\n+ * Incoming records are appended to data pages. When all records have been inserted (or when the\n+ * current thread's shuffle memory limit is reached), the in-memory records are sorted according to\n+ * their partition ids (using a {@link UnsafeShuffleSorter}). The sorted records are then written\n+ * to a single output file (or multiple files, if we've spilled). The format of the output files is\n+ * the same as the format of the final output file written by\n+ * {@link org.apache.spark.shuffle.sort.SortShuffleWriter}: each output partition's records are\n+ * written as a single serialized, compressed stream that can be read with a new decompression and\n+ * deserialization stream.\n+ * <p>\n+ * Unlike {@link org.apache.spark.util.collection.ExternalSorter}, this sorter does not merge its\n+ * spill files. Instead, this merging is performed in {@link UnsafeShuffleWriter}, which uses a\n+ * specialized merge procedure that avoids extra serialization/deserialization.\n+ */\n+final class UnsafeShuffleExternalSorter {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleExternalSorter.class);\n+\n+  private static final int PAGE_SIZE = PackedRecordPointer.MAXIMUM_PAGE_SIZE_BYTES;\n+  @VisibleForTesting\n+  static final int DISK_WRITE_BUFFER_SIZE = 1024 * 1024;\n+  @VisibleForTesting\n+  static final int MAX_RECORD_SIZE = PAGE_SIZE - 4;\n+\n+  private final int initialSize;\n+  private final int numPartitions;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final BlockManager blockManager;\n+  private final TaskContext taskContext;\n+  private final boolean spillingEnabled;\n+\n+  /** The buffer size to use when writing spills using DiskBlockObjectWriter */\n+  private final int fileBufferSize;\n+\n+  /**\n+   * Memory pages that hold the records being sorted. The pages in this list are freed when\n+   * spilling, although in principle we could recycle these pages across spills (on the other hand,\n+   * this might not be necessary if we maintained a pool of re-usable pages in the TaskMemoryManager\n+   * itself).\n+   */\n+  private final LinkedList<MemoryBlock> allocatedPages = new LinkedList<MemoryBlock>();\n+\n+  private final LinkedList<SpillInfo> spills = new LinkedList<SpillInfo>();\n+\n+  // All three of these variables are reset after spilling:\n+  private UnsafeShuffleSorter sorter;\n+  private MemoryBlock currentPage = null;\n+  private long currentPagePosition = -1;\n+  private long freeSpaceInCurrentPage = 0;\n+\n+  public UnsafeShuffleExternalSorter(\n+      TaskMemoryManager memoryManager,\n+      ShuffleMemoryManager shuffleMemoryManager,\n+      BlockManager blockManager,\n+      TaskContext taskContext,\n+      int initialSize,\n+      int numPartitions,\n+      SparkConf conf) throws IOException {\n+    this.memoryManager = memoryManager;\n+    this.shuffleMemoryManager = shuffleMemoryManager;\n+    this.blockManager = blockManager;\n+    this.taskContext = taskContext;\n+    this.initialSize = initialSize;\n+    this.numPartitions = numPartitions;\n+    this.spillingEnabled = conf.getBoolean(\"spark.shuffle.spill\", true);\n+    // Use getSizeAsKb (not bytes) to maintain backwards compatibility for units\n+    this.fileBufferSize = (int) conf.getSizeAsKb(\"spark.shuffle.file.buffer\", \"32k\") * 1024;\n+    openSorter();\n+  }\n+\n+  /**\n+   * Allocates a new sorter. Called when opening the spill writer for the first time and after\n+   * each spill.\n+   */\n+  private void openSorter() throws IOException {\n+    // TODO: move this sizing calculation logic into a static method of sorter:\n+    final long memoryRequested = initialSize * 8L;\n+    if (spillingEnabled) {\n+      final long memoryAcquired = shuffleMemoryManager.tryToAcquire(memoryRequested);\n+      if (memoryAcquired != memoryRequested) {\n+        shuffleMemoryManager.release(memoryAcquired);\n+        throw new IOException(\"Could not acquire memory!\");\n+      }\n+    }\n+\n+    this.sorter = new UnsafeShuffleSorter(initialSize);\n+  }\n+\n+  /**\n+   * Sorts the in-memory records and writes the sorted records to a spill file.\n+   * This method does not free the sort data structures.\n+   */\n+  private SpillInfo writeSpillFile() throws IOException {\n+    // This call performs the actual sort.\n+    final UnsafeShuffleSorter.UnsafeShuffleSorterIterator sortedRecords =\n+      sorter.getSortedIterator();\n+\n+    // Currently, we need to open a new DiskBlockObjectWriter for each partition; we can avoid this\n+    // after SPARK-5581 is fixed.\n+    BlockObjectWriter writer = null;\n+\n+    // Small writes to DiskBlockObjectWriter will be fairly inefficient. Since there doesn't seem to\n+    // be an API to directly transfer bytes from managed memory to the disk writer, we buffer\n+    // data through a byte array. This array does not need to be large enough to hold a single\n+    // record;\n+    final byte[] writeBuffer = new byte[DISK_WRITE_BUFFER_SIZE];\n+\n+    // Because this output will be read during shuffle, its compression codec must be controlled by\n+    // spark.shuffle.compress instead of spark.shuffle.spill.compress, so we need to use\n+    // createTempShuffleBlock here; see SPARK-3426 for more details.\n+    final Tuple2<TempShuffleBlockId, File> spilledFileInfo =\n+      blockManager.diskBlockManager().createTempShuffleBlock();\n+    final File file = spilledFileInfo._2();\n+    final TempShuffleBlockId blockId = spilledFileInfo._1();\n+    final SpillInfo spillInfo = new SpillInfo(numPartitions, file, blockId);\n+\n+    // Unfortunately, we need a serializer instance in order to construct a DiskBlockObjectWriter.\n+    // Our write path doesn't actually use this serializer (since we end up calling the `write()`\n+    // OutputStream methods), but DiskBlockObjectWriter still calls some methods on it. To work\n+    // around this, we pass a dummy no-op serializer.\n+    final SerializerInstance ser = DummySerializerInstance.INSTANCE;\n+    // TODO: audit the metrics-related code and ensure proper metrics integration:\n+    // It's not clear how we should handle shuffle write metrics for spill files; currently, Spark\n+    // doesn't report IO time spent writing spill files (see SPARK-7413). This method,\n+    // writeSpillFile(), is called both when writing spill files and when writing the single output\n+    // file in cases where we didn't spill. As a result, we don't necessarily know whether this\n+    // should be reported as bytes spilled or as shuffle bytes written. We could defer the updating\n+    // of these metrics until the end of the shuffle write, but that would mean that that users\n+    // wouldn't get useful metrics updates in the UI from long-running tasks. Given this complexity,\n+    // I'm deferring these decisions to a separate follow-up commit or patch.\n+    writer =\n+      blockManager.getDiskWriter(blockId, file, ser, fileBufferSize, new ShuffleWriteMetrics());\n+\n+    int currentPartition = -1;\n+    while (sortedRecords.hasNext()) {\n+      sortedRecords.loadNext();\n+      final int partition = sortedRecords.packedRecordPointer.getPartitionId();\n+      assert (partition >= currentPartition);\n+      if (partition != currentPartition) {\n+        // Switch to the new partition\n+        if (currentPartition != -1) {\n+          writer.commitAndClose();\n+          spillInfo.partitionLengths[currentPartition] = writer.fileSegment().length();\n+        }\n+        currentPartition = partition;\n+        writer =\n+          blockManager.getDiskWriter(blockId, file, ser, fileBufferSize, new ShuffleWriteMetrics());\n+      }\n+\n+      final long recordPointer = sortedRecords.packedRecordPointer.getRecordPointer();\n+      final Object recordPage = memoryManager.getPage(recordPointer);\n+      final long recordOffsetInPage = memoryManager.getOffsetInPage(recordPointer);\n+      int dataRemaining = PlatformDependent.UNSAFE.getInt(recordPage, recordOffsetInPage);\n+      long recordReadPosition = recordOffsetInPage + 4; // skip over record length\n+      while (dataRemaining > 0) {\n+        final int toTransfer = Math.min(DISK_WRITE_BUFFER_SIZE, dataRemaining);\n+        PlatformDependent.copyMemory(\n+          recordPage,\n+          recordReadPosition,\n+          writeBuffer,\n+          PlatformDependent.BYTE_ARRAY_OFFSET,\n+          toTransfer);\n+        assert (writer != null);  // To suppress an IntelliJ warning\n+        writer.write(writeBuffer, 0, toTransfer);\n+        recordReadPosition += toTransfer;\n+        dataRemaining -= toTransfer;\n+      }\n+      // TODO: add a test that detects whether we leave this call out:\n+      writer.recordWritten();\n+    }\n+\n+    if (writer != null) {\n+      writer.commitAndClose();\n+      // If `writeSpillFile()` was called from `closeAndGetSpills()` and no records were inserted,\n+      // then the spill file might be empty. Note that it might be better to avoid calling\n+      // writeSpillFile() in that case.\n+      if (currentPartition != -1) {\n+        spillInfo.partitionLengths[currentPartition] = writer.fileSegment().length();\n+        spills.add(spillInfo);\n+      }\n+    }\n+    return spillInfo;\n+  }\n+\n+  /**\n+   * Sort and spill the current records in response to memory pressure.\n+   */\n+  @VisibleForTesting\n+  void spill() throws IOException {\n+    final long threadId = Thread.currentThread().getId();\n+    logger.info(\"Thread \" + threadId + \" spilling sort data of \" +\n+      org.apache.spark.util.Utils.bytesToString(getMemoryUsage()) + \" to disk (\" +\n+      (spills.size() + (spills.size() > 1 ? \" times\" : \" time\")) + \" so far)\");\n+\n+    final SpillInfo spillInfo = writeSpillFile();\n+    final long sorterMemoryUsage = sorter.getMemoryUsage();\n+    sorter = null;\n+    shuffleMemoryManager.release(sorterMemoryUsage);\n+    final long spillSize = freeMemory();\n+    taskContext.taskMetrics().incMemoryBytesSpilled(spillSize);\n+    taskContext.taskMetrics().incDiskBytesSpilled(spillInfo.file.length());\n+\n+    openSorter();\n+  }\n+\n+  private long getMemoryUsage() {\n+    return sorter.getMemoryUsage() + (allocatedPages.size() * PAGE_SIZE);\n+  }\n+\n+  private long freeMemory() {\n+    long memoryFreed = 0;\n+    for (MemoryBlock block : allocatedPages) {\n+      memoryManager.freePage(block);\n+      shuffleMemoryManager.release(block.size());\n+      memoryFreed += block.size();\n+    }\n+    allocatedPages.clear();\n+    currentPage = null;\n+    currentPagePosition = -1;\n+    freeSpaceInCurrentPage = 0;\n+    return memoryFreed;\n+  }\n+\n+  /**\n+   * Force all memory and spill files to be deleted; called by shuffle error-handling code.\n+   */\n+  public void cleanupAfterError() {\n+    freeMemory();\n+    for (SpillInfo spill : spills) {\n+      if (spill.file.exists() && !spill.file.delete()) {\n+        logger.error(\"Unable to delete spill file {}\", spill.file.getPath());\n+      }\n+    }\n+    if (spillingEnabled && sorter != null) {\n+      shuffleMemoryManager.release(sorter.getMemoryUsage());\n+      sorter = null;\n+    }\n+  }\n+\n+  /**\n+   * Checks whether there is enough space to insert a new record into the sorter.\n+   *\n+   * @param requiredSpace the required space in the data page, in bytes, including space for storing\n+   *                      the record size.\n+\n+   * @return true if the record can be inserted without requiring more allocations, false otherwise.\n+   */\n+  private boolean haveSpaceForRecord(int requiredSpace) {\n+    assert (requiredSpace > 0);\n+    // The sort array will automatically expand when inserting a new record, so we only need to\n+    // worry about it having free space when spilling is enabled.\n+    final boolean sortBufferHasSpace = !spillingEnabled || sorter.hasSpaceForAnotherRecord();\n+    final boolean dataPageHasSpace = requiredSpace <= freeSpaceInCurrentPage;\n+    return (sortBufferHasSpace && dataPageHasSpace);\n+  }\n+\n+  /**\n+   * Allocates more memory in order to insert an additional record. If spilling is enabled, this\n+   * will request additional memory from the {@link ShuffleMemoryManager} and spill if the requested\n+   * memory can not be obtained. If spilling is disabled, then this will allocate memory without\n+   * coordinating with the ShuffleMemoryManager.\n+   *\n+   * @param requiredSpace the required space in the data page, in bytes, including space for storing\n+   *                      the record size.\n+   */\n+  private void allocateSpaceForRecord(int requiredSpace) throws IOException {\n+    if (spillingEnabled && !sorter.hasSpaceForAnotherRecord()) {"
  }],
  "prId": 5868
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "This branch is also missed by UnsafeShuffleWriter's standalone unit test coverage.\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-11T22:59:26Z",
    "diffHunk": "@@ -0,0 +1,403 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.LinkedList;\n+\n+import scala.Tuple2;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.storage.*;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.MemoryBlock;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+/**\n+ * An external sorter that is specialized for sort-based shuffle.\n+ * <p>\n+ * Incoming records are appended to data pages. When all records have been inserted (or when the\n+ * current thread's shuffle memory limit is reached), the in-memory records are sorted according to\n+ * their partition ids (using a {@link UnsafeShuffleSorter}). The sorted records are then written\n+ * to a single output file (or multiple files, if we've spilled). The format of the output files is\n+ * the same as the format of the final output file written by\n+ * {@link org.apache.spark.shuffle.sort.SortShuffleWriter}: each output partition's records are\n+ * written as a single serialized, compressed stream that can be read with a new decompression and\n+ * deserialization stream.\n+ * <p>\n+ * Unlike {@link org.apache.spark.util.collection.ExternalSorter}, this sorter does not merge its\n+ * spill files. Instead, this merging is performed in {@link UnsafeShuffleWriter}, which uses a\n+ * specialized merge procedure that avoids extra serialization/deserialization.\n+ */\n+final class UnsafeShuffleExternalSorter {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleExternalSorter.class);\n+\n+  private static final int PAGE_SIZE = PackedRecordPointer.MAXIMUM_PAGE_SIZE_BYTES;\n+  @VisibleForTesting\n+  static final int DISK_WRITE_BUFFER_SIZE = 1024 * 1024;\n+  @VisibleForTesting\n+  static final int MAX_RECORD_SIZE = PAGE_SIZE - 4;\n+\n+  private final int initialSize;\n+  private final int numPartitions;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final BlockManager blockManager;\n+  private final TaskContext taskContext;\n+  private final boolean spillingEnabled;\n+\n+  /** The buffer size to use when writing spills using DiskBlockObjectWriter */\n+  private final int fileBufferSize;\n+\n+  /**\n+   * Memory pages that hold the records being sorted. The pages in this list are freed when\n+   * spilling, although in principle we could recycle these pages across spills (on the other hand,\n+   * this might not be necessary if we maintained a pool of re-usable pages in the TaskMemoryManager\n+   * itself).\n+   */\n+  private final LinkedList<MemoryBlock> allocatedPages = new LinkedList<MemoryBlock>();\n+\n+  private final LinkedList<SpillInfo> spills = new LinkedList<SpillInfo>();\n+\n+  // All three of these variables are reset after spilling:\n+  private UnsafeShuffleSorter sorter;\n+  private MemoryBlock currentPage = null;\n+  private long currentPagePosition = -1;\n+  private long freeSpaceInCurrentPage = 0;\n+\n+  public UnsafeShuffleExternalSorter(\n+      TaskMemoryManager memoryManager,\n+      ShuffleMemoryManager shuffleMemoryManager,\n+      BlockManager blockManager,\n+      TaskContext taskContext,\n+      int initialSize,\n+      int numPartitions,\n+      SparkConf conf) throws IOException {\n+    this.memoryManager = memoryManager;\n+    this.shuffleMemoryManager = shuffleMemoryManager;\n+    this.blockManager = blockManager;\n+    this.taskContext = taskContext;\n+    this.initialSize = initialSize;\n+    this.numPartitions = numPartitions;\n+    this.spillingEnabled = conf.getBoolean(\"spark.shuffle.spill\", true);\n+    // Use getSizeAsKb (not bytes) to maintain backwards compatibility for units\n+    this.fileBufferSize = (int) conf.getSizeAsKb(\"spark.shuffle.file.buffer\", \"32k\") * 1024;\n+    openSorter();\n+  }\n+\n+  /**\n+   * Allocates a new sorter. Called when opening the spill writer for the first time and after\n+   * each spill.\n+   */\n+  private void openSorter() throws IOException {\n+    // TODO: move this sizing calculation logic into a static method of sorter:\n+    final long memoryRequested = initialSize * 8L;\n+    if (spillingEnabled) {\n+      final long memoryAcquired = shuffleMemoryManager.tryToAcquire(memoryRequested);\n+      if (memoryAcquired != memoryRequested) {\n+        shuffleMemoryManager.release(memoryAcquired);\n+        throw new IOException(\"Could not acquire memory!\");\n+      }\n+    }\n+\n+    this.sorter = new UnsafeShuffleSorter(initialSize);\n+  }\n+\n+  /**\n+   * Sorts the in-memory records and writes the sorted records to a spill file.\n+   * This method does not free the sort data structures.\n+   */\n+  private SpillInfo writeSpillFile() throws IOException {\n+    // This call performs the actual sort.\n+    final UnsafeShuffleSorter.UnsafeShuffleSorterIterator sortedRecords =\n+      sorter.getSortedIterator();\n+\n+    // Currently, we need to open a new DiskBlockObjectWriter for each partition; we can avoid this\n+    // after SPARK-5581 is fixed.\n+    BlockObjectWriter writer = null;\n+\n+    // Small writes to DiskBlockObjectWriter will be fairly inefficient. Since there doesn't seem to\n+    // be an API to directly transfer bytes from managed memory to the disk writer, we buffer\n+    // data through a byte array. This array does not need to be large enough to hold a single\n+    // record;\n+    final byte[] writeBuffer = new byte[DISK_WRITE_BUFFER_SIZE];\n+\n+    // Because this output will be read during shuffle, its compression codec must be controlled by\n+    // spark.shuffle.compress instead of spark.shuffle.spill.compress, so we need to use\n+    // createTempShuffleBlock here; see SPARK-3426 for more details.\n+    final Tuple2<TempShuffleBlockId, File> spilledFileInfo =\n+      blockManager.diskBlockManager().createTempShuffleBlock();\n+    final File file = spilledFileInfo._2();\n+    final TempShuffleBlockId blockId = spilledFileInfo._1();\n+    final SpillInfo spillInfo = new SpillInfo(numPartitions, file, blockId);\n+\n+    // Unfortunately, we need a serializer instance in order to construct a DiskBlockObjectWriter.\n+    // Our write path doesn't actually use this serializer (since we end up calling the `write()`\n+    // OutputStream methods), but DiskBlockObjectWriter still calls some methods on it. To work\n+    // around this, we pass a dummy no-op serializer.\n+    final SerializerInstance ser = DummySerializerInstance.INSTANCE;\n+    // TODO: audit the metrics-related code and ensure proper metrics integration:\n+    // It's not clear how we should handle shuffle write metrics for spill files; currently, Spark\n+    // doesn't report IO time spent writing spill files (see SPARK-7413). This method,\n+    // writeSpillFile(), is called both when writing spill files and when writing the single output\n+    // file in cases where we didn't spill. As a result, we don't necessarily know whether this\n+    // should be reported as bytes spilled or as shuffle bytes written. We could defer the updating\n+    // of these metrics until the end of the shuffle write, but that would mean that that users\n+    // wouldn't get useful metrics updates in the UI from long-running tasks. Given this complexity,\n+    // I'm deferring these decisions to a separate follow-up commit or patch.\n+    writer =\n+      blockManager.getDiskWriter(blockId, file, ser, fileBufferSize, new ShuffleWriteMetrics());\n+\n+    int currentPartition = -1;\n+    while (sortedRecords.hasNext()) {\n+      sortedRecords.loadNext();\n+      final int partition = sortedRecords.packedRecordPointer.getPartitionId();\n+      assert (partition >= currentPartition);\n+      if (partition != currentPartition) {\n+        // Switch to the new partition\n+        if (currentPartition != -1) {\n+          writer.commitAndClose();\n+          spillInfo.partitionLengths[currentPartition] = writer.fileSegment().length();\n+        }\n+        currentPartition = partition;\n+        writer =\n+          blockManager.getDiskWriter(blockId, file, ser, fileBufferSize, new ShuffleWriteMetrics());\n+      }\n+\n+      final long recordPointer = sortedRecords.packedRecordPointer.getRecordPointer();\n+      final Object recordPage = memoryManager.getPage(recordPointer);\n+      final long recordOffsetInPage = memoryManager.getOffsetInPage(recordPointer);\n+      int dataRemaining = PlatformDependent.UNSAFE.getInt(recordPage, recordOffsetInPage);\n+      long recordReadPosition = recordOffsetInPage + 4; // skip over record length\n+      while (dataRemaining > 0) {\n+        final int toTransfer = Math.min(DISK_WRITE_BUFFER_SIZE, dataRemaining);\n+        PlatformDependent.copyMemory(\n+          recordPage,\n+          recordReadPosition,\n+          writeBuffer,\n+          PlatformDependent.BYTE_ARRAY_OFFSET,\n+          toTransfer);\n+        assert (writer != null);  // To suppress an IntelliJ warning\n+        writer.write(writeBuffer, 0, toTransfer);\n+        recordReadPosition += toTransfer;\n+        dataRemaining -= toTransfer;\n+      }\n+      // TODO: add a test that detects whether we leave this call out:\n+      writer.recordWritten();\n+    }\n+\n+    if (writer != null) {\n+      writer.commitAndClose();\n+      // If `writeSpillFile()` was called from `closeAndGetSpills()` and no records were inserted,\n+      // then the spill file might be empty. Note that it might be better to avoid calling\n+      // writeSpillFile() in that case.\n+      if (currentPartition != -1) {\n+        spillInfo.partitionLengths[currentPartition] = writer.fileSegment().length();\n+        spills.add(spillInfo);\n+      }\n+    }\n+    return spillInfo;\n+  }\n+\n+  /**\n+   * Sort and spill the current records in response to memory pressure.\n+   */\n+  @VisibleForTesting\n+  void spill() throws IOException {\n+    final long threadId = Thread.currentThread().getId();\n+    logger.info(\"Thread \" + threadId + \" spilling sort data of \" +\n+      org.apache.spark.util.Utils.bytesToString(getMemoryUsage()) + \" to disk (\" +\n+      (spills.size() + (spills.size() > 1 ? \" times\" : \" time\")) + \" so far)\");\n+\n+    final SpillInfo spillInfo = writeSpillFile();\n+    final long sorterMemoryUsage = sorter.getMemoryUsage();\n+    sorter = null;\n+    shuffleMemoryManager.release(sorterMemoryUsage);\n+    final long spillSize = freeMemory();\n+    taskContext.taskMetrics().incMemoryBytesSpilled(spillSize);\n+    taskContext.taskMetrics().incDiskBytesSpilled(spillInfo.file.length());\n+\n+    openSorter();\n+  }\n+\n+  private long getMemoryUsage() {\n+    return sorter.getMemoryUsage() + (allocatedPages.size() * PAGE_SIZE);\n+  }\n+\n+  private long freeMemory() {\n+    long memoryFreed = 0;\n+    for (MemoryBlock block : allocatedPages) {\n+      memoryManager.freePage(block);\n+      shuffleMemoryManager.release(block.size());\n+      memoryFreed += block.size();\n+    }\n+    allocatedPages.clear();\n+    currentPage = null;\n+    currentPagePosition = -1;\n+    freeSpaceInCurrentPage = 0;\n+    return memoryFreed;\n+  }\n+\n+  /**\n+   * Force all memory and spill files to be deleted; called by shuffle error-handling code.\n+   */\n+  public void cleanupAfterError() {\n+    freeMemory();\n+    for (SpillInfo spill : spills) {\n+      if (spill.file.exists() && !spill.file.delete()) {\n+        logger.error(\"Unable to delete spill file {}\", spill.file.getPath());\n+      }\n+    }\n+    if (spillingEnabled && sorter != null) {\n+      shuffleMemoryManager.release(sorter.getMemoryUsage());\n+      sorter = null;\n+    }\n+  }\n+\n+  /**\n+   * Checks whether there is enough space to insert a new record into the sorter.\n+   *\n+   * @param requiredSpace the required space in the data page, in bytes, including space for storing\n+   *                      the record size.\n+\n+   * @return true if the record can be inserted without requiring more allocations, false otherwise.\n+   */\n+  private boolean haveSpaceForRecord(int requiredSpace) {\n+    assert (requiredSpace > 0);\n+    // The sort array will automatically expand when inserting a new record, so we only need to\n+    // worry about it having free space when spilling is enabled.\n+    final boolean sortBufferHasSpace = !spillingEnabled || sorter.hasSpaceForAnotherRecord();\n+    final boolean dataPageHasSpace = requiredSpace <= freeSpaceInCurrentPage;\n+    return (sortBufferHasSpace && dataPageHasSpace);\n+  }\n+\n+  /**\n+   * Allocates more memory in order to insert an additional record. If spilling is enabled, this\n+   * will request additional memory from the {@link ShuffleMemoryManager} and spill if the requested\n+   * memory can not be obtained. If spilling is disabled, then this will allocate memory without\n+   * coordinating with the ShuffleMemoryManager.\n+   *\n+   * @param requiredSpace the required space in the data page, in bytes, including space for storing\n+   *                      the record size.\n+   */\n+  private void allocateSpaceForRecord(int requiredSpace) throws IOException {\n+    if (spillingEnabled && !sorter.hasSpaceForAnotherRecord()) {\n+      logger.debug(\"Attempting to expand sort buffer\");\n+      final long oldSortBufferMemoryUsage = sorter.getMemoryUsage();\n+      final long memoryToGrowSortBuffer = oldSortBufferMemoryUsage * 2;\n+      final long memoryAcquired = shuffleMemoryManager.tryToAcquire(memoryToGrowSortBuffer);\n+      if (memoryAcquired < memoryToGrowSortBuffer) {\n+        shuffleMemoryManager.release(memoryAcquired);\n+        spill();\n+      } else {\n+        sorter.expandSortBuffer();\n+        shuffleMemoryManager.release(oldSortBufferMemoryUsage);\n+      }\n+    }\n+    if (requiredSpace > freeSpaceInCurrentPage) {\n+      logger.trace(\"Required space {} is less than free space in current page ({})\", requiredSpace,\n+        freeSpaceInCurrentPage);\n+      // TODO: we should track metrics on the amount of space wasted when we roll over to a new page\n+      // without using the free space at the end of the current page. We should also do this for\n+      // BytesToBytesMap.\n+      if (requiredSpace > PAGE_SIZE) {\n+        throw new IOException(\"Required space \" + requiredSpace + \" is greater than page size (\" +\n+          PAGE_SIZE + \")\");\n+      } else {\n+        if (spillingEnabled) {\n+          final long memoryAcquired = shuffleMemoryManager.tryToAcquire(PAGE_SIZE);\n+          if (memoryAcquired < PAGE_SIZE) {\n+            shuffleMemoryManager.release(memoryAcquired);"
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "This is now covered as well.\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-11T23:59:16Z",
    "diffHunk": "@@ -0,0 +1,403 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.LinkedList;\n+\n+import scala.Tuple2;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.storage.*;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.MemoryBlock;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+/**\n+ * An external sorter that is specialized for sort-based shuffle.\n+ * <p>\n+ * Incoming records are appended to data pages. When all records have been inserted (or when the\n+ * current thread's shuffle memory limit is reached), the in-memory records are sorted according to\n+ * their partition ids (using a {@link UnsafeShuffleSorter}). The sorted records are then written\n+ * to a single output file (or multiple files, if we've spilled). The format of the output files is\n+ * the same as the format of the final output file written by\n+ * {@link org.apache.spark.shuffle.sort.SortShuffleWriter}: each output partition's records are\n+ * written as a single serialized, compressed stream that can be read with a new decompression and\n+ * deserialization stream.\n+ * <p>\n+ * Unlike {@link org.apache.spark.util.collection.ExternalSorter}, this sorter does not merge its\n+ * spill files. Instead, this merging is performed in {@link UnsafeShuffleWriter}, which uses a\n+ * specialized merge procedure that avoids extra serialization/deserialization.\n+ */\n+final class UnsafeShuffleExternalSorter {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleExternalSorter.class);\n+\n+  private static final int PAGE_SIZE = PackedRecordPointer.MAXIMUM_PAGE_SIZE_BYTES;\n+  @VisibleForTesting\n+  static final int DISK_WRITE_BUFFER_SIZE = 1024 * 1024;\n+  @VisibleForTesting\n+  static final int MAX_RECORD_SIZE = PAGE_SIZE - 4;\n+\n+  private final int initialSize;\n+  private final int numPartitions;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final BlockManager blockManager;\n+  private final TaskContext taskContext;\n+  private final boolean spillingEnabled;\n+\n+  /** The buffer size to use when writing spills using DiskBlockObjectWriter */\n+  private final int fileBufferSize;\n+\n+  /**\n+   * Memory pages that hold the records being sorted. The pages in this list are freed when\n+   * spilling, although in principle we could recycle these pages across spills (on the other hand,\n+   * this might not be necessary if we maintained a pool of re-usable pages in the TaskMemoryManager\n+   * itself).\n+   */\n+  private final LinkedList<MemoryBlock> allocatedPages = new LinkedList<MemoryBlock>();\n+\n+  private final LinkedList<SpillInfo> spills = new LinkedList<SpillInfo>();\n+\n+  // All three of these variables are reset after spilling:\n+  private UnsafeShuffleSorter sorter;\n+  private MemoryBlock currentPage = null;\n+  private long currentPagePosition = -1;\n+  private long freeSpaceInCurrentPage = 0;\n+\n+  public UnsafeShuffleExternalSorter(\n+      TaskMemoryManager memoryManager,\n+      ShuffleMemoryManager shuffleMemoryManager,\n+      BlockManager blockManager,\n+      TaskContext taskContext,\n+      int initialSize,\n+      int numPartitions,\n+      SparkConf conf) throws IOException {\n+    this.memoryManager = memoryManager;\n+    this.shuffleMemoryManager = shuffleMemoryManager;\n+    this.blockManager = blockManager;\n+    this.taskContext = taskContext;\n+    this.initialSize = initialSize;\n+    this.numPartitions = numPartitions;\n+    this.spillingEnabled = conf.getBoolean(\"spark.shuffle.spill\", true);\n+    // Use getSizeAsKb (not bytes) to maintain backwards compatibility for units\n+    this.fileBufferSize = (int) conf.getSizeAsKb(\"spark.shuffle.file.buffer\", \"32k\") * 1024;\n+    openSorter();\n+  }\n+\n+  /**\n+   * Allocates a new sorter. Called when opening the spill writer for the first time and after\n+   * each spill.\n+   */\n+  private void openSorter() throws IOException {\n+    // TODO: move this sizing calculation logic into a static method of sorter:\n+    final long memoryRequested = initialSize * 8L;\n+    if (spillingEnabled) {\n+      final long memoryAcquired = shuffleMemoryManager.tryToAcquire(memoryRequested);\n+      if (memoryAcquired != memoryRequested) {\n+        shuffleMemoryManager.release(memoryAcquired);\n+        throw new IOException(\"Could not acquire memory!\");\n+      }\n+    }\n+\n+    this.sorter = new UnsafeShuffleSorter(initialSize);\n+  }\n+\n+  /**\n+   * Sorts the in-memory records and writes the sorted records to a spill file.\n+   * This method does not free the sort data structures.\n+   */\n+  private SpillInfo writeSpillFile() throws IOException {\n+    // This call performs the actual sort.\n+    final UnsafeShuffleSorter.UnsafeShuffleSorterIterator sortedRecords =\n+      sorter.getSortedIterator();\n+\n+    // Currently, we need to open a new DiskBlockObjectWriter for each partition; we can avoid this\n+    // after SPARK-5581 is fixed.\n+    BlockObjectWriter writer = null;\n+\n+    // Small writes to DiskBlockObjectWriter will be fairly inefficient. Since there doesn't seem to\n+    // be an API to directly transfer bytes from managed memory to the disk writer, we buffer\n+    // data through a byte array. This array does not need to be large enough to hold a single\n+    // record;\n+    final byte[] writeBuffer = new byte[DISK_WRITE_BUFFER_SIZE];\n+\n+    // Because this output will be read during shuffle, its compression codec must be controlled by\n+    // spark.shuffle.compress instead of spark.shuffle.spill.compress, so we need to use\n+    // createTempShuffleBlock here; see SPARK-3426 for more details.\n+    final Tuple2<TempShuffleBlockId, File> spilledFileInfo =\n+      blockManager.diskBlockManager().createTempShuffleBlock();\n+    final File file = spilledFileInfo._2();\n+    final TempShuffleBlockId blockId = spilledFileInfo._1();\n+    final SpillInfo spillInfo = new SpillInfo(numPartitions, file, blockId);\n+\n+    // Unfortunately, we need a serializer instance in order to construct a DiskBlockObjectWriter.\n+    // Our write path doesn't actually use this serializer (since we end up calling the `write()`\n+    // OutputStream methods), but DiskBlockObjectWriter still calls some methods on it. To work\n+    // around this, we pass a dummy no-op serializer.\n+    final SerializerInstance ser = DummySerializerInstance.INSTANCE;\n+    // TODO: audit the metrics-related code and ensure proper metrics integration:\n+    // It's not clear how we should handle shuffle write metrics for spill files; currently, Spark\n+    // doesn't report IO time spent writing spill files (see SPARK-7413). This method,\n+    // writeSpillFile(), is called both when writing spill files and when writing the single output\n+    // file in cases where we didn't spill. As a result, we don't necessarily know whether this\n+    // should be reported as bytes spilled or as shuffle bytes written. We could defer the updating\n+    // of these metrics until the end of the shuffle write, but that would mean that that users\n+    // wouldn't get useful metrics updates in the UI from long-running tasks. Given this complexity,\n+    // I'm deferring these decisions to a separate follow-up commit or patch.\n+    writer =\n+      blockManager.getDiskWriter(blockId, file, ser, fileBufferSize, new ShuffleWriteMetrics());\n+\n+    int currentPartition = -1;\n+    while (sortedRecords.hasNext()) {\n+      sortedRecords.loadNext();\n+      final int partition = sortedRecords.packedRecordPointer.getPartitionId();\n+      assert (partition >= currentPartition);\n+      if (partition != currentPartition) {\n+        // Switch to the new partition\n+        if (currentPartition != -1) {\n+          writer.commitAndClose();\n+          spillInfo.partitionLengths[currentPartition] = writer.fileSegment().length();\n+        }\n+        currentPartition = partition;\n+        writer =\n+          blockManager.getDiskWriter(blockId, file, ser, fileBufferSize, new ShuffleWriteMetrics());\n+      }\n+\n+      final long recordPointer = sortedRecords.packedRecordPointer.getRecordPointer();\n+      final Object recordPage = memoryManager.getPage(recordPointer);\n+      final long recordOffsetInPage = memoryManager.getOffsetInPage(recordPointer);\n+      int dataRemaining = PlatformDependent.UNSAFE.getInt(recordPage, recordOffsetInPage);\n+      long recordReadPosition = recordOffsetInPage + 4; // skip over record length\n+      while (dataRemaining > 0) {\n+        final int toTransfer = Math.min(DISK_WRITE_BUFFER_SIZE, dataRemaining);\n+        PlatformDependent.copyMemory(\n+          recordPage,\n+          recordReadPosition,\n+          writeBuffer,\n+          PlatformDependent.BYTE_ARRAY_OFFSET,\n+          toTransfer);\n+        assert (writer != null);  // To suppress an IntelliJ warning\n+        writer.write(writeBuffer, 0, toTransfer);\n+        recordReadPosition += toTransfer;\n+        dataRemaining -= toTransfer;\n+      }\n+      // TODO: add a test that detects whether we leave this call out:\n+      writer.recordWritten();\n+    }\n+\n+    if (writer != null) {\n+      writer.commitAndClose();\n+      // If `writeSpillFile()` was called from `closeAndGetSpills()` and no records were inserted,\n+      // then the spill file might be empty. Note that it might be better to avoid calling\n+      // writeSpillFile() in that case.\n+      if (currentPartition != -1) {\n+        spillInfo.partitionLengths[currentPartition] = writer.fileSegment().length();\n+        spills.add(spillInfo);\n+      }\n+    }\n+    return spillInfo;\n+  }\n+\n+  /**\n+   * Sort and spill the current records in response to memory pressure.\n+   */\n+  @VisibleForTesting\n+  void spill() throws IOException {\n+    final long threadId = Thread.currentThread().getId();\n+    logger.info(\"Thread \" + threadId + \" spilling sort data of \" +\n+      org.apache.spark.util.Utils.bytesToString(getMemoryUsage()) + \" to disk (\" +\n+      (spills.size() + (spills.size() > 1 ? \" times\" : \" time\")) + \" so far)\");\n+\n+    final SpillInfo spillInfo = writeSpillFile();\n+    final long sorterMemoryUsage = sorter.getMemoryUsage();\n+    sorter = null;\n+    shuffleMemoryManager.release(sorterMemoryUsage);\n+    final long spillSize = freeMemory();\n+    taskContext.taskMetrics().incMemoryBytesSpilled(spillSize);\n+    taskContext.taskMetrics().incDiskBytesSpilled(spillInfo.file.length());\n+\n+    openSorter();\n+  }\n+\n+  private long getMemoryUsage() {\n+    return sorter.getMemoryUsage() + (allocatedPages.size() * PAGE_SIZE);\n+  }\n+\n+  private long freeMemory() {\n+    long memoryFreed = 0;\n+    for (MemoryBlock block : allocatedPages) {\n+      memoryManager.freePage(block);\n+      shuffleMemoryManager.release(block.size());\n+      memoryFreed += block.size();\n+    }\n+    allocatedPages.clear();\n+    currentPage = null;\n+    currentPagePosition = -1;\n+    freeSpaceInCurrentPage = 0;\n+    return memoryFreed;\n+  }\n+\n+  /**\n+   * Force all memory and spill files to be deleted; called by shuffle error-handling code.\n+   */\n+  public void cleanupAfterError() {\n+    freeMemory();\n+    for (SpillInfo spill : spills) {\n+      if (spill.file.exists() && !spill.file.delete()) {\n+        logger.error(\"Unable to delete spill file {}\", spill.file.getPath());\n+      }\n+    }\n+    if (spillingEnabled && sorter != null) {\n+      shuffleMemoryManager.release(sorter.getMemoryUsage());\n+      sorter = null;\n+    }\n+  }\n+\n+  /**\n+   * Checks whether there is enough space to insert a new record into the sorter.\n+   *\n+   * @param requiredSpace the required space in the data page, in bytes, including space for storing\n+   *                      the record size.\n+\n+   * @return true if the record can be inserted without requiring more allocations, false otherwise.\n+   */\n+  private boolean haveSpaceForRecord(int requiredSpace) {\n+    assert (requiredSpace > 0);\n+    // The sort array will automatically expand when inserting a new record, so we only need to\n+    // worry about it having free space when spilling is enabled.\n+    final boolean sortBufferHasSpace = !spillingEnabled || sorter.hasSpaceForAnotherRecord();\n+    final boolean dataPageHasSpace = requiredSpace <= freeSpaceInCurrentPage;\n+    return (sortBufferHasSpace && dataPageHasSpace);\n+  }\n+\n+  /**\n+   * Allocates more memory in order to insert an additional record. If spilling is enabled, this\n+   * will request additional memory from the {@link ShuffleMemoryManager} and spill if the requested\n+   * memory can not be obtained. If spilling is disabled, then this will allocate memory without\n+   * coordinating with the ShuffleMemoryManager.\n+   *\n+   * @param requiredSpace the required space in the data page, in bytes, including space for storing\n+   *                      the record size.\n+   */\n+  private void allocateSpaceForRecord(int requiredSpace) throws IOException {\n+    if (spillingEnabled && !sorter.hasSpaceForAnotherRecord()) {\n+      logger.debug(\"Attempting to expand sort buffer\");\n+      final long oldSortBufferMemoryUsage = sorter.getMemoryUsage();\n+      final long memoryToGrowSortBuffer = oldSortBufferMemoryUsage * 2;\n+      final long memoryAcquired = shuffleMemoryManager.tryToAcquire(memoryToGrowSortBuffer);\n+      if (memoryAcquired < memoryToGrowSortBuffer) {\n+        shuffleMemoryManager.release(memoryAcquired);\n+        spill();\n+      } else {\n+        sorter.expandSortBuffer();\n+        shuffleMemoryManager.release(oldSortBufferMemoryUsage);\n+      }\n+    }\n+    if (requiredSpace > freeSpaceInCurrentPage) {\n+      logger.trace(\"Required space {} is less than free space in current page ({})\", requiredSpace,\n+        freeSpaceInCurrentPage);\n+      // TODO: we should track metrics on the amount of space wasted when we roll over to a new page\n+      // without using the free space at the end of the current page. We should also do this for\n+      // BytesToBytesMap.\n+      if (requiredSpace > PAGE_SIZE) {\n+        throw new IOException(\"Required space \" + requiredSpace + \" is greater than page size (\" +\n+          PAGE_SIZE + \")\");\n+      } else {\n+        if (spillingEnabled) {\n+          final long memoryAcquired = shuffleMemoryManager.tryToAcquire(PAGE_SIZE);\n+          if (memoryAcquired < PAGE_SIZE) {\n+            shuffleMemoryManager.release(memoryAcquired);"
  }],
  "prId": 5868
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "4?\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-12T06:31:22Z",
    "diffHunk": "@@ -0,0 +1,418 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.LinkedList;\n+\n+import scala.Tuple2;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.storage.*;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.MemoryBlock;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+/**\n+ * An external sorter that is specialized for sort-based shuffle.\n+ * <p>\n+ * Incoming records are appended to data pages. When all records have been inserted (or when the\n+ * current thread's shuffle memory limit is reached), the in-memory records are sorted according to\n+ * their partition ids (using a {@link UnsafeShuffleSorter}). The sorted records are then written\n+ * to a single output file (or multiple files, if we've spilled). The format of the output files is\n+ * the same as the format of the final output file written by\n+ * {@link org.apache.spark.shuffle.sort.SortShuffleWriter}: each output partition's records are\n+ * written as a single serialized, compressed stream that can be read with a new decompression and\n+ * deserialization stream.\n+ * <p>\n+ * Unlike {@link org.apache.spark.util.collection.ExternalSorter}, this sorter does not merge its\n+ * spill files. Instead, this merging is performed in {@link UnsafeShuffleWriter}, which uses a\n+ * specialized merge procedure that avoids extra serialization/deserialization.\n+ */\n+final class UnsafeShuffleExternalSorter {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleExternalSorter.class);\n+\n+  private static final int PAGE_SIZE = PackedRecordPointer.MAXIMUM_PAGE_SIZE_BYTES;\n+  @VisibleForTesting\n+  static final int DISK_WRITE_BUFFER_SIZE = 1024 * 1024;\n+  @VisibleForTesting\n+  static final int MAX_RECORD_SIZE = PAGE_SIZE - 4;\n+\n+  private final int initialSize;\n+  private final int numPartitions;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final BlockManager blockManager;\n+  private final TaskContext taskContext;\n+  private final boolean spillingEnabled;\n+  private final ShuffleWriteMetrics writeMetrics;\n+\n+  /** The buffer size to use when writing spills using DiskBlockObjectWriter */\n+  private final int fileBufferSize;\n+\n+  /**\n+   * Memory pages that hold the records being sorted. The pages in this list are freed when\n+   * spilling, although in principle we could recycle these pages across spills (on the other hand,\n+   * this might not be necessary if we maintained a pool of re-usable pages in the TaskMemoryManager\n+   * itself).\n+   */\n+  private final LinkedList<MemoryBlock> allocatedPages = new LinkedList<MemoryBlock>();\n+\n+  private final LinkedList<SpillInfo> spills = new LinkedList<SpillInfo>();\n+\n+  // All three of these variables are reset after spilling:"
  }],
  "prId": 5868
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "fileBufferSizeKB?\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-12T06:32:13Z",
    "diffHunk": "@@ -0,0 +1,418 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.LinkedList;\n+\n+import scala.Tuple2;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.storage.*;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.MemoryBlock;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+/**\n+ * An external sorter that is specialized for sort-based shuffle.\n+ * <p>\n+ * Incoming records are appended to data pages. When all records have been inserted (or when the\n+ * current thread's shuffle memory limit is reached), the in-memory records are sorted according to\n+ * their partition ids (using a {@link UnsafeShuffleSorter}). The sorted records are then written\n+ * to a single output file (or multiple files, if we've spilled). The format of the output files is\n+ * the same as the format of the final output file written by\n+ * {@link org.apache.spark.shuffle.sort.SortShuffleWriter}: each output partition's records are\n+ * written as a single serialized, compressed stream that can be read with a new decompression and\n+ * deserialization stream.\n+ * <p>\n+ * Unlike {@link org.apache.spark.util.collection.ExternalSorter}, this sorter does not merge its\n+ * spill files. Instead, this merging is performed in {@link UnsafeShuffleWriter}, which uses a\n+ * specialized merge procedure that avoids extra serialization/deserialization.\n+ */\n+final class UnsafeShuffleExternalSorter {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleExternalSorter.class);\n+\n+  private static final int PAGE_SIZE = PackedRecordPointer.MAXIMUM_PAGE_SIZE_BYTES;\n+  @VisibleForTesting\n+  static final int DISK_WRITE_BUFFER_SIZE = 1024 * 1024;\n+  @VisibleForTesting\n+  static final int MAX_RECORD_SIZE = PAGE_SIZE - 4;\n+\n+  private final int initialSize;\n+  private final int numPartitions;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final BlockManager blockManager;\n+  private final TaskContext taskContext;\n+  private final boolean spillingEnabled;\n+  private final ShuffleWriteMetrics writeMetrics;\n+\n+  /** The buffer size to use when writing spills using DiskBlockObjectWriter */\n+  private final int fileBufferSize;"
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "Actually, I think that this should be in bytes.  This is a little confusing because we aren't consistent about using unit prefixes in the existing code.  This value is passed to DiskBlockObjectWriter's constructor, which treats this as bytes.\n\nWhen we assign this value from Spark conf, though, we read the configuration as kilobytes then multiply it back into bytes:\n\n``` scala\n    this.fileBufferSize = (int) conf.getSizeAsKb(\"spark.shuffle.file.buffer\", \"32k\") * 1024;\n```\n\nThis same pattern appears in ExternalSorter.  This line was introduced as part of a refactoring to allow unit suffixes in our configurations.  I think the problem was that the old default units were kilobytes, so we need to call that method in order to preserve those semantics.\n\nThe comment in ExternalSorter wasn't super clear about this, though, so I'll revise the comments in both locations to explain this and add a `bytes` suffix here.\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-12T17:20:04Z",
    "diffHunk": "@@ -0,0 +1,418 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.LinkedList;\n+\n+import scala.Tuple2;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.storage.*;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.MemoryBlock;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+/**\n+ * An external sorter that is specialized for sort-based shuffle.\n+ * <p>\n+ * Incoming records are appended to data pages. When all records have been inserted (or when the\n+ * current thread's shuffle memory limit is reached), the in-memory records are sorted according to\n+ * their partition ids (using a {@link UnsafeShuffleSorter}). The sorted records are then written\n+ * to a single output file (or multiple files, if we've spilled). The format of the output files is\n+ * the same as the format of the final output file written by\n+ * {@link org.apache.spark.shuffle.sort.SortShuffleWriter}: each output partition's records are\n+ * written as a single serialized, compressed stream that can be read with a new decompression and\n+ * deserialization stream.\n+ * <p>\n+ * Unlike {@link org.apache.spark.util.collection.ExternalSorter}, this sorter does not merge its\n+ * spill files. Instead, this merging is performed in {@link UnsafeShuffleWriter}, which uses a\n+ * specialized merge procedure that avoids extra serialization/deserialization.\n+ */\n+final class UnsafeShuffleExternalSorter {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleExternalSorter.class);\n+\n+  private static final int PAGE_SIZE = PackedRecordPointer.MAXIMUM_PAGE_SIZE_BYTES;\n+  @VisibleForTesting\n+  static final int DISK_WRITE_BUFFER_SIZE = 1024 * 1024;\n+  @VisibleForTesting\n+  static final int MAX_RECORD_SIZE = PAGE_SIZE - 4;\n+\n+  private final int initialSize;\n+  private final int numPartitions;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final BlockManager blockManager;\n+  private final TaskContext taskContext;\n+  private final boolean spillingEnabled;\n+  private final ShuffleWriteMetrics writeMetrics;\n+\n+  /** The buffer size to use when writing spills using DiskBlockObjectWriter */\n+  private final int fileBufferSize;"
  }],
  "prId": 5868
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "i wonder if we should just ignore/deprecate this config, since it doesn't really make sense to run out of memory ...\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-12T06:33:16Z",
    "diffHunk": "@@ -0,0 +1,418 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.LinkedList;\n+\n+import scala.Tuple2;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.storage.*;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.MemoryBlock;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+/**\n+ * An external sorter that is specialized for sort-based shuffle.\n+ * <p>\n+ * Incoming records are appended to data pages. When all records have been inserted (or when the\n+ * current thread's shuffle memory limit is reached), the in-memory records are sorted according to\n+ * their partition ids (using a {@link UnsafeShuffleSorter}). The sorted records are then written\n+ * to a single output file (or multiple files, if we've spilled). The format of the output files is\n+ * the same as the format of the final output file written by\n+ * {@link org.apache.spark.shuffle.sort.SortShuffleWriter}: each output partition's records are\n+ * written as a single serialized, compressed stream that can be read with a new decompression and\n+ * deserialization stream.\n+ * <p>\n+ * Unlike {@link org.apache.spark.util.collection.ExternalSorter}, this sorter does not merge its\n+ * spill files. Instead, this merging is performed in {@link UnsafeShuffleWriter}, which uses a\n+ * specialized merge procedure that avoids extra serialization/deserialization.\n+ */\n+final class UnsafeShuffleExternalSorter {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleExternalSorter.class);\n+\n+  private static final int PAGE_SIZE = PackedRecordPointer.MAXIMUM_PAGE_SIZE_BYTES;\n+  @VisibleForTesting\n+  static final int DISK_WRITE_BUFFER_SIZE = 1024 * 1024;\n+  @VisibleForTesting\n+  static final int MAX_RECORD_SIZE = PAGE_SIZE - 4;\n+\n+  private final int initialSize;\n+  private final int numPartitions;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final BlockManager blockManager;\n+  private final TaskContext taskContext;\n+  private final boolean spillingEnabled;\n+  private final ShuffleWriteMetrics writeMetrics;\n+\n+  /** The buffer size to use when writing spills using DiskBlockObjectWriter */\n+  private final int fileBufferSize;\n+\n+  /**\n+   * Memory pages that hold the records being sorted. The pages in this list are freed when\n+   * spilling, although in principle we could recycle these pages across spills (on the other hand,\n+   * this might not be necessary if we maintained a pool of re-usable pages in the TaskMemoryManager\n+   * itself).\n+   */\n+  private final LinkedList<MemoryBlock> allocatedPages = new LinkedList<MemoryBlock>();\n+\n+  private final LinkedList<SpillInfo> spills = new LinkedList<SpillInfo>();\n+\n+  // All three of these variables are reset after spilling:\n+  private UnsafeShuffleSorter sorter;\n+  private MemoryBlock currentPage = null;\n+  private long currentPagePosition = -1;\n+  private long freeSpaceInCurrentPage = 0;\n+\n+  public UnsafeShuffleExternalSorter(\n+      TaskMemoryManager memoryManager,\n+      ShuffleMemoryManager shuffleMemoryManager,\n+      BlockManager blockManager,\n+      TaskContext taskContext,\n+      int initialSize,\n+      int numPartitions,\n+      SparkConf conf,\n+      ShuffleWriteMetrics writeMetrics) throws IOException {\n+    this.memoryManager = memoryManager;\n+    this.shuffleMemoryManager = shuffleMemoryManager;\n+    this.blockManager = blockManager;\n+    this.taskContext = taskContext;\n+    this.initialSize = initialSize;\n+    this.numPartitions = numPartitions;\n+    this.spillingEnabled = conf.getBoolean(\"spark.shuffle.spill\", true);"
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "Good point.  This configuration might have originally been introduced as a feature flag for the spilling code, but I agree that it doesn't really make sense to disable it now.  I'm in favor of removing it as a code simplification.  I'll log a warning message to let users know that this configuration is ignored (it might also be a good idea to do this for other configurations that we ignore, such as the spill serialization chunking size).\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-12T17:21:57Z",
    "diffHunk": "@@ -0,0 +1,418 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.LinkedList;\n+\n+import scala.Tuple2;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.storage.*;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.MemoryBlock;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+/**\n+ * An external sorter that is specialized for sort-based shuffle.\n+ * <p>\n+ * Incoming records are appended to data pages. When all records have been inserted (or when the\n+ * current thread's shuffle memory limit is reached), the in-memory records are sorted according to\n+ * their partition ids (using a {@link UnsafeShuffleSorter}). The sorted records are then written\n+ * to a single output file (or multiple files, if we've spilled). The format of the output files is\n+ * the same as the format of the final output file written by\n+ * {@link org.apache.spark.shuffle.sort.SortShuffleWriter}: each output partition's records are\n+ * written as a single serialized, compressed stream that can be read with a new decompression and\n+ * deserialization stream.\n+ * <p>\n+ * Unlike {@link org.apache.spark.util.collection.ExternalSorter}, this sorter does not merge its\n+ * spill files. Instead, this merging is performed in {@link UnsafeShuffleWriter}, which uses a\n+ * specialized merge procedure that avoids extra serialization/deserialization.\n+ */\n+final class UnsafeShuffleExternalSorter {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleExternalSorter.class);\n+\n+  private static final int PAGE_SIZE = PackedRecordPointer.MAXIMUM_PAGE_SIZE_BYTES;\n+  @VisibleForTesting\n+  static final int DISK_WRITE_BUFFER_SIZE = 1024 * 1024;\n+  @VisibleForTesting\n+  static final int MAX_RECORD_SIZE = PAGE_SIZE - 4;\n+\n+  private final int initialSize;\n+  private final int numPartitions;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final BlockManager blockManager;\n+  private final TaskContext taskContext;\n+  private final boolean spillingEnabled;\n+  private final ShuffleWriteMetrics writeMetrics;\n+\n+  /** The buffer size to use when writing spills using DiskBlockObjectWriter */\n+  private final int fileBufferSize;\n+\n+  /**\n+   * Memory pages that hold the records being sorted. The pages in this list are freed when\n+   * spilling, although in principle we could recycle these pages across spills (on the other hand,\n+   * this might not be necessary if we maintained a pool of re-usable pages in the TaskMemoryManager\n+   * itself).\n+   */\n+  private final LinkedList<MemoryBlock> allocatedPages = new LinkedList<MemoryBlock>();\n+\n+  private final LinkedList<SpillInfo> spills = new LinkedList<SpillInfo>();\n+\n+  // All three of these variables are reset after spilling:\n+  private UnsafeShuffleSorter sorter;\n+  private MemoryBlock currentPage = null;\n+  private long currentPagePosition = -1;\n+  private long freeSpaceInCurrentPage = 0;\n+\n+  public UnsafeShuffleExternalSorter(\n+      TaskMemoryManager memoryManager,\n+      ShuffleMemoryManager shuffleMemoryManager,\n+      BlockManager blockManager,\n+      TaskContext taskContext,\n+      int initialSize,\n+      int numPartitions,\n+      SparkConf conf,\n+      ShuffleWriteMetrics writeMetrics) throws IOException {\n+    this.memoryManager = memoryManager;\n+    this.shuffleMemoryManager = shuffleMemoryManager;\n+    this.blockManager = blockManager;\n+    this.taskContext = taskContext;\n+    this.initialSize = initialSize;\n+    this.numPartitions = numPartitions;\n+    this.spillingEnabled = conf.getBoolean(\"spark.shuffle.spill\", true);"
  }],
  "prId": 5868
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "include more information about how much we are requesting, and how much memory is left (if the latter part requires changing the shuffle memory manager interface, then we should do it later).\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-12T06:36:52Z",
    "diffHunk": "@@ -0,0 +1,418 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.LinkedList;\n+\n+import scala.Tuple2;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.storage.*;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.MemoryBlock;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+/**\n+ * An external sorter that is specialized for sort-based shuffle.\n+ * <p>\n+ * Incoming records are appended to data pages. When all records have been inserted (or when the\n+ * current thread's shuffle memory limit is reached), the in-memory records are sorted according to\n+ * their partition ids (using a {@link UnsafeShuffleSorter}). The sorted records are then written\n+ * to a single output file (or multiple files, if we've spilled). The format of the output files is\n+ * the same as the format of the final output file written by\n+ * {@link org.apache.spark.shuffle.sort.SortShuffleWriter}: each output partition's records are\n+ * written as a single serialized, compressed stream that can be read with a new decompression and\n+ * deserialization stream.\n+ * <p>\n+ * Unlike {@link org.apache.spark.util.collection.ExternalSorter}, this sorter does not merge its\n+ * spill files. Instead, this merging is performed in {@link UnsafeShuffleWriter}, which uses a\n+ * specialized merge procedure that avoids extra serialization/deserialization.\n+ */\n+final class UnsafeShuffleExternalSorter {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleExternalSorter.class);\n+\n+  private static final int PAGE_SIZE = PackedRecordPointer.MAXIMUM_PAGE_SIZE_BYTES;\n+  @VisibleForTesting\n+  static final int DISK_WRITE_BUFFER_SIZE = 1024 * 1024;\n+  @VisibleForTesting\n+  static final int MAX_RECORD_SIZE = PAGE_SIZE - 4;\n+\n+  private final int initialSize;\n+  private final int numPartitions;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final BlockManager blockManager;\n+  private final TaskContext taskContext;\n+  private final boolean spillingEnabled;\n+  private final ShuffleWriteMetrics writeMetrics;\n+\n+  /** The buffer size to use when writing spills using DiskBlockObjectWriter */\n+  private final int fileBufferSize;\n+\n+  /**\n+   * Memory pages that hold the records being sorted. The pages in this list are freed when\n+   * spilling, although in principle we could recycle these pages across spills (on the other hand,\n+   * this might not be necessary if we maintained a pool of re-usable pages in the TaskMemoryManager\n+   * itself).\n+   */\n+  private final LinkedList<MemoryBlock> allocatedPages = new LinkedList<MemoryBlock>();\n+\n+  private final LinkedList<SpillInfo> spills = new LinkedList<SpillInfo>();\n+\n+  // All three of these variables are reset after spilling:\n+  private UnsafeShuffleSorter sorter;\n+  private MemoryBlock currentPage = null;\n+  private long currentPagePosition = -1;\n+  private long freeSpaceInCurrentPage = 0;\n+\n+  public UnsafeShuffleExternalSorter(\n+      TaskMemoryManager memoryManager,\n+      ShuffleMemoryManager shuffleMemoryManager,\n+      BlockManager blockManager,\n+      TaskContext taskContext,\n+      int initialSize,\n+      int numPartitions,\n+      SparkConf conf,\n+      ShuffleWriteMetrics writeMetrics) throws IOException {\n+    this.memoryManager = memoryManager;\n+    this.shuffleMemoryManager = shuffleMemoryManager;\n+    this.blockManager = blockManager;\n+    this.taskContext = taskContext;\n+    this.initialSize = initialSize;\n+    this.numPartitions = numPartitions;\n+    this.spillingEnabled = conf.getBoolean(\"spark.shuffle.spill\", true);\n+    // Use getSizeAsKb (not bytes) to maintain backwards compatibility for units\n+    this.fileBufferSize = (int) conf.getSizeAsKb(\"spark.shuffle.file.buffer\", \"32k\") * 1024;\n+    this.writeMetrics = writeMetrics;\n+    openSorter();\n+  }\n+\n+  /**\n+   * Allocates a new sorter. Called when opening the spill writer for the first time and after\n+   * each spill.\n+   */\n+  private void openSorter() throws IOException {\n+    // TODO: move this sizing calculation logic into a static method of sorter:\n+    final long memoryRequested = initialSize * 8L;\n+    if (spillingEnabled) {\n+      final long memoryAcquired = shuffleMemoryManager.tryToAcquire(memoryRequested);\n+      if (memoryAcquired != memoryRequested) {\n+        shuffleMemoryManager.release(memoryAcquired);\n+        throw new IOException(\"Could not acquire memory!\");"
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "I'll extend this log message to include information on the size of the failed request.\n\nI find the semantics of `tryToAcquire` to be a little confusing because it allows for these weird sorts of partial failures where we request a given amount of memory but are granted _less_ memory rather than being granted no memory at all.  As far as I can tell, the other callers of `tryToAcquire` don't use the result of partially-fulfilled request but instead choose to spill and release _all_ shuffle memory for that thread.  Here, I'm being extra-defensive and am explicitly releasing whatever memory was granted to us rather than all memory.\n\nTL;DR: there are _lots_ of changes that I'd like to make to ShuffleMemoryManager, but let's do them later since they'll require more design.\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-12T17:28:06Z",
    "diffHunk": "@@ -0,0 +1,418 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.LinkedList;\n+\n+import scala.Tuple2;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.storage.*;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.MemoryBlock;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+/**\n+ * An external sorter that is specialized for sort-based shuffle.\n+ * <p>\n+ * Incoming records are appended to data pages. When all records have been inserted (or when the\n+ * current thread's shuffle memory limit is reached), the in-memory records are sorted according to\n+ * their partition ids (using a {@link UnsafeShuffleSorter}). The sorted records are then written\n+ * to a single output file (or multiple files, if we've spilled). The format of the output files is\n+ * the same as the format of the final output file written by\n+ * {@link org.apache.spark.shuffle.sort.SortShuffleWriter}: each output partition's records are\n+ * written as a single serialized, compressed stream that can be read with a new decompression and\n+ * deserialization stream.\n+ * <p>\n+ * Unlike {@link org.apache.spark.util.collection.ExternalSorter}, this sorter does not merge its\n+ * spill files. Instead, this merging is performed in {@link UnsafeShuffleWriter}, which uses a\n+ * specialized merge procedure that avoids extra serialization/deserialization.\n+ */\n+final class UnsafeShuffleExternalSorter {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleExternalSorter.class);\n+\n+  private static final int PAGE_SIZE = PackedRecordPointer.MAXIMUM_PAGE_SIZE_BYTES;\n+  @VisibleForTesting\n+  static final int DISK_WRITE_BUFFER_SIZE = 1024 * 1024;\n+  @VisibleForTesting\n+  static final int MAX_RECORD_SIZE = PAGE_SIZE - 4;\n+\n+  private final int initialSize;\n+  private final int numPartitions;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final BlockManager blockManager;\n+  private final TaskContext taskContext;\n+  private final boolean spillingEnabled;\n+  private final ShuffleWriteMetrics writeMetrics;\n+\n+  /** The buffer size to use when writing spills using DiskBlockObjectWriter */\n+  private final int fileBufferSize;\n+\n+  /**\n+   * Memory pages that hold the records being sorted. The pages in this list are freed when\n+   * spilling, although in principle we could recycle these pages across spills (on the other hand,\n+   * this might not be necessary if we maintained a pool of re-usable pages in the TaskMemoryManager\n+   * itself).\n+   */\n+  private final LinkedList<MemoryBlock> allocatedPages = new LinkedList<MemoryBlock>();\n+\n+  private final LinkedList<SpillInfo> spills = new LinkedList<SpillInfo>();\n+\n+  // All three of these variables are reset after spilling:\n+  private UnsafeShuffleSorter sorter;\n+  private MemoryBlock currentPage = null;\n+  private long currentPagePosition = -1;\n+  private long freeSpaceInCurrentPage = 0;\n+\n+  public UnsafeShuffleExternalSorter(\n+      TaskMemoryManager memoryManager,\n+      ShuffleMemoryManager shuffleMemoryManager,\n+      BlockManager blockManager,\n+      TaskContext taskContext,\n+      int initialSize,\n+      int numPartitions,\n+      SparkConf conf,\n+      ShuffleWriteMetrics writeMetrics) throws IOException {\n+    this.memoryManager = memoryManager;\n+    this.shuffleMemoryManager = shuffleMemoryManager;\n+    this.blockManager = blockManager;\n+    this.taskContext = taskContext;\n+    this.initialSize = initialSize;\n+    this.numPartitions = numPartitions;\n+    this.spillingEnabled = conf.getBoolean(\"spark.shuffle.spill\", true);\n+    // Use getSizeAsKb (not bytes) to maintain backwards compatibility for units\n+    this.fileBufferSize = (int) conf.getSizeAsKb(\"spark.shuffle.file.buffer\", \"32k\") * 1024;\n+    this.writeMetrics = writeMetrics;\n+    openSorter();\n+  }\n+\n+  /**\n+   * Allocates a new sorter. Called when opening the spill writer for the first time and after\n+   * each spill.\n+   */\n+  private void openSorter() throws IOException {\n+    // TODO: move this sizing calculation logic into a static method of sorter:\n+    final long memoryRequested = initialSize * 8L;\n+    if (spillingEnabled) {\n+      final long memoryAcquired = shuffleMemoryManager.tryToAcquire(memoryRequested);\n+      if (memoryAcquired != memoryRequested) {\n+        shuffleMemoryManager.release(memoryAcquired);\n+        throw new IOException(\"Could not acquire memory!\");"
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "I'll extend this log message to include information on the size of the failed request.\n\nI find the semantics of `tryToAcquire` to be a little confusing because it allows for these weird sorts of partial failures where we request a given amount of memory but are granted _less_ memory rather than being granted no memory at all.  As far as I can tell, the other callers of `tryToAcquire` don't use the result of partially-fulfilled request but instead choose to spill and release _all_ shuffle memory for that thread.  Here, I'm being extra-defensive and am explicitly releasing whatever memory was granted to us rather than all memory.\n\nTL;DR: there are _lots_ of changes that I'd like to make to ShuffleMemoryManager, but let's do them later since they'll require more design.\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-12T17:28:30Z",
    "diffHunk": "@@ -0,0 +1,418 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.LinkedList;\n+\n+import scala.Tuple2;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.storage.*;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.MemoryBlock;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+/**\n+ * An external sorter that is specialized for sort-based shuffle.\n+ * <p>\n+ * Incoming records are appended to data pages. When all records have been inserted (or when the\n+ * current thread's shuffle memory limit is reached), the in-memory records are sorted according to\n+ * their partition ids (using a {@link UnsafeShuffleSorter}). The sorted records are then written\n+ * to a single output file (or multiple files, if we've spilled). The format of the output files is\n+ * the same as the format of the final output file written by\n+ * {@link org.apache.spark.shuffle.sort.SortShuffleWriter}: each output partition's records are\n+ * written as a single serialized, compressed stream that can be read with a new decompression and\n+ * deserialization stream.\n+ * <p>\n+ * Unlike {@link org.apache.spark.util.collection.ExternalSorter}, this sorter does not merge its\n+ * spill files. Instead, this merging is performed in {@link UnsafeShuffleWriter}, which uses a\n+ * specialized merge procedure that avoids extra serialization/deserialization.\n+ */\n+final class UnsafeShuffleExternalSorter {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleExternalSorter.class);\n+\n+  private static final int PAGE_SIZE = PackedRecordPointer.MAXIMUM_PAGE_SIZE_BYTES;\n+  @VisibleForTesting\n+  static final int DISK_WRITE_BUFFER_SIZE = 1024 * 1024;\n+  @VisibleForTesting\n+  static final int MAX_RECORD_SIZE = PAGE_SIZE - 4;\n+\n+  private final int initialSize;\n+  private final int numPartitions;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final BlockManager blockManager;\n+  private final TaskContext taskContext;\n+  private final boolean spillingEnabled;\n+  private final ShuffleWriteMetrics writeMetrics;\n+\n+  /** The buffer size to use when writing spills using DiskBlockObjectWriter */\n+  private final int fileBufferSize;\n+\n+  /**\n+   * Memory pages that hold the records being sorted. The pages in this list are freed when\n+   * spilling, although in principle we could recycle these pages across spills (on the other hand,\n+   * this might not be necessary if we maintained a pool of re-usable pages in the TaskMemoryManager\n+   * itself).\n+   */\n+  private final LinkedList<MemoryBlock> allocatedPages = new LinkedList<MemoryBlock>();\n+\n+  private final LinkedList<SpillInfo> spills = new LinkedList<SpillInfo>();\n+\n+  // All three of these variables are reset after spilling:\n+  private UnsafeShuffleSorter sorter;\n+  private MemoryBlock currentPage = null;\n+  private long currentPagePosition = -1;\n+  private long freeSpaceInCurrentPage = 0;\n+\n+  public UnsafeShuffleExternalSorter(\n+      TaskMemoryManager memoryManager,\n+      ShuffleMemoryManager shuffleMemoryManager,\n+      BlockManager blockManager,\n+      TaskContext taskContext,\n+      int initialSize,\n+      int numPartitions,\n+      SparkConf conf,\n+      ShuffleWriteMetrics writeMetrics) throws IOException {\n+    this.memoryManager = memoryManager;\n+    this.shuffleMemoryManager = shuffleMemoryManager;\n+    this.blockManager = blockManager;\n+    this.taskContext = taskContext;\n+    this.initialSize = initialSize;\n+    this.numPartitions = numPartitions;\n+    this.spillingEnabled = conf.getBoolean(\"spark.shuffle.spill\", true);\n+    // Use getSizeAsKb (not bytes) to maintain backwards compatibility for units\n+    this.fileBufferSize = (int) conf.getSizeAsKb(\"spark.shuffle.file.buffer\", \"32k\") * 1024;\n+    this.writeMetrics = writeMetrics;\n+    openSorter();\n+  }\n+\n+  /**\n+   * Allocates a new sorter. Called when opening the spill writer for the first time and after\n+   * each spill.\n+   */\n+  private void openSorter() throws IOException {\n+    // TODO: move this sizing calculation logic into a static method of sorter:\n+    final long memoryRequested = initialSize * 8L;\n+    if (spillingEnabled) {\n+      final long memoryAcquired = shuffleMemoryManager.tryToAcquire(memoryRequested);\n+      if (memoryAcquired != memoryRequested) {\n+        shuffleMemoryManager.release(memoryAcquired);\n+        throw new IOException(\"Could not acquire memory!\");"
  }],
  "prId": 5868
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "writeSpillFile -> writeSortedChunk?\n\nisSpill -> isLastChunk\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-12T06:39:33Z",
    "diffHunk": "@@ -0,0 +1,418 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.LinkedList;\n+\n+import scala.Tuple2;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.storage.*;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.MemoryBlock;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+/**\n+ * An external sorter that is specialized for sort-based shuffle.\n+ * <p>\n+ * Incoming records are appended to data pages. When all records have been inserted (or when the\n+ * current thread's shuffle memory limit is reached), the in-memory records are sorted according to\n+ * their partition ids (using a {@link UnsafeShuffleSorter}). The sorted records are then written\n+ * to a single output file (or multiple files, if we've spilled). The format of the output files is\n+ * the same as the format of the final output file written by\n+ * {@link org.apache.spark.shuffle.sort.SortShuffleWriter}: each output partition's records are\n+ * written as a single serialized, compressed stream that can be read with a new decompression and\n+ * deserialization stream.\n+ * <p>\n+ * Unlike {@link org.apache.spark.util.collection.ExternalSorter}, this sorter does not merge its\n+ * spill files. Instead, this merging is performed in {@link UnsafeShuffleWriter}, which uses a\n+ * specialized merge procedure that avoids extra serialization/deserialization.\n+ */\n+final class UnsafeShuffleExternalSorter {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleExternalSorter.class);\n+\n+  private static final int PAGE_SIZE = PackedRecordPointer.MAXIMUM_PAGE_SIZE_BYTES;\n+  @VisibleForTesting\n+  static final int DISK_WRITE_BUFFER_SIZE = 1024 * 1024;\n+  @VisibleForTesting\n+  static final int MAX_RECORD_SIZE = PAGE_SIZE - 4;\n+\n+  private final int initialSize;\n+  private final int numPartitions;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final BlockManager blockManager;\n+  private final TaskContext taskContext;\n+  private final boolean spillingEnabled;\n+  private final ShuffleWriteMetrics writeMetrics;\n+\n+  /** The buffer size to use when writing spills using DiskBlockObjectWriter */\n+  private final int fileBufferSize;\n+\n+  /**\n+   * Memory pages that hold the records being sorted. The pages in this list are freed when\n+   * spilling, although in principle we could recycle these pages across spills (on the other hand,\n+   * this might not be necessary if we maintained a pool of re-usable pages in the TaskMemoryManager\n+   * itself).\n+   */\n+  private final LinkedList<MemoryBlock> allocatedPages = new LinkedList<MemoryBlock>();\n+\n+  private final LinkedList<SpillInfo> spills = new LinkedList<SpillInfo>();\n+\n+  // All three of these variables are reset after spilling:\n+  private UnsafeShuffleSorter sorter;\n+  private MemoryBlock currentPage = null;\n+  private long currentPagePosition = -1;\n+  private long freeSpaceInCurrentPage = 0;\n+\n+  public UnsafeShuffleExternalSorter(\n+      TaskMemoryManager memoryManager,\n+      ShuffleMemoryManager shuffleMemoryManager,\n+      BlockManager blockManager,\n+      TaskContext taskContext,\n+      int initialSize,\n+      int numPartitions,\n+      SparkConf conf,\n+      ShuffleWriteMetrics writeMetrics) throws IOException {\n+    this.memoryManager = memoryManager;\n+    this.shuffleMemoryManager = shuffleMemoryManager;\n+    this.blockManager = blockManager;\n+    this.taskContext = taskContext;\n+    this.initialSize = initialSize;\n+    this.numPartitions = numPartitions;\n+    this.spillingEnabled = conf.getBoolean(\"spark.shuffle.spill\", true);\n+    // Use getSizeAsKb (not bytes) to maintain backwards compatibility for units\n+    this.fileBufferSize = (int) conf.getSizeAsKb(\"spark.shuffle.file.buffer\", \"32k\") * 1024;\n+    this.writeMetrics = writeMetrics;\n+    openSorter();\n+  }\n+\n+  /**\n+   * Allocates a new sorter. Called when opening the spill writer for the first time and after\n+   * each spill.\n+   */\n+  private void openSorter() throws IOException {\n+    // TODO: move this sizing calculation logic into a static method of sorter:\n+    final long memoryRequested = initialSize * 8L;\n+    if (spillingEnabled) {\n+      final long memoryAcquired = shuffleMemoryManager.tryToAcquire(memoryRequested);\n+      if (memoryAcquired != memoryRequested) {\n+        shuffleMemoryManager.release(memoryAcquired);\n+        throw new IOException(\"Could not acquire memory!\");\n+      }\n+    }\n+\n+    this.sorter = new UnsafeShuffleSorter(initialSize);\n+  }\n+\n+  /**\n+   * Sorts the in-memory records and writes the sorted records to a spill file.\n+   * This method does not free the sort data structures.\n+   *\n+   * @param isSpill if true, this indicates that we're writing a spill and that bytes written should\n+   *                be counted towards shuffle spill metrics rather than shuffle write metrics.\n+   */\n+  private void writeSpillFile(boolean isSpill) throws IOException {"
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "I went with `writeSortedFile` to emphasize that this writes a new file.\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-12T20:31:22Z",
    "diffHunk": "@@ -0,0 +1,418 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.LinkedList;\n+\n+import scala.Tuple2;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.storage.*;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.MemoryBlock;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+/**\n+ * An external sorter that is specialized for sort-based shuffle.\n+ * <p>\n+ * Incoming records are appended to data pages. When all records have been inserted (or when the\n+ * current thread's shuffle memory limit is reached), the in-memory records are sorted according to\n+ * their partition ids (using a {@link UnsafeShuffleSorter}). The sorted records are then written\n+ * to a single output file (or multiple files, if we've spilled). The format of the output files is\n+ * the same as the format of the final output file written by\n+ * {@link org.apache.spark.shuffle.sort.SortShuffleWriter}: each output partition's records are\n+ * written as a single serialized, compressed stream that can be read with a new decompression and\n+ * deserialization stream.\n+ * <p>\n+ * Unlike {@link org.apache.spark.util.collection.ExternalSorter}, this sorter does not merge its\n+ * spill files. Instead, this merging is performed in {@link UnsafeShuffleWriter}, which uses a\n+ * specialized merge procedure that avoids extra serialization/deserialization.\n+ */\n+final class UnsafeShuffleExternalSorter {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleExternalSorter.class);\n+\n+  private static final int PAGE_SIZE = PackedRecordPointer.MAXIMUM_PAGE_SIZE_BYTES;\n+  @VisibleForTesting\n+  static final int DISK_WRITE_BUFFER_SIZE = 1024 * 1024;\n+  @VisibleForTesting\n+  static final int MAX_RECORD_SIZE = PAGE_SIZE - 4;\n+\n+  private final int initialSize;\n+  private final int numPartitions;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final BlockManager blockManager;\n+  private final TaskContext taskContext;\n+  private final boolean spillingEnabled;\n+  private final ShuffleWriteMetrics writeMetrics;\n+\n+  /** The buffer size to use when writing spills using DiskBlockObjectWriter */\n+  private final int fileBufferSize;\n+\n+  /**\n+   * Memory pages that hold the records being sorted. The pages in this list are freed when\n+   * spilling, although in principle we could recycle these pages across spills (on the other hand,\n+   * this might not be necessary if we maintained a pool of re-usable pages in the TaskMemoryManager\n+   * itself).\n+   */\n+  private final LinkedList<MemoryBlock> allocatedPages = new LinkedList<MemoryBlock>();\n+\n+  private final LinkedList<SpillInfo> spills = new LinkedList<SpillInfo>();\n+\n+  // All three of these variables are reset after spilling:\n+  private UnsafeShuffleSorter sorter;\n+  private MemoryBlock currentPage = null;\n+  private long currentPagePosition = -1;\n+  private long freeSpaceInCurrentPage = 0;\n+\n+  public UnsafeShuffleExternalSorter(\n+      TaskMemoryManager memoryManager,\n+      ShuffleMemoryManager shuffleMemoryManager,\n+      BlockManager blockManager,\n+      TaskContext taskContext,\n+      int initialSize,\n+      int numPartitions,\n+      SparkConf conf,\n+      ShuffleWriteMetrics writeMetrics) throws IOException {\n+    this.memoryManager = memoryManager;\n+    this.shuffleMemoryManager = shuffleMemoryManager;\n+    this.blockManager = blockManager;\n+    this.taskContext = taskContext;\n+    this.initialSize = initialSize;\n+    this.numPartitions = numPartitions;\n+    this.spillingEnabled = conf.getBoolean(\"spark.shuffle.spill\", true);\n+    // Use getSizeAsKb (not bytes) to maintain backwards compatibility for units\n+    this.fileBufferSize = (int) conf.getSizeAsKb(\"spark.shuffle.file.buffer\", \"32k\") * 1024;\n+    this.writeMetrics = writeMetrics;\n+    openSorter();\n+  }\n+\n+  /**\n+   * Allocates a new sorter. Called when opening the spill writer for the first time and after\n+   * each spill.\n+   */\n+  private void openSorter() throws IOException {\n+    // TODO: move this sizing calculation logic into a static method of sorter:\n+    final long memoryRequested = initialSize * 8L;\n+    if (spillingEnabled) {\n+      final long memoryAcquired = shuffleMemoryManager.tryToAcquire(memoryRequested);\n+      if (memoryAcquired != memoryRequested) {\n+        shuffleMemoryManager.release(memoryAcquired);\n+        throw new IOException(\"Could not acquire memory!\");\n+      }\n+    }\n+\n+    this.sorter = new UnsafeShuffleSorter(initialSize);\n+  }\n+\n+  /**\n+   * Sorts the in-memory records and writes the sorted records to a spill file.\n+   * This method does not free the sort data structures.\n+   *\n+   * @param isSpill if true, this indicates that we're writing a spill and that bytes written should\n+   *                be counted towards shuffle spill metrics rather than shuffle write metrics.\n+   */\n+  private void writeSpillFile(boolean isSpill) throws IOException {"
  }],
  "prId": 5868
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "use {} ?\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-12T07:17:30Z",
    "diffHunk": "@@ -0,0 +1,418 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.LinkedList;\n+\n+import scala.Tuple2;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.storage.*;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.MemoryBlock;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+/**\n+ * An external sorter that is specialized for sort-based shuffle.\n+ * <p>\n+ * Incoming records are appended to data pages. When all records have been inserted (or when the\n+ * current thread's shuffle memory limit is reached), the in-memory records are sorted according to\n+ * their partition ids (using a {@link UnsafeShuffleSorter}). The sorted records are then written\n+ * to a single output file (or multiple files, if we've spilled). The format of the output files is\n+ * the same as the format of the final output file written by\n+ * {@link org.apache.spark.shuffle.sort.SortShuffleWriter}: each output partition's records are\n+ * written as a single serialized, compressed stream that can be read with a new decompression and\n+ * deserialization stream.\n+ * <p>\n+ * Unlike {@link org.apache.spark.util.collection.ExternalSorter}, this sorter does not merge its\n+ * spill files. Instead, this merging is performed in {@link UnsafeShuffleWriter}, which uses a\n+ * specialized merge procedure that avoids extra serialization/deserialization.\n+ */\n+final class UnsafeShuffleExternalSorter {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleExternalSorter.class);\n+\n+  private static final int PAGE_SIZE = PackedRecordPointer.MAXIMUM_PAGE_SIZE_BYTES;\n+  @VisibleForTesting\n+  static final int DISK_WRITE_BUFFER_SIZE = 1024 * 1024;\n+  @VisibleForTesting\n+  static final int MAX_RECORD_SIZE = PAGE_SIZE - 4;\n+\n+  private final int initialSize;\n+  private final int numPartitions;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final BlockManager blockManager;\n+  private final TaskContext taskContext;\n+  private final boolean spillingEnabled;\n+  private final ShuffleWriteMetrics writeMetrics;\n+\n+  /** The buffer size to use when writing spills using DiskBlockObjectWriter */\n+  private final int fileBufferSize;\n+\n+  /**\n+   * Memory pages that hold the records being sorted. The pages in this list are freed when\n+   * spilling, although in principle we could recycle these pages across spills (on the other hand,\n+   * this might not be necessary if we maintained a pool of re-usable pages in the TaskMemoryManager\n+   * itself).\n+   */\n+  private final LinkedList<MemoryBlock> allocatedPages = new LinkedList<MemoryBlock>();\n+\n+  private final LinkedList<SpillInfo> spills = new LinkedList<SpillInfo>();\n+\n+  // All three of these variables are reset after spilling:\n+  private UnsafeShuffleSorter sorter;\n+  private MemoryBlock currentPage = null;\n+  private long currentPagePosition = -1;\n+  private long freeSpaceInCurrentPage = 0;\n+\n+  public UnsafeShuffleExternalSorter(\n+      TaskMemoryManager memoryManager,\n+      ShuffleMemoryManager shuffleMemoryManager,\n+      BlockManager blockManager,\n+      TaskContext taskContext,\n+      int initialSize,\n+      int numPartitions,\n+      SparkConf conf,\n+      ShuffleWriteMetrics writeMetrics) throws IOException {\n+    this.memoryManager = memoryManager;\n+    this.shuffleMemoryManager = shuffleMemoryManager;\n+    this.blockManager = blockManager;\n+    this.taskContext = taskContext;\n+    this.initialSize = initialSize;\n+    this.numPartitions = numPartitions;\n+    this.spillingEnabled = conf.getBoolean(\"spark.shuffle.spill\", true);\n+    // Use getSizeAsKb (not bytes) to maintain backwards compatibility for units\n+    this.fileBufferSize = (int) conf.getSizeAsKb(\"spark.shuffle.file.buffer\", \"32k\") * 1024;\n+    this.writeMetrics = writeMetrics;\n+    openSorter();\n+  }\n+\n+  /**\n+   * Allocates a new sorter. Called when opening the spill writer for the first time and after\n+   * each spill.\n+   */\n+  private void openSorter() throws IOException {\n+    // TODO: move this sizing calculation logic into a static method of sorter:\n+    final long memoryRequested = initialSize * 8L;\n+    if (spillingEnabled) {\n+      final long memoryAcquired = shuffleMemoryManager.tryToAcquire(memoryRequested);\n+      if (memoryAcquired != memoryRequested) {\n+        shuffleMemoryManager.release(memoryAcquired);\n+        throw new IOException(\"Could not acquire memory!\");\n+      }\n+    }\n+\n+    this.sorter = new UnsafeShuffleSorter(initialSize);\n+  }\n+\n+  /**\n+   * Sorts the in-memory records and writes the sorted records to a spill file.\n+   * This method does not free the sort data structures.\n+   *\n+   * @param isSpill if true, this indicates that we're writing a spill and that bytes written should\n+   *                be counted towards shuffle spill metrics rather than shuffle write metrics.\n+   */\n+  private void writeSpillFile(boolean isSpill) throws IOException {\n+\n+    final ShuffleWriteMetrics writeMetricsToUse;\n+\n+    if (isSpill) {\n+      // We're spilling, so bytes written should be counted towards spill rather than write.\n+      // Create a dummy WriteMetrics object to absorb these metrics, since we don't want to count\n+      // them towards shuffle bytes written.\n+      writeMetricsToUse = new ShuffleWriteMetrics();\n+    } else {\n+      // We're writing the final non-spill file, so we _do_ want to count this as shuffle bytes.\n+      writeMetricsToUse = writeMetrics;\n+    }\n+\n+    // This call performs the actual sort.\n+    final UnsafeShuffleSorter.UnsafeShuffleSorterIterator sortedRecords =\n+      sorter.getSortedIterator();\n+\n+    // Currently, we need to open a new DiskBlockObjectWriter for each partition; we can avoid this\n+    // after SPARK-5581 is fixed.\n+    BlockObjectWriter writer;\n+\n+    // Small writes to DiskBlockObjectWriter will be fairly inefficient. Since there doesn't seem to\n+    // be an API to directly transfer bytes from managed memory to the disk writer, we buffer\n+    // data through a byte array. This array does not need to be large enough to hold a single\n+    // record;\n+    final byte[] writeBuffer = new byte[DISK_WRITE_BUFFER_SIZE];\n+\n+    // Because this output will be read during shuffle, its compression codec must be controlled by\n+    // spark.shuffle.compress instead of spark.shuffle.spill.compress, so we need to use\n+    // createTempShuffleBlock here; see SPARK-3426 for more details.\n+    final Tuple2<TempShuffleBlockId, File> spilledFileInfo =\n+      blockManager.diskBlockManager().createTempShuffleBlock();\n+    final File file = spilledFileInfo._2();\n+    final TempShuffleBlockId blockId = spilledFileInfo._1();\n+    final SpillInfo spillInfo = new SpillInfo(numPartitions, file, blockId);\n+\n+    // Unfortunately, we need a serializer instance in order to construct a DiskBlockObjectWriter.\n+    // Our write path doesn't actually use this serializer (since we end up calling the `write()`\n+    // OutputStream methods), but DiskBlockObjectWriter still calls some methods on it. To work\n+    // around this, we pass a dummy no-op serializer.\n+    final SerializerInstance ser = DummySerializerInstance.INSTANCE;\n+\n+    writer = blockManager.getDiskWriter(blockId, file, ser, fileBufferSize, writeMetricsToUse);\n+\n+    int currentPartition = -1;\n+    while (sortedRecords.hasNext()) {\n+      sortedRecords.loadNext();\n+      final int partition = sortedRecords.packedRecordPointer.getPartitionId();\n+      assert (partition >= currentPartition);\n+      if (partition != currentPartition) {\n+        // Switch to the new partition\n+        if (currentPartition != -1) {\n+          writer.commitAndClose();\n+          spillInfo.partitionLengths[currentPartition] = writer.fileSegment().length();\n+        }\n+        currentPartition = partition;\n+        writer = blockManager.getDiskWriter(blockId, file, ser, fileBufferSize, writeMetricsToUse);\n+      }\n+\n+      final long recordPointer = sortedRecords.packedRecordPointer.getRecordPointer();\n+      final Object recordPage = memoryManager.getPage(recordPointer);\n+      final long recordOffsetInPage = memoryManager.getOffsetInPage(recordPointer);\n+      int dataRemaining = PlatformDependent.UNSAFE.getInt(recordPage, recordOffsetInPage);\n+      long recordReadPosition = recordOffsetInPage + 4; // skip over record length\n+      while (dataRemaining > 0) {\n+        final int toTransfer = Math.min(DISK_WRITE_BUFFER_SIZE, dataRemaining);\n+        PlatformDependent.copyMemory(\n+          recordPage,\n+          recordReadPosition,\n+          writeBuffer,\n+          PlatformDependent.BYTE_ARRAY_OFFSET,\n+          toTransfer);\n+        writer.write(writeBuffer, 0, toTransfer);\n+        recordReadPosition += toTransfer;\n+        dataRemaining -= toTransfer;\n+      }\n+      // TODO: add a test that detects whether we leave this call out:\n+      writer.recordWritten();\n+    }\n+\n+    if (writer != null) {\n+      writer.commitAndClose();\n+      // If `writeSpillFile()` was called from `closeAndGetSpills()` and no records were inserted,\n+      // then the spill file might be empty. Note that it might be better to avoid calling\n+      // writeSpillFile() in that case.\n+      if (currentPartition != -1) {\n+        spillInfo.partitionLengths[currentPartition] = writer.fileSegment().length();\n+        spills.add(spillInfo);\n+      }\n+    }\n+\n+    if (isSpill) {\n+      writeMetrics.incShuffleRecordsWritten(writeMetricsToUse.shuffleRecordsWritten());\n+      // Consistent with ExternalSorter, we do not count this IO towards shuffle write time.\n+      // This means that this IO time is not accounted for anywhere; SPARK-3577 will fix this.\n+      // writeMetrics.incShuffleWriteTime(writeMetricsToUse.shuffleWriteTime());\n+      taskContext.taskMetrics().incDiskBytesSpilled(writeMetricsToUse.shuffleBytesWritten());\n+    }\n+  }\n+\n+  /**\n+   * Sort and spill the current records in response to memory pressure.\n+   */\n+  @VisibleForTesting\n+  void spill() throws IOException {\n+    final long threadId = Thread.currentThread().getId();\n+    logger.info(\"Thread \" + threadId + \" spilling sort data of \" +"
  }],
  "prId": 5868
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "i'd call this maybe initForNewChunk or something like that to make it more obvious\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-12T07:18:41Z",
    "diffHunk": "@@ -0,0 +1,418 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.LinkedList;\n+\n+import scala.Tuple2;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.storage.*;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.MemoryBlock;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+/**\n+ * An external sorter that is specialized for sort-based shuffle.\n+ * <p>\n+ * Incoming records are appended to data pages. When all records have been inserted (or when the\n+ * current thread's shuffle memory limit is reached), the in-memory records are sorted according to\n+ * their partition ids (using a {@link UnsafeShuffleSorter}). The sorted records are then written\n+ * to a single output file (or multiple files, if we've spilled). The format of the output files is\n+ * the same as the format of the final output file written by\n+ * {@link org.apache.spark.shuffle.sort.SortShuffleWriter}: each output partition's records are\n+ * written as a single serialized, compressed stream that can be read with a new decompression and\n+ * deserialization stream.\n+ * <p>\n+ * Unlike {@link org.apache.spark.util.collection.ExternalSorter}, this sorter does not merge its\n+ * spill files. Instead, this merging is performed in {@link UnsafeShuffleWriter}, which uses a\n+ * specialized merge procedure that avoids extra serialization/deserialization.\n+ */\n+final class UnsafeShuffleExternalSorter {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleExternalSorter.class);\n+\n+  private static final int PAGE_SIZE = PackedRecordPointer.MAXIMUM_PAGE_SIZE_BYTES;\n+  @VisibleForTesting\n+  static final int DISK_WRITE_BUFFER_SIZE = 1024 * 1024;\n+  @VisibleForTesting\n+  static final int MAX_RECORD_SIZE = PAGE_SIZE - 4;\n+\n+  private final int initialSize;\n+  private final int numPartitions;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final BlockManager blockManager;\n+  private final TaskContext taskContext;\n+  private final boolean spillingEnabled;\n+  private final ShuffleWriteMetrics writeMetrics;\n+\n+  /** The buffer size to use when writing spills using DiskBlockObjectWriter */\n+  private final int fileBufferSize;\n+\n+  /**\n+   * Memory pages that hold the records being sorted. The pages in this list are freed when\n+   * spilling, although in principle we could recycle these pages across spills (on the other hand,\n+   * this might not be necessary if we maintained a pool of re-usable pages in the TaskMemoryManager\n+   * itself).\n+   */\n+  private final LinkedList<MemoryBlock> allocatedPages = new LinkedList<MemoryBlock>();\n+\n+  private final LinkedList<SpillInfo> spills = new LinkedList<SpillInfo>();\n+\n+  // All three of these variables are reset after spilling:\n+  private UnsafeShuffleSorter sorter;\n+  private MemoryBlock currentPage = null;\n+  private long currentPagePosition = -1;\n+  private long freeSpaceInCurrentPage = 0;\n+\n+  public UnsafeShuffleExternalSorter(\n+      TaskMemoryManager memoryManager,\n+      ShuffleMemoryManager shuffleMemoryManager,\n+      BlockManager blockManager,\n+      TaskContext taskContext,\n+      int initialSize,\n+      int numPartitions,\n+      SparkConf conf,\n+      ShuffleWriteMetrics writeMetrics) throws IOException {\n+    this.memoryManager = memoryManager;\n+    this.shuffleMemoryManager = shuffleMemoryManager;\n+    this.blockManager = blockManager;\n+    this.taskContext = taskContext;\n+    this.initialSize = initialSize;\n+    this.numPartitions = numPartitions;\n+    this.spillingEnabled = conf.getBoolean(\"spark.shuffle.spill\", true);\n+    // Use getSizeAsKb (not bytes) to maintain backwards compatibility for units\n+    this.fileBufferSize = (int) conf.getSizeAsKb(\"spark.shuffle.file.buffer\", \"32k\") * 1024;\n+    this.writeMetrics = writeMetrics;\n+    openSorter();\n+  }\n+\n+  /**\n+   * Allocates a new sorter. Called when opening the spill writer for the first time and after\n+   * each spill.\n+   */\n+  private void openSorter() throws IOException {"
  }],
  "prId": 5868
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "i thought we are doing one page at a time. doubling seems too much?\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-12T07:22:40Z",
    "diffHunk": "@@ -0,0 +1,418 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.LinkedList;\n+\n+import scala.Tuple2;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.storage.*;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.MemoryBlock;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+/**\n+ * An external sorter that is specialized for sort-based shuffle.\n+ * <p>\n+ * Incoming records are appended to data pages. When all records have been inserted (or when the\n+ * current thread's shuffle memory limit is reached), the in-memory records are sorted according to\n+ * their partition ids (using a {@link UnsafeShuffleSorter}). The sorted records are then written\n+ * to a single output file (or multiple files, if we've spilled). The format of the output files is\n+ * the same as the format of the final output file written by\n+ * {@link org.apache.spark.shuffle.sort.SortShuffleWriter}: each output partition's records are\n+ * written as a single serialized, compressed stream that can be read with a new decompression and\n+ * deserialization stream.\n+ * <p>\n+ * Unlike {@link org.apache.spark.util.collection.ExternalSorter}, this sorter does not merge its\n+ * spill files. Instead, this merging is performed in {@link UnsafeShuffleWriter}, which uses a\n+ * specialized merge procedure that avoids extra serialization/deserialization.\n+ */\n+final class UnsafeShuffleExternalSorter {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleExternalSorter.class);\n+\n+  private static final int PAGE_SIZE = PackedRecordPointer.MAXIMUM_PAGE_SIZE_BYTES;\n+  @VisibleForTesting\n+  static final int DISK_WRITE_BUFFER_SIZE = 1024 * 1024;\n+  @VisibleForTesting\n+  static final int MAX_RECORD_SIZE = PAGE_SIZE - 4;\n+\n+  private final int initialSize;\n+  private final int numPartitions;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final BlockManager blockManager;\n+  private final TaskContext taskContext;\n+  private final boolean spillingEnabled;\n+  private final ShuffleWriteMetrics writeMetrics;\n+\n+  /** The buffer size to use when writing spills using DiskBlockObjectWriter */\n+  private final int fileBufferSize;\n+\n+  /**\n+   * Memory pages that hold the records being sorted. The pages in this list are freed when\n+   * spilling, although in principle we could recycle these pages across spills (on the other hand,\n+   * this might not be necessary if we maintained a pool of re-usable pages in the TaskMemoryManager\n+   * itself).\n+   */\n+  private final LinkedList<MemoryBlock> allocatedPages = new LinkedList<MemoryBlock>();\n+\n+  private final LinkedList<SpillInfo> spills = new LinkedList<SpillInfo>();\n+\n+  // All three of these variables are reset after spilling:\n+  private UnsafeShuffleSorter sorter;\n+  private MemoryBlock currentPage = null;\n+  private long currentPagePosition = -1;\n+  private long freeSpaceInCurrentPage = 0;\n+\n+  public UnsafeShuffleExternalSorter(\n+      TaskMemoryManager memoryManager,\n+      ShuffleMemoryManager shuffleMemoryManager,\n+      BlockManager blockManager,\n+      TaskContext taskContext,\n+      int initialSize,\n+      int numPartitions,\n+      SparkConf conf,\n+      ShuffleWriteMetrics writeMetrics) throws IOException {\n+    this.memoryManager = memoryManager;\n+    this.shuffleMemoryManager = shuffleMemoryManager;\n+    this.blockManager = blockManager;\n+    this.taskContext = taskContext;\n+    this.initialSize = initialSize;\n+    this.numPartitions = numPartitions;\n+    this.spillingEnabled = conf.getBoolean(\"spark.shuffle.spill\", true);\n+    // Use getSizeAsKb (not bytes) to maintain backwards compatibility for units\n+    this.fileBufferSize = (int) conf.getSizeAsKb(\"spark.shuffle.file.buffer\", \"32k\") * 1024;\n+    this.writeMetrics = writeMetrics;\n+    openSorter();\n+  }\n+\n+  /**\n+   * Allocates a new sorter. Called when opening the spill writer for the first time and after\n+   * each spill.\n+   */\n+  private void openSorter() throws IOException {\n+    // TODO: move this sizing calculation logic into a static method of sorter:\n+    final long memoryRequested = initialSize * 8L;\n+    if (spillingEnabled) {\n+      final long memoryAcquired = shuffleMemoryManager.tryToAcquire(memoryRequested);\n+      if (memoryAcquired != memoryRequested) {\n+        shuffleMemoryManager.release(memoryAcquired);\n+        throw new IOException(\"Could not acquire memory!\");\n+      }\n+    }\n+\n+    this.sorter = new UnsafeShuffleSorter(initialSize);\n+  }\n+\n+  /**\n+   * Sorts the in-memory records and writes the sorted records to a spill file.\n+   * This method does not free the sort data structures.\n+   *\n+   * @param isSpill if true, this indicates that we're writing a spill and that bytes written should\n+   *                be counted towards shuffle spill metrics rather than shuffle write metrics.\n+   */\n+  private void writeSpillFile(boolean isSpill) throws IOException {\n+\n+    final ShuffleWriteMetrics writeMetricsToUse;\n+\n+    if (isSpill) {\n+      // We're spilling, so bytes written should be counted towards spill rather than write.\n+      // Create a dummy WriteMetrics object to absorb these metrics, since we don't want to count\n+      // them towards shuffle bytes written.\n+      writeMetricsToUse = new ShuffleWriteMetrics();\n+    } else {\n+      // We're writing the final non-spill file, so we _do_ want to count this as shuffle bytes.\n+      writeMetricsToUse = writeMetrics;\n+    }\n+\n+    // This call performs the actual sort.\n+    final UnsafeShuffleSorter.UnsafeShuffleSorterIterator sortedRecords =\n+      sorter.getSortedIterator();\n+\n+    // Currently, we need to open a new DiskBlockObjectWriter for each partition; we can avoid this\n+    // after SPARK-5581 is fixed.\n+    BlockObjectWriter writer;\n+\n+    // Small writes to DiskBlockObjectWriter will be fairly inefficient. Since there doesn't seem to\n+    // be an API to directly transfer bytes from managed memory to the disk writer, we buffer\n+    // data through a byte array. This array does not need to be large enough to hold a single\n+    // record;\n+    final byte[] writeBuffer = new byte[DISK_WRITE_BUFFER_SIZE];\n+\n+    // Because this output will be read during shuffle, its compression codec must be controlled by\n+    // spark.shuffle.compress instead of spark.shuffle.spill.compress, so we need to use\n+    // createTempShuffleBlock here; see SPARK-3426 for more details.\n+    final Tuple2<TempShuffleBlockId, File> spilledFileInfo =\n+      blockManager.diskBlockManager().createTempShuffleBlock();\n+    final File file = spilledFileInfo._2();\n+    final TempShuffleBlockId blockId = spilledFileInfo._1();\n+    final SpillInfo spillInfo = new SpillInfo(numPartitions, file, blockId);\n+\n+    // Unfortunately, we need a serializer instance in order to construct a DiskBlockObjectWriter.\n+    // Our write path doesn't actually use this serializer (since we end up calling the `write()`\n+    // OutputStream methods), but DiskBlockObjectWriter still calls some methods on it. To work\n+    // around this, we pass a dummy no-op serializer.\n+    final SerializerInstance ser = DummySerializerInstance.INSTANCE;\n+\n+    writer = blockManager.getDiskWriter(blockId, file, ser, fileBufferSize, writeMetricsToUse);\n+\n+    int currentPartition = -1;\n+    while (sortedRecords.hasNext()) {\n+      sortedRecords.loadNext();\n+      final int partition = sortedRecords.packedRecordPointer.getPartitionId();\n+      assert (partition >= currentPartition);\n+      if (partition != currentPartition) {\n+        // Switch to the new partition\n+        if (currentPartition != -1) {\n+          writer.commitAndClose();\n+          spillInfo.partitionLengths[currentPartition] = writer.fileSegment().length();\n+        }\n+        currentPartition = partition;\n+        writer = blockManager.getDiskWriter(blockId, file, ser, fileBufferSize, writeMetricsToUse);\n+      }\n+\n+      final long recordPointer = sortedRecords.packedRecordPointer.getRecordPointer();\n+      final Object recordPage = memoryManager.getPage(recordPointer);\n+      final long recordOffsetInPage = memoryManager.getOffsetInPage(recordPointer);\n+      int dataRemaining = PlatformDependent.UNSAFE.getInt(recordPage, recordOffsetInPage);\n+      long recordReadPosition = recordOffsetInPage + 4; // skip over record length\n+      while (dataRemaining > 0) {\n+        final int toTransfer = Math.min(DISK_WRITE_BUFFER_SIZE, dataRemaining);\n+        PlatformDependent.copyMemory(\n+          recordPage,\n+          recordReadPosition,\n+          writeBuffer,\n+          PlatformDependent.BYTE_ARRAY_OFFSET,\n+          toTransfer);\n+        writer.write(writeBuffer, 0, toTransfer);\n+        recordReadPosition += toTransfer;\n+        dataRemaining -= toTransfer;\n+      }\n+      // TODO: add a test that detects whether we leave this call out:\n+      writer.recordWritten();\n+    }\n+\n+    if (writer != null) {\n+      writer.commitAndClose();\n+      // If `writeSpillFile()` was called from `closeAndGetSpills()` and no records were inserted,\n+      // then the spill file might be empty. Note that it might be better to avoid calling\n+      // writeSpillFile() in that case.\n+      if (currentPartition != -1) {\n+        spillInfo.partitionLengths[currentPartition] = writer.fileSegment().length();\n+        spills.add(spillInfo);\n+      }\n+    }\n+\n+    if (isSpill) {\n+      writeMetrics.incShuffleRecordsWritten(writeMetricsToUse.shuffleRecordsWritten());\n+      // Consistent with ExternalSorter, we do not count this IO towards shuffle write time.\n+      // This means that this IO time is not accounted for anywhere; SPARK-3577 will fix this.\n+      // writeMetrics.incShuffleWriteTime(writeMetricsToUse.shuffleWriteTime());\n+      taskContext.taskMetrics().incDiskBytesSpilled(writeMetricsToUse.shuffleBytesWritten());\n+    }\n+  }\n+\n+  /**\n+   * Sort and spill the current records in response to memory pressure.\n+   */\n+  @VisibleForTesting\n+  void spill() throws IOException {\n+    final long threadId = Thread.currentThread().getId();\n+    logger.info(\"Thread \" + threadId + \" spilling sort data of \" +\n+      org.apache.spark.util.Utils.bytesToString(getMemoryUsage()) + \" to disk (\" +\n+      (spills.size() + (spills.size() > 1 ? \" times\" : \" time\")) + \" so far)\");\n+\n+    writeSpillFile(true);\n+    final long sorterMemoryUsage = sorter.getMemoryUsage();\n+    sorter = null;\n+    shuffleMemoryManager.release(sorterMemoryUsage);\n+    final long spillSize = freeMemory();\n+    taskContext.taskMetrics().incMemoryBytesSpilled(spillSize);\n+\n+    openSorter();\n+  }\n+\n+  private long getMemoryUsage() {\n+    return sorter.getMemoryUsage() + (allocatedPages.size() * (long) PAGE_SIZE);\n+  }\n+\n+  private long freeMemory() {\n+    long memoryFreed = 0;\n+    for (MemoryBlock block : allocatedPages) {\n+      memoryManager.freePage(block);\n+      shuffleMemoryManager.release(block.size());\n+      memoryFreed += block.size();\n+    }\n+    allocatedPages.clear();\n+    currentPage = null;\n+    currentPagePosition = -1;\n+    freeSpaceInCurrentPage = 0;\n+    return memoryFreed;\n+  }\n+\n+  /**\n+   * Force all memory and spill files to be deleted; called by shuffle error-handling code.\n+   */\n+  public void cleanupAfterError() {\n+    freeMemory();\n+    for (SpillInfo spill : spills) {\n+      if (spill.file.exists() && !spill.file.delete()) {\n+        logger.error(\"Unable to delete spill file {}\", spill.file.getPath());\n+      }\n+    }\n+    if (spillingEnabled && sorter != null) {\n+      shuffleMemoryManager.release(sorter.getMemoryUsage());\n+      sorter = null;\n+    }\n+  }\n+\n+  /**\n+   * Checks whether there is enough space to insert a new record into the sorter.\n+   *\n+   * @param requiredSpace the required space in the data page, in bytes, including space for storing\n+   *                      the record size.\n+\n+   * @return true if the record can be inserted without requiring more allocations, false otherwise.\n+   */\n+  private boolean haveSpaceForRecord(int requiredSpace) {\n+    assert (requiredSpace > 0);\n+    // The sort array will automatically expand when inserting a new record, so we only need to\n+    // worry about it having free space when spilling is enabled.\n+    final boolean sortBufferHasSpace = !spillingEnabled || sorter.hasSpaceForAnotherRecord();\n+    final boolean dataPageHasSpace = requiredSpace <= freeSpaceInCurrentPage;\n+    return (sortBufferHasSpace && dataPageHasSpace);\n+  }\n+\n+  /**\n+   * Allocates more memory in order to insert an additional record. If spilling is enabled, this\n+   * will request additional memory from the {@link ShuffleMemoryManager} and spill if the requested\n+   * memory can not be obtained. If spilling is disabled, then this will allocate memory without\n+   * coordinating with the ShuffleMemoryManager.\n+   *\n+   * @param requiredSpace the required space in the data page, in bytes, including space for storing\n+   *                      the record size.\n+   */\n+  private void allocateSpaceForRecord(int requiredSpace) throws IOException {\n+    if (spillingEnabled && !sorter.hasSpaceForAnotherRecord()) {\n+      logger.debug(\"Attempting to expand sort buffer\");\n+      final long oldSortBufferMemoryUsage = sorter.getMemoryUsage();\n+      final long memoryToGrowSortBuffer = oldSortBufferMemoryUsage * 2;"
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "This code is called in situations where we've filled up our sort buffer and need to expand it before being able to insert another record.  During the expansion, we need space to hold both the new and old sort buffer.\n\nAre you suggesting that we grow the buffer by some other fraction instead of doubling?\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-12T20:26:29Z",
    "diffHunk": "@@ -0,0 +1,418 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.LinkedList;\n+\n+import scala.Tuple2;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.storage.*;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.MemoryBlock;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+/**\n+ * An external sorter that is specialized for sort-based shuffle.\n+ * <p>\n+ * Incoming records are appended to data pages. When all records have been inserted (or when the\n+ * current thread's shuffle memory limit is reached), the in-memory records are sorted according to\n+ * their partition ids (using a {@link UnsafeShuffleSorter}). The sorted records are then written\n+ * to a single output file (or multiple files, if we've spilled). The format of the output files is\n+ * the same as the format of the final output file written by\n+ * {@link org.apache.spark.shuffle.sort.SortShuffleWriter}: each output partition's records are\n+ * written as a single serialized, compressed stream that can be read with a new decompression and\n+ * deserialization stream.\n+ * <p>\n+ * Unlike {@link org.apache.spark.util.collection.ExternalSorter}, this sorter does not merge its\n+ * spill files. Instead, this merging is performed in {@link UnsafeShuffleWriter}, which uses a\n+ * specialized merge procedure that avoids extra serialization/deserialization.\n+ */\n+final class UnsafeShuffleExternalSorter {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleExternalSorter.class);\n+\n+  private static final int PAGE_SIZE = PackedRecordPointer.MAXIMUM_PAGE_SIZE_BYTES;\n+  @VisibleForTesting\n+  static final int DISK_WRITE_BUFFER_SIZE = 1024 * 1024;\n+  @VisibleForTesting\n+  static final int MAX_RECORD_SIZE = PAGE_SIZE - 4;\n+\n+  private final int initialSize;\n+  private final int numPartitions;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final BlockManager blockManager;\n+  private final TaskContext taskContext;\n+  private final boolean spillingEnabled;\n+  private final ShuffleWriteMetrics writeMetrics;\n+\n+  /** The buffer size to use when writing spills using DiskBlockObjectWriter */\n+  private final int fileBufferSize;\n+\n+  /**\n+   * Memory pages that hold the records being sorted. The pages in this list are freed when\n+   * spilling, although in principle we could recycle these pages across spills (on the other hand,\n+   * this might not be necessary if we maintained a pool of re-usable pages in the TaskMemoryManager\n+   * itself).\n+   */\n+  private final LinkedList<MemoryBlock> allocatedPages = new LinkedList<MemoryBlock>();\n+\n+  private final LinkedList<SpillInfo> spills = new LinkedList<SpillInfo>();\n+\n+  // All three of these variables are reset after spilling:\n+  private UnsafeShuffleSorter sorter;\n+  private MemoryBlock currentPage = null;\n+  private long currentPagePosition = -1;\n+  private long freeSpaceInCurrentPage = 0;\n+\n+  public UnsafeShuffleExternalSorter(\n+      TaskMemoryManager memoryManager,\n+      ShuffleMemoryManager shuffleMemoryManager,\n+      BlockManager blockManager,\n+      TaskContext taskContext,\n+      int initialSize,\n+      int numPartitions,\n+      SparkConf conf,\n+      ShuffleWriteMetrics writeMetrics) throws IOException {\n+    this.memoryManager = memoryManager;\n+    this.shuffleMemoryManager = shuffleMemoryManager;\n+    this.blockManager = blockManager;\n+    this.taskContext = taskContext;\n+    this.initialSize = initialSize;\n+    this.numPartitions = numPartitions;\n+    this.spillingEnabled = conf.getBoolean(\"spark.shuffle.spill\", true);\n+    // Use getSizeAsKb (not bytes) to maintain backwards compatibility for units\n+    this.fileBufferSize = (int) conf.getSizeAsKb(\"spark.shuffle.file.buffer\", \"32k\") * 1024;\n+    this.writeMetrics = writeMetrics;\n+    openSorter();\n+  }\n+\n+  /**\n+   * Allocates a new sorter. Called when opening the spill writer for the first time and after\n+   * each spill.\n+   */\n+  private void openSorter() throws IOException {\n+    // TODO: move this sizing calculation logic into a static method of sorter:\n+    final long memoryRequested = initialSize * 8L;\n+    if (spillingEnabled) {\n+      final long memoryAcquired = shuffleMemoryManager.tryToAcquire(memoryRequested);\n+      if (memoryAcquired != memoryRequested) {\n+        shuffleMemoryManager.release(memoryAcquired);\n+        throw new IOException(\"Could not acquire memory!\");\n+      }\n+    }\n+\n+    this.sorter = new UnsafeShuffleSorter(initialSize);\n+  }\n+\n+  /**\n+   * Sorts the in-memory records and writes the sorted records to a spill file.\n+   * This method does not free the sort data structures.\n+   *\n+   * @param isSpill if true, this indicates that we're writing a spill and that bytes written should\n+   *                be counted towards shuffle spill metrics rather than shuffle write metrics.\n+   */\n+  private void writeSpillFile(boolean isSpill) throws IOException {\n+\n+    final ShuffleWriteMetrics writeMetricsToUse;\n+\n+    if (isSpill) {\n+      // We're spilling, so bytes written should be counted towards spill rather than write.\n+      // Create a dummy WriteMetrics object to absorb these metrics, since we don't want to count\n+      // them towards shuffle bytes written.\n+      writeMetricsToUse = new ShuffleWriteMetrics();\n+    } else {\n+      // We're writing the final non-spill file, so we _do_ want to count this as shuffle bytes.\n+      writeMetricsToUse = writeMetrics;\n+    }\n+\n+    // This call performs the actual sort.\n+    final UnsafeShuffleSorter.UnsafeShuffleSorterIterator sortedRecords =\n+      sorter.getSortedIterator();\n+\n+    // Currently, we need to open a new DiskBlockObjectWriter for each partition; we can avoid this\n+    // after SPARK-5581 is fixed.\n+    BlockObjectWriter writer;\n+\n+    // Small writes to DiskBlockObjectWriter will be fairly inefficient. Since there doesn't seem to\n+    // be an API to directly transfer bytes from managed memory to the disk writer, we buffer\n+    // data through a byte array. This array does not need to be large enough to hold a single\n+    // record;\n+    final byte[] writeBuffer = new byte[DISK_WRITE_BUFFER_SIZE];\n+\n+    // Because this output will be read during shuffle, its compression codec must be controlled by\n+    // spark.shuffle.compress instead of spark.shuffle.spill.compress, so we need to use\n+    // createTempShuffleBlock here; see SPARK-3426 for more details.\n+    final Tuple2<TempShuffleBlockId, File> spilledFileInfo =\n+      blockManager.diskBlockManager().createTempShuffleBlock();\n+    final File file = spilledFileInfo._2();\n+    final TempShuffleBlockId blockId = spilledFileInfo._1();\n+    final SpillInfo spillInfo = new SpillInfo(numPartitions, file, blockId);\n+\n+    // Unfortunately, we need a serializer instance in order to construct a DiskBlockObjectWriter.\n+    // Our write path doesn't actually use this serializer (since we end up calling the `write()`\n+    // OutputStream methods), but DiskBlockObjectWriter still calls some methods on it. To work\n+    // around this, we pass a dummy no-op serializer.\n+    final SerializerInstance ser = DummySerializerInstance.INSTANCE;\n+\n+    writer = blockManager.getDiskWriter(blockId, file, ser, fileBufferSize, writeMetricsToUse);\n+\n+    int currentPartition = -1;\n+    while (sortedRecords.hasNext()) {\n+      sortedRecords.loadNext();\n+      final int partition = sortedRecords.packedRecordPointer.getPartitionId();\n+      assert (partition >= currentPartition);\n+      if (partition != currentPartition) {\n+        // Switch to the new partition\n+        if (currentPartition != -1) {\n+          writer.commitAndClose();\n+          spillInfo.partitionLengths[currentPartition] = writer.fileSegment().length();\n+        }\n+        currentPartition = partition;\n+        writer = blockManager.getDiskWriter(blockId, file, ser, fileBufferSize, writeMetricsToUse);\n+      }\n+\n+      final long recordPointer = sortedRecords.packedRecordPointer.getRecordPointer();\n+      final Object recordPage = memoryManager.getPage(recordPointer);\n+      final long recordOffsetInPage = memoryManager.getOffsetInPage(recordPointer);\n+      int dataRemaining = PlatformDependent.UNSAFE.getInt(recordPage, recordOffsetInPage);\n+      long recordReadPosition = recordOffsetInPage + 4; // skip over record length\n+      while (dataRemaining > 0) {\n+        final int toTransfer = Math.min(DISK_WRITE_BUFFER_SIZE, dataRemaining);\n+        PlatformDependent.copyMemory(\n+          recordPage,\n+          recordReadPosition,\n+          writeBuffer,\n+          PlatformDependent.BYTE_ARRAY_OFFSET,\n+          toTransfer);\n+        writer.write(writeBuffer, 0, toTransfer);\n+        recordReadPosition += toTransfer;\n+        dataRemaining -= toTransfer;\n+      }\n+      // TODO: add a test that detects whether we leave this call out:\n+      writer.recordWritten();\n+    }\n+\n+    if (writer != null) {\n+      writer.commitAndClose();\n+      // If `writeSpillFile()` was called from `closeAndGetSpills()` and no records were inserted,\n+      // then the spill file might be empty. Note that it might be better to avoid calling\n+      // writeSpillFile() in that case.\n+      if (currentPartition != -1) {\n+        spillInfo.partitionLengths[currentPartition] = writer.fileSegment().length();\n+        spills.add(spillInfo);\n+      }\n+    }\n+\n+    if (isSpill) {\n+      writeMetrics.incShuffleRecordsWritten(writeMetricsToUse.shuffleRecordsWritten());\n+      // Consistent with ExternalSorter, we do not count this IO towards shuffle write time.\n+      // This means that this IO time is not accounted for anywhere; SPARK-3577 will fix this.\n+      // writeMetrics.incShuffleWriteTime(writeMetricsToUse.shuffleWriteTime());\n+      taskContext.taskMetrics().incDiskBytesSpilled(writeMetricsToUse.shuffleBytesWritten());\n+    }\n+  }\n+\n+  /**\n+   * Sort and spill the current records in response to memory pressure.\n+   */\n+  @VisibleForTesting\n+  void spill() throws IOException {\n+    final long threadId = Thread.currentThread().getId();\n+    logger.info(\"Thread \" + threadId + \" spilling sort data of \" +\n+      org.apache.spark.util.Utils.bytesToString(getMemoryUsage()) + \" to disk (\" +\n+      (spills.size() + (spills.size() > 1 ? \" times\" : \" time\")) + \" so far)\");\n+\n+    writeSpillFile(true);\n+    final long sorterMemoryUsage = sorter.getMemoryUsage();\n+    sorter = null;\n+    shuffleMemoryManager.release(sorterMemoryUsage);\n+    final long spillSize = freeMemory();\n+    taskContext.taskMetrics().incMemoryBytesSpilled(spillSize);\n+\n+    openSorter();\n+  }\n+\n+  private long getMemoryUsage() {\n+    return sorter.getMemoryUsage() + (allocatedPages.size() * (long) PAGE_SIZE);\n+  }\n+\n+  private long freeMemory() {\n+    long memoryFreed = 0;\n+    for (MemoryBlock block : allocatedPages) {\n+      memoryManager.freePage(block);\n+      shuffleMemoryManager.release(block.size());\n+      memoryFreed += block.size();\n+    }\n+    allocatedPages.clear();\n+    currentPage = null;\n+    currentPagePosition = -1;\n+    freeSpaceInCurrentPage = 0;\n+    return memoryFreed;\n+  }\n+\n+  /**\n+   * Force all memory and spill files to be deleted; called by shuffle error-handling code.\n+   */\n+  public void cleanupAfterError() {\n+    freeMemory();\n+    for (SpillInfo spill : spills) {\n+      if (spill.file.exists() && !spill.file.delete()) {\n+        logger.error(\"Unable to delete spill file {}\", spill.file.getPath());\n+      }\n+    }\n+    if (spillingEnabled && sorter != null) {\n+      shuffleMemoryManager.release(sorter.getMemoryUsage());\n+      sorter = null;\n+    }\n+  }\n+\n+  /**\n+   * Checks whether there is enough space to insert a new record into the sorter.\n+   *\n+   * @param requiredSpace the required space in the data page, in bytes, including space for storing\n+   *                      the record size.\n+\n+   * @return true if the record can be inserted without requiring more allocations, false otherwise.\n+   */\n+  private boolean haveSpaceForRecord(int requiredSpace) {\n+    assert (requiredSpace > 0);\n+    // The sort array will automatically expand when inserting a new record, so we only need to\n+    // worry about it having free space when spilling is enabled.\n+    final boolean sortBufferHasSpace = !spillingEnabled || sorter.hasSpaceForAnotherRecord();\n+    final boolean dataPageHasSpace = requiredSpace <= freeSpaceInCurrentPage;\n+    return (sortBufferHasSpace && dataPageHasSpace);\n+  }\n+\n+  /**\n+   * Allocates more memory in order to insert an additional record. If spilling is enabled, this\n+   * will request additional memory from the {@link ShuffleMemoryManager} and spill if the requested\n+   * memory can not be obtained. If spilling is disabled, then this will allocate memory without\n+   * coordinating with the ShuffleMemoryManager.\n+   *\n+   * @param requiredSpace the required space in the data page, in bytes, including space for storing\n+   *                      the record size.\n+   */\n+  private void allocateSpaceForRecord(int requiredSpace) throws IOException {\n+    if (spillingEnabled && !sorter.hasSpaceForAnotherRecord()) {\n+      logger.debug(\"Attempting to expand sort buffer\");\n+      final long oldSortBufferMemoryUsage = sorter.getMemoryUsage();\n+      final long memoryToGrowSortBuffer = oldSortBufferMemoryUsage * 2;"
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "Per offline discussion, the confusion here was regarding the name \"sort buffer\".  I'm going to rename this to `pointerArray` to make things clearer.\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-12T20:57:01Z",
    "diffHunk": "@@ -0,0 +1,418 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.LinkedList;\n+\n+import scala.Tuple2;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.executor.ShuffleWriteMetrics;\n+import org.apache.spark.serializer.SerializerInstance;\n+import org.apache.spark.shuffle.ShuffleMemoryManager;\n+import org.apache.spark.storage.*;\n+import org.apache.spark.unsafe.PlatformDependent;\n+import org.apache.spark.unsafe.memory.MemoryBlock;\n+import org.apache.spark.unsafe.memory.TaskMemoryManager;\n+\n+/**\n+ * An external sorter that is specialized for sort-based shuffle.\n+ * <p>\n+ * Incoming records are appended to data pages. When all records have been inserted (or when the\n+ * current thread's shuffle memory limit is reached), the in-memory records are sorted according to\n+ * their partition ids (using a {@link UnsafeShuffleSorter}). The sorted records are then written\n+ * to a single output file (or multiple files, if we've spilled). The format of the output files is\n+ * the same as the format of the final output file written by\n+ * {@link org.apache.spark.shuffle.sort.SortShuffleWriter}: each output partition's records are\n+ * written as a single serialized, compressed stream that can be read with a new decompression and\n+ * deserialization stream.\n+ * <p>\n+ * Unlike {@link org.apache.spark.util.collection.ExternalSorter}, this sorter does not merge its\n+ * spill files. Instead, this merging is performed in {@link UnsafeShuffleWriter}, which uses a\n+ * specialized merge procedure that avoids extra serialization/deserialization.\n+ */\n+final class UnsafeShuffleExternalSorter {\n+\n+  private final Logger logger = LoggerFactory.getLogger(UnsafeShuffleExternalSorter.class);\n+\n+  private static final int PAGE_SIZE = PackedRecordPointer.MAXIMUM_PAGE_SIZE_BYTES;\n+  @VisibleForTesting\n+  static final int DISK_WRITE_BUFFER_SIZE = 1024 * 1024;\n+  @VisibleForTesting\n+  static final int MAX_RECORD_SIZE = PAGE_SIZE - 4;\n+\n+  private final int initialSize;\n+  private final int numPartitions;\n+  private final TaskMemoryManager memoryManager;\n+  private final ShuffleMemoryManager shuffleMemoryManager;\n+  private final BlockManager blockManager;\n+  private final TaskContext taskContext;\n+  private final boolean spillingEnabled;\n+  private final ShuffleWriteMetrics writeMetrics;\n+\n+  /** The buffer size to use when writing spills using DiskBlockObjectWriter */\n+  private final int fileBufferSize;\n+\n+  /**\n+   * Memory pages that hold the records being sorted. The pages in this list are freed when\n+   * spilling, although in principle we could recycle these pages across spills (on the other hand,\n+   * this might not be necessary if we maintained a pool of re-usable pages in the TaskMemoryManager\n+   * itself).\n+   */\n+  private final LinkedList<MemoryBlock> allocatedPages = new LinkedList<MemoryBlock>();\n+\n+  private final LinkedList<SpillInfo> spills = new LinkedList<SpillInfo>();\n+\n+  // All three of these variables are reset after spilling:\n+  private UnsafeShuffleSorter sorter;\n+  private MemoryBlock currentPage = null;\n+  private long currentPagePosition = -1;\n+  private long freeSpaceInCurrentPage = 0;\n+\n+  public UnsafeShuffleExternalSorter(\n+      TaskMemoryManager memoryManager,\n+      ShuffleMemoryManager shuffleMemoryManager,\n+      BlockManager blockManager,\n+      TaskContext taskContext,\n+      int initialSize,\n+      int numPartitions,\n+      SparkConf conf,\n+      ShuffleWriteMetrics writeMetrics) throws IOException {\n+    this.memoryManager = memoryManager;\n+    this.shuffleMemoryManager = shuffleMemoryManager;\n+    this.blockManager = blockManager;\n+    this.taskContext = taskContext;\n+    this.initialSize = initialSize;\n+    this.numPartitions = numPartitions;\n+    this.spillingEnabled = conf.getBoolean(\"spark.shuffle.spill\", true);\n+    // Use getSizeAsKb (not bytes) to maintain backwards compatibility for units\n+    this.fileBufferSize = (int) conf.getSizeAsKb(\"spark.shuffle.file.buffer\", \"32k\") * 1024;\n+    this.writeMetrics = writeMetrics;\n+    openSorter();\n+  }\n+\n+  /**\n+   * Allocates a new sorter. Called when opening the spill writer for the first time and after\n+   * each spill.\n+   */\n+  private void openSorter() throws IOException {\n+    // TODO: move this sizing calculation logic into a static method of sorter:\n+    final long memoryRequested = initialSize * 8L;\n+    if (spillingEnabled) {\n+      final long memoryAcquired = shuffleMemoryManager.tryToAcquire(memoryRequested);\n+      if (memoryAcquired != memoryRequested) {\n+        shuffleMemoryManager.release(memoryAcquired);\n+        throw new IOException(\"Could not acquire memory!\");\n+      }\n+    }\n+\n+    this.sorter = new UnsafeShuffleSorter(initialSize);\n+  }\n+\n+  /**\n+   * Sorts the in-memory records and writes the sorted records to a spill file.\n+   * This method does not free the sort data structures.\n+   *\n+   * @param isSpill if true, this indicates that we're writing a spill and that bytes written should\n+   *                be counted towards shuffle spill metrics rather than shuffle write metrics.\n+   */\n+  private void writeSpillFile(boolean isSpill) throws IOException {\n+\n+    final ShuffleWriteMetrics writeMetricsToUse;\n+\n+    if (isSpill) {\n+      // We're spilling, so bytes written should be counted towards spill rather than write.\n+      // Create a dummy WriteMetrics object to absorb these metrics, since we don't want to count\n+      // them towards shuffle bytes written.\n+      writeMetricsToUse = new ShuffleWriteMetrics();\n+    } else {\n+      // We're writing the final non-spill file, so we _do_ want to count this as shuffle bytes.\n+      writeMetricsToUse = writeMetrics;\n+    }\n+\n+    // This call performs the actual sort.\n+    final UnsafeShuffleSorter.UnsafeShuffleSorterIterator sortedRecords =\n+      sorter.getSortedIterator();\n+\n+    // Currently, we need to open a new DiskBlockObjectWriter for each partition; we can avoid this\n+    // after SPARK-5581 is fixed.\n+    BlockObjectWriter writer;\n+\n+    // Small writes to DiskBlockObjectWriter will be fairly inefficient. Since there doesn't seem to\n+    // be an API to directly transfer bytes from managed memory to the disk writer, we buffer\n+    // data through a byte array. This array does not need to be large enough to hold a single\n+    // record;\n+    final byte[] writeBuffer = new byte[DISK_WRITE_BUFFER_SIZE];\n+\n+    // Because this output will be read during shuffle, its compression codec must be controlled by\n+    // spark.shuffle.compress instead of spark.shuffle.spill.compress, so we need to use\n+    // createTempShuffleBlock here; see SPARK-3426 for more details.\n+    final Tuple2<TempShuffleBlockId, File> spilledFileInfo =\n+      blockManager.diskBlockManager().createTempShuffleBlock();\n+    final File file = spilledFileInfo._2();\n+    final TempShuffleBlockId blockId = spilledFileInfo._1();\n+    final SpillInfo spillInfo = new SpillInfo(numPartitions, file, blockId);\n+\n+    // Unfortunately, we need a serializer instance in order to construct a DiskBlockObjectWriter.\n+    // Our write path doesn't actually use this serializer (since we end up calling the `write()`\n+    // OutputStream methods), but DiskBlockObjectWriter still calls some methods on it. To work\n+    // around this, we pass a dummy no-op serializer.\n+    final SerializerInstance ser = DummySerializerInstance.INSTANCE;\n+\n+    writer = blockManager.getDiskWriter(blockId, file, ser, fileBufferSize, writeMetricsToUse);\n+\n+    int currentPartition = -1;\n+    while (sortedRecords.hasNext()) {\n+      sortedRecords.loadNext();\n+      final int partition = sortedRecords.packedRecordPointer.getPartitionId();\n+      assert (partition >= currentPartition);\n+      if (partition != currentPartition) {\n+        // Switch to the new partition\n+        if (currentPartition != -1) {\n+          writer.commitAndClose();\n+          spillInfo.partitionLengths[currentPartition] = writer.fileSegment().length();\n+        }\n+        currentPartition = partition;\n+        writer = blockManager.getDiskWriter(blockId, file, ser, fileBufferSize, writeMetricsToUse);\n+      }\n+\n+      final long recordPointer = sortedRecords.packedRecordPointer.getRecordPointer();\n+      final Object recordPage = memoryManager.getPage(recordPointer);\n+      final long recordOffsetInPage = memoryManager.getOffsetInPage(recordPointer);\n+      int dataRemaining = PlatformDependent.UNSAFE.getInt(recordPage, recordOffsetInPage);\n+      long recordReadPosition = recordOffsetInPage + 4; // skip over record length\n+      while (dataRemaining > 0) {\n+        final int toTransfer = Math.min(DISK_WRITE_BUFFER_SIZE, dataRemaining);\n+        PlatformDependent.copyMemory(\n+          recordPage,\n+          recordReadPosition,\n+          writeBuffer,\n+          PlatformDependent.BYTE_ARRAY_OFFSET,\n+          toTransfer);\n+        writer.write(writeBuffer, 0, toTransfer);\n+        recordReadPosition += toTransfer;\n+        dataRemaining -= toTransfer;\n+      }\n+      // TODO: add a test that detects whether we leave this call out:\n+      writer.recordWritten();\n+    }\n+\n+    if (writer != null) {\n+      writer.commitAndClose();\n+      // If `writeSpillFile()` was called from `closeAndGetSpills()` and no records were inserted,\n+      // then the spill file might be empty. Note that it might be better to avoid calling\n+      // writeSpillFile() in that case.\n+      if (currentPartition != -1) {\n+        spillInfo.partitionLengths[currentPartition] = writer.fileSegment().length();\n+        spills.add(spillInfo);\n+      }\n+    }\n+\n+    if (isSpill) {\n+      writeMetrics.incShuffleRecordsWritten(writeMetricsToUse.shuffleRecordsWritten());\n+      // Consistent with ExternalSorter, we do not count this IO towards shuffle write time.\n+      // This means that this IO time is not accounted for anywhere; SPARK-3577 will fix this.\n+      // writeMetrics.incShuffleWriteTime(writeMetricsToUse.shuffleWriteTime());\n+      taskContext.taskMetrics().incDiskBytesSpilled(writeMetricsToUse.shuffleBytesWritten());\n+    }\n+  }\n+\n+  /**\n+   * Sort and spill the current records in response to memory pressure.\n+   */\n+  @VisibleForTesting\n+  void spill() throws IOException {\n+    final long threadId = Thread.currentThread().getId();\n+    logger.info(\"Thread \" + threadId + \" spilling sort data of \" +\n+      org.apache.spark.util.Utils.bytesToString(getMemoryUsage()) + \" to disk (\" +\n+      (spills.size() + (spills.size() > 1 ? \" times\" : \" time\")) + \" so far)\");\n+\n+    writeSpillFile(true);\n+    final long sorterMemoryUsage = sorter.getMemoryUsage();\n+    sorter = null;\n+    shuffleMemoryManager.release(sorterMemoryUsage);\n+    final long spillSize = freeMemory();\n+    taskContext.taskMetrics().incMemoryBytesSpilled(spillSize);\n+\n+    openSorter();\n+  }\n+\n+  private long getMemoryUsage() {\n+    return sorter.getMemoryUsage() + (allocatedPages.size() * (long) PAGE_SIZE);\n+  }\n+\n+  private long freeMemory() {\n+    long memoryFreed = 0;\n+    for (MemoryBlock block : allocatedPages) {\n+      memoryManager.freePage(block);\n+      shuffleMemoryManager.release(block.size());\n+      memoryFreed += block.size();\n+    }\n+    allocatedPages.clear();\n+    currentPage = null;\n+    currentPagePosition = -1;\n+    freeSpaceInCurrentPage = 0;\n+    return memoryFreed;\n+  }\n+\n+  /**\n+   * Force all memory and spill files to be deleted; called by shuffle error-handling code.\n+   */\n+  public void cleanupAfterError() {\n+    freeMemory();\n+    for (SpillInfo spill : spills) {\n+      if (spill.file.exists() && !spill.file.delete()) {\n+        logger.error(\"Unable to delete spill file {}\", spill.file.getPath());\n+      }\n+    }\n+    if (spillingEnabled && sorter != null) {\n+      shuffleMemoryManager.release(sorter.getMemoryUsage());\n+      sorter = null;\n+    }\n+  }\n+\n+  /**\n+   * Checks whether there is enough space to insert a new record into the sorter.\n+   *\n+   * @param requiredSpace the required space in the data page, in bytes, including space for storing\n+   *                      the record size.\n+\n+   * @return true if the record can be inserted without requiring more allocations, false otherwise.\n+   */\n+  private boolean haveSpaceForRecord(int requiredSpace) {\n+    assert (requiredSpace > 0);\n+    // The sort array will automatically expand when inserting a new record, so we only need to\n+    // worry about it having free space when spilling is enabled.\n+    final boolean sortBufferHasSpace = !spillingEnabled || sorter.hasSpaceForAnotherRecord();\n+    final boolean dataPageHasSpace = requiredSpace <= freeSpaceInCurrentPage;\n+    return (sortBufferHasSpace && dataPageHasSpace);\n+  }\n+\n+  /**\n+   * Allocates more memory in order to insert an additional record. If spilling is enabled, this\n+   * will request additional memory from the {@link ShuffleMemoryManager} and spill if the requested\n+   * memory can not be obtained. If spilling is disabled, then this will allocate memory without\n+   * coordinating with the ShuffleMemoryManager.\n+   *\n+   * @param requiredSpace the required space in the data page, in bytes, including space for storing\n+   *                      the record size.\n+   */\n+  private void allocateSpaceForRecord(int requiredSpace) throws IOException {\n+    if (spillingEnabled && !sorter.hasSpaceForAnotherRecord()) {\n+      logger.debug(\"Attempting to expand sort buffer\");\n+      final long oldSortBufferMemoryUsage = sorter.getMemoryUsage();\n+      final long memoryToGrowSortBuffer = oldSortBufferMemoryUsage * 2;"
  }],
  "prId": 5868
}]