[{
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "again, a comment here on how this is creating local index & data files, the only option with spark <= 2.4 would be helpful.",
    "commit": "7dceec971784049442ec3d4cb71ddaa225e1e21f",
    "createdAt": "2019-07-04T01:42:24Z",
    "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort.io;\n+\n+import java.io.BufferedOutputStream;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.channels.FileChannel;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.shuffle.ShuffleMapOutputWriter;\n+import org.apache.spark.api.shuffle.ShufflePartitionWriter;\n+import org.apache.spark.api.shuffle.SupportsTransferTo;\n+import org.apache.spark.api.shuffle.TransferrableWritableByteChannel;\n+import org.apache.spark.internal.config.package$;\n+import org.apache.spark.shuffle.sort.DefaultTransferrableWritableByteChannel;\n+import org.apache.spark.shuffle.ShuffleWriteMetricsReporter;\n+import org.apache.spark.shuffle.IndexShuffleBlockResolver;\n+import org.apache.spark.storage.TimeTrackingOutputStream;\n+import org.apache.spark.util.Utils;\n+\n+public class DefaultShuffleMapOutputWriter implements ShuffleMapOutputWriter {"
  }],
  "prId": 25007
}, {
  "comments": [{
    "author": {
      "login": "gczsjdy"
    },
    "body": "I am thinking about reusing template code across different implementations... For example, this class keeps numBytesWritten for each partition in records, and it's pretty general to reuse. ",
    "commit": "7dceec971784049442ec3d4cb71ddaa225e1e21f",
    "createdAt": "2019-07-05T08:59:11Z",
    "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort.io;\n+\n+import java.io.BufferedOutputStream;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.channels.FileChannel;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.shuffle.ShuffleMapOutputWriter;\n+import org.apache.spark.api.shuffle.ShufflePartitionWriter;\n+import org.apache.spark.api.shuffle.SupportsTransferTo;\n+import org.apache.spark.api.shuffle.TransferrableWritableByteChannel;\n+import org.apache.spark.internal.config.package$;\n+import org.apache.spark.shuffle.sort.DefaultTransferrableWritableByteChannel;\n+import org.apache.spark.shuffle.ShuffleWriteMetricsReporter;\n+import org.apache.spark.shuffle.IndexShuffleBlockResolver;\n+import org.apache.spark.storage.TimeTrackingOutputStream;\n+import org.apache.spark.util.Utils;\n+\n+public class DefaultShuffleMapOutputWriter implements ShuffleMapOutputWriter {\n+\n+  private static final Logger log =\n+    LoggerFactory.getLogger(DefaultShuffleMapOutputWriter.class);\n+\n+  private final int shuffleId;\n+  private final int mapId;\n+  private final ShuffleWriteMetricsReporter metrics;\n+  private final IndexShuffleBlockResolver blockResolver;\n+  private final long[] partitionLengths;\n+  private final int bufferSize;\n+  private int lastPartitionId = -1;\n+  private long currChannelPosition;\n+\n+  private final File outputFile;\n+  private File outputTempFile;\n+  private FileOutputStream outputFileStream;\n+  private FileChannel outputFileChannel;\n+  private TimeTrackingOutputStream ts;\n+  private BufferedOutputStream outputBufferedFileStream;\n+\n+  public DefaultShuffleMapOutputWriter(\n+      int shuffleId,\n+      int mapId,\n+      int numPartitions,\n+      ShuffleWriteMetricsReporter metrics,\n+      IndexShuffleBlockResolver blockResolver,\n+      SparkConf sparkConf) {\n+    this.shuffleId = shuffleId;\n+    this.mapId = mapId;\n+    this.metrics = metrics;\n+    this.blockResolver = blockResolver;\n+    this.bufferSize =\n+      (int) (long) sparkConf.get(\n+        package$.MODULE$.SHUFFLE_UNSAFE_FILE_OUTPUT_BUFFER_SIZE()) * 1024;\n+    this.partitionLengths = new long[numPartitions];\n+    this.outputFile = blockResolver.getDataFile(shuffleId, mapId);\n+    this.outputTempFile = null;\n+  }\n+\n+  @Override\n+  public ShufflePartitionWriter getPartitionWriter(int partitionId) throws IOException {\n+    if (partitionId <= lastPartitionId) {\n+      throw new IllegalArgumentException(\"Partitions should be requested in increasing order.\");\n+    }\n+    lastPartitionId = partitionId;\n+    if (outputTempFile == null) {\n+      outputTempFile = Utils.tempFileWith(outputFile);\n+    }\n+    if (outputFileChannel != null) {\n+      currChannelPosition = outputFileChannel.position();\n+    } else {\n+      currChannelPosition = 0L;\n+    }\n+    return new DefaultShufflePartitionWriter(partitionId);\n+  }\n+\n+  @Override\n+  public void commitAllPartitions() throws IOException {\n+    cleanUp();\n+    File resolvedTmp = outputTempFile != null && outputTempFile.isFile() ? outputTempFile : null;\n+    blockResolver.writeIndexFileAndCommit(shuffleId, mapId, partitionLengths, resolvedTmp);\n+  }\n+\n+  @Override\n+  public void abort(Throwable error) {\n+    try {\n+      cleanUp();\n+    } catch (Exception e) {\n+      log.error(\"Unable to close appropriate underlying file stream\", e);\n+    }\n+    if (outputTempFile != null && outputTempFile.exists() && !outputTempFile.delete()) {\n+      log.warn(\"Failed to delete temporary shuffle file at {}\", outputTempFile.getAbsolutePath());\n+    }\n+  }\n+\n+  private void cleanUp() throws IOException {\n+    if (outputBufferedFileStream != null) {\n+      outputBufferedFileStream.close();\n+    }\n+    if (outputFileChannel != null) {\n+      outputFileChannel.close();\n+    }\n+    if (outputFileStream != null) {\n+      outputFileStream.close();\n+    }\n+  }\n+\n+  private void initStream() throws IOException {\n+    if (outputFileStream == null) {\n+      outputFileStream = new FileOutputStream(outputTempFile, true);\n+      ts = new TimeTrackingOutputStream(metrics, outputFileStream);\n+    }\n+    if (outputBufferedFileStream == null) {\n+      outputBufferedFileStream = new BufferedOutputStream(ts, bufferSize);\n+    }\n+  }\n+\n+  private void initChannel() throws IOException {\n+    if (outputFileStream == null) {\n+      outputFileStream = new FileOutputStream(outputTempFile, true);\n+    }\n+    if (outputFileChannel == null) {\n+      outputFileChannel = outputFileStream.getChannel();\n+    }\n+  }\n+\n+  private class DefaultShufflePartitionWriter implements SupportsTransferTo {\n+\n+    private final int partitionId;\n+    private PartitionWriterStream partStream = null;\n+    private PartitionWriterChannel partChannel = null;\n+\n+    private DefaultShufflePartitionWriter(int partitionId) {\n+      this.partitionId = partitionId;\n+    }\n+\n+    @Override\n+    public OutputStream openStream() throws IOException {\n+      if (partStream == null) {\n+        if (outputFileChannel != null) {\n+          throw new IllegalStateException(\"Requested an output channel for a previous write but\" +\n+              \" now an output stream has been requested. Should not be using both channels\" +\n+              \" and streams to write.\");\n+        }\n+        initStream();\n+        partStream = new PartitionWriterStream(partitionId);\n+      }\n+      return partStream;\n+    }\n+\n+    @Override\n+    public TransferrableWritableByteChannel openTransferrableChannel() throws IOException {\n+      if (partChannel == null) {\n+        if (partStream != null) {\n+          throw new IllegalStateException(\"Requested an output stream for a previous write but\" +\n+              \" now an output channel has been requested. Should not be using both channels\" +\n+              \" and streams to write.\");\n+        }\n+        initChannel();\n+        partChannel = new PartitionWriterChannel(partitionId);\n+      }\n+      return partChannel;\n+    }\n+\n+    @Override\n+    public long getNumBytesWritten() {\n+      if (partChannel != null) {\n+        try {\n+          return partChannel.getCount();\n+        } catch (IOException e) {\n+          throw new RuntimeException(e);\n+        }\n+      } else if (partStream != null) {\n+        return partStream.getCount();\n+      } else {\n+        // Assume an empty partition if stream and channel are never created\n+        return 0;\n+      }\n+    }\n+  }\n+\n+  private class PartitionWriterStream extends OutputStream {"
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "We're thinking about this for the future, particularly when sharing the functionality to write shuffle data to local disk and asynchronously back up the files.\r\n\r\nFor now, I want to keep the API surface area minimal and only add public APIs after making deliberate decisions - probably with documentation on some forum (JIRA, mailing list)",
    "commit": "7dceec971784049442ec3d4cb71ddaa225e1e21f",
    "createdAt": "2019-07-08T21:55:11Z",
    "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort.io;\n+\n+import java.io.BufferedOutputStream;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.channels.FileChannel;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.shuffle.ShuffleMapOutputWriter;\n+import org.apache.spark.api.shuffle.ShufflePartitionWriter;\n+import org.apache.spark.api.shuffle.SupportsTransferTo;\n+import org.apache.spark.api.shuffle.TransferrableWritableByteChannel;\n+import org.apache.spark.internal.config.package$;\n+import org.apache.spark.shuffle.sort.DefaultTransferrableWritableByteChannel;\n+import org.apache.spark.shuffle.ShuffleWriteMetricsReporter;\n+import org.apache.spark.shuffle.IndexShuffleBlockResolver;\n+import org.apache.spark.storage.TimeTrackingOutputStream;\n+import org.apache.spark.util.Utils;\n+\n+public class DefaultShuffleMapOutputWriter implements ShuffleMapOutputWriter {\n+\n+  private static final Logger log =\n+    LoggerFactory.getLogger(DefaultShuffleMapOutputWriter.class);\n+\n+  private final int shuffleId;\n+  private final int mapId;\n+  private final ShuffleWriteMetricsReporter metrics;\n+  private final IndexShuffleBlockResolver blockResolver;\n+  private final long[] partitionLengths;\n+  private final int bufferSize;\n+  private int lastPartitionId = -1;\n+  private long currChannelPosition;\n+\n+  private final File outputFile;\n+  private File outputTempFile;\n+  private FileOutputStream outputFileStream;\n+  private FileChannel outputFileChannel;\n+  private TimeTrackingOutputStream ts;\n+  private BufferedOutputStream outputBufferedFileStream;\n+\n+  public DefaultShuffleMapOutputWriter(\n+      int shuffleId,\n+      int mapId,\n+      int numPartitions,\n+      ShuffleWriteMetricsReporter metrics,\n+      IndexShuffleBlockResolver blockResolver,\n+      SparkConf sparkConf) {\n+    this.shuffleId = shuffleId;\n+    this.mapId = mapId;\n+    this.metrics = metrics;\n+    this.blockResolver = blockResolver;\n+    this.bufferSize =\n+      (int) (long) sparkConf.get(\n+        package$.MODULE$.SHUFFLE_UNSAFE_FILE_OUTPUT_BUFFER_SIZE()) * 1024;\n+    this.partitionLengths = new long[numPartitions];\n+    this.outputFile = blockResolver.getDataFile(shuffleId, mapId);\n+    this.outputTempFile = null;\n+  }\n+\n+  @Override\n+  public ShufflePartitionWriter getPartitionWriter(int partitionId) throws IOException {\n+    if (partitionId <= lastPartitionId) {\n+      throw new IllegalArgumentException(\"Partitions should be requested in increasing order.\");\n+    }\n+    lastPartitionId = partitionId;\n+    if (outputTempFile == null) {\n+      outputTempFile = Utils.tempFileWith(outputFile);\n+    }\n+    if (outputFileChannel != null) {\n+      currChannelPosition = outputFileChannel.position();\n+    } else {\n+      currChannelPosition = 0L;\n+    }\n+    return new DefaultShufflePartitionWriter(partitionId);\n+  }\n+\n+  @Override\n+  public void commitAllPartitions() throws IOException {\n+    cleanUp();\n+    File resolvedTmp = outputTempFile != null && outputTempFile.isFile() ? outputTempFile : null;\n+    blockResolver.writeIndexFileAndCommit(shuffleId, mapId, partitionLengths, resolvedTmp);\n+  }\n+\n+  @Override\n+  public void abort(Throwable error) {\n+    try {\n+      cleanUp();\n+    } catch (Exception e) {\n+      log.error(\"Unable to close appropriate underlying file stream\", e);\n+    }\n+    if (outputTempFile != null && outputTempFile.exists() && !outputTempFile.delete()) {\n+      log.warn(\"Failed to delete temporary shuffle file at {}\", outputTempFile.getAbsolutePath());\n+    }\n+  }\n+\n+  private void cleanUp() throws IOException {\n+    if (outputBufferedFileStream != null) {\n+      outputBufferedFileStream.close();\n+    }\n+    if (outputFileChannel != null) {\n+      outputFileChannel.close();\n+    }\n+    if (outputFileStream != null) {\n+      outputFileStream.close();\n+    }\n+  }\n+\n+  private void initStream() throws IOException {\n+    if (outputFileStream == null) {\n+      outputFileStream = new FileOutputStream(outputTempFile, true);\n+      ts = new TimeTrackingOutputStream(metrics, outputFileStream);\n+    }\n+    if (outputBufferedFileStream == null) {\n+      outputBufferedFileStream = new BufferedOutputStream(ts, bufferSize);\n+    }\n+  }\n+\n+  private void initChannel() throws IOException {\n+    if (outputFileStream == null) {\n+      outputFileStream = new FileOutputStream(outputTempFile, true);\n+    }\n+    if (outputFileChannel == null) {\n+      outputFileChannel = outputFileStream.getChannel();\n+    }\n+  }\n+\n+  private class DefaultShufflePartitionWriter implements SupportsTransferTo {\n+\n+    private final int partitionId;\n+    private PartitionWriterStream partStream = null;\n+    private PartitionWriterChannel partChannel = null;\n+\n+    private DefaultShufflePartitionWriter(int partitionId) {\n+      this.partitionId = partitionId;\n+    }\n+\n+    @Override\n+    public OutputStream openStream() throws IOException {\n+      if (partStream == null) {\n+        if (outputFileChannel != null) {\n+          throw new IllegalStateException(\"Requested an output channel for a previous write but\" +\n+              \" now an output stream has been requested. Should not be using both channels\" +\n+              \" and streams to write.\");\n+        }\n+        initStream();\n+        partStream = new PartitionWriterStream(partitionId);\n+      }\n+      return partStream;\n+    }\n+\n+    @Override\n+    public TransferrableWritableByteChannel openTransferrableChannel() throws IOException {\n+      if (partChannel == null) {\n+        if (partStream != null) {\n+          throw new IllegalStateException(\"Requested an output stream for a previous write but\" +\n+              \" now an output channel has been requested. Should not be using both channels\" +\n+              \" and streams to write.\");\n+        }\n+        initChannel();\n+        partChannel = new PartitionWriterChannel(partitionId);\n+      }\n+      return partChannel;\n+    }\n+\n+    @Override\n+    public long getNumBytesWritten() {\n+      if (partChannel != null) {\n+        try {\n+          return partChannel.getCount();\n+        } catch (IOException e) {\n+          throw new RuntimeException(e);\n+        }\n+      } else if (partStream != null) {\n+        return partStream.getCount();\n+      } else {\n+        // Assume an empty partition if stream and channel are never created\n+        return 0;\n+      }\n+    }\n+  }\n+\n+  private class PartitionWriterStream extends OutputStream {"
  }, {
    "author": {
      "login": "gczsjdy"
    },
    "body": "Yes, probably we can leave a TODO and create some follow-up PRs later",
    "commit": "7dceec971784049442ec3d4cb71ddaa225e1e21f",
    "createdAt": "2019-07-17T01:52:11Z",
    "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort.io;\n+\n+import java.io.BufferedOutputStream;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.channels.FileChannel;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.shuffle.ShuffleMapOutputWriter;\n+import org.apache.spark.api.shuffle.ShufflePartitionWriter;\n+import org.apache.spark.api.shuffle.SupportsTransferTo;\n+import org.apache.spark.api.shuffle.TransferrableWritableByteChannel;\n+import org.apache.spark.internal.config.package$;\n+import org.apache.spark.shuffle.sort.DefaultTransferrableWritableByteChannel;\n+import org.apache.spark.shuffle.ShuffleWriteMetricsReporter;\n+import org.apache.spark.shuffle.IndexShuffleBlockResolver;\n+import org.apache.spark.storage.TimeTrackingOutputStream;\n+import org.apache.spark.util.Utils;\n+\n+public class DefaultShuffleMapOutputWriter implements ShuffleMapOutputWriter {\n+\n+  private static final Logger log =\n+    LoggerFactory.getLogger(DefaultShuffleMapOutputWriter.class);\n+\n+  private final int shuffleId;\n+  private final int mapId;\n+  private final ShuffleWriteMetricsReporter metrics;\n+  private final IndexShuffleBlockResolver blockResolver;\n+  private final long[] partitionLengths;\n+  private final int bufferSize;\n+  private int lastPartitionId = -1;\n+  private long currChannelPosition;\n+\n+  private final File outputFile;\n+  private File outputTempFile;\n+  private FileOutputStream outputFileStream;\n+  private FileChannel outputFileChannel;\n+  private TimeTrackingOutputStream ts;\n+  private BufferedOutputStream outputBufferedFileStream;\n+\n+  public DefaultShuffleMapOutputWriter(\n+      int shuffleId,\n+      int mapId,\n+      int numPartitions,\n+      ShuffleWriteMetricsReporter metrics,\n+      IndexShuffleBlockResolver blockResolver,\n+      SparkConf sparkConf) {\n+    this.shuffleId = shuffleId;\n+    this.mapId = mapId;\n+    this.metrics = metrics;\n+    this.blockResolver = blockResolver;\n+    this.bufferSize =\n+      (int) (long) sparkConf.get(\n+        package$.MODULE$.SHUFFLE_UNSAFE_FILE_OUTPUT_BUFFER_SIZE()) * 1024;\n+    this.partitionLengths = new long[numPartitions];\n+    this.outputFile = blockResolver.getDataFile(shuffleId, mapId);\n+    this.outputTempFile = null;\n+  }\n+\n+  @Override\n+  public ShufflePartitionWriter getPartitionWriter(int partitionId) throws IOException {\n+    if (partitionId <= lastPartitionId) {\n+      throw new IllegalArgumentException(\"Partitions should be requested in increasing order.\");\n+    }\n+    lastPartitionId = partitionId;\n+    if (outputTempFile == null) {\n+      outputTempFile = Utils.tempFileWith(outputFile);\n+    }\n+    if (outputFileChannel != null) {\n+      currChannelPosition = outputFileChannel.position();\n+    } else {\n+      currChannelPosition = 0L;\n+    }\n+    return new DefaultShufflePartitionWriter(partitionId);\n+  }\n+\n+  @Override\n+  public void commitAllPartitions() throws IOException {\n+    cleanUp();\n+    File resolvedTmp = outputTempFile != null && outputTempFile.isFile() ? outputTempFile : null;\n+    blockResolver.writeIndexFileAndCommit(shuffleId, mapId, partitionLengths, resolvedTmp);\n+  }\n+\n+  @Override\n+  public void abort(Throwable error) {\n+    try {\n+      cleanUp();\n+    } catch (Exception e) {\n+      log.error(\"Unable to close appropriate underlying file stream\", e);\n+    }\n+    if (outputTempFile != null && outputTempFile.exists() && !outputTempFile.delete()) {\n+      log.warn(\"Failed to delete temporary shuffle file at {}\", outputTempFile.getAbsolutePath());\n+    }\n+  }\n+\n+  private void cleanUp() throws IOException {\n+    if (outputBufferedFileStream != null) {\n+      outputBufferedFileStream.close();\n+    }\n+    if (outputFileChannel != null) {\n+      outputFileChannel.close();\n+    }\n+    if (outputFileStream != null) {\n+      outputFileStream.close();\n+    }\n+  }\n+\n+  private void initStream() throws IOException {\n+    if (outputFileStream == null) {\n+      outputFileStream = new FileOutputStream(outputTempFile, true);\n+      ts = new TimeTrackingOutputStream(metrics, outputFileStream);\n+    }\n+    if (outputBufferedFileStream == null) {\n+      outputBufferedFileStream = new BufferedOutputStream(ts, bufferSize);\n+    }\n+  }\n+\n+  private void initChannel() throws IOException {\n+    if (outputFileStream == null) {\n+      outputFileStream = new FileOutputStream(outputTempFile, true);\n+    }\n+    if (outputFileChannel == null) {\n+      outputFileChannel = outputFileStream.getChannel();\n+    }\n+  }\n+\n+  private class DefaultShufflePartitionWriter implements SupportsTransferTo {\n+\n+    private final int partitionId;\n+    private PartitionWriterStream partStream = null;\n+    private PartitionWriterChannel partChannel = null;\n+\n+    private DefaultShufflePartitionWriter(int partitionId) {\n+      this.partitionId = partitionId;\n+    }\n+\n+    @Override\n+    public OutputStream openStream() throws IOException {\n+      if (partStream == null) {\n+        if (outputFileChannel != null) {\n+          throw new IllegalStateException(\"Requested an output channel for a previous write but\" +\n+              \" now an output stream has been requested. Should not be using both channels\" +\n+              \" and streams to write.\");\n+        }\n+        initStream();\n+        partStream = new PartitionWriterStream(partitionId);\n+      }\n+      return partStream;\n+    }\n+\n+    @Override\n+    public TransferrableWritableByteChannel openTransferrableChannel() throws IOException {\n+      if (partChannel == null) {\n+        if (partStream != null) {\n+          throw new IllegalStateException(\"Requested an output stream for a previous write but\" +\n+              \" now an output channel has been requested. Should not be using both channels\" +\n+              \" and streams to write.\");\n+        }\n+        initChannel();\n+        partChannel = new PartitionWriterChannel(partitionId);\n+      }\n+      return partChannel;\n+    }\n+\n+    @Override\n+    public long getNumBytesWritten() {\n+      if (partChannel != null) {\n+        try {\n+          return partChannel.getCount();\n+        } catch (IOException e) {\n+          throw new RuntimeException(e);\n+        }\n+      } else if (partStream != null) {\n+        return partStream.getCount();\n+      } else {\n+        // Assume an empty partition if stream and channel are never created\n+        return 0;\n+      }\n+    }\n+  }\n+\n+  private class PartitionWriterStream extends OutputStream {"
  }],
  "prId": 25007
}]