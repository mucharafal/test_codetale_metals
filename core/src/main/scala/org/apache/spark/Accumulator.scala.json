[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "`DAGScheduler` will collect the accumulator output from executors and aggregate them, so we need the `merge` method to operate on `OUT` directly.\n\nActually this implies that we have to make the intermediate type same with output type, e.g. average accumulator can't implement `merge`.\n\nOne way to fix it is: we should send around the intermedia value, not the final output, between executors and driver.\n",
    "commit": "124568b3eeb7e0a657b2fbe4f54bb85543b7ffa3",
    "createdAt": "2016-04-24T21:26:43Z",
    "diffHunk": "@@ -17,121 +17,281 @@\n \n package org.apache.spark\n \n+import java.{lang => jl}\n+import java.io.{ObjectInputStream, ObjectOutputStream, Serializable}\n import java.util.concurrent.atomic.AtomicLong\n import javax.annotation.concurrent.GuardedBy\n \n-import scala.collection.mutable\n-import scala.ref.WeakReference\n+import scala.collection.generic.Growable\n+import scala.reflect.ClassTag\n \n-import org.apache.spark.internal.Logging\n-import org.apache.spark.storage.{BlockId, BlockStatus}\n+import org.apache.spark.scheduler.AccumulableInfo\n+import org.apache.spark.serializer.JavaSerializer\n+import org.apache.spark.util.Utils\n \n \n-/**\n- * A simpler value of [[Accumulable]] where the result type being accumulated is the same\n- * as the types of elements being merged, i.e. variables that are only \"added\" to through an\n- * associative and commutative operation and can therefore be efficiently supported in parallel.\n- * They can be used to implement counters (as in MapReduce) or sums. Spark natively supports\n- * accumulators of numeric value types, and programmers can add support for new types.\n- *\n- * An accumulator is created from an initial value `v` by calling [[SparkContext#accumulator]].\n- * Tasks running on the cluster can then add to it using the [[Accumulable#+=]] operator.\n- * However, they cannot read its value. Only the driver program can read the accumulator's value,\n- * using its value method.\n- *\n- * The interpreter session below shows an accumulator being used to add up the elements of an array:\n- *\n- * {{{\n- * scala> val accum = sc.accumulator(0)\n- * accum: spark.Accumulator[Int] = 0\n- *\n- * scala> sc.parallelize(Array(1, 2, 3, 4)).foreach(x => accum += x)\n- * ...\n- * 10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 s\n- *\n- * scala> accum.value\n- * res2: Int = 10\n- * }}}\n- *\n- * @param initialValue initial value of accumulator\n- * @param param helper object defining how to add elements of type `T`\n- * @param name human-readable name associated with this accumulator\n- * @param countFailedValues whether to accumulate values from failed tasks\n- * @tparam T result type\n- */\n-class Accumulator[T] private[spark] (\n-    // SI-8813: This must explicitly be a private val, or else scala 2.11 doesn't compile\n-    @transient private val initialValue: T,\n-    param: AccumulatorParam[T],\n-    name: Option[String] = None,\n-    countFailedValues: Boolean = false)\n-  extends Accumulable[T, T](initialValue, param, name, countFailedValues)\n-\n-\n-// TODO: The multi-thread support in accumulators is kind of lame; check\n-// if there's a more intuitive way of doing it right\n-private[spark] object Accumulators extends Logging {\n+abstract class Accumulator[IN, OUT](\n+    val name: Option[String],\n+    private[spark] val countFailedValues: Boolean) extends Serializable {\n+  private[spark] val id = AccumulatorContext.newId()\n+  private[this] var atDriverSide = true\n+\n+  private[spark] def register(sc: SparkContext): Unit = {\n+    if (isRegistered) {\n+      throw new UnsupportedOperationException(\"Cannot register an Accumulator twice.\")\n+    }\n+    AccumulatorContext.register(this)\n+    sc.cleaner.foreach(_.registerAccumulatorForCleanup(this))\n+  }\n+\n+  final def isRegistered: Boolean = AccumulatorContext.originals.containsKey(id)\n+\n+  def initialize(): Unit = {}\n+\n+  def add(v: IN): Unit\n+\n+  def +=(v: IN): Unit = add(v)\n+\n+  def merge(other: OUT): Unit"
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "i was thinking we should use merge internally to merge values. maybe the discrepancy is that we are not sending accumulators back, but rather the acc info back? If we send accumulators back maybe it would be more clear?\n",
    "commit": "124568b3eeb7e0a657b2fbe4f54bb85543b7ffa3",
    "createdAt": "2016-04-24T22:30:14Z",
    "diffHunk": "@@ -17,121 +17,281 @@\n \n package org.apache.spark\n \n+import java.{lang => jl}\n+import java.io.{ObjectInputStream, ObjectOutputStream, Serializable}\n import java.util.concurrent.atomic.AtomicLong\n import javax.annotation.concurrent.GuardedBy\n \n-import scala.collection.mutable\n-import scala.ref.WeakReference\n+import scala.collection.generic.Growable\n+import scala.reflect.ClassTag\n \n-import org.apache.spark.internal.Logging\n-import org.apache.spark.storage.{BlockId, BlockStatus}\n+import org.apache.spark.scheduler.AccumulableInfo\n+import org.apache.spark.serializer.JavaSerializer\n+import org.apache.spark.util.Utils\n \n \n-/**\n- * A simpler value of [[Accumulable]] where the result type being accumulated is the same\n- * as the types of elements being merged, i.e. variables that are only \"added\" to through an\n- * associative and commutative operation and can therefore be efficiently supported in parallel.\n- * They can be used to implement counters (as in MapReduce) or sums. Spark natively supports\n- * accumulators of numeric value types, and programmers can add support for new types.\n- *\n- * An accumulator is created from an initial value `v` by calling [[SparkContext#accumulator]].\n- * Tasks running on the cluster can then add to it using the [[Accumulable#+=]] operator.\n- * However, they cannot read its value. Only the driver program can read the accumulator's value,\n- * using its value method.\n- *\n- * The interpreter session below shows an accumulator being used to add up the elements of an array:\n- *\n- * {{{\n- * scala> val accum = sc.accumulator(0)\n- * accum: spark.Accumulator[Int] = 0\n- *\n- * scala> sc.parallelize(Array(1, 2, 3, 4)).foreach(x => accum += x)\n- * ...\n- * 10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 s\n- *\n- * scala> accum.value\n- * res2: Int = 10\n- * }}}\n- *\n- * @param initialValue initial value of accumulator\n- * @param param helper object defining how to add elements of type `T`\n- * @param name human-readable name associated with this accumulator\n- * @param countFailedValues whether to accumulate values from failed tasks\n- * @tparam T result type\n- */\n-class Accumulator[T] private[spark] (\n-    // SI-8813: This must explicitly be a private val, or else scala 2.11 doesn't compile\n-    @transient private val initialValue: T,\n-    param: AccumulatorParam[T],\n-    name: Option[String] = None,\n-    countFailedValues: Boolean = false)\n-  extends Accumulable[T, T](initialValue, param, name, countFailedValues)\n-\n-\n-// TODO: The multi-thread support in accumulators is kind of lame; check\n-// if there's a more intuitive way of doing it right\n-private[spark] object Accumulators extends Logging {\n+abstract class Accumulator[IN, OUT](\n+    val name: Option[String],\n+    private[spark] val countFailedValues: Boolean) extends Serializable {\n+  private[spark] val id = AccumulatorContext.newId()\n+  private[this] var atDriverSide = true\n+\n+  private[spark] def register(sc: SparkContext): Unit = {\n+    if (isRegistered) {\n+      throw new UnsupportedOperationException(\"Cannot register an Accumulator twice.\")\n+    }\n+    AccumulatorContext.register(this)\n+    sc.cleaner.foreach(_.registerAccumulatorForCleanup(this))\n+  }\n+\n+  final def isRegistered: Boolean = AccumulatorContext.originals.containsKey(id)\n+\n+  def initialize(): Unit = {}\n+\n+  def add(v: IN): Unit\n+\n+  def +=(v: IN): Unit = add(v)\n+\n+  def merge(other: OUT): Unit"
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "Let's talk more offline.\n",
    "commit": "124568b3eeb7e0a657b2fbe4f54bb85543b7ffa3",
    "createdAt": "2016-04-24T22:30:20Z",
    "diffHunk": "@@ -17,121 +17,281 @@\n \n package org.apache.spark\n \n+import java.{lang => jl}\n+import java.io.{ObjectInputStream, ObjectOutputStream, Serializable}\n import java.util.concurrent.atomic.AtomicLong\n import javax.annotation.concurrent.GuardedBy\n \n-import scala.collection.mutable\n-import scala.ref.WeakReference\n+import scala.collection.generic.Growable\n+import scala.reflect.ClassTag\n \n-import org.apache.spark.internal.Logging\n-import org.apache.spark.storage.{BlockId, BlockStatus}\n+import org.apache.spark.scheduler.AccumulableInfo\n+import org.apache.spark.serializer.JavaSerializer\n+import org.apache.spark.util.Utils\n \n \n-/**\n- * A simpler value of [[Accumulable]] where the result type being accumulated is the same\n- * as the types of elements being merged, i.e. variables that are only \"added\" to through an\n- * associative and commutative operation and can therefore be efficiently supported in parallel.\n- * They can be used to implement counters (as in MapReduce) or sums. Spark natively supports\n- * accumulators of numeric value types, and programmers can add support for new types.\n- *\n- * An accumulator is created from an initial value `v` by calling [[SparkContext#accumulator]].\n- * Tasks running on the cluster can then add to it using the [[Accumulable#+=]] operator.\n- * However, they cannot read its value. Only the driver program can read the accumulator's value,\n- * using its value method.\n- *\n- * The interpreter session below shows an accumulator being used to add up the elements of an array:\n- *\n- * {{{\n- * scala> val accum = sc.accumulator(0)\n- * accum: spark.Accumulator[Int] = 0\n- *\n- * scala> sc.parallelize(Array(1, 2, 3, 4)).foreach(x => accum += x)\n- * ...\n- * 10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 s\n- *\n- * scala> accum.value\n- * res2: Int = 10\n- * }}}\n- *\n- * @param initialValue initial value of accumulator\n- * @param param helper object defining how to add elements of type `T`\n- * @param name human-readable name associated with this accumulator\n- * @param countFailedValues whether to accumulate values from failed tasks\n- * @tparam T result type\n- */\n-class Accumulator[T] private[spark] (\n-    // SI-8813: This must explicitly be a private val, or else scala 2.11 doesn't compile\n-    @transient private val initialValue: T,\n-    param: AccumulatorParam[T],\n-    name: Option[String] = None,\n-    countFailedValues: Boolean = false)\n-  extends Accumulable[T, T](initialValue, param, name, countFailedValues)\n-\n-\n-// TODO: The multi-thread support in accumulators is kind of lame; check\n-// if there's a more intuitive way of doing it right\n-private[spark] object Accumulators extends Logging {\n+abstract class Accumulator[IN, OUT](\n+    val name: Option[String],\n+    private[spark] val countFailedValues: Boolean) extends Serializable {\n+  private[spark] val id = AccumulatorContext.newId()\n+  private[this] var atDriverSide = true\n+\n+  private[spark] def register(sc: SparkContext): Unit = {\n+    if (isRegistered) {\n+      throw new UnsupportedOperationException(\"Cannot register an Accumulator twice.\")\n+    }\n+    AccumulatorContext.register(this)\n+    sc.cleaner.foreach(_.registerAccumulatorForCleanup(this))\n+  }\n+\n+  final def isRegistered: Boolean = AccumulatorContext.originals.containsKey(id)\n+\n+  def initialize(): Unit = {}\n+\n+  def add(v: IN): Unit\n+\n+  def +=(v: IN): Unit = add(v)\n+\n+  def merge(other: OUT): Unit"
  }, {
    "author": {
      "login": "holdenk"
    },
    "body": "If these chats happen by e-mail or gchat or something where having another person included wouldn't be too difficult I'd like to be in the loop (but if its too much overhead to add another person no stress).\n",
    "commit": "124568b3eeb7e0a657b2fbe4f54bb85543b7ffa3",
    "createdAt": "2016-04-25T03:02:46Z",
    "diffHunk": "@@ -17,121 +17,281 @@\n \n package org.apache.spark\n \n+import java.{lang => jl}\n+import java.io.{ObjectInputStream, ObjectOutputStream, Serializable}\n import java.util.concurrent.atomic.AtomicLong\n import javax.annotation.concurrent.GuardedBy\n \n-import scala.collection.mutable\n-import scala.ref.WeakReference\n+import scala.collection.generic.Growable\n+import scala.reflect.ClassTag\n \n-import org.apache.spark.internal.Logging\n-import org.apache.spark.storage.{BlockId, BlockStatus}\n+import org.apache.spark.scheduler.AccumulableInfo\n+import org.apache.spark.serializer.JavaSerializer\n+import org.apache.spark.util.Utils\n \n \n-/**\n- * A simpler value of [[Accumulable]] where the result type being accumulated is the same\n- * as the types of elements being merged, i.e. variables that are only \"added\" to through an\n- * associative and commutative operation and can therefore be efficiently supported in parallel.\n- * They can be used to implement counters (as in MapReduce) or sums. Spark natively supports\n- * accumulators of numeric value types, and programmers can add support for new types.\n- *\n- * An accumulator is created from an initial value `v` by calling [[SparkContext#accumulator]].\n- * Tasks running on the cluster can then add to it using the [[Accumulable#+=]] operator.\n- * However, they cannot read its value. Only the driver program can read the accumulator's value,\n- * using its value method.\n- *\n- * The interpreter session below shows an accumulator being used to add up the elements of an array:\n- *\n- * {{{\n- * scala> val accum = sc.accumulator(0)\n- * accum: spark.Accumulator[Int] = 0\n- *\n- * scala> sc.parallelize(Array(1, 2, 3, 4)).foreach(x => accum += x)\n- * ...\n- * 10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 s\n- *\n- * scala> accum.value\n- * res2: Int = 10\n- * }}}\n- *\n- * @param initialValue initial value of accumulator\n- * @param param helper object defining how to add elements of type `T`\n- * @param name human-readable name associated with this accumulator\n- * @param countFailedValues whether to accumulate values from failed tasks\n- * @tparam T result type\n- */\n-class Accumulator[T] private[spark] (\n-    // SI-8813: This must explicitly be a private val, or else scala 2.11 doesn't compile\n-    @transient private val initialValue: T,\n-    param: AccumulatorParam[T],\n-    name: Option[String] = None,\n-    countFailedValues: Boolean = false)\n-  extends Accumulable[T, T](initialValue, param, name, countFailedValues)\n-\n-\n-// TODO: The multi-thread support in accumulators is kind of lame; check\n-// if there's a more intuitive way of doing it right\n-private[spark] object Accumulators extends Logging {\n+abstract class Accumulator[IN, OUT](\n+    val name: Option[String],\n+    private[spark] val countFailedValues: Boolean) extends Serializable {\n+  private[spark] val id = AccumulatorContext.newId()\n+  private[this] var atDriverSide = true\n+\n+  private[spark] def register(sc: SparkContext): Unit = {\n+    if (isRegistered) {\n+      throw new UnsupportedOperationException(\"Cannot register an Accumulator twice.\")\n+    }\n+    AccumulatorContext.register(this)\n+    sc.cleaner.foreach(_.registerAccumulatorForCleanup(this))\n+  }\n+\n+  final def isRegistered: Boolean = AccumulatorContext.originals.containsKey(id)\n+\n+  def initialize(): Unit = {}\n+\n+  def add(v: IN): Unit\n+\n+  def +=(v: IN): Unit = add(v)\n+\n+  def merge(other: OUT): Unit"
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "@cloud-fan merge should merge with another accumulator, not merging OUT.\n",
    "commit": "124568b3eeb7e0a657b2fbe4f54bb85543b7ffa3",
    "createdAt": "2016-04-25T04:00:05Z",
    "diffHunk": "@@ -17,121 +17,281 @@\n \n package org.apache.spark\n \n+import java.{lang => jl}\n+import java.io.{ObjectInputStream, ObjectOutputStream, Serializable}\n import java.util.concurrent.atomic.AtomicLong\n import javax.annotation.concurrent.GuardedBy\n \n-import scala.collection.mutable\n-import scala.ref.WeakReference\n+import scala.collection.generic.Growable\n+import scala.reflect.ClassTag\n \n-import org.apache.spark.internal.Logging\n-import org.apache.spark.storage.{BlockId, BlockStatus}\n+import org.apache.spark.scheduler.AccumulableInfo\n+import org.apache.spark.serializer.JavaSerializer\n+import org.apache.spark.util.Utils\n \n \n-/**\n- * A simpler value of [[Accumulable]] where the result type being accumulated is the same\n- * as the types of elements being merged, i.e. variables that are only \"added\" to through an\n- * associative and commutative operation and can therefore be efficiently supported in parallel.\n- * They can be used to implement counters (as in MapReduce) or sums. Spark natively supports\n- * accumulators of numeric value types, and programmers can add support for new types.\n- *\n- * An accumulator is created from an initial value `v` by calling [[SparkContext#accumulator]].\n- * Tasks running on the cluster can then add to it using the [[Accumulable#+=]] operator.\n- * However, they cannot read its value. Only the driver program can read the accumulator's value,\n- * using its value method.\n- *\n- * The interpreter session below shows an accumulator being used to add up the elements of an array:\n- *\n- * {{{\n- * scala> val accum = sc.accumulator(0)\n- * accum: spark.Accumulator[Int] = 0\n- *\n- * scala> sc.parallelize(Array(1, 2, 3, 4)).foreach(x => accum += x)\n- * ...\n- * 10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 s\n- *\n- * scala> accum.value\n- * res2: Int = 10\n- * }}}\n- *\n- * @param initialValue initial value of accumulator\n- * @param param helper object defining how to add elements of type `T`\n- * @param name human-readable name associated with this accumulator\n- * @param countFailedValues whether to accumulate values from failed tasks\n- * @tparam T result type\n- */\n-class Accumulator[T] private[spark] (\n-    // SI-8813: This must explicitly be a private val, or else scala 2.11 doesn't compile\n-    @transient private val initialValue: T,\n-    param: AccumulatorParam[T],\n-    name: Option[String] = None,\n-    countFailedValues: Boolean = false)\n-  extends Accumulable[T, T](initialValue, param, name, countFailedValues)\n-\n-\n-// TODO: The multi-thread support in accumulators is kind of lame; check\n-// if there's a more intuitive way of doing it right\n-private[spark] object Accumulators extends Logging {\n+abstract class Accumulator[IN, OUT](\n+    val name: Option[String],\n+    private[spark] val countFailedValues: Boolean) extends Serializable {\n+  private[spark] val id = AccumulatorContext.newId()\n+  private[this] var atDriverSide = true\n+\n+  private[spark] def register(sc: SparkContext): Unit = {\n+    if (isRegistered) {\n+      throw new UnsupportedOperationException(\"Cannot register an Accumulator twice.\")\n+    }\n+    AccumulatorContext.register(this)\n+    sc.cleaner.foreach(_.registerAccumulatorForCleanup(this))\n+  }\n+\n+  final def isRegistered: Boolean = AccumulatorContext.originals.containsKey(id)\n+\n+  def initialize(): Unit = {}\n+\n+  def add(v: IN): Unit\n+\n+  def +=(v: IN): Unit = add(v)\n+\n+  def merge(other: OUT): Unit"
  }],
  "prId": 12612
}]