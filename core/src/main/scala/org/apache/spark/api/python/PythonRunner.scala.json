[{
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "I'd leave few comments for methods that should be implemented here.",
    "commit": "7cd78b2aa44e830e0b8b466d1ca80e54359a3c3c",
    "createdAt": "2017-09-27T05:01:02Z",
    "diffHunk": "@@ -0,0 +1,429 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.python\n+\n+import java.io._\n+import java.net._\n+import java.nio.charset.StandardCharsets\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark._\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util._\n+\n+\n+/**\n+ * Enumerate the type of command that will be sent to the Python worker\n+ */\n+private[spark] object PythonEvalType {\n+  val NON_UDF = 0\n+  val SQL_BATCHED_UDF = 1\n+  val SQL_PANDAS_UDF = 2\n+}\n+\n+/**\n+ * A helper class to run Python mapPartition/UDFs in Spark.\n+ *\n+ * funcs is a list of independent Python functions, each one of them is a list of chained Python\n+ * functions (from bottom to top).\n+ */\n+private[spark] abstract class BasePythonRunner[IN, OUT](\n+    funcs: Seq[ChainedPythonFunctions],\n+    bufferSize: Int,\n+    reuseWorker: Boolean,\n+    evalType: Int,\n+    argOffsets: Array[Array[Int]])\n+  extends Logging {\n+\n+  require(funcs.length == argOffsets.length, \"argOffsets should have the same length as funcs\")\n+\n+  // All the Python functions should have the same exec, version and envvars.\n+  protected val envVars = funcs.head.funcs.head.envVars\n+  protected val pythonExec = funcs.head.funcs.head.pythonExec\n+  protected val pythonVer = funcs.head.funcs.head.pythonVer\n+\n+  // TODO: support accumulator in multiple UDF\n+  protected val accumulator = funcs.head.funcs.head.accumulator\n+\n+  def compute(\n+      inputIterator: Iterator[IN],\n+      partitionIndex: Int,\n+      context: TaskContext): Iterator[OUT] = {\n+    val startTime = System.currentTimeMillis\n+    val env = SparkEnv.get\n+    val localdir = env.blockManager.diskBlockManager.localDirs.map(f => f.getPath()).mkString(\",\")\n+    envVars.put(\"SPARK_LOCAL_DIRS\", localdir) // it's also used in monitor thread\n+    if (reuseWorker) {\n+      envVars.put(\"SPARK_REUSE_WORKER\", \"1\")\n+    }\n+    val worker: Socket = env.createPythonWorker(pythonExec, envVars.asScala.toMap)\n+    // Whether is the worker released into idle pool\n+    val released = new AtomicBoolean(false)\n+\n+    // Start a thread to feed the process input from our parent's iterator\n+    val writerThread = newWriterThread(env, worker, inputIterator, partitionIndex, context)\n+\n+    context.addTaskCompletionListener { context =>\n+      writerThread.shutdownOnTaskCompletion()\n+      if (!reuseWorker || !released.get) {\n+        try {\n+          worker.close()\n+        } catch {\n+          case e: Exception =>\n+            logWarning(\"Failed to close worker socket\", e)\n+        }\n+      }\n+    }\n+\n+    writerThread.start()\n+    new MonitorThread(env, worker, context).start()\n+\n+    // Return an iterator that read lines from the process's stdout\n+    val stream = new DataInputStream(new BufferedInputStream(worker.getInputStream, bufferSize))\n+\n+    val stdoutIterator = newReaderIterator(\n+      stream, writerThread, startTime, env, worker, released, context)\n+    new InterruptibleIterator(context, stdoutIterator)\n+  }\n+\n+  protected def newWriterThread(\n+      env: SparkEnv,\n+      worker: Socket,\n+      inputIterator: Iterator[IN],\n+      partitionIndex: Int,\n+      context: TaskContext): WriterThread\n+\n+  protected def newReaderIterator(\n+      stream: DataInputStream,\n+      writerThread: WriterThread,\n+      startTime: Long,\n+      env: SparkEnv,\n+      worker: Socket,\n+      released: AtomicBoolean,\n+      context: TaskContext): Iterator[OUT]\n+\n+  /**\n+   * The thread responsible for writing the data from the PythonRDD's parent iterator to the\n+   * Python process.\n+   */\n+  abstract class WriterThread(\n+      env: SparkEnv,\n+      worker: Socket,\n+      inputIterator: Iterator[IN],\n+      partitionIndex: Int,\n+      context: TaskContext)\n+    extends Thread(s\"stdout writer for $pythonExec\") {\n+\n+    @volatile private var _exception: Exception = null\n+\n+    private val pythonIncludes = funcs.flatMap(_.funcs.flatMap(_.pythonIncludes.asScala)).toSet\n+    private val broadcastVars = funcs.flatMap(_.funcs.flatMap(_.broadcastVars.asScala))\n+\n+    setDaemon(true)\n+\n+    /** Contains the exception thrown while writing the parent iterator to the Python process. */\n+    def exception: Option[Exception] = Option(_exception)\n+\n+    /** Terminates the writer thread, ignoring any exceptions that may occur due to cleanup. */\n+    def shutdownOnTaskCompletion() {\n+      assert(context.isCompleted)\n+      this.interrupt()\n+    }\n+\n+    def writeCommand(dataOut: DataOutputStream): Unit\n+    def writeIteratorToStream(dataOut: DataOutputStream): Unit"
  }, {
    "author": {
      "login": "ueshin"
    },
    "body": "Sure, I'll add comments.",
    "commit": "7cd78b2aa44e830e0b8b466d1ca80e54359a3c3c",
    "createdAt": "2017-09-27T06:02:12Z",
    "diffHunk": "@@ -0,0 +1,429 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.python\n+\n+import java.io._\n+import java.net._\n+import java.nio.charset.StandardCharsets\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark._\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util._\n+\n+\n+/**\n+ * Enumerate the type of command that will be sent to the Python worker\n+ */\n+private[spark] object PythonEvalType {\n+  val NON_UDF = 0\n+  val SQL_BATCHED_UDF = 1\n+  val SQL_PANDAS_UDF = 2\n+}\n+\n+/**\n+ * A helper class to run Python mapPartition/UDFs in Spark.\n+ *\n+ * funcs is a list of independent Python functions, each one of them is a list of chained Python\n+ * functions (from bottom to top).\n+ */\n+private[spark] abstract class BasePythonRunner[IN, OUT](\n+    funcs: Seq[ChainedPythonFunctions],\n+    bufferSize: Int,\n+    reuseWorker: Boolean,\n+    evalType: Int,\n+    argOffsets: Array[Array[Int]])\n+  extends Logging {\n+\n+  require(funcs.length == argOffsets.length, \"argOffsets should have the same length as funcs\")\n+\n+  // All the Python functions should have the same exec, version and envvars.\n+  protected val envVars = funcs.head.funcs.head.envVars\n+  protected val pythonExec = funcs.head.funcs.head.pythonExec\n+  protected val pythonVer = funcs.head.funcs.head.pythonVer\n+\n+  // TODO: support accumulator in multiple UDF\n+  protected val accumulator = funcs.head.funcs.head.accumulator\n+\n+  def compute(\n+      inputIterator: Iterator[IN],\n+      partitionIndex: Int,\n+      context: TaskContext): Iterator[OUT] = {\n+    val startTime = System.currentTimeMillis\n+    val env = SparkEnv.get\n+    val localdir = env.blockManager.diskBlockManager.localDirs.map(f => f.getPath()).mkString(\",\")\n+    envVars.put(\"SPARK_LOCAL_DIRS\", localdir) // it's also used in monitor thread\n+    if (reuseWorker) {\n+      envVars.put(\"SPARK_REUSE_WORKER\", \"1\")\n+    }\n+    val worker: Socket = env.createPythonWorker(pythonExec, envVars.asScala.toMap)\n+    // Whether is the worker released into idle pool\n+    val released = new AtomicBoolean(false)\n+\n+    // Start a thread to feed the process input from our parent's iterator\n+    val writerThread = newWriterThread(env, worker, inputIterator, partitionIndex, context)\n+\n+    context.addTaskCompletionListener { context =>\n+      writerThread.shutdownOnTaskCompletion()\n+      if (!reuseWorker || !released.get) {\n+        try {\n+          worker.close()\n+        } catch {\n+          case e: Exception =>\n+            logWarning(\"Failed to close worker socket\", e)\n+        }\n+      }\n+    }\n+\n+    writerThread.start()\n+    new MonitorThread(env, worker, context).start()\n+\n+    // Return an iterator that read lines from the process's stdout\n+    val stream = new DataInputStream(new BufferedInputStream(worker.getInputStream, bufferSize))\n+\n+    val stdoutIterator = newReaderIterator(\n+      stream, writerThread, startTime, env, worker, released, context)\n+    new InterruptibleIterator(context, stdoutIterator)\n+  }\n+\n+  protected def newWriterThread(\n+      env: SparkEnv,\n+      worker: Socket,\n+      inputIterator: Iterator[IN],\n+      partitionIndex: Int,\n+      context: TaskContext): WriterThread\n+\n+  protected def newReaderIterator(\n+      stream: DataInputStream,\n+      writerThread: WriterThread,\n+      startTime: Long,\n+      env: SparkEnv,\n+      worker: Socket,\n+      released: AtomicBoolean,\n+      context: TaskContext): Iterator[OUT]\n+\n+  /**\n+   * The thread responsible for writing the data from the PythonRDD's parent iterator to the\n+   * Python process.\n+   */\n+  abstract class WriterThread(\n+      env: SparkEnv,\n+      worker: Socket,\n+      inputIterator: Iterator[IN],\n+      partitionIndex: Int,\n+      context: TaskContext)\n+    extends Thread(s\"stdout writer for $pythonExec\") {\n+\n+    @volatile private var _exception: Exception = null\n+\n+    private val pythonIncludes = funcs.flatMap(_.funcs.flatMap(_.pythonIncludes.asScala)).toSet\n+    private val broadcastVars = funcs.flatMap(_.funcs.flatMap(_.broadcastVars.asScala))\n+\n+    setDaemon(true)\n+\n+    /** Contains the exception thrown while writing the parent iterator to the Python process. */\n+    def exception: Option[Exception] = Option(_exception)\n+\n+    /** Terminates the writer thread, ignoring any exceptions that may occur due to cleanup. */\n+    def shutdownOnTaskCompletion() {\n+      assert(context.isCompleted)\n+      this.interrupt()\n+    }\n+\n+    def writeCommand(dataOut: DataOutputStream): Unit\n+    def writeIteratorToStream(dataOut: DataOutputStream): Unit"
  }],
  "prId": 19349
}]