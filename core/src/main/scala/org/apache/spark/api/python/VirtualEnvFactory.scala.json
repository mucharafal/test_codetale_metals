[{
  "comments": [{
    "author": {
      "login": "holdenk"
    },
    "body": "Maybe we can clarify why we do the logic this way?",
    "commit": "d708997df59d91d1763d353e6d01f20f82e79969",
    "createdAt": "2018-01-08T02:36:08Z",
    "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.python\n+\n+import java.io.File\n+import java.util.{Map => JMap}\n+import java.util.Arrays\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+private[spark] class VirtualEnvFactory(pythonExec: String, conf: SparkConf, isDriver: Boolean)\n+  extends Logging {\n+\n+  private var virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+  private var virtualEnvPath = conf.get(\"spark.pyspark.virtualenv.bin.path\", \"\")\n+  private var virtualEnvName: String = _\n+  private var virtualPythonExec: String = _\n+  private val VIRTUALENV_ID = new AtomicInteger()\n+  private var isLauncher: Boolean = false\n+\n+  // used by launcher when user want to use virtualenv in pyspark shell. Launcher need this class\n+  // to create virtualenv for driver.\n+  def this(pythonExec: String, properties: JMap[String, String], isDriver: java.lang.Boolean) {\n+    this(pythonExec, new SparkConf(), isDriver)\n+    properties.asScala.foreach(entry => this.conf.set(entry._1, entry._2))\n+    virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+    virtualEnvPath = conf.get(\"spark.pyspark.virtualenv.bin.path\")\n+    this.isLauncher = true\n+  }\n+\n+  /*\n+   * Create virtualenv using native virtualenv or conda\n+   *\n+   * Native Virtualenv:\n+   *   -  Execute command: virtualenv -p <pythonExec> --no-site-packages <virtualenvName>\n+   *   -  Execute command: python -m pip --cache-dir <cache-dir> install -r <requirement_file>\n+   *\n+   * Conda\n+   *   -  Execute command: conda create --prefix <prefix> --file <requirement_file> -y\n+   *\n+   */\n+  def setupVirtualEnv(): String = {\n+    logInfo(\"Start to setup virtualenv...\")\n+    logDebug(\"user.dir=\" + System.getProperty(\"user.dir\"))\n+    logDebug(\"user.home=\" + System.getProperty(\"user.home\"))\n+\n+    require(virtualEnvType == \"native\" || virtualEnvType == \"conda\",\n+      s\"VirtualEnvType: ${virtualEnvType} is not supported.\" )\n+    require(new File(virtualEnvPath).exists(),\n+      s\"VirtualEnvPath: ${virtualEnvPath} is not defined or doesn't exist.\")\n+    // Use a temp directory for virtualenv in the following cases:\n+    // 1. driver of pyspark shell\n+    // 2. driver of yarn-client mode\n+    // otherwise create the virtualenv folder under the executor working directory."
  }],
  "prId": 13599
}, {
  "comments": [{
    "author": {
      "login": "holdenk"
    },
    "body": "I wouldn't document the commands called in the function doc string.",
    "commit": "d708997df59d91d1763d353e6d01f20f82e79969",
    "createdAt": "2018-01-08T02:36:38Z",
    "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.python\n+\n+import java.io.File\n+import java.util.{Map => JMap}\n+import java.util.Arrays\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+private[spark] class VirtualEnvFactory(pythonExec: String, conf: SparkConf, isDriver: Boolean)\n+  extends Logging {\n+\n+  private var virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+  private var virtualEnvPath = conf.get(\"spark.pyspark.virtualenv.bin.path\", \"\")\n+  private var virtualEnvName: String = _\n+  private var virtualPythonExec: String = _\n+  private val VIRTUALENV_ID = new AtomicInteger()\n+  private var isLauncher: Boolean = false\n+\n+  // used by launcher when user want to use virtualenv in pyspark shell. Launcher need this class\n+  // to create virtualenv for driver.\n+  def this(pythonExec: String, properties: JMap[String, String], isDriver: java.lang.Boolean) {\n+    this(pythonExec, new SparkConf(), isDriver)\n+    properties.asScala.foreach(entry => this.conf.set(entry._1, entry._2))\n+    virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+    virtualEnvPath = conf.get(\"spark.pyspark.virtualenv.bin.path\")\n+    this.isLauncher = true\n+  }\n+\n+  /*\n+   * Create virtualenv using native virtualenv or conda",
    "line": 52
  }],
  "prId": 13599
}, {
  "comments": [{
    "author": {
      "login": "holdenk"
    },
    "body": "This `virtualEnvPath` variable is confusing since you append a dynamically generated name to it for actual path determination. (If I read this code correctly)\r\n  ",
    "commit": "d708997df59d91d1763d353e6d01f20f82e79969",
    "createdAt": "2018-01-08T02:43:36Z",
    "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.python\n+\n+import java.io.File\n+import java.util.{Map => JMap}\n+import java.util.Arrays\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+private[spark] class VirtualEnvFactory(pythonExec: String, conf: SparkConf, isDriver: Boolean)\n+  extends Logging {\n+\n+  private var virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+  private var virtualEnvPath = conf.get(\"spark.pyspark.virtualenv.bin.path\", \"\")\n+  private var virtualEnvName: String = _\n+  private var virtualPythonExec: String = _\n+  private val VIRTUALENV_ID = new AtomicInteger()\n+  private var isLauncher: Boolean = false\n+\n+  // used by launcher when user want to use virtualenv in pyspark shell. Launcher need this class\n+  // to create virtualenv for driver.\n+  def this(pythonExec: String, properties: JMap[String, String], isDriver: java.lang.Boolean) {\n+    this(pythonExec, new SparkConf(), isDriver)\n+    properties.asScala.foreach(entry => this.conf.set(entry._1, entry._2))\n+    virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+    virtualEnvPath = conf.get(\"spark.pyspark.virtualenv.bin.path\")"
  }],
  "prId": 13599
}, {
  "comments": [{
    "author": {
      "login": "holdenk"
    },
    "body": "I'm confused why were testing this path exists instead of trying to create it if it doesn't already. It also appears to be ignored if in client mode.\r\n  ",
    "commit": "d708997df59d91d1763d353e6d01f20f82e79969",
    "createdAt": "2018-01-08T02:46:45Z",
    "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.python\n+\n+import java.io.File\n+import java.util.{Map => JMap}\n+import java.util.Arrays\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+private[spark] class VirtualEnvFactory(pythonExec: String, conf: SparkConf, isDriver: Boolean)\n+  extends Logging {\n+\n+  private var virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+  private var virtualEnvPath = conf.get(\"spark.pyspark.virtualenv.bin.path\", \"\")\n+  private var virtualEnvName: String = _\n+  private var virtualPythonExec: String = _\n+  private val VIRTUALENV_ID = new AtomicInteger()\n+  private var isLauncher: Boolean = false\n+\n+  // used by launcher when user want to use virtualenv in pyspark shell. Launcher need this class\n+  // to create virtualenv for driver.\n+  def this(pythonExec: String, properties: JMap[String, String], isDriver: java.lang.Boolean) {\n+    this(pythonExec, new SparkConf(), isDriver)\n+    properties.asScala.foreach(entry => this.conf.set(entry._1, entry._2))\n+    virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+    virtualEnvPath = conf.get(\"spark.pyspark.virtualenv.bin.path\")\n+    this.isLauncher = true\n+  }\n+\n+  /*\n+   * Create virtualenv using native virtualenv or conda\n+   *\n+   * Native Virtualenv:\n+   *   -  Execute command: virtualenv -p <pythonExec> --no-site-packages <virtualenvName>\n+   *   -  Execute command: python -m pip --cache-dir <cache-dir> install -r <requirement_file>\n+   *\n+   * Conda\n+   *   -  Execute command: conda create --prefix <prefix> --file <requirement_file> -y\n+   *\n+   */\n+  def setupVirtualEnv(): String = {\n+    logInfo(\"Start to setup virtualenv...\")\n+    logDebug(\"user.dir=\" + System.getProperty(\"user.dir\"))\n+    logDebug(\"user.home=\" + System.getProperty(\"user.home\"))\n+\n+    require(virtualEnvType == \"native\" || virtualEnvType == \"conda\",\n+      s\"VirtualEnvType: ${virtualEnvType} is not supported.\" )\n+    require(new File(virtualEnvPath).exists(),"
  }, {
    "author": {
      "login": "zjffdu"
    },
    "body": "Sorry, the variable name is a little misleading, it should be `virtualEnvBinPath` which is `spark.pyspark.virtualenv.bin.path`",
    "commit": "d708997df59d91d1763d353e6d01f20f82e79969",
    "createdAt": "2018-01-08T03:03:54Z",
    "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.python\n+\n+import java.io.File\n+import java.util.{Map => JMap}\n+import java.util.Arrays\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+private[spark] class VirtualEnvFactory(pythonExec: String, conf: SparkConf, isDriver: Boolean)\n+  extends Logging {\n+\n+  private var virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+  private var virtualEnvPath = conf.get(\"spark.pyspark.virtualenv.bin.path\", \"\")\n+  private var virtualEnvName: String = _\n+  private var virtualPythonExec: String = _\n+  private val VIRTUALENV_ID = new AtomicInteger()\n+  private var isLauncher: Boolean = false\n+\n+  // used by launcher when user want to use virtualenv in pyspark shell. Launcher need this class\n+  // to create virtualenv for driver.\n+  def this(pythonExec: String, properties: JMap[String, String], isDriver: java.lang.Boolean) {\n+    this(pythonExec, new SparkConf(), isDriver)\n+    properties.asScala.foreach(entry => this.conf.set(entry._1, entry._2))\n+    virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+    virtualEnvPath = conf.get(\"spark.pyspark.virtualenv.bin.path\")\n+    this.isLauncher = true\n+  }\n+\n+  /*\n+   * Create virtualenv using native virtualenv or conda\n+   *\n+   * Native Virtualenv:\n+   *   -  Execute command: virtualenv -p <pythonExec> --no-site-packages <virtualenvName>\n+   *   -  Execute command: python -m pip --cache-dir <cache-dir> install -r <requirement_file>\n+   *\n+   * Conda\n+   *   -  Execute command: conda create --prefix <prefix> --file <requirement_file> -y\n+   *\n+   */\n+  def setupVirtualEnv(): String = {\n+    logInfo(\"Start to setup virtualenv...\")\n+    logDebug(\"user.dir=\" + System.getProperty(\"user.dir\"))\n+    logDebug(\"user.home=\" + System.getProperty(\"user.home\"))\n+\n+    require(virtualEnvType == \"native\" || virtualEnvType == \"conda\",\n+      s\"VirtualEnvType: ${virtualEnvType} is not supported.\" )\n+    require(new File(virtualEnvPath).exists(),"
  }],
  "prId": 13599
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Hey @zjffdu, couldn't we actually test this by checking if the generated commands are as expected in strings?",
    "commit": "d708997df59d91d1763d353e6d01f20f82e79969",
    "createdAt": "2018-01-08T03:49:34Z",
    "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.python\n+\n+import java.io.File\n+import java.util.{Map => JMap}\n+import java.util.Arrays\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+private[spark] class VirtualEnvFactory(pythonExec: String, conf: SparkConf, isDriver: Boolean)\n+  extends Logging {\n+\n+  private var virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+  private var virtualEnvPath = conf.get(\"spark.pyspark.virtualenv.bin.path\", \"\")\n+  private var virtualEnvName: String = _\n+  private var virtualPythonExec: String = _\n+  private val VIRTUALENV_ID = new AtomicInteger()\n+  private var isLauncher: Boolean = false\n+\n+  // used by launcher when user want to use virtualenv in pyspark shell. Launcher need this class\n+  // to create virtualenv for driver.\n+  def this(pythonExec: String, properties: JMap[String, String], isDriver: java.lang.Boolean) {\n+    this(pythonExec, new SparkConf(), isDriver)\n+    properties.asScala.foreach(entry => this.conf.set(entry._1, entry._2))\n+    virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+    virtualEnvPath = conf.get(\"spark.pyspark.virtualenv.bin.path\")\n+    this.isLauncher = true\n+  }\n+\n+  /*\n+   * Create virtualenv using native virtualenv or conda\n+   *\n+   * Native Virtualenv:\n+   *   -  Execute command: virtualenv -p <pythonExec> --no-site-packages <virtualenvName>\n+   *   -  Execute command: python -m pip --cache-dir <cache-dir> install -r <requirement_file>\n+   *\n+   * Conda\n+   *   -  Execute command: conda create --prefix <prefix> --file <requirement_file> -y\n+   *\n+   */\n+  def setupVirtualEnv(): String = {"
  }],
  "prId": 13599
}, {
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "Do we need to use `java.lang.Boolean` instead of `Boolean`?\r\n  ",
    "commit": "d708997df59d91d1763d353e6d01f20f82e79969",
    "createdAt": "2018-01-08T06:11:34Z",
    "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.python\n+\n+import java.io.File\n+import java.util.{Map => JMap}\n+import java.util.Arrays\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+private[spark] class VirtualEnvFactory(pythonExec: String, conf: SparkConf, isDriver: Boolean)\n+  extends Logging {\n+\n+  private var virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+  private var virtualEnvPath = conf.get(\"spark.pyspark.virtualenv.bin.path\", \"\")\n+  private var virtualEnvName: String = _\n+  private var virtualPythonExec: String = _\n+  private val VIRTUALENV_ID = new AtomicInteger()\n+  private var isLauncher: Boolean = false\n+\n+  // used by launcher when user want to use virtualenv in pyspark shell. Launcher need this class\n+  // to create virtualenv for driver.\n+  def this(pythonExec: String, properties: JMap[String, String], isDriver: java.lang.Boolean) {",
    "line": 46
  }, {
    "author": {
      "login": "zjffdu"
    },
    "body": "This is because it will also be called in java side via java reflection. ",
    "commit": "d708997df59d91d1763d353e6d01f20f82e79969",
    "createdAt": "2018-01-08T13:05:06Z",
    "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.python\n+\n+import java.io.File\n+import java.util.{Map => JMap}\n+import java.util.Arrays\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+private[spark] class VirtualEnvFactory(pythonExec: String, conf: SparkConf, isDriver: Boolean)\n+  extends Logging {\n+\n+  private var virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+  private var virtualEnvPath = conf.get(\"spark.pyspark.virtualenv.bin.path\", \"\")\n+  private var virtualEnvName: String = _\n+  private var virtualPythonExec: String = _\n+  private val VIRTUALENV_ID = new AtomicInteger()\n+  private var isLauncher: Boolean = false\n+\n+  // used by launcher when user want to use virtualenv in pyspark shell. Launcher need this class\n+  // to create virtualenv for driver.\n+  def this(pythonExec: String, properties: JMap[String, String], isDriver: java.lang.Boolean) {",
    "line": 46
  }, {
    "author": {
      "login": "ueshin"
    },
    "body": "I guess we can use `boolean.class` in java reflection.",
    "commit": "d708997df59d91d1763d353e6d01f20f82e79969",
    "createdAt": "2018-01-23T10:52:16Z",
    "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.python\n+\n+import java.io.File\n+import java.util.{Map => JMap}\n+import java.util.Arrays\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+private[spark] class VirtualEnvFactory(pythonExec: String, conf: SparkConf, isDriver: Boolean)\n+  extends Logging {\n+\n+  private var virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+  private var virtualEnvPath = conf.get(\"spark.pyspark.virtualenv.bin.path\", \"\")\n+  private var virtualEnvName: String = _\n+  private var virtualPythonExec: String = _\n+  private val VIRTUALENV_ID = new AtomicInteger()\n+  private var isLauncher: Boolean = false\n+\n+  // used by launcher when user want to use virtualenv in pyspark shell. Launcher need this class\n+  // to create virtualenv for driver.\n+  def this(pythonExec: String, properties: JMap[String, String], isDriver: java.lang.Boolean) {",
    "line": 46
  }, {
    "author": {
      "login": "zjffdu"
    },
    "body": "It is used by launcher module which doesn't depend on scala. ",
    "commit": "d708997df59d91d1763d353e6d01f20f82e79969",
    "createdAt": "2018-01-30T05:47:45Z",
    "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.python\n+\n+import java.io.File\n+import java.util.{Map => JMap}\n+import java.util.Arrays\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+private[spark] class VirtualEnvFactory(pythonExec: String, conf: SparkConf, isDriver: Boolean)\n+  extends Logging {\n+\n+  private var virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+  private var virtualEnvPath = conf.get(\"spark.pyspark.virtualenv.bin.path\", \"\")\n+  private var virtualEnvName: String = _\n+  private var virtualPythonExec: String = _\n+  private val VIRTUALENV_ID = new AtomicInteger()\n+  private var isLauncher: Boolean = false\n+\n+  // used by launcher when user want to use virtualenv in pyspark shell. Launcher need this class\n+  // to create virtualenv for driver.\n+  def this(pythonExec: String, properties: JMap[String, String], isDriver: java.lang.Boolean) {",
    "line": 46
  }],
  "prId": 13599
}, {
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "How about `this(pythonExec, new SparkConf().setAll(properties.asScala), isDriver)` and removing the following 3 lines so we can make `virtualEnvType` and `virtualEnvPath` val?",
    "commit": "d708997df59d91d1763d353e6d01f20f82e79969",
    "createdAt": "2018-01-08T06:39:28Z",
    "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.python\n+\n+import java.io.File\n+import java.util.{Map => JMap}\n+import java.util.Arrays\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+private[spark] class VirtualEnvFactory(pythonExec: String, conf: SparkConf, isDriver: Boolean)\n+  extends Logging {\n+\n+  private var virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+  private var virtualEnvPath = conf.get(\"spark.pyspark.virtualenv.bin.path\", \"\")\n+  private var virtualEnvName: String = _\n+  private var virtualPythonExec: String = _\n+  private val VIRTUALENV_ID = new AtomicInteger()\n+  private var isLauncher: Boolean = false\n+\n+  // used by launcher when user want to use virtualenv in pyspark shell. Launcher need this class\n+  // to create virtualenv for driver.\n+  def this(pythonExec: String, properties: JMap[String, String], isDriver: java.lang.Boolean) {\n+    this(pythonExec, new SparkConf(), isDriver)"
  }],
  "prId": 13599
}, {
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "nit: `$virtualEnvType` instead of `${virtualEnvType}`.",
    "commit": "d708997df59d91d1763d353e6d01f20f82e79969",
    "createdAt": "2018-01-08T06:43:00Z",
    "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.python\n+\n+import java.io.File\n+import java.util.{Map => JMap}\n+import java.util.Arrays\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+private[spark] class VirtualEnvFactory(pythonExec: String, conf: SparkConf, isDriver: Boolean)\n+  extends Logging {\n+\n+  private var virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+  private var virtualEnvPath = conf.get(\"spark.pyspark.virtualenv.bin.path\", \"\")\n+  private var virtualEnvName: String = _\n+  private var virtualPythonExec: String = _\n+  private val VIRTUALENV_ID = new AtomicInteger()\n+  private var isLauncher: Boolean = false\n+\n+  // used by launcher when user want to use virtualenv in pyspark shell. Launcher need this class\n+  // to create virtualenv for driver.\n+  def this(pythonExec: String, properties: JMap[String, String], isDriver: java.lang.Boolean) {\n+    this(pythonExec, new SparkConf(), isDriver)\n+    properties.asScala.foreach(entry => this.conf.set(entry._1, entry._2))\n+    virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+    virtualEnvPath = conf.get(\"spark.pyspark.virtualenv.bin.path\")\n+    this.isLauncher = true\n+  }\n+\n+  /*\n+   * Create virtualenv using native virtualenv or conda\n+   *\n+   * Native Virtualenv:\n+   *   -  Execute command: virtualenv -p <pythonExec> --no-site-packages <virtualenvName>\n+   *   -  Execute command: python -m pip --cache-dir <cache-dir> install -r <requirement_file>\n+   *\n+   * Conda\n+   *   -  Execute command: conda create --prefix <prefix> --file <requirement_file> -y\n+   *\n+   */\n+  def setupVirtualEnv(): String = {\n+    logInfo(\"Start to setup virtualenv...\")\n+    logDebug(\"user.dir=\" + System.getProperty(\"user.dir\"))\n+    logDebug(\"user.home=\" + System.getProperty(\"user.home\"))\n+\n+    require(virtualEnvType == \"native\" || virtualEnvType == \"conda\",\n+      s\"VirtualEnvType: ${virtualEnvType} is not supported.\" )"
  }],
  "prId": 13599
}, {
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "nit: `$virtualEnvPath` instead of `${virtualEnvPath}`.",
    "commit": "d708997df59d91d1763d353e6d01f20f82e79969",
    "createdAt": "2018-01-08T06:43:28Z",
    "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.python\n+\n+import java.io.File\n+import java.util.{Map => JMap}\n+import java.util.Arrays\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+private[spark] class VirtualEnvFactory(pythonExec: String, conf: SparkConf, isDriver: Boolean)\n+  extends Logging {\n+\n+  private var virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+  private var virtualEnvPath = conf.get(\"spark.pyspark.virtualenv.bin.path\", \"\")\n+  private var virtualEnvName: String = _\n+  private var virtualPythonExec: String = _\n+  private val VIRTUALENV_ID = new AtomicInteger()\n+  private var isLauncher: Boolean = false\n+\n+  // used by launcher when user want to use virtualenv in pyspark shell. Launcher need this class\n+  // to create virtualenv for driver.\n+  def this(pythonExec: String, properties: JMap[String, String], isDriver: java.lang.Boolean) {\n+    this(pythonExec, new SparkConf(), isDriver)\n+    properties.asScala.foreach(entry => this.conf.set(entry._1, entry._2))\n+    virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+    virtualEnvPath = conf.get(\"spark.pyspark.virtualenv.bin.path\")\n+    this.isLauncher = true\n+  }\n+\n+  /*\n+   * Create virtualenv using native virtualenv or conda\n+   *\n+   * Native Virtualenv:\n+   *   -  Execute command: virtualenv -p <pythonExec> --no-site-packages <virtualenvName>\n+   *   -  Execute command: python -m pip --cache-dir <cache-dir> install -r <requirement_file>\n+   *\n+   * Conda\n+   *   -  Execute command: conda create --prefix <prefix> --file <requirement_file> -y\n+   *\n+   */\n+  def setupVirtualEnv(): String = {\n+    logInfo(\"Start to setup virtualenv...\")\n+    logDebug(\"user.dir=\" + System.getProperty(\"user.dir\"))\n+    logDebug(\"user.home=\" + System.getProperty(\"user.home\"))\n+\n+    require(virtualEnvType == \"native\" || virtualEnvType == \"conda\",\n+      s\"VirtualEnvType: ${virtualEnvType} is not supported.\" )\n+    require(new File(virtualEnvPath).exists(),\n+      s\"VirtualEnvPath: ${virtualEnvPath} is not defined or doesn't exist.\")"
  }],
  "prId": 13599
}, {
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "nit: `virtualenvBasedir`?",
    "commit": "d708997df59d91d1763d353e6d01f20f82e79969",
    "createdAt": "2018-01-08T07:10:34Z",
    "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.python\n+\n+import java.io.File\n+import java.util.{Map => JMap}\n+import java.util.Arrays\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+private[spark] class VirtualEnvFactory(pythonExec: String, conf: SparkConf, isDriver: Boolean)\n+  extends Logging {\n+\n+  private var virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+  private var virtualEnvPath = conf.get(\"spark.pyspark.virtualenv.bin.path\", \"\")\n+  private var virtualEnvName: String = _\n+  private var virtualPythonExec: String = _\n+  private val VIRTUALENV_ID = new AtomicInteger()\n+  private var isLauncher: Boolean = false\n+\n+  // used by launcher when user want to use virtualenv in pyspark shell. Launcher need this class\n+  // to create virtualenv for driver.\n+  def this(pythonExec: String, properties: JMap[String, String], isDriver: java.lang.Boolean) {\n+    this(pythonExec, new SparkConf(), isDriver)\n+    properties.asScala.foreach(entry => this.conf.set(entry._1, entry._2))\n+    virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+    virtualEnvPath = conf.get(\"spark.pyspark.virtualenv.bin.path\")\n+    this.isLauncher = true\n+  }\n+\n+  /*\n+   * Create virtualenv using native virtualenv or conda\n+   *\n+   * Native Virtualenv:\n+   *   -  Execute command: virtualenv -p <pythonExec> --no-site-packages <virtualenvName>\n+   *   -  Execute command: python -m pip --cache-dir <cache-dir> install -r <requirement_file>\n+   *\n+   * Conda\n+   *   -  Execute command: conda create --prefix <prefix> --file <requirement_file> -y\n+   *\n+   */\n+  def setupVirtualEnv(): String = {\n+    logInfo(\"Start to setup virtualenv...\")\n+    logDebug(\"user.dir=\" + System.getProperty(\"user.dir\"))\n+    logDebug(\"user.home=\" + System.getProperty(\"user.home\"))\n+\n+    require(virtualEnvType == \"native\" || virtualEnvType == \"conda\",\n+      s\"VirtualEnvType: ${virtualEnvType} is not supported.\" )\n+    require(new File(virtualEnvPath).exists(),\n+      s\"VirtualEnvPath: ${virtualEnvPath} is not defined or doesn't exist.\")\n+    // Use a temp directory for virtualenv in the following cases:\n+    // 1. driver of pyspark shell\n+    // 2. driver of yarn-client mode\n+    // otherwise create the virtualenv folder under the executor working directory.\n+    if (isLauncher ||\n+      (isDriver && conf.get(\"spark.submit.deployMode\") == \"client\")) {\n+      val virtualenv_basedir = Files.createTempDir()"
  }],
  "prId": 13599
}, {
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "nit: `pysparkRequirements`?",
    "commit": "d708997df59d91d1763d353e6d01f20f82e79969",
    "createdAt": "2018-01-08T07:12:52Z",
    "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.python\n+\n+import java.io.File\n+import java.util.{Map => JMap}\n+import java.util.Arrays\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+private[spark] class VirtualEnvFactory(pythonExec: String, conf: SparkConf, isDriver: Boolean)\n+  extends Logging {\n+\n+  private var virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+  private var virtualEnvPath = conf.get(\"spark.pyspark.virtualenv.bin.path\", \"\")\n+  private var virtualEnvName: String = _\n+  private var virtualPythonExec: String = _\n+  private val VIRTUALENV_ID = new AtomicInteger()\n+  private var isLauncher: Boolean = false\n+\n+  // used by launcher when user want to use virtualenv in pyspark shell. Launcher need this class\n+  // to create virtualenv for driver.\n+  def this(pythonExec: String, properties: JMap[String, String], isDriver: java.lang.Boolean) {\n+    this(pythonExec, new SparkConf(), isDriver)\n+    properties.asScala.foreach(entry => this.conf.set(entry._1, entry._2))\n+    virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+    virtualEnvPath = conf.get(\"spark.pyspark.virtualenv.bin.path\")\n+    this.isLauncher = true\n+  }\n+\n+  /*\n+   * Create virtualenv using native virtualenv or conda\n+   *\n+   * Native Virtualenv:\n+   *   -  Execute command: virtualenv -p <pythonExec> --no-site-packages <virtualenvName>\n+   *   -  Execute command: python -m pip --cache-dir <cache-dir> install -r <requirement_file>\n+   *\n+   * Conda\n+   *   -  Execute command: conda create --prefix <prefix> --file <requirement_file> -y\n+   *\n+   */\n+  def setupVirtualEnv(): String = {\n+    logInfo(\"Start to setup virtualenv...\")\n+    logDebug(\"user.dir=\" + System.getProperty(\"user.dir\"))\n+    logDebug(\"user.home=\" + System.getProperty(\"user.home\"))\n+\n+    require(virtualEnvType == \"native\" || virtualEnvType == \"conda\",\n+      s\"VirtualEnvType: ${virtualEnvType} is not supported.\" )\n+    require(new File(virtualEnvPath).exists(),\n+      s\"VirtualEnvPath: ${virtualEnvPath} is not defined or doesn't exist.\")\n+    // Use a temp directory for virtualenv in the following cases:\n+    // 1. driver of pyspark shell\n+    // 2. driver of yarn-client mode\n+    // otherwise create the virtualenv folder under the executor working directory.\n+    if (isLauncher ||\n+      (isDriver && conf.get(\"spark.submit.deployMode\") == \"client\")) {\n+      val virtualenv_basedir = Files.createTempDir()\n+      virtualenv_basedir.deleteOnExit()\n+      virtualEnvName = virtualenv_basedir.getAbsolutePath\n+    } else if (isDriver && conf.get(\"spark.submit.deployMode\") == \"cluster\") {\n+      virtualEnvName = \"virtualenv_driver\"\n+    } else {\n+      // use the working directory of Executor\n+      virtualEnvName = \"virtualenv_\" + conf.getAppId + \"_\" + VIRTUALENV_ID.getAndIncrement()\n+    }\n+\n+    // Use the absolute path of requirement file in the following cases\n+    // 1. driver of pyspark shell\n+    // 2. driver of yarn-client mode\n+    // otherwise just use filename as it would be downloaded to the working directory of Executor\n+    val pyspark_requirements ="
  }, {
    "author": {
      "login": "zjffdu"
    },
    "body": "Fixed",
    "commit": "d708997df59d91d1763d353e6d01f20f82e79969",
    "createdAt": "2018-01-08T13:07:17Z",
    "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.python\n+\n+import java.io.File\n+import java.util.{Map => JMap}\n+import java.util.Arrays\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+private[spark] class VirtualEnvFactory(pythonExec: String, conf: SparkConf, isDriver: Boolean)\n+  extends Logging {\n+\n+  private var virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+  private var virtualEnvPath = conf.get(\"spark.pyspark.virtualenv.bin.path\", \"\")\n+  private var virtualEnvName: String = _\n+  private var virtualPythonExec: String = _\n+  private val VIRTUALENV_ID = new AtomicInteger()\n+  private var isLauncher: Boolean = false\n+\n+  // used by launcher when user want to use virtualenv in pyspark shell. Launcher need this class\n+  // to create virtualenv for driver.\n+  def this(pythonExec: String, properties: JMap[String, String], isDriver: java.lang.Boolean) {\n+    this(pythonExec, new SparkConf(), isDriver)\n+    properties.asScala.foreach(entry => this.conf.set(entry._1, entry._2))\n+    virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+    virtualEnvPath = conf.get(\"spark.pyspark.virtualenv.bin.path\")\n+    this.isLauncher = true\n+  }\n+\n+  /*\n+   * Create virtualenv using native virtualenv or conda\n+   *\n+   * Native Virtualenv:\n+   *   -  Execute command: virtualenv -p <pythonExec> --no-site-packages <virtualenvName>\n+   *   -  Execute command: python -m pip --cache-dir <cache-dir> install -r <requirement_file>\n+   *\n+   * Conda\n+   *   -  Execute command: conda create --prefix <prefix> --file <requirement_file> -y\n+   *\n+   */\n+  def setupVirtualEnv(): String = {\n+    logInfo(\"Start to setup virtualenv...\")\n+    logDebug(\"user.dir=\" + System.getProperty(\"user.dir\"))\n+    logDebug(\"user.home=\" + System.getProperty(\"user.home\"))\n+\n+    require(virtualEnvType == \"native\" || virtualEnvType == \"conda\",\n+      s\"VirtualEnvType: ${virtualEnvType} is not supported.\" )\n+    require(new File(virtualEnvPath).exists(),\n+      s\"VirtualEnvPath: ${virtualEnvPath} is not defined or doesn't exist.\")\n+    // Use a temp directory for virtualenv in the following cases:\n+    // 1. driver of pyspark shell\n+    // 2. driver of yarn-client mode\n+    // otherwise create the virtualenv folder under the executor working directory.\n+    if (isLauncher ||\n+      (isDriver && conf.get(\"spark.submit.deployMode\") == \"client\")) {\n+      val virtualenv_basedir = Files.createTempDir()\n+      virtualenv_basedir.deleteOnExit()\n+      virtualEnvName = virtualenv_basedir.getAbsolutePath\n+    } else if (isDriver && conf.get(\"spark.submit.deployMode\") == \"cluster\") {\n+      virtualEnvName = \"virtualenv_driver\"\n+    } else {\n+      // use the working directory of Executor\n+      virtualEnvName = \"virtualenv_\" + conf.getAppId + \"_\" + VIRTUALENV_ID.getAndIncrement()\n+    }\n+\n+    // Use the absolute path of requirement file in the following cases\n+    // 1. driver of pyspark shell\n+    // 2. driver of yarn-client mode\n+    // otherwise just use filename as it would be downloaded to the working directory of Executor\n+    val pyspark_requirements ="
  }],
  "prId": 13599
}, {
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "According to the reference guide of `virtualenv`, `--no-site-packages` is deprecated. Do we still need this? https://virtualenv.pypa.io/en/stable/reference/#cmdoption-no-site-packages",
    "commit": "d708997df59d91d1763d353e6d01f20f82e79969",
    "createdAt": "2018-01-08T08:31:40Z",
    "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.python\n+\n+import java.io.File\n+import java.util.{Map => JMap}\n+import java.util.Arrays\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+private[spark] class VirtualEnvFactory(pythonExec: String, conf: SparkConf, isDriver: Boolean)\n+  extends Logging {\n+\n+  private var virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+  private var virtualEnvPath = conf.get(\"spark.pyspark.virtualenv.bin.path\", \"\")\n+  private var virtualEnvName: String = _\n+  private var virtualPythonExec: String = _\n+  private val VIRTUALENV_ID = new AtomicInteger()\n+  private var isLauncher: Boolean = false\n+\n+  // used by launcher when user want to use virtualenv in pyspark shell. Launcher need this class\n+  // to create virtualenv for driver.\n+  def this(pythonExec: String, properties: JMap[String, String], isDriver: java.lang.Boolean) {\n+    this(pythonExec, new SparkConf(), isDriver)\n+    properties.asScala.foreach(entry => this.conf.set(entry._1, entry._2))\n+    virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+    virtualEnvPath = conf.get(\"spark.pyspark.virtualenv.bin.path\")\n+    this.isLauncher = true\n+  }\n+\n+  /*\n+   * Create virtualenv using native virtualenv or conda\n+   *\n+   * Native Virtualenv:\n+   *   -  Execute command: virtualenv -p <pythonExec> --no-site-packages <virtualenvName>\n+   *   -  Execute command: python -m pip --cache-dir <cache-dir> install -r <requirement_file>\n+   *\n+   * Conda\n+   *   -  Execute command: conda create --prefix <prefix> --file <requirement_file> -y\n+   *\n+   */\n+  def setupVirtualEnv(): String = {\n+    logInfo(\"Start to setup virtualenv...\")\n+    logDebug(\"user.dir=\" + System.getProperty(\"user.dir\"))\n+    logDebug(\"user.home=\" + System.getProperty(\"user.home\"))\n+\n+    require(virtualEnvType == \"native\" || virtualEnvType == \"conda\",\n+      s\"VirtualEnvType: ${virtualEnvType} is not supported.\" )\n+    require(new File(virtualEnvPath).exists(),\n+      s\"VirtualEnvPath: ${virtualEnvPath} is not defined or doesn't exist.\")\n+    // Use a temp directory for virtualenv in the following cases:\n+    // 1. driver of pyspark shell\n+    // 2. driver of yarn-client mode\n+    // otherwise create the virtualenv folder under the executor working directory.\n+    if (isLauncher ||\n+      (isDriver && conf.get(\"spark.submit.deployMode\") == \"client\")) {\n+      val virtualenv_basedir = Files.createTempDir()\n+      virtualenv_basedir.deleteOnExit()\n+      virtualEnvName = virtualenv_basedir.getAbsolutePath\n+    } else if (isDriver && conf.get(\"spark.submit.deployMode\") == \"cluster\") {\n+      virtualEnvName = \"virtualenv_driver\"\n+    } else {\n+      // use the working directory of Executor\n+      virtualEnvName = \"virtualenv_\" + conf.getAppId + \"_\" + VIRTUALENV_ID.getAndIncrement()\n+    }\n+\n+    // Use the absolute path of requirement file in the following cases\n+    // 1. driver of pyspark shell\n+    // 2. driver of yarn-client mode\n+    // otherwise just use filename as it would be downloaded to the working directory of Executor\n+    val pyspark_requirements =\n+      if (isLauncher ||\n+        (isDriver && conf.get(\"spark.submit.deployMode\") == \"client\")) {\n+        conf.getOption(\"spark.pyspark.virtualenv.requirements\")\n+      } else {\n+        conf.getOption(\"spark.pyspark.virtualenv.requirements\").map(_.split(\"/\").last)\n+      }\n+\n+    val createEnvCommand =\n+      if (virtualEnvType == \"native\") {\n+        Arrays.asList(virtualEnvPath,\n+          \"-p\", pythonExec,\n+          \"--no-site-packages\", virtualEnvName)"
  }],
  "prId": 13599
}, {
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "Use `val`s for these three variables.",
    "commit": "d708997df59d91d1763d353e6d01f20f82e79969",
    "createdAt": "2018-01-23T10:33:18Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.python\n+\n+import java.io.File\n+import java.util.{Map => JMap}\n+import java.util.Arrays\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+class VirtualEnvFactory(pythonExec: String, conf: SparkConf, isDriver: Boolean)\n+  extends Logging {\n+\n+  private var virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+  private var virtualEnvBinPath = conf.get(\"spark.pyspark.virtualenv.bin.path\", \"\")\n+  private var initPythonPackages = conf.getOption(\"spark.pyspark.virtualenv.packages\")"
  }],
  "prId": 13599
}, {
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "Do we need to have these as an instance variables?",
    "commit": "d708997df59d91d1763d353e6d01f20f82e79969",
    "createdAt": "2018-01-23T10:37:28Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.python\n+\n+import java.io.File\n+import java.util.{Map => JMap}\n+import java.util.Arrays\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+class VirtualEnvFactory(pythonExec: String, conf: SparkConf, isDriver: Boolean)\n+  extends Logging {\n+\n+  private var virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+  private var virtualEnvBinPath = conf.get(\"spark.pyspark.virtualenv.bin.path\", \"\")\n+  private var initPythonPackages = conf.getOption(\"spark.pyspark.virtualenv.packages\")\n+  private var virtualEnvName: String = _\n+  private var virtualPythonExec: String = _"
  }],
  "prId": 13599
}, {
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "nit: `virtualenvBasedir`?",
    "commit": "d708997df59d91d1763d353e6d01f20f82e79969",
    "createdAt": "2018-01-23T10:39:24Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.python\n+\n+import java.io.File\n+import java.util.{Map => JMap}\n+import java.util.Arrays\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+class VirtualEnvFactory(pythonExec: String, conf: SparkConf, isDriver: Boolean)\n+  extends Logging {\n+\n+  private var virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+  private var virtualEnvBinPath = conf.get(\"spark.pyspark.virtualenv.bin.path\", \"\")\n+  private var initPythonPackages = conf.getOption(\"spark.pyspark.virtualenv.packages\")\n+  private var virtualEnvName: String = _\n+  private var virtualPythonExec: String = _\n+  private val VIRTUALENV_ID = new AtomicInteger()\n+  private var isLauncher: Boolean = false\n+\n+  // used by launcher when user want to use virtualenv in pyspark shell. Launcher need this class\n+  // to create virtualenv for driver.\n+  def this(pythonExec: String, properties: JMap[String, String], isDriver: java.lang.Boolean) {\n+    this(pythonExec, new SparkConf().setAll(properties.asScala), isDriver)\n+    this.isLauncher = true\n+  }\n+\n+  /*\n+   * Create virtualenv using native virtualenv or conda\n+   *\n+   */\n+  def setupVirtualEnv(): String = {\n+    /*\n+     *\n+     * Native Virtualenv:\n+     *   -  Execute command: virtualenv -p <pythonExec> --no-site-packages <virtualenvName>\n+     *   -  Execute command: python -m pip --cache-dir <cache-dir> install -r <requirement_file>\n+     *\n+     * Conda\n+     *   -  Execute command: conda create --prefix <prefix> --file <requirement_file> -y\n+     *\n+     */\n+    logInfo(\"Start to setup virtualenv...\")\n+    logDebug(\"user.dir=\" + System.getProperty(\"user.dir\"))\n+    logDebug(\"user.home=\" + System.getProperty(\"user.home\"))\n+\n+    require(virtualEnvType == \"native\" || virtualEnvType == \"conda\",\n+      s\"VirtualEnvType: $virtualEnvType is not supported.\" )\n+    require(new File(virtualEnvBinPath).exists(),\n+      s\"VirtualEnvBinPath: $virtualEnvBinPath is not defined or doesn't exist.\")\n+    // Two scenarios of creating virtualenv:\n+    // 1. created in yarn container. Yarn will clean it up after container is exited\n+    // 2. created outside yarn container. Spark need to create temp directory and clean it after app\n+    //    finish.\n+    //      - driver of PySpark shell\n+    //      - driver of yarn-client mode\n+    if (isLauncher ||\n+      (isDriver && conf.get(\"spark.submit.deployMode\") == \"client\")) {\n+      val virtualenv_basedir = Files.createTempDir()"
  }],
  "prId": 13599
}, {
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "nit: `$virtualPythonExec` instead of `${virtualPythonExec}`.",
    "commit": "d708997df59d91d1763d353e6d01f20f82e79969",
    "createdAt": "2018-01-23T10:40:51Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.python\n+\n+import java.io.File\n+import java.util.{Map => JMap}\n+import java.util.Arrays\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+class VirtualEnvFactory(pythonExec: String, conf: SparkConf, isDriver: Boolean)\n+  extends Logging {\n+\n+  private var virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+  private var virtualEnvBinPath = conf.get(\"spark.pyspark.virtualenv.bin.path\", \"\")\n+  private var initPythonPackages = conf.getOption(\"spark.pyspark.virtualenv.packages\")\n+  private var virtualEnvName: String = _\n+  private var virtualPythonExec: String = _\n+  private val VIRTUALENV_ID = new AtomicInteger()\n+  private var isLauncher: Boolean = false\n+\n+  // used by launcher when user want to use virtualenv in pyspark shell. Launcher need this class\n+  // to create virtualenv for driver.\n+  def this(pythonExec: String, properties: JMap[String, String], isDriver: java.lang.Boolean) {\n+    this(pythonExec, new SparkConf().setAll(properties.asScala), isDriver)\n+    this.isLauncher = true\n+  }\n+\n+  /*\n+   * Create virtualenv using native virtualenv or conda\n+   *\n+   */\n+  def setupVirtualEnv(): String = {\n+    /*\n+     *\n+     * Native Virtualenv:\n+     *   -  Execute command: virtualenv -p <pythonExec> --no-site-packages <virtualenvName>\n+     *   -  Execute command: python -m pip --cache-dir <cache-dir> install -r <requirement_file>\n+     *\n+     * Conda\n+     *   -  Execute command: conda create --prefix <prefix> --file <requirement_file> -y\n+     *\n+     */\n+    logInfo(\"Start to setup virtualenv...\")\n+    logDebug(\"user.dir=\" + System.getProperty(\"user.dir\"))\n+    logDebug(\"user.home=\" + System.getProperty(\"user.home\"))\n+\n+    require(virtualEnvType == \"native\" || virtualEnvType == \"conda\",\n+      s\"VirtualEnvType: $virtualEnvType is not supported.\" )\n+    require(new File(virtualEnvBinPath).exists(),\n+      s\"VirtualEnvBinPath: $virtualEnvBinPath is not defined or doesn't exist.\")\n+    // Two scenarios of creating virtualenv:\n+    // 1. created in yarn container. Yarn will clean it up after container is exited\n+    // 2. created outside yarn container. Spark need to create temp directory and clean it after app\n+    //    finish.\n+    //      - driver of PySpark shell\n+    //      - driver of yarn-client mode\n+    if (isLauncher ||\n+      (isDriver && conf.get(\"spark.submit.deployMode\") == \"client\")) {\n+      val virtualenv_basedir = Files.createTempDir()\n+      virtualenv_basedir.deleteOnExit()\n+      virtualEnvName = virtualenv_basedir.getAbsolutePath\n+    } else if (isDriver && conf.get(\"spark.submit.deployMode\") == \"cluster\") {\n+      virtualEnvName = \"virtualenv_driver\"\n+    } else {\n+      // use the working directory of Executor\n+      virtualEnvName = \"virtualenv_\" + conf.getAppId + \"_\" + VIRTUALENV_ID.getAndIncrement()\n+    }\n+\n+    // Use the absolute path of requirement file in the following cases\n+    // 1. driver of pyspark shell\n+    // 2. driver of yarn-client mode\n+    // otherwise just use filename as it would be downloaded to the working directory of Executor\n+    val pysparkRequirements =\n+      if (isLauncher ||\n+        (isDriver && conf.get(\"spark.submit.deployMode\") == \"client\")) {\n+        conf.getOption(\"spark.pyspark.virtualenv.requirements\")\n+      } else {\n+        conf.getOption(\"spark.pyspark.virtualenv.requirements\").map(_.split(\"/\").last)\n+      }\n+\n+    val createEnvCommand =\n+      if (virtualEnvType == \"native\") {\n+        List(virtualEnvBinPath,\n+          \"-p\", pythonExec,\n+          \"--no-site-packages\", virtualEnvName)\n+      } else {\n+        // Two cases of conda\n+        //    1. requirement file is specified. (Batch mode)\n+        //    2. requirement file is not specified. (Interactive mode).\n+        //       In this case `spark.pyspark.virtualenv.python_version` must be specified.\n+\n+        if (pysparkRequirements.isDefined) {\n+          List(virtualEnvBinPath,\n+            \"create\", \"--prefix\", virtualEnvName,\n+            \"--file\", pysparkRequirements.get, \"-y\")\n+        } else {\n+          require(conf.contains(\"spark.pyspark.virtualenv.python_version\"),\n+            \"spark.pyspark.virtualenv.python_version is not set when using conda \" +\n+              \"in interactive mode\")\n+          val pythonVersion = conf.get(\"spark.pyspark.virtualenv.python_version\")\n+          List(virtualEnvBinPath,\n+            \"create\", \"--prefix\", virtualEnvName,\n+            \"python=\" + pythonVersion, \"-y\")\n+        }\n+      }\n+    execCommand(createEnvCommand)\n+\n+    virtualPythonExec = virtualEnvName + \"/bin/python\"\n+    if (virtualEnvType == \"native\" && pysparkRequirements.isDefined) {\n+      // requirement file for native is not mandatory, run this only when requirement file\n+      // is specified.\n+      execCommand(List(virtualPythonExec, \"-m\", \"pip\",\n+        \"--cache-dir\", System.getProperty(\"user.home\"),\n+        \"install\", \"-r\", pysparkRequirements.get))\n+    }\n+    // install additional packages\n+    if (initPythonPackages.isDefined) {\n+      execCommand(List(virtualPythonExec, \"-m\", \"pip\",\n+        \"install\") ::: initPythonPackages.get.split(\",\").toList);\n+    }\n+    logInfo(s\"virtualenv is created at to ${virtualPythonExec}\")"
  }],
  "prId": 13599
}, {
  "comments": [{
    "author": {
      "login": "holdenk"
    },
    "body": "So if the factory is made once then how will these get updated?",
    "commit": "d708997df59d91d1763d353e6d01f20f82e79969",
    "createdAt": "2018-01-26T04:06:10Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.python\n+\n+import java.io.File\n+import java.util.{Map => JMap}\n+import java.util.Arrays\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+class VirtualEnvFactory(pythonExec: String, conf: SparkConf, isDriver: Boolean)\n+  extends Logging {\n+\n+  private val virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+  private val virtualEnvBinPath = conf.get(\"spark.pyspark.virtualenv.bin.path\", \"\")\n+  private val initPythonPackages = conf.getOption(\"spark.pyspark.virtualenv.packages\")",
    "line": 38
  }, {
    "author": {
      "login": "zjffdu"
    },
    "body": "For the existing running executor,  the only way to install additional packages is via `sc.intall_packages`.  And `spark.pyspark.virtualenv.packages` will be updated on driver side first when `sc.install_packges` is called, then new allocated executor will fetch this property and install all the additional packages properly. ",
    "commit": "d708997df59d91d1763d353e6d01f20f82e79969",
    "createdAt": "2018-01-26T09:41:37Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.python\n+\n+import java.io.File\n+import java.util.{Map => JMap}\n+import java.util.Arrays\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+class VirtualEnvFactory(pythonExec: String, conf: SparkConf, isDriver: Boolean)\n+  extends Logging {\n+\n+  private val virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+  private val virtualEnvBinPath = conf.get(\"spark.pyspark.virtualenv.bin.path\", \"\")\n+  private val initPythonPackages = conf.getOption(\"spark.pyspark.virtualenv.packages\")",
    "line": 38
  }],
  "prId": 13599
}, {
  "comments": [{
    "author": {
      "login": "holdenk"
    },
    "body": "Why chacing in the user home dir?",
    "commit": "d708997df59d91d1763d353e6d01f20f82e79969",
    "createdAt": "2018-01-26T04:13:14Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.python\n+\n+import java.io.File\n+import java.util.{Map => JMap}\n+import java.util.Arrays\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+class VirtualEnvFactory(pythonExec: String, conf: SparkConf, isDriver: Boolean)\n+  extends Logging {\n+\n+  private val virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+  private val virtualEnvBinPath = conf.get(\"spark.pyspark.virtualenv.bin.path\", \"\")\n+  private val initPythonPackages = conf.getOption(\"spark.pyspark.virtualenv.packages\")\n+  private var virtualEnvName: String = _\n+  private var virtualPythonExec: String = _\n+  private val VIRTUALENV_ID = new AtomicInteger()\n+  private var isLauncher: Boolean = false\n+\n+  // used by launcher when user want to use virtualenv in pyspark shell. Launcher need this class\n+  // to create virtualenv for driver.\n+  def this(pythonExec: String, properties: JMap[String, String], isDriver: java.lang.Boolean) {\n+    this(pythonExec, new SparkConf().setAll(properties.asScala), isDriver)\n+    this.isLauncher = true\n+  }\n+\n+  /*\n+   * Create virtualenv using native virtualenv or conda\n+   *\n+   */\n+  def setupVirtualEnv(): String = {\n+    /*\n+     *\n+     * Native Virtualenv:\n+     *   -  Execute command: virtualenv -p <pythonExec> --no-site-packages <virtualenvName>\n+     *   -  Execute command: python -m pip --cache-dir <cache-dir> install -r <requirement_file>\n+     *\n+     * Conda\n+     *   -  Execute command: conda create --prefix <prefix> --file <requirement_file> -y\n+     *\n+     */\n+    logInfo(\"Start to setup virtualenv...\")\n+    logDebug(\"user.dir=\" + System.getProperty(\"user.dir\"))\n+    logDebug(\"user.home=\" + System.getProperty(\"user.home\"))\n+\n+    require(virtualEnvType == \"native\" || virtualEnvType == \"conda\",\n+      s\"VirtualEnvType: $virtualEnvType is not supported.\" )\n+    require(new File(virtualEnvBinPath).exists(),\n+      s\"VirtualEnvBinPath: $virtualEnvBinPath is not defined or doesn't exist.\")\n+    // Two scenarios of creating virtualenv:\n+    // 1. created in yarn container. Yarn will clean it up after container is exited\n+    // 2. created outside yarn container. Spark need to create temp directory and clean it after app\n+    //    finish.\n+    //      - driver of PySpark shell\n+    //      - driver of yarn-client mode\n+    if (isLauncher ||\n+      (isDriver && conf.get(\"spark.submit.deployMode\") == \"client\")) {\n+      val virtualenvBasedir = Files.createTempDir()\n+      virtualenvBasedir.deleteOnExit()\n+      virtualEnvName = virtualenvBasedir.getAbsolutePath\n+    } else if (isDriver && conf.get(\"spark.submit.deployMode\") == \"cluster\") {\n+      virtualEnvName = \"virtualenv_driver\"\n+    } else {\n+      // use the working directory of Executor\n+      virtualEnvName = \"virtualenv_\" + conf.getAppId + \"_\" + VIRTUALENV_ID.getAndIncrement()\n+    }\n+\n+    // Use the absolute path of requirement file in the following cases\n+    // 1. driver of pyspark shell\n+    // 2. driver of yarn-client mode\n+    // otherwise just use filename as it would be downloaded to the working directory of Executor\n+    val pysparkRequirements =\n+      if (isLauncher ||\n+        (isDriver && conf.get(\"spark.submit.deployMode\") == \"client\")) {\n+        conf.getOption(\"spark.pyspark.virtualenv.requirements\")\n+      } else {\n+        conf.getOption(\"spark.pyspark.virtualenv.requirements\").map(_.split(\"/\").last)\n+      }\n+\n+    val createEnvCommand =\n+      if (virtualEnvType == \"native\") {\n+        List(virtualEnvBinPath,\n+          \"-p\", pythonExec,\n+          \"--no-site-packages\", virtualEnvName)\n+      } else {\n+        // Two cases of conda\n+        //    1. requirement file is specified. (Batch mode)\n+        //    2. requirement file is not specified. (Interactive mode).\n+        //       In this case `spark.pyspark.virtualenv.python_version` must be specified.\n+\n+        if (pysparkRequirements.isDefined) {\n+          List(virtualEnvBinPath,\n+            \"create\", \"--prefix\", virtualEnvName,\n+            \"--file\", pysparkRequirements.get, \"-y\")\n+        } else {\n+          require(conf.contains(\"spark.pyspark.virtualenv.python_version\"),\n+            \"spark.pyspark.virtualenv.python_version is not set when using conda \" +\n+              \"in interactive mode\")\n+          val pythonVersion = conf.get(\"spark.pyspark.virtualenv.python_version\")\n+          List(virtualEnvBinPath,\n+            \"create\", \"--prefix\", virtualEnvName,\n+            \"python=\" + pythonVersion, \"-y\")\n+        }\n+      }\n+    execCommand(createEnvCommand)\n+\n+    virtualPythonExec = virtualEnvName + \"/bin/python\"\n+    if (virtualEnvType == \"native\" && pysparkRequirements.isDefined) {\n+      // requirement file for native is not mandatory, run this only when requirement file\n+      // is specified.\n+      execCommand(List(virtualPythonExec, \"-m\", \"pip\",\n+        \"--cache-dir\", System.getProperty(\"user.home\"),",
    "line": 136
  }, {
    "author": {
      "login": "zjffdu"
    },
    "body": "In yarn mode, executor runs under user `yarn`, while pip would store cache in other directory that `yarn` don't have permission to write. So here I specify the cache dir to yarn's home directory.",
    "commit": "d708997df59d91d1763d353e6d01f20f82e79969",
    "createdAt": "2018-01-26T05:09:23Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.python\n+\n+import java.io.File\n+import java.util.{Map => JMap}\n+import java.util.Arrays\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+class VirtualEnvFactory(pythonExec: String, conf: SparkConf, isDriver: Boolean)\n+  extends Logging {\n+\n+  private val virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+  private val virtualEnvBinPath = conf.get(\"spark.pyspark.virtualenv.bin.path\", \"\")\n+  private val initPythonPackages = conf.getOption(\"spark.pyspark.virtualenv.packages\")\n+  private var virtualEnvName: String = _\n+  private var virtualPythonExec: String = _\n+  private val VIRTUALENV_ID = new AtomicInteger()\n+  private var isLauncher: Boolean = false\n+\n+  // used by launcher when user want to use virtualenv in pyspark shell. Launcher need this class\n+  // to create virtualenv for driver.\n+  def this(pythonExec: String, properties: JMap[String, String], isDriver: java.lang.Boolean) {\n+    this(pythonExec, new SparkConf().setAll(properties.asScala), isDriver)\n+    this.isLauncher = true\n+  }\n+\n+  /*\n+   * Create virtualenv using native virtualenv or conda\n+   *\n+   */\n+  def setupVirtualEnv(): String = {\n+    /*\n+     *\n+     * Native Virtualenv:\n+     *   -  Execute command: virtualenv -p <pythonExec> --no-site-packages <virtualenvName>\n+     *   -  Execute command: python -m pip --cache-dir <cache-dir> install -r <requirement_file>\n+     *\n+     * Conda\n+     *   -  Execute command: conda create --prefix <prefix> --file <requirement_file> -y\n+     *\n+     */\n+    logInfo(\"Start to setup virtualenv...\")\n+    logDebug(\"user.dir=\" + System.getProperty(\"user.dir\"))\n+    logDebug(\"user.home=\" + System.getProperty(\"user.home\"))\n+\n+    require(virtualEnvType == \"native\" || virtualEnvType == \"conda\",\n+      s\"VirtualEnvType: $virtualEnvType is not supported.\" )\n+    require(new File(virtualEnvBinPath).exists(),\n+      s\"VirtualEnvBinPath: $virtualEnvBinPath is not defined or doesn't exist.\")\n+    // Two scenarios of creating virtualenv:\n+    // 1. created in yarn container. Yarn will clean it up after container is exited\n+    // 2. created outside yarn container. Spark need to create temp directory and clean it after app\n+    //    finish.\n+    //      - driver of PySpark shell\n+    //      - driver of yarn-client mode\n+    if (isLauncher ||\n+      (isDriver && conf.get(\"spark.submit.deployMode\") == \"client\")) {\n+      val virtualenvBasedir = Files.createTempDir()\n+      virtualenvBasedir.deleteOnExit()\n+      virtualEnvName = virtualenvBasedir.getAbsolutePath\n+    } else if (isDriver && conf.get(\"spark.submit.deployMode\") == \"cluster\") {\n+      virtualEnvName = \"virtualenv_driver\"\n+    } else {\n+      // use the working directory of Executor\n+      virtualEnvName = \"virtualenv_\" + conf.getAppId + \"_\" + VIRTUALENV_ID.getAndIncrement()\n+    }\n+\n+    // Use the absolute path of requirement file in the following cases\n+    // 1. driver of pyspark shell\n+    // 2. driver of yarn-client mode\n+    // otherwise just use filename as it would be downloaded to the working directory of Executor\n+    val pysparkRequirements =\n+      if (isLauncher ||\n+        (isDriver && conf.get(\"spark.submit.deployMode\") == \"client\")) {\n+        conf.getOption(\"spark.pyspark.virtualenv.requirements\")\n+      } else {\n+        conf.getOption(\"spark.pyspark.virtualenv.requirements\").map(_.split(\"/\").last)\n+      }\n+\n+    val createEnvCommand =\n+      if (virtualEnvType == \"native\") {\n+        List(virtualEnvBinPath,\n+          \"-p\", pythonExec,\n+          \"--no-site-packages\", virtualEnvName)\n+      } else {\n+        // Two cases of conda\n+        //    1. requirement file is specified. (Batch mode)\n+        //    2. requirement file is not specified. (Interactive mode).\n+        //       In this case `spark.pyspark.virtualenv.python_version` must be specified.\n+\n+        if (pysparkRequirements.isDefined) {\n+          List(virtualEnvBinPath,\n+            \"create\", \"--prefix\", virtualEnvName,\n+            \"--file\", pysparkRequirements.get, \"-y\")\n+        } else {\n+          require(conf.contains(\"spark.pyspark.virtualenv.python_version\"),\n+            \"spark.pyspark.virtualenv.python_version is not set when using conda \" +\n+              \"in interactive mode\")\n+          val pythonVersion = conf.get(\"spark.pyspark.virtualenv.python_version\")\n+          List(virtualEnvBinPath,\n+            \"create\", \"--prefix\", virtualEnvName,\n+            \"python=\" + pythonVersion, \"-y\")\n+        }\n+      }\n+    execCommand(createEnvCommand)\n+\n+    virtualPythonExec = virtualEnvName + \"/bin/python\"\n+    if (virtualEnvType == \"native\" && pysparkRequirements.isDefined) {\n+      // requirement file for native is not mandatory, run this only when requirement file\n+      // is specified.\n+      execCommand(List(virtualPythonExec, \"-m\", \"pip\",\n+        \"--cache-dir\", System.getProperty(\"user.home\"),",
    "line": 136
  }, {
    "author": {
      "login": "ifilonenko"
    },
    "body": "How does this logic carry across cluster managers? Has this been considered for use-cases for Mesos? In Kubernetes, this should be fine but we also should have some documentation about this somewhere and test this with an integration test, ",
    "commit": "d708997df59d91d1763d353e6d01f20f82e79969",
    "createdAt": "2018-06-17T07:04:08Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.python\n+\n+import java.io.File\n+import java.util.{Map => JMap}\n+import java.util.Arrays\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+class VirtualEnvFactory(pythonExec: String, conf: SparkConf, isDriver: Boolean)\n+  extends Logging {\n+\n+  private val virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+  private val virtualEnvBinPath = conf.get(\"spark.pyspark.virtualenv.bin.path\", \"\")\n+  private val initPythonPackages = conf.getOption(\"spark.pyspark.virtualenv.packages\")\n+  private var virtualEnvName: String = _\n+  private var virtualPythonExec: String = _\n+  private val VIRTUALENV_ID = new AtomicInteger()\n+  private var isLauncher: Boolean = false\n+\n+  // used by launcher when user want to use virtualenv in pyspark shell. Launcher need this class\n+  // to create virtualenv for driver.\n+  def this(pythonExec: String, properties: JMap[String, String], isDriver: java.lang.Boolean) {\n+    this(pythonExec, new SparkConf().setAll(properties.asScala), isDriver)\n+    this.isLauncher = true\n+  }\n+\n+  /*\n+   * Create virtualenv using native virtualenv or conda\n+   *\n+   */\n+  def setupVirtualEnv(): String = {\n+    /*\n+     *\n+     * Native Virtualenv:\n+     *   -  Execute command: virtualenv -p <pythonExec> --no-site-packages <virtualenvName>\n+     *   -  Execute command: python -m pip --cache-dir <cache-dir> install -r <requirement_file>\n+     *\n+     * Conda\n+     *   -  Execute command: conda create --prefix <prefix> --file <requirement_file> -y\n+     *\n+     */\n+    logInfo(\"Start to setup virtualenv...\")\n+    logDebug(\"user.dir=\" + System.getProperty(\"user.dir\"))\n+    logDebug(\"user.home=\" + System.getProperty(\"user.home\"))\n+\n+    require(virtualEnvType == \"native\" || virtualEnvType == \"conda\",\n+      s\"VirtualEnvType: $virtualEnvType is not supported.\" )\n+    require(new File(virtualEnvBinPath).exists(),\n+      s\"VirtualEnvBinPath: $virtualEnvBinPath is not defined or doesn't exist.\")\n+    // Two scenarios of creating virtualenv:\n+    // 1. created in yarn container. Yarn will clean it up after container is exited\n+    // 2. created outside yarn container. Spark need to create temp directory and clean it after app\n+    //    finish.\n+    //      - driver of PySpark shell\n+    //      - driver of yarn-client mode\n+    if (isLauncher ||\n+      (isDriver && conf.get(\"spark.submit.deployMode\") == \"client\")) {\n+      val virtualenvBasedir = Files.createTempDir()\n+      virtualenvBasedir.deleteOnExit()\n+      virtualEnvName = virtualenvBasedir.getAbsolutePath\n+    } else if (isDriver && conf.get(\"spark.submit.deployMode\") == \"cluster\") {\n+      virtualEnvName = \"virtualenv_driver\"\n+    } else {\n+      // use the working directory of Executor\n+      virtualEnvName = \"virtualenv_\" + conf.getAppId + \"_\" + VIRTUALENV_ID.getAndIncrement()\n+    }\n+\n+    // Use the absolute path of requirement file in the following cases\n+    // 1. driver of pyspark shell\n+    // 2. driver of yarn-client mode\n+    // otherwise just use filename as it would be downloaded to the working directory of Executor\n+    val pysparkRequirements =\n+      if (isLauncher ||\n+        (isDriver && conf.get(\"spark.submit.deployMode\") == \"client\")) {\n+        conf.getOption(\"spark.pyspark.virtualenv.requirements\")\n+      } else {\n+        conf.getOption(\"spark.pyspark.virtualenv.requirements\").map(_.split(\"/\").last)\n+      }\n+\n+    val createEnvCommand =\n+      if (virtualEnvType == \"native\") {\n+        List(virtualEnvBinPath,\n+          \"-p\", pythonExec,\n+          \"--no-site-packages\", virtualEnvName)\n+      } else {\n+        // Two cases of conda\n+        //    1. requirement file is specified. (Batch mode)\n+        //    2. requirement file is not specified. (Interactive mode).\n+        //       In this case `spark.pyspark.virtualenv.python_version` must be specified.\n+\n+        if (pysparkRequirements.isDefined) {\n+          List(virtualEnvBinPath,\n+            \"create\", \"--prefix\", virtualEnvName,\n+            \"--file\", pysparkRequirements.get, \"-y\")\n+        } else {\n+          require(conf.contains(\"spark.pyspark.virtualenv.python_version\"),\n+            \"spark.pyspark.virtualenv.python_version is not set when using conda \" +\n+              \"in interactive mode\")\n+          val pythonVersion = conf.get(\"spark.pyspark.virtualenv.python_version\")\n+          List(virtualEnvBinPath,\n+            \"create\", \"--prefix\", virtualEnvName,\n+            \"python=\" + pythonVersion, \"-y\")\n+        }\n+      }\n+    execCommand(createEnvCommand)\n+\n+    virtualPythonExec = virtualEnvName + \"/bin/python\"\n+    if (virtualEnvType == \"native\" && pysparkRequirements.isDefined) {\n+      // requirement file for native is not mandatory, run this only when requirement file\n+      // is specified.\n+      execCommand(List(virtualPythonExec, \"-m\", \"pip\",\n+        \"--cache-dir\", System.getProperty(\"user.home\"),",
    "line": 136
  }, {
    "author": {
      "login": "mstreuhofer"
    },
    "body": "this builds a cache for each user where you'd end up with multiple caches for the same packages. how about a config to set a common cache (which would have to be writeable by everybody of course)?\r\n\r\nalso i'm wondering about runaway disk usage. who cleans up when things get tight?",
    "commit": "d708997df59d91d1763d353e6d01f20f82e79969",
    "createdAt": "2018-07-28T18:53:15Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.python\n+\n+import java.io.File\n+import java.util.{Map => JMap}\n+import java.util.Arrays\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+class VirtualEnvFactory(pythonExec: String, conf: SparkConf, isDriver: Boolean)\n+  extends Logging {\n+\n+  private val virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+  private val virtualEnvBinPath = conf.get(\"spark.pyspark.virtualenv.bin.path\", \"\")\n+  private val initPythonPackages = conf.getOption(\"spark.pyspark.virtualenv.packages\")\n+  private var virtualEnvName: String = _\n+  private var virtualPythonExec: String = _\n+  private val VIRTUALENV_ID = new AtomicInteger()\n+  private var isLauncher: Boolean = false\n+\n+  // used by launcher when user want to use virtualenv in pyspark shell. Launcher need this class\n+  // to create virtualenv for driver.\n+  def this(pythonExec: String, properties: JMap[String, String], isDriver: java.lang.Boolean) {\n+    this(pythonExec, new SparkConf().setAll(properties.asScala), isDriver)\n+    this.isLauncher = true\n+  }\n+\n+  /*\n+   * Create virtualenv using native virtualenv or conda\n+   *\n+   */\n+  def setupVirtualEnv(): String = {\n+    /*\n+     *\n+     * Native Virtualenv:\n+     *   -  Execute command: virtualenv -p <pythonExec> --no-site-packages <virtualenvName>\n+     *   -  Execute command: python -m pip --cache-dir <cache-dir> install -r <requirement_file>\n+     *\n+     * Conda\n+     *   -  Execute command: conda create --prefix <prefix> --file <requirement_file> -y\n+     *\n+     */\n+    logInfo(\"Start to setup virtualenv...\")\n+    logDebug(\"user.dir=\" + System.getProperty(\"user.dir\"))\n+    logDebug(\"user.home=\" + System.getProperty(\"user.home\"))\n+\n+    require(virtualEnvType == \"native\" || virtualEnvType == \"conda\",\n+      s\"VirtualEnvType: $virtualEnvType is not supported.\" )\n+    require(new File(virtualEnvBinPath).exists(),\n+      s\"VirtualEnvBinPath: $virtualEnvBinPath is not defined or doesn't exist.\")\n+    // Two scenarios of creating virtualenv:\n+    // 1. created in yarn container. Yarn will clean it up after container is exited\n+    // 2. created outside yarn container. Spark need to create temp directory and clean it after app\n+    //    finish.\n+    //      - driver of PySpark shell\n+    //      - driver of yarn-client mode\n+    if (isLauncher ||\n+      (isDriver && conf.get(\"spark.submit.deployMode\") == \"client\")) {\n+      val virtualenvBasedir = Files.createTempDir()\n+      virtualenvBasedir.deleteOnExit()\n+      virtualEnvName = virtualenvBasedir.getAbsolutePath\n+    } else if (isDriver && conf.get(\"spark.submit.deployMode\") == \"cluster\") {\n+      virtualEnvName = \"virtualenv_driver\"\n+    } else {\n+      // use the working directory of Executor\n+      virtualEnvName = \"virtualenv_\" + conf.getAppId + \"_\" + VIRTUALENV_ID.getAndIncrement()\n+    }\n+\n+    // Use the absolute path of requirement file in the following cases\n+    // 1. driver of pyspark shell\n+    // 2. driver of yarn-client mode\n+    // otherwise just use filename as it would be downloaded to the working directory of Executor\n+    val pysparkRequirements =\n+      if (isLauncher ||\n+        (isDriver && conf.get(\"spark.submit.deployMode\") == \"client\")) {\n+        conf.getOption(\"spark.pyspark.virtualenv.requirements\")\n+      } else {\n+        conf.getOption(\"spark.pyspark.virtualenv.requirements\").map(_.split(\"/\").last)\n+      }\n+\n+    val createEnvCommand =\n+      if (virtualEnvType == \"native\") {\n+        List(virtualEnvBinPath,\n+          \"-p\", pythonExec,\n+          \"--no-site-packages\", virtualEnvName)\n+      } else {\n+        // Two cases of conda\n+        //    1. requirement file is specified. (Batch mode)\n+        //    2. requirement file is not specified. (Interactive mode).\n+        //       In this case `spark.pyspark.virtualenv.python_version` must be specified.\n+\n+        if (pysparkRequirements.isDefined) {\n+          List(virtualEnvBinPath,\n+            \"create\", \"--prefix\", virtualEnvName,\n+            \"--file\", pysparkRequirements.get, \"-y\")\n+        } else {\n+          require(conf.contains(\"spark.pyspark.virtualenv.python_version\"),\n+            \"spark.pyspark.virtualenv.python_version is not set when using conda \" +\n+              \"in interactive mode\")\n+          val pythonVersion = conf.get(\"spark.pyspark.virtualenv.python_version\")\n+          List(virtualEnvBinPath,\n+            \"create\", \"--prefix\", virtualEnvName,\n+            \"python=\" + pythonVersion, \"-y\")\n+        }\n+      }\n+    execCommand(createEnvCommand)\n+\n+    virtualPythonExec = virtualEnvName + \"/bin/python\"\n+    if (virtualEnvType == \"native\" && pysparkRequirements.isDefined) {\n+      // requirement file for native is not mandatory, run this only when requirement file\n+      // is specified.\n+      execCommand(List(virtualPythonExec, \"-m\", \"pip\",\n+        \"--cache-dir\", System.getProperty(\"user.home\"),",
    "line": 136
  }],
  "prId": 13599
}, {
  "comments": [{
    "author": {
      "login": "holdenk"
    },
    "body": "So we only use the pythonVersion if there is no requirements file? Why?",
    "commit": "d708997df59d91d1763d353e6d01f20f82e79969",
    "createdAt": "2018-01-26T04:16:24Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.python\n+\n+import java.io.File\n+import java.util.{Map => JMap}\n+import java.util.Arrays\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+class VirtualEnvFactory(pythonExec: String, conf: SparkConf, isDriver: Boolean)\n+  extends Logging {\n+\n+  private val virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+  private val virtualEnvBinPath = conf.get(\"spark.pyspark.virtualenv.bin.path\", \"\")\n+  private val initPythonPackages = conf.getOption(\"spark.pyspark.virtualenv.packages\")\n+  private var virtualEnvName: String = _\n+  private var virtualPythonExec: String = _\n+  private val VIRTUALENV_ID = new AtomicInteger()\n+  private var isLauncher: Boolean = false\n+\n+  // used by launcher when user want to use virtualenv in pyspark shell. Launcher need this class\n+  // to create virtualenv for driver.\n+  def this(pythonExec: String, properties: JMap[String, String], isDriver: java.lang.Boolean) {\n+    this(pythonExec, new SparkConf().setAll(properties.asScala), isDriver)\n+    this.isLauncher = true\n+  }\n+\n+  /*\n+   * Create virtualenv using native virtualenv or conda\n+   *\n+   */\n+  def setupVirtualEnv(): String = {\n+    /*\n+     *\n+     * Native Virtualenv:\n+     *   -  Execute command: virtualenv -p <pythonExec> --no-site-packages <virtualenvName>\n+     *   -  Execute command: python -m pip --cache-dir <cache-dir> install -r <requirement_file>\n+     *\n+     * Conda\n+     *   -  Execute command: conda create --prefix <prefix> --file <requirement_file> -y\n+     *\n+     */\n+    logInfo(\"Start to setup virtualenv...\")\n+    logDebug(\"user.dir=\" + System.getProperty(\"user.dir\"))\n+    logDebug(\"user.home=\" + System.getProperty(\"user.home\"))\n+\n+    require(virtualEnvType == \"native\" || virtualEnvType == \"conda\",\n+      s\"VirtualEnvType: $virtualEnvType is not supported.\" )\n+    require(new File(virtualEnvBinPath).exists(),\n+      s\"VirtualEnvBinPath: $virtualEnvBinPath is not defined or doesn't exist.\")\n+    // Two scenarios of creating virtualenv:\n+    // 1. created in yarn container. Yarn will clean it up after container is exited\n+    // 2. created outside yarn container. Spark need to create temp directory and clean it after app\n+    //    finish.\n+    //      - driver of PySpark shell\n+    //      - driver of yarn-client mode\n+    if (isLauncher ||\n+      (isDriver && conf.get(\"spark.submit.deployMode\") == \"client\")) {\n+      val virtualenvBasedir = Files.createTempDir()\n+      virtualenvBasedir.deleteOnExit()\n+      virtualEnvName = virtualenvBasedir.getAbsolutePath\n+    } else if (isDriver && conf.get(\"spark.submit.deployMode\") == \"cluster\") {\n+      virtualEnvName = \"virtualenv_driver\"\n+    } else {\n+      // use the working directory of Executor\n+      virtualEnvName = \"virtualenv_\" + conf.getAppId + \"_\" + VIRTUALENV_ID.getAndIncrement()\n+    }\n+\n+    // Use the absolute path of requirement file in the following cases\n+    // 1. driver of pyspark shell\n+    // 2. driver of yarn-client mode\n+    // otherwise just use filename as it would be downloaded to the working directory of Executor\n+    val pysparkRequirements =\n+      if (isLauncher ||\n+        (isDriver && conf.get(\"spark.submit.deployMode\") == \"client\")) {\n+        conf.getOption(\"spark.pyspark.virtualenv.requirements\")\n+      } else {\n+        conf.getOption(\"spark.pyspark.virtualenv.requirements\").map(_.split(\"/\").last)\n+      }\n+\n+    val createEnvCommand =\n+      if (virtualEnvType == \"native\") {\n+        List(virtualEnvBinPath,\n+          \"-p\", pythonExec,\n+          \"--no-site-packages\", virtualEnvName)\n+      } else {\n+        // Two cases of conda\n+        //    1. requirement file is specified. (Batch mode)\n+        //    2. requirement file is not specified. (Interactive mode).\n+        //       In this case `spark.pyspark.virtualenv.python_version` must be specified.\n+\n+        if (pysparkRequirements.isDefined) {\n+          List(virtualEnvBinPath,\n+            \"create\", \"--prefix\", virtualEnvName,\n+            \"--file\", pysparkRequirements.get, \"-y\")\n+        } else {\n+          require(conf.contains(\"spark.pyspark.virtualenv.python_version\"),\n+            \"spark.pyspark.virtualenv.python_version is not set when using conda \" +\n+              \"in interactive mode\")\n+          val pythonVersion = conf.get(\"spark.pyspark.virtualenv.python_version\")\n+          List(virtualEnvBinPath,\n+            \"create\", \"--prefix\", virtualEnvName,\n+            \"python=\" + pythonVersion, \"-y\")",
    "line": 126
  }, {
    "author": {
      "login": "zjffdu"
    },
    "body": "It is only for conda. Conda's requirement file contains python while native virtualenv don't have python. That's why user don't need to specify python_version when using requirement file but have to specify python_version when no requirement is specified for conda, otherwise conda will create a virtualenv without python. ",
    "commit": "d708997df59d91d1763d353e6d01f20f82e79969",
    "createdAt": "2018-01-26T05:33:29Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.python\n+\n+import java.io.File\n+import java.util.{Map => JMap}\n+import java.util.Arrays\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+class VirtualEnvFactory(pythonExec: String, conf: SparkConf, isDriver: Boolean)\n+  extends Logging {\n+\n+  private val virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+  private val virtualEnvBinPath = conf.get(\"spark.pyspark.virtualenv.bin.path\", \"\")\n+  private val initPythonPackages = conf.getOption(\"spark.pyspark.virtualenv.packages\")\n+  private var virtualEnvName: String = _\n+  private var virtualPythonExec: String = _\n+  private val VIRTUALENV_ID = new AtomicInteger()\n+  private var isLauncher: Boolean = false\n+\n+  // used by launcher when user want to use virtualenv in pyspark shell. Launcher need this class\n+  // to create virtualenv for driver.\n+  def this(pythonExec: String, properties: JMap[String, String], isDriver: java.lang.Boolean) {\n+    this(pythonExec, new SparkConf().setAll(properties.asScala), isDriver)\n+    this.isLauncher = true\n+  }\n+\n+  /*\n+   * Create virtualenv using native virtualenv or conda\n+   *\n+   */\n+  def setupVirtualEnv(): String = {\n+    /*\n+     *\n+     * Native Virtualenv:\n+     *   -  Execute command: virtualenv -p <pythonExec> --no-site-packages <virtualenvName>\n+     *   -  Execute command: python -m pip --cache-dir <cache-dir> install -r <requirement_file>\n+     *\n+     * Conda\n+     *   -  Execute command: conda create --prefix <prefix> --file <requirement_file> -y\n+     *\n+     */\n+    logInfo(\"Start to setup virtualenv...\")\n+    logDebug(\"user.dir=\" + System.getProperty(\"user.dir\"))\n+    logDebug(\"user.home=\" + System.getProperty(\"user.home\"))\n+\n+    require(virtualEnvType == \"native\" || virtualEnvType == \"conda\",\n+      s\"VirtualEnvType: $virtualEnvType is not supported.\" )\n+    require(new File(virtualEnvBinPath).exists(),\n+      s\"VirtualEnvBinPath: $virtualEnvBinPath is not defined or doesn't exist.\")\n+    // Two scenarios of creating virtualenv:\n+    // 1. created in yarn container. Yarn will clean it up after container is exited\n+    // 2. created outside yarn container. Spark need to create temp directory and clean it after app\n+    //    finish.\n+    //      - driver of PySpark shell\n+    //      - driver of yarn-client mode\n+    if (isLauncher ||\n+      (isDriver && conf.get(\"spark.submit.deployMode\") == \"client\")) {\n+      val virtualenvBasedir = Files.createTempDir()\n+      virtualenvBasedir.deleteOnExit()\n+      virtualEnvName = virtualenvBasedir.getAbsolutePath\n+    } else if (isDriver && conf.get(\"spark.submit.deployMode\") == \"cluster\") {\n+      virtualEnvName = \"virtualenv_driver\"\n+    } else {\n+      // use the working directory of Executor\n+      virtualEnvName = \"virtualenv_\" + conf.getAppId + \"_\" + VIRTUALENV_ID.getAndIncrement()\n+    }\n+\n+    // Use the absolute path of requirement file in the following cases\n+    // 1. driver of pyspark shell\n+    // 2. driver of yarn-client mode\n+    // otherwise just use filename as it would be downloaded to the working directory of Executor\n+    val pysparkRequirements =\n+      if (isLauncher ||\n+        (isDriver && conf.get(\"spark.submit.deployMode\") == \"client\")) {\n+        conf.getOption(\"spark.pyspark.virtualenv.requirements\")\n+      } else {\n+        conf.getOption(\"spark.pyspark.virtualenv.requirements\").map(_.split(\"/\").last)\n+      }\n+\n+    val createEnvCommand =\n+      if (virtualEnvType == \"native\") {\n+        List(virtualEnvBinPath,\n+          \"-p\", pythonExec,\n+          \"--no-site-packages\", virtualEnvName)\n+      } else {\n+        // Two cases of conda\n+        //    1. requirement file is specified. (Batch mode)\n+        //    2. requirement file is not specified. (Interactive mode).\n+        //       In this case `spark.pyspark.virtualenv.python_version` must be specified.\n+\n+        if (pysparkRequirements.isDefined) {\n+          List(virtualEnvBinPath,\n+            \"create\", \"--prefix\", virtualEnvName,\n+            \"--file\", pysparkRequirements.get, \"-y\")\n+        } else {\n+          require(conf.contains(\"spark.pyspark.virtualenv.python_version\"),\n+            \"spark.pyspark.virtualenv.python_version is not set when using conda \" +\n+              \"in interactive mode\")\n+          val pythonVersion = conf.get(\"spark.pyspark.virtualenv.python_version\")\n+          List(virtualEnvBinPath,\n+            \"create\", \"--prefix\", virtualEnvName,\n+            \"python=\" + pythonVersion, \"-y\")",
    "line": 126
  }],
  "prId": 13599
}, {
  "comments": [{
    "author": {
      "login": "ifilonenko"
    },
    "body": "In addition how are we handling the case for an existing `s\"$virtualEnvBinPath/$virtualEnvName\"`? ",
    "commit": "d708997df59d91d1763d353e6d01f20f82e79969",
    "createdAt": "2018-06-17T06:38:59Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.python\n+\n+import java.io.File\n+import java.util.{Map => JMap}\n+import java.util.Arrays\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+class VirtualEnvFactory(pythonExec: String, conf: SparkConf, isDriver: Boolean)\n+  extends Logging {\n+\n+  private val virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+  private val virtualEnvBinPath = conf.get(\"spark.pyspark.virtualenv.bin.path\", \"\")\n+  private val initPythonPackages = conf.getOption(\"spark.pyspark.virtualenv.packages\")\n+  private var virtualEnvName: String = _\n+  private var virtualPythonExec: String = _\n+  private val VIRTUALENV_ID = new AtomicInteger()\n+  private var isLauncher: Boolean = false\n+\n+  // used by launcher when user want to use virtualenv in pyspark shell. Launcher need this class\n+  // to create virtualenv for driver.\n+  def this(pythonExec: String, properties: JMap[String, String], isDriver: java.lang.Boolean) {\n+    this(pythonExec, new SparkConf().setAll(properties.asScala), isDriver)\n+    this.isLauncher = true\n+  }\n+\n+  /*\n+   * Create virtualenv using native virtualenv or conda\n+   *\n+   */\n+  def setupVirtualEnv(): String = {\n+    /*\n+     *\n+     * Native Virtualenv:\n+     *   -  Execute command: virtualenv -p <pythonExec> --no-site-packages <virtualenvName>\n+     *   -  Execute command: python -m pip --cache-dir <cache-dir> install -r <requirement_file>\n+     *\n+     * Conda\n+     *   -  Execute command: conda create --prefix <prefix> --file <requirement_file> -y\n+     *\n+     */\n+    logInfo(\"Start to setup virtualenv...\")\n+    logDebug(\"user.dir=\" + System.getProperty(\"user.dir\"))\n+    logDebug(\"user.home=\" + System.getProperty(\"user.home\"))\n+\n+    require(virtualEnvType == \"native\" || virtualEnvType == \"conda\",\n+      s\"VirtualEnvType: $virtualEnvType is not supported.\" )\n+    require(new File(virtualEnvBinPath).exists(),\n+      s\"VirtualEnvBinPath: $virtualEnvBinPath is not defined or doesn't exist.\")",
    "line": 73
  }],
  "prId": 13599
}, {
  "comments": [{
    "author": {
      "login": "ifilonenko"
    },
    "body": "In the case of Kubernetes:\r\n\r\nThis will be created in the base spark-py Docker image, which is shared between the driver and executors and the containers will be cleaned up upon termination of the job via owner-labels (for the executor) and the k8s API-Server (for the driver).\r\n\r\nAs such, (hopefully with client-mode support being completed soon), the below logic should hold as well. \r\n\r\nIs this work going to be cluster-manage agnostic? Or is this supposed to only support Yarn? I would like to see this be applicable to all first-class cluster-management systems. \r\n\r\nI can help with appending to this PR: k8s Support and the appropriate integration tests. ",
    "commit": "d708997df59d91d1763d353e6d01f20f82e79969",
    "createdAt": "2018-06-17T06:42:50Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.python\n+\n+import java.io.File\n+import java.util.{Map => JMap}\n+import java.util.Arrays\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+class VirtualEnvFactory(pythonExec: String, conf: SparkConf, isDriver: Boolean)\n+  extends Logging {\n+\n+  private val virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+  private val virtualEnvBinPath = conf.get(\"spark.pyspark.virtualenv.bin.path\", \"\")\n+  private val initPythonPackages = conf.getOption(\"spark.pyspark.virtualenv.packages\")\n+  private var virtualEnvName: String = _\n+  private var virtualPythonExec: String = _\n+  private val VIRTUALENV_ID = new AtomicInteger()\n+  private var isLauncher: Boolean = false\n+\n+  // used by launcher when user want to use virtualenv in pyspark shell. Launcher need this class\n+  // to create virtualenv for driver.\n+  def this(pythonExec: String, properties: JMap[String, String], isDriver: java.lang.Boolean) {\n+    this(pythonExec, new SparkConf().setAll(properties.asScala), isDriver)\n+    this.isLauncher = true\n+  }\n+\n+  /*\n+   * Create virtualenv using native virtualenv or conda\n+   *\n+   */\n+  def setupVirtualEnv(): String = {\n+    /*\n+     *\n+     * Native Virtualenv:\n+     *   -  Execute command: virtualenv -p <pythonExec> --no-site-packages <virtualenvName>\n+     *   -  Execute command: python -m pip --cache-dir <cache-dir> install -r <requirement_file>\n+     *\n+     * Conda\n+     *   -  Execute command: conda create --prefix <prefix> --file <requirement_file> -y\n+     *\n+     */\n+    logInfo(\"Start to setup virtualenv...\")\n+    logDebug(\"user.dir=\" + System.getProperty(\"user.dir\"))\n+    logDebug(\"user.home=\" + System.getProperty(\"user.home\"))\n+\n+    require(virtualEnvType == \"native\" || virtualEnvType == \"conda\",\n+      s\"VirtualEnvType: $virtualEnvType is not supported.\" )\n+    require(new File(virtualEnvBinPath).exists(),\n+      s\"VirtualEnvBinPath: $virtualEnvBinPath is not defined or doesn't exist.\")\n+    // Two scenarios of creating virtualenv:\n+    // 1. created in yarn container. Yarn will clean it up after container is exited\n+    // 2. created outside yarn container. Spark need to create temp directory and clean it after app\n+    //    finish.\n+    //      - driver of PySpark shell\n+    //      - driver of yarn-client mode",
    "line": 79
  }],
  "prId": 13599
}, {
  "comments": [{
    "author": {
      "login": "ifilonenko"
    },
    "body": "In Kubernetes world, I might want to use a `requirements.txt` file that is stored locally in the base docker image, regardless of client or cluster mode. Is that something that you think should be supported? Maybe a config variable `spark.pyspark.virtualenv.kubernetes.localrequirements` that points to a file stored as `local:///var/files/requirements.txt` for example. \r\n\r\nFurthermore, when we introduce a Resource Staging Server that allows us to stage files locally, this setting will be inter-changable between something that is locally baked in vs. staged. ",
    "commit": "d708997df59d91d1763d353e6d01f20f82e79969",
    "createdAt": "2018-06-17T06:53:02Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.python\n+\n+import java.io.File\n+import java.util.{Map => JMap}\n+import java.util.Arrays\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+class VirtualEnvFactory(pythonExec: String, conf: SparkConf, isDriver: Boolean)\n+  extends Logging {\n+\n+  private val virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+  private val virtualEnvBinPath = conf.get(\"spark.pyspark.virtualenv.bin.path\", \"\")\n+  private val initPythonPackages = conf.getOption(\"spark.pyspark.virtualenv.packages\")\n+  private var virtualEnvName: String = _\n+  private var virtualPythonExec: String = _\n+  private val VIRTUALENV_ID = new AtomicInteger()\n+  private var isLauncher: Boolean = false\n+\n+  // used by launcher when user want to use virtualenv in pyspark shell. Launcher need this class\n+  // to create virtualenv for driver.\n+  def this(pythonExec: String, properties: JMap[String, String], isDriver: java.lang.Boolean) {\n+    this(pythonExec, new SparkConf().setAll(properties.asScala), isDriver)\n+    this.isLauncher = true\n+  }\n+\n+  /*\n+   * Create virtualenv using native virtualenv or conda\n+   *\n+   */\n+  def setupVirtualEnv(): String = {\n+    /*\n+     *\n+     * Native Virtualenv:\n+     *   -  Execute command: virtualenv -p <pythonExec> --no-site-packages <virtualenvName>\n+     *   -  Execute command: python -m pip --cache-dir <cache-dir> install -r <requirement_file>\n+     *\n+     * Conda\n+     *   -  Execute command: conda create --prefix <prefix> --file <requirement_file> -y\n+     *\n+     */\n+    logInfo(\"Start to setup virtualenv...\")\n+    logDebug(\"user.dir=\" + System.getProperty(\"user.dir\"))\n+    logDebug(\"user.home=\" + System.getProperty(\"user.home\"))\n+\n+    require(virtualEnvType == \"native\" || virtualEnvType == \"conda\",\n+      s\"VirtualEnvType: $virtualEnvType is not supported.\" )\n+    require(new File(virtualEnvBinPath).exists(),\n+      s\"VirtualEnvBinPath: $virtualEnvBinPath is not defined or doesn't exist.\")\n+    // Two scenarios of creating virtualenv:\n+    // 1. created in yarn container. Yarn will clean it up after container is exited\n+    // 2. created outside yarn container. Spark need to create temp directory and clean it after app\n+    //    finish.\n+    //      - driver of PySpark shell\n+    //      - driver of yarn-client mode\n+    if (isLauncher ||\n+      (isDriver && conf.get(\"spark.submit.deployMode\") == \"client\")) {\n+      val virtualenvBasedir = Files.createTempDir()\n+      virtualenvBasedir.deleteOnExit()\n+      virtualEnvName = virtualenvBasedir.getAbsolutePath\n+    } else if (isDriver && conf.get(\"spark.submit.deployMode\") == \"cluster\") {\n+      virtualEnvName = \"virtualenv_driver\"\n+    } else {\n+      // use the working directory of Executor\n+      virtualEnvName = \"virtualenv_\" + conf.getAppId + \"_\" + VIRTUALENV_ID.getAndIncrement()\n+    }\n+\n+    // Use the absolute path of requirement file in the following cases\n+    // 1. driver of pyspark shell\n+    // 2. driver of yarn-client mode\n+    // otherwise just use filename as it would be downloaded to the working directory of Executor",
    "line": 95
  }],
  "prId": 13599
}, {
  "comments": [{
    "author": {
      "login": "ifilonenko"
    },
    "body": "Please re-write as a `.map(...).getOrElse(..)` as if(`.isDefined`) not idiomatic Scala",
    "commit": "d708997df59d91d1763d353e6d01f20f82e79969",
    "createdAt": "2018-06-17T07:00:17Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.python\n+\n+import java.io.File\n+import java.util.{Map => JMap}\n+import java.util.Arrays\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+class VirtualEnvFactory(pythonExec: String, conf: SparkConf, isDriver: Boolean)\n+  extends Logging {\n+\n+  private val virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+  private val virtualEnvBinPath = conf.get(\"spark.pyspark.virtualenv.bin.path\", \"\")\n+  private val initPythonPackages = conf.getOption(\"spark.pyspark.virtualenv.packages\")\n+  private var virtualEnvName: String = _\n+  private var virtualPythonExec: String = _\n+  private val VIRTUALENV_ID = new AtomicInteger()\n+  private var isLauncher: Boolean = false\n+\n+  // used by launcher when user want to use virtualenv in pyspark shell. Launcher need this class\n+  // to create virtualenv for driver.\n+  def this(pythonExec: String, properties: JMap[String, String], isDriver: java.lang.Boolean) {\n+    this(pythonExec, new SparkConf().setAll(properties.asScala), isDriver)\n+    this.isLauncher = true\n+  }\n+\n+  /*\n+   * Create virtualenv using native virtualenv or conda\n+   *\n+   */\n+  def setupVirtualEnv(): String = {\n+    /*\n+     *\n+     * Native Virtualenv:\n+     *   -  Execute command: virtualenv -p <pythonExec> --no-site-packages <virtualenvName>\n+     *   -  Execute command: python -m pip --cache-dir <cache-dir> install -r <requirement_file>\n+     *\n+     * Conda\n+     *   -  Execute command: conda create --prefix <prefix> --file <requirement_file> -y\n+     *\n+     */\n+    logInfo(\"Start to setup virtualenv...\")\n+    logDebug(\"user.dir=\" + System.getProperty(\"user.dir\"))\n+    logDebug(\"user.home=\" + System.getProperty(\"user.home\"))\n+\n+    require(virtualEnvType == \"native\" || virtualEnvType == \"conda\",\n+      s\"VirtualEnvType: $virtualEnvType is not supported.\" )\n+    require(new File(virtualEnvBinPath).exists(),\n+      s\"VirtualEnvBinPath: $virtualEnvBinPath is not defined or doesn't exist.\")\n+    // Two scenarios of creating virtualenv:\n+    // 1. created in yarn container. Yarn will clean it up after container is exited\n+    // 2. created outside yarn container. Spark need to create temp directory and clean it after app\n+    //    finish.\n+    //      - driver of PySpark shell\n+    //      - driver of yarn-client mode\n+    if (isLauncher ||\n+      (isDriver && conf.get(\"spark.submit.deployMode\") == \"client\")) {\n+      val virtualenvBasedir = Files.createTempDir()\n+      virtualenvBasedir.deleteOnExit()\n+      virtualEnvName = virtualenvBasedir.getAbsolutePath\n+    } else if (isDriver && conf.get(\"spark.submit.deployMode\") == \"cluster\") {\n+      virtualEnvName = \"virtualenv_driver\"\n+    } else {\n+      // use the working directory of Executor\n+      virtualEnvName = \"virtualenv_\" + conf.getAppId + \"_\" + VIRTUALENV_ID.getAndIncrement()\n+    }\n+\n+    // Use the absolute path of requirement file in the following cases\n+    // 1. driver of pyspark shell\n+    // 2. driver of yarn-client mode\n+    // otherwise just use filename as it would be downloaded to the working directory of Executor\n+    val pysparkRequirements =\n+      if (isLauncher ||\n+        (isDriver && conf.get(\"spark.submit.deployMode\") == \"client\")) {\n+        conf.getOption(\"spark.pyspark.virtualenv.requirements\")\n+      } else {\n+        conf.getOption(\"spark.pyspark.virtualenv.requirements\").map(_.split(\"/\").last)\n+      }\n+\n+    val createEnvCommand =\n+      if (virtualEnvType == \"native\") {\n+        List(virtualEnvBinPath,\n+          \"-p\", pythonExec,\n+          \"--no-site-packages\", virtualEnvName)\n+      } else {\n+        // Two cases of conda\n+        //    1. requirement file is specified. (Batch mode)\n+        //    2. requirement file is not specified. (Interactive mode).\n+        //       In this case `spark.pyspark.virtualenv.python_version` must be specified.\n+\n+        if (pysparkRequirements.isDefined) {",
    "line": 115
  }],
  "prId": 13599
}, {
  "comments": [{
    "author": {
      "login": "ifilonenko"
    },
    "body": "`.forEach` not `isDefined`",
    "commit": "d708997df59d91d1763d353e6d01f20f82e79969",
    "createdAt": "2018-06-17T07:00:58Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.python\n+\n+import java.io.File\n+import java.util.{Map => JMap}\n+import java.util.Arrays\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+class VirtualEnvFactory(pythonExec: String, conf: SparkConf, isDriver: Boolean)\n+  extends Logging {\n+\n+  private val virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+  private val virtualEnvBinPath = conf.get(\"spark.pyspark.virtualenv.bin.path\", \"\")\n+  private val initPythonPackages = conf.getOption(\"spark.pyspark.virtualenv.packages\")\n+  private var virtualEnvName: String = _\n+  private var virtualPythonExec: String = _\n+  private val VIRTUALENV_ID = new AtomicInteger()\n+  private var isLauncher: Boolean = false\n+\n+  // used by launcher when user want to use virtualenv in pyspark shell. Launcher need this class\n+  // to create virtualenv for driver.\n+  def this(pythonExec: String, properties: JMap[String, String], isDriver: java.lang.Boolean) {\n+    this(pythonExec, new SparkConf().setAll(properties.asScala), isDriver)\n+    this.isLauncher = true\n+  }\n+\n+  /*\n+   * Create virtualenv using native virtualenv or conda\n+   *\n+   */\n+  def setupVirtualEnv(): String = {\n+    /*\n+     *\n+     * Native Virtualenv:\n+     *   -  Execute command: virtualenv -p <pythonExec> --no-site-packages <virtualenvName>\n+     *   -  Execute command: python -m pip --cache-dir <cache-dir> install -r <requirement_file>\n+     *\n+     * Conda\n+     *   -  Execute command: conda create --prefix <prefix> --file <requirement_file> -y\n+     *\n+     */\n+    logInfo(\"Start to setup virtualenv...\")\n+    logDebug(\"user.dir=\" + System.getProperty(\"user.dir\"))\n+    logDebug(\"user.home=\" + System.getProperty(\"user.home\"))\n+\n+    require(virtualEnvType == \"native\" || virtualEnvType == \"conda\",\n+      s\"VirtualEnvType: $virtualEnvType is not supported.\" )\n+    require(new File(virtualEnvBinPath).exists(),\n+      s\"VirtualEnvBinPath: $virtualEnvBinPath is not defined or doesn't exist.\")\n+    // Two scenarios of creating virtualenv:\n+    // 1. created in yarn container. Yarn will clean it up after container is exited\n+    // 2. created outside yarn container. Spark need to create temp directory and clean it after app\n+    //    finish.\n+    //      - driver of PySpark shell\n+    //      - driver of yarn-client mode\n+    if (isLauncher ||\n+      (isDriver && conf.get(\"spark.submit.deployMode\") == \"client\")) {\n+      val virtualenvBasedir = Files.createTempDir()\n+      virtualenvBasedir.deleteOnExit()\n+      virtualEnvName = virtualenvBasedir.getAbsolutePath\n+    } else if (isDriver && conf.get(\"spark.submit.deployMode\") == \"cluster\") {\n+      virtualEnvName = \"virtualenv_driver\"\n+    } else {\n+      // use the working directory of Executor\n+      virtualEnvName = \"virtualenv_\" + conf.getAppId + \"_\" + VIRTUALENV_ID.getAndIncrement()\n+    }\n+\n+    // Use the absolute path of requirement file in the following cases\n+    // 1. driver of pyspark shell\n+    // 2. driver of yarn-client mode\n+    // otherwise just use filename as it would be downloaded to the working directory of Executor\n+    val pysparkRequirements =\n+      if (isLauncher ||\n+        (isDriver && conf.get(\"spark.submit.deployMode\") == \"client\")) {\n+        conf.getOption(\"spark.pyspark.virtualenv.requirements\")\n+      } else {\n+        conf.getOption(\"spark.pyspark.virtualenv.requirements\").map(_.split(\"/\").last)\n+      }\n+\n+    val createEnvCommand =\n+      if (virtualEnvType == \"native\") {\n+        List(virtualEnvBinPath,\n+          \"-p\", pythonExec,\n+          \"--no-site-packages\", virtualEnvName)\n+      } else {\n+        // Two cases of conda\n+        //    1. requirement file is specified. (Batch mode)\n+        //    2. requirement file is not specified. (Interactive mode).\n+        //       In this case `spark.pyspark.virtualenv.python_version` must be specified.\n+\n+        if (pysparkRequirements.isDefined) {\n+          List(virtualEnvBinPath,\n+            \"create\", \"--prefix\", virtualEnvName,\n+            \"--file\", pysparkRequirements.get, \"-y\")\n+        } else {\n+          require(conf.contains(\"spark.pyspark.virtualenv.python_version\"),\n+            \"spark.pyspark.virtualenv.python_version is not set when using conda \" +\n+              \"in interactive mode\")\n+          val pythonVersion = conf.get(\"spark.pyspark.virtualenv.python_version\")\n+          List(virtualEnvBinPath,\n+            \"create\", \"--prefix\", virtualEnvName,\n+            \"python=\" + pythonVersion, \"-y\")\n+        }\n+      }\n+    execCommand(createEnvCommand)\n+\n+    virtualPythonExec = virtualEnvName + \"/bin/python\"\n+    if (virtualEnvType == \"native\" && pysparkRequirements.isDefined) {\n+      // requirement file for native is not mandatory, run this only when requirement file\n+      // is specified.\n+      execCommand(List(virtualPythonExec, \"-m\", \"pip\",\n+        \"--cache-dir\", System.getProperty(\"user.home\"),\n+        \"install\", \"-r\", pysparkRequirements.get))\n+    }\n+    // install additional packages\n+    if (initPythonPackages.isDefined) {",
    "line": 140
  }],
  "prId": 13599
}, {
  "comments": [{
    "author": {
      "login": "ifilonenko"
    },
    "body": "`.forEach` not `.isDefined`",
    "commit": "d708997df59d91d1763d353e6d01f20f82e79969",
    "createdAt": "2018-06-17T07:01:36Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.python\n+\n+import java.io.File\n+import java.util.{Map => JMap}\n+import java.util.Arrays\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+class VirtualEnvFactory(pythonExec: String, conf: SparkConf, isDriver: Boolean)\n+  extends Logging {\n+\n+  private val virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+  private val virtualEnvBinPath = conf.get(\"spark.pyspark.virtualenv.bin.path\", \"\")\n+  private val initPythonPackages = conf.getOption(\"spark.pyspark.virtualenv.packages\")\n+  private var virtualEnvName: String = _\n+  private var virtualPythonExec: String = _\n+  private val VIRTUALENV_ID = new AtomicInteger()\n+  private var isLauncher: Boolean = false\n+\n+  // used by launcher when user want to use virtualenv in pyspark shell. Launcher need this class\n+  // to create virtualenv for driver.\n+  def this(pythonExec: String, properties: JMap[String, String], isDriver: java.lang.Boolean) {\n+    this(pythonExec, new SparkConf().setAll(properties.asScala), isDriver)\n+    this.isLauncher = true\n+  }\n+\n+  /*\n+   * Create virtualenv using native virtualenv or conda\n+   *\n+   */\n+  def setupVirtualEnv(): String = {\n+    /*\n+     *\n+     * Native Virtualenv:\n+     *   -  Execute command: virtualenv -p <pythonExec> --no-site-packages <virtualenvName>\n+     *   -  Execute command: python -m pip --cache-dir <cache-dir> install -r <requirement_file>\n+     *\n+     * Conda\n+     *   -  Execute command: conda create --prefix <prefix> --file <requirement_file> -y\n+     *\n+     */\n+    logInfo(\"Start to setup virtualenv...\")\n+    logDebug(\"user.dir=\" + System.getProperty(\"user.dir\"))\n+    logDebug(\"user.home=\" + System.getProperty(\"user.home\"))\n+\n+    require(virtualEnvType == \"native\" || virtualEnvType == \"conda\",\n+      s\"VirtualEnvType: $virtualEnvType is not supported.\" )\n+    require(new File(virtualEnvBinPath).exists(),\n+      s\"VirtualEnvBinPath: $virtualEnvBinPath is not defined or doesn't exist.\")\n+    // Two scenarios of creating virtualenv:\n+    // 1. created in yarn container. Yarn will clean it up after container is exited\n+    // 2. created outside yarn container. Spark need to create temp directory and clean it after app\n+    //    finish.\n+    //      - driver of PySpark shell\n+    //      - driver of yarn-client mode\n+    if (isLauncher ||\n+      (isDriver && conf.get(\"spark.submit.deployMode\") == \"client\")) {\n+      val virtualenvBasedir = Files.createTempDir()\n+      virtualenvBasedir.deleteOnExit()\n+      virtualEnvName = virtualenvBasedir.getAbsolutePath\n+    } else if (isDriver && conf.get(\"spark.submit.deployMode\") == \"cluster\") {\n+      virtualEnvName = \"virtualenv_driver\"\n+    } else {\n+      // use the working directory of Executor\n+      virtualEnvName = \"virtualenv_\" + conf.getAppId + \"_\" + VIRTUALENV_ID.getAndIncrement()\n+    }\n+\n+    // Use the absolute path of requirement file in the following cases\n+    // 1. driver of pyspark shell\n+    // 2. driver of yarn-client mode\n+    // otherwise just use filename as it would be downloaded to the working directory of Executor\n+    val pysparkRequirements =\n+      if (isLauncher ||\n+        (isDriver && conf.get(\"spark.submit.deployMode\") == \"client\")) {\n+        conf.getOption(\"spark.pyspark.virtualenv.requirements\")\n+      } else {\n+        conf.getOption(\"spark.pyspark.virtualenv.requirements\").map(_.split(\"/\").last)\n+      }\n+\n+    val createEnvCommand =\n+      if (virtualEnvType == \"native\") {\n+        List(virtualEnvBinPath,\n+          \"-p\", pythonExec,\n+          \"--no-site-packages\", virtualEnvName)\n+      } else {\n+        // Two cases of conda\n+        //    1. requirement file is specified. (Batch mode)\n+        //    2. requirement file is not specified. (Interactive mode).\n+        //       In this case `spark.pyspark.virtualenv.python_version` must be specified.\n+\n+        if (pysparkRequirements.isDefined) {\n+          List(virtualEnvBinPath,\n+            \"create\", \"--prefix\", virtualEnvName,\n+            \"--file\", pysparkRequirements.get, \"-y\")\n+        } else {\n+          require(conf.contains(\"spark.pyspark.virtualenv.python_version\"),\n+            \"spark.pyspark.virtualenv.python_version is not set when using conda \" +\n+              \"in interactive mode\")\n+          val pythonVersion = conf.get(\"spark.pyspark.virtualenv.python_version\")\n+          List(virtualEnvBinPath,\n+            \"create\", \"--prefix\", virtualEnvName,\n+            \"python=\" + pythonVersion, \"-y\")\n+        }\n+      }\n+    execCommand(createEnvCommand)\n+\n+    virtualPythonExec = virtualEnvName + \"/bin/python\"\n+    if (virtualEnvType == \"native\" && pysparkRequirements.isDefined) {",
    "line": 132
  }],
  "prId": 13599
}, {
  "comments": [{
    "author": {
      "login": "ifilonenko"
    },
    "body": "Might be better to pass args into this function so that is could be properly unit-tested. It seems that there are no unit-tests for this class, so that seems to be a necessary addition. ",
    "commit": "d708997df59d91d1763d353e6d01f20f82e79969",
    "createdAt": "2018-06-17T07:07:54Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.python\n+\n+import java.io.File\n+import java.util.{Map => JMap}\n+import java.util.Arrays\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+class VirtualEnvFactory(pythonExec: String, conf: SparkConf, isDriver: Boolean)\n+  extends Logging {\n+\n+  private val virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+  private val virtualEnvBinPath = conf.get(\"spark.pyspark.virtualenv.bin.path\", \"\")\n+  private val initPythonPackages = conf.getOption(\"spark.pyspark.virtualenv.packages\")\n+  private var virtualEnvName: String = _\n+  private var virtualPythonExec: String = _\n+  private val VIRTUALENV_ID = new AtomicInteger()\n+  private var isLauncher: Boolean = false\n+\n+  // used by launcher when user want to use virtualenv in pyspark shell. Launcher need this class\n+  // to create virtualenv for driver.\n+  def this(pythonExec: String, properties: JMap[String, String], isDriver: java.lang.Boolean) {\n+    this(pythonExec, new SparkConf().setAll(properties.asScala), isDriver)\n+    this.isLauncher = true\n+  }\n+\n+  /*\n+   * Create virtualenv using native virtualenv or conda\n+   *\n+   */\n+  def setupVirtualEnv(): String = {",
    "line": 55
  }],
  "prId": 13599
}, {
  "comments": [{
    "author": {
      "login": "mstreuhofer"
    },
    "body": "the temporary directory is not being deleted on exit.",
    "commit": "d708997df59d91d1763d353e6d01f20f82e79969",
    "createdAt": "2018-07-28T18:15:53Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.python\n+\n+import java.io.File\n+import java.util.{Map => JMap}\n+import java.util.Arrays\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+class VirtualEnvFactory(pythonExec: String, conf: SparkConf, isDriver: Boolean)\n+  extends Logging {\n+\n+  private val virtualEnvType = conf.get(\"spark.pyspark.virtualenv.type\", \"native\")\n+  private val virtualEnvBinPath = conf.get(\"spark.pyspark.virtualenv.bin.path\", \"\")\n+  private val initPythonPackages = conf.getOption(\"spark.pyspark.virtualenv.packages\")\n+  private var virtualEnvName: String = _\n+  private var virtualPythonExec: String = _\n+  private val VIRTUALENV_ID = new AtomicInteger()\n+  private var isLauncher: Boolean = false\n+\n+  // used by launcher when user want to use virtualenv in pyspark shell. Launcher need this class\n+  // to create virtualenv for driver.\n+  def this(pythonExec: String, properties: JMap[String, String], isDriver: java.lang.Boolean) {\n+    this(pythonExec, new SparkConf().setAll(properties.asScala), isDriver)\n+    this.isLauncher = true\n+  }\n+\n+  /*\n+   * Create virtualenv using native virtualenv or conda\n+   *\n+   */\n+  def setupVirtualEnv(): String = {\n+    /*\n+     *\n+     * Native Virtualenv:\n+     *   -  Execute command: virtualenv -p <pythonExec> --no-site-packages <virtualenvName>\n+     *   -  Execute command: python -m pip --cache-dir <cache-dir> install -r <requirement_file>\n+     *\n+     * Conda\n+     *   -  Execute command: conda create --prefix <prefix> --file <requirement_file> -y\n+     *\n+     */\n+    logInfo(\"Start to setup virtualenv...\")\n+    logDebug(\"user.dir=\" + System.getProperty(\"user.dir\"))\n+    logDebug(\"user.home=\" + System.getProperty(\"user.home\"))\n+\n+    require(virtualEnvType == \"native\" || virtualEnvType == \"conda\",\n+      s\"VirtualEnvType: $virtualEnvType is not supported.\" )\n+    require(new File(virtualEnvBinPath).exists(),\n+      s\"VirtualEnvBinPath: $virtualEnvBinPath is not defined or doesn't exist.\")\n+    // Two scenarios of creating virtualenv:\n+    // 1. created in yarn container. Yarn will clean it up after container is exited\n+    // 2. created outside yarn container. Spark need to create temp directory and clean it after app\n+    //    finish.\n+    //      - driver of PySpark shell\n+    //      - driver of yarn-client mode\n+    if (isLauncher ||\n+      (isDriver && conf.get(\"spark.submit.deployMode\") == \"client\")) {\n+      val virtualenvBasedir = Files.createTempDir()\n+      virtualenvBasedir.deleteOnExit()",
    "line": 83
  }],
  "prId": 13599
}]