[{
  "comments": [{
    "author": {
      "login": "felixcheung"
    },
    "body": "I think there's a JIRA on the connection timeout not being configurable...",
    "commit": "dc894269ef5f5bb2f1d1e4da236546dd626cb48a",
    "createdAt": "2019-03-06T18:31:35Z",
    "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.r\n+\n+import java.io._\n+import java.net.{InetAddress, ServerSocket}\n+import java.util.Arrays\n+\n+import scala.io.Source\n+import scala.util.Try\n+\n+import org.apache.spark._\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.BUFFER_SIZE\n+import org.apache.spark.internal.config.R._\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A helper class to run R UDFs in Spark.\n+ */\n+private[spark] abstract class BaseRRunner[IN, OUT](\n+    func: Array[Byte],\n+    deserializer: String,\n+    serializer: String,\n+    packageNames: Array[Byte],\n+    broadcastVars: Array[Broadcast[Object]],\n+    numPartitions: Int,\n+    isDataFrame: Boolean,\n+    colNames: Array[String],\n+    mode: Int)\n+  extends Logging {\n+  protected var bootTime: Double = _\n+  protected var dataStream: DataInputStream = _\n+\n+  def compute(\n+      inputIterator: Iterator[IN],\n+      partitionIndex: Int): Iterator[OUT] = {\n+    // Timing start\n+    bootTime = System.currentTimeMillis / 1000.0\n+\n+    // we expect two connections\n+    val serverSocket = new ServerSocket(0, 2, InetAddress.getByName(\"localhost\"))\n+    val listenPort = serverSocket.getLocalPort()\n+\n+    // The stdout/stderr is shared by multiple tasks, because we use one daemon\n+    // to launch child process as worker.\n+    val errThread = BaseRRunner.createRWorker(listenPort)\n+\n+    // We use two sockets to separate input and output, then it's easy to manage\n+    // the lifecycle of them to avoid deadlock.\n+    // TODO: optimize it to use one socket\n+\n+    // the socket used to send out the input of task\n+    serverSocket.setSoTimeout(10000)",
    "line": 70
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "D'oh, I only checked R side. Let me check Scala side as well.",
    "commit": "dc894269ef5f5bb2f1d1e4da236546dd626cb48a",
    "createdAt": "2019-03-07T00:19:59Z",
    "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.r\n+\n+import java.io._\n+import java.net.{InetAddress, ServerSocket}\n+import java.util.Arrays\n+\n+import scala.io.Source\n+import scala.util.Try\n+\n+import org.apache.spark._\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.BUFFER_SIZE\n+import org.apache.spark.internal.config.R._\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A helper class to run R UDFs in Spark.\n+ */\n+private[spark] abstract class BaseRRunner[IN, OUT](\n+    func: Array[Byte],\n+    deserializer: String,\n+    serializer: String,\n+    packageNames: Array[Byte],\n+    broadcastVars: Array[Broadcast[Object]],\n+    numPartitions: Int,\n+    isDataFrame: Boolean,\n+    colNames: Array[String],\n+    mode: Int)\n+  extends Logging {\n+  protected var bootTime: Double = _\n+  protected var dataStream: DataInputStream = _\n+\n+  def compute(\n+      inputIterator: Iterator[IN],\n+      partitionIndex: Int): Iterator[OUT] = {\n+    // Timing start\n+    bootTime = System.currentTimeMillis / 1000.0\n+\n+    // we expect two connections\n+    val serverSocket = new ServerSocket(0, 2, InetAddress.getByName(\"localhost\"))\n+    val listenPort = serverSocket.getLocalPort()\n+\n+    // The stdout/stderr is shared by multiple tasks, because we use one daemon\n+    // to launch child process as worker.\n+    val errThread = BaseRRunner.createRWorker(listenPort)\n+\n+    // We use two sockets to separate input and output, then it's easy to manage\n+    // the lifecycle of them to avoid deadlock.\n+    // TODO: optimize it to use one socket\n+\n+    // the socket used to send out the input of task\n+    serverSocket.setSoTimeout(10000)",
    "line": 70
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Actually, this one is something already existing before in RRunner side, and there are multiple instances in Python side as well. The default value of `spark.r.backendConnectionTimeout` is 6000. So, if we change this, this could cause potential behaviour change. Let me leave those out in this PR.",
    "commit": "dc894269ef5f5bb2f1d1e4da236546dd626cb48a",
    "createdAt": "2019-03-08T03:44:25Z",
    "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.r\n+\n+import java.io._\n+import java.net.{InetAddress, ServerSocket}\n+import java.util.Arrays\n+\n+import scala.io.Source\n+import scala.util.Try\n+\n+import org.apache.spark._\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.BUFFER_SIZE\n+import org.apache.spark.internal.config.R._\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A helper class to run R UDFs in Spark.\n+ */\n+private[spark] abstract class BaseRRunner[IN, OUT](\n+    func: Array[Byte],\n+    deserializer: String,\n+    serializer: String,\n+    packageNames: Array[Byte],\n+    broadcastVars: Array[Broadcast[Object]],\n+    numPartitions: Int,\n+    isDataFrame: Boolean,\n+    colNames: Array[String],\n+    mode: Int)\n+  extends Logging {\n+  protected var bootTime: Double = _\n+  protected var dataStream: DataInputStream = _\n+\n+  def compute(\n+      inputIterator: Iterator[IN],\n+      partitionIndex: Int): Iterator[OUT] = {\n+    // Timing start\n+    bootTime = System.currentTimeMillis / 1000.0\n+\n+    // we expect two connections\n+    val serverSocket = new ServerSocket(0, 2, InetAddress.getByName(\"localhost\"))\n+    val listenPort = serverSocket.getLocalPort()\n+\n+    // The stdout/stderr is shared by multiple tasks, because we use one daemon\n+    // to launch child process as worker.\n+    val errThread = BaseRRunner.createRWorker(listenPort)\n+\n+    // We use two sockets to separate input and output, then it's easy to manage\n+    // the lifecycle of them to avoid deadlock.\n+    // TODO: optimize it to use one socket\n+\n+    // the socket used to send out the input of task\n+    serverSocket.setSoTimeout(10000)",
    "line": 70
  }, {
    "author": {
      "login": "felixcheung"
    },
    "body": "yes I think it's https://issues.apache.org/jira/browse/SPARK-12609",
    "commit": "dc894269ef5f5bb2f1d1e4da236546dd626cb48a",
    "createdAt": "2019-03-09T06:32:48Z",
    "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.r\n+\n+import java.io._\n+import java.net.{InetAddress, ServerSocket}\n+import java.util.Arrays\n+\n+import scala.io.Source\n+import scala.util.Try\n+\n+import org.apache.spark._\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.BUFFER_SIZE\n+import org.apache.spark.internal.config.R._\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A helper class to run R UDFs in Spark.\n+ */\n+private[spark] abstract class BaseRRunner[IN, OUT](\n+    func: Array[Byte],\n+    deserializer: String,\n+    serializer: String,\n+    packageNames: Array[Byte],\n+    broadcastVars: Array[Broadcast[Object]],\n+    numPartitions: Int,\n+    isDataFrame: Boolean,\n+    colNames: Array[String],\n+    mode: Int)\n+  extends Logging {\n+  protected var bootTime: Double = _\n+  protected var dataStream: DataInputStream = _\n+\n+  def compute(\n+      inputIterator: Iterator[IN],\n+      partitionIndex: Int): Iterator[OUT] = {\n+    // Timing start\n+    bootTime = System.currentTimeMillis / 1000.0\n+\n+    // we expect two connections\n+    val serverSocket = new ServerSocket(0, 2, InetAddress.getByName(\"localhost\"))\n+    val listenPort = serverSocket.getLocalPort()\n+\n+    // The stdout/stderr is shared by multiple tasks, because we use one daemon\n+    // to launch child process as worker.\n+    val errThread = BaseRRunner.createRWorker(listenPort)\n+\n+    // We use two sockets to separate input and output, then it's easy to manage\n+    // the lifecycle of them to avoid deadlock.\n+    // TODO: optimize it to use one socket\n+\n+    // the socket used to send out the input of task\n+    serverSocket.setSoTimeout(10000)",
    "line": 70
  }],
  "prId": 23977
}]