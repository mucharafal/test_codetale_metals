[{
  "comments": [{
    "author": {
      "login": "mateiz"
    },
    "body": "Formatting here is weird, perhaps due to tabs. It would look better as\n\n```\n    sc.parallelize(data, numSlices = 2)\n      .map { case (k, v) =>\n        (new IntWritable(k), new ArrayWritable(classOf[DoubleWritable], v.map(new DoubleWritable(_))))\n      }.saveAsNewAPIHadoopFile[SequenceFileOutputFormat[IntWritable, ArrayWritable]](arrPath)\n```\n",
    "commit": "268df7ed65f75f963b31ff6a5caa3a0a0229eb16",
    "createdAt": "2014-05-28T18:52:04Z",
    "diffHunk": "@@ -0,0 +1,137 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.python\n+\n+import org.apache.spark.SparkContext\n+import org.apache.hadoop.io._\n+import scala.Array\n+import java.io.{DataOutput, DataInput}\n+import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\n+import org.apache.spark.api.java.JavaSparkContext\n+\n+/**\n+ * A class to test MsgPack serialization on the Scala side, that will be deserialized\n+ * in Python\n+ * @param str\n+ * @param int\n+ * @param double\n+ */\n+case class TestWritable(var str: String, var int: Int, var double: Double) extends Writable {\n+  def this() = this(\"\", 0, 0.0)\n+\n+  def getStr = str\n+  def setStr(str: String) { this.str = str }\n+  def getInt = int\n+  def setInt(int: Int) { this.int = int }\n+  def getDouble = double\n+  def setDouble(double: Double) { this.double = double }\n+\n+  def write(out: DataOutput) = {\n+    out.writeUTF(str)\n+    out.writeInt(int)\n+    out.writeDouble(double)\n+  }\n+\n+  def readFields(in: DataInput) = {\n+    str = in.readUTF()\n+    int = in.readInt()\n+    double = in.readDouble()\n+  }\n+}\n+\n+/**\n+ * This object contains method to generate SequenceFile test data and write it to a\n+ * given directory (probably a temp directory)\n+ */\n+object WriteInputFormatTestDataGenerator extends App {\n+  import SparkContext._\n+\n+  def generateData(path: String, jsc: JavaSparkContext) {\n+    val sc = jsc.sc\n+\n+    val basePath = s\"$path/sftestdata/\"\n+    val textPath = s\"$basePath/sftext/\"\n+    val intPath = s\"$basePath/sfint/\"\n+    val doublePath = s\"$basePath/sfdouble/\"\n+    val arrPath = s\"$basePath/sfarray/\"\n+    val mapPath = s\"$basePath/sfmap/\"\n+    val classPath = s\"$basePath/sfclass/\"\n+    val bytesPath = s\"$basePath/sfbytes/\"\n+    val boolPath = s\"$basePath/sfbool/\"\n+    val nullPath = s\"$basePath/sfnull/\"\n+\n+    /*\n+     * Create test data for IntWritable, DoubleWritable, Text, BytesWritable,\n+     * BooleanWritable and NullWritable\n+     */\n+    val intKeys = Seq((1, \"aa\"), (2, \"bb\"), (2, \"aa\"), (3, \"cc\"), (2, \"bb\"), (1, \"aa\"))\n+    sc.parallelize(intKeys).saveAsSequenceFile(intPath)\n+    sc.parallelize(intKeys.map{ case (k, v) => (k.toDouble, v) }).saveAsSequenceFile(doublePath)\n+    sc.parallelize(intKeys.map{ case (k, v) => (k.toString, v) }).saveAsSequenceFile(textPath)\n+    sc.parallelize(intKeys.map{ case (k, v) => (k, v.getBytes) }).saveAsSequenceFile(bytesPath)\n+    val bools = Seq((1, true), (2, true), (2, false), (3, true), (2, false), (1, false))\n+    sc.parallelize(bools).saveAsSequenceFile(boolPath)\n+    sc.parallelize(intKeys).map{ case (k, v) =>\n+      (new IntWritable(k), NullWritable.get())\n+    }.saveAsSequenceFile(nullPath)\n+\n+    // Create test data for ArrayWritable\n+    val data = Seq(\n+      (1, Array(1.0, 2.0, 3.0)),\n+      (2, Array(3.0, 4.0, 5.0)),\n+      (3, Array(4.0, 5.0, 6.0))\n+    )\n+    sc.parallelize(data, numSlices = 2)\n+      .map{ case (k, v) =>\n+      (new IntWritable(k), new ArrayWritable(classOf[DoubleWritable], v.map(new DoubleWritable(_))))\n+    }\n+      .saveAsNewAPIHadoopFile[SequenceFileOutputFormat[IntWritable, ArrayWritable]](arrPath)"
  }],
  "prId": 455
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "All the classes in this file should be `private[spark]` since they are not in the test package.\n",
    "commit": "268df7ed65f75f963b31ff6a5caa3a0a0229eb16",
    "createdAt": "2014-06-07T17:09:13Z",
    "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.api.python\n+\n+import org.apache.spark.SparkContext\n+import org.apache.hadoop.io._\n+import scala.Array\n+import java.io.{DataOutput, DataInput}\n+import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\n+import org.apache.spark.api.java.JavaSparkContext\n+\n+/**\n+ * A class to test MsgPack serialization on the Scala side, that will be deserialized\n+ * in Python\n+ * @param str\n+ * @param int\n+ * @param double\n+ */\n+case class TestWritable(var str: String, var int: Int, var double: Double) extends Writable {",
    "line": 34
  }],
  "prId": 455
}]