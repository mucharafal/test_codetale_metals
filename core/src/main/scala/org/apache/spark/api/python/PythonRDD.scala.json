[{
  "comments": [{
    "author": {
      "login": "jiangxb1987"
    },
    "body": "Shall we mark this code block synchronized ?",
    "commit": "5cdbab3659edd219bff462ce4d004724cff68e10",
    "createdAt": "2019-07-30T23:25:21Z",
    "diffHunk": "@@ -717,33 +720,36 @@ private[spark] class PythonBroadcast(@transient var path: String) extends Serial\n   }\n \n   /**\n-   * Write data into disk, using randomly generated name.\n+   * Write data into disk and map it to a broadcast block.\n    */\n-  private def readObject(in: ObjectInputStream): Unit = Utils.tryOrIOException {\n-    val dir = new File(Utils.getLocalDir(SparkEnv.get.conf))\n-    val file = File.createTempFile(\"broadcast\", \"\", dir)\n-    path = file.getAbsolutePath\n-    val out = new FileOutputStream(file)\n-    Utils.tryWithSafeFinally {\n-      Utils.copyStream(in, out)\n-    } {\n-      out.close()\n-    }\n-  }\n-\n-  /**\n-   * Delete the file once the object is GCed.\n-   */\n-  override def finalize() {\n-    if (!path.isEmpty) {\n-      val file = new File(path)\n-      if (file.exists()) {\n-        if (!file.delete()) {\n-          logWarning(s\"Error deleting ${file.getPath}\")\n+  private def readObject(in: ObjectInputStream): Unit = {\n+    broadcastId = in.readLong()\n+    val blockId = BroadcastBlockId(broadcastId, \"python\")\n+    val blockManager = SparkEnv.get.blockManager\n+    val diskBlockManager = blockManager.diskBlockManager\n+    if (!diskBlockManager.containsBlock(blockId)) {\n+      Utils.tryOrIOException {\n+        val dir = new File(Utils.getLocalDir(SparkEnv.get.conf))\n+        val file = File.createTempFile(\"broadcast\", \"\", dir)\n+        val out = new FileOutputStream(file)\n+        Utils.tryWithSafeFinally {\n+          val size = Utils.copyStream(in, out)",
    "line": 76
  }, {
    "author": {
      "login": "Ngone51"
    },
    "body": "Is it necessary ? I think we've already got the write lock on the block before we're going to modify it ?",
    "commit": "5cdbab3659edd219bff462ce4d004724cff68e10",
    "createdAt": "2019-07-31T15:15:31Z",
    "diffHunk": "@@ -717,33 +720,36 @@ private[spark] class PythonBroadcast(@transient var path: String) extends Serial\n   }\n \n   /**\n-   * Write data into disk, using randomly generated name.\n+   * Write data into disk and map it to a broadcast block.\n    */\n-  private def readObject(in: ObjectInputStream): Unit = Utils.tryOrIOException {\n-    val dir = new File(Utils.getLocalDir(SparkEnv.get.conf))\n-    val file = File.createTempFile(\"broadcast\", \"\", dir)\n-    path = file.getAbsolutePath\n-    val out = new FileOutputStream(file)\n-    Utils.tryWithSafeFinally {\n-      Utils.copyStream(in, out)\n-    } {\n-      out.close()\n-    }\n-  }\n-\n-  /**\n-   * Delete the file once the object is GCed.\n-   */\n-  override def finalize() {\n-    if (!path.isEmpty) {\n-      val file = new File(path)\n-      if (file.exists()) {\n-        if (!file.delete()) {\n-          logWarning(s\"Error deleting ${file.getPath}\")\n+  private def readObject(in: ObjectInputStream): Unit = {\n+    broadcastId = in.readLong()\n+    val blockId = BroadcastBlockId(broadcastId, \"python\")\n+    val blockManager = SparkEnv.get.blockManager\n+    val diskBlockManager = blockManager.diskBlockManager\n+    if (!diskBlockManager.containsBlock(blockId)) {\n+      Utils.tryOrIOException {\n+        val dir = new File(Utils.getLocalDir(SparkEnv.get.conf))\n+        val file = File.createTempFile(\"broadcast\", \"\", dir)\n+        val out = new FileOutputStream(file)\n+        Utils.tryWithSafeFinally {\n+          val size = Utils.copyStream(in, out)",
    "line": 76
  }],
  "prId": 25262
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Can we revive this newline when you address comments?",
    "commit": "5cdbab3659edd219bff462ce4d004724cff68e10",
    "createdAt": "2019-07-31T00:54:00Z",
    "diffHunk": "@@ -39,9 +40,9 @@ import org.apache.spark.internal.config.BUFFER_SIZE\n import org.apache.spark.network.util.JavaUtils\n import org.apache.spark.rdd.RDD\n import org.apache.spark.security.{SocketAuthHelper, SocketAuthServer, SocketFuncServer}\n+import org.apache.spark.storage.{BroadcastBlockId, StorageLevel}\n import org.apache.spark.util._\n \n-"
  }],
  "prId": 25262
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Ah I missed this part. I now I get why it was specific to Python's ..",
    "commit": "5cdbab3659edd219bff462ce4d004724cff68e10",
    "createdAt": "2019-07-31T01:12:53Z",
    "diffHunk": "@@ -717,33 +720,36 @@ private[spark] class PythonBroadcast(@transient var path: String) extends Serial\n   }\n \n   /**\n-   * Write data into disk, using randomly generated name.\n+   * Write data into disk and map it to a broadcast block.\n    */\n-  private def readObject(in: ObjectInputStream): Unit = Utils.tryOrIOException {\n-    val dir = new File(Utils.getLocalDir(SparkEnv.get.conf))\n-    val file = File.createTempFile(\"broadcast\", \"\", dir)\n-    path = file.getAbsolutePath\n-    val out = new FileOutputStream(file)\n-    Utils.tryWithSafeFinally {\n-      Utils.copyStream(in, out)\n-    } {\n-      out.close()\n-    }\n-  }\n-\n-  /**\n-   * Delete the file once the object is GCed.\n-   */\n-  override def finalize() {\n-    if (!path.isEmpty) {\n-      val file = new File(path)\n-      if (file.exists()) {\n-        if (!file.delete()) {",
    "line": 63
  }],
  "prId": 25262
}]