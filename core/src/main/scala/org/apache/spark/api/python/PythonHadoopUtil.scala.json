[{
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "I'm wondering if this doesn't break anything. Did you run the UT locally?",
    "commit": "612dadbf25a3f273a38bc68f7bc35832dc0a6744",
    "createdAt": "2019-07-03T04:01:16Z",
    "diffHunk": "@@ -156,7 +156,7 @@ private[python] object PythonHadoopUtil {\n    * Convert a [[java.util.Map]] of properties to a [[org.apache.hadoop.conf.Configuration]]\n    */\n   def mapToConf(map: java.util.Map[String, String]): Configuration = {\n-    val conf = new Configuration()\n+    val conf = new Configuration(false)",
    "line": 5
  }, {
    "author": {
      "login": "advancedxy"
    },
    "body": "Internally this is only called in PythonRDD and I have replaced all the invocations with merged SparkContext's hadoop conf. So it shouldn't break things in spark side. I ran the UTs of Scala side, haven't run python unit tests though.",
    "commit": "612dadbf25a3f273a38bc68f7bc35832dc0a6744",
    "createdAt": "2019-07-03T05:28:43Z",
    "diffHunk": "@@ -156,7 +156,7 @@ private[python] object PythonHadoopUtil {\n    * Convert a [[java.util.Map]] of properties to a [[org.apache.hadoop.conf.Configuration]]\n    */\n   def mapToConf(map: java.util.Map[String, String]): Configuration = {\n-    val conf = new Configuration()\n+    val conf = new Configuration(false)",
    "line": 5
  }],
  "prId": 25002
}]