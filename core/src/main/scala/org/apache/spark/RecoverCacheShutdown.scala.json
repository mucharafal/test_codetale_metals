[{
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "The name of this class is very cryptic. After a lot of mental gymnastics it sounds like an action, which is a bad name for a class.\r\n\r\nPerhaps `CacheRecoveryManager`?\r\n\r\nAlso seems like the constructor should be private too? (`class RecoverCacheShutdown private (...`)",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-09-13T22:19:20Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class RecoverCacheShutdown("
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "I like that better, renamed.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-09-14T20:34:13Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class RecoverCacheShutdown("
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "I use the constructor in the unit tests.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-09-15T15:01:36Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class RecoverCacheShutdown("
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Could you import the types instead? Also the code after the `=>` would look better in the next line, given that's how the other cases do it.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-09-13T22:26:10Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class RecoverCacheShutdown(\n+    state: RecoverCacheShutdownState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext = ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, NoMoreBlocks) => state.killExecutor(executorId)\n+    case (executorId, NotEnoughMemory) => state.killExecutor(executorId)\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case scala.util.Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case scala.util.Success(false) => checkForReplicableBlocks(Seq(execId))"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "There was an issue with scala.util.Success getting shadowed by the success object in TaskEndReason.scala, but that's fixed.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-09-14T20:37:14Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class RecoverCacheShutdown(\n+    state: RecoverCacheShutdownState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext = ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, NoMoreBlocks) => state.killExecutor(executorId)\n+    case (executorId, NotEnoughMemory) => state.killExecutor(executorId)\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case scala.util.Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case scala.util.Success(false) => checkForReplicableBlocks(Seq(execId))"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Please add config constants.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-09-13T22:26:58Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class RecoverCacheShutdown(\n+    state: RecoverCacheShutdownState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext = ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, NoMoreBlocks) => state.killExecutor(executorId)\n+    case (executorId, NotEnoughMemory) => state.killExecutor(executorId)\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case scala.util.Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case scala.util.Success(false) => checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object RecoverCacheShutdown {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): RecoverCacheShutdown = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new RecoverCacheShutdownState(bmme, eam, conf)\n+    new RecoverCacheShutdown(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class RecoverCacheShutdownState(\n+   blockManagerMasterEndpoint: RpcEndpointRef,\n+   executorAllocationManager: ExecutorAllocationManager,\n+   conf: SparkConf\n+ ) extends Logging {\n+\n+  type ExecMap[T] = mutable.Map[String, T]\n+\n+  private val forceKillAfterS =\n+    conf.getTimeAsSeconds(\"spark.dynamicAllocation.recoverCachedData.timeout\", \"120s\")"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "Fixed.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-09-14T20:41:05Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class RecoverCacheShutdown(\n+    state: RecoverCacheShutdownState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext = ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, NoMoreBlocks) => state.killExecutor(executorId)\n+    case (executorId, NotEnoughMemory) => state.killExecutor(executorId)\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case scala.util.Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case scala.util.Success(false) => checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object RecoverCacheShutdown {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): RecoverCacheShutdown = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new RecoverCacheShutdownState(bmme, eam, conf)\n+    new RecoverCacheShutdown(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class RecoverCacheShutdownState(\n+   blockManagerMasterEndpoint: RpcEndpointRef,\n+   executorAllocationManager: ExecutorAllocationManager,\n+   conf: SparkConf\n+ ) extends Logging {\n+\n+  type ExecMap[T] = mutable.Map[String, T]\n+\n+  private val forceKillAfterS =\n+    conf.getTimeAsSeconds(\"spark.dynamicAllocation.recoverCachedData.timeout\", \"120s\")"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`Map[String, blah]` is actually more readable than `ExecMap[blah]`, since nobody really knows what an `ExecMap` is without reading the rest of the code. In fact you can just avoid any of this by doing:\r\n\r\n    private val blocksToSave = new mutable.HashMap[String, mutable.PriorityQueue[RDDBlockId]]()\r\n\r\nWhich is perfectly fine.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-09-13T22:27:52Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class RecoverCacheShutdown(\n+    state: RecoverCacheShutdownState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext = ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, NoMoreBlocks) => state.killExecutor(executorId)\n+    case (executorId, NotEnoughMemory) => state.killExecutor(executorId)\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case scala.util.Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case scala.util.Success(false) => checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object RecoverCacheShutdown {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): RecoverCacheShutdown = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new RecoverCacheShutdownState(bmme, eam, conf)\n+    new RecoverCacheShutdown(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class RecoverCacheShutdownState(\n+   blockManagerMasterEndpoint: RpcEndpointRef,\n+   executorAllocationManager: ExecutorAllocationManager,\n+   conf: SparkConf\n+ ) extends Logging {\n+\n+  type ExecMap[T] = mutable.Map[String, T]"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "That is much better.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-09-14T20:40:56Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class RecoverCacheShutdown(\n+    state: RecoverCacheShutdownState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext = ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, NoMoreBlocks) => state.killExecutor(executorId)\n+    case (executorId, NotEnoughMemory) => state.killExecutor(executorId)\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case scala.util.Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case scala.util.Success(false) => checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object RecoverCacheShutdown {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): RecoverCacheShutdown = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new RecoverCacheShutdownState(bmme, eam, conf)\n+    new RecoverCacheShutdown(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class RecoverCacheShutdownState(\n+   blockManagerMasterEndpoint: RpcEndpointRef,\n+   executorAllocationManager: ExecutorAllocationManager,\n+   conf: SparkConf\n+ ) extends Logging {\n+\n+  type ExecMap[T] = mutable.Map[String, T]"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: we generally add a blank line before the parameter list.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-09-13T22:30:28Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class RecoverCacheShutdown(\n+    state: RecoverCacheShutdownState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext = ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, NoMoreBlocks) => state.killExecutor(executorId)\n+    case (executorId, NotEnoughMemory) => state.killExecutor(executorId)\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case scala.util.Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case scala.util.Success(false) => checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object RecoverCacheShutdown {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): RecoverCacheShutdown = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new RecoverCacheShutdownState(bmme, eam, conf)\n+    new RecoverCacheShutdown(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "fixed.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-09-14T20:37:24Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class RecoverCacheShutdown(\n+    state: RecoverCacheShutdownState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext = ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, NoMoreBlocks) => state.killExecutor(executorId)\n+    case (executorId, NotEnoughMemory) => state.killExecutor(executorId)\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case scala.util.Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case scala.util.Success(false) => checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object RecoverCacheShutdown {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): RecoverCacheShutdown = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new RecoverCacheShutdownState(bmme, eam, conf)\n+    new RecoverCacheShutdown(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: this is not how we do multi-line arg lists.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-09-13T22:33:37Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class RecoverCacheShutdown(\n+    state: RecoverCacheShutdownState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext = ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, NoMoreBlocks) => state.killExecutor(executorId)\n+    case (executorId, NotEnoughMemory) => state.killExecutor(executorId)\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case scala.util.Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case scala.util.Success(false) => checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object RecoverCacheShutdown {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): RecoverCacheShutdown = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new RecoverCacheShutdownState(bmme, eam, conf)\n+    new RecoverCacheShutdown(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class RecoverCacheShutdownState(\n+   blockManagerMasterEndpoint: RpcEndpointRef,\n+   executorAllocationManager: ExecutorAllocationManager,\n+   conf: SparkConf\n+ ) extends Logging {\n+\n+  type ExecMap[T] = mutable.Map[String, T]\n+\n+  private val forceKillAfterS =\n+    conf.getTimeAsSeconds(\"spark.dynamicAllocation.recoverCachedData.timeout\", \"120s\")\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"recover-cache-shutdown-timers\")\n+\n+  private val blocksToSave: ExecMap[mutable.PriorityQueue[RDDBlockId]] = new mutable.HashMap\n+  private val savedBlocks: ExecMap[mutable.HashSet[RDDBlockId]] = new mutable.HashMap\n+  private val killTimers: ExecMap[ScheduledFuture[_]] = new mutable.HashMap\n+\n+  /**\n+   * Query Block Manager Master for cached blocks.\n+   * @param execIds the executors to query\n+   * @return a map of executorId to its replication state.\n+   */\n+  def getBlocks(execIds: Seq[String]): Map[String, ExecutorReplicationState] = synchronized {\n+    logDebug(s\"Get all RDD blocks for executors: ${execIds.mkString(\", \")}.\")\n+    execIds.map { id =>\n+      if (isThereEnoughMemory(id)) {\n+        updateBlockState(id)\n+        val blocksRemain = blocksToSave.get(id).exists(_.nonEmpty)\n+        id -> (if (blocksRemain) HasCachedBlocks else NoMoreBlocks)\n+      } else {\n+        id -> NotEnoughMemory\n+      }\n+    }(collection.breakOut)\n+  }\n+\n+  /**\n+   *  Is there enough memory on the cluster to replicate the cached data on execId before we delete\n+   *  it. Take into account all the blocks that are currently on track to be deleted.\n+   * @param execId the id of the executor we want to delete.\n+   * @return true if there is room\n+   */\n+  private def isThereEnoughMemory(execId: String): Boolean = {\n+    val currentMemStatus = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+    val remaining = currentMemStatus - execId -- blocksToSave.keys\n+\n+    val thisExecBytes = if (!blocksToSave.contains(execId) && currentMemStatus.contains(execId)) {\n+      currentMemStatus(execId)._1 - currentMemStatus(execId)._2\n+    } else {\n+      0\n+    }\n+\n+    remaining.nonEmpty && everyExecutorHasEnoughMemory(remaining, thisExecBytes)\n+  }\n+\n+  private def everyExecutorHasEnoughMemory(execMem: Map[String, (Long, Long)], bytes: Long) = {\n+    val blocksInQueueBytes = blockManagerMasterEndpoint.askSync[Long](GetSizeOfBlocks(blockList))\n+    val bytesPerExec = (blocksInQueueBytes + bytes) / execMem.size.toFloat\n+    execMem.forall { case (_, (_, remaining)) => remaining - bytesPerExec >= 0 }\n+  }\n+\n+  private def blockList = for {\n+    (id, queue) <- blocksToSave.toSeq\n+    block <- queue\n+  } yield (id, block)\n+\n+  private def updateBlockState(execId: String): Unit = {\n+    val blocks: mutable.Set[RDDBlockId] = blockManagerMasterEndpoint\n+      .askSync[collection.Set[BlockId]](GetCachedBlocks(execId))\n+      .flatMap(_.asRDDId)(collection.breakOut)\n+\n+    blocks --= savedBlocks.getOrElse(execId, new mutable.HashSet)\n+    blocksToSave(execId) = mutable.PriorityQueue[RDDBlockId](blocks.toSeq: _*)(Ordering.by(_.rddId))\n+    killTimers.getOrElseUpdate(execId, scheduleKill(new KillRunner(execId)))\n+  }\n+\n+  class KillRunner(execId: String) extends Runnable { def run(): Unit = killExecutor(execId) }\n+  private def scheduleKill(k: KillRunner) = scheduler.schedule(k, forceKillAfterS, TimeUnit.SECONDS)\n+\n+  /**\n+   * Ask block manager master to replicate one cached block.\n+   * @param execId the executor to replicate a block from\n+   * @return false if there are no more blocks or if there is an issue\n+   */\n+  def replicateFirstBlock(execId: String): (Future[Boolean], Option[RDDBlockId]) = synchronized {\n+    logDebug(s\"Replicate block on executor $execId.\")\n+    blocksToSave.get(execId) match {\n+      case Some(p) if p.nonEmpty => doReplication(execId, p.dequeue())\n+      case _ => (Future.successful(false), None)\n+    }\n+  }\n+\n+  private def doReplication(execId: String,\n+                            blockId: RDDBlockId): (Future[Boolean], Option[RDDBlockId]) = {"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "Fixed.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-09-14T20:41:16Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class RecoverCacheShutdown(\n+    state: RecoverCacheShutdownState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext = ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, NoMoreBlocks) => state.killExecutor(executorId)\n+    case (executorId, NotEnoughMemory) => state.killExecutor(executorId)\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case scala.util.Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case scala.util.Success(false) => checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object RecoverCacheShutdown {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): RecoverCacheShutdown = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new RecoverCacheShutdownState(bmme, eam, conf)\n+    new RecoverCacheShutdown(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class RecoverCacheShutdownState(\n+   blockManagerMasterEndpoint: RpcEndpointRef,\n+   executorAllocationManager: ExecutorAllocationManager,\n+   conf: SparkConf\n+ ) extends Logging {\n+\n+  type ExecMap[T] = mutable.Map[String, T]\n+\n+  private val forceKillAfterS =\n+    conf.getTimeAsSeconds(\"spark.dynamicAllocation.recoverCachedData.timeout\", \"120s\")\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"recover-cache-shutdown-timers\")\n+\n+  private val blocksToSave: ExecMap[mutable.PriorityQueue[RDDBlockId]] = new mutable.HashMap\n+  private val savedBlocks: ExecMap[mutable.HashSet[RDDBlockId]] = new mutable.HashMap\n+  private val killTimers: ExecMap[ScheduledFuture[_]] = new mutable.HashMap\n+\n+  /**\n+   * Query Block Manager Master for cached blocks.\n+   * @param execIds the executors to query\n+   * @return a map of executorId to its replication state.\n+   */\n+  def getBlocks(execIds: Seq[String]): Map[String, ExecutorReplicationState] = synchronized {\n+    logDebug(s\"Get all RDD blocks for executors: ${execIds.mkString(\", \")}.\")\n+    execIds.map { id =>\n+      if (isThereEnoughMemory(id)) {\n+        updateBlockState(id)\n+        val blocksRemain = blocksToSave.get(id).exists(_.nonEmpty)\n+        id -> (if (blocksRemain) HasCachedBlocks else NoMoreBlocks)\n+      } else {\n+        id -> NotEnoughMemory\n+      }\n+    }(collection.breakOut)\n+  }\n+\n+  /**\n+   *  Is there enough memory on the cluster to replicate the cached data on execId before we delete\n+   *  it. Take into account all the blocks that are currently on track to be deleted.\n+   * @param execId the id of the executor we want to delete.\n+   * @return true if there is room\n+   */\n+  private def isThereEnoughMemory(execId: String): Boolean = {\n+    val currentMemStatus = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+    val remaining = currentMemStatus - execId -- blocksToSave.keys\n+\n+    val thisExecBytes = if (!blocksToSave.contains(execId) && currentMemStatus.contains(execId)) {\n+      currentMemStatus(execId)._1 - currentMemStatus(execId)._2\n+    } else {\n+      0\n+    }\n+\n+    remaining.nonEmpty && everyExecutorHasEnoughMemory(remaining, thisExecBytes)\n+  }\n+\n+  private def everyExecutorHasEnoughMemory(execMem: Map[String, (Long, Long)], bytes: Long) = {\n+    val blocksInQueueBytes = blockManagerMasterEndpoint.askSync[Long](GetSizeOfBlocks(blockList))\n+    val bytesPerExec = (blocksInQueueBytes + bytes) / execMem.size.toFloat\n+    execMem.forall { case (_, (_, remaining)) => remaining - bytesPerExec >= 0 }\n+  }\n+\n+  private def blockList = for {\n+    (id, queue) <- blocksToSave.toSeq\n+    block <- queue\n+  } yield (id, block)\n+\n+  private def updateBlockState(execId: String): Unit = {\n+    val blocks: mutable.Set[RDDBlockId] = blockManagerMasterEndpoint\n+      .askSync[collection.Set[BlockId]](GetCachedBlocks(execId))\n+      .flatMap(_.asRDDId)(collection.breakOut)\n+\n+    blocks --= savedBlocks.getOrElse(execId, new mutable.HashSet)\n+    blocksToSave(execId) = mutable.PriorityQueue[RDDBlockId](blocks.toSeq: _*)(Ordering.by(_.rddId))\n+    killTimers.getOrElseUpdate(execId, scheduleKill(new KillRunner(execId)))\n+  }\n+\n+  class KillRunner(execId: String) extends Runnable { def run(): Unit = killExecutor(execId) }\n+  private def scheduleKill(k: KillRunner) = scheduler.schedule(k, forceKillAfterS, TimeUnit.SECONDS)\n+\n+  /**\n+   * Ask block manager master to replicate one cached block.\n+   * @param execId the executor to replicate a block from\n+   * @return false if there are no more blocks or if there is an issue\n+   */\n+  def replicateFirstBlock(execId: String): (Future[Boolean], Option[RDDBlockId]) = synchronized {\n+    logDebug(s\"Replicate block on executor $execId.\")\n+    blocksToSave.get(execId) match {\n+      case Some(p) if p.nonEmpty => doReplication(execId, p.dequeue())\n+      case _ => (Future.successful(false), None)\n+    }\n+  }\n+\n+  private def doReplication(execId: String,\n+                            blockId: RDDBlockId): (Future[Boolean], Option[RDDBlockId]) = {"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "not 100% sure scalac will accept it, but maybe:\r\n\r\n    case (executorId, NoMoreBlocks | NotEnoughMemory) =>\r\n\r\nOr you could invert things.\r\n\r\n    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\r\n    case (executorId, _) = kill()\r\n",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-09-13T22:35:27Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class RecoverCacheShutdown(\n+    state: RecoverCacheShutdownState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext = ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, NoMoreBlocks) => state.killExecutor(executorId)"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "That does work.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-09-14T20:34:48Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class RecoverCacheShutdown(\n+    state: RecoverCacheShutdownState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext = ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, NoMoreBlocks) => state.killExecutor(executorId)"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "If I understand things correctly, shouldn't you be considering all executors being killed in this call? Otherwise won't you be counting the memory available at executors that soon will be killed?\r\n\r\ne.g. if you're killing 5 executors, when you call this for the first one, it seems to me like that would still consider the memory of the other 4 as available.\r\n\r\nFeels like you should calculate the available memory before iterating over the executors; that could help with fixing the above issue, and would also be cheaper (single BM call vs. one per executor being killed).",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-09-13T22:39:43Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class RecoverCacheShutdown(\n+    state: RecoverCacheShutdownState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext = ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, NoMoreBlocks) => state.killExecutor(executorId)\n+    case (executorId, NotEnoughMemory) => state.killExecutor(executorId)\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case scala.util.Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case scala.util.Success(false) => checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object RecoverCacheShutdown {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): RecoverCacheShutdown = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new RecoverCacheShutdownState(bmme, eam, conf)\n+    new RecoverCacheShutdown(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class RecoverCacheShutdownState(\n+   blockManagerMasterEndpoint: RpcEndpointRef,\n+   executorAllocationManager: ExecutorAllocationManager,\n+   conf: SparkConf\n+ ) extends Logging {\n+\n+  type ExecMap[T] = mutable.Map[String, T]\n+\n+  private val forceKillAfterS =\n+    conf.getTimeAsSeconds(\"spark.dynamicAllocation.recoverCachedData.timeout\", \"120s\")\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"recover-cache-shutdown-timers\")\n+\n+  private val blocksToSave: ExecMap[mutable.PriorityQueue[RDDBlockId]] = new mutable.HashMap\n+  private val savedBlocks: ExecMap[mutable.HashSet[RDDBlockId]] = new mutable.HashMap\n+  private val killTimers: ExecMap[ScheduledFuture[_]] = new mutable.HashMap\n+\n+  /**\n+   * Query Block Manager Master for cached blocks.\n+   * @param execIds the executors to query\n+   * @return a map of executorId to its replication state.\n+   */\n+  def getBlocks(execIds: Seq[String]): Map[String, ExecutorReplicationState] = synchronized {\n+    logDebug(s\"Get all RDD blocks for executors: ${execIds.mkString(\", \")}.\")\n+    execIds.map { id =>\n+      if (isThereEnoughMemory(id)) {"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "I rewrote this code and simplified it. I don't call GetMemoryStatus for every executor anymore. \r\n",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-09-25T17:23:46Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class RecoverCacheShutdown(\n+    state: RecoverCacheShutdownState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext = ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, NoMoreBlocks) => state.killExecutor(executorId)\n+    case (executorId, NotEnoughMemory) => state.killExecutor(executorId)\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case scala.util.Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case scala.util.Success(false) => checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object RecoverCacheShutdown {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): RecoverCacheShutdown = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new RecoverCacheShutdownState(bmme, eam, conf)\n+    new RecoverCacheShutdown(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class RecoverCacheShutdownState(\n+   blockManagerMasterEndpoint: RpcEndpointRef,\n+   executorAllocationManager: ExecutorAllocationManager,\n+   conf: SparkConf\n+ ) extends Logging {\n+\n+  type ExecMap[T] = mutable.Map[String, T]\n+\n+  private val forceKillAfterS =\n+    conf.getTimeAsSeconds(\"spark.dynamicAllocation.recoverCachedData.timeout\", \"120s\")\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"recover-cache-shutdown-timers\")\n+\n+  private val blocksToSave: ExecMap[mutable.PriorityQueue[RDDBlockId]] = new mutable.HashMap\n+  private val savedBlocks: ExecMap[mutable.HashSet[RDDBlockId]] = new mutable.HashMap\n+  private val killTimers: ExecMap[ScheduledFuture[_]] = new mutable.HashMap\n+\n+  /**\n+   * Query Block Manager Master for cached blocks.\n+   * @param execIds the executors to query\n+   * @return a map of executorId to its replication state.\n+   */\n+  def getBlocks(execIds: Seq[String]): Map[String, ExecutorReplicationState] = synchronized {\n+    logDebug(s\"Get all RDD blocks for executors: ${execIds.mkString(\", \")}.\")\n+    execIds.map { id =>\n+      if (isThereEnoughMemory(id)) {"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`killTimers.remove(execId).foreach { ... }`",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-09-13T22:43:46Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class RecoverCacheShutdown(\n+    state: RecoverCacheShutdownState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext = ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, NoMoreBlocks) => state.killExecutor(executorId)\n+    case (executorId, NotEnoughMemory) => state.killExecutor(executorId)\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case scala.util.Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case scala.util.Success(false) => checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object RecoverCacheShutdown {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): RecoverCacheShutdown = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new RecoverCacheShutdownState(bmme, eam, conf)\n+    new RecoverCacheShutdown(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class RecoverCacheShutdownState(\n+   blockManagerMasterEndpoint: RpcEndpointRef,\n+   executorAllocationManager: ExecutorAllocationManager,\n+   conf: SparkConf\n+ ) extends Logging {\n+\n+  type ExecMap[T] = mutable.Map[String, T]\n+\n+  private val forceKillAfterS =\n+    conf.getTimeAsSeconds(\"spark.dynamicAllocation.recoverCachedData.timeout\", \"120s\")\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"recover-cache-shutdown-timers\")\n+\n+  private val blocksToSave: ExecMap[mutable.PriorityQueue[RDDBlockId]] = new mutable.HashMap\n+  private val savedBlocks: ExecMap[mutable.HashSet[RDDBlockId]] = new mutable.HashMap\n+  private val killTimers: ExecMap[ScheduledFuture[_]] = new mutable.HashMap\n+\n+  /**\n+   * Query Block Manager Master for cached blocks.\n+   * @param execIds the executors to query\n+   * @return a map of executorId to its replication state.\n+   */\n+  def getBlocks(execIds: Seq[String]): Map[String, ExecutorReplicationState] = synchronized {\n+    logDebug(s\"Get all RDD blocks for executors: ${execIds.mkString(\", \")}.\")\n+    execIds.map { id =>\n+      if (isThereEnoughMemory(id)) {\n+        updateBlockState(id)\n+        val blocksRemain = blocksToSave.get(id).exists(_.nonEmpty)\n+        id -> (if (blocksRemain) HasCachedBlocks else NoMoreBlocks)\n+      } else {\n+        id -> NotEnoughMemory\n+      }\n+    }(collection.breakOut)\n+  }\n+\n+  /**\n+   *  Is there enough memory on the cluster to replicate the cached data on execId before we delete\n+   *  it. Take into account all the blocks that are currently on track to be deleted.\n+   * @param execId the id of the executor we want to delete.\n+   * @return true if there is room\n+   */\n+  private def isThereEnoughMemory(execId: String): Boolean = {\n+    val currentMemStatus = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+    val remaining = currentMemStatus - execId -- blocksToSave.keys\n+\n+    val thisExecBytes = if (!blocksToSave.contains(execId) && currentMemStatus.contains(execId)) {\n+      currentMemStatus(execId)._1 - currentMemStatus(execId)._2\n+    } else {\n+      0\n+    }\n+\n+    remaining.nonEmpty && everyExecutorHasEnoughMemory(remaining, thisExecBytes)\n+  }\n+\n+  private def everyExecutorHasEnoughMemory(execMem: Map[String, (Long, Long)], bytes: Long) = {\n+    val blocksInQueueBytes = blockManagerMasterEndpoint.askSync[Long](GetSizeOfBlocks(blockList))\n+    val bytesPerExec = (blocksInQueueBytes + bytes) / execMem.size.toFloat\n+    execMem.forall { case (_, (_, remaining)) => remaining - bytesPerExec >= 0 }\n+  }\n+\n+  private def blockList = for {\n+    (id, queue) <- blocksToSave.toSeq\n+    block <- queue\n+  } yield (id, block)\n+\n+  private def updateBlockState(execId: String): Unit = {\n+    val blocks: mutable.Set[RDDBlockId] = blockManagerMasterEndpoint\n+      .askSync[collection.Set[BlockId]](GetCachedBlocks(execId))\n+      .flatMap(_.asRDDId)(collection.breakOut)\n+\n+    blocks --= savedBlocks.getOrElse(execId, new mutable.HashSet)\n+    blocksToSave(execId) = mutable.PriorityQueue[RDDBlockId](blocks.toSeq: _*)(Ordering.by(_.rddId))\n+    killTimers.getOrElseUpdate(execId, scheduleKill(new KillRunner(execId)))\n+  }\n+\n+  class KillRunner(execId: String) extends Runnable { def run(): Unit = killExecutor(execId) }\n+  private def scheduleKill(k: KillRunner) = scheduler.schedule(k, forceKillAfterS, TimeUnit.SECONDS)\n+\n+  /**\n+   * Ask block manager master to replicate one cached block.\n+   * @param execId the executor to replicate a block from\n+   * @return false if there are no more blocks or if there is an issue\n+   */\n+  def replicateFirstBlock(execId: String): (Future[Boolean], Option[RDDBlockId]) = synchronized {\n+    logDebug(s\"Replicate block on executor $execId.\")\n+    blocksToSave.get(execId) match {\n+      case Some(p) if p.nonEmpty => doReplication(execId, p.dequeue())\n+      case _ => (Future.successful(false), None)\n+    }\n+  }\n+\n+  private def doReplication(execId: String,\n+                            blockId: RDDBlockId): (Future[Boolean], Option[RDDBlockId]) = {\n+    val savedBlocksForExecutor = savedBlocks.getOrElseUpdate(execId, new mutable.HashSet)\n+    savedBlocksForExecutor += blockId\n+    val replicateMessage = ReplicateOneBlock(execId, blockId, blocksToSave.keys.toSeq)\n+    logTrace(s\"Started replicating block $blockId on exec $execId.\")\n+    val future = blockManagerMasterEndpoint.askSync[Future[Boolean]](replicateMessage)\n+    (future, Some(blockId))\n+  }\n+\n+  /**\n+   * Ask ExecutorAllocationManager to kill executor and clean up state\n+   * Note executorAllocationManger.killExecutors is blocking, this function should be fixed\n+   * to use a non blocking version\n+   * @param execId the executor to kill\n+   */\n+  def killExecutor(execId: String): Unit = synchronized {\n+    logDebug(s\"Send request to kill executor $execId.\")\n+    killTimers.get(execId).foreach(_.cancel(false))"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "Fixed.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-09-14T20:41:23Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class RecoverCacheShutdown(\n+    state: RecoverCacheShutdownState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext = ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, NoMoreBlocks) => state.killExecutor(executorId)\n+    case (executorId, NotEnoughMemory) => state.killExecutor(executorId)\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case scala.util.Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case scala.util.Success(false) => checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object RecoverCacheShutdown {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): RecoverCacheShutdown = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new RecoverCacheShutdownState(bmme, eam, conf)\n+    new RecoverCacheShutdown(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class RecoverCacheShutdownState(\n+   blockManagerMasterEndpoint: RpcEndpointRef,\n+   executorAllocationManager: ExecutorAllocationManager,\n+   conf: SparkConf\n+ ) extends Logging {\n+\n+  type ExecMap[T] = mutable.Map[String, T]\n+\n+  private val forceKillAfterS =\n+    conf.getTimeAsSeconds(\"spark.dynamicAllocation.recoverCachedData.timeout\", \"120s\")\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"recover-cache-shutdown-timers\")\n+\n+  private val blocksToSave: ExecMap[mutable.PriorityQueue[RDDBlockId]] = new mutable.HashMap\n+  private val savedBlocks: ExecMap[mutable.HashSet[RDDBlockId]] = new mutable.HashMap\n+  private val killTimers: ExecMap[ScheduledFuture[_]] = new mutable.HashMap\n+\n+  /**\n+   * Query Block Manager Master for cached blocks.\n+   * @param execIds the executors to query\n+   * @return a map of executorId to its replication state.\n+   */\n+  def getBlocks(execIds: Seq[String]): Map[String, ExecutorReplicationState] = synchronized {\n+    logDebug(s\"Get all RDD blocks for executors: ${execIds.mkString(\", \")}.\")\n+    execIds.map { id =>\n+      if (isThereEnoughMemory(id)) {\n+        updateBlockState(id)\n+        val blocksRemain = blocksToSave.get(id).exists(_.nonEmpty)\n+        id -> (if (blocksRemain) HasCachedBlocks else NoMoreBlocks)\n+      } else {\n+        id -> NotEnoughMemory\n+      }\n+    }(collection.breakOut)\n+  }\n+\n+  /**\n+   *  Is there enough memory on the cluster to replicate the cached data on execId before we delete\n+   *  it. Take into account all the blocks that are currently on track to be deleted.\n+   * @param execId the id of the executor we want to delete.\n+   * @return true if there is room\n+   */\n+  private def isThereEnoughMemory(execId: String): Boolean = {\n+    val currentMemStatus = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+    val remaining = currentMemStatus - execId -- blocksToSave.keys\n+\n+    val thisExecBytes = if (!blocksToSave.contains(execId) && currentMemStatus.contains(execId)) {\n+      currentMemStatus(execId)._1 - currentMemStatus(execId)._2\n+    } else {\n+      0\n+    }\n+\n+    remaining.nonEmpty && everyExecutorHasEnoughMemory(remaining, thisExecBytes)\n+  }\n+\n+  private def everyExecutorHasEnoughMemory(execMem: Map[String, (Long, Long)], bytes: Long) = {\n+    val blocksInQueueBytes = blockManagerMasterEndpoint.askSync[Long](GetSizeOfBlocks(blockList))\n+    val bytesPerExec = (blocksInQueueBytes + bytes) / execMem.size.toFloat\n+    execMem.forall { case (_, (_, remaining)) => remaining - bytesPerExec >= 0 }\n+  }\n+\n+  private def blockList = for {\n+    (id, queue) <- blocksToSave.toSeq\n+    block <- queue\n+  } yield (id, block)\n+\n+  private def updateBlockState(execId: String): Unit = {\n+    val blocks: mutable.Set[RDDBlockId] = blockManagerMasterEndpoint\n+      .askSync[collection.Set[BlockId]](GetCachedBlocks(execId))\n+      .flatMap(_.asRDDId)(collection.breakOut)\n+\n+    blocks --= savedBlocks.getOrElse(execId, new mutable.HashSet)\n+    blocksToSave(execId) = mutable.PriorityQueue[RDDBlockId](blocks.toSeq: _*)(Ordering.by(_.rddId))\n+    killTimers.getOrElseUpdate(execId, scheduleKill(new KillRunner(execId)))\n+  }\n+\n+  class KillRunner(execId: String) extends Runnable { def run(): Unit = killExecutor(execId) }\n+  private def scheduleKill(k: KillRunner) = scheduler.schedule(k, forceKillAfterS, TimeUnit.SECONDS)\n+\n+  /**\n+   * Ask block manager master to replicate one cached block.\n+   * @param execId the executor to replicate a block from\n+   * @return false if there are no more blocks or if there is an issue\n+   */\n+  def replicateFirstBlock(execId: String): (Future[Boolean], Option[RDDBlockId]) = synchronized {\n+    logDebug(s\"Replicate block on executor $execId.\")\n+    blocksToSave.get(execId) match {\n+      case Some(p) if p.nonEmpty => doReplication(execId, p.dequeue())\n+      case _ => (Future.successful(false), None)\n+    }\n+  }\n+\n+  private def doReplication(execId: String,\n+                            blockId: RDDBlockId): (Future[Boolean], Option[RDDBlockId]) = {\n+    val savedBlocksForExecutor = savedBlocks.getOrElseUpdate(execId, new mutable.HashSet)\n+    savedBlocksForExecutor += blockId\n+    val replicateMessage = ReplicateOneBlock(execId, blockId, blocksToSave.keys.toSeq)\n+    logTrace(s\"Started replicating block $blockId on exec $execId.\")\n+    val future = blockManagerMasterEndpoint.askSync[Future[Boolean]](replicateMessage)\n+    (future, Some(blockId))\n+  }\n+\n+  /**\n+   * Ask ExecutorAllocationManager to kill executor and clean up state\n+   * Note executorAllocationManger.killExecutors is blocking, this function should be fixed\n+   * to use a non blocking version\n+   * @param execId the executor to kill\n+   */\n+  def killExecutor(execId: String): Unit = synchronized {\n+    logDebug(s\"Send request to kill executor $execId.\")\n+    killTimers.get(execId).foreach(_.cancel(false))"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Does this need to be a separate class? Seems like the code could easily live inside `RecoverCacheShutdown`.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-09-13T22:53:00Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class RecoverCacheShutdown(\n+    state: RecoverCacheShutdownState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext = ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, NoMoreBlocks) => state.killExecutor(executorId)\n+    case (executorId, NotEnoughMemory) => state.killExecutor(executorId)\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case scala.util.Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case scala.util.Success(false) => checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object RecoverCacheShutdown {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): RecoverCacheShutdown = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new RecoverCacheShutdownState(bmme, eam, conf)\n+    new RecoverCacheShutdown(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class RecoverCacheShutdownState("
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "These could be the same class. I thought keeping the state mutating methods together in their own class would be a little easier to understand. ",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-09-14T20:40:45Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class RecoverCacheShutdown(\n+    state: RecoverCacheShutdownState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext = ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, NoMoreBlocks) => state.killExecutor(executorId)\n+    case (executorId, NotEnoughMemory) => state.killExecutor(executorId)\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case scala.util.Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case scala.util.Success(false) => checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object RecoverCacheShutdown {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): RecoverCacheShutdown = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new RecoverCacheShutdownState(bmme, eam, conf)\n+    new RecoverCacheShutdown(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class RecoverCacheShutdownState("
  }],
  "prId": 19041
}]