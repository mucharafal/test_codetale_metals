[{
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "should `allowedExecutorResources` and `allowedTaskResources` just be a `HashSet`?  Meaning of the value isn't clear, you only check for existence of the key.",
    "commit": "246de3c66c8c5a656b766ccd2f9bb3df12c9da0d",
    "createdAt": "2019-10-31T18:44:19Z",
    "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.resource\n+\n+import java.util.{Map => JMap}\n+import java.util.concurrent.atomic.{AtomicInteger, AtomicReference}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.immutable.HashMap\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.annotation.Evolving\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.resource.ResourceUtils.{RESOURCE_DOT, RESOURCE_PREFIX}\n+\n+/**\n+ * Resource profile to associate with an RDD. A ResourceProfile allows the user to\n+ * specify executor and task requirements for an RDD that will get applied during a\n+ * stage. This allows the user to change the resource requirements between stages.\n+ *\n+ * Only support a subset of the resources for now. The config names supported correspond to the\n+ * regular Spark configs with the prefix removed. For instance overhead memory in this api\n+ * is memoryOverhead, which is spark.executor.memoryOverhead with spark.executor removed.\n+ * Resources like GPUs are resource.gpu (spark configs spark.executor.resource.gpu.*)\n+ *\n+ * Executor:\n+ *   memory - heap\n+ *   memoryOverhead\n+ *   pyspark.memory\n+ *   cores\n+ *   resource.[resourceName] - GPU, FPGA, etc\n+ *\n+ * Task requirements:\n+ *   cpus\n+ *   resource.[resourceName] - GPU, FPGA, etc\n+ *\n+ * This class is private now for initial development, once we have the feature in place\n+ * this will become public.\n+ */\n+@Evolving\n+private[spark] class ResourceProfile() extends Serializable {\n+\n+  private val _id = ResourceProfile.getNextProfileId\n+  private val _taskResources = new mutable.HashMap[String, TaskResourceRequest]()\n+  private val _executorResources = new mutable.HashMap[String, ExecutorResourceRequest]()\n+\n+  private val allowedExecutorResources = HashMap[String, Boolean](\n+    (ResourceProfile.MEMORY -> true),\n+    (ResourceProfile.OVERHEAD_MEM -> true),\n+    (ResourceProfile.PYSPARK_MEM -> true),\n+    (ResourceProfile.CORES -> true))\n+\n+  private val allowedTaskResources = HashMap[String, Boolean](("
  }, {
    "author": {
      "login": "tgravescs"
    },
    "body": "yes it should be, I'll update",
    "commit": "246de3c66c8c5a656b766ccd2f9bb3df12c9da0d",
    "createdAt": "2019-10-31T19:22:36Z",
    "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.resource\n+\n+import java.util.{Map => JMap}\n+import java.util.concurrent.atomic.{AtomicInteger, AtomicReference}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.immutable.HashMap\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.annotation.Evolving\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.resource.ResourceUtils.{RESOURCE_DOT, RESOURCE_PREFIX}\n+\n+/**\n+ * Resource profile to associate with an RDD. A ResourceProfile allows the user to\n+ * specify executor and task requirements for an RDD that will get applied during a\n+ * stage. This allows the user to change the resource requirements between stages.\n+ *\n+ * Only support a subset of the resources for now. The config names supported correspond to the\n+ * regular Spark configs with the prefix removed. For instance overhead memory in this api\n+ * is memoryOverhead, which is spark.executor.memoryOverhead with spark.executor removed.\n+ * Resources like GPUs are resource.gpu (spark configs spark.executor.resource.gpu.*)\n+ *\n+ * Executor:\n+ *   memory - heap\n+ *   memoryOverhead\n+ *   pyspark.memory\n+ *   cores\n+ *   resource.[resourceName] - GPU, FPGA, etc\n+ *\n+ * Task requirements:\n+ *   cpus\n+ *   resource.[resourceName] - GPU, FPGA, etc\n+ *\n+ * This class is private now for initial development, once we have the feature in place\n+ * this will become public.\n+ */\n+@Evolving\n+private[spark] class ResourceProfile() extends Serializable {\n+\n+  private val _id = ResourceProfile.getNextProfileId\n+  private val _taskResources = new mutable.HashMap[String, TaskResourceRequest]()\n+  private val _executorResources = new mutable.HashMap[String, ExecutorResourceRequest]()\n+\n+  private val allowedExecutorResources = HashMap[String, Boolean](\n+    (ResourceProfile.MEMORY -> true),\n+    (ResourceProfile.OVERHEAD_MEM -> true),\n+    (ResourceProfile.PYSPARK_MEM -> true),\n+    (ResourceProfile.CORES -> true))\n+\n+  private val allowedTaskResources = HashMap[String, Boolean](("
  }],
  "prId": 26284
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "can you do these checks in the constructors of TaskResourceRequest and ExecutorResourceRequest instead?\r\n\r\nalso, is there any advantage to having these be strings instead of Enums?  If we really want to enforce its a closed universe here, then enum seems better?",
    "commit": "246de3c66c8c5a656b766ccd2f9bb3df12c9da0d",
    "createdAt": "2019-11-07T18:11:02Z",
    "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.resource\n+\n+import java.util.{Map => JMap}\n+import java.util.concurrent.atomic.{AtomicInteger, AtomicReference}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.immutable.HashSet\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.annotation.Evolving\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.resource.ResourceUtils.{RESOURCE_DOT, RESOURCE_PREFIX}\n+\n+/**\n+ * Resource profile to associate with an RDD. A ResourceProfile allows the user to\n+ * specify executor and task requirements for an RDD that will get applied during a\n+ * stage. This allows the user to change the resource requirements between stages.\n+ *\n+ * Only supports a subset of the resources for now. The config names supported correspond to the\n+ * regular Spark configs with the prefix removed. For instance overhead memory in this api\n+ * is memoryOverhead, which is spark.executor.memoryOverhead with spark.executor removed.\n+ * Resources like GPUs are resource.gpu (spark configs spark.executor.resource.gpu.*)\n+ *\n+ * Executor:\n+ *   memory - heap\n+ *   memoryOverhead\n+ *   pyspark.memory\n+ *   cores\n+ *   resource.[resourceName] - GPU, FPGA, etc\n+ *\n+ * Task requirements:\n+ *   cpus\n+ *   resource.[resourceName] - GPU, FPGA, etc\n+ *\n+ * This class is private now for initial development, once we have the feature in place\n+ * this will become public.\n+ */\n+@Evolving\n+private[spark] class ResourceProfile() extends Serializable {\n+\n+  private val _id = ResourceProfile.getNextProfileId\n+  private val _taskResources = new mutable.HashMap[String, TaskResourceRequest]()\n+  private val _executorResources = new mutable.HashMap[String, ExecutorResourceRequest]()\n+\n+  private val allowedExecutorResources = HashSet[String](\n+    ResourceProfile.MEMORY,\n+    ResourceProfile.OVERHEAD_MEM,\n+    ResourceProfile.PYSPARK_MEM,\n+    ResourceProfile.CORES)\n+\n+  private val allowedTaskResources = HashSet[String](ResourceProfile.CPUS)\n+\n+  def id: Int = _id\n+\n+  def taskResources: Map[String, TaskResourceRequest] = _taskResources.toMap\n+\n+  def executorResources: Map[String, ExecutorResourceRequest] = _executorResources.toMap\n+\n+  /**\n+   * (Java-specific) gets a Java Map of resources to TaskResourceRequest\n+   */\n+  def taskResourcesJMap: JMap[String, TaskResourceRequest] = _taskResources.asJava\n+\n+  /**\n+   * (Java-specific) gets a Java Map of resources to ExecutorResourceRequest\n+   */\n+  def executorResourcesJMap: JMap[String, ExecutorResourceRequest] = _executorResources.asJava\n+\n+\n+  def reset(): Unit = {\n+    _taskResources.clear()\n+    _executorResources.clear()\n+  }\n+\n+  def require(request: TaskResourceRequest): this.type = {\n+    val rName = request.resourceName\n+    if (allowedTaskResources.contains(rName) || rName.startsWith(RESOURCE_DOT)) {"
  }, {
    "author": {
      "login": "tgravescs"
    },
    "body": "yes, good idea, I'll move them to the constructors.\r\n\r\nFor Enums, just started with Strings based on the way the name parameter being  based on the regular spark config names.  spark.executor.memoryOverhead -> memoryOverhead, like comment below we can certainly go away from this if we want.",
    "commit": "246de3c66c8c5a656b766ccd2f9bb3df12c9da0d",
    "createdAt": "2019-11-07T18:52:08Z",
    "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.resource\n+\n+import java.util.{Map => JMap}\n+import java.util.concurrent.atomic.{AtomicInteger, AtomicReference}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.immutable.HashSet\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.annotation.Evolving\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.resource.ResourceUtils.{RESOURCE_DOT, RESOURCE_PREFIX}\n+\n+/**\n+ * Resource profile to associate with an RDD. A ResourceProfile allows the user to\n+ * specify executor and task requirements for an RDD that will get applied during a\n+ * stage. This allows the user to change the resource requirements between stages.\n+ *\n+ * Only supports a subset of the resources for now. The config names supported correspond to the\n+ * regular Spark configs with the prefix removed. For instance overhead memory in this api\n+ * is memoryOverhead, which is spark.executor.memoryOverhead with spark.executor removed.\n+ * Resources like GPUs are resource.gpu (spark configs spark.executor.resource.gpu.*)\n+ *\n+ * Executor:\n+ *   memory - heap\n+ *   memoryOverhead\n+ *   pyspark.memory\n+ *   cores\n+ *   resource.[resourceName] - GPU, FPGA, etc\n+ *\n+ * Task requirements:\n+ *   cpus\n+ *   resource.[resourceName] - GPU, FPGA, etc\n+ *\n+ * This class is private now for initial development, once we have the feature in place\n+ * this will become public.\n+ */\n+@Evolving\n+private[spark] class ResourceProfile() extends Serializable {\n+\n+  private val _id = ResourceProfile.getNextProfileId\n+  private val _taskResources = new mutable.HashMap[String, TaskResourceRequest]()\n+  private val _executorResources = new mutable.HashMap[String, ExecutorResourceRequest]()\n+\n+  private val allowedExecutorResources = HashSet[String](\n+    ResourceProfile.MEMORY,\n+    ResourceProfile.OVERHEAD_MEM,\n+    ResourceProfile.PYSPARK_MEM,\n+    ResourceProfile.CORES)\n+\n+  private val allowedTaskResources = HashSet[String](ResourceProfile.CPUS)\n+\n+  def id: Int = _id\n+\n+  def taskResources: Map[String, TaskResourceRequest] = _taskResources.toMap\n+\n+  def executorResources: Map[String, ExecutorResourceRequest] = _executorResources.toMap\n+\n+  /**\n+   * (Java-specific) gets a Java Map of resources to TaskResourceRequest\n+   */\n+  def taskResourcesJMap: JMap[String, TaskResourceRequest] = _taskResources.asJava\n+\n+  /**\n+   * (Java-specific) gets a Java Map of resources to ExecutorResourceRequest\n+   */\n+  def executorResourcesJMap: JMap[String, ExecutorResourceRequest] = _executorResources.asJava\n+\n+\n+  def reset(): Unit = {\n+    _taskResources.clear()\n+    _executorResources.clear()\n+  }\n+\n+  def require(request: TaskResourceRequest): this.type = {\n+    val rName = request.resourceName\n+    if (allowedTaskResources.contains(rName) || rName.startsWith(RESOURCE_DOT)) {"
  }],
  "prId": 26284
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "cpus and cores are really the same thing, right?  its unfortunate we already chose different names for task vs. executor in existing confs.",
    "commit": "246de3c66c8c5a656b766ccd2f9bb3df12c9da0d",
    "createdAt": "2019-11-07T18:12:10Z",
    "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.resource\n+\n+import java.util.{Map => JMap}\n+import java.util.concurrent.atomic.{AtomicInteger, AtomicReference}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.immutable.HashSet\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.annotation.Evolving\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.resource.ResourceUtils.{RESOURCE_DOT, RESOURCE_PREFIX}\n+\n+/**\n+ * Resource profile to associate with an RDD. A ResourceProfile allows the user to\n+ * specify executor and task requirements for an RDD that will get applied during a\n+ * stage. This allows the user to change the resource requirements between stages.\n+ *\n+ * Only supports a subset of the resources for now. The config names supported correspond to the\n+ * regular Spark configs with the prefix removed. For instance overhead memory in this api\n+ * is memoryOverhead, which is spark.executor.memoryOverhead with spark.executor removed.\n+ * Resources like GPUs are resource.gpu (spark configs spark.executor.resource.gpu.*)\n+ *\n+ * Executor:\n+ *   memory - heap\n+ *   memoryOverhead\n+ *   pyspark.memory\n+ *   cores\n+ *   resource.[resourceName] - GPU, FPGA, etc\n+ *\n+ * Task requirements:\n+ *   cpus\n+ *   resource.[resourceName] - GPU, FPGA, etc\n+ *\n+ * This class is private now for initial development, once we have the feature in place\n+ * this will become public.\n+ */\n+@Evolving\n+private[spark] class ResourceProfile() extends Serializable {\n+\n+  private val _id = ResourceProfile.getNextProfileId\n+  private val _taskResources = new mutable.HashMap[String, TaskResourceRequest]()\n+  private val _executorResources = new mutable.HashMap[String, ExecutorResourceRequest]()\n+\n+  private val allowedExecutorResources = HashSet[String](\n+    ResourceProfile.MEMORY,\n+    ResourceProfile.OVERHEAD_MEM,\n+    ResourceProfile.PYSPARK_MEM,\n+    ResourceProfile.CORES)\n+\n+  private val allowedTaskResources = HashSet[String](ResourceProfile.CPUS)\n+\n+  def id: Int = _id\n+\n+  def taskResources: Map[String, TaskResourceRequest] = _taskResources.toMap\n+\n+  def executorResources: Map[String, ExecutorResourceRequest] = _executorResources.toMap\n+\n+  /**\n+   * (Java-specific) gets a Java Map of resources to TaskResourceRequest\n+   */\n+  def taskResourcesJMap: JMap[String, TaskResourceRequest] = _taskResources.asJava\n+\n+  /**\n+   * (Java-specific) gets a Java Map of resources to ExecutorResourceRequest\n+   */\n+  def executorResourcesJMap: JMap[String, ExecutorResourceRequest] = _executorResources.asJava\n+\n+\n+  def reset(): Unit = {\n+    _taskResources.clear()\n+    _executorResources.clear()\n+  }\n+\n+  def require(request: TaskResourceRequest): this.type = {\n+    val rName = request.resourceName\n+    if (allowedTaskResources.contains(rName) || rName.startsWith(RESOURCE_DOT)) {\n+      _taskResources(request.resourceName) = request\n+    } else {\n+      throw new IllegalArgumentException(s\"Task resource not allowed: ${request.resourceName}\")\n+    }\n+    this\n+  }\n+\n+  def require(request: ExecutorResourceRequest): this.type = {\n+    val rName = request.resourceName\n+    if (allowedExecutorResources.contains(rName) || rName.startsWith(RESOURCE_DOT)) {\n+      _executorResources(request.resourceName) = request\n+    } else {\n+      throw new IllegalArgumentException(s\"Executor resource not allowed: ${request.resourceName}\")\n+    }\n+    this\n+  }\n+\n+  override def toString(): String = {\n+    s\"Profile: id = ${_id}, executor resources: ${_executorResources}, \" +\n+      s\"task resources: ${_taskResources}\"\n+  }\n+}\n+\n+private[spark] object ResourceProfile extends Logging {\n+  val UNKNOWN_RESOURCE_PROFILE_ID = -1\n+  val DEFAULT_RESOURCE_PROFILE_ID = 0\n+\n+  val CPUS = \"cpus\"\n+  val CORES = \"cores\"",
    "line": 87
  }, {
    "author": {
      "login": "tgravescs"
    },
    "body": "yes they are, I kept the consistency with the spark configs, I was trying to make it so the parameters to ExecutorResourceRequest and TaskResourceRequest matched the regular spark configs names minus the spark.executor or spark.task prefix.\r\n\r\nFor instance overhead memory in this api is memoryOverhead, which is spark.executor.memoryOverhead with spark.executor removed. Resources like GPUs are resource.gpu (spark configs spark.executor.resource.gpu.*)\r\n\r\nIf you don't think that matters we can change them here to be the same, thoughts?",
    "commit": "246de3c66c8c5a656b766ccd2f9bb3df12c9da0d",
    "createdAt": "2019-11-07T18:54:34Z",
    "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.resource\n+\n+import java.util.{Map => JMap}\n+import java.util.concurrent.atomic.{AtomicInteger, AtomicReference}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.immutable.HashSet\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.annotation.Evolving\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.resource.ResourceUtils.{RESOURCE_DOT, RESOURCE_PREFIX}\n+\n+/**\n+ * Resource profile to associate with an RDD. A ResourceProfile allows the user to\n+ * specify executor and task requirements for an RDD that will get applied during a\n+ * stage. This allows the user to change the resource requirements between stages.\n+ *\n+ * Only supports a subset of the resources for now. The config names supported correspond to the\n+ * regular Spark configs with the prefix removed. For instance overhead memory in this api\n+ * is memoryOverhead, which is spark.executor.memoryOverhead with spark.executor removed.\n+ * Resources like GPUs are resource.gpu (spark configs spark.executor.resource.gpu.*)\n+ *\n+ * Executor:\n+ *   memory - heap\n+ *   memoryOverhead\n+ *   pyspark.memory\n+ *   cores\n+ *   resource.[resourceName] - GPU, FPGA, etc\n+ *\n+ * Task requirements:\n+ *   cpus\n+ *   resource.[resourceName] - GPU, FPGA, etc\n+ *\n+ * This class is private now for initial development, once we have the feature in place\n+ * this will become public.\n+ */\n+@Evolving\n+private[spark] class ResourceProfile() extends Serializable {\n+\n+  private val _id = ResourceProfile.getNextProfileId\n+  private val _taskResources = new mutable.HashMap[String, TaskResourceRequest]()\n+  private val _executorResources = new mutable.HashMap[String, ExecutorResourceRequest]()\n+\n+  private val allowedExecutorResources = HashSet[String](\n+    ResourceProfile.MEMORY,\n+    ResourceProfile.OVERHEAD_MEM,\n+    ResourceProfile.PYSPARK_MEM,\n+    ResourceProfile.CORES)\n+\n+  private val allowedTaskResources = HashSet[String](ResourceProfile.CPUS)\n+\n+  def id: Int = _id\n+\n+  def taskResources: Map[String, TaskResourceRequest] = _taskResources.toMap\n+\n+  def executorResources: Map[String, ExecutorResourceRequest] = _executorResources.toMap\n+\n+  /**\n+   * (Java-specific) gets a Java Map of resources to TaskResourceRequest\n+   */\n+  def taskResourcesJMap: JMap[String, TaskResourceRequest] = _taskResources.asJava\n+\n+  /**\n+   * (Java-specific) gets a Java Map of resources to ExecutorResourceRequest\n+   */\n+  def executorResourcesJMap: JMap[String, ExecutorResourceRequest] = _executorResources.asJava\n+\n+\n+  def reset(): Unit = {\n+    _taskResources.clear()\n+    _executorResources.clear()\n+  }\n+\n+  def require(request: TaskResourceRequest): this.type = {\n+    val rName = request.resourceName\n+    if (allowedTaskResources.contains(rName) || rName.startsWith(RESOURCE_DOT)) {\n+      _taskResources(request.resourceName) = request\n+    } else {\n+      throw new IllegalArgumentException(s\"Task resource not allowed: ${request.resourceName}\")\n+    }\n+    this\n+  }\n+\n+  def require(request: ExecutorResourceRequest): this.type = {\n+    val rName = request.resourceName\n+    if (allowedExecutorResources.contains(rName) || rName.startsWith(RESOURCE_DOT)) {\n+      _executorResources(request.resourceName) = request\n+    } else {\n+      throw new IllegalArgumentException(s\"Executor resource not allowed: ${request.resourceName}\")\n+    }\n+    this\n+  }\n+\n+  override def toString(): String = {\n+    s\"Profile: id = ${_id}, executor resources: ${_executorResources}, \" +\n+      s\"task resources: ${_taskResources}\"\n+  }\n+}\n+\n+private[spark] object ResourceProfile extends Logging {\n+  val UNKNOWN_RESOURCE_PROFILE_ID = -1\n+  val DEFAULT_RESOURCE_PROFILE_ID = 0\n+\n+  val CPUS = \"cpus\"\n+  val CORES = \"cores\"",
    "line": 87
  }, {
    "author": {
      "login": "squito"
    },
    "body": "sorry that was more just a general complaint from me about what we've already done, not really related to your change.  I think keeping it the same as the confs makes sense.",
    "commit": "246de3c66c8c5a656b766ccd2f9bb3df12c9da0d",
    "createdAt": "2019-11-07T20:37:16Z",
    "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.resource\n+\n+import java.util.{Map => JMap}\n+import java.util.concurrent.atomic.{AtomicInteger, AtomicReference}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.immutable.HashSet\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.annotation.Evolving\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.resource.ResourceUtils.{RESOURCE_DOT, RESOURCE_PREFIX}\n+\n+/**\n+ * Resource profile to associate with an RDD. A ResourceProfile allows the user to\n+ * specify executor and task requirements for an RDD that will get applied during a\n+ * stage. This allows the user to change the resource requirements between stages.\n+ *\n+ * Only supports a subset of the resources for now. The config names supported correspond to the\n+ * regular Spark configs with the prefix removed. For instance overhead memory in this api\n+ * is memoryOverhead, which is spark.executor.memoryOverhead with spark.executor removed.\n+ * Resources like GPUs are resource.gpu (spark configs spark.executor.resource.gpu.*)\n+ *\n+ * Executor:\n+ *   memory - heap\n+ *   memoryOverhead\n+ *   pyspark.memory\n+ *   cores\n+ *   resource.[resourceName] - GPU, FPGA, etc\n+ *\n+ * Task requirements:\n+ *   cpus\n+ *   resource.[resourceName] - GPU, FPGA, etc\n+ *\n+ * This class is private now for initial development, once we have the feature in place\n+ * this will become public.\n+ */\n+@Evolving\n+private[spark] class ResourceProfile() extends Serializable {\n+\n+  private val _id = ResourceProfile.getNextProfileId\n+  private val _taskResources = new mutable.HashMap[String, TaskResourceRequest]()\n+  private val _executorResources = new mutable.HashMap[String, ExecutorResourceRequest]()\n+\n+  private val allowedExecutorResources = HashSet[String](\n+    ResourceProfile.MEMORY,\n+    ResourceProfile.OVERHEAD_MEM,\n+    ResourceProfile.PYSPARK_MEM,\n+    ResourceProfile.CORES)\n+\n+  private val allowedTaskResources = HashSet[String](ResourceProfile.CPUS)\n+\n+  def id: Int = _id\n+\n+  def taskResources: Map[String, TaskResourceRequest] = _taskResources.toMap\n+\n+  def executorResources: Map[String, ExecutorResourceRequest] = _executorResources.toMap\n+\n+  /**\n+   * (Java-specific) gets a Java Map of resources to TaskResourceRequest\n+   */\n+  def taskResourcesJMap: JMap[String, TaskResourceRequest] = _taskResources.asJava\n+\n+  /**\n+   * (Java-specific) gets a Java Map of resources to ExecutorResourceRequest\n+   */\n+  def executorResourcesJMap: JMap[String, ExecutorResourceRequest] = _executorResources.asJava\n+\n+\n+  def reset(): Unit = {\n+    _taskResources.clear()\n+    _executorResources.clear()\n+  }\n+\n+  def require(request: TaskResourceRequest): this.type = {\n+    val rName = request.resourceName\n+    if (allowedTaskResources.contains(rName) || rName.startsWith(RESOURCE_DOT)) {\n+      _taskResources(request.resourceName) = request\n+    } else {\n+      throw new IllegalArgumentException(s\"Task resource not allowed: ${request.resourceName}\")\n+    }\n+    this\n+  }\n+\n+  def require(request: ExecutorResourceRequest): this.type = {\n+    val rName = request.resourceName\n+    if (allowedExecutorResources.contains(rName) || rName.startsWith(RESOURCE_DOT)) {\n+      _executorResources(request.resourceName) = request\n+    } else {\n+      throw new IllegalArgumentException(s\"Executor resource not allowed: ${request.resourceName}\")\n+    }\n+    this\n+  }\n+\n+  override def toString(): String = {\n+    s\"Profile: id = ${_id}, executor resources: ${_executorResources}, \" +\n+      s\"task resources: ${_taskResources}\"\n+  }\n+}\n+\n+private[spark] object ResourceProfile extends Logging {\n+  val UNKNOWN_RESOURCE_PROFILE_ID = -1\n+  val DEFAULT_RESOURCE_PROFILE_ID = 0\n+\n+  val CPUS = \"cpus\"\n+  val CORES = \"cores\"",
    "line": 87
  }],
  "prId": 26284
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "I know we discussed defaulting to gpu = 1 if there were any exeuctor gpus, but didn't you have a case you wanted to make task gpus configurable as well?",
    "commit": "246de3c66c8c5a656b766ccd2f9bb3df12c9da0d",
    "createdAt": "2019-11-07T18:12:52Z",
    "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.resource\n+\n+import java.util.{Map => JMap}\n+import java.util.concurrent.atomic.{AtomicInteger, AtomicReference}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.immutable.HashSet\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.annotation.Evolving\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.resource.ResourceUtils.{RESOURCE_DOT, RESOURCE_PREFIX}\n+\n+/**\n+ * Resource profile to associate with an RDD. A ResourceProfile allows the user to\n+ * specify executor and task requirements for an RDD that will get applied during a\n+ * stage. This allows the user to change the resource requirements between stages.\n+ *\n+ * Only supports a subset of the resources for now. The config names supported correspond to the\n+ * regular Spark configs with the prefix removed. For instance overhead memory in this api\n+ * is memoryOverhead, which is spark.executor.memoryOverhead with spark.executor removed.\n+ * Resources like GPUs are resource.gpu (spark configs spark.executor.resource.gpu.*)\n+ *\n+ * Executor:\n+ *   memory - heap\n+ *   memoryOverhead\n+ *   pyspark.memory\n+ *   cores\n+ *   resource.[resourceName] - GPU, FPGA, etc\n+ *\n+ * Task requirements:\n+ *   cpus\n+ *   resource.[resourceName] - GPU, FPGA, etc\n+ *\n+ * This class is private now for initial development, once we have the feature in place\n+ * this will become public.\n+ */\n+@Evolving\n+private[spark] class ResourceProfile() extends Serializable {\n+\n+  private val _id = ResourceProfile.getNextProfileId\n+  private val _taskResources = new mutable.HashMap[String, TaskResourceRequest]()\n+  private val _executorResources = new mutable.HashMap[String, ExecutorResourceRequest]()\n+\n+  private val allowedExecutorResources = HashSet[String](\n+    ResourceProfile.MEMORY,\n+    ResourceProfile.OVERHEAD_MEM,\n+    ResourceProfile.PYSPARK_MEM,\n+    ResourceProfile.CORES)\n+\n+  private val allowedTaskResources = HashSet[String](ResourceProfile.CPUS)"
  }, {
    "author": {
      "login": "tgravescs"
    },
    "body": "yes, I filed a separate jira to have resources default to 1 if task resources not specified, but we still want users to be able to specify if they want.  There are 2 usecases I know of:\r\n\r\n- Fractional resources - I want 4 tasks to run on 1 gpu for instance\r\n- ML algorithms can use multiple GPUs, so I would want 1 task to have 2 GPUs for instance. There are algorithms available that can do this.",
    "commit": "246de3c66c8c5a656b766ccd2f9bb3df12c9da0d",
    "createdAt": "2019-11-07T18:51:37Z",
    "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.resource\n+\n+import java.util.{Map => JMap}\n+import java.util.concurrent.atomic.{AtomicInteger, AtomicReference}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.immutable.HashSet\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.annotation.Evolving\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.resource.ResourceUtils.{RESOURCE_DOT, RESOURCE_PREFIX}\n+\n+/**\n+ * Resource profile to associate with an RDD. A ResourceProfile allows the user to\n+ * specify executor and task requirements for an RDD that will get applied during a\n+ * stage. This allows the user to change the resource requirements between stages.\n+ *\n+ * Only supports a subset of the resources for now. The config names supported correspond to the\n+ * regular Spark configs with the prefix removed. For instance overhead memory in this api\n+ * is memoryOverhead, which is spark.executor.memoryOverhead with spark.executor removed.\n+ * Resources like GPUs are resource.gpu (spark configs spark.executor.resource.gpu.*)\n+ *\n+ * Executor:\n+ *   memory - heap\n+ *   memoryOverhead\n+ *   pyspark.memory\n+ *   cores\n+ *   resource.[resourceName] - GPU, FPGA, etc\n+ *\n+ * Task requirements:\n+ *   cpus\n+ *   resource.[resourceName] - GPU, FPGA, etc\n+ *\n+ * This class is private now for initial development, once we have the feature in place\n+ * this will become public.\n+ */\n+@Evolving\n+private[spark] class ResourceProfile() extends Serializable {\n+\n+  private val _id = ResourceProfile.getNextProfileId\n+  private val _taskResources = new mutable.HashMap[String, TaskResourceRequest]()\n+  private val _executorResources = new mutable.HashMap[String, ExecutorResourceRequest]()\n+\n+  private val allowedExecutorResources = HashSet[String](\n+    ResourceProfile.MEMORY,\n+    ResourceProfile.OVERHEAD_MEM,\n+    ResourceProfile.PYSPARK_MEM,\n+    ResourceProfile.CORES)\n+\n+  private val allowedTaskResources = HashSet[String](ResourceProfile.CPUS)"
  }, {
    "author": {
      "login": "squito"
    },
    "body": "setting aside the defaulting thing -- then don't you want ResourceProfile.GPU in this list too?",
    "commit": "246de3c66c8c5a656b766ccd2f9bb3df12c9da0d",
    "createdAt": "2019-11-07T20:37:48Z",
    "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.resource\n+\n+import java.util.{Map => JMap}\n+import java.util.concurrent.atomic.{AtomicInteger, AtomicReference}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.immutable.HashSet\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.annotation.Evolving\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.resource.ResourceUtils.{RESOURCE_DOT, RESOURCE_PREFIX}\n+\n+/**\n+ * Resource profile to associate with an RDD. A ResourceProfile allows the user to\n+ * specify executor and task requirements for an RDD that will get applied during a\n+ * stage. This allows the user to change the resource requirements between stages.\n+ *\n+ * Only supports a subset of the resources for now. The config names supported correspond to the\n+ * regular Spark configs with the prefix removed. For instance overhead memory in this api\n+ * is memoryOverhead, which is spark.executor.memoryOverhead with spark.executor removed.\n+ * Resources like GPUs are resource.gpu (spark configs spark.executor.resource.gpu.*)\n+ *\n+ * Executor:\n+ *   memory - heap\n+ *   memoryOverhead\n+ *   pyspark.memory\n+ *   cores\n+ *   resource.[resourceName] - GPU, FPGA, etc\n+ *\n+ * Task requirements:\n+ *   cpus\n+ *   resource.[resourceName] - GPU, FPGA, etc\n+ *\n+ * This class is private now for initial development, once we have the feature in place\n+ * this will become public.\n+ */\n+@Evolving\n+private[spark] class ResourceProfile() extends Serializable {\n+\n+  private val _id = ResourceProfile.getNextProfileId\n+  private val _taskResources = new mutable.HashMap[String, TaskResourceRequest]()\n+  private val _executorResources = new mutable.HashMap[String, ExecutorResourceRequest]()\n+\n+  private val allowedExecutorResources = HashSet[String](\n+    ResourceProfile.MEMORY,\n+    ResourceProfile.OVERHEAD_MEM,\n+    ResourceProfile.PYSPARK_MEM,\n+    ResourceProfile.CORES)\n+\n+  private val allowedTaskResources = HashSet[String](ResourceProfile.CPUS)"
  }, {
    "author": {
      "login": "tgravescs"
    },
    "body": "The code below checks for anything in this list or the name starts with resource.",
    "commit": "246de3c66c8c5a656b766ccd2f9bb3df12c9da0d",
    "createdAt": "2019-11-07T20:58:18Z",
    "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.resource\n+\n+import java.util.{Map => JMap}\n+import java.util.concurrent.atomic.{AtomicInteger, AtomicReference}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.immutable.HashSet\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.annotation.Evolving\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.resource.ResourceUtils.{RESOURCE_DOT, RESOURCE_PREFIX}\n+\n+/**\n+ * Resource profile to associate with an RDD. A ResourceProfile allows the user to\n+ * specify executor and task requirements for an RDD that will get applied during a\n+ * stage. This allows the user to change the resource requirements between stages.\n+ *\n+ * Only supports a subset of the resources for now. The config names supported correspond to the\n+ * regular Spark configs with the prefix removed. For instance overhead memory in this api\n+ * is memoryOverhead, which is spark.executor.memoryOverhead with spark.executor removed.\n+ * Resources like GPUs are resource.gpu (spark configs spark.executor.resource.gpu.*)\n+ *\n+ * Executor:\n+ *   memory - heap\n+ *   memoryOverhead\n+ *   pyspark.memory\n+ *   cores\n+ *   resource.[resourceName] - GPU, FPGA, etc\n+ *\n+ * Task requirements:\n+ *   cpus\n+ *   resource.[resourceName] - GPU, FPGA, etc\n+ *\n+ * This class is private now for initial development, once we have the feature in place\n+ * this will become public.\n+ */\n+@Evolving\n+private[spark] class ResourceProfile() extends Serializable {\n+\n+  private val _id = ResourceProfile.getNextProfileId\n+  private val _taskResources = new mutable.HashMap[String, TaskResourceRequest]()\n+  private val _executorResources = new mutable.HashMap[String, ExecutorResourceRequest]()\n+\n+  private val allowedExecutorResources = HashSet[String](\n+    ResourceProfile.MEMORY,\n+    ResourceProfile.OVERHEAD_MEM,\n+    ResourceProfile.PYSPARK_MEM,\n+    ResourceProfile.CORES)\n+\n+  private val allowedTaskResources = HashSet[String](ResourceProfile.CPUS)"
  }, {
    "author": {
      "login": "squito"
    },
    "body": "oops, my fault",
    "commit": "246de3c66c8c5a656b766ccd2f9bb3df12c9da0d",
    "createdAt": "2019-11-07T22:27:40Z",
    "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.resource\n+\n+import java.util.{Map => JMap}\n+import java.util.concurrent.atomic.{AtomicInteger, AtomicReference}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.immutable.HashSet\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.annotation.Evolving\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.resource.ResourceUtils.{RESOURCE_DOT, RESOURCE_PREFIX}\n+\n+/**\n+ * Resource profile to associate with an RDD. A ResourceProfile allows the user to\n+ * specify executor and task requirements for an RDD that will get applied during a\n+ * stage. This allows the user to change the resource requirements between stages.\n+ *\n+ * Only supports a subset of the resources for now. The config names supported correspond to the\n+ * regular Spark configs with the prefix removed. For instance overhead memory in this api\n+ * is memoryOverhead, which is spark.executor.memoryOverhead with spark.executor removed.\n+ * Resources like GPUs are resource.gpu (spark configs spark.executor.resource.gpu.*)\n+ *\n+ * Executor:\n+ *   memory - heap\n+ *   memoryOverhead\n+ *   pyspark.memory\n+ *   cores\n+ *   resource.[resourceName] - GPU, FPGA, etc\n+ *\n+ * Task requirements:\n+ *   cpus\n+ *   resource.[resourceName] - GPU, FPGA, etc\n+ *\n+ * This class is private now for initial development, once we have the feature in place\n+ * this will become public.\n+ */\n+@Evolving\n+private[spark] class ResourceProfile() extends Serializable {\n+\n+  private val _id = ResourceProfile.getNextProfileId\n+  private val _taskResources = new mutable.HashMap[String, TaskResourceRequest]()\n+  private val _executorResources = new mutable.HashMap[String, ExecutorResourceRequest]()\n+\n+  private val allowedExecutorResources = HashSet[String](\n+    ResourceProfile.MEMORY,\n+    ResourceProfile.OVERHEAD_MEM,\n+    ResourceProfile.PYSPARK_MEM,\n+    ResourceProfile.CORES)\n+\n+  private val allowedTaskResources = HashSet[String](ResourceProfile.CPUS)"
  }],
  "prId": 26284
}]