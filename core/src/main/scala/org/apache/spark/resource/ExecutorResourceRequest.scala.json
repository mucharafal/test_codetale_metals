[{
  "comments": [{
    "author": {
      "login": "holdenk"
    },
    "body": "I'd love to see more about this. I'm a bit confused about how we expect folks to use this. I was assuming we'd maybe ask the cluster manager for resources but this seems to indicate a very different approach. Is this because cluster managers don't have a consistent way of keeping track of the acceleration resources?",
    "commit": "246de3c66c8c5a656b766ccd2f9bb3df12c9da0d",
    "createdAt": "2019-11-01T11:33:36Z",
    "diffHunk": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.resource\n+\n+/**\n+ * An executor resource request. This is used in conjuntion with the ResourceProfile to\n+ * programmatically specify the resources needed for an RDD that will be applied at the\n+ * stage level.\n+ * There are alternative constructors for working with Java.\n+ *\n+ * @param resourceName Name of the resource\n+ * @param amount Amount requesting\n+ * @param units Units of amount for things like Memory, default is no units, only byte\n+ *              types (b, mb, gb, etc) are supported.\n+ * @param discoveryScript Script used to discovery the resources"
  }, {
    "author": {
      "login": "tgravescs"
    },
    "body": "not all cluster managers  tell you what addresses for the accelerators they have allocated to you so the discovery Script adds the ability to find them.  If you aren't running in an isolated environment you can also envision someone writing a discovery script that handles getting an accelerator not being used by another process.",
    "commit": "246de3c66c8c5a656b766ccd2f9bb3df12c9da0d",
    "createdAt": "2019-11-01T13:09:03Z",
    "diffHunk": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.resource\n+\n+/**\n+ * An executor resource request. This is used in conjuntion with the ResourceProfile to\n+ * programmatically specify the resources needed for an RDD that will be applied at the\n+ * stage level.\n+ * There are alternative constructors for working with Java.\n+ *\n+ * @param resourceName Name of the resource\n+ * @param amount Amount requesting\n+ * @param units Units of amount for things like Memory, default is no units, only byte\n+ *              types (b, mb, gb, etc) are supported.\n+ * @param discoveryScript Script used to discovery the resources"
  }, {
    "author": {
      "login": "tgravescs"
    },
    "body": "the discovery Script is really part of the original accelerator aware scheduling, it just carries over to this stage level api.  You can find the docs on that in: https://issues.apache.org/jira/browse/SPARK-27495",
    "commit": "246de3c66c8c5a656b766ccd2f9bb3df12c9da0d",
    "createdAt": "2019-11-01T13:13:07Z",
    "diffHunk": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.resource\n+\n+/**\n+ * An executor resource request. This is used in conjuntion with the ResourceProfile to\n+ * programmatically specify the resources needed for an RDD that will be applied at the\n+ * stage level.\n+ * There are alternative constructors for working with Java.\n+ *\n+ * @param resourceName Name of the resource\n+ * @param amount Amount requesting\n+ * @param units Units of amount for things like Memory, default is no units, only byte\n+ *              types (b, mb, gb, etc) are supported.\n+ * @param discoveryScript Script used to discovery the resources"
  }, {
    "author": {
      "login": "holdenk"
    },
    "body": "So there isn't a lot about the discovery script in 27495 or 24615 but I get that we've made that decision. I still think we should clarify it a bit more (and I think you've done this).",
    "commit": "246de3c66c8c5a656b766ccd2f9bb3df12c9da0d",
    "createdAt": "2019-11-15T17:33:43Z",
    "diffHunk": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.resource\n+\n+/**\n+ * An executor resource request. This is used in conjuntion with the ResourceProfile to\n+ * programmatically specify the resources needed for an RDD that will be applied at the\n+ * stage level.\n+ * There are alternative constructors for working with Java.\n+ *\n+ * @param resourceName Name of the resource\n+ * @param amount Amount requesting\n+ * @param units Units of amount for things like Memory, default is no units, only byte\n+ *              types (b, mb, gb, etc) are supported.\n+ * @param discoveryScript Script used to discovery the resources"
  }],
  "prId": 26284
}, {
  "comments": [{
    "author": {
      "login": "holdenk"
    },
    "body": "If we only support bytes types is a string the way to do this?",
    "commit": "246de3c66c8c5a656b766ccd2f9bb3df12c9da0d",
    "createdAt": "2019-11-01T11:34:07Z",
    "diffHunk": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.resource\n+\n+/**\n+ * An executor resource request. This is used in conjuntion with the ResourceProfile to\n+ * programmatically specify the resources needed for an RDD that will be applied at the\n+ * stage level.\n+ * There are alternative constructors for working with Java.\n+ *\n+ * @param resourceName Name of the resource\n+ * @param amount Amount requesting\n+ * @param units Units of amount for things like Memory, default is no units, only byte"
  }, {
    "author": {
      "login": "tgravescs"
    },
    "body": "I was leaving it a string in case future enhancements required types other then bytes so we wouldn't have to change API, but we can restrict for now and add something else later if you prefer?",
    "commit": "246de3c66c8c5a656b766ccd2f9bb3df12c9da0d",
    "createdAt": "2019-11-01T13:09:54Z",
    "diffHunk": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.resource\n+\n+/**\n+ * An executor resource request. This is used in conjuntion with the ResourceProfile to\n+ * programmatically specify the resources needed for an RDD that will be applied at the\n+ * stage level.\n+ * There are alternative constructors for working with Java.\n+ *\n+ * @param resourceName Name of the resource\n+ * @param amount Amount requesting\n+ * @param units Units of amount for things like Memory, default is no units, only byte"
  }],
  "prId": 26284
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "I think the use of units here is really confusing -- you'd need to add in more code later to support comparison of units, etc.\r\n\r\nif you really just want to make it easy to specify units for memory, how about adding a util constructor or factory method just for memory?  `ExecutorResourceRequest.memory(amount: double, units: String)` or `ExecutorResourceRequest.memoryGB(amount: Double)`?",
    "commit": "246de3c66c8c5a656b766ccd2f9bb3df12c9da0d",
    "createdAt": "2019-11-07T18:06:55Z",
    "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.resource\n+\n+/**\n+ * An Executor resource request. This is used in conjunction with the ResourceProfile to\n+ * programmatically specify the resources needed for an RDD that will be applied at the\n+ * stage level.\n+ *\n+ * This is used to specify what the resource requirements are for an Executor and how\n+ * Spark can find out specific details about those resources. Not all the parameters are\n+ * required for every resource type. The resources names supported\n+ * correspond to the regular Spark configs with the prefix removed. For instance overhead\n+ * memory in this api is memoryOverhead, which is spark.executor.memoryOverhead with\n+ * spark.executor removed. Resources like GPUs are resource.gpu\n+ * (spark configs spark.executor.resource.gpu.*). The amount, discoveryScript, and vendor\n+ * parameters for resources are all the same parameters a user would specify through the\n+ * configs: spark.executor.resource.{resourceName}.{amount, discoveryScript, vendor}.\n+ *\n+ * For instance, a user wants to allocate an Executor with GPU resources on YARN. The user has\n+ * to specify the resource name (resource.gpu), the amount or number of GPUs per Executor,\n+ * units would not be used as its not a memory config, the discovery script would be specified\n+ * so that when the Executor starts up it can discovery what GPU addresses are available for it to\n+ * use because YARN doesn't tell Spark that, then vendor would not be used because\n+ * its specific for Kubernetes.\n+ *\n+ * See the configuration and cluster specific docs for more details.\n+ *\n+ * There are alternative constructors for working with Java.\n+ *\n+ * @param resourceName Name of the resource\n+ * @param amount Amount requesting\n+ * @param units Optional units of the amount. For things like Memory, default is no units, only byte\n+ *              types (b, mb, gb, etc) are currently supported."
  }, {
    "author": {
      "login": "tgravescs"
    },
    "body": "yeah the code that needs to use the units isn't in this pr and I agree that could get ugly to compare units. This started out just being modeled after the YARN api. How complex that gets somewhat depends on if when comparing ResourceProfiles, the units parameter has to match.  For instance if I specify memory as 1024m and then I compare to a profile with 1g, do we consider that the same?  If we don't care about that we can just normalize the memory to be bytes or MB internally.  Alternatively we could get rid of the units and just say memory amount must be specified in MB (similar to your suggestion ExecutorResourceRequest.memoryGB(amount: Double)) , but that doesn't match the way we do the other memory configs.\r\n\r\nI'm not sure how I feel about the api being through the constructor for everything except for memory. I can see that being confusing to the user as well.\r\nWe could remove all the parameters from the constructor and make setters for all the things we support - memory, overheadMemory, core, resources (like GPUs), etc.  This would remove the need for the Enum/String checks to see if we support it.  Really at that point you could put directly in ResourceProfile and not need ExecutorResourceRequest.\r\n\r\nIf we don't like any of those options, I could remove the units parameter for now from this PR and can add it back in with the PR that uses it.  Which will be the scheduler code for merging and scheduling tasks and the cluster manager code.\r\nthoughts?",
    "commit": "246de3c66c8c5a656b766ccd2f9bb3df12c9da0d",
    "createdAt": "2019-11-07T19:24:55Z",
    "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.resource\n+\n+/**\n+ * An Executor resource request. This is used in conjunction with the ResourceProfile to\n+ * programmatically specify the resources needed for an RDD that will be applied at the\n+ * stage level.\n+ *\n+ * This is used to specify what the resource requirements are for an Executor and how\n+ * Spark can find out specific details about those resources. Not all the parameters are\n+ * required for every resource type. The resources names supported\n+ * correspond to the regular Spark configs with the prefix removed. For instance overhead\n+ * memory in this api is memoryOverhead, which is spark.executor.memoryOverhead with\n+ * spark.executor removed. Resources like GPUs are resource.gpu\n+ * (spark configs spark.executor.resource.gpu.*). The amount, discoveryScript, and vendor\n+ * parameters for resources are all the same parameters a user would specify through the\n+ * configs: spark.executor.resource.{resourceName}.{amount, discoveryScript, vendor}.\n+ *\n+ * For instance, a user wants to allocate an Executor with GPU resources on YARN. The user has\n+ * to specify the resource name (resource.gpu), the amount or number of GPUs per Executor,\n+ * units would not be used as its not a memory config, the discovery script would be specified\n+ * so that when the Executor starts up it can discovery what GPU addresses are available for it to\n+ * use because YARN doesn't tell Spark that, then vendor would not be used because\n+ * its specific for Kubernetes.\n+ *\n+ * See the configuration and cluster specific docs for more details.\n+ *\n+ * There are alternative constructors for working with Java.\n+ *\n+ * @param resourceName Name of the resource\n+ * @param amount Amount requesting\n+ * @param units Optional units of the amount. For things like Memory, default is no units, only byte\n+ *              types (b, mb, gb, etc) are currently supported."
  }, {
    "author": {
      "login": "squito"
    },
    "body": "I like having a list of specific methods for the types, personally I think that is clearer (you get code completion to tell you the options, rather than having to look through docs).  I guess I was wondering if you specifically wanted more extensibility here by having looser typing, eg. for compatibility.  But I don't see a compelling case for that.  You've got a hard check that the type is understood;  and its not like you could take code written for some future version of spark with another resource type, and then run it against an old version of spark, as your resource requirement wouldn't be honored on the old spark.\r\n\r\nalso I don't think I was super clear about that extra method for memory -- I meant that you could still use the general constructor, but it would only be for bytes.  Then those other methods would just be conveniences.  They would return something in bytes, and all comparisons would then be done in bytes.",
    "commit": "246de3c66c8c5a656b766ccd2f9bb3df12c9da0d",
    "createdAt": "2019-11-07T20:46:02Z",
    "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.resource\n+\n+/**\n+ * An Executor resource request. This is used in conjunction with the ResourceProfile to\n+ * programmatically specify the resources needed for an RDD that will be applied at the\n+ * stage level.\n+ *\n+ * This is used to specify what the resource requirements are for an Executor and how\n+ * Spark can find out specific details about those resources. Not all the parameters are\n+ * required for every resource type. The resources names supported\n+ * correspond to the regular Spark configs with the prefix removed. For instance overhead\n+ * memory in this api is memoryOverhead, which is spark.executor.memoryOverhead with\n+ * spark.executor removed. Resources like GPUs are resource.gpu\n+ * (spark configs spark.executor.resource.gpu.*). The amount, discoveryScript, and vendor\n+ * parameters for resources are all the same parameters a user would specify through the\n+ * configs: spark.executor.resource.{resourceName}.{amount, discoveryScript, vendor}.\n+ *\n+ * For instance, a user wants to allocate an Executor with GPU resources on YARN. The user has\n+ * to specify the resource name (resource.gpu), the amount or number of GPUs per Executor,\n+ * units would not be used as its not a memory config, the discovery script would be specified\n+ * so that when the Executor starts up it can discovery what GPU addresses are available for it to\n+ * use because YARN doesn't tell Spark that, then vendor would not be used because\n+ * its specific for Kubernetes.\n+ *\n+ * See the configuration and cluster specific docs for more details.\n+ *\n+ * There are alternative constructors for working with Java.\n+ *\n+ * @param resourceName Name of the resource\n+ * @param amount Amount requesting\n+ * @param units Optional units of the amount. For things like Memory, default is no units, only byte\n+ *              types (b, mb, gb, etc) are currently supported."
  }, {
    "author": {
      "login": "tgravescs"
    },
    "body": "Ok the api taking bytes by default makes more sense.\r\nLet me look at changing to just have setters directly to see what that looks like.  I was originally thinking making it more generic would make it easier to add things in the future.  The api would change just now pass in new things, bit not sure that really matters. I do envision people wanting many more options here like the dynamic allocation min max, potentially many other spark confs so need to look at how many apis that adds",
    "commit": "246de3c66c8c5a656b766ccd2f9bb3df12c9da0d",
    "createdAt": "2019-11-07T21:09:54Z",
    "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.resource\n+\n+/**\n+ * An Executor resource request. This is used in conjunction with the ResourceProfile to\n+ * programmatically specify the resources needed for an RDD that will be applied at the\n+ * stage level.\n+ *\n+ * This is used to specify what the resource requirements are for an Executor and how\n+ * Spark can find out specific details about those resources. Not all the parameters are\n+ * required for every resource type. The resources names supported\n+ * correspond to the regular Spark configs with the prefix removed. For instance overhead\n+ * memory in this api is memoryOverhead, which is spark.executor.memoryOverhead with\n+ * spark.executor removed. Resources like GPUs are resource.gpu\n+ * (spark configs spark.executor.resource.gpu.*). The amount, discoveryScript, and vendor\n+ * parameters for resources are all the same parameters a user would specify through the\n+ * configs: spark.executor.resource.{resourceName}.{amount, discoveryScript, vendor}.\n+ *\n+ * For instance, a user wants to allocate an Executor with GPU resources on YARN. The user has\n+ * to specify the resource name (resource.gpu), the amount or number of GPUs per Executor,\n+ * units would not be used as its not a memory config, the discovery script would be specified\n+ * so that when the Executor starts up it can discovery what GPU addresses are available for it to\n+ * use because YARN doesn't tell Spark that, then vendor would not be used because\n+ * its specific for Kubernetes.\n+ *\n+ * See the configuration and cluster specific docs for more details.\n+ *\n+ * There are alternative constructors for working with Java.\n+ *\n+ * @param resourceName Name of the resource\n+ * @param amount Amount requesting\n+ * @param units Optional units of the amount. For things like Memory, default is no units, only byte\n+ *              types (b, mb, gb, etc) are currently supported."
  }, {
    "author": {
      "login": "tgravescs"
    },
    "body": "so I'll write it out a bit more but I think I like the setters for each, the only thing is whether they are in the ResourceProfile class or you keep them in the ExecutorResourceRequest type class. The reason to keep them out of ResourceProfile is to allow it to more easily handle future things like having a ResourceProfile.prefer - which would have preference of a Resource but not a strict requirement.  It also makes is so you have a bit cleaner separation of Executor and Task resources.\r\nfor example the api could be:\r\n\r\nval rp = new ResourceProfile()\r\nrp.executorMemory(1, \"g\")\r\nrp.executorCores(2)\r\nrp.executorResources(\"gpu\", 1, \"discoveryScript:)\r\nrp.taskResources(\"gpu\", 1)\r\n\r\nOr keep it like:\r\nval rp = new ResourceProfile()\r\nval e = new ExecutorResourceRequest()\r\ne.memory(1, \"g\")\r\ne.cores(2)\r\ne.resources(\"gpu\", 1, \"discoveryScript\")\r\nval t = new TaskResourceRequest()\r\nt.resources(\"gpu\", 1, \"discoveryScript\")\r\nrp.require(e)\r\nrp.require(t)\r\n\r\nThe first one is shorter but the second one gives us more flexibility.\r\n Alternatively for things like ResourceProfile.prefer would could come up with other ways to support it like having optional parameter to the api. \r\n\r\nthoughts on that? ",
    "commit": "246de3c66c8c5a656b766ccd2f9bb3df12c9da0d",
    "createdAt": "2019-11-07T21:59:18Z",
    "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.resource\n+\n+/**\n+ * An Executor resource request. This is used in conjunction with the ResourceProfile to\n+ * programmatically specify the resources needed for an RDD that will be applied at the\n+ * stage level.\n+ *\n+ * This is used to specify what the resource requirements are for an Executor and how\n+ * Spark can find out specific details about those resources. Not all the parameters are\n+ * required for every resource type. The resources names supported\n+ * correspond to the regular Spark configs with the prefix removed. For instance overhead\n+ * memory in this api is memoryOverhead, which is spark.executor.memoryOverhead with\n+ * spark.executor removed. Resources like GPUs are resource.gpu\n+ * (spark configs spark.executor.resource.gpu.*). The amount, discoveryScript, and vendor\n+ * parameters for resources are all the same parameters a user would specify through the\n+ * configs: spark.executor.resource.{resourceName}.{amount, discoveryScript, vendor}.\n+ *\n+ * For instance, a user wants to allocate an Executor with GPU resources on YARN. The user has\n+ * to specify the resource name (resource.gpu), the amount or number of GPUs per Executor,\n+ * units would not be used as its not a memory config, the discovery script would be specified\n+ * so that when the Executor starts up it can discovery what GPU addresses are available for it to\n+ * use because YARN doesn't tell Spark that, then vendor would not be used because\n+ * its specific for Kubernetes.\n+ *\n+ * See the configuration and cluster specific docs for more details.\n+ *\n+ * There are alternative constructors for working with Java.\n+ *\n+ * @param resourceName Name of the resource\n+ * @param amount Amount requesting\n+ * @param units Optional units of the amount. For things like Memory, default is no units, only byte\n+ *              types (b, mb, gb, etc) are currently supported."
  }, {
    "author": {
      "login": "tgravescs"
    },
    "body": "I quickly ran into issues with the second option I talked about above because the TaskResourceRequest and ExecutorResourceRequest then become mutable which I think complicates things.\r\n\r\nI came up with a compromise that I think still gives us ability to add things like .prefer to ResourceProfiles and that is just adding an intermediate class TAskResourceRequests and ExecutorResourceRequests (note the s on the end). In these classes it adds the convenient setters for cpu, memory, etc. But it also leaves the base ExecutorResourceRequest and TaskResourceRequest classes.  I was going to make those internal classes but the downside to that is if we want the user to be able to get the resources back from the ExecutorResourceRequests those classes are essentially needed.\r\n\r\nHere is an example:\r\n\r\nWith the existing PR:\r\n```\r\n-    val cpuTaskReq = new TaskResourceRequest(ResourceProfile.CPUS, 1)\r\n-    val coresExecReq = new ExecutorResourceRequest(ResourceProfile.CORES, 2)\r\n-    val memExecReq = new ExecutorResourceRequest(ResourceProfile.MEMORY, 4096, \"mb\")\r\n-    val omemExecReq = new ExecutorResourceRequest(ResourceProfile.OVERHEAD_MEM, 2048)\r\n-    val pysparkMemExecReq = new ExecutorResourceRequest(ResourceProfile.PYSPARK_MEM, 1024)\r\n-\r\n-    rprof.require(cpuTaskReq)\r\n-    rprof.require(coresExecReq)\r\n-    rprof.require(memExecReq)\r\n-    rprof.require(omemExecReq)\r\n-    rprof.require(pysparkMemExecReq)\r\n```\r\n\r\nWith the changes I talked about adding in ExecutorResourceRequests and TaskResourceRequests:\r\n```\r\n+    val ereqs = new ExecutorResourceRequests()\r\n+    ereqs.cores(2).memory(4096, \"m\")\r\n+    ereqs.memoryOverhead(2048, \"m\").pysparkMemory(1024, \"m\")\r\n+    val treqs = new TaskResourceRequests()\r\n+    treqs.cpus(1)\r\n+\r\n+    rprof.require(treqs)\r\n+    rprof.require(ereqs)\r\n \r\n```\r\n\r\nThis leaves us the flexibility in the future to add things like .prefer to ResourceProfile.\r\nthoughts on this?  I'll probably push these changes shortly for you to take a look at the code. I can always role back if we don't like it.",
    "commit": "246de3c66c8c5a656b766ccd2f9bb3df12c9da0d",
    "createdAt": "2019-11-08T19:28:19Z",
    "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.resource\n+\n+/**\n+ * An Executor resource request. This is used in conjunction with the ResourceProfile to\n+ * programmatically specify the resources needed for an RDD that will be applied at the\n+ * stage level.\n+ *\n+ * This is used to specify what the resource requirements are for an Executor and how\n+ * Spark can find out specific details about those resources. Not all the parameters are\n+ * required for every resource type. The resources names supported\n+ * correspond to the regular Spark configs with the prefix removed. For instance overhead\n+ * memory in this api is memoryOverhead, which is spark.executor.memoryOverhead with\n+ * spark.executor removed. Resources like GPUs are resource.gpu\n+ * (spark configs spark.executor.resource.gpu.*). The amount, discoveryScript, and vendor\n+ * parameters for resources are all the same parameters a user would specify through the\n+ * configs: spark.executor.resource.{resourceName}.{amount, discoveryScript, vendor}.\n+ *\n+ * For instance, a user wants to allocate an Executor with GPU resources on YARN. The user has\n+ * to specify the resource name (resource.gpu), the amount or number of GPUs per Executor,\n+ * units would not be used as its not a memory config, the discovery script would be specified\n+ * so that when the Executor starts up it can discovery what GPU addresses are available for it to\n+ * use because YARN doesn't tell Spark that, then vendor would not be used because\n+ * its specific for Kubernetes.\n+ *\n+ * See the configuration and cluster specific docs for more details.\n+ *\n+ * There are alternative constructors for working with Java.\n+ *\n+ * @param resourceName Name of the resource\n+ * @param amount Amount requesting\n+ * @param units Optional units of the amount. For things like Memory, default is no units, only byte\n+ *              types (b, mb, gb, etc) are currently supported."
  }, {
    "author": {
      "login": "squito"
    },
    "body": "yes I do like that version.\r\n\r\nSo, I realized I didn't understand your initial version of the code, though -- I thought you had a hard requirement that the resources were in the set of things you had listed.  I had missed that check `|| rName.startsWith(RESOURCE_DOT)`.  The one advantage of leaving it as strings is that you get some extensibility for free.  Eg., with the old code, would things just work if the user requested FPGA or quantum cpus, if the resource discovery scripts returned them?  both of those sound a little far-fetched to me, so I'm OK requiring changes to code before supporting new things, but just wnated to discuss it.  I think I'd still prefer static methods.",
    "commit": "246de3c66c8c5a656b766ccd2f9bb3df12c9da0d",
    "createdAt": "2019-11-12T17:24:54Z",
    "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.resource\n+\n+/**\n+ * An Executor resource request. This is used in conjunction with the ResourceProfile to\n+ * programmatically specify the resources needed for an RDD that will be applied at the\n+ * stage level.\n+ *\n+ * This is used to specify what the resource requirements are for an Executor and how\n+ * Spark can find out specific details about those resources. Not all the parameters are\n+ * required for every resource type. The resources names supported\n+ * correspond to the regular Spark configs with the prefix removed. For instance overhead\n+ * memory in this api is memoryOverhead, which is spark.executor.memoryOverhead with\n+ * spark.executor removed. Resources like GPUs are resource.gpu\n+ * (spark configs spark.executor.resource.gpu.*). The amount, discoveryScript, and vendor\n+ * parameters for resources are all the same parameters a user would specify through the\n+ * configs: spark.executor.resource.{resourceName}.{amount, discoveryScript, vendor}.\n+ *\n+ * For instance, a user wants to allocate an Executor with GPU resources on YARN. The user has\n+ * to specify the resource name (resource.gpu), the amount or number of GPUs per Executor,\n+ * units would not be used as its not a memory config, the discovery script would be specified\n+ * so that when the Executor starts up it can discovery what GPU addresses are available for it to\n+ * use because YARN doesn't tell Spark that, then vendor would not be used because\n+ * its specific for Kubernetes.\n+ *\n+ * See the configuration and cluster specific docs for more details.\n+ *\n+ * There are alternative constructors for working with Java.\n+ *\n+ * @param resourceName Name of the resource\n+ * @param amount Amount requesting\n+ * @param units Optional units of the amount. For things like Memory, default is no units, only byte\n+ *              types (b, mb, gb, etc) are currently supported."
  }, {
    "author": {
      "login": "tgravescs"
    },
    "body": "So either the new code or the old code both support extensibility for custom resources (gpu, fpga, etc). That doesn't change with either approach.  You do that via the ExecutorResourceRequests.resource(...)  call.  I guess I didn't put an example of that in my previous comment.  I realize resource() is potentially a bit vague so thought about renaming it - maybe customResource().  But the configs for those resources are spark.executor.resource.{resourceName} (spark.executor.resource.gpu, spark.executor.resource.fpga, etc).  If you have thoughts on better name let me know.\r\n\r\nSo the only thing that is really hardcoded are those resources Spark already strictly handles - heap memory, overhead memory, cores, pyspark memory, etc... Any other custom resource that one could allocate (say from YARN or k8s) you can still specify here via that resource() option: https://github.com/apache/spark/pull/26284/files#diff-f251b8505059be76856f40dae5f8f8a0R141\r\n\r\nI wouldn't want to remove that functionality.",
    "commit": "246de3c66c8c5a656b766ccd2f9bb3df12c9da0d",
    "createdAt": "2019-11-12T17:42:07Z",
    "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.resource\n+\n+/**\n+ * An Executor resource request. This is used in conjunction with the ResourceProfile to\n+ * programmatically specify the resources needed for an RDD that will be applied at the\n+ * stage level.\n+ *\n+ * This is used to specify what the resource requirements are for an Executor and how\n+ * Spark can find out specific details about those resources. Not all the parameters are\n+ * required for every resource type. The resources names supported\n+ * correspond to the regular Spark configs with the prefix removed. For instance overhead\n+ * memory in this api is memoryOverhead, which is spark.executor.memoryOverhead with\n+ * spark.executor removed. Resources like GPUs are resource.gpu\n+ * (spark configs spark.executor.resource.gpu.*). The amount, discoveryScript, and vendor\n+ * parameters for resources are all the same parameters a user would specify through the\n+ * configs: spark.executor.resource.{resourceName}.{amount, discoveryScript, vendor}.\n+ *\n+ * For instance, a user wants to allocate an Executor with GPU resources on YARN. The user has\n+ * to specify the resource name (resource.gpu), the amount or number of GPUs per Executor,\n+ * units would not be used as its not a memory config, the discovery script would be specified\n+ * so that when the Executor starts up it can discovery what GPU addresses are available for it to\n+ * use because YARN doesn't tell Spark that, then vendor would not be used because\n+ * its specific for Kubernetes.\n+ *\n+ * See the configuration and cluster specific docs for more details.\n+ *\n+ * There are alternative constructors for working with Java.\n+ *\n+ * @param resourceName Name of the resource\n+ * @param amount Amount requesting\n+ * @param units Optional units of the amount. For things like Memory, default is no units, only byte\n+ *              types (b, mb, gb, etc) are currently supported."
  }, {
    "author": {
      "login": "tgravescs"
    },
    "body": "enhanced version of my example using GPU resource:\r\n\r\n```\r\nval ereqs = new ExecutorResourceRequests()\r\nereqs.cores(2).memory(4096, \"m\")\r\nereqs.memoryOverhead(2048, \"m\").pysparkMemory(1024, \"m\")\r\n**ereqs.resource(\"resource.gpu\", 2, \"discoveryScript\")**\r\nval treqs = new TaskResourceRequests()\r\ntreqs.cpus(1)\r\n**treqs.resource(\"resource.gpu\", 1.0)**\r\n\r\nrprof.require(treqs)\r\nrprof.require(ereqs)\r\n```",
    "commit": "246de3c66c8c5a656b766ccd2f9bb3df12c9da0d",
    "createdAt": "2019-11-12T17:45:03Z",
    "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.resource\n+\n+/**\n+ * An Executor resource request. This is used in conjunction with the ResourceProfile to\n+ * programmatically specify the resources needed for an RDD that will be applied at the\n+ * stage level.\n+ *\n+ * This is used to specify what the resource requirements are for an Executor and how\n+ * Spark can find out specific details about those resources. Not all the parameters are\n+ * required for every resource type. The resources names supported\n+ * correspond to the regular Spark configs with the prefix removed. For instance overhead\n+ * memory in this api is memoryOverhead, which is spark.executor.memoryOverhead with\n+ * spark.executor removed. Resources like GPUs are resource.gpu\n+ * (spark configs spark.executor.resource.gpu.*). The amount, discoveryScript, and vendor\n+ * parameters for resources are all the same parameters a user would specify through the\n+ * configs: spark.executor.resource.{resourceName}.{amount, discoveryScript, vendor}.\n+ *\n+ * For instance, a user wants to allocate an Executor with GPU resources on YARN. The user has\n+ * to specify the resource name (resource.gpu), the amount or number of GPUs per Executor,\n+ * units would not be used as its not a memory config, the discovery script would be specified\n+ * so that when the Executor starts up it can discovery what GPU addresses are available for it to\n+ * use because YARN doesn't tell Spark that, then vendor would not be used because\n+ * its specific for Kubernetes.\n+ *\n+ * See the configuration and cluster specific docs for more details.\n+ *\n+ * There are alternative constructors for working with Java.\n+ *\n+ * @param resourceName Name of the resource\n+ * @param amount Amount requesting\n+ * @param units Optional units of the amount. For things like Memory, default is no units, only byte\n+ *              types (b, mb, gb, etc) are currently supported."
  }, {
    "author": {
      "login": "squito"
    },
    "body": "ok, I like this new api, thanks for clarifying.\r\n\r\nI think it might make sense to add a static method for gpu, too -- imo that actually seems like the most common use case for this entire api -- but that can be added later.",
    "commit": "246de3c66c8c5a656b766ccd2f9bb3df12c9da0d",
    "createdAt": "2019-11-12T20:47:12Z",
    "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.resource\n+\n+/**\n+ * An Executor resource request. This is used in conjunction with the ResourceProfile to\n+ * programmatically specify the resources needed for an RDD that will be applied at the\n+ * stage level.\n+ *\n+ * This is used to specify what the resource requirements are for an Executor and how\n+ * Spark can find out specific details about those resources. Not all the parameters are\n+ * required for every resource type. The resources names supported\n+ * correspond to the regular Spark configs with the prefix removed. For instance overhead\n+ * memory in this api is memoryOverhead, which is spark.executor.memoryOverhead with\n+ * spark.executor removed. Resources like GPUs are resource.gpu\n+ * (spark configs spark.executor.resource.gpu.*). The amount, discoveryScript, and vendor\n+ * parameters for resources are all the same parameters a user would specify through the\n+ * configs: spark.executor.resource.{resourceName}.{amount, discoveryScript, vendor}.\n+ *\n+ * For instance, a user wants to allocate an Executor with GPU resources on YARN. The user has\n+ * to specify the resource name (resource.gpu), the amount or number of GPUs per Executor,\n+ * units would not be used as its not a memory config, the discovery script would be specified\n+ * so that when the Executor starts up it can discovery what GPU addresses are available for it to\n+ * use because YARN doesn't tell Spark that, then vendor would not be used because\n+ * its specific for Kubernetes.\n+ *\n+ * See the configuration and cluster specific docs for more details.\n+ *\n+ * There are alternative constructors for working with Java.\n+ *\n+ * @param resourceName Name of the resource\n+ * @param amount Amount requesting\n+ * @param units Optional units of the amount. For things like Memory, default is no units, only byte\n+ *              types (b, mb, gb, etc) are currently supported."
  }],
  "prId": 26284
}, {
  "comments": [{
    "author": {
      "login": "holdenk"
    },
    "body": "Maybe just add a comment here that this is just for the Spark internal resources, and GPUs & FPGAs and everything inside inside of `resource.` is allowed?",
    "commit": "246de3c66c8c5a656b766ccd2f9bb3df12c9da0d",
    "createdAt": "2019-11-15T17:27:44Z",
    "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.resource\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.resource.ResourceUtils.RESOURCE_DOT\n+\n+/**\n+ * An Executor resource request. This is used in conjunction with the ResourceProfile to\n+ * programmatically specify the resources needed for an RDD that will be applied at the\n+ * stage level.\n+ *\n+ * This is used to specify what the resource requirements are for an Executor and how\n+ * Spark can find out specific details about those resources. Not all the parameters are\n+ * required for every resource type. The resources names supported\n+ * correspond to the regular Spark configs with the prefix removed. For instance overhead\n+ * memory in this api is memoryOverhead, which is spark.executor.memoryOverhead with\n+ * spark.executor removed. Resources like GPUs are resource.gpu\n+ * (spark configs spark.executor.resource.gpu.*). The amount, discoveryScript, and vendor\n+ * parameters for resources are all the same parameters a user would specify through the\n+ * configs: spark.executor.resource.{resourceName}.{amount, discoveryScript, vendor}.\n+ *\n+ * For instance, a user wants to allocate an Executor with GPU resources on YARN. The user has\n+ * to specify the resource name (resource.gpu), the amount or number of GPUs per Executor,\n+ * the discovery script would be specified so that when the Executor starts up it can\n+ * discovery what GPU addresses are available for it to use because YARN doesn't tell\n+ * Spark that, then vendor would not be used because its specific for Kubernetes.\n+ *\n+ * See the configuration and cluster specific docs for more details.\n+ *\n+ * Use ExecutorResourceRequests class as a convenience API.\n+ *\n+ * @param resourceName Name of the resource\n+ * @param amount Amount requesting\n+ * @param discoveryScript Optional script used to discover the resources. This is required on some\n+ *                        cluster managers that don't tell Spark the addresses of the resources\n+ *                        allocated. The script runs on Executors startup to discover the addresses\n+ *                        of the resources available.\n+ * @param vendor Optional vendor, required for some cluster managers\n+ *\n+ * This api is currently private until the rest of the pieces are in place and then it\n+ * will become public.\n+ */\n+private[spark] class ExecutorResourceRequest(\n+    val resourceName: String,\n+    val amount: Long,\n+    val discoveryScript: String = \"\",\n+    val vendor: String = \"\") extends Serializable {\n+\n+  private val allowedExecutorResources = mutable.HashSet[String](",
    "line": 68
  }, {
    "author": {
      "login": "tgravescs"
    },
    "body": "sure, will update",
    "commit": "246de3c66c8c5a656b766ccd2f9bb3df12c9da0d",
    "createdAt": "2019-11-15T21:46:46Z",
    "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.resource\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.resource.ResourceUtils.RESOURCE_DOT\n+\n+/**\n+ * An Executor resource request. This is used in conjunction with the ResourceProfile to\n+ * programmatically specify the resources needed for an RDD that will be applied at the\n+ * stage level.\n+ *\n+ * This is used to specify what the resource requirements are for an Executor and how\n+ * Spark can find out specific details about those resources. Not all the parameters are\n+ * required for every resource type. The resources names supported\n+ * correspond to the regular Spark configs with the prefix removed. For instance overhead\n+ * memory in this api is memoryOverhead, which is spark.executor.memoryOverhead with\n+ * spark.executor removed. Resources like GPUs are resource.gpu\n+ * (spark configs spark.executor.resource.gpu.*). The amount, discoveryScript, and vendor\n+ * parameters for resources are all the same parameters a user would specify through the\n+ * configs: spark.executor.resource.{resourceName}.{amount, discoveryScript, vendor}.\n+ *\n+ * For instance, a user wants to allocate an Executor with GPU resources on YARN. The user has\n+ * to specify the resource name (resource.gpu), the amount or number of GPUs per Executor,\n+ * the discovery script would be specified so that when the Executor starts up it can\n+ * discovery what GPU addresses are available for it to use because YARN doesn't tell\n+ * Spark that, then vendor would not be used because its specific for Kubernetes.\n+ *\n+ * See the configuration and cluster specific docs for more details.\n+ *\n+ * Use ExecutorResourceRequests class as a convenience API.\n+ *\n+ * @param resourceName Name of the resource\n+ * @param amount Amount requesting\n+ * @param discoveryScript Optional script used to discover the resources. This is required on some\n+ *                        cluster managers that don't tell Spark the addresses of the resources\n+ *                        allocated. The script runs on Executors startup to discover the addresses\n+ *                        of the resources available.\n+ * @param vendor Optional vendor, required for some cluster managers\n+ *\n+ * This api is currently private until the rest of the pieces are in place and then it\n+ * will become public.\n+ */\n+private[spark] class ExecutorResourceRequest(\n+    val resourceName: String,\n+    val amount: Long,\n+    val discoveryScript: String = \"\",\n+    val vendor: String = \"\") extends Serializable {\n+\n+  private val allowedExecutorResources = mutable.HashSet[String](",
    "line": 68
  }],
  "prId": 26284
}, {
  "comments": [{
    "author": {
      "login": "holdenk"
    },
    "body": "From looking at the design I thin the discoveryScript seems to need to be resident on the executors already, maybe call that out since another option would have been to broadcast the discovery script.",
    "commit": "246de3c66c8c5a656b766ccd2f9bb3df12c9da0d",
    "createdAt": "2019-11-15T17:30:18Z",
    "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.resource\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.resource.ResourceUtils.RESOURCE_DOT\n+\n+/**\n+ * An Executor resource request. This is used in conjunction with the ResourceProfile to\n+ * programmatically specify the resources needed for an RDD that will be applied at the\n+ * stage level.\n+ *\n+ * This is used to specify what the resource requirements are for an Executor and how\n+ * Spark can find out specific details about those resources. Not all the parameters are\n+ * required for every resource type. The resources names supported\n+ * correspond to the regular Spark configs with the prefix removed. For instance overhead\n+ * memory in this api is memoryOverhead, which is spark.executor.memoryOverhead with\n+ * spark.executor removed. Resources like GPUs are resource.gpu\n+ * (spark configs spark.executor.resource.gpu.*). The amount, discoveryScript, and vendor\n+ * parameters for resources are all the same parameters a user would specify through the\n+ * configs: spark.executor.resource.{resourceName}.{amount, discoveryScript, vendor}.\n+ *\n+ * For instance, a user wants to allocate an Executor with GPU resources on YARN. The user has\n+ * to specify the resource name (resource.gpu), the amount or number of GPUs per Executor,\n+ * the discovery script would be specified so that when the Executor starts up it can\n+ * discovery what GPU addresses are available for it to use because YARN doesn't tell\n+ * Spark that, then vendor would not be used because its specific for Kubernetes.\n+ *\n+ * See the configuration and cluster specific docs for more details.\n+ *\n+ * Use ExecutorResourceRequests class as a convenience API.\n+ *\n+ * @param resourceName Name of the resource\n+ * @param amount Amount requesting\n+ * @param discoveryScript Optional script used to discover the resources. This is required on some",
    "line": 51
  }, {
    "author": {
      "login": "tgravescs"
    },
    "body": "it depends on the cluster manager. yarn doesn't need it resident as it will ship it and it will be there before startup, standalone mode does.  You can't broadcast it as it needs to be there at process startup unless I'm missing what you are saying",
    "commit": "246de3c66c8c5a656b766ccd2f9bb3df12c9da0d",
    "createdAt": "2019-11-15T21:44:19Z",
    "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.resource\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.resource.ResourceUtils.RESOURCE_DOT\n+\n+/**\n+ * An Executor resource request. This is used in conjunction with the ResourceProfile to\n+ * programmatically specify the resources needed for an RDD that will be applied at the\n+ * stage level.\n+ *\n+ * This is used to specify what the resource requirements are for an Executor and how\n+ * Spark can find out specific details about those resources. Not all the parameters are\n+ * required for every resource type. The resources names supported\n+ * correspond to the regular Spark configs with the prefix removed. For instance overhead\n+ * memory in this api is memoryOverhead, which is spark.executor.memoryOverhead with\n+ * spark.executor removed. Resources like GPUs are resource.gpu\n+ * (spark configs spark.executor.resource.gpu.*). The amount, discoveryScript, and vendor\n+ * parameters for resources are all the same parameters a user would specify through the\n+ * configs: spark.executor.resource.{resourceName}.{amount, discoveryScript, vendor}.\n+ *\n+ * For instance, a user wants to allocate an Executor with GPU resources on YARN. The user has\n+ * to specify the resource name (resource.gpu), the amount or number of GPUs per Executor,\n+ * the discovery script would be specified so that when the Executor starts up it can\n+ * discovery what GPU addresses are available for it to use because YARN doesn't tell\n+ * Spark that, then vendor would not be used because its specific for Kubernetes.\n+ *\n+ * See the configuration and cluster specific docs for more details.\n+ *\n+ * Use ExecutorResourceRequests class as a convenience API.\n+ *\n+ * @param resourceName Name of the resource\n+ * @param amount Amount requesting\n+ * @param discoveryScript Optional script used to discover the resources. This is required on some",
    "line": 51
  }],
  "prId": 26284
}]