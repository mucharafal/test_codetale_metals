[{
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "I'm not 100% confident that the wrapping / delegation is handled properly in all of these close methods, so I'm going to take a self-review pass to try to figure that out.\n",
    "commit": "0fee6042a7cff69747f454056fd5cec6640dc381",
    "createdAt": "2016-03-03T19:26:49Z",
    "diffHunk": "@@ -0,0 +1,125 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.storage.disk\n+\n+import java.io.{File, OutputStream}\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.serializer.{SerializationStream, SerializerInstance}\n+import org.apache.spark.storage.BlockId\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A class for writing JVM objects directly to a file on disk. This class allows data to be appended\n+ * to an existing block and can guarantee atomicity in the case of faults as it allows the caller to\n+ * revert partial writes.\n+ *\n+ * This class does not support concurrent writes. Also, once the writer has been opened it cannot be\n+ * reopened again.\n+ */\n+private[spark] class DiskBlockObjectWriter(\n+    diskBlockWriter: DiskBlockWriter,\n+    serializerInstance: SerializerInstance,\n+    val blockId: BlockId = null)\n+  extends Logging {\n+\n+  private var bs: OutputStream = null\n+  private var objOut: SerializationStream = null\n+  private var initialized = false\n+  private var hasBeenClosed = false\n+  private var commitAndCloseHasBeenCalled = false\n+\n+  def file: File = diskBlockWriter.file\n+\n+  def open(): DiskBlockObjectWriter = {\n+    if (hasBeenClosed) {\n+      throw new IllegalStateException(\"Writer already closed. Cannot be reopened.\")\n+    }\n+    bs = diskBlockWriter.open()\n+    objOut = serializerInstance.serializeStream(bs)\n+    initialized = true\n+    this\n+  }\n+\n+  def close() {\n+    if (initialized) {\n+      Utils.tryWithSafeFinally {\n+        if (diskBlockWriter.syncWrites) {\n+          objOut.flush()\n+        }\n+      } {\n+        objOut.close()\n+      }\n+      bs = null\n+      objOut = null\n+      initialized = false\n+      hasBeenClosed = true\n+    }\n+  }\n+\n+  /**\n+   * Flush the partial writes and commit them as a single atomic block.\n+   */\n+  def commitAndClose(): Unit = {\n+    if (initialized) {\n+      objOut.flush()\n+    }\n+    diskBlockWriter.commitAndClose()\n+    commitAndCloseHasBeenCalled = true\n+  }\n+\n+\n+  /**\n+   * Reverts writes that haven't been flushed yet. Callers should invoke this function\n+   * when there are runtime exceptions. This method will not throw, though it may be\n+   * unsuccessful in truncating written data.\n+   *\n+   * @return the file that this DiskBlockObjectWriter wrote to.\n+   */\n+  def revertPartialWritesAndClose(): File = {\n+    if (initialized) {\n+      objOut.flush()\n+    }\n+    diskBlockWriter.revertPartialWritesAndClose()"
  }],
  "prId": 11498
}, {
  "comments": [{
    "author": {
      "login": "davies"
    },
    "body": "Existing: Should these in the final clause?\n",
    "commit": "0fee6042a7cff69747f454056fd5cec6640dc381",
    "createdAt": "2016-03-22T18:54:08Z",
    "diffHunk": "@@ -0,0 +1,125 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.storage.disk\n+\n+import java.io.{File, OutputStream}\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.serializer.{SerializationStream, SerializerInstance}\n+import org.apache.spark.storage.BlockId\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A class for writing JVM objects directly to a file on disk. This class allows data to be appended\n+ * to an existing block and can guarantee atomicity in the case of faults as it allows the caller to\n+ * revert partial writes.\n+ *\n+ * This class does not support concurrent writes. Also, once the writer has been opened it cannot be\n+ * reopened again.\n+ */\n+private[spark] class DiskBlockObjectWriter(\n+    diskBlockWriter: DiskBlockWriter,\n+    serializerInstance: SerializerInstance,\n+    val blockId: BlockId = null)\n+  extends Logging {\n+\n+  private var bs: OutputStream = null\n+  private var objOut: SerializationStream = null\n+  private var initialized = false\n+  private var hasBeenClosed = false\n+  private var commitAndCloseHasBeenCalled = false\n+\n+  def file: File = diskBlockWriter.file\n+\n+  def open(): DiskBlockObjectWriter = {\n+    if (hasBeenClosed) {\n+      throw new IllegalStateException(\"Writer already closed. Cannot be reopened.\")\n+    }\n+    bs = diskBlockWriter.open()\n+    objOut = serializerInstance.serializeStream(bs)\n+    initialized = true\n+    this\n+  }\n+\n+  def close() {\n+    if (initialized) {\n+      Utils.tryWithSafeFinally {\n+        if (diskBlockWriter.syncWrites) {\n+          objOut.flush()\n+        }\n+      } {\n+        objOut.close()\n+      }\n+      bs = null\n+      objOut = null\n+      initialized = false\n+      hasBeenClosed = true"
  }],
  "prId": 11498
}, {
  "comments": [{
    "author": {
      "login": "davies"
    },
    "body": "It maybe easier to understand if we use diskBlockWriter instead of this `bs`.\n",
    "commit": "0fee6042a7cff69747f454056fd5cec6640dc381",
    "createdAt": "2016-03-22T18:55:58Z",
    "diffHunk": "@@ -0,0 +1,125 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.storage.disk\n+\n+import java.io.{File, OutputStream}\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.serializer.{SerializationStream, SerializerInstance}\n+import org.apache.spark.storage.BlockId\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A class for writing JVM objects directly to a file on disk. This class allows data to be appended\n+ * to an existing block and can guarantee atomicity in the case of faults as it allows the caller to\n+ * revert partial writes.\n+ *\n+ * This class does not support concurrent writes. Also, once the writer has been opened it cannot be\n+ * reopened again.\n+ */\n+private[spark] class DiskBlockObjectWriter(\n+    diskBlockWriter: DiskBlockWriter,\n+    serializerInstance: SerializerInstance,\n+    val blockId: BlockId = null)\n+  extends Logging {\n+\n+  private var bs: OutputStream = null"
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "Agreed; good catch.\n",
    "commit": "0fee6042a7cff69747f454056fd5cec6640dc381",
    "createdAt": "2016-03-23T01:21:30Z",
    "diffHunk": "@@ -0,0 +1,125 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.storage.disk\n+\n+import java.io.{File, OutputStream}\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.serializer.{SerializationStream, SerializerInstance}\n+import org.apache.spark.storage.BlockId\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A class for writing JVM objects directly to a file on disk. This class allows data to be appended\n+ * to an existing block and can guarantee atomicity in the case of faults as it allows the caller to\n+ * revert partial writes.\n+ *\n+ * This class does not support concurrent writes. Also, once the writer has been opened it cannot be\n+ * reopened again.\n+ */\n+private[spark] class DiskBlockObjectWriter(\n+    diskBlockWriter: DiskBlockWriter,\n+    serializerInstance: SerializerInstance,\n+    val blockId: BlockId = null)\n+  extends Logging {\n+\n+  private var bs: OutputStream = null"
  }],
  "prId": 11498
}, {
  "comments": [{
    "author": {
      "login": "davies"
    },
    "body": "Should we catch the exception here? (to have the same behavior as before)\n",
    "commit": "0fee6042a7cff69747f454056fd5cec6640dc381",
    "createdAt": "2016-03-22T18:58:44Z",
    "diffHunk": "@@ -0,0 +1,125 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.storage.disk\n+\n+import java.io.{File, OutputStream}\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.serializer.{SerializationStream, SerializerInstance}\n+import org.apache.spark.storage.BlockId\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A class for writing JVM objects directly to a file on disk. This class allows data to be appended\n+ * to an existing block and can guarantee atomicity in the case of faults as it allows the caller to\n+ * revert partial writes.\n+ *\n+ * This class does not support concurrent writes. Also, once the writer has been opened it cannot be\n+ * reopened again.\n+ */\n+private[spark] class DiskBlockObjectWriter(\n+    diskBlockWriter: DiskBlockWriter,\n+    serializerInstance: SerializerInstance,\n+    val blockId: BlockId = null)\n+  extends Logging {\n+\n+  private var bs: OutputStream = null\n+  private var objOut: SerializationStream = null\n+  private var initialized = false\n+  private var hasBeenClosed = false\n+  private var commitAndCloseHasBeenCalled = false\n+\n+  def file: File = diskBlockWriter.file\n+\n+  def open(): DiskBlockObjectWriter = {\n+    if (hasBeenClosed) {\n+      throw new IllegalStateException(\"Writer already closed. Cannot be reopened.\")\n+    }\n+    bs = diskBlockWriter.open()\n+    objOut = serializerInstance.serializeStream(bs)\n+    initialized = true\n+    this\n+  }\n+\n+  def close() {\n+    if (initialized) {\n+      Utils.tryWithSafeFinally {\n+        if (diskBlockWriter.syncWrites) {\n+          objOut.flush()\n+        }\n+      } {\n+        objOut.close()\n+      }\n+      bs = null\n+      objOut = null\n+      initialized = false\n+      hasBeenClosed = true\n+    }\n+  }\n+\n+  /**\n+   * Flush the partial writes and commit them as a single atomic block.\n+   */\n+  def commitAndClose(): Unit = {\n+    if (initialized) {\n+      objOut.flush()\n+    }\n+    diskBlockWriter.commitAndClose()\n+    commitAndCloseHasBeenCalled = true\n+  }\n+\n+\n+  /**\n+   * Reverts writes that haven't been flushed yet. Callers should invoke this function\n+   * when there are runtime exceptions. This method will not throw, though it may be\n+   * unsuccessful in truncating written data.\n+   *\n+   * @return the file that this DiskBlockObjectWriter wrote to.\n+   */\n+  def revertPartialWritesAndClose(): File = {\n+    if (initialized) {\n+      objOut.flush()"
  }],
  "prId": 11498
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "Note that we don't explicitly close `objOut` here. Neither did the old code, which is why I preserved that behavior after the split.\n",
    "commit": "0fee6042a7cff69747f454056fd5cec6640dc381",
    "createdAt": "2016-03-23T01:33:40Z",
    "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.storage.disk\n+\n+import java.io.File\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.serializer.{SerializationStream, SerializerInstance}\n+import org.apache.spark.storage.BlockId\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A class for writing JVM objects directly to a file on disk. This class allows data to be appended\n+ * to an existing block and can guarantee atomicity in the case of faults as it allows the caller to\n+ * revert partial writes.\n+ *\n+ * This class does not support concurrent writes. Also, once the writer has been opened it cannot be\n+ * reopened again.\n+ */\n+private[spark] class DiskBlockObjectWriter(\n+    private[this] var diskBlockWriter: DiskBlockWriter,\n+    serializerInstance: SerializerInstance,\n+    val blockId: BlockId = null)\n+  extends Logging {\n+\n+  private var objOut: SerializationStream = null\n+  private var initialized = false\n+  private var hasBeenClosed = false\n+  private var commitAndCloseHasBeenCalled = false\n+\n+  def file: File = diskBlockWriter.file\n+\n+  def open(): DiskBlockObjectWriter = {\n+    if (hasBeenClosed) {\n+      throw new IllegalStateException(\"Writer already closed. Cannot be reopened.\")\n+    }\n+    diskBlockWriter.open()\n+    objOut = serializerInstance.serializeStream(diskBlockWriter)\n+    initialized = true\n+    this\n+  }\n+\n+  def close() {\n+    Utils.tryWithSafeFinally {\n+      if (initialized) {\n+        if (diskBlockWriter.syncWrites) {\n+          objOut.flush()\n+        }\n+      }\n+    } {\n+      if (initialized) {\n+        objOut.close()\n+      }\n+      diskBlockWriter = null\n+      objOut = null\n+      initialized = false\n+      hasBeenClosed = true\n+    }\n+  }\n+\n+  /**\n+   * Flush the partial writes and commit them as a single atomic block.\n+   */\n+  def commitAndClose(): Unit = {\n+    if (initialized) {\n+      objOut.flush()",
    "line": 81
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "Update: actually, the old code called `objOut.close()` as part of `close()`, so we still need to figure out how to make sure that happens here :( \n",
    "commit": "0fee6042a7cff69747f454056fd5cec6640dc381",
    "createdAt": "2016-03-23T01:34:43Z",
    "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.storage.disk\n+\n+import java.io.File\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.serializer.{SerializationStream, SerializerInstance}\n+import org.apache.spark.storage.BlockId\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A class for writing JVM objects directly to a file on disk. This class allows data to be appended\n+ * to an existing block and can guarantee atomicity in the case of faults as it allows the caller to\n+ * revert partial writes.\n+ *\n+ * This class does not support concurrent writes. Also, once the writer has been opened it cannot be\n+ * reopened again.\n+ */\n+private[spark] class DiskBlockObjectWriter(\n+    private[this] var diskBlockWriter: DiskBlockWriter,\n+    serializerInstance: SerializerInstance,\n+    val blockId: BlockId = null)\n+  extends Logging {\n+\n+  private var objOut: SerializationStream = null\n+  private var initialized = false\n+  private var hasBeenClosed = false\n+  private var commitAndCloseHasBeenCalled = false\n+\n+  def file: File = diskBlockWriter.file\n+\n+  def open(): DiskBlockObjectWriter = {\n+    if (hasBeenClosed) {\n+      throw new IllegalStateException(\"Writer already closed. Cannot be reopened.\")\n+    }\n+    diskBlockWriter.open()\n+    objOut = serializerInstance.serializeStream(diskBlockWriter)\n+    initialized = true\n+    this\n+  }\n+\n+  def close() {\n+    Utils.tryWithSafeFinally {\n+      if (initialized) {\n+        if (diskBlockWriter.syncWrites) {\n+          objOut.flush()\n+        }\n+      }\n+    } {\n+      if (initialized) {\n+        objOut.close()\n+      }\n+      diskBlockWriter = null\n+      objOut = null\n+      initialized = false\n+      hasBeenClosed = true\n+    }\n+  }\n+\n+  /**\n+   * Flush the partial writes and commit them as a single atomic block.\n+   */\n+  def commitAndClose(): Unit = {\n+    if (initialized) {\n+      objOut.flush()",
    "line": 81
  }],
  "prId": 11498
}]