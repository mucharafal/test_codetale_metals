[{
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "> How will it look if one of the 2 copies of the blocks fall off from the executor?\n\n`2x` will become `1x`. When one of 2 copies is removed, an update info with `StorageLevel.None` will be sent. Then `removeBlockFromBlockManager` will create a new `StorageLevel` with `replication = 1` for this block.\n",
    "commit": "df2c1d8579e3643e646a1f37534cc4cfb2c7db54",
    "createdAt": "2015-06-05T23:35:50Z",
    "diffHunk": "@@ -0,0 +1,125 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.storage\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.scheduler._\n+\n+private[spark] case class BlockUIData(\n+    blockId: BlockId,\n+    storageLevel: StorageLevel,\n+    memSize: Long,\n+    diskSize: Long,\n+    externalBlockStoreSize: Long,\n+    locations: Set[String])\n+\n+private[spark] class BlockStatusListener extends SparkListener {\n+\n+  private val blockManagers = new mutable.HashMap[BlockManagerId, mutable.HashSet[BlockId]]\n+  private val blocks = new mutable.HashMap[BlockId, BlockUIData]\n+\n+  override def onBlockUpdated(blockUpdated: SparkListenerBlockUpdated): Unit = {\n+    val blockManagerId = blockUpdated.updateBlockInfo.blockManagerId\n+    val blockId = blockUpdated.updateBlockInfo.blockId\n+    val storageLevel = blockUpdated.updateBlockInfo.storageLevel\n+    val memSize = blockUpdated.updateBlockInfo.memSize\n+    val diskSize = blockUpdated.updateBlockInfo.diskSize\n+    val externalBlockStoreSize = blockUpdated.updateBlockInfo.externalBlockStoreSize\n+\n+    synchronized {\n+      // Drop the update info if the block manager is not registered\n+      blockManagers.get(blockManagerId) foreach { blocksInBlockManager =>\n+        if (storageLevel.isValid) {\n+          blocksInBlockManager.add(blockId)\n+          val location = s\"${blockManagerId.hostPort} / ${blockManagerId.executorId}\"\n+          val newLocations =\n+            blocks.get(blockId).map(_.locations).getOrElse(Set.empty) + location\n+          val newStorageLevel = StorageLevel(\n+            useDisk = diskSize > 0,\n+            useMemory = memSize > 0,\n+            useOffHeap = externalBlockStoreSize > 0,\n+            deserialized = storageLevel.deserialized,\n+            replication = newLocations.size\n+          )\n+          blocks.put(blockId,\n+            BlockUIData(\n+              blockId,\n+              newStorageLevel,\n+              memSize,\n+              diskSize,\n+              externalBlockStoreSize,\n+              newLocations))\n+        } else {\n+          // If isValid is not true, it means we should drop the block.\n+          blocksInBlockManager -= blockId\n+          removeBlockFromBlockManager(blockId, blockManagerId)"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "That's cool then. Will take a more detailed look at the code soon.\n\nOn Fri, Jun 5, 2015 at 4:36 PM, Shixiong Zhu notifications@github.com\nwrote:\n\n> In core/src/main/scala/org/apache/spark/storage/BlockStatusListener.scala\n> https://github.com/apache/spark/pull/6672#discussion_r31860010:\n> \n> > -            useOffHeap = externalBlockStoreSize > 0,\n> > -            deserialized = storageLevel.deserialized,\n> > -            replication = newLocations.size\n> > -          )\n> > -          blocks.put(blockId,\n> > -            BlockUIData(\n> > -              blockId,\n> > -              newStorageLevel,\n> > -              memSize,\n> > -              diskSize,\n> > -              externalBlockStoreSize,\n> > -              newLocations))\n> > -        } else {\n> > -          // If isValid is not true, it means we should drop the block.\n> > -          blocksInBlockManager -= blockId\n> > -          removeBlockFromBlockManager(blockId, blockManagerId)\n> \n>  How will it look if one of the 2 copies of the blocks fall off from the\n> executor?\n> \n> 2x will become 1x. When one of 2 copies is removed, an update info with\n> StorageLevel.None will be sent. Then removeBlockFromBlockManager will\n> create a new StorageLevel with replication = 1 for this block.\n> \n> â€”\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/spark/pull/6672/files#r31860010.\n",
    "commit": "df2c1d8579e3643e646a1f37534cc4cfb2c7db54",
    "createdAt": "2015-06-06T02:54:07Z",
    "diffHunk": "@@ -0,0 +1,125 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.storage\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.scheduler._\n+\n+private[spark] case class BlockUIData(\n+    blockId: BlockId,\n+    storageLevel: StorageLevel,\n+    memSize: Long,\n+    diskSize: Long,\n+    externalBlockStoreSize: Long,\n+    locations: Set[String])\n+\n+private[spark] class BlockStatusListener extends SparkListener {\n+\n+  private val blockManagers = new mutable.HashMap[BlockManagerId, mutable.HashSet[BlockId]]\n+  private val blocks = new mutable.HashMap[BlockId, BlockUIData]\n+\n+  override def onBlockUpdated(blockUpdated: SparkListenerBlockUpdated): Unit = {\n+    val blockManagerId = blockUpdated.updateBlockInfo.blockManagerId\n+    val blockId = blockUpdated.updateBlockInfo.blockId\n+    val storageLevel = blockUpdated.updateBlockInfo.storageLevel\n+    val memSize = blockUpdated.updateBlockInfo.memSize\n+    val diskSize = blockUpdated.updateBlockInfo.diskSize\n+    val externalBlockStoreSize = blockUpdated.updateBlockInfo.externalBlockStoreSize\n+\n+    synchronized {\n+      // Drop the update info if the block manager is not registered\n+      blockManagers.get(blockManagerId) foreach { blocksInBlockManager =>\n+        if (storageLevel.isValid) {\n+          blocksInBlockManager.add(blockId)\n+          val location = s\"${blockManagerId.hostPort} / ${blockManagerId.executorId}\"\n+          val newLocations =\n+            blocks.get(blockId).map(_.locations).getOrElse(Set.empty) + location\n+          val newStorageLevel = StorageLevel(\n+            useDisk = diskSize > 0,\n+            useMemory = memSize > 0,\n+            useOffHeap = externalBlockStoreSize > 0,\n+            deserialized = storageLevel.deserialized,\n+            replication = newLocations.size\n+          )\n+          blocks.put(blockId,\n+            BlockUIData(\n+              blockId,\n+              newStorageLevel,\n+              memSize,\n+              diskSize,\n+              externalBlockStoreSize,\n+              newLocations))\n+        } else {\n+          // If isValid is not true, it means we should drop the block.\n+          blocksInBlockManager -= blockId\n+          removeBlockFromBlockManager(blockId, blockManagerId)"
  }],
  "prId": 6672
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: please add a `{` in this line.\n",
    "commit": "df2c1d8579e3643e646a1f37534cc4cfb2c7db54",
    "createdAt": "2015-06-20T01:24:16Z",
    "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.storage\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.scheduler._\n+\n+private[spark] case class BlockUIData(\n+    blockId: BlockId,\n+    storageLevel: StorageLevel,\n+    memSize: Long,\n+    diskSize: Long,\n+    externalBlockStoreSize: Long,\n+    locations: Set[String])\n+\n+private[spark] class BlockStatusListener extends SparkListener {\n+\n+  private val blockManagers = new mutable.HashMap[BlockManagerId, mutable.HashSet[BlockId]]\n+  private val blocks = new mutable.HashMap[BlockId, BlockUIData]\n+\n+  override def onBlockUpdated(blockUpdated: SparkListenerBlockUpdated): Unit = {\n+    val blockId = blockUpdated.updateBlockInfo.blockId\n+    if (!blockId.isInstanceOf[StreamBlockId]) {\n+      // Now we only monitor StreamBlocks\n+      return\n+    }\n+    val blockManagerId = blockUpdated.updateBlockInfo.blockManagerId\n+    val storageLevel = blockUpdated.updateBlockInfo.storageLevel\n+    val memSize = blockUpdated.updateBlockInfo.memSize\n+    val diskSize = blockUpdated.updateBlockInfo.diskSize\n+    val externalBlockStoreSize = blockUpdated.updateBlockInfo.externalBlockStoreSize\n+\n+    synchronized {\n+      // Drop the update info if the block manager is not registered\n+      blockManagers.get(blockManagerId) foreach { blocksInBlockManager =>\n+        if (storageLevel.isValid) {\n+          blocksInBlockManager.add(blockId)\n+          val location = blockManagerId.hostPort\n+          val newLocations =\n+            blocks.get(blockId).map(_.locations).getOrElse(Set.empty) + location\n+          val newStorageLevel = StorageLevel(\n+            useDisk = diskSize > 0,\n+            useMemory = memSize > 0,\n+            useOffHeap = externalBlockStoreSize > 0,\n+            deserialized = storageLevel.deserialized,\n+            replication = newLocations.size\n+          )\n+          blocks.put(blockId,\n+            BlockUIData(\n+              blockId,\n+              newStorageLevel,\n+              memSize,\n+              diskSize,\n+              externalBlockStoreSize,\n+              newLocations))\n+        } else {\n+          // If isValid is not true, it means we should drop the block.\n+          blocksInBlockManager -= blockId\n+          removeBlockFromBlockManager(blockId, blockManagerId)\n+        }\n+      }\n+    }\n+  }\n+\n+  override def onBlockManagerAdded(blockManagerAdded: SparkListenerBlockManagerAdded): Unit ="
  }],
  "prId": 6672
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "Style nit: should have a `.` before `foreach`\n",
    "commit": "df2c1d8579e3643e646a1f37534cc4cfb2c7db54",
    "createdAt": "2015-06-23T17:27:28Z",
    "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.storage\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.scheduler._\n+\n+private[spark] case class BlockUIData(\n+    blockId: BlockId,\n+    storageLevel: StorageLevel,\n+    memSize: Long,\n+    diskSize: Long,\n+    externalBlockStoreSize: Long,\n+    locations: Set[String])\n+\n+private[spark] class BlockStatusListener extends SparkListener {\n+\n+  private val blockManagers = new mutable.HashMap[BlockManagerId, mutable.HashSet[BlockId]]\n+  private val blocks = new mutable.HashMap[BlockId, BlockUIData]\n+\n+  override def onBlockUpdated(blockUpdated: SparkListenerBlockUpdated): Unit = {\n+    val blockId = blockUpdated.blockUpdatedInfo.blockId\n+    if (!blockId.isInstanceOf[StreamBlockId]) {\n+      // Now we only monitor StreamBlocks\n+      return\n+    }\n+    val blockManagerId = blockUpdated.blockUpdatedInfo.blockManagerId\n+    val storageLevel = blockUpdated.blockUpdatedInfo.storageLevel\n+    val memSize = blockUpdated.blockUpdatedInfo.memSize\n+    val diskSize = blockUpdated.blockUpdatedInfo.diskSize\n+    val externalBlockStoreSize = blockUpdated.blockUpdatedInfo.externalBlockStoreSize\n+\n+    synchronized {\n+      // Drop the update info if the block manager is not registered\n+      blockManagers.get(blockManagerId) foreach { blocksInBlockManager =>"
  }],
  "prId": 6672
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "Same style nit here regarding infix notation.\n",
    "commit": "df2c1d8579e3643e646a1f37534cc4cfb2c7db54",
    "createdAt": "2015-06-23T17:29:06Z",
    "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.storage\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.scheduler._\n+\n+private[spark] case class BlockUIData(\n+    blockId: BlockId,\n+    storageLevel: StorageLevel,\n+    memSize: Long,\n+    diskSize: Long,\n+    externalBlockStoreSize: Long,\n+    locations: Set[String])\n+\n+private[spark] class BlockStatusListener extends SparkListener {\n+\n+  private val blockManagers = new mutable.HashMap[BlockManagerId, mutable.HashSet[BlockId]]\n+  private val blocks = new mutable.HashMap[BlockId, BlockUIData]\n+\n+  override def onBlockUpdated(blockUpdated: SparkListenerBlockUpdated): Unit = {\n+    val blockId = blockUpdated.blockUpdatedInfo.blockId\n+    if (!blockId.isInstanceOf[StreamBlockId]) {\n+      // Now we only monitor StreamBlocks\n+      return\n+    }\n+    val blockManagerId = blockUpdated.blockUpdatedInfo.blockManagerId\n+    val storageLevel = blockUpdated.blockUpdatedInfo.storageLevel\n+    val memSize = blockUpdated.blockUpdatedInfo.memSize\n+    val diskSize = blockUpdated.blockUpdatedInfo.diskSize\n+    val externalBlockStoreSize = blockUpdated.blockUpdatedInfo.externalBlockStoreSize\n+\n+    synchronized {\n+      // Drop the update info if the block manager is not registered\n+      blockManagers.get(blockManagerId) foreach { blocksInBlockManager =>\n+        if (storageLevel.isValid) {\n+          blocksInBlockManager.add(blockId)\n+          val location = blockManagerId.hostPort\n+          val newLocations =\n+            blocks.get(blockId).map(_.locations).getOrElse(Set.empty) + location\n+          val newStorageLevel = StorageLevel(\n+            useDisk = diskSize > 0,\n+            useMemory = memSize > 0,\n+            useOffHeap = externalBlockStoreSize > 0,\n+            deserialized = storageLevel.deserialized,\n+            replication = newLocations.size\n+          )\n+          blocks.put(blockId,\n+            BlockUIData(\n+              blockId,\n+              newStorageLevel,\n+              memSize,\n+              diskSize,\n+              externalBlockStoreSize,\n+              newLocations))\n+        } else {\n+          // If isValid is not true, it means we should drop the block.\n+          blocksInBlockManager -= blockId\n+          removeBlockFromBlockManager(blockId, blockManagerId)\n+        }\n+      }\n+    }\n+  }\n+\n+  override def onBlockManagerAdded(blockManagerAdded: SparkListenerBlockManagerAdded): Unit = {\n+    synchronized {\n+      blockManagers.put(blockManagerAdded.blockManagerId, mutable.HashSet())\n+    }\n+  }\n+\n+  override def onBlockManagerRemoved(\n+      blockManagerRemoved: SparkListenerBlockManagerRemoved): Unit = {\n+    val blockManagerId = blockManagerRemoved.blockManagerId\n+    synchronized {\n+      blockManagers.remove(blockManagerId) foreach { blockIds =>"
  }],
  "prId": 6672
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "I think it is worth putting this method inside synchronized so that someone in future does not accidentally call this method without synchronizing.\n",
    "commit": "df2c1d8579e3643e646a1f37534cc4cfb2c7db54",
    "createdAt": "2015-07-02T00:20:19Z",
    "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.storage\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.scheduler._\n+\n+private[spark] case class BlockUIData(\n+    blockId: BlockId,\n+    storageLevel: StorageLevel,\n+    memSize: Long,\n+    diskSize: Long,\n+    externalBlockStoreSize: Long,\n+    locations: Set[String])\n+\n+private[spark] class BlockStatusListener extends SparkListener {\n+\n+  private val blockManagers = new mutable.HashMap[BlockManagerId, mutable.HashSet[BlockId]]\n+  private val blocks = new mutable.HashMap[BlockId, BlockUIData]\n+\n+  override def onBlockUpdated(blockUpdated: SparkListenerBlockUpdated): Unit = {\n+    val blockId = blockUpdated.blockUpdatedInfo.blockId\n+    if (!blockId.isInstanceOf[StreamBlockId]) {\n+      // Now we only monitor StreamBlocks\n+      return\n+    }\n+    val blockManagerId = blockUpdated.blockUpdatedInfo.blockManagerId\n+    val storageLevel = blockUpdated.blockUpdatedInfo.storageLevel\n+    val memSize = blockUpdated.blockUpdatedInfo.memSize\n+    val diskSize = blockUpdated.blockUpdatedInfo.diskSize\n+    val externalBlockStoreSize = blockUpdated.blockUpdatedInfo.externalBlockStoreSize\n+\n+    synchronized {\n+      // Drop the update info if the block manager is not registered\n+      blockManagers.get(blockManagerId).foreach { blocksInBlockManager =>\n+        if (storageLevel.isValid) {\n+          blocksInBlockManager.add(blockId)\n+          val location = blockManagerId.hostPort\n+          val newLocations =\n+            blocks.get(blockId).map(_.locations).getOrElse(Set.empty) + location\n+          val newStorageLevel = StorageLevel(\n+            useDisk = diskSize > 0,\n+            useMemory = memSize > 0,\n+            useOffHeap = externalBlockStoreSize > 0,\n+            deserialized = storageLevel.deserialized,\n+            replication = newLocations.size\n+          )\n+          blocks.put(blockId,\n+            BlockUIData(\n+              blockId,\n+              newStorageLevel,\n+              memSize,\n+              diskSize,\n+              externalBlockStoreSize,\n+              newLocations))\n+        } else {\n+          // If isValid is not true, it means we should drop the block.\n+          blocksInBlockManager -= blockId\n+          removeBlockFromBlockManager(blockId, blockManagerId)\n+        }\n+      }\n+    }\n+  }\n+\n+  override def onBlockManagerAdded(blockManagerAdded: SparkListenerBlockManagerAdded): Unit = {\n+    synchronized {\n+      blockManagers.put(blockManagerAdded.blockManagerId, mutable.HashSet())\n+    }\n+  }\n+\n+  override def onBlockManagerRemoved(\n+      blockManagerRemoved: SparkListenerBlockManagerRemoved): Unit = {\n+    val blockManagerId = blockManagerRemoved.blockManagerId\n+    synchronized {\n+      blockManagers.remove(blockManagerId).foreach { blockIds =>\n+        for (blockId <- blockIds) {\n+          removeBlockFromBlockManager(blockId, blockManagerId)\n+        }\n+      }\n+    }\n+  }\n+\n+  private def removeBlockFromBlockManager(\n+      blockId: BlockId, blockManagerId: BlockManagerId): Unit = {"
  }],
  "prId": 6672
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "This set of lines seems duplicated in two methods. May be worth putting into a method to dedup. Also, it will make these 2 methods look less verbose.\n",
    "commit": "df2c1d8579e3643e646a1f37534cc4cfb2c7db54",
    "createdAt": "2015-07-02T00:22:48Z",
    "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.storage\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.scheduler._\n+\n+private[spark] case class BlockUIData(\n+    blockId: BlockId,\n+    storageLevel: StorageLevel,\n+    memSize: Long,\n+    diskSize: Long,\n+    externalBlockStoreSize: Long,\n+    locations: Set[String])\n+\n+private[spark] class BlockStatusListener extends SparkListener {\n+\n+  private val blockManagers = new mutable.HashMap[BlockManagerId, mutable.HashSet[BlockId]]\n+  private val blocks = new mutable.HashMap[BlockId, BlockUIData]\n+\n+  override def onBlockUpdated(blockUpdated: SparkListenerBlockUpdated): Unit = {\n+    val blockId = blockUpdated.blockUpdatedInfo.blockId\n+    if (!blockId.isInstanceOf[StreamBlockId]) {\n+      // Now we only monitor StreamBlocks\n+      return\n+    }\n+    val blockManagerId = blockUpdated.blockUpdatedInfo.blockManagerId\n+    val storageLevel = blockUpdated.blockUpdatedInfo.storageLevel\n+    val memSize = blockUpdated.blockUpdatedInfo.memSize\n+    val diskSize = blockUpdated.blockUpdatedInfo.diskSize\n+    val externalBlockStoreSize = blockUpdated.blockUpdatedInfo.externalBlockStoreSize\n+\n+    synchronized {\n+      // Drop the update info if the block manager is not registered\n+      blockManagers.get(blockManagerId).foreach { blocksInBlockManager =>\n+        if (storageLevel.isValid) {\n+          blocksInBlockManager.add(blockId)\n+          val location = blockManagerId.hostPort\n+          val newLocations =\n+            blocks.get(blockId).map(_.locations).getOrElse(Set.empty) + location\n+          val newStorageLevel = StorageLevel("
  }],
  "prId": 6672
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "toBuffer or toSeq?\n",
    "commit": "df2c1d8579e3643e646a1f37534cc4cfb2c7db54",
    "createdAt": "2015-07-02T00:24:14Z",
    "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.storage\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.scheduler._\n+\n+private[spark] case class BlockUIData(\n+    blockId: BlockId,\n+    storageLevel: StorageLevel,\n+    memSize: Long,\n+    diskSize: Long,\n+    externalBlockStoreSize: Long,\n+    locations: Set[String])\n+\n+private[spark] class BlockStatusListener extends SparkListener {\n+\n+  private val blockManagers = new mutable.HashMap[BlockManagerId, mutable.HashSet[BlockId]]\n+  private val blocks = new mutable.HashMap[BlockId, BlockUIData]\n+\n+  override def onBlockUpdated(blockUpdated: SparkListenerBlockUpdated): Unit = {\n+    val blockId = blockUpdated.blockUpdatedInfo.blockId\n+    if (!blockId.isInstanceOf[StreamBlockId]) {\n+      // Now we only monitor StreamBlocks\n+      return\n+    }\n+    val blockManagerId = blockUpdated.blockUpdatedInfo.blockManagerId\n+    val storageLevel = blockUpdated.blockUpdatedInfo.storageLevel\n+    val memSize = blockUpdated.blockUpdatedInfo.memSize\n+    val diskSize = blockUpdated.blockUpdatedInfo.diskSize\n+    val externalBlockStoreSize = blockUpdated.blockUpdatedInfo.externalBlockStoreSize\n+\n+    synchronized {\n+      // Drop the update info if the block manager is not registered\n+      blockManagers.get(blockManagerId).foreach { blocksInBlockManager =>\n+        if (storageLevel.isValid) {\n+          blocksInBlockManager.add(blockId)\n+          val location = blockManagerId.hostPort\n+          val newLocations =\n+            blocks.get(blockId).map(_.locations).getOrElse(Set.empty) + location\n+          val newStorageLevel = StorageLevel(\n+            useDisk = diskSize > 0,\n+            useMemory = memSize > 0,\n+            useOffHeap = externalBlockStoreSize > 0,\n+            deserialized = storageLevel.deserialized,\n+            replication = newLocations.size\n+          )\n+          blocks.put(blockId,\n+            BlockUIData(\n+              blockId,\n+              newStorageLevel,\n+              memSize,\n+              diskSize,\n+              externalBlockStoreSize,\n+              newLocations))\n+        } else {\n+          // If isValid is not true, it means we should drop the block.\n+          blocksInBlockManager -= blockId\n+          removeBlockFromBlockManager(blockId, blockManagerId)\n+        }\n+      }\n+    }\n+  }\n+\n+  override def onBlockManagerAdded(blockManagerAdded: SparkListenerBlockManagerAdded): Unit = {\n+    synchronized {\n+      blockManagers.put(blockManagerAdded.blockManagerId, mutable.HashSet())\n+    }\n+  }\n+\n+  override def onBlockManagerRemoved(\n+      blockManagerRemoved: SparkListenerBlockManagerRemoved): Unit = {\n+    val blockManagerId = blockManagerRemoved.blockManagerId\n+    synchronized {\n+      blockManagers.remove(blockManagerId).foreach { blockIds =>\n+        for (blockId <- blockIds) {\n+          removeBlockFromBlockManager(blockId, blockManagerId)\n+        }\n+      }\n+    }\n+  }\n+\n+  private def removeBlockFromBlockManager(\n+      blockId: BlockId, blockManagerId: BlockManagerId): Unit = {\n+    val location = blockManagerId.hostPort\n+    blocks.get(blockId) foreach { blockUIData =>\n+      val newLocations = blockUIData.locations - location\n+      if (newLocations.isEmpty) {\n+        // This block is removed from all block managers, so remove it\n+        blocks -= blockId\n+      } else {\n+        val newStorageLevel = StorageLevel(\n+          useDisk = blockUIData.diskSize > 0,\n+          useMemory = blockUIData.memSize > 0,\n+          useOffHeap = blockUIData.externalBlockStoreSize > 0,\n+          deserialized = blockUIData.storageLevel.deserialized,\n+          replication = newLocations.size\n+        )\n+        blocks.put(blockId,\n+          BlockUIData(\n+            blockId,\n+            newStorageLevel,\n+            blockUIData.memSize,\n+            blockUIData.diskSize,\n+            blockUIData.externalBlockStoreSize,\n+            newLocations))\n+      }\n+    }\n+  }\n+\n+  def allBlocks: Seq[BlockUIData] = synchronized {\n+    blocks.values.toBuffer"
  }],
  "prId": 6672
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Why do you need to actually keep track of the block managers and corresponding blockIds in them? If all you need to provide to the UI is all block information (def allBlocks) then what is the need to keep track of block managers? You can just add or remove locations from the BlockUIData in the `blocks` map. Isnt it? \n",
    "commit": "df2c1d8579e3643e646a1f37534cc4cfb2c7db54",
    "createdAt": "2015-07-02T00:35:15Z",
    "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.storage\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.scheduler._\n+\n+private[spark] case class BlockUIData(\n+    blockId: BlockId,\n+    storageLevel: StorageLevel,\n+    memSize: Long,\n+    diskSize: Long,\n+    externalBlockStoreSize: Long,\n+    locations: Set[String])\n+\n+private[spark] class BlockStatusListener extends SparkListener {\n+\n+  private val blockManagers = new mutable.HashMap[BlockManagerId, mutable.HashSet[BlockId]]"
  }, {
    "author": {
      "login": "zsxwing"
    },
    "body": "It would be helpful when a block manager is removed and we need to remove the blocks on this block manager.\n",
    "commit": "df2c1d8579e3643e646a1f37534cc4cfb2c7db54",
    "createdAt": "2015-07-02T14:25:09Z",
    "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.storage\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.scheduler._\n+\n+private[spark] case class BlockUIData(\n+    blockId: BlockId,\n+    storageLevel: StorageLevel,\n+    memSize: Long,\n+    diskSize: Long,\n+    externalBlockStoreSize: Long,\n+    locations: Set[String])\n+\n+private[spark] class BlockStatusListener extends SparkListener {\n+\n+  private val blockManagers = new mutable.HashMap[BlockManagerId, mutable.HashSet[BlockId]]"
  }],
  "prId": 6672
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Actually, isnt this effectively overwriting the previous storage level? What happens of a block was stored in memory is exec1 and then it gets an update that the exec2 adds it in disk. What should be the final storage level combining these two information. \n\nIn fact there should be one distinct storage level per location. Isnt it? May be we should make it clear in the UI what is the level for each location. Otherwise the semantics of combining the multiple levels to show one level is fuzzy. What do you think?\n",
    "commit": "df2c1d8579e3643e646a1f37534cc4cfb2c7db54",
    "createdAt": "2015-07-02T00:37:10Z",
    "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.storage\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.scheduler._\n+\n+private[spark] case class BlockUIData(\n+    blockId: BlockId,\n+    storageLevel: StorageLevel,\n+    memSize: Long,\n+    diskSize: Long,\n+    externalBlockStoreSize: Long,\n+    locations: Set[String])\n+\n+private[spark] class BlockStatusListener extends SparkListener {\n+\n+  private val blockManagers = new mutable.HashMap[BlockManagerId, mutable.HashSet[BlockId]]\n+  private val blocks = new mutable.HashMap[BlockId, BlockUIData]\n+\n+  override def onBlockUpdated(blockUpdated: SparkListenerBlockUpdated): Unit = {\n+    val blockId = blockUpdated.blockUpdatedInfo.blockId\n+    if (!blockId.isInstanceOf[StreamBlockId]) {\n+      // Now we only monitor StreamBlocks\n+      return\n+    }\n+    val blockManagerId = blockUpdated.blockUpdatedInfo.blockManagerId\n+    val storageLevel = blockUpdated.blockUpdatedInfo.storageLevel\n+    val memSize = blockUpdated.blockUpdatedInfo.memSize\n+    val diskSize = blockUpdated.blockUpdatedInfo.diskSize\n+    val externalBlockStoreSize = blockUpdated.blockUpdatedInfo.externalBlockStoreSize\n+\n+    synchronized {\n+      // Drop the update info if the block manager is not registered\n+      blockManagers.get(blockManagerId).foreach { blocksInBlockManager =>\n+        if (storageLevel.isValid) {\n+          blocksInBlockManager.add(blockId)\n+          val location = blockManagerId.hostPort\n+          val newLocations =\n+            blocks.get(blockId).map(_.locations).getOrElse(Set.empty) + location\n+          val newStorageLevel = StorageLevel(\n+            useDisk = diskSize > 0,\n+            useMemory = memSize > 0,\n+            useOffHeap = externalBlockStoreSize > 0,"
  }, {
    "author": {
      "login": "zsxwing"
    },
    "body": "How about displaying like this:\n![screen shot 2015-07-02 at 10 18 44 pm](https://cloud.githubusercontent.com/assets/1000778/8479165/6ccb98d8-2108-11e5-831b-2ba273fcafaf.png)\n\nBut it does not support sorting. If I click to sort some column, it will become like this:\n![screen shot 2015-07-02 at 10 18 52 pm](https://cloud.githubusercontent.com/assets/1000778/8479175/82aa5630-2108-11e5-92b2-d83a4b557fc4.png)\n\nBut I think the sorting feature is very important for this table, so I don't want to disable it. People usually want to find the biggest block.\n\nThoughts?\n",
    "commit": "df2c1d8579e3643e646a1f37534cc4cfb2c7db54",
    "createdAt": "2015-07-02T14:22:44Z",
    "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.storage\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.scheduler._\n+\n+private[spark] case class BlockUIData(\n+    blockId: BlockId,\n+    storageLevel: StorageLevel,\n+    memSize: Long,\n+    diskSize: Long,\n+    externalBlockStoreSize: Long,\n+    locations: Set[String])\n+\n+private[spark] class BlockStatusListener extends SparkListener {\n+\n+  private val blockManagers = new mutable.HashMap[BlockManagerId, mutable.HashSet[BlockId]]\n+  private val blocks = new mutable.HashMap[BlockId, BlockUIData]\n+\n+  override def onBlockUpdated(blockUpdated: SparkListenerBlockUpdated): Unit = {\n+    val blockId = blockUpdated.blockUpdatedInfo.blockId\n+    if (!blockId.isInstanceOf[StreamBlockId]) {\n+      // Now we only monitor StreamBlocks\n+      return\n+    }\n+    val blockManagerId = blockUpdated.blockUpdatedInfo.blockManagerId\n+    val storageLevel = blockUpdated.blockUpdatedInfo.storageLevel\n+    val memSize = blockUpdated.blockUpdatedInfo.memSize\n+    val diskSize = blockUpdated.blockUpdatedInfo.diskSize\n+    val externalBlockStoreSize = blockUpdated.blockUpdatedInfo.externalBlockStoreSize\n+\n+    synchronized {\n+      // Drop the update info if the block manager is not registered\n+      blockManagers.get(blockManagerId).foreach { blocksInBlockManager =>\n+        if (storageLevel.isValid) {\n+          blocksInBlockManager.add(blockId)\n+          val location = blockManagerId.hostPort\n+          val newLocations =\n+            blocks.get(blockId).map(_.locations).getOrElse(Set.empty) + location\n+          val newStorageLevel = StorageLevel(\n+            useDisk = diskSize > 0,\n+            useMemory = memSize > 0,\n+            useOffHeap = externalBlockStoreSize > 0,"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Yikes! Yeah we cant break sorting like that. Hmmm .. we need to think from scratch. Unlike RDDs, blocks can only be in one level per location. So showing \"size in memory\" AND \"size in disk\" does not make sense. It made sense for RDDs because some partitions may be in memory and some may be in disk. Let me thinks whats a better organization.\n",
    "commit": "df2c1d8579e3643e646a1f37534cc4cfb2c7db54",
    "createdAt": "2015-07-02T20:59:57Z",
    "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.storage\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.scheduler._\n+\n+private[spark] case class BlockUIData(\n+    blockId: BlockId,\n+    storageLevel: StorageLevel,\n+    memSize: Long,\n+    diskSize: Long,\n+    externalBlockStoreSize: Long,\n+    locations: Set[String])\n+\n+private[spark] class BlockStatusListener extends SparkListener {\n+\n+  private val blockManagers = new mutable.HashMap[BlockManagerId, mutable.HashSet[BlockId]]\n+  private val blocks = new mutable.HashMap[BlockId, BlockUIData]\n+\n+  override def onBlockUpdated(blockUpdated: SparkListenerBlockUpdated): Unit = {\n+    val blockId = blockUpdated.blockUpdatedInfo.blockId\n+    if (!blockId.isInstanceOf[StreamBlockId]) {\n+      // Now we only monitor StreamBlocks\n+      return\n+    }\n+    val blockManagerId = blockUpdated.blockUpdatedInfo.blockManagerId\n+    val storageLevel = blockUpdated.blockUpdatedInfo.storageLevel\n+    val memSize = blockUpdated.blockUpdatedInfo.memSize\n+    val diskSize = blockUpdated.blockUpdatedInfo.diskSize\n+    val externalBlockStoreSize = blockUpdated.blockUpdatedInfo.externalBlockStoreSize\n+\n+    synchronized {\n+      // Drop the update info if the block manager is not registered\n+      blockManagers.get(blockManagerId).foreach { blocksInBlockManager =>\n+        if (storageLevel.isValid) {\n+          blocksInBlockManager.add(blockId)\n+          val location = blockManagerId.hostPort\n+          val newLocations =\n+            blocks.get(blockId).map(_.locations).getOrElse(Set.empty) + location\n+          val newStorageLevel = StorageLevel(\n+            useDisk = diskSize > 0,\n+            useMemory = memSize > 0,\n+            useOffHeap = externalBlockStoreSize > 0,"
  }],
  "prId": 6672
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "incorrect formatting.\n",
    "commit": "df2c1d8579e3643e646a1f37534cc4cfb2c7db54",
    "createdAt": "2015-07-07T02:12:46Z",
    "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.storage\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.scheduler._\n+\n+private[spark] case class BlockUIData(\n+    blockId: BlockId,\n+    storageLevel: StorageLevel,\n+    memSize: Long,\n+    diskSize: Long,\n+    externalBlockStoreSize: Long)\n+\n+/**\n+ * The aggregated status of stream blocks in an executor\n+ */\n+private[spark] case class ExecutorStreamBlockStatus (\n+  executorId: String,"
  }],
  "prId": 6672
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "These public functions can get called at different times leading to inconsistent values in the UI. Can you define a method that returns both information in a single synchronized call?\n",
    "commit": "df2c1d8579e3643e646a1f37534cc4cfb2c7db54",
    "createdAt": "2015-07-09T08:34:36Z",
    "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.storage\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.scheduler._\n+\n+private[spark] case class BlockUIData(\n+    blockId: BlockId,\n+    location: String,\n+    storageLevel: StorageLevel,\n+    memSize: Long,\n+    diskSize: Long,\n+    externalBlockStoreSize: Long)\n+\n+/**\n+ * The aggregated status of stream blocks in an executor\n+ */\n+private[spark] case class ExecutorStreamBlockStatus(\n+    executorId: String,\n+    location: String,\n+    blocks: Seq[BlockUIData]) {\n+\n+  def totalMemSize: Long = blocks.map(_.memSize).sum\n+\n+  def totalDiskSize: Long = blocks.map(_.diskSize).sum\n+\n+  def totalExternalBlockStoreSize: Long = blocks.map(_.externalBlockStoreSize).sum\n+\n+  def numStreamBlocks: Int = blocks.size\n+\n+}\n+\n+private[spark] class BlockStatusListener extends SparkListener {\n+\n+  private val blockManagers =\n+    new mutable.HashMap[BlockManagerId, mutable.HashMap[BlockId, BlockUIData]]\n+  /**\n+   * The replication in StorageLevel may be out of date. E.g., when the first block is added, the\n+   * replication is 1. But when the second block with the same ID is added, the replication should\n+   * become 2. To avoid scanning \"blockManagers\" to modify the replication number, we maintain\n+   * \"blockLocations\" to get the replication quickly.\n+   */\n+  private val blockLocations = new mutable.HashMap[BlockId, mutable.ArrayBuffer[String]]\n+\n+  override def onBlockUpdated(blockUpdated: SparkListenerBlockUpdated): Unit = {\n+    val blockId = blockUpdated.blockUpdatedInfo.blockId\n+    if (!blockId.isInstanceOf[StreamBlockId]) {\n+      // Now we only monitor StreamBlocks\n+      return\n+    }\n+    val blockManagerId = blockUpdated.blockUpdatedInfo.blockManagerId\n+    val storageLevel = blockUpdated.blockUpdatedInfo.storageLevel\n+    val memSize = blockUpdated.blockUpdatedInfo.memSize\n+    val diskSize = blockUpdated.blockUpdatedInfo.diskSize\n+    val externalBlockStoreSize = blockUpdated.blockUpdatedInfo.externalBlockStoreSize\n+\n+    synchronized {\n+      // Drop the update info if the block manager is not registered\n+      blockManagers.get(blockManagerId).foreach { blocksInBlockManager =>\n+        if (storageLevel.isValid) {\n+          blocksInBlockManager.put(blockId,\n+            BlockUIData(\n+              blockId,\n+              blockManagerId.hostPort,\n+              storageLevel,\n+              memSize,\n+              diskSize,\n+              externalBlockStoreSize)\n+          )\n+          val locations = blockLocations.getOrElseUpdate(blockId, new mutable.ArrayBuffer[String])\n+          locations += blockManagerId.hostPort\n+        } else {\n+          // If isValid is not true, it means we should drop the block.\n+          blocksInBlockManager -= blockId\n+          removeLocationFromBlockLocations(blockId, blockManagerId.hostPort)\n+        }\n+      }\n+    }\n+  }\n+\n+  override def onBlockManagerAdded(blockManagerAdded: SparkListenerBlockManagerAdded): Unit = {\n+    synchronized {\n+      blockManagers.put(blockManagerAdded.blockManagerId, mutable.HashMap())\n+    }\n+  }\n+\n+  private def removeLocationFromBlockLocations(blockId: BlockId, location: String): Unit = {\n+    synchronized {\n+      blockLocations.get(blockId).foreach { locations =>\n+        locations -= location\n+        if (locations.isEmpty) {\n+          blockLocations -= blockId\n+        }\n+      }\n+    }\n+  }\n+\n+  override def onBlockManagerRemoved(\n+      blockManagerRemoved: SparkListenerBlockManagerRemoved): Unit = synchronized {\n+    val blockManagerId = blockManagerRemoved.blockManagerId\n+    blockManagers.get(blockManagerId).foreach { blocksInBlockManager =>\n+      blocksInBlockManager.keys.foreach(\n+        removeLocationFromBlockLocations(_, blockManagerId.hostPort))\n+    }\n+    blockManagers -= blockManagerId\n+  }\n+\n+  def allExecutorStreamBlockStatus: Seq[ExecutorStreamBlockStatus] = synchronized {"
  }],
  "prId": 6672
}]