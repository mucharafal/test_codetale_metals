[{
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "I think these functions can be renamed to `serializeStream`, `deserializeStream`, etc. That is, remove `data` from the names.\n",
    "commit": "cb8bec01b44a509265d2cb2367c56dbf641092ae",
    "createdAt": "2014-12-25T03:01:09Z",
    "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.storage\n+\n+import java.io.{ByteArrayOutputStream, BufferedOutputStream, InputStream, OutputStream}\n+import java.nio.ByteBuffer\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.serializer.{SerializationStream, DeserializationStream, Serializer}\n+import org.apache.spark.util.ByteBufferInputStream\n+\n+class BlockSerializer(conf: SparkConf, serializer: Serializer) {\n+  // Whether to compress broadcast variables that are stored\n+  private val compressBroadcast = conf.getBoolean(\"spark.broadcast.compress\", true)\n+  // Whether to compress shuffle output that are stored\n+  private val compressShuffle = conf.getBoolean(\"spark.shuffle.compress\", true)\n+  // Whether to compress RDD partitions that are stored serialized\n+  private val compressRdds = conf.getBoolean(\"spark.rdd.compress\", false)\n+  // Whether to compress shuffle output temporarily spilled to disk\n+  private val compressShuffleSpill = conf.getBoolean(\"spark.shuffle.spill.compress\", true)\n+\n+  /* The compression codec to use. Note that the \"lazy\" val is necessary because we want to delay\n+   * the initialization of the compression codec until it is first used. The reason is that a Spark\n+   * program could be using a user-defined codec in a third party jar, which is loaded in\n+   * Executor.updateDependencies. When the BlockManager is initialized, user level jars hasn't been\n+   * loaded yet. */\n+  private lazy val compressionCodec: CompressionCodec = CompressionCodec.createCodec(conf)\n+\n+  private def shouldCompress(blockId: BlockId): Boolean = {\n+    blockId match {\n+      case _: ShuffleBlockId => compressShuffle\n+      case _: BroadcastBlockId => compressBroadcast\n+      case _: RDDBlockId => compressRdds\n+      case _: TempLocalBlockId => compressShuffleSpill\n+      case _: TempShuffleBlockId => compressShuffle\n+      case _ => false\n+    }\n+  }\n+\n+  /**\n+   * Wrap an output stream for compression if block compression is enabled for its block type\n+   */\n+  private def wrapForCompression(blockId: BlockId, s: OutputStream): OutputStream = {\n+    if (shouldCompress(blockId)) compressionCodec.compressedOutputStream(s) else s\n+  }\n+\n+  /**\n+   * Wrap an input stream for compression if block compression is enabled for its block type\n+   */\n+  private def wrapForCompression(blockId: BlockId, s: InputStream): InputStream = {\n+    if (shouldCompress(blockId)) compressionCodec.compressedInputStream(s) else s\n+  }\n+\n+  /** Serializes into a stream. */\n+  def dataSerializeStream(",
    "line": 71
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "In fact, the names can be made more imperative (similar to `wrapForCompression`): `serializeToStream`, `deserializeFromStream`, etc. \n",
    "commit": "cb8bec01b44a509265d2cb2367c56dbf641092ae",
    "createdAt": "2014-12-25T03:02:49Z",
    "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.storage\n+\n+import java.io.{ByteArrayOutputStream, BufferedOutputStream, InputStream, OutputStream}\n+import java.nio.ByteBuffer\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.serializer.{SerializationStream, DeserializationStream, Serializer}\n+import org.apache.spark.util.ByteBufferInputStream\n+\n+class BlockSerializer(conf: SparkConf, serializer: Serializer) {\n+  // Whether to compress broadcast variables that are stored\n+  private val compressBroadcast = conf.getBoolean(\"spark.broadcast.compress\", true)\n+  // Whether to compress shuffle output that are stored\n+  private val compressShuffle = conf.getBoolean(\"spark.shuffle.compress\", true)\n+  // Whether to compress RDD partitions that are stored serialized\n+  private val compressRdds = conf.getBoolean(\"spark.rdd.compress\", false)\n+  // Whether to compress shuffle output temporarily spilled to disk\n+  private val compressShuffleSpill = conf.getBoolean(\"spark.shuffle.spill.compress\", true)\n+\n+  /* The compression codec to use. Note that the \"lazy\" val is necessary because we want to delay\n+   * the initialization of the compression codec until it is first used. The reason is that a Spark\n+   * program could be using a user-defined codec in a third party jar, which is loaded in\n+   * Executor.updateDependencies. When the BlockManager is initialized, user level jars hasn't been\n+   * loaded yet. */\n+  private lazy val compressionCodec: CompressionCodec = CompressionCodec.createCodec(conf)\n+\n+  private def shouldCompress(blockId: BlockId): Boolean = {\n+    blockId match {\n+      case _: ShuffleBlockId => compressShuffle\n+      case _: BroadcastBlockId => compressBroadcast\n+      case _: RDDBlockId => compressRdds\n+      case _: TempLocalBlockId => compressShuffleSpill\n+      case _: TempShuffleBlockId => compressShuffle\n+      case _ => false\n+    }\n+  }\n+\n+  /**\n+   * Wrap an output stream for compression if block compression is enabled for its block type\n+   */\n+  private def wrapForCompression(blockId: BlockId, s: OutputStream): OutputStream = {\n+    if (shouldCompress(blockId)) compressionCodec.compressedOutputStream(s) else s\n+  }\n+\n+  /**\n+   * Wrap an input stream for compression if block compression is enabled for its block type\n+   */\n+  private def wrapForCompression(blockId: BlockId, s: InputStream): InputStream = {\n+    if (shouldCompress(blockId)) compressionCodec.compressedInputStream(s) else s\n+  }\n+\n+  /** Serializes into a stream. */\n+  def dataSerializeStream(",
    "line": 71
  }],
  "prId": 3065
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "This had always confused me. This should rather be named as `startSerializationStream`, because this method does not actually write and serialize any data, only opens up the stream for the caller to write data into.\n",
    "commit": "cb8bec01b44a509265d2cb2367c56dbf641092ae",
    "createdAt": "2014-12-25T03:04:22Z",
    "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.storage\n+\n+import java.io.{ByteArrayOutputStream, BufferedOutputStream, InputStream, OutputStream}\n+import java.nio.ByteBuffer\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.serializer.{SerializationStream, DeserializationStream, Serializer}\n+import org.apache.spark.util.ByteBufferInputStream\n+\n+class BlockSerializer(conf: SparkConf, serializer: Serializer) {\n+  // Whether to compress broadcast variables that are stored\n+  private val compressBroadcast = conf.getBoolean(\"spark.broadcast.compress\", true)\n+  // Whether to compress shuffle output that are stored\n+  private val compressShuffle = conf.getBoolean(\"spark.shuffle.compress\", true)\n+  // Whether to compress RDD partitions that are stored serialized\n+  private val compressRdds = conf.getBoolean(\"spark.rdd.compress\", false)\n+  // Whether to compress shuffle output temporarily spilled to disk\n+  private val compressShuffleSpill = conf.getBoolean(\"spark.shuffle.spill.compress\", true)\n+\n+  /* The compression codec to use. Note that the \"lazy\" val is necessary because we want to delay\n+   * the initialization of the compression codec until it is first used. The reason is that a Spark\n+   * program could be using a user-defined codec in a third party jar, which is loaded in\n+   * Executor.updateDependencies. When the BlockManager is initialized, user level jars hasn't been\n+   * loaded yet. */\n+  private lazy val compressionCodec: CompressionCodec = CompressionCodec.createCodec(conf)\n+\n+  private def shouldCompress(blockId: BlockId): Boolean = {\n+    blockId match {\n+      case _: ShuffleBlockId => compressShuffle\n+      case _: BroadcastBlockId => compressBroadcast\n+      case _: RDDBlockId => compressRdds\n+      case _: TempLocalBlockId => compressShuffleSpill\n+      case _: TempShuffleBlockId => compressShuffle\n+      case _ => false\n+    }\n+  }\n+\n+  /**\n+   * Wrap an output stream for compression if block compression is enabled for its block type\n+   */\n+  private def wrapForCompression(blockId: BlockId, s: OutputStream): OutputStream = {\n+    if (shouldCompress(blockId)) compressionCodec.compressedOutputStream(s) else s\n+  }\n+\n+  /**\n+   * Wrap an input stream for compression if block compression is enabled for its block type\n+   */\n+  private def wrapForCompression(blockId: BlockId, s: InputStream): InputStream = {\n+    if (shouldCompress(blockId)) compressionCodec.compressedInputStream(s) else s\n+  }\n+\n+  /** Serializes into a stream. */\n+  def dataSerializeStream(\n+                           blockId: BlockId,\n+                           outputStream: OutputStream,\n+                           values: Iterator[Any]): Unit = {\n+    val byteStream = new BufferedOutputStream(outputStream)\n+    val ser = serializer.newInstance()\n+    ser.serializeStream(wrapForCompression(blockId, byteStream)).writeAll(values).close()\n+  }\n+\n+  /** Serializes into a stream. */\n+  def dataSerializeStream(",
    "line": 81
  }],
  "prId": 3065
}]