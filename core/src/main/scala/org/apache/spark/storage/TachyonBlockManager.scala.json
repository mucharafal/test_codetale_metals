[{
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "Why does the `TachyonFilePathResolver` interface exist?\n",
    "commit": "72b7768c0e289618834b74e9ab85082e8aea4c90",
    "createdAt": "2014-03-27T20:20:23Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.storage\n+\n+import java.text.SimpleDateFormat\n+import java.util.{Date, Random}\n+\n+import tachyon.client.TachyonFS\n+import tachyon.client.TachyonFile\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.executor.ExecutorExitCode\n+import org.apache.spark.network.netty.{TachyonFilePathResolver, ShuffleSender}\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * Creates and maintains the logical mapping between logical blocks and tachyon fs locations. By\n+ * default, one block is mapped to one file with a name given by its BlockId.\n+ *\n+ * @param rootDirs The directories to use for storing block files. Data will be hashed among these.\n+ */\n+private[spark] class TachyonBlockManager(\n+    shuffleManager: ShuffleBlockManager, \n+    rootDirs: String, \n+    val master: String)\n+  extends TachyonFilePathResolver with Logging {"
  }],
  "prId": 158
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "Does anything ever actually call this? \n",
    "commit": "72b7768c0e289618834b74e9ab85082e8aea4c90",
    "createdAt": "2014-03-27T20:20:40Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.storage\n+\n+import java.text.SimpleDateFormat\n+import java.util.{Date, Random}\n+\n+import tachyon.client.TachyonFS\n+import tachyon.client.TachyonFile\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.executor.ExecutorExitCode\n+import org.apache.spark.network.netty.{TachyonFilePathResolver, ShuffleSender}\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * Creates and maintains the logical mapping between logical blocks and tachyon fs locations. By\n+ * default, one block is mapped to one file with a name given by its BlockId.\n+ *\n+ * @param rootDirs The directories to use for storing block files. Data will be hashed among these.\n+ */\n+private[spark] class TachyonBlockManager(\n+    shuffleManager: ShuffleBlockManager, \n+    rootDirs: String, \n+    val master: String)\n+  extends TachyonFilePathResolver with Logging {\n+\n+  val client = if (master != null && master != \"\") TachyonFS.get(master) else null\n+\n+  if (client == null) {\n+    logError(\"Failed to connect to the Tachyon as the master address is not configured\")\n+    System.exit(ExecutorExitCode.TACHYON_STORE_FAILED_TO_INITIALIZE)\n+  }\n+\n+  private val MAX_DIR_CREATION_ATTEMPTS = 10\n+  private val subDirsPerTachyonDir = \n+    shuffleManager.conf.get(\"spark.tachyonStore.subDirectories\", \"64\").toInt\n+\n+  // Create one Tachyon directory for each path mentioned in spark.tachyonStore.folderName.dir; \n+  // then, inside this directory, create multiple subdirectories that we will hash files into, \n+  // in order to avoid having really large inodes at the top level in Tachyon.\n+  private val tachyonDirs: Array[TachyonFile] = createTachyonDirs()\n+  private val subDirs = Array.fill(tachyonDirs.length)(new Array[TachyonFile](subDirsPerTachyonDir))\n+\n+  addShutdownHook()\n+\n+  /**\n+   * Returns the physical tachyon file segment in which the given BlockId is located.\n+   * If the BlockId has been mapped to a specific FileSegment, that will be returned.\n+   * Otherwise, we assume the Block is mapped to a whole file identified by the BlockId directly.\n+   */\n+  def getBlockLocation(blockId: BlockId): TachyonFileSegment = {"
  }, {
    "author": {
      "login": "pwendell"
    },
    "body": "It might be possible to just remove this and the `TachyonFileSegment` class. If they aren't used we should remove them.\n",
    "commit": "72b7768c0e289618834b74e9ab85082e8aea4c90",
    "createdAt": "2014-03-27T21:34:04Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.storage\n+\n+import java.text.SimpleDateFormat\n+import java.util.{Date, Random}\n+\n+import tachyon.client.TachyonFS\n+import tachyon.client.TachyonFile\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.executor.ExecutorExitCode\n+import org.apache.spark.network.netty.{TachyonFilePathResolver, ShuffleSender}\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * Creates and maintains the logical mapping between logical blocks and tachyon fs locations. By\n+ * default, one block is mapped to one file with a name given by its BlockId.\n+ *\n+ * @param rootDirs The directories to use for storing block files. Data will be hashed among these.\n+ */\n+private[spark] class TachyonBlockManager(\n+    shuffleManager: ShuffleBlockManager, \n+    rootDirs: String, \n+    val master: String)\n+  extends TachyonFilePathResolver with Logging {\n+\n+  val client = if (master != null && master != \"\") TachyonFS.get(master) else null\n+\n+  if (client == null) {\n+    logError(\"Failed to connect to the Tachyon as the master address is not configured\")\n+    System.exit(ExecutorExitCode.TACHYON_STORE_FAILED_TO_INITIALIZE)\n+  }\n+\n+  private val MAX_DIR_CREATION_ATTEMPTS = 10\n+  private val subDirsPerTachyonDir = \n+    shuffleManager.conf.get(\"spark.tachyonStore.subDirectories\", \"64\").toInt\n+\n+  // Create one Tachyon directory for each path mentioned in spark.tachyonStore.folderName.dir; \n+  // then, inside this directory, create multiple subdirectories that we will hash files into, \n+  // in order to avoid having really large inodes at the top level in Tachyon.\n+  private val tachyonDirs: Array[TachyonFile] = createTachyonDirs()\n+  private val subDirs = Array.fill(tachyonDirs.length)(new Array[TachyonFile](subDirsPerTachyonDir))\n+\n+  addShutdownHook()\n+\n+  /**\n+   * Returns the physical tachyon file segment in which the given BlockId is located.\n+   * If the BlockId has been mapped to a specific FileSegment, that will be returned.\n+   * Otherwise, we assume the Block is mapped to a whole file identified by the BlockId directly.\n+   */\n+  def getBlockLocation(blockId: BlockId): TachyonFileSegment = {"
  }, {
    "author": {
      "login": "mateiz"
    },
    "body": "Yeah I agree, this seems like it was copied from the disk store, which does fancy stuff with consolidating files. But we won't put shuffle blocks in Tachyon presumably.\n",
    "commit": "72b7768c0e289618834b74e9ab85082e8aea4c90",
    "createdAt": "2014-03-28T01:27:53Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.storage\n+\n+import java.text.SimpleDateFormat\n+import java.util.{Date, Random}\n+\n+import tachyon.client.TachyonFS\n+import tachyon.client.TachyonFile\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.executor.ExecutorExitCode\n+import org.apache.spark.network.netty.{TachyonFilePathResolver, ShuffleSender}\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * Creates and maintains the logical mapping between logical blocks and tachyon fs locations. By\n+ * default, one block is mapped to one file with a name given by its BlockId.\n+ *\n+ * @param rootDirs The directories to use for storing block files. Data will be hashed among these.\n+ */\n+private[spark] class TachyonBlockManager(\n+    shuffleManager: ShuffleBlockManager, \n+    rootDirs: String, \n+    val master: String)\n+  extends TachyonFilePathResolver with Logging {\n+\n+  val client = if (master != null && master != \"\") TachyonFS.get(master) else null\n+\n+  if (client == null) {\n+    logError(\"Failed to connect to the Tachyon as the master address is not configured\")\n+    System.exit(ExecutorExitCode.TACHYON_STORE_FAILED_TO_INITIALIZE)\n+  }\n+\n+  private val MAX_DIR_CREATION_ATTEMPTS = 10\n+  private val subDirsPerTachyonDir = \n+    shuffleManager.conf.get(\"spark.tachyonStore.subDirectories\", \"64\").toInt\n+\n+  // Create one Tachyon directory for each path mentioned in spark.tachyonStore.folderName.dir; \n+  // then, inside this directory, create multiple subdirectories that we will hash files into, \n+  // in order to avoid having really large inodes at the top level in Tachyon.\n+  private val tachyonDirs: Array[TachyonFile] = createTachyonDirs()\n+  private val subDirs = Array.fill(tachyonDirs.length)(new Array[TachyonFile](subDirsPerTachyonDir))\n+\n+  addShutdownHook()\n+\n+  /**\n+   * Returns the physical tachyon file segment in which the given BlockId is located.\n+   * If the BlockId has been mapped to a specific FileSegment, that will be returned.\n+   * Otherwise, we assume the Block is mapped to a whole file identified by the BlockId directly.\n+   */\n+  def getBlockLocation(blockId: BlockId): TachyonFileSegment = {"
  }],
  "prId": 158
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "I'll defer to @haoyuan here, but I don't think it's necessary to create all these directories for Tachyon. I think it's fine to store each of the RDD directories just in the root dir specified by the user. It wills simplify a bunch of other things too.\n",
    "commit": "72b7768c0e289618834b74e9ab85082e8aea4c90",
    "createdAt": "2014-03-28T06:20:04Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.storage\n+\n+import java.text.SimpleDateFormat\n+import java.util.{Date, Random}\n+\n+import tachyon.client.TachyonFS\n+import tachyon.client.TachyonFile\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.executor.ExecutorExitCode\n+import org.apache.spark.network.netty.{TachyonFilePathResolver, ShuffleSender}\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * Creates and maintains the logical mapping between logical blocks and tachyon fs locations. By\n+ * default, one block is mapped to one file with a name given by its BlockId.\n+ *\n+ * @param rootDirs The directories to use for storing block files. Data will be hashed among these.\n+ */\n+private[spark] class TachyonBlockManager(\n+    shuffleManager: ShuffleBlockManager, \n+    rootDirs: String, \n+    val master: String)\n+  extends TachyonFilePathResolver with Logging {\n+\n+  val client = if (master != null && master != \"\") TachyonFS.get(master) else null\n+\n+  if (client == null) {\n+    logError(\"Failed to connect to the Tachyon as the master address is not configured\")\n+    System.exit(ExecutorExitCode.TACHYON_STORE_FAILED_TO_INITIALIZE)\n+  }\n+\n+  private val MAX_DIR_CREATION_ATTEMPTS = 10\n+  private val subDirsPerTachyonDir = \n+    shuffleManager.conf.get(\"spark.tachyonStore.subDirectories\", \"64\").toInt\n+\n+  // Create one Tachyon directory for each path mentioned in spark.tachyonStore.folderName.dir; \n+  // then, inside this directory, create multiple subdirectories that we will hash files into, \n+  // in order to avoid having really large inodes at the top level in Tachyon.\n+  private val tachyonDirs: Array[TachyonFile] = createTachyonDirs()\n+  private val subDirs = Array.fill(tachyonDirs.length)(new Array[TachyonFile](subDirsPerTachyonDir))\n+\n+  addShutdownHook()\n+\n+  /**\n+   * Returns the physical tachyon file segment in which the given BlockId is located.\n+   * If the BlockId has been mapped to a specific FileSegment, that will be returned.\n+   * Otherwise, we assume the Block is mapped to a whole file identified by the BlockId directly.\n+   */\n+  def getBlockLocation(blockId: BlockId): TachyonFileSegment = {\n+    val file = getFile(blockId.name)\n+    new TachyonFileSegment(file, 0, file.length())\n+  }\n+  \n+  def removeFile(file: TachyonFile): Boolean = {\n+    client.delete(file.getPath(), false)\n+  }\n+  \n+  def fileExists(file: TachyonFile): Boolean = {\n+    client.exist(file.getPath())\n+  }\n+\n+  def getFile(filename: String): TachyonFile = {\n+    // Figure out which tachyon directory it hashes to, and which subdirectory in that\n+    val hash = Utils.nonNegativeHash(filename)\n+    val dirId = hash % tachyonDirs.length\n+    val subDirId = (hash / tachyonDirs.length) % subDirsPerTachyonDir\n+\n+    // Create the subdirectory if it doesn't already exist\n+    var subDir = subDirs(dirId)(subDirId)\n+    if (subDir == null) {\n+      subDir = subDirs(dirId).synchronized {\n+        val old = subDirs(dirId)(subDirId)\n+        if (old != null) {\n+          old\n+        } else {\n+          val path = tachyonDirs(dirId) + \"/\" + \"%02x\".format(subDirId)\n+          client.mkdir(path)\n+          val newDir = client.getFile(path)\n+          subDirs(dirId)(subDirId) = newDir\n+          newDir\n+        }\n+      }\n+    }\n+    val filePath = subDir + \"/\" + filename\n+    if(!client.exist(filePath)) {\n+      client.createFile(filePath)\n+    }\n+    val file = client.getFile(filePath)\n+    file\n+  }\n+\n+  def getFile(blockId: BlockId): TachyonFile = getFile(blockId.name)\n+\n+  private def createTachyonDirs(): Array[TachyonFile] = {"
  }],
  "prId": 158
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "I think you can just remove all of the logic relevant to hashing which will make this much simpler.\n",
    "commit": "72b7768c0e289618834b74e9ab85082e8aea4c90",
    "createdAt": "2014-03-28T06:21:09Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.storage\n+\n+import java.text.SimpleDateFormat\n+import java.util.{Date, Random}\n+\n+import tachyon.client.TachyonFS\n+import tachyon.client.TachyonFile\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.executor.ExecutorExitCode\n+import org.apache.spark.network.netty.{TachyonFilePathResolver, ShuffleSender}\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * Creates and maintains the logical mapping between logical blocks and tachyon fs locations. By\n+ * default, one block is mapped to one file with a name given by its BlockId.\n+ *\n+ * @param rootDirs The directories to use for storing block files. Data will be hashed among these.\n+ */\n+private[spark] class TachyonBlockManager(\n+    shuffleManager: ShuffleBlockManager, \n+    rootDirs: String, \n+    val master: String)\n+  extends TachyonFilePathResolver with Logging {\n+\n+  val client = if (master != null && master != \"\") TachyonFS.get(master) else null\n+\n+  if (client == null) {\n+    logError(\"Failed to connect to the Tachyon as the master address is not configured\")\n+    System.exit(ExecutorExitCode.TACHYON_STORE_FAILED_TO_INITIALIZE)\n+  }\n+\n+  private val MAX_DIR_CREATION_ATTEMPTS = 10\n+  private val subDirsPerTachyonDir = \n+    shuffleManager.conf.get(\"spark.tachyonStore.subDirectories\", \"64\").toInt\n+\n+  // Create one Tachyon directory for each path mentioned in spark.tachyonStore.folderName.dir; \n+  // then, inside this directory, create multiple subdirectories that we will hash files into, \n+  // in order to avoid having really large inodes at the top level in Tachyon.\n+  private val tachyonDirs: Array[TachyonFile] = createTachyonDirs()\n+  private val subDirs = Array.fill(tachyonDirs.length)(new Array[TachyonFile](subDirsPerTachyonDir))\n+\n+  addShutdownHook()\n+\n+  /**\n+   * Returns the physical tachyon file segment in which the given BlockId is located.\n+   * If the BlockId has been mapped to a specific FileSegment, that will be returned.\n+   * Otherwise, we assume the Block is mapped to a whole file identified by the BlockId directly.\n+   */\n+  def getBlockLocation(blockId: BlockId): TachyonFileSegment = {\n+    val file = getFile(blockId.name)\n+    new TachyonFileSegment(file, 0, file.length())\n+  }\n+  \n+  def removeFile(file: TachyonFile): Boolean = {\n+    client.delete(file.getPath(), false)\n+  }\n+  \n+  def fileExists(file: TachyonFile): Boolean = {\n+    client.exist(file.getPath())\n+  }\n+\n+  def getFile(filename: String): TachyonFile = {\n+    // Figure out which tachyon directory it hashes to, and which subdirectory in that\n+    val hash = Utils.nonNegativeHash(filename)",
    "line": 73
  }],
  "prId": 158
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "I think the shutdown hook for Tachyon can be way less complicated. You just need to recursively delete the application directory. All of these other functions related to the shutdown hook I think can be removed.\n",
    "commit": "72b7768c0e289618834b74e9ab85082e8aea4c90",
    "createdAt": "2014-03-28T06:27:53Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.storage\n+\n+import java.text.SimpleDateFormat\n+import java.util.{Date, Random}\n+\n+import tachyon.client.TachyonFS\n+import tachyon.client.TachyonFile\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.executor.ExecutorExitCode\n+import org.apache.spark.network.netty.{TachyonFilePathResolver, ShuffleSender}\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * Creates and maintains the logical mapping between logical blocks and tachyon fs locations. By\n+ * default, one block is mapped to one file with a name given by its BlockId.\n+ *\n+ * @param rootDirs The directories to use for storing block files. Data will be hashed among these.\n+ */\n+private[spark] class TachyonBlockManager(\n+    shuffleManager: ShuffleBlockManager, \n+    rootDirs: String, \n+    val master: String)\n+  extends TachyonFilePathResolver with Logging {\n+\n+  val client = if (master != null && master != \"\") TachyonFS.get(master) else null\n+\n+  if (client == null) {\n+    logError(\"Failed to connect to the Tachyon as the master address is not configured\")\n+    System.exit(ExecutorExitCode.TACHYON_STORE_FAILED_TO_INITIALIZE)\n+  }\n+\n+  private val MAX_DIR_CREATION_ATTEMPTS = 10\n+  private val subDirsPerTachyonDir = \n+    shuffleManager.conf.get(\"spark.tachyonStore.subDirectories\", \"64\").toInt\n+\n+  // Create one Tachyon directory for each path mentioned in spark.tachyonStore.folderName.dir; \n+  // then, inside this directory, create multiple subdirectories that we will hash files into, \n+  // in order to avoid having really large inodes at the top level in Tachyon.\n+  private val tachyonDirs: Array[TachyonFile] = createTachyonDirs()\n+  private val subDirs = Array.fill(tachyonDirs.length)(new Array[TachyonFile](subDirsPerTachyonDir))\n+\n+  addShutdownHook()\n+\n+  /**\n+   * Returns the physical tachyon file segment in which the given BlockId is located.\n+   * If the BlockId has been mapped to a specific FileSegment, that will be returned.\n+   * Otherwise, we assume the Block is mapped to a whole file identified by the BlockId directly.\n+   */\n+  def getBlockLocation(blockId: BlockId): TachyonFileSegment = {\n+    val file = getFile(blockId.name)\n+    new TachyonFileSegment(file, 0, file.length())\n+  }\n+  \n+  def removeFile(file: TachyonFile): Boolean = {\n+    client.delete(file.getPath(), false)\n+  }\n+  \n+  def fileExists(file: TachyonFile): Boolean = {\n+    client.exist(file.getPath())\n+  }\n+\n+  def getFile(filename: String): TachyonFile = {\n+    // Figure out which tachyon directory it hashes to, and which subdirectory in that\n+    val hash = Utils.nonNegativeHash(filename)\n+    val dirId = hash % tachyonDirs.length\n+    val subDirId = (hash / tachyonDirs.length) % subDirsPerTachyonDir\n+\n+    // Create the subdirectory if it doesn't already exist\n+    var subDir = subDirs(dirId)(subDirId)\n+    if (subDir == null) {\n+      subDir = subDirs(dirId).synchronized {\n+        val old = subDirs(dirId)(subDirId)\n+        if (old != null) {\n+          old\n+        } else {\n+          val path = tachyonDirs(dirId) + \"/\" + \"%02x\".format(subDirId)\n+          client.mkdir(path)\n+          val newDir = client.getFile(path)\n+          subDirs(dirId)(subDirId) = newDir\n+          newDir\n+        }\n+      }\n+    }\n+    val filePath = subDir + \"/\" + filename\n+    if(!client.exist(filePath)) {\n+      client.createFile(filePath)\n+    }\n+    val file = client.getFile(filePath)\n+    file\n+  }\n+\n+  def getFile(blockId: BlockId): TachyonFile = getFile(blockId.name)\n+\n+  private def createTachyonDirs(): Array[TachyonFile] = {\n+    logDebug(\"Creating tachyon directories at root dirs '\" + rootDirs + \"'\")\n+    val dateFormat = new SimpleDateFormat(\"yyyyMMddHHmmss\")\n+    rootDirs.split(\",\").map { rootDir =>\n+      var foundLocalDir = false\n+      var tachyonDir: TachyonFile = null\n+      var tachyonDirId: String = null\n+      var tries = 0\n+      val rand = new Random()\n+      while (!foundLocalDir && tries < MAX_DIR_CREATION_ATTEMPTS) {\n+        tries += 1\n+        try {\n+          tachyonDirId = \"%s-%04x\".format(dateFormat.format(new Date), rand.nextInt(65536))\n+          val path = rootDir + \"/\" + \"spark-tachyon-\" + tachyonDirId\n+          if (!client.exist(path)) {\n+            foundLocalDir = client.mkdir(path)\n+            tachyonDir = client.getFile(path)\n+          }\n+        } catch {\n+          case e: Exception =>\n+            logWarning(\"Attempt \" + tries + \" to create tachyon dir \" + tachyonDir + \" failed\", e)\n+        }\n+      }\n+      if (!foundLocalDir) {\n+        logError(\"Failed \" + MAX_DIR_CREATION_ATTEMPTS + \" attempts to create tachyon dir in \" +\n+          rootDir)\n+        System.exit(ExecutorExitCode.TACHYON_STORE_FAILED_TO_CREATE_DIR)\n+      }\n+      logInfo(\"Created tachyon directory at \" + tachyonDir)\n+      tachyonDir\n+    }\n+  }\n+\n+  private def addShutdownHook() {\n+    tachyonDirs.foreach(tachyonDir => Utils.registerShutdownDeleteDir(tachyonDir))",
    "line": 138
  }],
  "prId": 158
}, {
  "comments": [{
    "author": {
      "login": "aarondav"
    },
    "body": "I don't think \"spark.tachyonStore.folderName.dir\" is a thing, perhaps just without the \".dir\"?\n",
    "commit": "72b7768c0e289618834b74e9ab85082e8aea4c90",
    "createdAt": "2014-04-02T07:19:39Z",
    "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.storage\n+\n+import java.text.SimpleDateFormat\n+import java.util.{Date, Random}\n+\n+import tachyon.client.TachyonFS\n+import tachyon.client.TachyonFile\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.executor.ExecutorExitCode\n+import org.apache.spark.network.netty.ShuffleSender\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * Creates and maintains the logical mapping between logical blocks and tachyon fs locations. By\n+ * default, one block is mapped to one file with a name given by its BlockId.\n+ *\n+ * @param rootDirs The directories to use for storing block files. Data will be hashed among these.\n+ */\n+private[spark] class TachyonBlockManager(\n+    shuffleManager: ShuffleBlockManager, \n+    rootDirs: String, \n+    val master: String)\n+  extends Logging {\n+\n+  val client = if (master != null && master != \"\") TachyonFS.get(master) else null\n+\n+  if (client == null) {\n+    logError(\"Failed to connect to the Tachyon as the master address is not configured\")\n+    System.exit(ExecutorExitCode.TACHYON_STORE_FAILED_TO_INITIALIZE)\n+  }\n+\n+  private val MAX_DIR_CREATION_ATTEMPTS = 10\n+  private val subDirsPerTachyonDir = \n+    shuffleManager.conf.get(\"spark.tachyonStore.subDirectories\", \"64\").toInt\n+\n+  // Create one Tachyon directory for each path mentioned in spark.tachyonStore.folderName.dir; "
  }],
  "prId": 158
}]