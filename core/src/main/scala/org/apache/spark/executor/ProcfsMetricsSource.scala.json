[{
  "comments": [{
    "author": {
      "login": "ankuriitg"
    },
    "body": "nit: insert an extra line before this",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-13T18:00:30Z",
    "diffHunk": "@@ -0,0 +1,64 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import com.codahale.metrics.{Gauge, MetricRegistry}\n+\n+import org.apache.spark.internal.config\n+import org.apache.spark.metrics.source.Source\n+import org.apache.spark.SparkEnv\n+\n+private[executor] class ProcfsMetricsSource extends Source {\n+  override val sourceName = \"procfs\"\n+  override val metricRegistry = new MetricRegistry()\n+  var numMetrics: Int = 0\n+  var metrics: Map[String, Long] = Map.empty\n+  val shouldAddProcessTreeMetricsToMetricsSet =\n+    SparkEnv.get.conf.get(config.METRICS_PROCESS_TREE_METRICS)\n+\n+  private def getProcfsMetrics: Map[String, Long] = {\n+    if (numMetrics == 0) {\n+      metrics = Map.empty\n+      val p = ProcfsMetricsGetter.pTreeInfo.computeAllMetrics()\n+      metrics = Map(\"ProcessTreeJVMVMemory\" -> p.jvmVmemTotal,\n+        \"ProcessTreeJVMRSSMemory\" -> p.jvmRSSTotal,\n+        \"ProcessTreePythonVMemory\" -> p.pythonVmemTotal,\n+        \"ProcessTreePythonRSSMemory\" -> p.pythonRSSTotal,\n+        \"ProcessTreeOtherVMemory\" -> p.otherVmemTotal,\n+        \"ProcessTreeOtherRSSMemory\" -> p.otherRSSTotal)\n+    }\n+    numMetrics = numMetrics + 1\n+    if (numMetrics == 6) {\n+      numMetrics = 0}\n+    metrics\n+  }\n+  private def registerProcfsMetrics[Long]( name: String) = {"
  }],
  "prId": 23306
}, {
  "comments": [{
    "author": {
      "login": "ankuriitg"
    },
    "body": "nit: remove extra space before \"_name_\"",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-13T18:01:34Z",
    "diffHunk": "@@ -0,0 +1,64 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import com.codahale.metrics.{Gauge, MetricRegistry}\n+\n+import org.apache.spark.internal.config\n+import org.apache.spark.metrics.source.Source\n+import org.apache.spark.SparkEnv\n+\n+private[executor] class ProcfsMetricsSource extends Source {\n+  override val sourceName = \"procfs\"\n+  override val metricRegistry = new MetricRegistry()\n+  var numMetrics: Int = 0\n+  var metrics: Map[String, Long] = Map.empty\n+  val shouldAddProcessTreeMetricsToMetricsSet =\n+    SparkEnv.get.conf.get(config.METRICS_PROCESS_TREE_METRICS)\n+\n+  private def getProcfsMetrics: Map[String, Long] = {\n+    if (numMetrics == 0) {\n+      metrics = Map.empty\n+      val p = ProcfsMetricsGetter.pTreeInfo.computeAllMetrics()\n+      metrics = Map(\"ProcessTreeJVMVMemory\" -> p.jvmVmemTotal,\n+        \"ProcessTreeJVMRSSMemory\" -> p.jvmRSSTotal,\n+        \"ProcessTreePythonVMemory\" -> p.pythonVmemTotal,\n+        \"ProcessTreePythonRSSMemory\" -> p.pythonRSSTotal,\n+        \"ProcessTreeOtherVMemory\" -> p.otherVmemTotal,\n+        \"ProcessTreeOtherRSSMemory\" -> p.otherRSSTotal)\n+    }\n+    numMetrics = numMetrics + 1\n+    if (numMetrics == 6) {\n+      numMetrics = 0}\n+    metrics\n+  }\n+  private def registerProcfsMetrics[Long]( name: String) = {"
  }],
  "prId": 23306
}, {
  "comments": [{
    "author": {
      "login": "ankuriitg"
    },
    "body": "nit: I think you can remove the \"ProcessTree\" from metric names, since you are already appending the metric names with \"processTree.\"",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-13T18:03:41Z",
    "diffHunk": "@@ -0,0 +1,64 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import com.codahale.metrics.{Gauge, MetricRegistry}\n+\n+import org.apache.spark.internal.config\n+import org.apache.spark.metrics.source.Source\n+import org.apache.spark.SparkEnv\n+\n+private[executor] class ProcfsMetricsSource extends Source {\n+  override val sourceName = \"procfs\"\n+  override val metricRegistry = new MetricRegistry()\n+  var numMetrics: Int = 0\n+  var metrics: Map[String, Long] = Map.empty\n+  val shouldAddProcessTreeMetricsToMetricsSet =\n+    SparkEnv.get.conf.get(config.METRICS_PROCESS_TREE_METRICS)\n+\n+  private def getProcfsMetrics: Map[String, Long] = {\n+    if (numMetrics == 0) {\n+      metrics = Map.empty\n+      val p = ProcfsMetricsGetter.pTreeInfo.computeAllMetrics()\n+      metrics = Map(\"ProcessTreeJVMVMemory\" -> p.jvmVmemTotal,\n+        \"ProcessTreeJVMRSSMemory\" -> p.jvmRSSTotal,\n+        \"ProcessTreePythonVMemory\" -> p.pythonVmemTotal,\n+        \"ProcessTreePythonRSSMemory\" -> p.pythonRSSTotal,\n+        \"ProcessTreeOtherVMemory\" -> p.otherVmemTotal,\n+        \"ProcessTreeOtherRSSMemory\" -> p.otherRSSTotal)\n+    }\n+    numMetrics = numMetrics + 1\n+    if (numMetrics == 6) {\n+      numMetrics = 0}\n+    metrics\n+  }\n+  private def registerProcfsMetrics[Long]( name: String) = {\n+    metricRegistry.register(MetricRegistry.name(\"processTree\", name), new Gauge[Long] {\n+      override def getValue: Long = getProcfsMetrics(name).asInstanceOf[Long]\n+    })\n+  }\n+\n+  if (shouldAddProcessTreeMetricsToMetricsSet) {\n+    registerProcfsMetrics(\"ProcessTreeJVMVMemory\")"
  }],
  "prId": 23306
}, {
  "comments": [{
    "author": {
      "login": "ankuriitg"
    },
    "body": "nit: I am not sure but is the cast to Long required? I think since map is a generic, this cast is not needed.",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-13T18:07:57Z",
    "diffHunk": "@@ -0,0 +1,64 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import com.codahale.metrics.{Gauge, MetricRegistry}\n+\n+import org.apache.spark.internal.config\n+import org.apache.spark.metrics.source.Source\n+import org.apache.spark.SparkEnv\n+\n+private[executor] class ProcfsMetricsSource extends Source {\n+  override val sourceName = \"procfs\"\n+  override val metricRegistry = new MetricRegistry()\n+  var numMetrics: Int = 0\n+  var metrics: Map[String, Long] = Map.empty\n+  val shouldAddProcessTreeMetricsToMetricsSet =\n+    SparkEnv.get.conf.get(config.METRICS_PROCESS_TREE_METRICS)\n+\n+  private def getProcfsMetrics: Map[String, Long] = {\n+    if (numMetrics == 0) {\n+      metrics = Map.empty\n+      val p = ProcfsMetricsGetter.pTreeInfo.computeAllMetrics()\n+      metrics = Map(\"ProcessTreeJVMVMemory\" -> p.jvmVmemTotal,\n+        \"ProcessTreeJVMRSSMemory\" -> p.jvmRSSTotal,\n+        \"ProcessTreePythonVMemory\" -> p.pythonVmemTotal,\n+        \"ProcessTreePythonRSSMemory\" -> p.pythonRSSTotal,\n+        \"ProcessTreeOtherVMemory\" -> p.otherVmemTotal,\n+        \"ProcessTreeOtherRSSMemory\" -> p.otherRSSTotal)\n+    }\n+    numMetrics = numMetrics + 1\n+    if (numMetrics == 6) {\n+      numMetrics = 0}\n+    metrics\n+  }\n+  private def registerProcfsMetrics[Long]( name: String) = {\n+    metricRegistry.register(MetricRegistry.name(\"processTree\", name), new Gauge[Long] {\n+      override def getValue: Long = getProcfsMetrics(name).asInstanceOf[Long]",
    "line": 59
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": " There will be a compile error without that cast. I think because of difference between Scala.Long and java's long. I wasn't expecting that ",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-13T18:50:39Z",
    "diffHunk": "@@ -0,0 +1,64 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import com.codahale.metrics.{Gauge, MetricRegistry}\n+\n+import org.apache.spark.internal.config\n+import org.apache.spark.metrics.source.Source\n+import org.apache.spark.SparkEnv\n+\n+private[executor] class ProcfsMetricsSource extends Source {\n+  override val sourceName = \"procfs\"\n+  override val metricRegistry = new MetricRegistry()\n+  var numMetrics: Int = 0\n+  var metrics: Map[String, Long] = Map.empty\n+  val shouldAddProcessTreeMetricsToMetricsSet =\n+    SparkEnv.get.conf.get(config.METRICS_PROCESS_TREE_METRICS)\n+\n+  private def getProcfsMetrics: Map[String, Long] = {\n+    if (numMetrics == 0) {\n+      metrics = Map.empty\n+      val p = ProcfsMetricsGetter.pTreeInfo.computeAllMetrics()\n+      metrics = Map(\"ProcessTreeJVMVMemory\" -> p.jvmVmemTotal,\n+        \"ProcessTreeJVMRSSMemory\" -> p.jvmRSSTotal,\n+        \"ProcessTreePythonVMemory\" -> p.pythonVmemTotal,\n+        \"ProcessTreePythonRSSMemory\" -> p.pythonRSSTotal,\n+        \"ProcessTreeOtherVMemory\" -> p.otherVmemTotal,\n+        \"ProcessTreeOtherRSSMemory\" -> p.otherRSSTotal)\n+    }\n+    numMetrics = numMetrics + 1\n+    if (numMetrics == 6) {\n+      numMetrics = 0}\n+    metrics\n+  }\n+  private def registerProcfsMetrics[Long]( name: String) = {\n+    metricRegistry.register(MetricRegistry.name(\"processTree\", name), new Gauge[Long] {\n+      override def getValue: Long = getProcfsMetrics(name).asInstanceOf[Long]",
    "line": 59
  }],
  "prId": 23306
}, {
  "comments": [{
    "author": {
      "login": "ankuriitg"
    },
    "body": "nit: closing brackets on the next line. Also, it will be useful to add a comment about the purpose of numMetrics. I am guessing it ensures that procfs metrics are not polled very often.",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-13T18:10:46Z",
    "diffHunk": "@@ -0,0 +1,64 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import com.codahale.metrics.{Gauge, MetricRegistry}\n+\n+import org.apache.spark.internal.config\n+import org.apache.spark.metrics.source.Source\n+import org.apache.spark.SparkEnv\n+\n+private[executor] class ProcfsMetricsSource extends Source {\n+  override val sourceName = \"procfs\"\n+  override val metricRegistry = new MetricRegistry()\n+  var numMetrics: Int = 0\n+  var metrics: Map[String, Long] = Map.empty\n+  val shouldAddProcessTreeMetricsToMetricsSet =\n+    SparkEnv.get.conf.get(config.METRICS_PROCESS_TREE_METRICS)\n+\n+  private def getProcfsMetrics: Map[String, Long] = {\n+    if (numMetrics == 0) {\n+      metrics = Map.empty\n+      val p = ProcfsMetricsGetter.pTreeInfo.computeAllMetrics()\n+      metrics = Map(\"ProcessTreeJVMVMemory\" -> p.jvmVmemTotal,\n+        \"ProcessTreeJVMRSSMemory\" -> p.jvmRSSTotal,\n+        \"ProcessTreePythonVMemory\" -> p.pythonVmemTotal,\n+        \"ProcessTreePythonRSSMemory\" -> p.pythonRSSTotal,\n+        \"ProcessTreeOtherVMemory\" -> p.otherVmemTotal,\n+        \"ProcessTreeOtherRSSMemory\" -> p.otherRSSTotal)\n+    }\n+    numMetrics = numMetrics + 1\n+    if (numMetrics == 6) {\n+      numMetrics = 0}"
  }],
  "prId": 23306
}, {
  "comments": [{
    "author": {
      "login": "ankuriitg"
    },
    "body": "Can we remove this now that we have caching? This looks like a hacky way to achieve this anyway, it will be better to have some alternate way.",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-17T18:48:41Z",
    "diffHunk": "@@ -19,14 +19,15 @@ package org.apache.spark.executor\n \n import com.codahale.metrics.{Gauge, MetricRegistry}\n \n+import org.apache.spark.SparkEnv\n import org.apache.spark.internal.config\n import org.apache.spark.metrics.source.Source\n-import org.apache.spark.SparkEnv\n \n private[executor] class ProcfsMetricsSource extends Source {\n   override val sourceName = \"procfs\"\n-  override val metricRegistry = new MetricRegistry()\n+  // We use numMetrics for tracking to only call computAllMetrics once per set of metrics"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "I thought that this still can save us from unnecessary calls. So I kept it. Why you think it is hacky? The way that Metrics system is designed is that it just return a single value from a guage. There are some other methods to return a set of metrics, but to use that we need to make more changes to the procfsgetter to implement Dropwised metric interface for each metric that we are going to report.  I think that isn't necessary and it make the code uglier.",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-17T19:17:27Z",
    "diffHunk": "@@ -19,14 +19,15 @@ package org.apache.spark.executor\n \n import com.codahale.metrics.{Gauge, MetricRegistry}\n \n+import org.apache.spark.SparkEnv\n import org.apache.spark.internal.config\n import org.apache.spark.metrics.source.Source\n-import org.apache.spark.SparkEnv\n \n private[executor] class ProcfsMetricsSource extends Source {\n   override val sourceName = \"procfs\"\n-  override val metricRegistry = new MetricRegistry()\n+  // We use numMetrics for tracking to only call computAllMetrics once per set of metrics"
  }, {
    "author": {
      "login": "ankuriitg"
    },
    "body": "The reason I think this is hacky is that `getProcfsMetrics` relies on an internal state (`numMetrics`) to determine whether to call `computeAllMetrics` or not. I will prefer that `getProcfsMetrics` is a stateless method though.\r\n\r\nI understand the limitations imposed by the MetricsSystem but would still like you to evaluate alternate approaches. If you think this is the best way to achieve this, then it is fine.",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-17T19:34:53Z",
    "diffHunk": "@@ -19,14 +19,15 @@ package org.apache.spark.executor\n \n import com.codahale.metrics.{Gauge, MetricRegistry}\n \n+import org.apache.spark.SparkEnv\n import org.apache.spark.internal.config\n import org.apache.spark.metrics.source.Source\n-import org.apache.spark.SparkEnv\n \n private[executor] class ProcfsMetricsSource extends Source {\n   override val sourceName = \"procfs\"\n-  override val metricRegistry = new MetricRegistry()\n+  // We use numMetrics for tracking to only call computAllMetrics once per set of metrics"
  }, {
    "author": {
      "login": "squito"
    },
    "body": "I'm also with Ankur here, I don't understand the point of this.  other metrics, eg. `NettyMemoryMetrics` dont' seem to need to do the same thing w/ `numMetrics`, and nothing about the metrics api makes it look like you'd need to.  Why do you think this is necessary?  It seems you just need to return a `MetricRegistry` with all of the metrics registered there.",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-17T23:04:29Z",
    "diffHunk": "@@ -19,14 +19,15 @@ package org.apache.spark.executor\n \n import com.codahale.metrics.{Gauge, MetricRegistry}\n \n+import org.apache.spark.SparkEnv\n import org.apache.spark.internal.config\n import org.apache.spark.metrics.source.Source\n-import org.apache.spark.SparkEnv\n \n private[executor] class ProcfsMetricsSource extends Source {\n   override val sourceName = \"procfs\"\n-  override val metricRegistry = new MetricRegistry()\n+  // We use numMetrics for tracking to only call computAllMetrics once per set of metrics"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "I think this is good to be done here to avoid calling procfsMetricsGetter.computeAllMetrics to compute the same set of metrics multiple times. I think we had this discusion in other review as well, but there we removed the need for this by changing the ExecutorMetricType API. Here we can't change the dropwizard API",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-17T23:17:10Z",
    "diffHunk": "@@ -19,14 +19,15 @@ package org.apache.spark.executor\n \n import com.codahale.metrics.{Gauge, MetricRegistry}\n \n+import org.apache.spark.SparkEnv\n import org.apache.spark.internal.config\n import org.apache.spark.metrics.source.Source\n-import org.apache.spark.SparkEnv\n \n private[executor] class ProcfsMetricsSource extends Source {\n   override val sourceName = \"procfs\"\n-  override val metricRegistry = new MetricRegistry()\n+  // We use numMetrics for tracking to only call computAllMetrics once per set of metrics"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "BTW, NettyMemoryMetrics is implementing MetricSet and each metrics there also  have implemented Metric interface. As I responded in my earlier comment, If I go that route I will avoid this here, but then code in ProcfsMetricGetter will be much uglier and to be honest I don't want to change that since it took 5 months for us to reach an agreement there. The gain also wouldn't be that much. The purpose of this code here is to have a less impact on the performance by removing unnecessary calls.",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-17T23:42:27Z",
    "diffHunk": "@@ -19,14 +19,15 @@ package org.apache.spark.executor\n \n import com.codahale.metrics.{Gauge, MetricRegistry}\n \n+import org.apache.spark.SparkEnv\n import org.apache.spark.internal.config\n import org.apache.spark.metrics.source.Source\n-import org.apache.spark.SparkEnv\n \n private[executor] class ProcfsMetricsSource extends Source {\n   override val sourceName = \"procfs\"\n-  override val metricRegistry = new MetricRegistry()\n+  // We use numMetrics for tracking to only call computAllMetrics once per set of metrics"
  }],
  "prId": 23306
}]