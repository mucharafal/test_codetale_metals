[{
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "I'd include a brief discussion of threading concerns here -- that this object is accessed by all the task runner threads and by the metric polling thread.  Also mention that one thread might be updating the metrics at the same time as their being read, and so the metrics might not be consistent with each other (as some might be updated and some might not), but we're OK with that.",
    "commit": "7556d6ab8615e4fd216da16ec02a4d0020f65ce3",
    "createdAt": "2019-03-25T16:36:34Z",
    "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.executor\n+\n+import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}\n+\n+import scala.collection.mutable.HashMap\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.MemoryManager\n+import org.apache.spark.metrics.ExecutorMetricType\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * A class that polls executor metrics, and tracks their peaks per task and per stage.\n+ * Each executor keeps an instance of this class.\n+ *",
    "line": 45
  }, {
    "author": {
      "login": "wypoon"
    },
    "body": "Ok, I can do that. I already mention in a comment in the poll method that task runner threads may update the data structures used in this object concurrently with the polling.",
    "commit": "7556d6ab8615e4fd216da16ec02a4d0020f65ce3",
    "createdAt": "2019-03-25T18:40:00Z",
    "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.executor\n+\n+import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}\n+\n+import scala.collection.mutable.HashMap\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.MemoryManager\n+import org.apache.spark.metrics.ExecutorMetricType\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * A class that polls executor metrics, and tracks their peaks per task and per stage.\n+ * Each executor keeps an instance of this class.\n+ *",
    "line": 45
  }],
  "prId": 23767
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "I'd prefer executor-metric-poller",
    "commit": "7556d6ab8615e4fd216da16ec02a4d0020f65ce3",
    "createdAt": "2019-03-25T16:39:03Z",
    "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.executor\n+\n+import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}\n+\n+import scala.collection.mutable.HashMap\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.MemoryManager\n+import org.apache.spark.metrics.ExecutorMetricType\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * A class that polls executor metrics, and tracks their peaks per task and per stage.\n+ * Each executor keeps an instance of this class.\n+ *\n+ * @param memoryManager the memory manager used by the executor.\n+ * @param pollingInterval the polling interval in milliseconds.\n+ */\n+@DeveloperApi\n+private[spark] class ExecutorMetricsPoller(\n+    memoryManager: MemoryManager,\n+    pollingInterval: Long)\n+  extends Logging {\n+\n+  type StageKey = (Int, Int)\n+  // tuple for Task Count and Metric Peaks\n+  type TCMP = (AtomicLong, AtomicLongArray)\n+\n+  // Map of (stageId, stageAttemptId) to (count of running tasks, executor metric peaks)\n+  private val stageTCMP = new ConcurrentHashMap[StageKey, TCMP]\n+\n+  // Map of taskId to executor metric peaks\n+  private val taskMetricPeaks = new ConcurrentHashMap[Long, AtomicLongArray]\n+\n+  private val poller =\n+    if (pollingInterval > 0) {\n+      ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"executor-poller\")"
  }, {
    "author": {
      "login": "wypoon"
    },
    "body": "Sure, I can make the change.",
    "commit": "7556d6ab8615e4fd216da16ec02a4d0020f65ce3",
    "createdAt": "2019-03-25T18:41:00Z",
    "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.executor\n+\n+import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}\n+\n+import scala.collection.mutable.HashMap\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.MemoryManager\n+import org.apache.spark.metrics.ExecutorMetricType\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * A class that polls executor metrics, and tracks their peaks per task and per stage.\n+ * Each executor keeps an instance of this class.\n+ *\n+ * @param memoryManager the memory manager used by the executor.\n+ * @param pollingInterval the polling interval in milliseconds.\n+ */\n+@DeveloperApi\n+private[spark] class ExecutorMetricsPoller(\n+    memoryManager: MemoryManager,\n+    pollingInterval: Long)\n+  extends Logging {\n+\n+  type StageKey = (Int, Int)\n+  // tuple for Task Count and Metric Peaks\n+  type TCMP = (AtomicLong, AtomicLongArray)\n+\n+  // Map of (stageId, stageAttemptId) to (count of running tasks, executor metric peaks)\n+  private val stageTCMP = new ConcurrentHashMap[StageKey, TCMP]\n+\n+  // Map of taskId to executor metric peaks\n+  private val taskMetricPeaks = new ConcurrentHashMap[Long, AtomicLongArray]\n+\n+  private val poller =\n+    if (pollingInterval > 0) {\n+      ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"executor-poller\")"
  }],
  "prId": 23767
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "any reason you need this method, instead of just doing this in `onTaskStart`?  you have access to the `taskId` from there too, its a bit confusing to have them split up.",
    "commit": "7556d6ab8615e4fd216da16ec02a4d0020f65ce3",
    "createdAt": "2019-03-25T16:41:12Z",
    "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.executor\n+\n+import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}\n+\n+import scala.collection.mutable.HashMap\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.MemoryManager\n+import org.apache.spark.metrics.ExecutorMetricType\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * A class that polls executor metrics, and tracks their peaks per task and per stage.\n+ * Each executor keeps an instance of this class.\n+ *\n+ * @param memoryManager the memory manager used by the executor.\n+ * @param pollingInterval the polling interval in milliseconds.\n+ */\n+@DeveloperApi\n+private[spark] class ExecutorMetricsPoller(\n+    memoryManager: MemoryManager,\n+    pollingInterval: Long)\n+  extends Logging {\n+\n+  type StageKey = (Int, Int)\n+  // tuple for Task Count and Metric Peaks\n+  type TCMP = (AtomicLong, AtomicLongArray)\n+\n+  // Map of (stageId, stageAttemptId) to (count of running tasks, executor metric peaks)\n+  private val stageTCMP = new ConcurrentHashMap[StageKey, TCMP]\n+\n+  // Map of taskId to executor metric peaks\n+  private val taskMetricPeaks = new ConcurrentHashMap[Long, AtomicLongArray]\n+\n+  private val poller =\n+    if (pollingInterval > 0) {\n+      ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"executor-poller\")\n+    } else {\n+      null\n+    }\n+\n+  /**\n+   * Function to poll executor metrics.\n+   * On start, if pollingInterval is positive, this is scheduled to run at that interval.\n+   * Otherwise, this is called by the reportHeartBeat function defined in Executor and passed\n+   * to its Heartbeater.\n+   */\n+  def poll(): Unit = {\n+    // Note: Task runner threads may update stageTCMP or read from taskMetricPeaks concurrently\n+    // with this function via calls to methods of this class.\n+\n+    // get the latest values for the metrics\n+    val latestMetrics = ExecutorMetrics.getCurrentMetrics(memoryManager)\n+\n+    def compareAndUpdate(current: Long, latest: Long): Long =\n+      if (latest > current) latest else current\n+\n+    def updatePeaks(metrics: AtomicLongArray): Unit = {\n+      (0 until metrics.length).foreach { i =>\n+        metrics.getAndAccumulate(i, latestMetrics(i), compareAndUpdate)\n+      }\n+    }\n+\n+    // for each active stage, update the peaks\n+    stageTCMP.forEachValue(LONG_MAX_VALUE, v => updatePeaks(v._2))\n+\n+    // for each running task, update the peaks\n+    taskMetricPeaks.forEachValue(LONG_MAX_VALUE, updatePeaks)\n+  }\n+\n+  /** Starts the polling thread. */\n+  def start(): Unit = {\n+    if (poller != null) {\n+      val pollingTask = new Runnable() {\n+        override def run(): Unit = Utils.logUncaughtExceptions(poll())\n+      }\n+      poller.scheduleAtFixedRate(pollingTask, 0L, pollingInterval, TimeUnit.MILLISECONDS)\n+    }\n+  }\n+\n+  /**\n+   * Called by Executor#launchTask.\n+   *\n+   * @param taskId the id of the task being launched.\n+   */\n+  def onTaskLaunch(taskId: Long): Unit = {"
  }, {
    "author": {
      "login": "wypoon"
    },
    "body": "It seemed like the logical place to do so. Executor#launchTask puts a new entry in runningTasks, and that is when it should put a new entry in taskMetricPeaks too.",
    "commit": "7556d6ab8615e4fd216da16ec02a4d0020f65ce3",
    "createdAt": "2019-03-25T18:42:58Z",
    "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.executor\n+\n+import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}\n+\n+import scala.collection.mutable.HashMap\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.MemoryManager\n+import org.apache.spark.metrics.ExecutorMetricType\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * A class that polls executor metrics, and tracks their peaks per task and per stage.\n+ * Each executor keeps an instance of this class.\n+ *\n+ * @param memoryManager the memory manager used by the executor.\n+ * @param pollingInterval the polling interval in milliseconds.\n+ */\n+@DeveloperApi\n+private[spark] class ExecutorMetricsPoller(\n+    memoryManager: MemoryManager,\n+    pollingInterval: Long)\n+  extends Logging {\n+\n+  type StageKey = (Int, Int)\n+  // tuple for Task Count and Metric Peaks\n+  type TCMP = (AtomicLong, AtomicLongArray)\n+\n+  // Map of (stageId, stageAttemptId) to (count of running tasks, executor metric peaks)\n+  private val stageTCMP = new ConcurrentHashMap[StageKey, TCMP]\n+\n+  // Map of taskId to executor metric peaks\n+  private val taskMetricPeaks = new ConcurrentHashMap[Long, AtomicLongArray]\n+\n+  private val poller =\n+    if (pollingInterval > 0) {\n+      ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"executor-poller\")\n+    } else {\n+      null\n+    }\n+\n+  /**\n+   * Function to poll executor metrics.\n+   * On start, if pollingInterval is positive, this is scheduled to run at that interval.\n+   * Otherwise, this is called by the reportHeartBeat function defined in Executor and passed\n+   * to its Heartbeater.\n+   */\n+  def poll(): Unit = {\n+    // Note: Task runner threads may update stageTCMP or read from taskMetricPeaks concurrently\n+    // with this function via calls to methods of this class.\n+\n+    // get the latest values for the metrics\n+    val latestMetrics = ExecutorMetrics.getCurrentMetrics(memoryManager)\n+\n+    def compareAndUpdate(current: Long, latest: Long): Long =\n+      if (latest > current) latest else current\n+\n+    def updatePeaks(metrics: AtomicLongArray): Unit = {\n+      (0 until metrics.length).foreach { i =>\n+        metrics.getAndAccumulate(i, latestMetrics(i), compareAndUpdate)\n+      }\n+    }\n+\n+    // for each active stage, update the peaks\n+    stageTCMP.forEachValue(LONG_MAX_VALUE, v => updatePeaks(v._2))\n+\n+    // for each running task, update the peaks\n+    taskMetricPeaks.forEachValue(LONG_MAX_VALUE, updatePeaks)\n+  }\n+\n+  /** Starts the polling thread. */\n+  def start(): Unit = {\n+    if (poller != null) {\n+      val pollingTask = new Runnable() {\n+        override def run(): Unit = Utils.logUncaughtExceptions(poll())\n+      }\n+      poller.scheduleAtFixedRate(pollingTask, 0L, pollingInterval, TimeUnit.MILLISECONDS)\n+    }\n+  }\n+\n+  /**\n+   * Called by Executor#launchTask.\n+   *\n+   * @param taskId the id of the task being launched.\n+   */\n+  def onTaskLaunch(taskId: Long): Unit = {"
  }, {
    "author": {
      "login": "wypoon"
    },
    "body": "Combined with onTaskStart.",
    "commit": "7556d6ab8615e4fd216da16ec02a4d0020f65ce3",
    "createdAt": "2019-03-26T23:40:36Z",
    "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.executor\n+\n+import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}\n+\n+import scala.collection.mutable.HashMap\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.MemoryManager\n+import org.apache.spark.metrics.ExecutorMetricType\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * A class that polls executor metrics, and tracks their peaks per task and per stage.\n+ * Each executor keeps an instance of this class.\n+ *\n+ * @param memoryManager the memory manager used by the executor.\n+ * @param pollingInterval the polling interval in milliseconds.\n+ */\n+@DeveloperApi\n+private[spark] class ExecutorMetricsPoller(\n+    memoryManager: MemoryManager,\n+    pollingInterval: Long)\n+  extends Logging {\n+\n+  type StageKey = (Int, Int)\n+  // tuple for Task Count and Metric Peaks\n+  type TCMP = (AtomicLong, AtomicLongArray)\n+\n+  // Map of (stageId, stageAttemptId) to (count of running tasks, executor metric peaks)\n+  private val stageTCMP = new ConcurrentHashMap[StageKey, TCMP]\n+\n+  // Map of taskId to executor metric peaks\n+  private val taskMetricPeaks = new ConcurrentHashMap[Long, AtomicLongArray]\n+\n+  private val poller =\n+    if (pollingInterval > 0) {\n+      ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"executor-poller\")\n+    } else {\n+      null\n+    }\n+\n+  /**\n+   * Function to poll executor metrics.\n+   * On start, if pollingInterval is positive, this is scheduled to run at that interval.\n+   * Otherwise, this is called by the reportHeartBeat function defined in Executor and passed\n+   * to its Heartbeater.\n+   */\n+  def poll(): Unit = {\n+    // Note: Task runner threads may update stageTCMP or read from taskMetricPeaks concurrently\n+    // with this function via calls to methods of this class.\n+\n+    // get the latest values for the metrics\n+    val latestMetrics = ExecutorMetrics.getCurrentMetrics(memoryManager)\n+\n+    def compareAndUpdate(current: Long, latest: Long): Long =\n+      if (latest > current) latest else current\n+\n+    def updatePeaks(metrics: AtomicLongArray): Unit = {\n+      (0 until metrics.length).foreach { i =>\n+        metrics.getAndAccumulate(i, latestMetrics(i), compareAndUpdate)\n+      }\n+    }\n+\n+    // for each active stage, update the peaks\n+    stageTCMP.forEachValue(LONG_MAX_VALUE, v => updatePeaks(v._2))\n+\n+    // for each running task, update the peaks\n+    taskMetricPeaks.forEachValue(LONG_MAX_VALUE, updatePeaks)\n+  }\n+\n+  /** Starts the polling thread. */\n+  def start(): Unit = {\n+    if (poller != null) {\n+      val pollingTask = new Runnable() {\n+        override def run(): Unit = Utils.logUncaughtExceptions(poll())\n+      }\n+      poller.scheduleAtFixedRate(pollingTask, 0L, pollingInterval, TimeUnit.MILLISECONDS)\n+    }\n+  }\n+\n+  /**\n+   * Called by Executor#launchTask.\n+   *\n+   * @param taskId the id of the task being launched.\n+   */\n+  def onTaskLaunch(taskId: Long): Unit = {"
  }],
  "prId": 23767
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "you need to do all of this in `onTaskCleanup` too, don't you?  or rather, couldn't you just put everything in `onTaskCleanup` and get rid fo this?",
    "commit": "7556d6ab8615e4fd216da16ec02a4d0020f65ce3",
    "createdAt": "2019-03-25T16:43:15Z",
    "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.executor\n+\n+import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}\n+\n+import scala.collection.mutable.HashMap\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.MemoryManager\n+import org.apache.spark.metrics.ExecutorMetricType\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * A class that polls executor metrics, and tracks their peaks per task and per stage.\n+ * Each executor keeps an instance of this class.\n+ *\n+ * @param memoryManager the memory manager used by the executor.\n+ * @param pollingInterval the polling interval in milliseconds.\n+ */\n+@DeveloperApi\n+private[spark] class ExecutorMetricsPoller(\n+    memoryManager: MemoryManager,\n+    pollingInterval: Long)\n+  extends Logging {\n+\n+  type StageKey = (Int, Int)\n+  // tuple for Task Count and Metric Peaks\n+  type TCMP = (AtomicLong, AtomicLongArray)\n+\n+  // Map of (stageId, stageAttemptId) to (count of running tasks, executor metric peaks)\n+  private val stageTCMP = new ConcurrentHashMap[StageKey, TCMP]\n+\n+  // Map of taskId to executor metric peaks\n+  private val taskMetricPeaks = new ConcurrentHashMap[Long, AtomicLongArray]\n+\n+  private val poller =\n+    if (pollingInterval > 0) {\n+      ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"executor-poller\")\n+    } else {\n+      null\n+    }\n+\n+  /**\n+   * Function to poll executor metrics.\n+   * On start, if pollingInterval is positive, this is scheduled to run at that interval.\n+   * Otherwise, this is called by the reportHeartBeat function defined in Executor and passed\n+   * to its Heartbeater.\n+   */\n+  def poll(): Unit = {\n+    // Note: Task runner threads may update stageTCMP or read from taskMetricPeaks concurrently\n+    // with this function via calls to methods of this class.\n+\n+    // get the latest values for the metrics\n+    val latestMetrics = ExecutorMetrics.getCurrentMetrics(memoryManager)\n+\n+    def compareAndUpdate(current: Long, latest: Long): Long =\n+      if (latest > current) latest else current\n+\n+    def updatePeaks(metrics: AtomicLongArray): Unit = {\n+      (0 until metrics.length).foreach { i =>\n+        metrics.getAndAccumulate(i, latestMetrics(i), compareAndUpdate)\n+      }\n+    }\n+\n+    // for each active stage, update the peaks\n+    stageTCMP.forEachValue(LONG_MAX_VALUE, v => updatePeaks(v._2))\n+\n+    // for each running task, update the peaks\n+    taskMetricPeaks.forEachValue(LONG_MAX_VALUE, updatePeaks)\n+  }\n+\n+  /** Starts the polling thread. */\n+  def start(): Unit = {\n+    if (poller != null) {\n+      val pollingTask = new Runnable() {\n+        override def run(): Unit = Utils.logUncaughtExceptions(poll())\n+      }\n+      poller.scheduleAtFixedRate(pollingTask, 0L, pollingInterval, TimeUnit.MILLISECONDS)\n+    }\n+  }\n+\n+  /**\n+   * Called by Executor#launchTask.\n+   *\n+   * @param taskId the id of the task being launched.\n+   */\n+  def onTaskLaunch(taskId: Long): Unit = {\n+    taskMetricPeaks.put(taskId, new AtomicLongArray(ExecutorMetricType.numMetrics))\n+  }\n+\n+  /**\n+   * Called by TaskRunner#run.\n+   *\n+   * @param stageId the id of the stage the task belongs to.\n+   * @param stageAttemptId the attempt number of the stage the task belongs to.\n+   */\n+  def onTaskStart(stageId: Int, stageAttemptId: Int): Unit = {\n+    // Put a new entry in stageTCMP for the stage if there isn't one already.\n+    // Increment the task count.\n+    val (count, _) = stageTCMP.computeIfAbsent((stageId, stageAttemptId),\n+      _ => (new AtomicLong(0), new AtomicLongArray(ExecutorMetricType.numMetrics)))\n+    val stageCount = count.incrementAndGet()\n+    logDebug(s\"stageTCMP: ($stageId, $stageAttemptId) -> $stageCount\")\n+  }\n+\n+  /**\n+   * Called by TaskRunner#run.\n+   *\n+   * @param stageId the id of the stage the task belongs to.\n+   * @param stageAttemptId the attempt number of the stage the task belongs to.\n+   */\n+  def onTaskCompletion(stageId: Int, stageAttemptId: Int): Unit = {"
  }, {
    "author": {
      "login": "wypoon"
    },
    "body": "No, we do not need to do this in onTaskCleanup. onTaskCleanup is called in the finally block of a try-catch-finally in TaskRunner#run. The only things to clean up are the entry in runningTasks and taskMetricPeaks.",
    "commit": "7556d6ab8615e4fd216da16ec02a4d0020f65ce3",
    "createdAt": "2019-03-25T18:46:27Z",
    "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.executor\n+\n+import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}\n+\n+import scala.collection.mutable.HashMap\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.MemoryManager\n+import org.apache.spark.metrics.ExecutorMetricType\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * A class that polls executor metrics, and tracks their peaks per task and per stage.\n+ * Each executor keeps an instance of this class.\n+ *\n+ * @param memoryManager the memory manager used by the executor.\n+ * @param pollingInterval the polling interval in milliseconds.\n+ */\n+@DeveloperApi\n+private[spark] class ExecutorMetricsPoller(\n+    memoryManager: MemoryManager,\n+    pollingInterval: Long)\n+  extends Logging {\n+\n+  type StageKey = (Int, Int)\n+  // tuple for Task Count and Metric Peaks\n+  type TCMP = (AtomicLong, AtomicLongArray)\n+\n+  // Map of (stageId, stageAttemptId) to (count of running tasks, executor metric peaks)\n+  private val stageTCMP = new ConcurrentHashMap[StageKey, TCMP]\n+\n+  // Map of taskId to executor metric peaks\n+  private val taskMetricPeaks = new ConcurrentHashMap[Long, AtomicLongArray]\n+\n+  private val poller =\n+    if (pollingInterval > 0) {\n+      ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"executor-poller\")\n+    } else {\n+      null\n+    }\n+\n+  /**\n+   * Function to poll executor metrics.\n+   * On start, if pollingInterval is positive, this is scheduled to run at that interval.\n+   * Otherwise, this is called by the reportHeartBeat function defined in Executor and passed\n+   * to its Heartbeater.\n+   */\n+  def poll(): Unit = {\n+    // Note: Task runner threads may update stageTCMP or read from taskMetricPeaks concurrently\n+    // with this function via calls to methods of this class.\n+\n+    // get the latest values for the metrics\n+    val latestMetrics = ExecutorMetrics.getCurrentMetrics(memoryManager)\n+\n+    def compareAndUpdate(current: Long, latest: Long): Long =\n+      if (latest > current) latest else current\n+\n+    def updatePeaks(metrics: AtomicLongArray): Unit = {\n+      (0 until metrics.length).foreach { i =>\n+        metrics.getAndAccumulate(i, latestMetrics(i), compareAndUpdate)\n+      }\n+    }\n+\n+    // for each active stage, update the peaks\n+    stageTCMP.forEachValue(LONG_MAX_VALUE, v => updatePeaks(v._2))\n+\n+    // for each running task, update the peaks\n+    taskMetricPeaks.forEachValue(LONG_MAX_VALUE, updatePeaks)\n+  }\n+\n+  /** Starts the polling thread. */\n+  def start(): Unit = {\n+    if (poller != null) {\n+      val pollingTask = new Runnable() {\n+        override def run(): Unit = Utils.logUncaughtExceptions(poll())\n+      }\n+      poller.scheduleAtFixedRate(pollingTask, 0L, pollingInterval, TimeUnit.MILLISECONDS)\n+    }\n+  }\n+\n+  /**\n+   * Called by Executor#launchTask.\n+   *\n+   * @param taskId the id of the task being launched.\n+   */\n+  def onTaskLaunch(taskId: Long): Unit = {\n+    taskMetricPeaks.put(taskId, new AtomicLongArray(ExecutorMetricType.numMetrics))\n+  }\n+\n+  /**\n+   * Called by TaskRunner#run.\n+   *\n+   * @param stageId the id of the stage the task belongs to.\n+   * @param stageAttemptId the attempt number of the stage the task belongs to.\n+   */\n+  def onTaskStart(stageId: Int, stageAttemptId: Int): Unit = {\n+    // Put a new entry in stageTCMP for the stage if there isn't one already.\n+    // Increment the task count.\n+    val (count, _) = stageTCMP.computeIfAbsent((stageId, stageAttemptId),\n+      _ => (new AtomicLong(0), new AtomicLongArray(ExecutorMetricType.numMetrics)))\n+    val stageCount = count.incrementAndGet()\n+    logDebug(s\"stageTCMP: ($stageId, $stageAttemptId) -> $stageCount\")\n+  }\n+\n+  /**\n+   * Called by TaskRunner#run.\n+   *\n+   * @param stageId the id of the stage the task belongs to.\n+   * @param stageAttemptId the attempt number of the stage the task belongs to.\n+   */\n+  def onTaskCompletion(stageId: Int, stageAttemptId: Int): Unit = {"
  }, {
    "author": {
      "login": "squito"
    },
    "body": "but don't you need this code executed in the `finally` too?  Suppose the task fails with an exception -- won't `stageTCMP` not get updated correctly?",
    "commit": "7556d6ab8615e4fd216da16ec02a4d0020f65ce3",
    "createdAt": "2019-03-25T19:41:49Z",
    "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.executor\n+\n+import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}\n+\n+import scala.collection.mutable.HashMap\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.MemoryManager\n+import org.apache.spark.metrics.ExecutorMetricType\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * A class that polls executor metrics, and tracks their peaks per task and per stage.\n+ * Each executor keeps an instance of this class.\n+ *\n+ * @param memoryManager the memory manager used by the executor.\n+ * @param pollingInterval the polling interval in milliseconds.\n+ */\n+@DeveloperApi\n+private[spark] class ExecutorMetricsPoller(\n+    memoryManager: MemoryManager,\n+    pollingInterval: Long)\n+  extends Logging {\n+\n+  type StageKey = (Int, Int)\n+  // tuple for Task Count and Metric Peaks\n+  type TCMP = (AtomicLong, AtomicLongArray)\n+\n+  // Map of (stageId, stageAttemptId) to (count of running tasks, executor metric peaks)\n+  private val stageTCMP = new ConcurrentHashMap[StageKey, TCMP]\n+\n+  // Map of taskId to executor metric peaks\n+  private val taskMetricPeaks = new ConcurrentHashMap[Long, AtomicLongArray]\n+\n+  private val poller =\n+    if (pollingInterval > 0) {\n+      ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"executor-poller\")\n+    } else {\n+      null\n+    }\n+\n+  /**\n+   * Function to poll executor metrics.\n+   * On start, if pollingInterval is positive, this is scheduled to run at that interval.\n+   * Otherwise, this is called by the reportHeartBeat function defined in Executor and passed\n+   * to its Heartbeater.\n+   */\n+  def poll(): Unit = {\n+    // Note: Task runner threads may update stageTCMP or read from taskMetricPeaks concurrently\n+    // with this function via calls to methods of this class.\n+\n+    // get the latest values for the metrics\n+    val latestMetrics = ExecutorMetrics.getCurrentMetrics(memoryManager)\n+\n+    def compareAndUpdate(current: Long, latest: Long): Long =\n+      if (latest > current) latest else current\n+\n+    def updatePeaks(metrics: AtomicLongArray): Unit = {\n+      (0 until metrics.length).foreach { i =>\n+        metrics.getAndAccumulate(i, latestMetrics(i), compareAndUpdate)\n+      }\n+    }\n+\n+    // for each active stage, update the peaks\n+    stageTCMP.forEachValue(LONG_MAX_VALUE, v => updatePeaks(v._2))\n+\n+    // for each running task, update the peaks\n+    taskMetricPeaks.forEachValue(LONG_MAX_VALUE, updatePeaks)\n+  }\n+\n+  /** Starts the polling thread. */\n+  def start(): Unit = {\n+    if (poller != null) {\n+      val pollingTask = new Runnable() {\n+        override def run(): Unit = Utils.logUncaughtExceptions(poll())\n+      }\n+      poller.scheduleAtFixedRate(pollingTask, 0L, pollingInterval, TimeUnit.MILLISECONDS)\n+    }\n+  }\n+\n+  /**\n+   * Called by Executor#launchTask.\n+   *\n+   * @param taskId the id of the task being launched.\n+   */\n+  def onTaskLaunch(taskId: Long): Unit = {\n+    taskMetricPeaks.put(taskId, new AtomicLongArray(ExecutorMetricType.numMetrics))\n+  }\n+\n+  /**\n+   * Called by TaskRunner#run.\n+   *\n+   * @param stageId the id of the stage the task belongs to.\n+   * @param stageAttemptId the attempt number of the stage the task belongs to.\n+   */\n+  def onTaskStart(stageId: Int, stageAttemptId: Int): Unit = {\n+    // Put a new entry in stageTCMP for the stage if there isn't one already.\n+    // Increment the task count.\n+    val (count, _) = stageTCMP.computeIfAbsent((stageId, stageAttemptId),\n+      _ => (new AtomicLong(0), new AtomicLongArray(ExecutorMetricType.numMetrics)))\n+    val stageCount = count.incrementAndGet()\n+    logDebug(s\"stageTCMP: ($stageId, $stageAttemptId) -> $stageCount\")\n+  }\n+\n+  /**\n+   * Called by TaskRunner#run.\n+   *\n+   * @param stageId the id of the stage the task belongs to.\n+   * @param stageAttemptId the attempt number of the stage the task belongs to.\n+   */\n+  def onTaskCompletion(stageId: Int, stageAttemptId: Int): Unit = {"
  }, {
    "author": {
      "login": "wypoon"
    },
    "body": "You're right. I'll combine onTaskCompletion and onTaskCleanup into one method that gets called in that finally block.",
    "commit": "7556d6ab8615e4fd216da16ec02a4d0020f65ce3",
    "createdAt": "2019-03-26T18:38:26Z",
    "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.executor\n+\n+import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}\n+\n+import scala.collection.mutable.HashMap\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.MemoryManager\n+import org.apache.spark.metrics.ExecutorMetricType\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * A class that polls executor metrics, and tracks their peaks per task and per stage.\n+ * Each executor keeps an instance of this class.\n+ *\n+ * @param memoryManager the memory manager used by the executor.\n+ * @param pollingInterval the polling interval in milliseconds.\n+ */\n+@DeveloperApi\n+private[spark] class ExecutorMetricsPoller(\n+    memoryManager: MemoryManager,\n+    pollingInterval: Long)\n+  extends Logging {\n+\n+  type StageKey = (Int, Int)\n+  // tuple for Task Count and Metric Peaks\n+  type TCMP = (AtomicLong, AtomicLongArray)\n+\n+  // Map of (stageId, stageAttemptId) to (count of running tasks, executor metric peaks)\n+  private val stageTCMP = new ConcurrentHashMap[StageKey, TCMP]\n+\n+  // Map of taskId to executor metric peaks\n+  private val taskMetricPeaks = new ConcurrentHashMap[Long, AtomicLongArray]\n+\n+  private val poller =\n+    if (pollingInterval > 0) {\n+      ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"executor-poller\")\n+    } else {\n+      null\n+    }\n+\n+  /**\n+   * Function to poll executor metrics.\n+   * On start, if pollingInterval is positive, this is scheduled to run at that interval.\n+   * Otherwise, this is called by the reportHeartBeat function defined in Executor and passed\n+   * to its Heartbeater.\n+   */\n+  def poll(): Unit = {\n+    // Note: Task runner threads may update stageTCMP or read from taskMetricPeaks concurrently\n+    // with this function via calls to methods of this class.\n+\n+    // get the latest values for the metrics\n+    val latestMetrics = ExecutorMetrics.getCurrentMetrics(memoryManager)\n+\n+    def compareAndUpdate(current: Long, latest: Long): Long =\n+      if (latest > current) latest else current\n+\n+    def updatePeaks(metrics: AtomicLongArray): Unit = {\n+      (0 until metrics.length).foreach { i =>\n+        metrics.getAndAccumulate(i, latestMetrics(i), compareAndUpdate)\n+      }\n+    }\n+\n+    // for each active stage, update the peaks\n+    stageTCMP.forEachValue(LONG_MAX_VALUE, v => updatePeaks(v._2))\n+\n+    // for each running task, update the peaks\n+    taskMetricPeaks.forEachValue(LONG_MAX_VALUE, updatePeaks)\n+  }\n+\n+  /** Starts the polling thread. */\n+  def start(): Unit = {\n+    if (poller != null) {\n+      val pollingTask = new Runnable() {\n+        override def run(): Unit = Utils.logUncaughtExceptions(poll())\n+      }\n+      poller.scheduleAtFixedRate(pollingTask, 0L, pollingInterval, TimeUnit.MILLISECONDS)\n+    }\n+  }\n+\n+  /**\n+   * Called by Executor#launchTask.\n+   *\n+   * @param taskId the id of the task being launched.\n+   */\n+  def onTaskLaunch(taskId: Long): Unit = {\n+    taskMetricPeaks.put(taskId, new AtomicLongArray(ExecutorMetricType.numMetrics))\n+  }\n+\n+  /**\n+   * Called by TaskRunner#run.\n+   *\n+   * @param stageId the id of the stage the task belongs to.\n+   * @param stageAttemptId the attempt number of the stage the task belongs to.\n+   */\n+  def onTaskStart(stageId: Int, stageAttemptId: Int): Unit = {\n+    // Put a new entry in stageTCMP for the stage if there isn't one already.\n+    // Increment the task count.\n+    val (count, _) = stageTCMP.computeIfAbsent((stageId, stageAttemptId),\n+      _ => (new AtomicLong(0), new AtomicLongArray(ExecutorMetricType.numMetrics)))\n+    val stageCount = count.incrementAndGet()\n+    logDebug(s\"stageTCMP: ($stageId, $stageAttemptId) -> $stageCount\")\n+  }\n+\n+  /**\n+   * Called by TaskRunner#run.\n+   *\n+   * @param stageId the id of the stage the task belongs to.\n+   * @param stageAttemptId the attempt number of the stage the task belongs to.\n+   */\n+  def onTaskCompletion(stageId: Int, stageAttemptId: Int): Unit = {"
  }, {
    "author": {
      "login": "wypoon"
    },
    "body": "It turns out that there is some trickiness here. The stageId and stageAttemptId are defined in the try block after the task is deserialized, and are not available in the finally block. If task deserialization fails, for some reason, stageId and stageAttemptId will be undefined, and we don't want to execute this code in either the catch block or the finally block.\r\n\r\nI use a flag to indicate that the task was successfully deserialized (so stageId and stageAttemptId are known) and onTaskStart was called. Then in the finally block I can recall the stageId and stageAttemptId and call onTaskCompletion.",
    "commit": "7556d6ab8615e4fd216da16ec02a4d0020f65ce3",
    "createdAt": "2019-03-26T23:15:01Z",
    "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.executor\n+\n+import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}\n+\n+import scala.collection.mutable.HashMap\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.MemoryManager\n+import org.apache.spark.metrics.ExecutorMetricType\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * A class that polls executor metrics, and tracks their peaks per task and per stage.\n+ * Each executor keeps an instance of this class.\n+ *\n+ * @param memoryManager the memory manager used by the executor.\n+ * @param pollingInterval the polling interval in milliseconds.\n+ */\n+@DeveloperApi\n+private[spark] class ExecutorMetricsPoller(\n+    memoryManager: MemoryManager,\n+    pollingInterval: Long)\n+  extends Logging {\n+\n+  type StageKey = (Int, Int)\n+  // tuple for Task Count and Metric Peaks\n+  type TCMP = (AtomicLong, AtomicLongArray)\n+\n+  // Map of (stageId, stageAttemptId) to (count of running tasks, executor metric peaks)\n+  private val stageTCMP = new ConcurrentHashMap[StageKey, TCMP]\n+\n+  // Map of taskId to executor metric peaks\n+  private val taskMetricPeaks = new ConcurrentHashMap[Long, AtomicLongArray]\n+\n+  private val poller =\n+    if (pollingInterval > 0) {\n+      ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"executor-poller\")\n+    } else {\n+      null\n+    }\n+\n+  /**\n+   * Function to poll executor metrics.\n+   * On start, if pollingInterval is positive, this is scheduled to run at that interval.\n+   * Otherwise, this is called by the reportHeartBeat function defined in Executor and passed\n+   * to its Heartbeater.\n+   */\n+  def poll(): Unit = {\n+    // Note: Task runner threads may update stageTCMP or read from taskMetricPeaks concurrently\n+    // with this function via calls to methods of this class.\n+\n+    // get the latest values for the metrics\n+    val latestMetrics = ExecutorMetrics.getCurrentMetrics(memoryManager)\n+\n+    def compareAndUpdate(current: Long, latest: Long): Long =\n+      if (latest > current) latest else current\n+\n+    def updatePeaks(metrics: AtomicLongArray): Unit = {\n+      (0 until metrics.length).foreach { i =>\n+        metrics.getAndAccumulate(i, latestMetrics(i), compareAndUpdate)\n+      }\n+    }\n+\n+    // for each active stage, update the peaks\n+    stageTCMP.forEachValue(LONG_MAX_VALUE, v => updatePeaks(v._2))\n+\n+    // for each running task, update the peaks\n+    taskMetricPeaks.forEachValue(LONG_MAX_VALUE, updatePeaks)\n+  }\n+\n+  /** Starts the polling thread. */\n+  def start(): Unit = {\n+    if (poller != null) {\n+      val pollingTask = new Runnable() {\n+        override def run(): Unit = Utils.logUncaughtExceptions(poll())\n+      }\n+      poller.scheduleAtFixedRate(pollingTask, 0L, pollingInterval, TimeUnit.MILLISECONDS)\n+    }\n+  }\n+\n+  /**\n+   * Called by Executor#launchTask.\n+   *\n+   * @param taskId the id of the task being launched.\n+   */\n+  def onTaskLaunch(taskId: Long): Unit = {\n+    taskMetricPeaks.put(taskId, new AtomicLongArray(ExecutorMetricType.numMetrics))\n+  }\n+\n+  /**\n+   * Called by TaskRunner#run.\n+   *\n+   * @param stageId the id of the stage the task belongs to.\n+   * @param stageAttemptId the attempt number of the stage the task belongs to.\n+   */\n+  def onTaskStart(stageId: Int, stageAttemptId: Int): Unit = {\n+    // Put a new entry in stageTCMP for the stage if there isn't one already.\n+    // Increment the task count.\n+    val (count, _) = stageTCMP.computeIfAbsent((stageId, stageAttemptId),\n+      _ => (new AtomicLong(0), new AtomicLongArray(ExecutorMetricType.numMetrics)))\n+    val stageCount = count.incrementAndGet()\n+    logDebug(s\"stageTCMP: ($stageId, $stageAttemptId) -> $stageCount\")\n+  }\n+\n+  /**\n+   * Called by TaskRunner#run.\n+   *\n+   * @param stageId the id of the stage the task belongs to.\n+   * @param stageAttemptId the attempt number of the stage the task belongs to.\n+   */\n+  def onTaskCompletion(stageId: Int, stageAttemptId: Int): Unit = {"
  }],
  "prId": 23767
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "couldn't you use the version of `forEach` which only needs one argument here?\r\n\r\n```\r\ndef accumulatePeaks(stage: StageKey, metrics: TCMP): Unit = {\r\n  // only count stage if taskcount > 0\r\n  if (metrics._1.get() > 0) executorUpdates.put(k, new ExecutorMetrics(metrics._2))\r\n}\r\nstageTCMP.forEach(accumulatePeaks)\r\n```\r\n\r\nwhat's the situation where you'd have an entry for the stage, but the taskcount == 0?\r\n\r\nalso just curious -- are the type parameters really necessary here?  I would have expected the scala compiler to be fine without them, though maybe the java 8 lambda types make this not work somehow?",
    "commit": "7556d6ab8615e4fd216da16ec02a4d0020f65ce3",
    "createdAt": "2019-03-25T16:55:28Z",
    "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.executor\n+\n+import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}\n+\n+import scala.collection.mutable.HashMap\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.MemoryManager\n+import org.apache.spark.metrics.ExecutorMetricType\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * A class that polls executor metrics, and tracks their peaks per task and per stage.\n+ * Each executor keeps an instance of this class.\n+ *\n+ * @param memoryManager the memory manager used by the executor.\n+ * @param pollingInterval the polling interval in milliseconds.\n+ */\n+@DeveloperApi\n+private[spark] class ExecutorMetricsPoller(\n+    memoryManager: MemoryManager,\n+    pollingInterval: Long)\n+  extends Logging {\n+\n+  type StageKey = (Int, Int)\n+  // tuple for Task Count and Metric Peaks\n+  type TCMP = (AtomicLong, AtomicLongArray)\n+\n+  // Map of (stageId, stageAttemptId) to (count of running tasks, executor metric peaks)\n+  private val stageTCMP = new ConcurrentHashMap[StageKey, TCMP]\n+\n+  // Map of taskId to executor metric peaks\n+  private val taskMetricPeaks = new ConcurrentHashMap[Long, AtomicLongArray]\n+\n+  private val poller =\n+    if (pollingInterval > 0) {\n+      ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"executor-poller\")\n+    } else {\n+      null\n+    }\n+\n+  /**\n+   * Function to poll executor metrics.\n+   * On start, if pollingInterval is positive, this is scheduled to run at that interval.\n+   * Otherwise, this is called by the reportHeartBeat function defined in Executor and passed\n+   * to its Heartbeater.\n+   */\n+  def poll(): Unit = {\n+    // Note: Task runner threads may update stageTCMP or read from taskMetricPeaks concurrently\n+    // with this function via calls to methods of this class.\n+\n+    // get the latest values for the metrics\n+    val latestMetrics = ExecutorMetrics.getCurrentMetrics(memoryManager)\n+\n+    def compareAndUpdate(current: Long, latest: Long): Long =\n+      if (latest > current) latest else current\n+\n+    def updatePeaks(metrics: AtomicLongArray): Unit = {\n+      (0 until metrics.length).foreach { i =>\n+        metrics.getAndAccumulate(i, latestMetrics(i), compareAndUpdate)\n+      }\n+    }\n+\n+    // for each active stage, update the peaks\n+    stageTCMP.forEachValue(LONG_MAX_VALUE, v => updatePeaks(v._2))\n+\n+    // for each running task, update the peaks\n+    taskMetricPeaks.forEachValue(LONG_MAX_VALUE, updatePeaks)\n+  }\n+\n+  /** Starts the polling thread. */\n+  def start(): Unit = {\n+    if (poller != null) {\n+      val pollingTask = new Runnable() {\n+        override def run(): Unit = Utils.logUncaughtExceptions(poll())\n+      }\n+      poller.scheduleAtFixedRate(pollingTask, 0L, pollingInterval, TimeUnit.MILLISECONDS)\n+    }\n+  }\n+\n+  /**\n+   * Called by Executor#launchTask.\n+   *\n+   * @param taskId the id of the task being launched.\n+   */\n+  def onTaskLaunch(taskId: Long): Unit = {\n+    taskMetricPeaks.put(taskId, new AtomicLongArray(ExecutorMetricType.numMetrics))\n+  }\n+\n+  /**\n+   * Called by TaskRunner#run.\n+   *\n+   * @param stageId the id of the stage the task belongs to.\n+   * @param stageAttemptId the attempt number of the stage the task belongs to.\n+   */\n+  def onTaskStart(stageId: Int, stageAttemptId: Int): Unit = {\n+    // Put a new entry in stageTCMP for the stage if there isn't one already.\n+    // Increment the task count.\n+    val (count, _) = stageTCMP.computeIfAbsent((stageId, stageAttemptId),\n+      _ => (new AtomicLong(0), new AtomicLongArray(ExecutorMetricType.numMetrics)))\n+    val stageCount = count.incrementAndGet()\n+    logDebug(s\"stageTCMP: ($stageId, $stageAttemptId) -> $stageCount\")\n+  }\n+\n+  /**\n+   * Called by TaskRunner#run.\n+   *\n+   * @param stageId the id of the stage the task belongs to.\n+   * @param stageAttemptId the attempt number of the stage the task belongs to.\n+   */\n+  def onTaskCompletion(stageId: Int, stageAttemptId: Int): Unit = {\n+    // Decrement the task count.\n+    // Remove the entry from stageTCMP if the task count reaches zero\n+\n+    def decrementCount(stage: StageKey, countAndPeaks: TCMP): TCMP = {\n+      val count = countAndPeaks._1\n+      val countValue = count.decrementAndGet()\n+      if (countValue == 0L) {\n+        logDebug(s\"removing (${stage._1}, ${stage._2}) from stageTCMP\")\n+        null\n+      } else {\n+        logDebug(s\"stageTCMP: (${stage._1}, ${stage._2}) -> \" + countValue)\n+        countAndPeaks\n+      }\n+    }\n+\n+    stageTCMP.computeIfPresent((stageId, stageAttemptId), decrementCount)\n+  }\n+\n+  /**\n+   * Called by TaskRunner#run.\n+   *\n+   * @param taskId the id of the task that was run.\n+   */\n+  def onTaskCleanup(taskId: Long): Unit = {\n+    taskMetricPeaks.remove(taskId)\n+  }\n+\n+  /**\n+   * Called by TaskRunner#run.\n+   *\n+   * @param taskId the id of the task that was run.\n+   */\n+  def getTaskMetricPeaks(taskId: Long): Array[Long] = {\n+    val currentPeaks = taskMetricPeaks.get(taskId)\n+    val metricPeaks = new Array[Long](ExecutorMetricType.numMetrics)\n+    ExecutorMetricType.metricToOffset.foreach { case (_, i) =>\n+      metricPeaks(i) = currentPeaks.get(i)\n+    }\n+    metricPeaks\n+  }\n+\n+\n+  /**\n+   * Called by the reportHeartBeat function defined in Executor and passed to its Heartbeater.\n+   * It resets the metric peaks in stageTCMP before returning the executor updates.\n+   * Thus, the executor updates contains the per-stage metric peaks since the last heartbeat\n+   * (the last time this method was called).\n+   */\n+  def getExecutorUpdates(): HashMap[StageKey, ExecutorMetrics] = {\n+    // build the executor level memory metrics\n+    val executorUpdates = new HashMap[StageKey, ExecutorMetrics]\n+\n+    def peaksForStage(k: StageKey, v: TCMP): (StageKey, AtomicLongArray) =\n+      if (v._1.get() > 0) (k, v._2) else null\n+\n+    def addPeaks(nested: (StageKey, AtomicLongArray)): Unit = {\n+      val (k, v) = nested\n+      executorUpdates.put(k, new ExecutorMetrics(v))\n+    }\n+\n+    stageTCMP.forEach[(StageKey, AtomicLongArray)](LONG_MAX_VALUE, peaksForStage, addPeaks)"
  }, {
    "author": {
      "login": "wypoon"
    },
    "body": "I think I first used explicit type parameters to try to get IntelliJ to understand the method call when I was trying to use inlined lambdas. In the end, I gave up on the inlined lambdas. I guess now I don't need the type parameters anymore.\r\n\r\nTo answer the more important question, peaksForStage is a relic of when I had the separate CHMs, activeStages and stageMetricPeaks. There should never be an entry for the stage with the taskcount == 0.\r\n\r\n",
    "commit": "7556d6ab8615e4fd216da16ec02a4d0020f65ce3",
    "createdAt": "2019-03-26T20:19:38Z",
    "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.executor\n+\n+import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}\n+\n+import scala.collection.mutable.HashMap\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.MemoryManager\n+import org.apache.spark.metrics.ExecutorMetricType\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * A class that polls executor metrics, and tracks their peaks per task and per stage.\n+ * Each executor keeps an instance of this class.\n+ *\n+ * @param memoryManager the memory manager used by the executor.\n+ * @param pollingInterval the polling interval in milliseconds.\n+ */\n+@DeveloperApi\n+private[spark] class ExecutorMetricsPoller(\n+    memoryManager: MemoryManager,\n+    pollingInterval: Long)\n+  extends Logging {\n+\n+  type StageKey = (Int, Int)\n+  // tuple for Task Count and Metric Peaks\n+  type TCMP = (AtomicLong, AtomicLongArray)\n+\n+  // Map of (stageId, stageAttemptId) to (count of running tasks, executor metric peaks)\n+  private val stageTCMP = new ConcurrentHashMap[StageKey, TCMP]\n+\n+  // Map of taskId to executor metric peaks\n+  private val taskMetricPeaks = new ConcurrentHashMap[Long, AtomicLongArray]\n+\n+  private val poller =\n+    if (pollingInterval > 0) {\n+      ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"executor-poller\")\n+    } else {\n+      null\n+    }\n+\n+  /**\n+   * Function to poll executor metrics.\n+   * On start, if pollingInterval is positive, this is scheduled to run at that interval.\n+   * Otherwise, this is called by the reportHeartBeat function defined in Executor and passed\n+   * to its Heartbeater.\n+   */\n+  def poll(): Unit = {\n+    // Note: Task runner threads may update stageTCMP or read from taskMetricPeaks concurrently\n+    // with this function via calls to methods of this class.\n+\n+    // get the latest values for the metrics\n+    val latestMetrics = ExecutorMetrics.getCurrentMetrics(memoryManager)\n+\n+    def compareAndUpdate(current: Long, latest: Long): Long =\n+      if (latest > current) latest else current\n+\n+    def updatePeaks(metrics: AtomicLongArray): Unit = {\n+      (0 until metrics.length).foreach { i =>\n+        metrics.getAndAccumulate(i, latestMetrics(i), compareAndUpdate)\n+      }\n+    }\n+\n+    // for each active stage, update the peaks\n+    stageTCMP.forEachValue(LONG_MAX_VALUE, v => updatePeaks(v._2))\n+\n+    // for each running task, update the peaks\n+    taskMetricPeaks.forEachValue(LONG_MAX_VALUE, updatePeaks)\n+  }\n+\n+  /** Starts the polling thread. */\n+  def start(): Unit = {\n+    if (poller != null) {\n+      val pollingTask = new Runnable() {\n+        override def run(): Unit = Utils.logUncaughtExceptions(poll())\n+      }\n+      poller.scheduleAtFixedRate(pollingTask, 0L, pollingInterval, TimeUnit.MILLISECONDS)\n+    }\n+  }\n+\n+  /**\n+   * Called by Executor#launchTask.\n+   *\n+   * @param taskId the id of the task being launched.\n+   */\n+  def onTaskLaunch(taskId: Long): Unit = {\n+    taskMetricPeaks.put(taskId, new AtomicLongArray(ExecutorMetricType.numMetrics))\n+  }\n+\n+  /**\n+   * Called by TaskRunner#run.\n+   *\n+   * @param stageId the id of the stage the task belongs to.\n+   * @param stageAttemptId the attempt number of the stage the task belongs to.\n+   */\n+  def onTaskStart(stageId: Int, stageAttemptId: Int): Unit = {\n+    // Put a new entry in stageTCMP for the stage if there isn't one already.\n+    // Increment the task count.\n+    val (count, _) = stageTCMP.computeIfAbsent((stageId, stageAttemptId),\n+      _ => (new AtomicLong(0), new AtomicLongArray(ExecutorMetricType.numMetrics)))\n+    val stageCount = count.incrementAndGet()\n+    logDebug(s\"stageTCMP: ($stageId, $stageAttemptId) -> $stageCount\")\n+  }\n+\n+  /**\n+   * Called by TaskRunner#run.\n+   *\n+   * @param stageId the id of the stage the task belongs to.\n+   * @param stageAttemptId the attempt number of the stage the task belongs to.\n+   */\n+  def onTaskCompletion(stageId: Int, stageAttemptId: Int): Unit = {\n+    // Decrement the task count.\n+    // Remove the entry from stageTCMP if the task count reaches zero\n+\n+    def decrementCount(stage: StageKey, countAndPeaks: TCMP): TCMP = {\n+      val count = countAndPeaks._1\n+      val countValue = count.decrementAndGet()\n+      if (countValue == 0L) {\n+        logDebug(s\"removing (${stage._1}, ${stage._2}) from stageTCMP\")\n+        null\n+      } else {\n+        logDebug(s\"stageTCMP: (${stage._1}, ${stage._2}) -> \" + countValue)\n+        countAndPeaks\n+      }\n+    }\n+\n+    stageTCMP.computeIfPresent((stageId, stageAttemptId), decrementCount)\n+  }\n+\n+  /**\n+   * Called by TaskRunner#run.\n+   *\n+   * @param taskId the id of the task that was run.\n+   */\n+  def onTaskCleanup(taskId: Long): Unit = {\n+    taskMetricPeaks.remove(taskId)\n+  }\n+\n+  /**\n+   * Called by TaskRunner#run.\n+   *\n+   * @param taskId the id of the task that was run.\n+   */\n+  def getTaskMetricPeaks(taskId: Long): Array[Long] = {\n+    val currentPeaks = taskMetricPeaks.get(taskId)\n+    val metricPeaks = new Array[Long](ExecutorMetricType.numMetrics)\n+    ExecutorMetricType.metricToOffset.foreach { case (_, i) =>\n+      metricPeaks(i) = currentPeaks.get(i)\n+    }\n+    metricPeaks\n+  }\n+\n+\n+  /**\n+   * Called by the reportHeartBeat function defined in Executor and passed to its Heartbeater.\n+   * It resets the metric peaks in stageTCMP before returning the executor updates.\n+   * Thus, the executor updates contains the per-stage metric peaks since the last heartbeat\n+   * (the last time this method was called).\n+   */\n+  def getExecutorUpdates(): HashMap[StageKey, ExecutorMetrics] = {\n+    // build the executor level memory metrics\n+    val executorUpdates = new HashMap[StageKey, ExecutorMetrics]\n+\n+    def peaksForStage(k: StageKey, v: TCMP): (StageKey, AtomicLongArray) =\n+      if (v._1.get() > 0) (k, v._2) else null\n+\n+    def addPeaks(nested: (StageKey, AtomicLongArray)): Unit = {\n+      val (k, v) = nested\n+      executorUpdates.put(k, new ExecutorMetrics(v))\n+    }\n+\n+    stageTCMP.forEach[(StageKey, AtomicLongArray)](LONG_MAX_VALUE, peaksForStage, addPeaks)"
  }],
  "prId": 23767
}, {
  "comments": [{
    "author": {
      "login": "rezasafi"
    },
    "body": "Seems like a lot of threading and concurrency are added here. How much is the overhead? Is there a number?",
    "commit": "7556d6ab8615e4fd216da16ec02a4d0020f65ce3",
    "createdAt": "2019-03-28T23:40:12Z",
    "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.executor\n+\n+import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}\n+\n+import scala.collection.mutable.HashMap\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.MemoryManager\n+import org.apache.spark.metrics.ExecutorMetricType\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * A class that polls executor metrics, and tracks their peaks per task and per stage.\n+ * Each executor keeps an instance of this class.\n+ * The poll method polls the executor metrics, and is either run in its own thread or\n+ * called by the executor's heartbeater thread, depending on configuration.\n+ * The class keeps two ConcurrentHashMaps that are accessed (via its methods) by the\n+ * executor's task runner threads concurrently with the polling thread. One thread may\n+ * update one of these maps while another reads it, so the reading thread may not get\n+ * the latest metrics, but this is ok.\n+ *\n+ * @param memoryManager the memory manager used by the executor.\n+ * @param pollingInterval the polling interval in milliseconds.\n+ */\n+@DeveloperApi\n+private[spark] class ExecutorMetricsPoller(",
    "line": 49
  }, {
    "author": {
      "login": "wypoon"
    },
    "body": "As I explained, there is only ever one thread that polls executor metrics, either a dedicated polling thread, or the heartbeater thread. So, yes, there is possibly one more thread than before. However, I don't think that by itself adds much overhead. There are two ConcurrentHashMaps that we keep here, that the thread that polls updates in a bulk operation while task runner threads may concurrently read from them; this is what CHMs are designed to do well with little overhead. Task runner threads also put an entry in the CHMs when a task starts and updates an entry or removes it when the task ends. This requires synchronization but the operation is short.",
    "commit": "7556d6ab8615e4fd216da16ec02a4d0020f65ce3",
    "createdAt": "2019-03-29T05:44:35Z",
    "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.executor\n+\n+import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}\n+\n+import scala.collection.mutable.HashMap\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.MemoryManager\n+import org.apache.spark.metrics.ExecutorMetricType\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * A class that polls executor metrics, and tracks their peaks per task and per stage.\n+ * Each executor keeps an instance of this class.\n+ * The poll method polls the executor metrics, and is either run in its own thread or\n+ * called by the executor's heartbeater thread, depending on configuration.\n+ * The class keeps two ConcurrentHashMaps that are accessed (via its methods) by the\n+ * executor's task runner threads concurrently with the polling thread. One thread may\n+ * update one of these maps while another reads it, so the reading thread may not get\n+ * the latest metrics, but this is ok.\n+ *\n+ * @param memoryManager the memory manager used by the executor.\n+ * @param pollingInterval the polling interval in milliseconds.\n+ */\n+@DeveloperApi\n+private[spark] class ExecutorMetricsPoller(",
    "line": 49
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "So I just asked if there was an overhead analysis and if there is a number like 10-20% overhead. If you are saying that you have done some testing on an actual cluster with some large/normal/low workload and concluded that the overhead isn't that much to even being noticeable, I'm totally fine. It is for guiding users to what expect if they want to use this. ",
    "commit": "7556d6ab8615e4fd216da16ec02a4d0020f65ce3",
    "createdAt": "2019-03-29T14:02:49Z",
    "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.executor\n+\n+import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}\n+\n+import scala.collection.mutable.HashMap\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.MemoryManager\n+import org.apache.spark.metrics.ExecutorMetricType\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * A class that polls executor metrics, and tracks their peaks per task and per stage.\n+ * Each executor keeps an instance of this class.\n+ * The poll method polls the executor metrics, and is either run in its own thread or\n+ * called by the executor's heartbeater thread, depending on configuration.\n+ * The class keeps two ConcurrentHashMaps that are accessed (via its methods) by the\n+ * executor's task runner threads concurrently with the polling thread. One thread may\n+ * update one of these maps while another reads it, so the reading thread may not get\n+ * the latest metrics, but this is ok.\n+ *\n+ * @param memoryManager the memory manager used by the executor.\n+ * @param pollingInterval the polling interval in milliseconds.\n+ */\n+@DeveloperApi\n+private[spark] class ExecutorMetricsPoller(",
    "line": 49
  }, {
    "author": {
      "login": "wypoon"
    },
    "body": "I have done testing on an actual cluster. I ran a few applications with and without faster polling (100ms interval). In both cases, I used 1 second for the heartbeat interval. In both cases, I did not enable proc fs metrics. There was no noticeable difference to the task times in the executors or to the stage times. However, the workload is small and the tasks took ~15s to ~1 minute.",
    "commit": "7556d6ab8615e4fd216da16ec02a4d0020f65ce3",
    "createdAt": "2019-03-29T22:02:58Z",
    "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.executor\n+\n+import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}\n+\n+import scala.collection.mutable.HashMap\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.MemoryManager\n+import org.apache.spark.metrics.ExecutorMetricType\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * A class that polls executor metrics, and tracks their peaks per task and per stage.\n+ * Each executor keeps an instance of this class.\n+ * The poll method polls the executor metrics, and is either run in its own thread or\n+ * called by the executor's heartbeater thread, depending on configuration.\n+ * The class keeps two ConcurrentHashMaps that are accessed (via its methods) by the\n+ * executor's task runner threads concurrently with the polling thread. One thread may\n+ * update one of these maps while another reads it, so the reading thread may not get\n+ * the latest metrics, but this is ok.\n+ *\n+ * @param memoryManager the memory manager used by the executor.\n+ * @param pollingInterval the polling interval in milliseconds.\n+ */\n+@DeveloperApi\n+private[spark] class ExecutorMetricsPoller(",
    "line": 49
  }],
  "prId": 23767
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "minor: with Scala 2.12 and java8, I think you don't even need to make this anonymous `Runnable` anymore:\r\n\r\nhttps://www.scala-lang.org/news/2.12.0/#lambda-syntax-for-sam-types\r\n\r\nhttps://github.com/apache/spark/pull/24241",
    "commit": "7556d6ab8615e4fd216da16ec02a4d0020f65ce3",
    "createdAt": "2019-04-16T16:11:29Z",
    "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.executor\n+\n+import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}\n+\n+import scala.collection.mutable.HashMap\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.MemoryManager\n+import org.apache.spark.metrics.ExecutorMetricType\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * A class that polls executor metrics, and tracks their peaks per task and per stage.\n+ * Each executor keeps an instance of this class.\n+ * The poll method polls the executor metrics, and is either run in its own thread or\n+ * called by the executor's heartbeater thread, depending on configuration.\n+ * The class keeps two ConcurrentHashMaps that are accessed (via its methods) by the\n+ * executor's task runner threads concurrently with the polling thread. One thread may\n+ * update one of these maps while another reads it, so the reading thread may not get\n+ * the latest metrics, but this is ok.\n+ *\n+ * @param memoryManager the memory manager used by the executor.\n+ * @param pollingInterval the polling interval in milliseconds.\n+ */\n+@DeveloperApi\n+private[spark] class ExecutorMetricsPoller(\n+    memoryManager: MemoryManager,\n+    pollingInterval: Long)\n+  extends Logging {\n+\n+  type StageKey = (Int, Int)\n+  // tuple for Task Count and Metric Peaks\n+  type TCMP = (AtomicLong, AtomicLongArray)\n+\n+  // Map of (stageId, stageAttemptId) to (count of running tasks, executor metric peaks)\n+  private val stageTCMP = new ConcurrentHashMap[StageKey, TCMP]\n+\n+  // Map of taskId to executor metric peaks\n+  private val taskMetricPeaks = new ConcurrentHashMap[Long, AtomicLongArray]\n+\n+  private val poller =\n+    if (pollingInterval > 0) {\n+      ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"executor-metrics-poller\")\n+    } else {\n+      null\n+    }\n+\n+  /**\n+   * Function to poll executor metrics.\n+   * On start, if pollingInterval is positive, this is scheduled to run at that interval.\n+   * Otherwise, this is called by the reportHeartBeat function defined in Executor and passed\n+   * to its Heartbeater.\n+   */\n+  def poll(): Unit = {\n+    // Note: Task runner threads may update stageTCMP or read from taskMetricPeaks concurrently\n+    // with this function via calls to methods of this class.\n+\n+    // get the latest values for the metrics\n+    val latestMetrics = ExecutorMetrics.getCurrentMetrics(memoryManager)\n+\n+    def compareAndUpdate(current: Long, latest: Long): Long =\n+      if (latest > current) latest else current\n+\n+    def updatePeaks(metrics: AtomicLongArray): Unit = {\n+      (0 until metrics.length).foreach { i =>\n+        metrics.getAndAccumulate(i, latestMetrics(i), compareAndUpdate)\n+      }\n+    }\n+\n+    // for each active stage, update the peaks\n+    stageTCMP.forEachValue(LONG_MAX_VALUE, v => updatePeaks(v._2))\n+\n+    // for each running task, update the peaks\n+    taskMetricPeaks.forEachValue(LONG_MAX_VALUE, updatePeaks)\n+  }\n+\n+  /** Starts the polling thread. */\n+  def start(): Unit = {\n+    if (poller != null) {\n+      val pollingTask = new Runnable() {\n+        override def run(): Unit = Utils.logUncaughtExceptions(poll())\n+      }\n+      poller.scheduleAtFixedRate(pollingTask, 0L, pollingInterval, TimeUnit.MILLISECONDS)"
  }, {
    "author": {
      "login": "wypoon"
    },
    "body": "Changed to use lambda.",
    "commit": "7556d6ab8615e4fd216da16ec02a4d0020f65ce3",
    "createdAt": "2019-05-20T20:55:31Z",
    "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.executor\n+\n+import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}\n+\n+import scala.collection.mutable.HashMap\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.MemoryManager\n+import org.apache.spark.metrics.ExecutorMetricType\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * A class that polls executor metrics, and tracks their peaks per task and per stage.\n+ * Each executor keeps an instance of this class.\n+ * The poll method polls the executor metrics, and is either run in its own thread or\n+ * called by the executor's heartbeater thread, depending on configuration.\n+ * The class keeps two ConcurrentHashMaps that are accessed (via its methods) by the\n+ * executor's task runner threads concurrently with the polling thread. One thread may\n+ * update one of these maps while another reads it, so the reading thread may not get\n+ * the latest metrics, but this is ok.\n+ *\n+ * @param memoryManager the memory manager used by the executor.\n+ * @param pollingInterval the polling interval in milliseconds.\n+ */\n+@DeveloperApi\n+private[spark] class ExecutorMetricsPoller(\n+    memoryManager: MemoryManager,\n+    pollingInterval: Long)\n+  extends Logging {\n+\n+  type StageKey = (Int, Int)\n+  // tuple for Task Count and Metric Peaks\n+  type TCMP = (AtomicLong, AtomicLongArray)\n+\n+  // Map of (stageId, stageAttemptId) to (count of running tasks, executor metric peaks)\n+  private val stageTCMP = new ConcurrentHashMap[StageKey, TCMP]\n+\n+  // Map of taskId to executor metric peaks\n+  private val taskMetricPeaks = new ConcurrentHashMap[Long, AtomicLongArray]\n+\n+  private val poller =\n+    if (pollingInterval > 0) {\n+      ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"executor-metrics-poller\")\n+    } else {\n+      null\n+    }\n+\n+  /**\n+   * Function to poll executor metrics.\n+   * On start, if pollingInterval is positive, this is scheduled to run at that interval.\n+   * Otherwise, this is called by the reportHeartBeat function defined in Executor and passed\n+   * to its Heartbeater.\n+   */\n+  def poll(): Unit = {\n+    // Note: Task runner threads may update stageTCMP or read from taskMetricPeaks concurrently\n+    // with this function via calls to methods of this class.\n+\n+    // get the latest values for the metrics\n+    val latestMetrics = ExecutorMetrics.getCurrentMetrics(memoryManager)\n+\n+    def compareAndUpdate(current: Long, latest: Long): Long =\n+      if (latest > current) latest else current\n+\n+    def updatePeaks(metrics: AtomicLongArray): Unit = {\n+      (0 until metrics.length).foreach { i =>\n+        metrics.getAndAccumulate(i, latestMetrics(i), compareAndUpdate)\n+      }\n+    }\n+\n+    // for each active stage, update the peaks\n+    stageTCMP.forEachValue(LONG_MAX_VALUE, v => updatePeaks(v._2))\n+\n+    // for each running task, update the peaks\n+    taskMetricPeaks.forEachValue(LONG_MAX_VALUE, updatePeaks)\n+  }\n+\n+  /** Starts the polling thread. */\n+  def start(): Unit = {\n+    if (poller != null) {\n+      val pollingTask = new Runnable() {\n+        override def run(): Unit = Utils.logUncaughtExceptions(poll())\n+      }\n+      poller.scheduleAtFixedRate(pollingTask, 0L, pollingInterval, TimeUnit.MILLISECONDS)"
  }],
  "prId": 23767
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "I think you should reset the peaks within the above `forEach`.  First, it avoids a small race, as otherwise you might grab the last saved value of the metrics, then another thread updates them to new peaks, and then you clear them in this thread.  (its not horrible to lose that update, but if we can avoid it we should.)  Second its best to avoid too many passes over these structures.",
    "commit": "7556d6ab8615e4fd216da16ec02a4d0020f65ce3",
    "createdAt": "2019-04-16T16:19:15Z",
    "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.executor\n+\n+import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}\n+\n+import scala.collection.mutable.HashMap\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.MemoryManager\n+import org.apache.spark.metrics.ExecutorMetricType\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * A class that polls executor metrics, and tracks their peaks per task and per stage.\n+ * Each executor keeps an instance of this class.\n+ * The poll method polls the executor metrics, and is either run in its own thread or\n+ * called by the executor's heartbeater thread, depending on configuration.\n+ * The class keeps two ConcurrentHashMaps that are accessed (via its methods) by the\n+ * executor's task runner threads concurrently with the polling thread. One thread may\n+ * update one of these maps while another reads it, so the reading thread may not get\n+ * the latest metrics, but this is ok.\n+ *\n+ * @param memoryManager the memory manager used by the executor.\n+ * @param pollingInterval the polling interval in milliseconds.\n+ */\n+@DeveloperApi\n+private[spark] class ExecutorMetricsPoller(\n+    memoryManager: MemoryManager,\n+    pollingInterval: Long)\n+  extends Logging {\n+\n+  type StageKey = (Int, Int)\n+  // tuple for Task Count and Metric Peaks\n+  type TCMP = (AtomicLong, AtomicLongArray)\n+\n+  // Map of (stageId, stageAttemptId) to (count of running tasks, executor metric peaks)\n+  private val stageTCMP = new ConcurrentHashMap[StageKey, TCMP]\n+\n+  // Map of taskId to executor metric peaks\n+  private val taskMetricPeaks = new ConcurrentHashMap[Long, AtomicLongArray]\n+\n+  private val poller =\n+    if (pollingInterval > 0) {\n+      ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"executor-metrics-poller\")\n+    } else {\n+      null\n+    }\n+\n+  /**\n+   * Function to poll executor metrics.\n+   * On start, if pollingInterval is positive, this is scheduled to run at that interval.\n+   * Otherwise, this is called by the reportHeartBeat function defined in Executor and passed\n+   * to its Heartbeater.\n+   */\n+  def poll(): Unit = {\n+    // Note: Task runner threads may update stageTCMP or read from taskMetricPeaks concurrently\n+    // with this function via calls to methods of this class.\n+\n+    // get the latest values for the metrics\n+    val latestMetrics = ExecutorMetrics.getCurrentMetrics(memoryManager)\n+\n+    def compareAndUpdate(current: Long, latest: Long): Long =\n+      if (latest > current) latest else current\n+\n+    def updatePeaks(metrics: AtomicLongArray): Unit = {\n+      (0 until metrics.length).foreach { i =>\n+        metrics.getAndAccumulate(i, latestMetrics(i), compareAndUpdate)\n+      }\n+    }\n+\n+    // for each active stage, update the peaks\n+    stageTCMP.forEachValue(LONG_MAX_VALUE, v => updatePeaks(v._2))\n+\n+    // for each running task, update the peaks\n+    taskMetricPeaks.forEachValue(LONG_MAX_VALUE, updatePeaks)\n+  }\n+\n+  /** Starts the polling thread. */\n+  def start(): Unit = {\n+    if (poller != null) {\n+      val pollingTask = new Runnable() {\n+        override def run(): Unit = Utils.logUncaughtExceptions(poll())\n+      }\n+      poller.scheduleAtFixedRate(pollingTask, 0L, pollingInterval, TimeUnit.MILLISECONDS)\n+    }\n+  }\n+\n+  /**\n+   * Called by TaskRunner#run.\n+   *\n+   * @param taskId the id of the task being run.\n+   * @param stageId the id of the stage the task belongs to.\n+   * @param stageAttemptId the attempt number of the stage the task belongs to.\n+   */\n+  def onTaskStart(taskId: Long, stageId: Int, stageAttemptId: Int): Unit = {\n+    // Put an entry in taskMetricPeaks for the task.\n+    taskMetricPeaks.put(taskId, new AtomicLongArray(ExecutorMetricType.numMetrics))\n+\n+    // Put a new entry in stageTCMP for the stage if there isn't one already.\n+    // Increment the task count.\n+    val (count, _) = stageTCMP.computeIfAbsent((stageId, stageAttemptId),\n+      _ => (new AtomicLong(0), new AtomicLongArray(ExecutorMetricType.numMetrics)))\n+    val stageCount = count.incrementAndGet()\n+    logDebug(s\"stageTCMP: ($stageId, $stageAttemptId) -> $stageCount\")\n+  }\n+\n+  /**\n+   * Called by TaskRunner#run. It should only be called if onTaskStart has been called with\n+   * the same arguments.\n+   *\n+   * @param taskId the id of the task that was run.\n+   * @param stageId the id of the stage the task belongs to.\n+   * @param stageAttemptId the attempt number of the stage the task belongs to.\n+   */\n+  def onTaskCompletion(taskId: Long, stageId: Int, stageAttemptId: Int): Unit = {\n+    // Decrement the task count.\n+    // Remove the entry from stageTCMP if the task count reaches zero.\n+\n+    def decrementCount(stage: StageKey, countAndPeaks: TCMP): TCMP = {\n+      val count = countAndPeaks._1\n+      val countValue = count.decrementAndGet()\n+      if (countValue == 0L) {\n+        logDebug(s\"removing (${stage._1}, ${stage._2}) from stageTCMP\")\n+        null\n+      } else {\n+        logDebug(s\"stageTCMP: (${stage._1}, ${stage._2}) -> \" + countValue)\n+        countAndPeaks\n+      }\n+    }\n+\n+    stageTCMP.computeIfPresent((stageId, stageAttemptId), decrementCount)\n+\n+    // Remove the entry from taskMetricPeaks for the task.\n+    taskMetricPeaks.remove(taskId)\n+  }\n+\n+  /**\n+   * Called by TaskRunner#run.\n+   *\n+   * @param taskId the id of the task that was run.\n+   */\n+  def getTaskMetricPeaks(taskId: Long): Array[Long] = {\n+    // If this is called with an invalid taskId or a valid taskId but the task was killed and\n+    // onTaskStart was therefore not called, then we return an array of zeros.\n+    val currentPeaks = taskMetricPeaks.get(taskId) // may be null\n+    val metricPeaks = new Array[Long](ExecutorMetricType.numMetrics) // initialized to zeros\n+    if (currentPeaks != null) {\n+      ExecutorMetricType.metricToOffset.foreach { case (_, i) =>\n+        metricPeaks(i) = currentPeaks.get(i)\n+      }\n+    }\n+    metricPeaks\n+  }\n+\n+\n+  /**\n+   * Called by the reportHeartBeat function defined in Executor and passed to its Heartbeater.\n+   * It resets the metric peaks in stageTCMP before returning the executor updates.\n+   * Thus, the executor updates contains the per-stage metric peaks since the last heartbeat\n+   * (the last time this method was called).\n+   */\n+  def getExecutorUpdates(): HashMap[StageKey, ExecutorMetrics] = {\n+    // build the executor level memory metrics\n+    val executorUpdates = new HashMap[StageKey, ExecutorMetrics]\n+    stageTCMP.forEach((k, v) => executorUpdates.put(k, new ExecutorMetrics(v._2)))\n+\n+    // reset the peaks\n+    def resetPeaks(k: StageKey, v: TCMP): TCMP =\n+      (v._1, new AtomicLongArray(ExecutorMetricType.numMetrics))"
  }, {
    "author": {
      "login": "wypoon"
    },
    "body": "Combined extracting the metrics and resetting them in one pass.",
    "commit": "7556d6ab8615e4fd216da16ec02a4d0020f65ce3",
    "createdAt": "2019-05-20T20:58:02Z",
    "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.executor\n+\n+import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}\n+\n+import scala.collection.mutable.HashMap\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.MemoryManager\n+import org.apache.spark.metrics.ExecutorMetricType\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * A class that polls executor metrics, and tracks their peaks per task and per stage.\n+ * Each executor keeps an instance of this class.\n+ * The poll method polls the executor metrics, and is either run in its own thread or\n+ * called by the executor's heartbeater thread, depending on configuration.\n+ * The class keeps two ConcurrentHashMaps that are accessed (via its methods) by the\n+ * executor's task runner threads concurrently with the polling thread. One thread may\n+ * update one of these maps while another reads it, so the reading thread may not get\n+ * the latest metrics, but this is ok.\n+ *\n+ * @param memoryManager the memory manager used by the executor.\n+ * @param pollingInterval the polling interval in milliseconds.\n+ */\n+@DeveloperApi\n+private[spark] class ExecutorMetricsPoller(\n+    memoryManager: MemoryManager,\n+    pollingInterval: Long)\n+  extends Logging {\n+\n+  type StageKey = (Int, Int)\n+  // tuple for Task Count and Metric Peaks\n+  type TCMP = (AtomicLong, AtomicLongArray)\n+\n+  // Map of (stageId, stageAttemptId) to (count of running tasks, executor metric peaks)\n+  private val stageTCMP = new ConcurrentHashMap[StageKey, TCMP]\n+\n+  // Map of taskId to executor metric peaks\n+  private val taskMetricPeaks = new ConcurrentHashMap[Long, AtomicLongArray]\n+\n+  private val poller =\n+    if (pollingInterval > 0) {\n+      ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"executor-metrics-poller\")\n+    } else {\n+      null\n+    }\n+\n+  /**\n+   * Function to poll executor metrics.\n+   * On start, if pollingInterval is positive, this is scheduled to run at that interval.\n+   * Otherwise, this is called by the reportHeartBeat function defined in Executor and passed\n+   * to its Heartbeater.\n+   */\n+  def poll(): Unit = {\n+    // Note: Task runner threads may update stageTCMP or read from taskMetricPeaks concurrently\n+    // with this function via calls to methods of this class.\n+\n+    // get the latest values for the metrics\n+    val latestMetrics = ExecutorMetrics.getCurrentMetrics(memoryManager)\n+\n+    def compareAndUpdate(current: Long, latest: Long): Long =\n+      if (latest > current) latest else current\n+\n+    def updatePeaks(metrics: AtomicLongArray): Unit = {\n+      (0 until metrics.length).foreach { i =>\n+        metrics.getAndAccumulate(i, latestMetrics(i), compareAndUpdate)\n+      }\n+    }\n+\n+    // for each active stage, update the peaks\n+    stageTCMP.forEachValue(LONG_MAX_VALUE, v => updatePeaks(v._2))\n+\n+    // for each running task, update the peaks\n+    taskMetricPeaks.forEachValue(LONG_MAX_VALUE, updatePeaks)\n+  }\n+\n+  /** Starts the polling thread. */\n+  def start(): Unit = {\n+    if (poller != null) {\n+      val pollingTask = new Runnable() {\n+        override def run(): Unit = Utils.logUncaughtExceptions(poll())\n+      }\n+      poller.scheduleAtFixedRate(pollingTask, 0L, pollingInterval, TimeUnit.MILLISECONDS)\n+    }\n+  }\n+\n+  /**\n+   * Called by TaskRunner#run.\n+   *\n+   * @param taskId the id of the task being run.\n+   * @param stageId the id of the stage the task belongs to.\n+   * @param stageAttemptId the attempt number of the stage the task belongs to.\n+   */\n+  def onTaskStart(taskId: Long, stageId: Int, stageAttemptId: Int): Unit = {\n+    // Put an entry in taskMetricPeaks for the task.\n+    taskMetricPeaks.put(taskId, new AtomicLongArray(ExecutorMetricType.numMetrics))\n+\n+    // Put a new entry in stageTCMP for the stage if there isn't one already.\n+    // Increment the task count.\n+    val (count, _) = stageTCMP.computeIfAbsent((stageId, stageAttemptId),\n+      _ => (new AtomicLong(0), new AtomicLongArray(ExecutorMetricType.numMetrics)))\n+    val stageCount = count.incrementAndGet()\n+    logDebug(s\"stageTCMP: ($stageId, $stageAttemptId) -> $stageCount\")\n+  }\n+\n+  /**\n+   * Called by TaskRunner#run. It should only be called if onTaskStart has been called with\n+   * the same arguments.\n+   *\n+   * @param taskId the id of the task that was run.\n+   * @param stageId the id of the stage the task belongs to.\n+   * @param stageAttemptId the attempt number of the stage the task belongs to.\n+   */\n+  def onTaskCompletion(taskId: Long, stageId: Int, stageAttemptId: Int): Unit = {\n+    // Decrement the task count.\n+    // Remove the entry from stageTCMP if the task count reaches zero.\n+\n+    def decrementCount(stage: StageKey, countAndPeaks: TCMP): TCMP = {\n+      val count = countAndPeaks._1\n+      val countValue = count.decrementAndGet()\n+      if (countValue == 0L) {\n+        logDebug(s\"removing (${stage._1}, ${stage._2}) from stageTCMP\")\n+        null\n+      } else {\n+        logDebug(s\"stageTCMP: (${stage._1}, ${stage._2}) -> \" + countValue)\n+        countAndPeaks\n+      }\n+    }\n+\n+    stageTCMP.computeIfPresent((stageId, stageAttemptId), decrementCount)\n+\n+    // Remove the entry from taskMetricPeaks for the task.\n+    taskMetricPeaks.remove(taskId)\n+  }\n+\n+  /**\n+   * Called by TaskRunner#run.\n+   *\n+   * @param taskId the id of the task that was run.\n+   */\n+  def getTaskMetricPeaks(taskId: Long): Array[Long] = {\n+    // If this is called with an invalid taskId or a valid taskId but the task was killed and\n+    // onTaskStart was therefore not called, then we return an array of zeros.\n+    val currentPeaks = taskMetricPeaks.get(taskId) // may be null\n+    val metricPeaks = new Array[Long](ExecutorMetricType.numMetrics) // initialized to zeros\n+    if (currentPeaks != null) {\n+      ExecutorMetricType.metricToOffset.foreach { case (_, i) =>\n+        metricPeaks(i) = currentPeaks.get(i)\n+      }\n+    }\n+    metricPeaks\n+  }\n+\n+\n+  /**\n+   * Called by the reportHeartBeat function defined in Executor and passed to its Heartbeater.\n+   * It resets the metric peaks in stageTCMP before returning the executor updates.\n+   * Thus, the executor updates contains the per-stage metric peaks since the last heartbeat\n+   * (the last time this method was called).\n+   */\n+  def getExecutorUpdates(): HashMap[StageKey, ExecutorMetrics] = {\n+    // build the executor level memory metrics\n+    val executorUpdates = new HashMap[StageKey, ExecutorMetrics]\n+    stageTCMP.forEach((k, v) => executorUpdates.put(k, new ExecutorMetrics(v._2)))\n+\n+    // reset the peaks\n+    def resetPeaks(k: StageKey, v: TCMP): TCMP =\n+      (v._1, new AtomicLongArray(ExecutorMetricType.numMetrics))"
  }],
  "prId": 23767
}, {
  "comments": [{
    "author": {
      "login": "attilapiros"
    },
    "body": "Nit1: I like the type aliases but are not they [discouraged in the production code](https://github.com/databricks/scala-style-guide#java-type-alias)?\r\n\r\nNit2: For me, the name TCMP abbrev is very cryptic. I would prefer a bit longer name but this is really my own preference. ",
    "commit": "7556d6ab8615e4fd216da16ec02a4d0020f65ce3",
    "createdAt": "2019-05-29T11:18:49Z",
    "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.executor\n+\n+import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}\n+\n+import scala.collection.mutable.HashMap\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.MemoryManager\n+import org.apache.spark.metrics.ExecutorMetricType\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * A class that polls executor metrics, and tracks their peaks per task and per stage.\n+ * Each executor keeps an instance of this class.\n+ * The poll method polls the executor metrics, and is either run in its own thread or\n+ * called by the executor's heartbeater thread, depending on configuration.\n+ * The class keeps two ConcurrentHashMaps that are accessed (via its methods) by the\n+ * executor's task runner threads concurrently with the polling thread. One thread may\n+ * update one of these maps while another reads it, so the reading thread may not get\n+ * the latest metrics, but this is ok.\n+ *\n+ * @param memoryManager the memory manager used by the executor.\n+ * @param pollingInterval the polling interval in milliseconds.\n+ */\n+@DeveloperApi\n+private[spark] class ExecutorMetricsPoller(\n+    memoryManager: MemoryManager,\n+    pollingInterval: Long)\n+  extends Logging {\n+\n+  type StageKey = (Int, Int)\n+  // tuple for Task Count and Metric Peaks\n+  type TCMP = (AtomicLong, AtomicLongArray)"
  }, {
    "author": {
      "login": "attilapiros"
    },
    "body": "After reading the whole code of this class I can see the advantage to use case class instead of the TCMP type alias. As a case class it could have some useful functions, like:\r\n- `toExecutorMetrics`: which would transform the `AtomicLongArray` to executor metrics, this could be used in `getExecutorUpdates`\r\n- decrease and increase of `AtomicLong` \r\n- `updatePeaks` could be added too\r\n- and some functionality of `getTaskMetricPeaks` something like `getSnapshot` returning an `Array[Long]`\r\n",
    "commit": "7556d6ab8615e4fd216da16ec02a4d0020f65ce3",
    "createdAt": "2019-05-29T13:20:18Z",
    "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.executor\n+\n+import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}\n+\n+import scala.collection.mutable.HashMap\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.MemoryManager\n+import org.apache.spark.metrics.ExecutorMetricType\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * A class that polls executor metrics, and tracks their peaks per task and per stage.\n+ * Each executor keeps an instance of this class.\n+ * The poll method polls the executor metrics, and is either run in its own thread or\n+ * called by the executor's heartbeater thread, depending on configuration.\n+ * The class keeps two ConcurrentHashMaps that are accessed (via its methods) by the\n+ * executor's task runner threads concurrently with the polling thread. One thread may\n+ * update one of these maps while another reads it, so the reading thread may not get\n+ * the latest metrics, but this is ok.\n+ *\n+ * @param memoryManager the memory manager used by the executor.\n+ * @param pollingInterval the polling interval in milliseconds.\n+ */\n+@DeveloperApi\n+private[spark] class ExecutorMetricsPoller(\n+    memoryManager: MemoryManager,\n+    pollingInterval: Long)\n+  extends Logging {\n+\n+  type StageKey = (Int, Int)\n+  // tuple for Task Count and Metric Peaks\n+  type TCMP = (AtomicLong, AtomicLongArray)"
  }, {
    "author": {
      "login": "squito"
    },
    "body": "I'm not sure I see the value in the extra util methods, but I agree about having a small case class -- if nothing else, it would make the code a lot cleared if instead of `v._2` it was `taskCountAndMetrics.metrics`",
    "commit": "7556d6ab8615e4fd216da16ec02a4d0020f65ce3",
    "createdAt": "2019-07-16T20:52:09Z",
    "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.executor\n+\n+import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}\n+\n+import scala.collection.mutable.HashMap\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.MemoryManager\n+import org.apache.spark.metrics.ExecutorMetricType\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * A class that polls executor metrics, and tracks their peaks per task and per stage.\n+ * Each executor keeps an instance of this class.\n+ * The poll method polls the executor metrics, and is either run in its own thread or\n+ * called by the executor's heartbeater thread, depending on configuration.\n+ * The class keeps two ConcurrentHashMaps that are accessed (via its methods) by the\n+ * executor's task runner threads concurrently with the polling thread. One thread may\n+ * update one of these maps while another reads it, so the reading thread may not get\n+ * the latest metrics, but this is ok.\n+ *\n+ * @param memoryManager the memory manager used by the executor.\n+ * @param pollingInterval the polling interval in milliseconds.\n+ */\n+@DeveloperApi\n+private[spark] class ExecutorMetricsPoller(\n+    memoryManager: MemoryManager,\n+    pollingInterval: Long)\n+  extends Logging {\n+\n+  type StageKey = (Int, Int)\n+  // tuple for Task Count and Metric Peaks\n+  type TCMP = (AtomicLong, AtomicLongArray)"
  }],
  "prId": 23767
}, {
  "comments": [{
    "author": {
      "login": "attilapiros"
    },
    "body": "This could be an option just by adding `Some` around the `ThreadUtils...` call and changing the `null` to `None`. \r\nThis way in the `start` and `stop` methods the `poller != null` could be a replaced by `poller.foreach {...}`",
    "commit": "7556d6ab8615e4fd216da16ec02a4d0020f65ce3",
    "createdAt": "2019-05-29T11:55:38Z",
    "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.executor\n+\n+import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}\n+\n+import scala.collection.mutable.HashMap\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.MemoryManager\n+import org.apache.spark.metrics.ExecutorMetricType\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * A class that polls executor metrics, and tracks their peaks per task and per stage.\n+ * Each executor keeps an instance of this class.\n+ * The poll method polls the executor metrics, and is either run in its own thread or\n+ * called by the executor's heartbeater thread, depending on configuration.\n+ * The class keeps two ConcurrentHashMaps that are accessed (via its methods) by the\n+ * executor's task runner threads concurrently with the polling thread. One thread may\n+ * update one of these maps while another reads it, so the reading thread may not get\n+ * the latest metrics, but this is ok.\n+ *\n+ * @param memoryManager the memory manager used by the executor.\n+ * @param pollingInterval the polling interval in milliseconds.\n+ */\n+@DeveloperApi\n+private[spark] class ExecutorMetricsPoller(\n+    memoryManager: MemoryManager,\n+    pollingInterval: Long)\n+  extends Logging {\n+\n+  type StageKey = (Int, Int)\n+  // tuple for Task Count and Metric Peaks\n+  type TCMP = (AtomicLong, AtomicLongArray)\n+\n+  // Map of (stageId, stageAttemptId) to (count of running tasks, executor metric peaks)\n+  private val stageTCMP = new ConcurrentHashMap[StageKey, TCMP]\n+\n+  // Map of taskId to executor metric peaks\n+  private val taskMetricPeaks = new ConcurrentHashMap[Long, AtomicLongArray]\n+\n+  private val poller =",
    "line": 63
  }, {
    "author": {
      "login": "wypoon"
    },
    "body": "I think this is simply stylistic preference. I'm not opposed to using an Option but I don't think one is better than the other in this case, that it makes the code any safer or easier to understand.",
    "commit": "7556d6ab8615e4fd216da16ec02a4d0020f65ce3",
    "createdAt": "2019-07-02T06:29:57Z",
    "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.executor\n+\n+import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}\n+\n+import scala.collection.mutable.HashMap\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.MemoryManager\n+import org.apache.spark.metrics.ExecutorMetricType\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * A class that polls executor metrics, and tracks their peaks per task and per stage.\n+ * Each executor keeps an instance of this class.\n+ * The poll method polls the executor metrics, and is either run in its own thread or\n+ * called by the executor's heartbeater thread, depending on configuration.\n+ * The class keeps two ConcurrentHashMaps that are accessed (via its methods) by the\n+ * executor's task runner threads concurrently with the polling thread. One thread may\n+ * update one of these maps while another reads it, so the reading thread may not get\n+ * the latest metrics, but this is ok.\n+ *\n+ * @param memoryManager the memory manager used by the executor.\n+ * @param pollingInterval the polling interval in milliseconds.\n+ */\n+@DeveloperApi\n+private[spark] class ExecutorMetricsPoller(\n+    memoryManager: MemoryManager,\n+    pollingInterval: Long)\n+  extends Logging {\n+\n+  type StageKey = (Int, Int)\n+  // tuple for Task Count and Metric Peaks\n+  type TCMP = (AtomicLong, AtomicLongArray)\n+\n+  // Map of (stageId, stageAttemptId) to (count of running tasks, executor metric peaks)\n+  private val stageTCMP = new ConcurrentHashMap[StageKey, TCMP]\n+\n+  // Map of taskId to executor metric peaks\n+  private val taskMetricPeaks = new ConcurrentHashMap[Long, AtomicLongArray]\n+\n+  private val poller =",
    "line": 63
  }, {
    "author": {
      "login": "attilapiros"
    },
    "body": "You could make this argument for every `Option` usage, is not it?  But this way `Option` would not be invented at all. \r\n\r\nI do not agree that `Option` is just different style of `if` as I see it brings the no value case at the type level and thus checked by the compiler.",
    "commit": "7556d6ab8615e4fd216da16ec02a4d0020f65ce3",
    "createdAt": "2019-07-02T17:26:47Z",
    "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.executor\n+\n+import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}\n+\n+import scala.collection.mutable.HashMap\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.MemoryManager\n+import org.apache.spark.metrics.ExecutorMetricType\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * A class that polls executor metrics, and tracks their peaks per task and per stage.\n+ * Each executor keeps an instance of this class.\n+ * The poll method polls the executor metrics, and is either run in its own thread or\n+ * called by the executor's heartbeater thread, depending on configuration.\n+ * The class keeps two ConcurrentHashMaps that are accessed (via its methods) by the\n+ * executor's task runner threads concurrently with the polling thread. One thread may\n+ * update one of these maps while another reads it, so the reading thread may not get\n+ * the latest metrics, but this is ok.\n+ *\n+ * @param memoryManager the memory manager used by the executor.\n+ * @param pollingInterval the polling interval in milliseconds.\n+ */\n+@DeveloperApi\n+private[spark] class ExecutorMetricsPoller(\n+    memoryManager: MemoryManager,\n+    pollingInterval: Long)\n+  extends Logging {\n+\n+  type StageKey = (Int, Int)\n+  // tuple for Task Count and Metric Peaks\n+  type TCMP = (AtomicLong, AtomicLongArray)\n+\n+  // Map of (stageId, stageAttemptId) to (count of running tasks, executor metric peaks)\n+  private val stageTCMP = new ConcurrentHashMap[StageKey, TCMP]\n+\n+  // Map of taskId to executor metric peaks\n+  private val taskMetricPeaks = new ConcurrentHashMap[Long, AtomicLongArray]\n+\n+  private val poller =",
    "line": 63
  }, {
    "author": {
      "login": "wypoon"
    },
    "body": "I'd say that it is different if you are returning something to another consumer and that something could be null, in which case it would be better to return an Option. In this case, poller is a private field that is not exposed to any user of ExecutorMetricsPoller, so it is fine if it can be null as we know exactly when it is null.\r\nHaving said that, I'm fine to use an Option too.",
    "commit": "7556d6ab8615e4fd216da16ec02a4d0020f65ce3",
    "createdAt": "2019-07-02T20:22:10Z",
    "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.executor\n+\n+import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}\n+\n+import scala.collection.mutable.HashMap\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.MemoryManager\n+import org.apache.spark.metrics.ExecutorMetricType\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * A class that polls executor metrics, and tracks their peaks per task and per stage.\n+ * Each executor keeps an instance of this class.\n+ * The poll method polls the executor metrics, and is either run in its own thread or\n+ * called by the executor's heartbeater thread, depending on configuration.\n+ * The class keeps two ConcurrentHashMaps that are accessed (via its methods) by the\n+ * executor's task runner threads concurrently with the polling thread. One thread may\n+ * update one of these maps while another reads it, so the reading thread may not get\n+ * the latest metrics, but this is ok.\n+ *\n+ * @param memoryManager the memory manager used by the executor.\n+ * @param pollingInterval the polling interval in milliseconds.\n+ */\n+@DeveloperApi\n+private[spark] class ExecutorMetricsPoller(\n+    memoryManager: MemoryManager,\n+    pollingInterval: Long)\n+  extends Logging {\n+\n+  type StageKey = (Int, Int)\n+  // tuple for Task Count and Metric Peaks\n+  type TCMP = (AtomicLong, AtomicLongArray)\n+\n+  // Map of (stageId, stageAttemptId) to (count of running tasks, executor metric peaks)\n+  private val stageTCMP = new ConcurrentHashMap[StageKey, TCMP]\n+\n+  // Map of taskId to executor metric peaks\n+  private val taskMetricPeaks = new ConcurrentHashMap[Long, AtomicLongArray]\n+\n+  private val poller =",
    "line": 63
  }, {
    "author": {
      "login": "wypoon"
    },
    "body": "Changed poller to an Option.",
    "commit": "7556d6ab8615e4fd216da16ec02a4d0020f65ce3",
    "createdAt": "2019-07-04T01:34:28Z",
    "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.executor\n+\n+import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}\n+\n+import scala.collection.mutable.HashMap\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.MemoryManager\n+import org.apache.spark.metrics.ExecutorMetricType\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * A class that polls executor metrics, and tracks their peaks per task and per stage.\n+ * Each executor keeps an instance of this class.\n+ * The poll method polls the executor metrics, and is either run in its own thread or\n+ * called by the executor's heartbeater thread, depending on configuration.\n+ * The class keeps two ConcurrentHashMaps that are accessed (via its methods) by the\n+ * executor's task runner threads concurrently with the polling thread. One thread may\n+ * update one of these maps while another reads it, so the reading thread may not get\n+ * the latest metrics, but this is ok.\n+ *\n+ * @param memoryManager the memory manager used by the executor.\n+ * @param pollingInterval the polling interval in milliseconds.\n+ */\n+@DeveloperApi\n+private[spark] class ExecutorMetricsPoller(\n+    memoryManager: MemoryManager,\n+    pollingInterval: Long)\n+  extends Logging {\n+\n+  type StageKey = (Int, Int)\n+  // tuple for Task Count and Metric Peaks\n+  type TCMP = (AtomicLong, AtomicLongArray)\n+\n+  // Map of (stageId, stageAttemptId) to (count of running tasks, executor metric peaks)\n+  private val stageTCMP = new ConcurrentHashMap[StageKey, TCMP]\n+\n+  // Map of taskId to executor metric peaks\n+  private val taskMetricPeaks = new ConcurrentHashMap[Long, AtomicLongArray]\n+\n+  private val poller =",
    "line": 63
  }],
  "prId": 23767
}, {
  "comments": [{
    "author": {
      "login": "attilapiros"
    },
    "body": "You can remove this method and replace its usage with the `math.max`.",
    "commit": "7556d6ab8615e4fd216da16ec02a4d0020f65ce3",
    "createdAt": "2019-05-29T12:24:37Z",
    "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.executor\n+\n+import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}\n+\n+import scala.collection.mutable.HashMap\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.MemoryManager\n+import org.apache.spark.metrics.ExecutorMetricType\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * A class that polls executor metrics, and tracks their peaks per task and per stage.\n+ * Each executor keeps an instance of this class.\n+ * The poll method polls the executor metrics, and is either run in its own thread or\n+ * called by the executor's heartbeater thread, depending on configuration.\n+ * The class keeps two ConcurrentHashMaps that are accessed (via its methods) by the\n+ * executor's task runner threads concurrently with the polling thread. One thread may\n+ * update one of these maps while another reads it, so the reading thread may not get\n+ * the latest metrics, but this is ok.\n+ *\n+ * @param memoryManager the memory manager used by the executor.\n+ * @param pollingInterval the polling interval in milliseconds.\n+ */\n+@DeveloperApi\n+private[spark] class ExecutorMetricsPoller(\n+    memoryManager: MemoryManager,\n+    pollingInterval: Long)\n+  extends Logging {\n+\n+  type StageKey = (Int, Int)\n+  // tuple for Task Count and Metric Peaks\n+  type TCMP = (AtomicLong, AtomicLongArray)\n+\n+  // Map of (stageId, stageAttemptId) to (count of running tasks, executor metric peaks)\n+  private val stageTCMP = new ConcurrentHashMap[StageKey, TCMP]\n+\n+  // Map of taskId to executor metric peaks\n+  private val taskMetricPeaks = new ConcurrentHashMap[Long, AtomicLongArray]\n+\n+  private val poller =\n+    if (pollingInterval > 0) {\n+      ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"executor-metrics-poller\")\n+    } else {\n+      null\n+    }\n+\n+  /**\n+   * Function to poll executor metrics.\n+   * On start, if pollingInterval is positive, this is scheduled to run at that interval.\n+   * Otherwise, this is called by the reportHeartBeat function defined in Executor and passed\n+   * to its Heartbeater.\n+   */\n+  def poll(): Unit = {\n+    // Note: Task runner threads may update stageTCMP or read from taskMetricPeaks concurrently\n+    // with this function via calls to methods of this class.\n+\n+    // get the latest values for the metrics\n+    val latestMetrics = ExecutorMetrics.getCurrentMetrics(memoryManager)\n+\n+    def compareAndUpdate(current: Long, latest: Long): Long ="
  }, {
    "author": {
      "login": "wypoon"
    },
    "body": "Yes.",
    "commit": "7556d6ab8615e4fd216da16ec02a4d0020f65ce3",
    "createdAt": "2019-07-02T06:26:08Z",
    "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.executor\n+\n+import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}\n+\n+import scala.collection.mutable.HashMap\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.MemoryManager\n+import org.apache.spark.metrics.ExecutorMetricType\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * A class that polls executor metrics, and tracks their peaks per task and per stage.\n+ * Each executor keeps an instance of this class.\n+ * The poll method polls the executor metrics, and is either run in its own thread or\n+ * called by the executor's heartbeater thread, depending on configuration.\n+ * The class keeps two ConcurrentHashMaps that are accessed (via its methods) by the\n+ * executor's task runner threads concurrently with the polling thread. One thread may\n+ * update one of these maps while another reads it, so the reading thread may not get\n+ * the latest metrics, but this is ok.\n+ *\n+ * @param memoryManager the memory manager used by the executor.\n+ * @param pollingInterval the polling interval in milliseconds.\n+ */\n+@DeveloperApi\n+private[spark] class ExecutorMetricsPoller(\n+    memoryManager: MemoryManager,\n+    pollingInterval: Long)\n+  extends Logging {\n+\n+  type StageKey = (Int, Int)\n+  // tuple for Task Count and Metric Peaks\n+  type TCMP = (AtomicLong, AtomicLongArray)\n+\n+  // Map of (stageId, stageAttemptId) to (count of running tasks, executor metric peaks)\n+  private val stageTCMP = new ConcurrentHashMap[StageKey, TCMP]\n+\n+  // Map of taskId to executor metric peaks\n+  private val taskMetricPeaks = new ConcurrentHashMap[Long, AtomicLongArray]\n+\n+  private val poller =\n+    if (pollingInterval > 0) {\n+      ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"executor-metrics-poller\")\n+    } else {\n+      null\n+    }\n+\n+  /**\n+   * Function to poll executor metrics.\n+   * On start, if pollingInterval is positive, this is scheduled to run at that interval.\n+   * Otherwise, this is called by the reportHeartBeat function defined in Executor and passed\n+   * to its Heartbeater.\n+   */\n+  def poll(): Unit = {\n+    // Note: Task runner threads may update stageTCMP or read from taskMetricPeaks concurrently\n+    // with this function via calls to methods of this class.\n+\n+    // get the latest values for the metrics\n+    val latestMetrics = ExecutorMetrics.getCurrentMetrics(memoryManager)\n+\n+    def compareAndUpdate(current: Long, latest: Long): Long ="
  }],
  "prId": 23767
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "why is this a developerapi?  I think it should just be internal",
    "commit": "7556d6ab8615e4fd216da16ec02a4d0020f65ce3",
    "createdAt": "2019-07-16T20:13:17Z",
    "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.executor\n+\n+import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}\n+\n+import scala.collection.mutable.HashMap\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.MemoryManager\n+import org.apache.spark.metrics.ExecutorMetricType\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+/**\n+ * :: DeveloperApi ::"
  }, {
    "author": {
      "login": "wypoon"
    },
    "body": "You're right. This is just internal implementation detail.",
    "commit": "7556d6ab8615e4fd216da16ec02a4d0020f65ce3",
    "createdAt": "2019-07-17T05:13:35Z",
    "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.executor\n+\n+import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}\n+\n+import scala.collection.mutable.HashMap\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.MemoryManager\n+import org.apache.spark.metrics.ExecutorMetricType\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+/**\n+ * :: DeveloperApi ::"
  }],
  "prId": 23767
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "nit: move this up to the line above\r\n\r\n```scala\r\n    pollingInterval: Long) extends Logging {\r\n```",
    "commit": "7556d6ab8615e4fd216da16ec02a4d0020f65ce3",
    "createdAt": "2019-07-16T20:14:29Z",
    "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.executor\n+\n+import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}\n+\n+import scala.collection.mutable.HashMap\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.MemoryManager\n+import org.apache.spark.metrics.ExecutorMetricType\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * A class that polls executor metrics, and tracks their peaks per task and per stage.\n+ * Each executor keeps an instance of this class.\n+ * The poll method polls the executor metrics, and is either run in its own thread or\n+ * called by the executor's heartbeater thread, depending on configuration.\n+ * The class keeps two ConcurrentHashMaps that are accessed (via its methods) by the\n+ * executor's task runner threads concurrently with the polling thread. One thread may\n+ * update one of these maps while another reads it, so the reading thread may not get\n+ * the latest metrics, but this is ok.\n+ *\n+ * @param memoryManager the memory manager used by the executor.\n+ * @param pollingInterval the polling interval in milliseconds.\n+ */\n+@DeveloperApi\n+private[spark] class ExecutorMetricsPoller(\n+    memoryManager: MemoryManager,\n+    pollingInterval: Long)\n+  extends Logging {"
  }],
  "prId": 23767
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "its ok (and probably better) to leave out docs on all the params for internal methods, if they don't say anything useful\r\n\r\nhere and all other methods on this class",
    "commit": "7556d6ab8615e4fd216da16ec02a4d0020f65ce3",
    "createdAt": "2019-07-16T20:20:19Z",
    "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.executor\n+\n+import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}\n+\n+import scala.collection.mutable.HashMap\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.MemoryManager\n+import org.apache.spark.metrics.ExecutorMetricType\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * A class that polls executor metrics, and tracks their peaks per task and per stage.\n+ * Each executor keeps an instance of this class.\n+ * The poll method polls the executor metrics, and is either run in its own thread or\n+ * called by the executor's heartbeater thread, depending on configuration.\n+ * The class keeps two ConcurrentHashMaps that are accessed (via its methods) by the\n+ * executor's task runner threads concurrently with the polling thread. One thread may\n+ * update one of these maps while another reads it, so the reading thread may not get\n+ * the latest metrics, but this is ok.\n+ *\n+ * @param memoryManager the memory manager used by the executor.\n+ * @param pollingInterval the polling interval in milliseconds.\n+ */\n+@DeveloperApi\n+private[spark] class ExecutorMetricsPoller(\n+    memoryManager: MemoryManager,\n+    pollingInterval: Long)\n+  extends Logging {\n+\n+  type StageKey = (Int, Int)\n+  // tuple for Task Count and Metric Peaks\n+  type TCMP = (AtomicLong, AtomicLongArray)\n+\n+  // Map of (stageId, stageAttemptId) to (count of running tasks, executor metric peaks)\n+  private val stageTCMP = new ConcurrentHashMap[StageKey, TCMP]\n+\n+  // Map of taskId to executor metric peaks\n+  private val taskMetricPeaks = new ConcurrentHashMap[Long, AtomicLongArray]\n+\n+  private val poller =\n+    if (pollingInterval > 0) {\n+      Some(ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"executor-metrics-poller\"))\n+    } else {\n+      None\n+    }\n+\n+  /**\n+   * Function to poll executor metrics.\n+   * On start, if pollingInterval is positive, this is scheduled to run at that interval.\n+   * Otherwise, this is called by the reportHeartBeat function defined in Executor and passed\n+   * to its Heartbeater.\n+   */\n+  def poll(): Unit = {\n+    // Note: Task runner threads may update stageTCMP or read from taskMetricPeaks concurrently\n+    // with this function via calls to methods of this class.\n+\n+    // get the latest values for the metrics\n+    val latestMetrics = ExecutorMetrics.getCurrentMetrics(memoryManager)\n+\n+    def updatePeaks(metrics: AtomicLongArray): Unit = {\n+      (0 until metrics.length).foreach { i =>\n+        metrics.getAndAccumulate(i, latestMetrics(i), math.max)\n+      }\n+    }\n+\n+    // for each active stage, update the peaks\n+    stageTCMP.forEachValue(LONG_MAX_VALUE, v => updatePeaks(v._2))\n+\n+    // for each running task, update the peaks\n+    taskMetricPeaks.forEachValue(LONG_MAX_VALUE, updatePeaks)\n+  }\n+\n+  /** Starts the polling thread. */\n+  def start(): Unit = {\n+    poller.foreach { exec =>\n+      val pollingTask: Runnable = () => Utils.logUncaughtExceptions(poll())\n+      exec.scheduleAtFixedRate(pollingTask, 0L, pollingInterval, TimeUnit.MILLISECONDS)\n+    }\n+  }\n+\n+  /**\n+   * Called by TaskRunner#run.\n+   *\n+   * @param taskId the id of the task being run.\n+   * @param stageId the id of the stage the task belongs to.\n+   * @param stageAttemptId the attempt number of the stage the task belongs to.\n+   */\n+  def onTaskStart(taskId: Long, stageId: Int, stageAttemptId: Int): Unit = {\n+    // Put an entry in taskMetricPeaks for the task.\n+    taskMetricPeaks.put(taskId, new AtomicLongArray(ExecutorMetricType.numMetrics))\n+\n+    // Put a new entry in stageTCMP for the stage if there isn't one already.\n+    // Increment the task count.\n+    val (count, _) = stageTCMP.computeIfAbsent((stageId, stageAttemptId),\n+      _ => (new AtomicLong(0), new AtomicLongArray(ExecutorMetricType.numMetrics)))\n+    val stageCount = count.incrementAndGet()\n+    logDebug(s\"stageTCMP: ($stageId, $stageAttemptId) -> $stageCount\")\n+  }\n+\n+  /**\n+   * Called by TaskRunner#run. It should only be called if onTaskStart has been called with\n+   * the same arguments.\n+   *\n+   * @param taskId the id of the task that was run.\n+   * @param stageId the id of the stage the task belongs to.\n+   * @param stageAttemptId the attempt number of the stage the task belongs to."
  }, {
    "author": {
      "login": "wypoon"
    },
    "body": "Ok, I can remove them. I guess the params are obvious enough.",
    "commit": "7556d6ab8615e4fd216da16ec02a4d0020f65ce3",
    "createdAt": "2019-07-17T05:15:55Z",
    "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.executor\n+\n+import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}\n+\n+import scala.collection.mutable.HashMap\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.MemoryManager\n+import org.apache.spark.metrics.ExecutorMetricType\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * A class that polls executor metrics, and tracks their peaks per task and per stage.\n+ * Each executor keeps an instance of this class.\n+ * The poll method polls the executor metrics, and is either run in its own thread or\n+ * called by the executor's heartbeater thread, depending on configuration.\n+ * The class keeps two ConcurrentHashMaps that are accessed (via its methods) by the\n+ * executor's task runner threads concurrently with the polling thread. One thread may\n+ * update one of these maps while another reads it, so the reading thread may not get\n+ * the latest metrics, but this is ok.\n+ *\n+ * @param memoryManager the memory manager used by the executor.\n+ * @param pollingInterval the polling interval in milliseconds.\n+ */\n+@DeveloperApi\n+private[spark] class ExecutorMetricsPoller(\n+    memoryManager: MemoryManager,\n+    pollingInterval: Long)\n+  extends Logging {\n+\n+  type StageKey = (Int, Int)\n+  // tuple for Task Count and Metric Peaks\n+  type TCMP = (AtomicLong, AtomicLongArray)\n+\n+  // Map of (stageId, stageAttemptId) to (count of running tasks, executor metric peaks)\n+  private val stageTCMP = new ConcurrentHashMap[StageKey, TCMP]\n+\n+  // Map of taskId to executor metric peaks\n+  private val taskMetricPeaks = new ConcurrentHashMap[Long, AtomicLongArray]\n+\n+  private val poller =\n+    if (pollingInterval > 0) {\n+      Some(ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"executor-metrics-poller\"))\n+    } else {\n+      None\n+    }\n+\n+  /**\n+   * Function to poll executor metrics.\n+   * On start, if pollingInterval is positive, this is scheduled to run at that interval.\n+   * Otherwise, this is called by the reportHeartBeat function defined in Executor and passed\n+   * to its Heartbeater.\n+   */\n+  def poll(): Unit = {\n+    // Note: Task runner threads may update stageTCMP or read from taskMetricPeaks concurrently\n+    // with this function via calls to methods of this class.\n+\n+    // get the latest values for the metrics\n+    val latestMetrics = ExecutorMetrics.getCurrentMetrics(memoryManager)\n+\n+    def updatePeaks(metrics: AtomicLongArray): Unit = {\n+      (0 until metrics.length).foreach { i =>\n+        metrics.getAndAccumulate(i, latestMetrics(i), math.max)\n+      }\n+    }\n+\n+    // for each active stage, update the peaks\n+    stageTCMP.forEachValue(LONG_MAX_VALUE, v => updatePeaks(v._2))\n+\n+    // for each running task, update the peaks\n+    taskMetricPeaks.forEachValue(LONG_MAX_VALUE, updatePeaks)\n+  }\n+\n+  /** Starts the polling thread. */\n+  def start(): Unit = {\n+    poller.foreach { exec =>\n+      val pollingTask: Runnable = () => Utils.logUncaughtExceptions(poll())\n+      exec.scheduleAtFixedRate(pollingTask, 0L, pollingInterval, TimeUnit.MILLISECONDS)\n+    }\n+  }\n+\n+  /**\n+   * Called by TaskRunner#run.\n+   *\n+   * @param taskId the id of the task being run.\n+   * @param stageId the id of the stage the task belongs to.\n+   * @param stageAttemptId the attempt number of the stage the task belongs to.\n+   */\n+  def onTaskStart(taskId: Long, stageId: Int, stageAttemptId: Int): Unit = {\n+    // Put an entry in taskMetricPeaks for the task.\n+    taskMetricPeaks.put(taskId, new AtomicLongArray(ExecutorMetricType.numMetrics))\n+\n+    // Put a new entry in stageTCMP for the stage if there isn't one already.\n+    // Increment the task count.\n+    val (count, _) = stageTCMP.computeIfAbsent((stageId, stageAttemptId),\n+      _ => (new AtomicLong(0), new AtomicLongArray(ExecutorMetricType.numMetrics)))\n+    val stageCount = count.incrementAndGet()\n+    logDebug(s\"stageTCMP: ($stageId, $stageAttemptId) -> $stageCount\")\n+  }\n+\n+  /**\n+   * Called by TaskRunner#run. It should only be called if onTaskStart has been called with\n+   * the same arguments.\n+   *\n+   * @param taskId the id of the task that was run.\n+   * @param stageId the id of the stage the task belongs to.\n+   * @param stageAttemptId the attempt number of the stage the task belongs to."
  }],
  "prId": 23767
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "We should also explain why there are metrics tracked per stage and also per task.  I think mostly just your comment here: https://issues.apache.org/jira/browse/SPARK-26329?focusedCommentId=16745313&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16745313",
    "commit": "7556d6ab8615e4fd216da16ec02a4d0020f65ce3",
    "createdAt": "2019-07-16T21:03:13Z",
    "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.executor\n+\n+import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}\n+\n+import scala.collection.mutable.HashMap\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.MemoryManager\n+import org.apache.spark.metrics.ExecutorMetricType\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * A class that polls executor metrics, and tracks their peaks per task and per stage.\n+ * Each executor keeps an instance of this class.\n+ * The poll method polls the executor metrics, and is either run in its own thread or\n+ * called by the executor's heartbeater thread, depending on configuration.\n+ * The class keeps two ConcurrentHashMaps that are accessed (via its methods) by the\n+ * executor's task runner threads concurrently with the polling thread. One thread may\n+ * update one of these maps while another reads it, so the reading thread may not get\n+ * the latest metrics, but this is ok.\n+ *\n+ * @param memoryManager the memory manager used by the executor.",
    "line": 46
  }],
  "prId": 23767
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "Coming back to this after a break, I felt this still didn't fully explain the \"why\" to me, I had to think about it a bit more.  Can I suggest changing the last 3 paragraphs to something like:\r\n\r\nWe track peaks for each stage, and also for each task.  The per-stage peaks are sent in heartbeats; that way we get incremental updates of the metrics as the tasks are running, and if the executor dies we still have some metrics.  The per-task metrics are used for fast tasks; that way, if there are no heartbeats during the task, we'll still get whatever metrics we've polled for that task.",
    "commit": "7556d6ab8615e4fd216da16ec02a4d0020f65ce3",
    "createdAt": "2019-07-31T01:38:33Z",
    "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.executor\n+\n+import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}\n+\n+import scala.collection.mutable.HashMap\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.MemoryManager\n+import org.apache.spark.metrics.ExecutorMetricType\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+/**\n+ * A class that polls executor metrics, and tracks their peaks per task and per stage.\n+ * Each executor keeps an instance of this class.\n+ * The poll method polls the executor metrics, and is either run in its own thread or\n+ * called by the executor's heartbeater thread, depending on configuration.\n+ * The class keeps two ConcurrentHashMaps that are accessed (via its methods) by the\n+ * executor's task runner threads concurrently with the polling thread. One thread may\n+ * update one of these maps while another reads it, so the reading thread may not get\n+ * the latest metrics, but this is ok.\n+ * One ConcurrentHashMap tracks the number of running tasks and the executor metric\n+ * peaks for each stage. A positive task count means the stage is active. When the task\n+ * count reaches zero for a stage, we remove the entry from the map. That way, the map\n+ * only contains entries for active stages and does not grow without bound. On every\n+ * heartbeat, the executor gets the per-stage metric peaks from this class and sends\n+ * them and the peaks are reset.\n+ * The other ConcurrentHashMap tracks the executor metric peaks for each task (the peaks\n+ * seen while each task is running). At task end, these peaks are sent with the task\n+ * result by the task runner.\n+ * The reason we track executor metric peaks per task in addition to per stage is:\n+ * If between heartbeats, a stage completes, so there are no more running tasks for that\n+ * stage, then in the next heartbeat, there are no metrics sent for that stage; however,\n+ * at the end of a task that belonged to that stage, the metrics would have been sent\n+ * in the task result, so we do not lose those peaks."
  }, {
    "author": {
      "login": "wypoon"
    },
    "body": "Sounds fine. I'll update the comment.",
    "commit": "7556d6ab8615e4fd216da16ec02a4d0020f65ce3",
    "createdAt": "2019-07-31T17:21:29Z",
    "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.executor\n+\n+import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}\n+\n+import scala.collection.mutable.HashMap\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.MemoryManager\n+import org.apache.spark.metrics.ExecutorMetricType\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+/**\n+ * A class that polls executor metrics, and tracks their peaks per task and per stage.\n+ * Each executor keeps an instance of this class.\n+ * The poll method polls the executor metrics, and is either run in its own thread or\n+ * called by the executor's heartbeater thread, depending on configuration.\n+ * The class keeps two ConcurrentHashMaps that are accessed (via its methods) by the\n+ * executor's task runner threads concurrently with the polling thread. One thread may\n+ * update one of these maps while another reads it, so the reading thread may not get\n+ * the latest metrics, but this is ok.\n+ * One ConcurrentHashMap tracks the number of running tasks and the executor metric\n+ * peaks for each stage. A positive task count means the stage is active. When the task\n+ * count reaches zero for a stage, we remove the entry from the map. That way, the map\n+ * only contains entries for active stages and does not grow without bound. On every\n+ * heartbeat, the executor gets the per-stage metric peaks from this class and sends\n+ * them and the peaks are reset.\n+ * The other ConcurrentHashMap tracks the executor metric peaks for each task (the peaks\n+ * seen while each task is running). At task end, these peaks are sent with the task\n+ * result by the task runner.\n+ * The reason we track executor metric peaks per task in addition to per stage is:\n+ * If between heartbeats, a stage completes, so there are no more running tasks for that\n+ * stage, then in the next heartbeat, there are no metrics sent for that stage; however,\n+ * at the end of a task that belonged to that stage, the metrics would have been sent\n+ * in the task result, so we do not lose those peaks."
  }, {
    "author": {
      "login": "wypoon"
    },
    "body": "I replaced the last 3 paragraphs with the single paragraph you suggest, with some slight amplification.",
    "commit": "7556d6ab8615e4fd216da16ec02a4d0020f65ce3",
    "createdAt": "2019-07-31T17:36:10Z",
    "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.executor\n+\n+import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}\n+\n+import scala.collection.mutable.HashMap\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.MemoryManager\n+import org.apache.spark.metrics.ExecutorMetricType\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+/**\n+ * A class that polls executor metrics, and tracks their peaks per task and per stage.\n+ * Each executor keeps an instance of this class.\n+ * The poll method polls the executor metrics, and is either run in its own thread or\n+ * called by the executor's heartbeater thread, depending on configuration.\n+ * The class keeps two ConcurrentHashMaps that are accessed (via its methods) by the\n+ * executor's task runner threads concurrently with the polling thread. One thread may\n+ * update one of these maps while another reads it, so the reading thread may not get\n+ * the latest metrics, but this is ok.\n+ * One ConcurrentHashMap tracks the number of running tasks and the executor metric\n+ * peaks for each stage. A positive task count means the stage is active. When the task\n+ * count reaches zero for a stage, we remove the entry from the map. That way, the map\n+ * only contains entries for active stages and does not grow without bound. On every\n+ * heartbeat, the executor gets the per-stage metric peaks from this class and sends\n+ * them and the peaks are reset.\n+ * The other ConcurrentHashMap tracks the executor metric peaks for each task (the peaks\n+ * seen while each task is running). At task end, these peaks are sent with the task\n+ * result by the task runner.\n+ * The reason we track executor metric peaks per task in addition to per stage is:\n+ * If between heartbeats, a stage completes, so there are no more running tasks for that\n+ * stage, then in the next heartbeat, there are no metrics sent for that stage; however,\n+ * at the end of a task that belonged to that stage, the metrics would have been sent\n+ * in the task result, so we do not lose those peaks."
  }],
  "prId": 23767
}]