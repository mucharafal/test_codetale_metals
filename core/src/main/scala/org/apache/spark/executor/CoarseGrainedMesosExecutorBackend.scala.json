[{
  "comments": [{
    "author": {
      "login": "ash211"
    },
    "body": "Grained\n",
    "commit": "99415c3bc9973f2f80faaf7f5742b3bc860bc900",
    "createdAt": "2015-01-02T18:46:20Z",
    "diffHunk": "@@ -0,0 +1,212 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import org.apache.spark.{SparkConf, Logging, SecurityManager}\n+import org.apache.mesos.{Executor => MesosExecutor, ExecutorDriver, MesosExecutorDriver, MesosNativeLibrary}\n+import org.apache.spark.util.{Utils, SignalLogger}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.mesos.Protos._\n+import org.apache.spark.deploy.worker.StandaloneWorkerShuffleService\n+import scala.collection.JavaConversions._\n+import scala.io.Source\n+import java.io.{File, PrintWriter}\n+\n+/**\n+ * The Coarse grained Mesos executor backend is responsible for launching the shuffle service\n+ * and the CoarseGrainedExecutorBackend actor.\n+ * This is assuming the scheduler detected that the shuffle service is enabled and launches\n+ * this class instead of CoarseGrainedExecutorBackend directly.\n+ */\n+private[spark] class CoarseGrainedMesosExecutorBackend(val sparkConf: SparkConf)\n+  extends MesosExecutor\n+  with Logging {\n+\n+  private var shuffleService: StandaloneWorkerShuffleService = null\n+  private var driver: ExecutorDriver = null\n+  private var executorProc: Process = null\n+  private var taskId: TaskID = null\n+  @volatile var killed = false\n+\n+  override def registered(\n+      driver: ExecutorDriver,\n+      executorInfo: ExecutorInfo,\n+      frameworkInfo: FrameworkInfo,\n+      slaveInfo: SlaveInfo) {\n+    this.driver = driver\n+    logInfo(\"Coarse Grain Mesos Executor '\" + executorInfo.getExecutorId.getValue +"
  }],
  "prId": 3861
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "we should reorder these imports as we do in other files\n",
    "commit": "99415c3bc9973f2f80faaf7f5742b3bc860bc900",
    "createdAt": "2015-02-13T04:42:54Z",
    "diffHunk": "@@ -0,0 +1,212 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import org.apache.spark.{SparkConf, Logging, SecurityManager}\n+import org.apache.mesos.{Executor => MesosExecutor, ExecutorDriver, MesosExecutorDriver, MesosNativeLibrary}\n+import org.apache.spark.util.{Utils, SignalLogger}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.mesos.Protos._\n+import org.apache.spark.deploy.worker.StandaloneWorkerShuffleService\n+import scala.collection.JavaConversions._\n+import scala.io.Source\n+import java.io.{File, PrintWriter}",
    "line": 28
  }],
  "prId": 3861
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "Could you do these through `RedirectThread` in `Utils.scala`?\n",
    "commit": "99415c3bc9973f2f80faaf7f5742b3bc860bc900",
    "createdAt": "2015-02-13T04:44:43Z",
    "diffHunk": "@@ -0,0 +1,212 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import org.apache.spark.{SparkConf, Logging, SecurityManager}\n+import org.apache.mesos.{Executor => MesosExecutor, ExecutorDriver, MesosExecutorDriver, MesosNativeLibrary}\n+import org.apache.spark.util.{Utils, SignalLogger}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.mesos.Protos._\n+import org.apache.spark.deploy.worker.StandaloneWorkerShuffleService\n+import scala.collection.JavaConversions._\n+import scala.io.Source\n+import java.io.{File, PrintWriter}\n+\n+/**\n+ * The Coarse grained Mesos executor backend is responsible for launching the shuffle service\n+ * and the CoarseGrainedExecutorBackend actor.\n+ * This is assuming the scheduler detected that the shuffle service is enabled and launches\n+ * this class instead of CoarseGrainedExecutorBackend directly.\n+ */\n+private[spark] class CoarseGrainedMesosExecutorBackend(val sparkConf: SparkConf)\n+  extends MesosExecutor\n+  with Logging {\n+\n+  private var shuffleService: StandaloneWorkerShuffleService = null\n+  private var driver: ExecutorDriver = null\n+  private var executorProc: Process = null\n+  private var taskId: TaskID = null\n+  @volatile var killed = false\n+\n+  override def registered(\n+      driver: ExecutorDriver,\n+      executorInfo: ExecutorInfo,\n+      frameworkInfo: FrameworkInfo,\n+      slaveInfo: SlaveInfo) {\n+    this.driver = driver\n+    logInfo(\"Coarse Grained Mesos Executor '\" + executorInfo.getExecutorId.getValue +\n+      \"' is registered.\")\n+\n+    if (shuffleService == null) {\n+      sparkConf.set(\"spark.shuffle.service.enabled\", \"true\")\n+      shuffleService = new StandaloneWorkerShuffleService(sparkConf, new SecurityManager(sparkConf))\n+      shuffleService.startIfEnabled()\n+    }\n+  }\n+\n+  override def launchTask(d: ExecutorDriver, taskInfo: TaskInfo) {\n+    if (executorProc != null) {\n+      logError(\"Received LaunchTask while executor is already running\")\n+      val status = TaskStatus.newBuilder()\n+        .setTaskId(taskInfo.getTaskId)\n+        .setSlaveId(taskInfo.getSlaveId)\n+        .setState(TaskState.TASK_FAILED)\n+        .setMessage(\"Received LaunchTask while executor is already running\")\n+        .build()\n+      d.sendStatusUpdate(status)\n+      return\n+    }\n+\n+    killed = false\n+\n+    logInfo(\"Launching task id: \" + taskInfo.getTaskId.getValue)\n+\n+    // We are launching the CoarseGrainedExecutorBackend via subprocess\n+    // because the backend is designed to run in its own process.\n+    // Since it's a shared class we are preserving the existing behavior\n+    // and launching it as a subprocess here.\n+    val command = \"exec \" + Utils.deserialize[String](taskInfo.getData().toByteArray)\n+\n+    logInfo(\"Running command: \" + command)\n+\n+    // Mesos only work on linux platforms, so we assume bash is available is Mesos is used.\n+    val pb = new ProcessBuilder(\"/bin/bash\", \"-c\", command)\n+\n+    val currentEnvVars = pb.environment()\n+    for (variable <- taskInfo.getExecutor.getCommand.getEnvironment.getVariablesList()) {\n+      currentEnvVars.put(variable.getName, variable.getValue)\n+    }\n+\n+    executorProc = pb.start()\n+\n+    new Thread(\"stderr reader for task \" + taskInfo.getTaskId.getValue) {\n+      override def run() {\n+        for (line <- Source.fromInputStream(executorProc.getErrorStream).getLines) {\n+          System.err.println(line)\n+        }\n+      }\n+    }.start()\n+\n+    new Thread(\"stdout reader for task \" + taskInfo.getTaskId.getValue) {\n+      override def run() {\n+        for (line <- Source.fromInputStream(executorProc.getInputStream).getLines) {\n+          System.out.println(line)\n+        }\n+      }\n+    }.start()",
    "line": 111
  }],
  "prId": 3861
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "methods without return types should use `: Unit` (see https://cwiki.apache.org/confluence/display/SPARK/Spark+Code+Style+Guide)\n",
    "commit": "99415c3bc9973f2f80faaf7f5742b3bc860bc900",
    "createdAt": "2015-02-13T04:45:52Z",
    "diffHunk": "@@ -0,0 +1,212 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import org.apache.spark.{SparkConf, Logging, SecurityManager}\n+import org.apache.mesos.{Executor => MesosExecutor, ExecutorDriver, MesosExecutorDriver, MesosNativeLibrary}\n+import org.apache.spark.util.{Utils, SignalLogger}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.mesos.Protos._\n+import org.apache.spark.deploy.worker.StandaloneWorkerShuffleService\n+import scala.collection.JavaConversions._\n+import scala.io.Source\n+import java.io.{File, PrintWriter}\n+\n+/**\n+ * The Coarse grained Mesos executor backend is responsible for launching the shuffle service\n+ * and the CoarseGrainedExecutorBackend actor.\n+ * This is assuming the scheduler detected that the shuffle service is enabled and launches\n+ * this class instead of CoarseGrainedExecutorBackend directly.\n+ */\n+private[spark] class CoarseGrainedMesosExecutorBackend(val sparkConf: SparkConf)\n+  extends MesosExecutor\n+  with Logging {\n+\n+  private var shuffleService: StandaloneWorkerShuffleService = null\n+  private var driver: ExecutorDriver = null\n+  private var executorProc: Process = null\n+  private var taskId: TaskID = null\n+  @volatile var killed = false\n+\n+  override def registered(\n+      driver: ExecutorDriver,\n+      executorInfo: ExecutorInfo,\n+      frameworkInfo: FrameworkInfo,\n+      slaveInfo: SlaveInfo) {\n+    this.driver = driver\n+    logInfo(\"Coarse Grained Mesos Executor '\" + executorInfo.getExecutorId.getValue +\n+      \"' is registered.\")\n+\n+    if (shuffleService == null) {\n+      sparkConf.set(\"spark.shuffle.service.enabled\", \"true\")\n+      shuffleService = new StandaloneWorkerShuffleService(sparkConf, new SecurityManager(sparkConf))\n+      shuffleService.startIfEnabled()\n+    }\n+  }\n+\n+  override def launchTask(d: ExecutorDriver, taskInfo: TaskInfo) {\n+    if (executorProc != null) {\n+      logError(\"Received LaunchTask while executor is already running\")\n+      val status = TaskStatus.newBuilder()\n+        .setTaskId(taskInfo.getTaskId)\n+        .setSlaveId(taskInfo.getSlaveId)\n+        .setState(TaskState.TASK_FAILED)\n+        .setMessage(\"Received LaunchTask while executor is already running\")\n+        .build()\n+      d.sendStatusUpdate(status)\n+      return\n+    }\n+\n+    killed = false\n+\n+    logInfo(\"Launching task id: \" + taskInfo.getTaskId.getValue)\n+\n+    // We are launching the CoarseGrainedExecutorBackend via subprocess\n+    // because the backend is designed to run in its own process.\n+    // Since it's a shared class we are preserving the existing behavior\n+    // and launching it as a subprocess here.\n+    val command = \"exec \" + Utils.deserialize[String](taskInfo.getData().toByteArray)\n+\n+    logInfo(\"Running command: \" + command)\n+\n+    // Mesos only work on linux platforms, so we assume bash is available is Mesos is used.\n+    val pb = new ProcessBuilder(\"/bin/bash\", \"-c\", command)\n+\n+    val currentEnvVars = pb.environment()\n+    for (variable <- taskInfo.getExecutor.getCommand.getEnvironment.getVariablesList()) {\n+      currentEnvVars.put(variable.getName, variable.getValue)\n+    }\n+\n+    executorProc = pb.start()\n+\n+    new Thread(\"stderr reader for task \" + taskInfo.getTaskId.getValue) {\n+      override def run() {\n+        for (line <- Source.fromInputStream(executorProc.getErrorStream).getLines) {\n+          System.err.println(line)\n+        }\n+      }\n+    }.start()\n+\n+    new Thread(\"stdout reader for task \" + taskInfo.getTaskId.getValue) {\n+      override def run() {\n+        for (line <- Source.fromInputStream(executorProc.getInputStream).getLines) {\n+          System.out.println(line)\n+        }\n+      }\n+    }.start()\n+\n+    driver.sendStatusUpdate(TaskStatus.newBuilder()\n+      .setState(TaskState.TASK_RUNNING)\n+      .setTaskId(taskInfo.getTaskId)\n+      .build)\n+\n+    new Thread(\"process waiter for mesos executor for task \" + taskInfo.getTaskId.getValue) {\n+      override def run() {\n+        executorProc.waitFor()\n+        val (state, msg) = if (killed) {\n+          (TaskState.TASK_KILLED, \"\")\n+        }  else if (executorProc.exitValue() == 0) {\n+          (TaskState.TASK_FINISHED, \"\")\n+        } else {\n+          (TaskState.TASK_FAILED, \"Exited with status: \" + executorProc.exitValue().toString)\n+        }\n+        // We leave the shuffle service running after the task.\n+        cleanup(state, msg)\n+      }\n+    }.start()\n+\n+    taskId = taskInfo.getTaskId\n+  }\n+\n+  override def error(d: ExecutorDriver, message: String) {",
    "line": 136
  }],
  "prId": 3861
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "For readability I would prefer that you put the builder in a `val` first, and then calling `driver.sendStatusUpdate` on it. Also, please use parentheses for `build()` here because it's not a getter without side-effects.\n",
    "commit": "99415c3bc9973f2f80faaf7f5742b3bc860bc900",
    "createdAt": "2015-02-13T04:47:00Z",
    "diffHunk": "@@ -0,0 +1,212 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import org.apache.spark.{SparkConf, Logging, SecurityManager}\n+import org.apache.mesos.{Executor => MesosExecutor, ExecutorDriver, MesosExecutorDriver, MesosNativeLibrary}\n+import org.apache.spark.util.{Utils, SignalLogger}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.mesos.Protos._\n+import org.apache.spark.deploy.worker.StandaloneWorkerShuffleService\n+import scala.collection.JavaConversions._\n+import scala.io.Source\n+import java.io.{File, PrintWriter}\n+\n+/**\n+ * The Coarse grained Mesos executor backend is responsible for launching the shuffle service\n+ * and the CoarseGrainedExecutorBackend actor.\n+ * This is assuming the scheduler detected that the shuffle service is enabled and launches\n+ * this class instead of CoarseGrainedExecutorBackend directly.\n+ */\n+private[spark] class CoarseGrainedMesosExecutorBackend(val sparkConf: SparkConf)\n+  extends MesosExecutor\n+  with Logging {\n+\n+  private var shuffleService: StandaloneWorkerShuffleService = null\n+  private var driver: ExecutorDriver = null\n+  private var executorProc: Process = null\n+  private var taskId: TaskID = null\n+  @volatile var killed = false\n+\n+  override def registered(\n+      driver: ExecutorDriver,\n+      executorInfo: ExecutorInfo,\n+      frameworkInfo: FrameworkInfo,\n+      slaveInfo: SlaveInfo) {\n+    this.driver = driver\n+    logInfo(\"Coarse Grained Mesos Executor '\" + executorInfo.getExecutorId.getValue +\n+      \"' is registered.\")\n+\n+    if (shuffleService == null) {\n+      sparkConf.set(\"spark.shuffle.service.enabled\", \"true\")\n+      shuffleService = new StandaloneWorkerShuffleService(sparkConf, new SecurityManager(sparkConf))\n+      shuffleService.startIfEnabled()\n+    }\n+  }\n+\n+  override def launchTask(d: ExecutorDriver, taskInfo: TaskInfo) {\n+    if (executorProc != null) {\n+      logError(\"Received LaunchTask while executor is already running\")\n+      val status = TaskStatus.newBuilder()\n+        .setTaskId(taskInfo.getTaskId)\n+        .setSlaveId(taskInfo.getSlaveId)\n+        .setState(TaskState.TASK_FAILED)\n+        .setMessage(\"Received LaunchTask while executor is already running\")\n+        .build()\n+      d.sendStatusUpdate(status)\n+      return\n+    }\n+\n+    killed = false\n+\n+    logInfo(\"Launching task id: \" + taskInfo.getTaskId.getValue)\n+\n+    // We are launching the CoarseGrainedExecutorBackend via subprocess\n+    // because the backend is designed to run in its own process.\n+    // Since it's a shared class we are preserving the existing behavior\n+    // and launching it as a subprocess here.\n+    val command = \"exec \" + Utils.deserialize[String](taskInfo.getData().toByteArray)\n+\n+    logInfo(\"Running command: \" + command)\n+\n+    // Mesos only work on linux platforms, so we assume bash is available is Mesos is used.\n+    val pb = new ProcessBuilder(\"/bin/bash\", \"-c\", command)\n+\n+    val currentEnvVars = pb.environment()\n+    for (variable <- taskInfo.getExecutor.getCommand.getEnvironment.getVariablesList()) {\n+      currentEnvVars.put(variable.getName, variable.getValue)\n+    }\n+\n+    executorProc = pb.start()\n+\n+    new Thread(\"stderr reader for task \" + taskInfo.getTaskId.getValue) {\n+      override def run() {\n+        for (line <- Source.fromInputStream(executorProc.getErrorStream).getLines) {\n+          System.err.println(line)\n+        }\n+      }\n+    }.start()\n+\n+    new Thread(\"stdout reader for task \" + taskInfo.getTaskId.getValue) {\n+      override def run() {\n+        for (line <- Source.fromInputStream(executorProc.getInputStream).getLines) {\n+          System.out.println(line)\n+        }\n+      }\n+    }.start()\n+\n+    driver.sendStatusUpdate(TaskStatus.newBuilder()\n+      .setState(TaskState.TASK_RUNNING)\n+      .setTaskId(taskInfo.getTaskId)\n+      .build)",
    "line": 116
  }],
  "prId": 3861
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "We shouldn't set this config ourselves; it is meant to be set by the user. What we should do here is check whether the user set it and then start the shuffle service if so.\n",
    "commit": "99415c3bc9973f2f80faaf7f5742b3bc860bc900",
    "createdAt": "2015-02-13T05:01:10Z",
    "diffHunk": "@@ -0,0 +1,212 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import org.apache.spark.{SparkConf, Logging, SecurityManager}\n+import org.apache.mesos.{Executor => MesosExecutor, ExecutorDriver, MesosExecutorDriver, MesosNativeLibrary}\n+import org.apache.spark.util.{Utils, SignalLogger}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.mesos.Protos._\n+import org.apache.spark.deploy.worker.StandaloneWorkerShuffleService\n+import scala.collection.JavaConversions._\n+import scala.io.Source\n+import java.io.{File, PrintWriter}\n+\n+/**\n+ * The Coarse grained Mesos executor backend is responsible for launching the shuffle service\n+ * and the CoarseGrainedExecutorBackend actor.\n+ * This is assuming the scheduler detected that the shuffle service is enabled and launches\n+ * this class instead of CoarseGrainedExecutorBackend directly.\n+ */\n+private[spark] class CoarseGrainedMesosExecutorBackend(val sparkConf: SparkConf)\n+  extends MesosExecutor\n+  with Logging {\n+\n+  private var shuffleService: StandaloneWorkerShuffleService = null\n+  private var driver: ExecutorDriver = null\n+  private var executorProc: Process = null\n+  private var taskId: TaskID = null\n+  @volatile var killed = false\n+\n+  override def registered(\n+      driver: ExecutorDriver,\n+      executorInfo: ExecutorInfo,\n+      frameworkInfo: FrameworkInfo,\n+      slaveInfo: SlaveInfo) {\n+    this.driver = driver\n+    logInfo(\"Coarse Grained Mesos Executor '\" + executorInfo.getExecutorId.getValue +\n+      \"' is registered.\")\n+\n+    if (shuffleService == null) {\n+      sparkConf.set(\"spark.shuffle.service.enabled\", \"true\")",
    "line": 56
  }],
  "prId": 3861
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "this should be private, otherwise anyone with a reference to this backend can \"kill\" it.\n",
    "commit": "99415c3bc9973f2f80faaf7f5742b3bc860bc900",
    "createdAt": "2015-02-13T05:01:49Z",
    "diffHunk": "@@ -0,0 +1,212 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import org.apache.spark.{SparkConf, Logging, SecurityManager}\n+import org.apache.mesos.{Executor => MesosExecutor, ExecutorDriver, MesosExecutorDriver, MesosNativeLibrary}\n+import org.apache.spark.util.{Utils, SignalLogger}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.mesos.Protos._\n+import org.apache.spark.deploy.worker.StandaloneWorkerShuffleService\n+import scala.collection.JavaConversions._\n+import scala.io.Source\n+import java.io.{File, PrintWriter}\n+\n+/**\n+ * The Coarse grained Mesos executor backend is responsible for launching the shuffle service\n+ * and the CoarseGrainedExecutorBackend actor.\n+ * This is assuming the scheduler detected that the shuffle service is enabled and launches\n+ * this class instead of CoarseGrainedExecutorBackend directly.\n+ */\n+private[spark] class CoarseGrainedMesosExecutorBackend(val sparkConf: SparkConf)\n+  extends MesosExecutor\n+  with Logging {\n+\n+  private var shuffleService: StandaloneWorkerShuffleService = null\n+  private var driver: ExecutorDriver = null\n+  private var executorProc: Process = null\n+  private var taskId: TaskID = null\n+  @volatile var killed = false",
    "line": 44
  }],
  "prId": 3861
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "what happens if you set an empty message? Does it still get logged? Should we set the message only if it's not empty?\n",
    "commit": "99415c3bc9973f2f80faaf7f5742b3bc860bc900",
    "createdAt": "2015-02-13T05:03:21Z",
    "diffHunk": "@@ -0,0 +1,212 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import org.apache.spark.{SparkConf, Logging, SecurityManager}\n+import org.apache.mesos.{Executor => MesosExecutor, ExecutorDriver, MesosExecutorDriver, MesosNativeLibrary}\n+import org.apache.spark.util.{Utils, SignalLogger}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.mesos.Protos._\n+import org.apache.spark.deploy.worker.StandaloneWorkerShuffleService\n+import scala.collection.JavaConversions._\n+import scala.io.Source\n+import java.io.{File, PrintWriter}\n+\n+/**\n+ * The Coarse grained Mesos executor backend is responsible for launching the shuffle service\n+ * and the CoarseGrainedExecutorBackend actor.\n+ * This is assuming the scheduler detected that the shuffle service is enabled and launches\n+ * this class instead of CoarseGrainedExecutorBackend directly.\n+ */\n+private[spark] class CoarseGrainedMesosExecutorBackend(val sparkConf: SparkConf)\n+  extends MesosExecutor\n+  with Logging {\n+\n+  private var shuffleService: StandaloneWorkerShuffleService = null\n+  private var driver: ExecutorDriver = null\n+  private var executorProc: Process = null\n+  private var taskId: TaskID = null\n+  @volatile var killed = false\n+\n+  override def registered(\n+      driver: ExecutorDriver,\n+      executorInfo: ExecutorInfo,\n+      frameworkInfo: FrameworkInfo,\n+      slaveInfo: SlaveInfo) {\n+    this.driver = driver\n+    logInfo(\"Coarse Grained Mesos Executor '\" + executorInfo.getExecutorId.getValue +\n+      \"' is registered.\")\n+\n+    if (shuffleService == null) {\n+      sparkConf.set(\"spark.shuffle.service.enabled\", \"true\")\n+      shuffleService = new StandaloneWorkerShuffleService(sparkConf, new SecurityManager(sparkConf))\n+      shuffleService.startIfEnabled()\n+    }\n+  }\n+\n+  override def launchTask(d: ExecutorDriver, taskInfo: TaskInfo) {\n+    if (executorProc != null) {\n+      logError(\"Received LaunchTask while executor is already running\")\n+      val status = TaskStatus.newBuilder()\n+        .setTaskId(taskInfo.getTaskId)\n+        .setSlaveId(taskInfo.getSlaveId)\n+        .setState(TaskState.TASK_FAILED)\n+        .setMessage(\"Received LaunchTask while executor is already running\")\n+        .build()\n+      d.sendStatusUpdate(status)\n+      return\n+    }\n+\n+    killed = false\n+\n+    logInfo(\"Launching task id: \" + taskInfo.getTaskId.getValue)\n+\n+    // We are launching the CoarseGrainedExecutorBackend via subprocess\n+    // because the backend is designed to run in its own process.\n+    // Since it's a shared class we are preserving the existing behavior\n+    // and launching it as a subprocess here.\n+    val command = \"exec \" + Utils.deserialize[String](taskInfo.getData().toByteArray)\n+\n+    logInfo(\"Running command: \" + command)\n+\n+    // Mesos only work on linux platforms, so we assume bash is available is Mesos is used.\n+    val pb = new ProcessBuilder(\"/bin/bash\", \"-c\", command)\n+\n+    val currentEnvVars = pb.environment()\n+    for (variable <- taskInfo.getExecutor.getCommand.getEnvironment.getVariablesList()) {\n+      currentEnvVars.put(variable.getName, variable.getValue)\n+    }\n+\n+    executorProc = pb.start()\n+\n+    new Thread(\"stderr reader for task \" + taskInfo.getTaskId.getValue) {\n+      override def run() {\n+        for (line <- Source.fromInputStream(executorProc.getErrorStream).getLines) {\n+          System.err.println(line)\n+        }\n+      }\n+    }.start()\n+\n+    new Thread(\"stdout reader for task \" + taskInfo.getTaskId.getValue) {\n+      override def run() {\n+        for (line <- Source.fromInputStream(executorProc.getInputStream).getLines) {\n+          System.out.println(line)\n+        }\n+      }\n+    }.start()\n+\n+    driver.sendStatusUpdate(TaskStatus.newBuilder()\n+      .setState(TaskState.TASK_RUNNING)\n+      .setTaskId(taskInfo.getTaskId)\n+      .build)\n+\n+    new Thread(\"process waiter for mesos executor for task \" + taskInfo.getTaskId.getValue) {\n+      override def run() {\n+        executorProc.waitFor()\n+        val (state, msg) = if (killed) {\n+          (TaskState.TASK_KILLED, \"\")\n+        }  else if (executorProc.exitValue() == 0) {\n+          (TaskState.TASK_FINISHED, \"\")\n+        } else {\n+          (TaskState.TASK_FAILED, \"Exited with status: \" + executorProc.exitValue().toString)\n+        }\n+        // We leave the shuffle service running after the task.\n+        cleanup(state, msg)\n+      }\n+    }.start()\n+\n+    taskId = taskInfo.getTaskId\n+  }\n+\n+  override def error(d: ExecutorDriver, message: String) {\n+    logError(\"Error from Mesos: \" + message)\n+  }\n+\n+  override def killTask(d: ExecutorDriver, t: TaskID) {\n+    if (taskId == null) {\n+      logError(\"Received killtask when no process is initialized\")\n+      return\n+    }\n+\n+    if (!taskId.getValue.equals(t.getValue)) {\n+      logError(\"Asked to kill task '\" + t.getValue + \"' but executor is running task '\" +\n+        taskId.getValue + \"'\")\n+      return\n+    }\n+\n+    assert(executorProc != null)\n+    killed = true\n+    // We only destroy the coarse grained executor but leave the shuffle\n+    // service running for other tasks that might be reusing this executor.\n+    // This is no-op if the process already finished.\n+    executorProc.destroy()\n+  }\n+\n+  def cleanup(state: TaskState, msg: String = \"\"): Unit = synchronized {\n+    if (driver == null) {\n+      logError(\"Cleaning up process but driver is not initialized\")\n+      return\n+    }\n+\n+    if (executorProc == null) {\n+      logDebug(\"Process is not started or already cleaned up\")\n+      return\n+    }\n+\n+    assert(taskId != null)\n+\n+    driver.sendStatusUpdate(TaskStatus.newBuilder()\n+      .setState(state)\n+      .setMessage(msg)",
    "line": 175
  }],
  "prId": 3861
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "here `command` is a giant string instead of a list of command line arguments. Should we do something like `Utils.splitCommandString` here to preserve the command properly when we pass it to the `ProcessBuilder`?\n",
    "commit": "99415c3bc9973f2f80faaf7f5742b3bc860bc900",
    "createdAt": "2015-02-13T05:05:17Z",
    "diffHunk": "@@ -0,0 +1,212 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import org.apache.spark.{SparkConf, Logging, SecurityManager}\n+import org.apache.mesos.{Executor => MesosExecutor, ExecutorDriver, MesosExecutorDriver, MesosNativeLibrary}\n+import org.apache.spark.util.{Utils, SignalLogger}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.mesos.Protos._\n+import org.apache.spark.deploy.worker.StandaloneWorkerShuffleService\n+import scala.collection.JavaConversions._\n+import scala.io.Source\n+import java.io.{File, PrintWriter}\n+\n+/**\n+ * The Coarse grained Mesos executor backend is responsible for launching the shuffle service\n+ * and the CoarseGrainedExecutorBackend actor.\n+ * This is assuming the scheduler detected that the shuffle service is enabled and launches\n+ * this class instead of CoarseGrainedExecutorBackend directly.\n+ */\n+private[spark] class CoarseGrainedMesosExecutorBackend(val sparkConf: SparkConf)\n+  extends MesosExecutor\n+  with Logging {\n+\n+  private var shuffleService: StandaloneWorkerShuffleService = null\n+  private var driver: ExecutorDriver = null\n+  private var executorProc: Process = null\n+  private var taskId: TaskID = null\n+  @volatile var killed = false\n+\n+  override def registered(\n+      driver: ExecutorDriver,\n+      executorInfo: ExecutorInfo,\n+      frameworkInfo: FrameworkInfo,\n+      slaveInfo: SlaveInfo) {\n+    this.driver = driver\n+    logInfo(\"Coarse Grained Mesos Executor '\" + executorInfo.getExecutorId.getValue +\n+      \"' is registered.\")\n+\n+    if (shuffleService == null) {\n+      sparkConf.set(\"spark.shuffle.service.enabled\", \"true\")\n+      shuffleService = new StandaloneWorkerShuffleService(sparkConf, new SecurityManager(sparkConf))\n+      shuffleService.startIfEnabled()\n+    }\n+  }\n+\n+  override def launchTask(d: ExecutorDriver, taskInfo: TaskInfo) {\n+    if (executorProc != null) {\n+      logError(\"Received LaunchTask while executor is already running\")\n+      val status = TaskStatus.newBuilder()\n+        .setTaskId(taskInfo.getTaskId)\n+        .setSlaveId(taskInfo.getSlaveId)\n+        .setState(TaskState.TASK_FAILED)\n+        .setMessage(\"Received LaunchTask while executor is already running\")\n+        .build()\n+      d.sendStatusUpdate(status)\n+      return\n+    }\n+\n+    killed = false\n+\n+    logInfo(\"Launching task id: \" + taskInfo.getTaskId.getValue)\n+\n+    // We are launching the CoarseGrainedExecutorBackend via subprocess\n+    // because the backend is designed to run in its own process.\n+    // Since it's a shared class we are preserving the existing behavior\n+    // and launching it as a subprocess here.\n+    val command = \"exec \" + Utils.deserialize[String](taskInfo.getData().toByteArray)",
    "line": 83
  }],
  "prId": 3861
}]