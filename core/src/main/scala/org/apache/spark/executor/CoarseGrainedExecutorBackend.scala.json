[{
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "``` scala\ns\"An unknown ($remoteAddress) driver disconnected.\"\n```\n",
    "commit": "287e9f84811205a0f24699ac18632c354bed59c7",
    "createdAt": "2015-03-31T01:10:27Z",
    "diffHunk": "@@ -92,23 +96,28 @@ private[spark] class CoarseGrainedExecutorBackend(\n         executor.killTask(taskId, interruptThread)\n       }\n \n-    case x: DisassociatedEvent =>\n-      if (x.remoteAddress == driver.anchorPath.address) {\n-        logError(s\"Driver $x disassociated! Shutting down.\")\n-        System.exit(1)\n-      } else {\n-        logWarning(s\"Received irrelevant DisassociatedEvent $x\")\n-      }\n-\n     case StopExecutor =>\n       logInfo(\"Driver commanded a shutdown\")\n       executor.stop()\n-      context.stop(self)\n-      context.system.shutdown()\n+      stop()\n+      rpcEnv.shutdown()\n+  }\n+\n+  override def onDisconnected(remoteAddress: RpcAddress): Unit = {\n+    if (driver.exists(_.address == remoteAddress)) {\n+      logError(s\"Driver $remoteAddress disassociated! Shutting down.\")\n+      System.exit(1)\n+    } else {\n+      logWarning(s\"Received irrelevant DisassociatedEvent $remoteAddress\")"
  }, {
    "author": {
      "login": "zsxwing"
    },
    "body": "Fixed.\n",
    "commit": "287e9f84811205a0f24699ac18632c354bed59c7",
    "createdAt": "2015-03-31T15:33:36Z",
    "diffHunk": "@@ -92,23 +96,28 @@ private[spark] class CoarseGrainedExecutorBackend(\n         executor.killTask(taskId, interruptThread)\n       }\n \n-    case x: DisassociatedEvent =>\n-      if (x.remoteAddress == driver.anchorPath.address) {\n-        logError(s\"Driver $x disassociated! Shutting down.\")\n-        System.exit(1)\n-      } else {\n-        logWarning(s\"Received irrelevant DisassociatedEvent $x\")\n-      }\n-\n     case StopExecutor =>\n       logInfo(\"Driver commanded a shutdown\")\n       executor.stop()\n-      context.stop(self)\n-      context.system.shutdown()\n+      stop()\n+      rpcEnv.shutdown()\n+  }\n+\n+  override def onDisconnected(remoteAddress: RpcAddress): Unit = {\n+    if (driver.exists(_.address == remoteAddress)) {\n+      logError(s\"Driver $remoteAddress disassociated! Shutting down.\")\n+      System.exit(1)\n+    } else {\n+      logWarning(s\"Received irrelevant DisassociatedEvent $remoteAddress\")"
  }],
  "prId": 5268
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "register with driver\n",
    "commit": "287e9f84811205a0f24699ac18632c354bed59c7",
    "createdAt": "2015-03-31T01:10:42Z",
    "diffHunk": "@@ -21,39 +21,43 @@ import java.net.URL\n import java.nio.ByteBuffer\n \n import scala.collection.mutable\n-import scala.concurrent.Await\n+import scala.util.{Failure, Success}\n \n-import akka.actor.{Actor, ActorSelection, Props}\n-import akka.pattern.Patterns\n-import akka.remote.{RemotingLifecycleEvent, DisassociatedEvent}\n-\n-import org.apache.spark.{Logging, SecurityManager, SparkConf, SparkEnv}\n+import org.apache.spark.rpc._\n+import org.apache.spark._\n import org.apache.spark.TaskState.TaskState\n import org.apache.spark.deploy.SparkHadoopUtil\n import org.apache.spark.deploy.worker.WorkerWatcher\n import org.apache.spark.scheduler.TaskDescription\n import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages._\n-import org.apache.spark.util.{ActorLogReceive, AkkaUtils, SignalLogger, Utils}\n+import org.apache.spark.util.{SignalLogger, Utils}\n \n private[spark] class CoarseGrainedExecutorBackend(\n+    override val rpcEnv: RpcEnv,\n     driverUrl: String,\n     executorId: String,\n     hostPort: String,\n     cores: Int,\n     userClassPath: Seq[URL],\n     env: SparkEnv)\n-  extends Actor with ActorLogReceive with ExecutorBackend with Logging {\n+  extends RpcEndpoint with ExecutorBackend with Logging {\n \n   Utils.checkHostPort(hostPort, \"Expected hostport\")\n \n   var executor: Executor = null\n-  var driver: ActorSelection = null\n+  @volatile var driver: Option[RpcEndpointRef] = None\n \n-  override def preStart() {\n+  override def onStart() {\n+    import scala.concurrent.ExecutionContext.Implicits.global\n     logInfo(\"Connecting to driver: \" + driverUrl)\n-    driver = context.actorSelection(driverUrl)\n-    driver ! RegisterExecutor(executorId, hostPort, cores, extractLogUrls)\n-    context.system.eventStream.subscribe(self, classOf[RemotingLifecycleEvent])\n+    rpcEnv.asyncSetupEndpointRefByURI(driverUrl).flatMap { ref =>\n+      driver = Some(ref)\n+      ref.sendWithReply[RegisteredExecutor.type](\n+        RegisterExecutor(executorId, self, hostPort, cores, extractLogUrls))\n+    } onComplete {\n+      case Success(msg) => self.send(msg)\n+      case Failure(e) => logError(s\"Cannot register to driver: $driverUrl\", e)"
  }, {
    "author": {
      "login": "zsxwing"
    },
    "body": "Fixed.\n",
    "commit": "287e9f84811205a0f24699ac18632c354bed59c7",
    "createdAt": "2015-03-31T15:33:30Z",
    "diffHunk": "@@ -21,39 +21,43 @@ import java.net.URL\n import java.nio.ByteBuffer\n \n import scala.collection.mutable\n-import scala.concurrent.Await\n+import scala.util.{Failure, Success}\n \n-import akka.actor.{Actor, ActorSelection, Props}\n-import akka.pattern.Patterns\n-import akka.remote.{RemotingLifecycleEvent, DisassociatedEvent}\n-\n-import org.apache.spark.{Logging, SecurityManager, SparkConf, SparkEnv}\n+import org.apache.spark.rpc._\n+import org.apache.spark._\n import org.apache.spark.TaskState.TaskState\n import org.apache.spark.deploy.SparkHadoopUtil\n import org.apache.spark.deploy.worker.WorkerWatcher\n import org.apache.spark.scheduler.TaskDescription\n import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages._\n-import org.apache.spark.util.{ActorLogReceive, AkkaUtils, SignalLogger, Utils}\n+import org.apache.spark.util.{SignalLogger, Utils}\n \n private[spark] class CoarseGrainedExecutorBackend(\n+    override val rpcEnv: RpcEnv,\n     driverUrl: String,\n     executorId: String,\n     hostPort: String,\n     cores: Int,\n     userClassPath: Seq[URL],\n     env: SparkEnv)\n-  extends Actor with ActorLogReceive with ExecutorBackend with Logging {\n+  extends RpcEndpoint with ExecutorBackend with Logging {\n \n   Utils.checkHostPort(hostPort, \"Expected hostport\")\n \n   var executor: Executor = null\n-  var driver: ActorSelection = null\n+  @volatile var driver: Option[RpcEndpointRef] = None\n \n-  override def preStart() {\n+  override def onStart() {\n+    import scala.concurrent.ExecutionContext.Implicits.global\n     logInfo(\"Connecting to driver: \" + driverUrl)\n-    driver = context.actorSelection(driverUrl)\n-    driver ! RegisterExecutor(executorId, hostPort, cores, extractLogUrls)\n-    context.system.eventStream.subscribe(self, classOf[RemotingLifecycleEvent])\n+    rpcEnv.asyncSetupEndpointRefByURI(driverUrl).flatMap { ref =>\n+      driver = Some(ref)\n+      ref.sendWithReply[RegisteredExecutor.type](\n+        RegisterExecutor(executorId, self, hostPort, cores, extractLogUrls))\n+    } onComplete {\n+      case Success(msg) => self.send(msg)\n+      case Failure(e) => logError(s\"Cannot register to driver: $driverUrl\", e)"
  }],
  "prId": 5268
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "how about replacing this with a AtomicReference?\n",
    "commit": "287e9f84811205a0f24699ac18632c354bed59c7",
    "createdAt": "2015-03-31T01:11:05Z",
    "diffHunk": "@@ -21,39 +21,43 @@ import java.net.URL\n import java.nio.ByteBuffer\n \n import scala.collection.mutable\n-import scala.concurrent.Await\n+import scala.util.{Failure, Success}\n \n-import akka.actor.{Actor, ActorSelection, Props}\n-import akka.pattern.Patterns\n-import akka.remote.{RemotingLifecycleEvent, DisassociatedEvent}\n-\n-import org.apache.spark.{Logging, SecurityManager, SparkConf, SparkEnv}\n+import org.apache.spark.rpc._\n+import org.apache.spark._\n import org.apache.spark.TaskState.TaskState\n import org.apache.spark.deploy.SparkHadoopUtil\n import org.apache.spark.deploy.worker.WorkerWatcher\n import org.apache.spark.scheduler.TaskDescription\n import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages._\n-import org.apache.spark.util.{ActorLogReceive, AkkaUtils, SignalLogger, Utils}\n+import org.apache.spark.util.{SignalLogger, Utils}\n \n private[spark] class CoarseGrainedExecutorBackend(\n+    override val rpcEnv: RpcEnv,\n     driverUrl: String,\n     executorId: String,\n     hostPort: String,\n     cores: Int,\n     userClassPath: Seq[URL],\n     env: SparkEnv)\n-  extends Actor with ActorLogReceive with ExecutorBackend with Logging {\n+  extends RpcEndpoint with ExecutorBackend with Logging {\n \n   Utils.checkHostPort(hostPort, \"Expected hostport\")\n \n   var executor: Executor = null\n-  var driver: ActorSelection = null\n+  @volatile var driver: Option[RpcEndpointRef] = None",
    "line": 37
  }],
  "prId": 5268
}]