[{
  "comments": [{
    "author": {
      "login": "ankuriitg"
    },
    "body": "Did you mean to use the ^ operator? I think || operator will suffice here.",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-13T17:50:09Z",
    "diffHunk": "@@ -62,7 +62,21 @@ private[spark] class ProcfsMetricsGetter(procfsDir: String = \"/proc/\") extends L\n         SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n       val shouldLogStageExecutorProcessTreeMetrics =\n         SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n-      procDirExists.get && shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+      val shouldAddProcessTreeMetricsToMetricsSet =\n+        SparkEnv.get.conf.get(config.METRICS_PROCESS_TREE_METRICS)\n+      val pickEitherUIOrMetricsSet = shouldLogStageExecutorProcessTreeMetrics ^"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "Thanks for the review @ankuriitg. My goal was to just allow one of them, so I used the Xor. @squito had some ideas with CachedGauge, which I will give it a try and after finishing the testing will update here. I think that way this logic will be simplify ",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-13T18:47:02Z",
    "diffHunk": "@@ -62,7 +62,21 @@ private[spark] class ProcfsMetricsGetter(procfsDir: String = \"/proc/\") extends L\n         SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n       val shouldLogStageExecutorProcessTreeMetrics =\n         SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n-      procDirExists.get && shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+      val shouldAddProcessTreeMetricsToMetricsSet =\n+        SparkEnv.get.conf.get(config.METRICS_PROCESS_TREE_METRICS)\n+      val pickEitherUIOrMetricsSet = shouldLogStageExecutorProcessTreeMetrics ^"
  }],
  "prId": 23306
}, {
  "comments": [{
    "author": {
      "login": "ankuriitg"
    },
    "body": "Can the return statement be simplified to:\r\n\r\n`procDirExists.get && pickEitherUIOrMetricsSet && !areBothUIMetricsEnabled`",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-13T17:58:34Z",
    "diffHunk": "@@ -62,7 +62,21 @@ private[spark] class ProcfsMetricsGetter(procfsDir: String = \"/proc/\") extends L\n         SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n       val shouldLogStageExecutorProcessTreeMetrics =\n         SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n-      procDirExists.get && shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+      val shouldAddProcessTreeMetricsToMetricsSet =\n+        SparkEnv.get.conf.get(config.METRICS_PROCESS_TREE_METRICS)\n+      val pickEitherUIOrMetricsSet = shouldLogStageExecutorProcessTreeMetrics ^\n+        shouldAddProcessTreeMetricsToMetricsSet\n+      val areBothUIMetricsEnabled = shouldLogStageExecutorProcessTreeMetrics &&\n+        shouldAddProcessTreeMetricsToMetricsSet\n+      if (areBothUIMetricsEnabled) {\n+        logWarning(\"You have enabled \" +\n+          \"both spark.eventLog.logStageExecutorProcessTreeMetrics.enabled\" +\n+          \" and spark.metrics.logStageExecutorProcessTreeMetrics.enabled. This isn't \" +\n+          \"allowed. As a result Procfs metrics won't be reported to UI or Metricsset\")\n+      }\n+      (procDirExists.get && shouldLogStageExecutorMetrics && pickEitherUIOrMetricsSet) ||"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "For process tree metrics that are going to be reported to REST API, logging for stage executor metrics should be enabled. Anyway using CachedGauge this may become simpler. ",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-13T18:49:01Z",
    "diffHunk": "@@ -62,7 +62,21 @@ private[spark] class ProcfsMetricsGetter(procfsDir: String = \"/proc/\") extends L\n         SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n       val shouldLogStageExecutorProcessTreeMetrics =\n         SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n-      procDirExists.get && shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+      val shouldAddProcessTreeMetricsToMetricsSet =\n+        SparkEnv.get.conf.get(config.METRICS_PROCESS_TREE_METRICS)\n+      val pickEitherUIOrMetricsSet = shouldLogStageExecutorProcessTreeMetrics ^\n+        shouldAddProcessTreeMetricsToMetricsSet\n+      val areBothUIMetricsEnabled = shouldLogStageExecutorProcessTreeMetrics &&\n+        shouldAddProcessTreeMetricsToMetricsSet\n+      if (areBothUIMetricsEnabled) {\n+        logWarning(\"You have enabled \" +\n+          \"both spark.eventLog.logStageExecutorProcessTreeMetrics.enabled\" +\n+          \" and spark.metrics.logStageExecutorProcessTreeMetrics.enabled. This isn't \" +\n+          \"allowed. As a result Procfs metrics won't be reported to UI or Metricsset\")\n+      }\n+      (procDirExists.get && shouldLogStageExecutorMetrics && pickEitherUIOrMetricsSet) ||"
  }, {
    "author": {
      "login": "ankuriitg"
    },
    "body": "I think if pickEitherUIOrMetricsSet is changed to use || operator, then the above return statement should suffice.\r\n\r\nRegarding, caching of the metrics, I agree with that as well. I will just recommend to have caching built in ProcfsMetricsGetter.pTreeInfo.computeAllMetrics(). This method should just return last cached metrics if they are not stale, otherwise compute new metrics. This way it can be used by both of MetricSet and Logging.",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-13T19:46:22Z",
    "diffHunk": "@@ -62,7 +62,21 @@ private[spark] class ProcfsMetricsGetter(procfsDir: String = \"/proc/\") extends L\n         SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n       val shouldLogStageExecutorProcessTreeMetrics =\n         SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n-      procDirExists.get && shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+      val shouldAddProcessTreeMetricsToMetricsSet =\n+        SparkEnv.get.conf.get(config.METRICS_PROCESS_TREE_METRICS)\n+      val pickEitherUIOrMetricsSet = shouldLogStageExecutorProcessTreeMetrics ^\n+        shouldAddProcessTreeMetricsToMetricsSet\n+      val areBothUIMetricsEnabled = shouldLogStageExecutorProcessTreeMetrics &&\n+        shouldAddProcessTreeMetricsToMetricsSet\n+      if (areBothUIMetricsEnabled) {\n+        logWarning(\"You have enabled \" +\n+          \"both spark.eventLog.logStageExecutorProcessTreeMetrics.enabled\" +\n+          \" and spark.metrics.logStageExecutorProcessTreeMetrics.enabled. This isn't \" +\n+          \"allowed. As a result Procfs metrics won't be reported to UI or Metricsset\")\n+      }\n+      (procDirExists.get && shouldLogStageExecutorMetrics && pickEitherUIOrMetricsSet) ||"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "I also think that way is better, since it is simpler. However the pulling rate and heart beat aren't the same. So I guess we can document that to set expectations.",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-13T19:59:17Z",
    "diffHunk": "@@ -62,7 +62,21 @@ private[spark] class ProcfsMetricsGetter(procfsDir: String = \"/proc/\") extends L\n         SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n       val shouldLogStageExecutorProcessTreeMetrics =\n         SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n-      procDirExists.get && shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+      val shouldAddProcessTreeMetricsToMetricsSet =\n+        SparkEnv.get.conf.get(config.METRICS_PROCESS_TREE_METRICS)\n+      val pickEitherUIOrMetricsSet = shouldLogStageExecutorProcessTreeMetrics ^\n+        shouldAddProcessTreeMetricsToMetricsSet\n+      val areBothUIMetricsEnabled = shouldLogStageExecutorProcessTreeMetrics &&\n+        shouldAddProcessTreeMetricsToMetricsSet\n+      if (areBothUIMetricsEnabled) {\n+        logWarning(\"You have enabled \" +\n+          \"both spark.eventLog.logStageExecutorProcessTreeMetrics.enabled\" +\n+          \" and spark.metrics.logStageExecutorProcessTreeMetrics.enabled. This isn't \" +\n+          \"allowed. As a result Procfs metrics won't be reported to UI or Metricsset\")\n+      }\n+      (procDirExists.get && shouldLogStageExecutorMetrics && pickEitherUIOrMetricsSet) ||"
  }],
  "prId": 23306
}, {
  "comments": [{
    "author": {
      "login": "ankuriitg"
    },
    "body": "I think the return statement can be further simplified to just `procDirExists.get && pickAnyOfUIOrMetricsSet`",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-17T18:34:40Z",
    "diffHunk": "@@ -64,19 +74,11 @@ private[spark] class ProcfsMetricsGetter(procfsDir: String = \"/proc/\") extends L\n         SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n       val shouldAddProcessTreeMetricsToMetricsSet =\n         SparkEnv.get.conf.get(config.METRICS_PROCESS_TREE_METRICS)\n-      val pickEitherUIOrMetricsSet = shouldLogStageExecutorProcessTreeMetrics ^\n-        shouldAddProcessTreeMetricsToMetricsSet\n-      val areBothUIMetricsEnabled = shouldLogStageExecutorProcessTreeMetrics &&\n+      val pickAnyOfUIOrMetricsSet = shouldLogStageExecutorProcessTreeMetrics ||\n         shouldAddProcessTreeMetricsToMetricsSet\n-      if (areBothUIMetricsEnabled) {\n-        logWarning(\"You have enabled \" +\n-          \"both spark.eventLog.logStageExecutorProcessTreeMetrics.enabled\" +\n-          \" and spark.metrics.logStageExecutorProcessTreeMetrics.enabled. This isn't \" +\n-          \"allowed. As a result Procfs metrics won't be reported to UI or Metricsset\")\n-      }\n-      (procDirExists.get && shouldLogStageExecutorMetrics && pickEitherUIOrMetricsSet) ||\n-        (procDirExists.get && !shouldLogStageExecutorMetrics &&\n-          pickEitherUIOrMetricsSet && shouldAddProcessTreeMetricsToMetricsSet)\n+\n+      (procDirExists.get && shouldLogStageExecutorMetrics && pickAnyOfUIOrMetricsSet) ||"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "Not really. The procfs metrics shouldn't be reported to REST API or UI if the stage executor metrics aren't enables, but they can be reported to metrics system if it is enabled. ",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-17T19:11:08Z",
    "diffHunk": "@@ -64,19 +74,11 @@ private[spark] class ProcfsMetricsGetter(procfsDir: String = \"/proc/\") extends L\n         SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n       val shouldAddProcessTreeMetricsToMetricsSet =\n         SparkEnv.get.conf.get(config.METRICS_PROCESS_TREE_METRICS)\n-      val pickEitherUIOrMetricsSet = shouldLogStageExecutorProcessTreeMetrics ^\n-        shouldAddProcessTreeMetricsToMetricsSet\n-      val areBothUIMetricsEnabled = shouldLogStageExecutorProcessTreeMetrics &&\n+      val pickAnyOfUIOrMetricsSet = shouldLogStageExecutorProcessTreeMetrics ||\n         shouldAddProcessTreeMetricsToMetricsSet\n-      if (areBothUIMetricsEnabled) {\n-        logWarning(\"You have enabled \" +\n-          \"both spark.eventLog.logStageExecutorProcessTreeMetrics.enabled\" +\n-          \" and spark.metrics.logStageExecutorProcessTreeMetrics.enabled. This isn't \" +\n-          \"allowed. As a result Procfs metrics won't be reported to UI or Metricsset\")\n-      }\n-      (procDirExists.get && shouldLogStageExecutorMetrics && pickEitherUIOrMetricsSet) ||\n-        (procDirExists.get && !shouldLogStageExecutorMetrics &&\n-          pickEitherUIOrMetricsSet && shouldAddProcessTreeMetricsToMetricsSet)\n+\n+      (procDirExists.get && shouldLogStageExecutorMetrics && pickAnyOfUIOrMetricsSet) ||"
  }, {
    "author": {
      "login": "ankuriitg"
    },
    "body": "Right, isn't that what `pickAnyOfUIOrMetricsSet` is for?",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-17T19:23:18Z",
    "diffHunk": "@@ -64,19 +74,11 @@ private[spark] class ProcfsMetricsGetter(procfsDir: String = \"/proc/\") extends L\n         SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n       val shouldAddProcessTreeMetricsToMetricsSet =\n         SparkEnv.get.conf.get(config.METRICS_PROCESS_TREE_METRICS)\n-      val pickEitherUIOrMetricsSet = shouldLogStageExecutorProcessTreeMetrics ^\n-        shouldAddProcessTreeMetricsToMetricsSet\n-      val areBothUIMetricsEnabled = shouldLogStageExecutorProcessTreeMetrics &&\n+      val pickAnyOfUIOrMetricsSet = shouldLogStageExecutorProcessTreeMetrics ||\n         shouldAddProcessTreeMetricsToMetricsSet\n-      if (areBothUIMetricsEnabled) {\n-        logWarning(\"You have enabled \" +\n-          \"both spark.eventLog.logStageExecutorProcessTreeMetrics.enabled\" +\n-          \" and spark.metrics.logStageExecutorProcessTreeMetrics.enabled. This isn't \" +\n-          \"allowed. As a result Procfs metrics won't be reported to UI or Metricsset\")\n-      }\n-      (procDirExists.get && shouldLogStageExecutorMetrics && pickEitherUIOrMetricsSet) ||\n-        (procDirExists.get && !shouldLogStageExecutorMetrics &&\n-          pickEitherUIOrMetricsSet && shouldAddProcessTreeMetricsToMetricsSet)\n+\n+      (procDirExists.get && shouldLogStageExecutorMetrics && pickAnyOfUIOrMetricsSet) ||"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "pickAnyOfUIOrMetricsSet is just for checking whether one of UI or metrics set is enabled. Enabling stage level metrics is through the other metric If we want process tree metrics to be reported the stage level executor metrics which report metrics over heartbeat should also be enabled.",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-17T19:45:41Z",
    "diffHunk": "@@ -64,19 +74,11 @@ private[spark] class ProcfsMetricsGetter(procfsDir: String = \"/proc/\") extends L\n         SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n       val shouldAddProcessTreeMetricsToMetricsSet =\n         SparkEnv.get.conf.get(config.METRICS_PROCESS_TREE_METRICS)\n-      val pickEitherUIOrMetricsSet = shouldLogStageExecutorProcessTreeMetrics ^\n-        shouldAddProcessTreeMetricsToMetricsSet\n-      val areBothUIMetricsEnabled = shouldLogStageExecutorProcessTreeMetrics &&\n+      val pickAnyOfUIOrMetricsSet = shouldLogStageExecutorProcessTreeMetrics ||\n         shouldAddProcessTreeMetricsToMetricsSet\n-      if (areBothUIMetricsEnabled) {\n-        logWarning(\"You have enabled \" +\n-          \"both spark.eventLog.logStageExecutorProcessTreeMetrics.enabled\" +\n-          \" and spark.metrics.logStageExecutorProcessTreeMetrics.enabled. This isn't \" +\n-          \"allowed. As a result Procfs metrics won't be reported to UI or Metricsset\")\n-      }\n-      (procDirExists.get && shouldLogStageExecutorMetrics && pickEitherUIOrMetricsSet) ||\n-        (procDirExists.get && !shouldLogStageExecutorMetrics &&\n-          pickEitherUIOrMetricsSet && shouldAddProcessTreeMetricsToMetricsSet)\n+\n+      (procDirExists.get && shouldLogStageExecutorMetrics && pickAnyOfUIOrMetricsSet) ||"
  }, {
    "author": {
      "login": "squito"
    },
    "body": "I think this is really confusing b/c `shouldLogStageExecutorMetrics` and `shouldLogStageExecutorProcessTreeMetrics` look so similar.  and I don't think `pickAnyOf...` is really helping to simplify.  Maybe\r\n\r\n```scala\r\nprocDirExists.get && (\r\n  (stageMetricLoggingEnabled && logProcfsMetrics) ||\r\n  procFsMetricsSystemEnabled\r\n)\r\n```\r\n\r\n\r\nbut actually -- are any of the checks other than `procDirExists` actually necessary?  you're now doing those checks outside of here (which makes more sense, I think), no sense in repeating them here.  Now this class just needs to report metrics when somebody asks for them, whomever that is.",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-17T22:44:11Z",
    "diffHunk": "@@ -64,19 +74,11 @@ private[spark] class ProcfsMetricsGetter(procfsDir: String = \"/proc/\") extends L\n         SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n       val shouldAddProcessTreeMetricsToMetricsSet =\n         SparkEnv.get.conf.get(config.METRICS_PROCESS_TREE_METRICS)\n-      val pickEitherUIOrMetricsSet = shouldLogStageExecutorProcessTreeMetrics ^\n-        shouldAddProcessTreeMetricsToMetricsSet\n-      val areBothUIMetricsEnabled = shouldLogStageExecutorProcessTreeMetrics &&\n+      val pickAnyOfUIOrMetricsSet = shouldLogStageExecutorProcessTreeMetrics ||\n         shouldAddProcessTreeMetricsToMetricsSet\n-      if (areBothUIMetricsEnabled) {\n-        logWarning(\"You have enabled \" +\n-          \"both spark.eventLog.logStageExecutorProcessTreeMetrics.enabled\" +\n-          \" and spark.metrics.logStageExecutorProcessTreeMetrics.enabled. This isn't \" +\n-          \"allowed. As a result Procfs metrics won't be reported to UI or Metricsset\")\n-      }\n-      (procDirExists.get && shouldLogStageExecutorMetrics && pickEitherUIOrMetricsSet) ||\n-        (procDirExists.get && !shouldLogStageExecutorMetrics &&\n-          pickEitherUIOrMetricsSet && shouldAddProcessTreeMetricsToMetricsSet)\n+\n+      (procDirExists.get && shouldLogStageExecutorMetrics && pickAnyOfUIOrMetricsSet) ||"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "I think I can remove the checks from here per your suggestion and just do a little update to one od the tests in ExecutorMetricsType. I will do more testing to avoid mistakes. There are 8 possible configs that 6 of them are acceptable but two are not.",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-17T23:32:38Z",
    "diffHunk": "@@ -64,19 +74,11 @@ private[spark] class ProcfsMetricsGetter(procfsDir: String = \"/proc/\") extends L\n         SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n       val shouldAddProcessTreeMetricsToMetricsSet =\n         SparkEnv.get.conf.get(config.METRICS_PROCESS_TREE_METRICS)\n-      val pickEitherUIOrMetricsSet = shouldLogStageExecutorProcessTreeMetrics ^\n-        shouldAddProcessTreeMetricsToMetricsSet\n-      val areBothUIMetricsEnabled = shouldLogStageExecutorProcessTreeMetrics &&\n+      val pickAnyOfUIOrMetricsSet = shouldLogStageExecutorProcessTreeMetrics ||\n         shouldAddProcessTreeMetricsToMetricsSet\n-      if (areBothUIMetricsEnabled) {\n-        logWarning(\"You have enabled \" +\n-          \"both spark.eventLog.logStageExecutorProcessTreeMetrics.enabled\" +\n-          \" and spark.metrics.logStageExecutorProcessTreeMetrics.enabled. This isn't \" +\n-          \"allowed. As a result Procfs metrics won't be reported to UI or Metricsset\")\n-      }\n-      (procDirExists.get && shouldLogStageExecutorMetrics && pickEitherUIOrMetricsSet) ||\n-        (procDirExists.get && !shouldLogStageExecutorMetrics &&\n-          pickEitherUIOrMetricsSet && shouldAddProcessTreeMetricsToMetricsSet)\n+\n+      (procDirExists.get && shouldLogStageExecutorMetrics && pickAnyOfUIOrMetricsSet) ||"
  }],
  "prId": 23306
}, {
  "comments": [{
    "author": {
      "login": "ankuriitg"
    },
    "body": "I think it will be better if we keep the timestamp metric outside of ProcfsMetrics, as this is not a metric and I could not find any benefit of keeping inside ProcfsMetrics",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-17T18:39:57Z",
    "diffHunk": "@@ -37,7 +37,9 @@ private[spark] case class ProcfsMetrics(\n     pythonVmemTotal: Long,\n     pythonRSSTotal: Long,\n     otherVmemTotal: Long,\n-    otherRSSTotal: Long)\n+    otherRSSTotal: Long,\n+    timeStamp: Long)"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "I considered timestamp as a property of the set of metrics, so probably better to keep it like this.",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-17T19:12:20Z",
    "diffHunk": "@@ -37,7 +37,9 @@ private[spark] case class ProcfsMetrics(\n     pythonVmemTotal: Long,\n     pythonRSSTotal: Long,\n     otherVmemTotal: Long,\n-    otherRSSTotal: Long)\n+    otherRSSTotal: Long,\n+    timeStamp: Long)"
  }, {
    "author": {
      "login": "squito"
    },
    "body": "I agree with Ankur, whats the point of including the timestamp here?",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-17T22:55:56Z",
    "diffHunk": "@@ -37,7 +37,9 @@ private[spark] case class ProcfsMetrics(\n     pythonVmemTotal: Long,\n     pythonRSSTotal: Long,\n     otherVmemTotal: Long,\n-    otherRSSTotal: Long)\n+    otherRSSTotal: Long,\n+    timeStamp: Long)"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "I think responded to that. What is the point of separating it when it is used as a property of the set of metrics?",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-17T23:15:36Z",
    "diffHunk": "@@ -37,7 +37,9 @@ private[spark] case class ProcfsMetrics(\n     pythonVmemTotal: Long,\n     pythonRSSTotal: Long,\n     otherVmemTotal: Long,\n-    otherRSSTotal: Long)\n+    otherRSSTotal: Long,\n+    timeStamp: Long)"
  }, {
    "author": {
      "login": "ankuriitg"
    },
    "body": "I think the point that both I and Imran are trying to make here is that timestamp is not a metric and thus it should not be kept inside ProcfsMetrics. The purpose of timestamp is to evict older cached metrics and thus it should be a part of ProcfsMetricsGetter along with `cachedAllMetric`.",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-17T23:38:41Z",
    "diffHunk": "@@ -37,7 +37,9 @@ private[spark] case class ProcfsMetrics(\n     pythonVmemTotal: Long,\n     pythonRSSTotal: Long,\n     otherVmemTotal: Long,\n-    otherRSSTotal: Long)\n+    otherRSSTotal: Long,\n+    timeStamp: Long)"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": " I considered ProcfsMetrics as a class. The objects of this class have properties which are metrics and now a timestamp. So I think it is better to have timestamp defined along with other properties. This make the code cleaner as well.  Sorry if I can't understand why it shouldn't be here.",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-17T23:53:36Z",
    "diffHunk": "@@ -37,7 +37,9 @@ private[spark] case class ProcfsMetrics(\n     pythonVmemTotal: Long,\n     pythonRSSTotal: Long,\n     otherVmemTotal: Long,\n-    otherRSSTotal: Long)\n+    otherRSSTotal: Long,\n+    timeStamp: Long)"
  }],
  "prId": 23306
}, {
  "comments": [{
    "author": {
      "login": "ankuriitg"
    },
    "body": "I think it makes sense to make it configurable, defaulting to 10s maybe",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-17T18:42:17Z",
    "diffHunk": "@@ -195,45 +197,56 @@ private[spark] class ProcfsMetricsGetter(procfsDir: String = \"/proc/\") extends L\n         if (procInfoSplit(1).toLowerCase(Locale.US).contains(\"java\")) {\n           allMetrics.copy(\n             jvmVmemTotal = allMetrics.jvmVmemTotal + vmem,\n-            jvmRSSTotal = allMetrics.jvmRSSTotal + (rssMem)\n+            jvmRSSTotal = allMetrics.jvmRSSTotal + (rssMem),\n+            timeStamp = System.currentTimeMillis\n           )\n         }\n         else if (procInfoSplit(1).toLowerCase(Locale.US).contains(\"python\")) {\n           allMetrics.copy(\n             pythonVmemTotal = allMetrics.pythonVmemTotal + vmem,\n-            pythonRSSTotal = allMetrics.pythonRSSTotal + (rssMem)\n+            pythonRSSTotal = allMetrics.pythonRSSTotal + (rssMem),\n+            timeStamp = System.currentTimeMillis\n           )\n         }\n         else {\n           allMetrics.copy(\n             otherVmemTotal = allMetrics.otherVmemTotal + vmem,\n-            otherRSSTotal = allMetrics.otherRSSTotal + (rssMem)\n+            otherRSSTotal = allMetrics.otherRSSTotal + (rssMem),\n+            timeStamp = System.currentTimeMillis\n           )\n         }\n       }\n     } catch {\n       case f: IOException =>\n         logWarning(\"There was a problem with reading\" +\n           \" the stat file of the process. \", f)\n-        ProcfsMetrics(0, 0, 0, 0, 0, 0)\n+        ProcfsMetrics(0, 0, 0, 0, 0, 0, System.currentTimeMillis)\n     }\n   }\n \n   private[spark] def computeAllMetrics(): ProcfsMetrics = {\n     if (!isAvailable) {\n-      return ProcfsMetrics(0, 0, 0, 0, 0, 0)\n+      return ProcfsMetrics(0, 0, 0, 0, 0, 0, System.currentTimeMillis)\n     }\n-    val pids = computeProcessTree\n-    var allMetrics = ProcfsMetrics(0, 0, 0, 0, 0, 0)\n-    for (p <- pids) {\n-      allMetrics = addProcfsMetricsFromOneProcess(allMetrics, p)\n-      // if we had an error getting any of the metrics, we don't want to report partial metrics, as\n-      // that would be misleading.\n-      if (!isAvailable) {\n-        return ProcfsMetrics(0, 0, 0, 0, 0, 0)\n+    val lastMetricComputation = System.currentTimeMillis() - cachedAllMetric.timeStamp\n+    // Check whether we have computed the metrics in the past 1s\n+    // ToDo: Should we make this configurable?\n+    if(lastMetricComputation > Math.min(1000, HEARTBEAT_INTERVAL_MS)) {"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "Yeah, I also think it makes sense to be configurable. What do you think @squito?  ",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-17T19:20:57Z",
    "diffHunk": "@@ -195,45 +197,56 @@ private[spark] class ProcfsMetricsGetter(procfsDir: String = \"/proc/\") extends L\n         if (procInfoSplit(1).toLowerCase(Locale.US).contains(\"java\")) {\n           allMetrics.copy(\n             jvmVmemTotal = allMetrics.jvmVmemTotal + vmem,\n-            jvmRSSTotal = allMetrics.jvmRSSTotal + (rssMem)\n+            jvmRSSTotal = allMetrics.jvmRSSTotal + (rssMem),\n+            timeStamp = System.currentTimeMillis\n           )\n         }\n         else if (procInfoSplit(1).toLowerCase(Locale.US).contains(\"python\")) {\n           allMetrics.copy(\n             pythonVmemTotal = allMetrics.pythonVmemTotal + vmem,\n-            pythonRSSTotal = allMetrics.pythonRSSTotal + (rssMem)\n+            pythonRSSTotal = allMetrics.pythonRSSTotal + (rssMem),\n+            timeStamp = System.currentTimeMillis\n           )\n         }\n         else {\n           allMetrics.copy(\n             otherVmemTotal = allMetrics.otherVmemTotal + vmem,\n-            otherRSSTotal = allMetrics.otherRSSTotal + (rssMem)\n+            otherRSSTotal = allMetrics.otherRSSTotal + (rssMem),\n+            timeStamp = System.currentTimeMillis\n           )\n         }\n       }\n     } catch {\n       case f: IOException =>\n         logWarning(\"There was a problem with reading\" +\n           \" the stat file of the process. \", f)\n-        ProcfsMetrics(0, 0, 0, 0, 0, 0)\n+        ProcfsMetrics(0, 0, 0, 0, 0, 0, System.currentTimeMillis)\n     }\n   }\n \n   private[spark] def computeAllMetrics(): ProcfsMetrics = {\n     if (!isAvailable) {\n-      return ProcfsMetrics(0, 0, 0, 0, 0, 0)\n+      return ProcfsMetrics(0, 0, 0, 0, 0, 0, System.currentTimeMillis)\n     }\n-    val pids = computeProcessTree\n-    var allMetrics = ProcfsMetrics(0, 0, 0, 0, 0, 0)\n-    for (p <- pids) {\n-      allMetrics = addProcfsMetricsFromOneProcess(allMetrics, p)\n-      // if we had an error getting any of the metrics, we don't want to report partial metrics, as\n-      // that would be misleading.\n-      if (!isAvailable) {\n-        return ProcfsMetrics(0, 0, 0, 0, 0, 0)\n+    val lastMetricComputation = System.currentTimeMillis() - cachedAllMetric.timeStamp\n+    // Check whether we have computed the metrics in the past 1s\n+    // ToDo: Should we make this configurable?\n+    if(lastMetricComputation > Math.min(1000, HEARTBEAT_INTERVAL_MS)) {"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "Just to add more context about having 1000ms here. The pulling request for Metrics system can't be less than 1 second. So user can configure the caching period using heartbeat interval if they want to cache for less than 1 second. The configuration option can let them to have a cache that is valid for more than 1 second. ",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-17T19:58:51Z",
    "diffHunk": "@@ -195,45 +197,56 @@ private[spark] class ProcfsMetricsGetter(procfsDir: String = \"/proc/\") extends L\n         if (procInfoSplit(1).toLowerCase(Locale.US).contains(\"java\")) {\n           allMetrics.copy(\n             jvmVmemTotal = allMetrics.jvmVmemTotal + vmem,\n-            jvmRSSTotal = allMetrics.jvmRSSTotal + (rssMem)\n+            jvmRSSTotal = allMetrics.jvmRSSTotal + (rssMem),\n+            timeStamp = System.currentTimeMillis\n           )\n         }\n         else if (procInfoSplit(1).toLowerCase(Locale.US).contains(\"python\")) {\n           allMetrics.copy(\n             pythonVmemTotal = allMetrics.pythonVmemTotal + vmem,\n-            pythonRSSTotal = allMetrics.pythonRSSTotal + (rssMem)\n+            pythonRSSTotal = allMetrics.pythonRSSTotal + (rssMem),\n+            timeStamp = System.currentTimeMillis\n           )\n         }\n         else {\n           allMetrics.copy(\n             otherVmemTotal = allMetrics.otherVmemTotal + vmem,\n-            otherRSSTotal = allMetrics.otherRSSTotal + (rssMem)\n+            otherRSSTotal = allMetrics.otherRSSTotal + (rssMem),\n+            timeStamp = System.currentTimeMillis\n           )\n         }\n       }\n     } catch {\n       case f: IOException =>\n         logWarning(\"There was a problem with reading\" +\n           \" the stat file of the process. \", f)\n-        ProcfsMetrics(0, 0, 0, 0, 0, 0)\n+        ProcfsMetrics(0, 0, 0, 0, 0, 0, System.currentTimeMillis)\n     }\n   }\n \n   private[spark] def computeAllMetrics(): ProcfsMetrics = {\n     if (!isAvailable) {\n-      return ProcfsMetrics(0, 0, 0, 0, 0, 0)\n+      return ProcfsMetrics(0, 0, 0, 0, 0, 0, System.currentTimeMillis)\n     }\n-    val pids = computeProcessTree\n-    var allMetrics = ProcfsMetrics(0, 0, 0, 0, 0, 0)\n-    for (p <- pids) {\n-      allMetrics = addProcfsMetricsFromOneProcess(allMetrics, p)\n-      // if we had an error getting any of the metrics, we don't want to report partial metrics, as\n-      // that would be misleading.\n-      if (!isAvailable) {\n-        return ProcfsMetrics(0, 0, 0, 0, 0, 0)\n+    val lastMetricComputation = System.currentTimeMillis() - cachedAllMetric.timeStamp\n+    // Check whether we have computed the metrics in the past 1s\n+    // ToDo: Should we make this configurable?\n+    if(lastMetricComputation > Math.min(1000, HEARTBEAT_INTERVAL_MS)) {"
  }],
  "prId": 23306
}, {
  "comments": [{
    "author": {
      "login": "ankuriitg"
    },
    "body": "Also, maybe ensure that no prior computation it currently going on, just to ensure that if a user sets the configurable value too low, this does not get into a continuous loop of computing metrics",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-17T18:54:09Z",
    "diffHunk": "@@ -195,45 +197,56 @@ private[spark] class ProcfsMetricsGetter(procfsDir: String = \"/proc/\") extends L\n         if (procInfoSplit(1).toLowerCase(Locale.US).contains(\"java\")) {\n           allMetrics.copy(\n             jvmVmemTotal = allMetrics.jvmVmemTotal + vmem,\n-            jvmRSSTotal = allMetrics.jvmRSSTotal + (rssMem)\n+            jvmRSSTotal = allMetrics.jvmRSSTotal + (rssMem),\n+            timeStamp = System.currentTimeMillis\n           )\n         }\n         else if (procInfoSplit(1).toLowerCase(Locale.US).contains(\"python\")) {\n           allMetrics.copy(\n             pythonVmemTotal = allMetrics.pythonVmemTotal + vmem,\n-            pythonRSSTotal = allMetrics.pythonRSSTotal + (rssMem)\n+            pythonRSSTotal = allMetrics.pythonRSSTotal + (rssMem),\n+            timeStamp = System.currentTimeMillis\n           )\n         }\n         else {\n           allMetrics.copy(\n             otherVmemTotal = allMetrics.otherVmemTotal + vmem,\n-            otherRSSTotal = allMetrics.otherRSSTotal + (rssMem)\n+            otherRSSTotal = allMetrics.otherRSSTotal + (rssMem),\n+            timeStamp = System.currentTimeMillis\n           )\n         }\n       }\n     } catch {\n       case f: IOException =>\n         logWarning(\"There was a problem with reading\" +\n           \" the stat file of the process. \", f)\n-        ProcfsMetrics(0, 0, 0, 0, 0, 0)\n+        ProcfsMetrics(0, 0, 0, 0, 0, 0, System.currentTimeMillis)\n     }\n   }\n \n   private[spark] def computeAllMetrics(): ProcfsMetrics = {\n     if (!isAvailable) {\n-      return ProcfsMetrics(0, 0, 0, 0, 0, 0)\n+      return ProcfsMetrics(0, 0, 0, 0, 0, 0, System.currentTimeMillis)\n     }\n-    val pids = computeProcessTree\n-    var allMetrics = ProcfsMetrics(0, 0, 0, 0, 0, 0)\n-    for (p <- pids) {\n-      allMetrics = addProcfsMetricsFromOneProcess(allMetrics, p)\n-      // if we had an error getting any of the metrics, we don't want to report partial metrics, as\n-      // that would be misleading.\n-      if (!isAvailable) {\n-        return ProcfsMetrics(0, 0, 0, 0, 0, 0)\n+    val lastMetricComputation = System.currentTimeMillis() - cachedAllMetric.timeStamp\n+    // Check whether we have computed the metrics in the past 1s\n+    // ToDo: Should we make this configurable?\n+    if(lastMetricComputation > Math.min(1000, HEARTBEAT_INTERVAL_MS)) {"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "This is a really good point. I need to make sure that it will be safe to have multiple calls to procfs from ExcutorMetricsType and ProcfsSource at the same time. It is possible that someone really wants to bypass this and always compute the metrics. \r\nI will consider your idea of checking whether a computation is already in progress as well. I want to avoid waiting or sleep though since it makes since a little bit messy",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-17T19:26:32Z",
    "diffHunk": "@@ -195,45 +197,56 @@ private[spark] class ProcfsMetricsGetter(procfsDir: String = \"/proc/\") extends L\n         if (procInfoSplit(1).toLowerCase(Locale.US).contains(\"java\")) {\n           allMetrics.copy(\n             jvmVmemTotal = allMetrics.jvmVmemTotal + vmem,\n-            jvmRSSTotal = allMetrics.jvmRSSTotal + (rssMem)\n+            jvmRSSTotal = allMetrics.jvmRSSTotal + (rssMem),\n+            timeStamp = System.currentTimeMillis\n           )\n         }\n         else if (procInfoSplit(1).toLowerCase(Locale.US).contains(\"python\")) {\n           allMetrics.copy(\n             pythonVmemTotal = allMetrics.pythonVmemTotal + vmem,\n-            pythonRSSTotal = allMetrics.pythonRSSTotal + (rssMem)\n+            pythonRSSTotal = allMetrics.pythonRSSTotal + (rssMem),\n+            timeStamp = System.currentTimeMillis\n           )\n         }\n         else {\n           allMetrics.copy(\n             otherVmemTotal = allMetrics.otherVmemTotal + vmem,\n-            otherRSSTotal = allMetrics.otherRSSTotal + (rssMem)\n+            otherRSSTotal = allMetrics.otherRSSTotal + (rssMem),\n+            timeStamp = System.currentTimeMillis\n           )\n         }\n       }\n     } catch {\n       case f: IOException =>\n         logWarning(\"There was a problem with reading\" +\n           \" the stat file of the process. \", f)\n-        ProcfsMetrics(0, 0, 0, 0, 0, 0)\n+        ProcfsMetrics(0, 0, 0, 0, 0, 0, System.currentTimeMillis)\n     }\n   }\n \n   private[spark] def computeAllMetrics(): ProcfsMetrics = {\n     if (!isAvailable) {\n-      return ProcfsMetrics(0, 0, 0, 0, 0, 0)\n+      return ProcfsMetrics(0, 0, 0, 0, 0, 0, System.currentTimeMillis)\n     }\n-    val pids = computeProcessTree\n-    var allMetrics = ProcfsMetrics(0, 0, 0, 0, 0, 0)\n-    for (p <- pids) {\n-      allMetrics = addProcfsMetricsFromOneProcess(allMetrics, p)\n-      // if we had an error getting any of the metrics, we don't want to report partial metrics, as\n-      // that would be misleading.\n-      if (!isAvailable) {\n-        return ProcfsMetrics(0, 0, 0, 0, 0, 0)\n+    val lastMetricComputation = System.currentTimeMillis() - cachedAllMetric.timeStamp\n+    // Check whether we have computed the metrics in the past 1s\n+    // ToDo: Should we make this configurable?\n+    if(lastMetricComputation > Math.min(1000, HEARTBEAT_INTERVAL_MS)) {"
  }, {
    "author": {
      "login": "squito"
    },
    "body": "yeah you probably need to add `synchronized` to `computeMetrics` .  (also `addProcfsMetricsFromOneProcess` should be `private`).",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-17T22:57:20Z",
    "diffHunk": "@@ -195,45 +197,56 @@ private[spark] class ProcfsMetricsGetter(procfsDir: String = \"/proc/\") extends L\n         if (procInfoSplit(1).toLowerCase(Locale.US).contains(\"java\")) {\n           allMetrics.copy(\n             jvmVmemTotal = allMetrics.jvmVmemTotal + vmem,\n-            jvmRSSTotal = allMetrics.jvmRSSTotal + (rssMem)\n+            jvmRSSTotal = allMetrics.jvmRSSTotal + (rssMem),\n+            timeStamp = System.currentTimeMillis\n           )\n         }\n         else if (procInfoSplit(1).toLowerCase(Locale.US).contains(\"python\")) {\n           allMetrics.copy(\n             pythonVmemTotal = allMetrics.pythonVmemTotal + vmem,\n-            pythonRSSTotal = allMetrics.pythonRSSTotal + (rssMem)\n+            pythonRSSTotal = allMetrics.pythonRSSTotal + (rssMem),\n+            timeStamp = System.currentTimeMillis\n           )\n         }\n         else {\n           allMetrics.copy(\n             otherVmemTotal = allMetrics.otherVmemTotal + vmem,\n-            otherRSSTotal = allMetrics.otherRSSTotal + (rssMem)\n+            otherRSSTotal = allMetrics.otherRSSTotal + (rssMem),\n+            timeStamp = System.currentTimeMillis\n           )\n         }\n       }\n     } catch {\n       case f: IOException =>\n         logWarning(\"There was a problem with reading\" +\n           \" the stat file of the process. \", f)\n-        ProcfsMetrics(0, 0, 0, 0, 0, 0)\n+        ProcfsMetrics(0, 0, 0, 0, 0, 0, System.currentTimeMillis)\n     }\n   }\n \n   private[spark] def computeAllMetrics(): ProcfsMetrics = {\n     if (!isAvailable) {\n-      return ProcfsMetrics(0, 0, 0, 0, 0, 0)\n+      return ProcfsMetrics(0, 0, 0, 0, 0, 0, System.currentTimeMillis)\n     }\n-    val pids = computeProcessTree\n-    var allMetrics = ProcfsMetrics(0, 0, 0, 0, 0, 0)\n-    for (p <- pids) {\n-      allMetrics = addProcfsMetricsFromOneProcess(allMetrics, p)\n-      // if we had an error getting any of the metrics, we don't want to report partial metrics, as\n-      // that would be misleading.\n-      if (!isAvailable) {\n-        return ProcfsMetrics(0, 0, 0, 0, 0, 0)\n+    val lastMetricComputation = System.currentTimeMillis() - cachedAllMetric.timeStamp\n+    // Check whether we have computed the metrics in the past 1s\n+    // ToDo: Should we make this configurable?\n+    if(lastMetricComputation > Math.min(1000, HEARTBEAT_INTERVAL_MS)) {"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "I want to do some testing before adding a synchronized method. Metrics doesn't seems to be that critical and a benign race that removing it will cause bad performance isn't a good idea. As a last solution I think defining this method as synchronized is ok.",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-17T23:19:56Z",
    "diffHunk": "@@ -195,45 +197,56 @@ private[spark] class ProcfsMetricsGetter(procfsDir: String = \"/proc/\") extends L\n         if (procInfoSplit(1).toLowerCase(Locale.US).contains(\"java\")) {\n           allMetrics.copy(\n             jvmVmemTotal = allMetrics.jvmVmemTotal + vmem,\n-            jvmRSSTotal = allMetrics.jvmRSSTotal + (rssMem)\n+            jvmRSSTotal = allMetrics.jvmRSSTotal + (rssMem),\n+            timeStamp = System.currentTimeMillis\n           )\n         }\n         else if (procInfoSplit(1).toLowerCase(Locale.US).contains(\"python\")) {\n           allMetrics.copy(\n             pythonVmemTotal = allMetrics.pythonVmemTotal + vmem,\n-            pythonRSSTotal = allMetrics.pythonRSSTotal + (rssMem)\n+            pythonRSSTotal = allMetrics.pythonRSSTotal + (rssMem),\n+            timeStamp = System.currentTimeMillis\n           )\n         }\n         else {\n           allMetrics.copy(\n             otherVmemTotal = allMetrics.otherVmemTotal + vmem,\n-            otherRSSTotal = allMetrics.otherRSSTotal + (rssMem)\n+            otherRSSTotal = allMetrics.otherRSSTotal + (rssMem),\n+            timeStamp = System.currentTimeMillis\n           )\n         }\n       }\n     } catch {\n       case f: IOException =>\n         logWarning(\"There was a problem with reading\" +\n           \" the stat file of the process. \", f)\n-        ProcfsMetrics(0, 0, 0, 0, 0, 0)\n+        ProcfsMetrics(0, 0, 0, 0, 0, 0, System.currentTimeMillis)\n     }\n   }\n \n   private[spark] def computeAllMetrics(): ProcfsMetrics = {\n     if (!isAvailable) {\n-      return ProcfsMetrics(0, 0, 0, 0, 0, 0)\n+      return ProcfsMetrics(0, 0, 0, 0, 0, 0, System.currentTimeMillis)\n     }\n-    val pids = computeProcessTree\n-    var allMetrics = ProcfsMetrics(0, 0, 0, 0, 0, 0)\n-    for (p <- pids) {\n-      allMetrics = addProcfsMetricsFromOneProcess(allMetrics, p)\n-      // if we had an error getting any of the metrics, we don't want to report partial metrics, as\n-      // that would be misleading.\n-      if (!isAvailable) {\n-        return ProcfsMetrics(0, 0, 0, 0, 0, 0)\n+    val lastMetricComputation = System.currentTimeMillis() - cachedAllMetric.timeStamp\n+    // Check whether we have computed the metrics in the past 1s\n+    // ToDo: Should we make this configurable?\n+    if(lastMetricComputation > Math.min(1000, HEARTBEAT_INTERVAL_MS)) {"
  }],
  "prId": 23306
}, {
  "comments": [{
    "author": {
      "login": "ankuriitg"
    },
    "body": "nit: private",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-19T19:01:32Z",
    "diffHunk": "@@ -47,6 +48,14 @@ private[spark] class ProcfsMetricsGetter(procfsDir: String = \"/proc/\") extends L\n   private val pageSize = computePageSize()\n   private var isAvailable: Boolean = isProcfsAvailable\n   private val pid = computePid()\n+  var cachedAllMetric = ProcfsMetrics(0, 0, 0, 0, 0, 0)"
  }],
  "prId": 23306
}, {
  "comments": [{
    "author": {
      "login": "ankuriitg"
    },
    "body": "nit: private",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-19T19:01:44Z",
    "diffHunk": "@@ -47,6 +48,14 @@ private[spark] class ProcfsMetricsGetter(procfsDir: String = \"/proc/\") extends L\n   private val pageSize = computePageSize()\n   private var isAvailable: Boolean = isProcfsAvailable\n   private val pid = computePid()\n+  var cachedAllMetric = ProcfsMetrics(0, 0, 0, 0, 0, 0)\n+  var lastimeMetricsComputed = 0L"
  }],
  "prId": 23306
}, {
  "comments": [{
    "author": {
      "login": "ankuriitg"
    },
    "body": "Configurable cache validity duration?",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-19T19:02:10Z",
    "diffHunk": "@@ -205,21 +210,44 @@ private[spark] class ProcfsMetricsGetter(procfsDir: String = \"/proc/\") extends L\n     }\n   }\n \n+  private[spark] def isCacheValid(): Boolean = {\n+    val lastMetricComputation = System.currentTimeMillis() - lastimeMetricsComputed\n+    // ToDo: Should we make this configurable?\n+    return  Math.min(1000, HEARTBEAT_INTERVAL_MS) > lastMetricComputation"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "Waiting for @squito opinion as well. ",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-19T19:23:49Z",
    "diffHunk": "@@ -205,21 +210,44 @@ private[spark] class ProcfsMetricsGetter(procfsDir: String = \"/proc/\") extends L\n     }\n   }\n \n+  private[spark] def isCacheValid(): Boolean = {\n+    val lastMetricComputation = System.currentTimeMillis() - lastimeMetricsComputed\n+    // ToDo: Should we make this configurable?\n+    return  Math.min(1000, HEARTBEAT_INTERVAL_MS) > lastMetricComputation"
  }, {
    "author": {
      "login": "squito"
    },
    "body": "yes, configurable",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-19T19:53:30Z",
    "diffHunk": "@@ -205,21 +210,44 @@ private[spark] class ProcfsMetricsGetter(procfsDir: String = \"/proc/\") extends L\n     }\n   }\n \n+  private[spark] def isCacheValid(): Boolean = {\n+    val lastMetricComputation = System.currentTimeMillis() - lastimeMetricsComputed\n+    // ToDo: Should we make this configurable?\n+    return  Math.min(1000, HEARTBEAT_INTERVAL_MS) > lastMetricComputation"
  }],
  "prId": 23306
}, {
  "comments": [{
    "author": {
      "login": "ankuriitg"
    },
    "body": "nit: Change the name to `timeSinceLastComputation`",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-19T19:02:39Z",
    "diffHunk": "@@ -205,21 +210,44 @@ private[spark] class ProcfsMetricsGetter(procfsDir: String = \"/proc/\") extends L\n     }\n   }\n \n+  private[spark] def isCacheValid(): Boolean = {\n+    val lastMetricComputation = System.currentTimeMillis() - lastimeMetricsComputed"
  }],
  "prId": 23306
}, {
  "comments": [{
    "author": {
      "login": "ankuriitg"
    },
    "body": "return cachedAllMetric\r\nAdditionally, I see that we are creating a lot of instances for empty for ProcfsMetrics, maybe keep a static empty instance around and just return that if metrics are not available instead of creating a new instance everytime",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-19T19:06:08Z",
    "diffHunk": "@@ -205,21 +210,44 @@ private[spark] class ProcfsMetricsGetter(procfsDir: String = \"/proc/\") extends L\n     }\n   }\n \n+  private[spark] def isCacheValid(): Boolean = {\n+    val lastMetricComputation = System.currentTimeMillis() - lastimeMetricsComputed\n+    // ToDo: Should we make this configurable?\n+    return  Math.min(1000, HEARTBEAT_INTERVAL_MS) > lastMetricComputation\n+  }\n+\n   private[spark] def computeAllMetrics(): ProcfsMetrics = {\n     if (!isAvailable) {\n+      lastimeMetricsComputed = System.currentTimeMillis\n+      cachedAllMetric = ProcfsMetrics(0, 0, 0, 0, 0, 0)\n       return ProcfsMetrics(0, 0, 0, 0, 0, 0)"
  }],
  "prId": 23306
}, {
  "comments": [{
    "author": {
      "login": "ankuriitg"
    },
    "body": "For double-checked locking, I think you need to make both cachedAllMetric and lastimeMetricsComputed volatile",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-19T19:07:59Z",
    "diffHunk": "@@ -205,21 +210,44 @@ private[spark] class ProcfsMetricsGetter(procfsDir: String = \"/proc/\") extends L\n     }\n   }\n \n+  private[spark] def isCacheValid(): Boolean = {\n+    val lastMetricComputation = System.currentTimeMillis() - lastimeMetricsComputed\n+    // ToDo: Should we make this configurable?\n+    return  Math.min(1000, HEARTBEAT_INTERVAL_MS) > lastMetricComputation\n+  }\n+\n   private[spark] def computeAllMetrics(): ProcfsMetrics = {\n     if (!isAvailable) {\n+      lastimeMetricsComputed = System.currentTimeMillis\n+      cachedAllMetric = ProcfsMetrics(0, 0, 0, 0, 0, 0)\n       return ProcfsMetrics(0, 0, 0, 0, 0, 0)\n     }\n-    val pids = computeProcessTree\n-    var allMetrics = ProcfsMetrics(0, 0, 0, 0, 0, 0)\n-    for (p <- pids) {\n-      allMetrics = addProcfsMetricsFromOneProcess(allMetrics, p)\n-      // if we had an error getting any of the metrics, we don't want to report partial metrics, as\n-      // that would be misleading.\n-      if (!isAvailable) {\n-        return ProcfsMetrics(0, 0, 0, 0, 0, 0)\n+\n+    if (!isCacheValid) {\n+      this.synchronized {\n+        if (isCacheValid) {",
    "line": 65
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "This is a good point. I am worried that this whole caching thing with synchronization and volatility can be an unnecessary overhead for the whole system. Should we really care about having both metrics systems and UI metric updater work together? Isn't it better to just enable one of them at the time to avoid overhead. @squito I especially want to hear from you since you first mentioned the idea of caching here.",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-19T19:58:21Z",
    "diffHunk": "@@ -205,21 +210,44 @@ private[spark] class ProcfsMetricsGetter(procfsDir: String = \"/proc/\") extends L\n     }\n   }\n \n+  private[spark] def isCacheValid(): Boolean = {\n+    val lastMetricComputation = System.currentTimeMillis() - lastimeMetricsComputed\n+    // ToDo: Should we make this configurable?\n+    return  Math.min(1000, HEARTBEAT_INTERVAL_MS) > lastMetricComputation\n+  }\n+\n   private[spark] def computeAllMetrics(): ProcfsMetrics = {\n     if (!isAvailable) {\n+      lastimeMetricsComputed = System.currentTimeMillis\n+      cachedAllMetric = ProcfsMetrics(0, 0, 0, 0, 0, 0)\n       return ProcfsMetrics(0, 0, 0, 0, 0, 0)\n     }\n-    val pids = computeProcessTree\n-    var allMetrics = ProcfsMetrics(0, 0, 0, 0, 0, 0)\n-    for (p <- pids) {\n-      allMetrics = addProcfsMetricsFromOneProcess(allMetrics, p)\n-      // if we had an error getting any of the metrics, we don't want to report partial metrics, as\n-      // that would be misleading.\n-      if (!isAvailable) {\n-        return ProcfsMetrics(0, 0, 0, 0, 0, 0)\n+\n+    if (!isCacheValid) {\n+      this.synchronized {\n+        if (isCacheValid) {",
    "line": 65
  }],
  "prId": 23306
}, {
  "comments": [{
    "author": {
      "login": "ankuriitg"
    },
    "body": "nit: closing brackets and else statement in the same line",
    "commit": "e3b23b87656c06c8b5a037c0b37e6bd1355dc3d6",
    "createdAt": "2018-12-19T19:08:32Z",
    "diffHunk": "@@ -205,21 +210,44 @@ private[spark] class ProcfsMetricsGetter(procfsDir: String = \"/proc/\") extends L\n     }\n   }\n \n+  private[spark] def isCacheValid(): Boolean = {\n+    val lastMetricComputation = System.currentTimeMillis() - lastimeMetricsComputed\n+    // ToDo: Should we make this configurable?\n+    return  Math.min(1000, HEARTBEAT_INTERVAL_MS) > lastMetricComputation\n+  }\n+\n   private[spark] def computeAllMetrics(): ProcfsMetrics = {\n     if (!isAvailable) {\n+      lastimeMetricsComputed = System.currentTimeMillis\n+      cachedAllMetric = ProcfsMetrics(0, 0, 0, 0, 0, 0)\n       return ProcfsMetrics(0, 0, 0, 0, 0, 0)\n     }\n-    val pids = computeProcessTree\n-    var allMetrics = ProcfsMetrics(0, 0, 0, 0, 0, 0)\n-    for (p <- pids) {\n-      allMetrics = addProcfsMetricsFromOneProcess(allMetrics, p)\n-      // if we had an error getting any of the metrics, we don't want to report partial metrics, as\n-      // that would be misleading.\n-      if (!isAvailable) {\n-        return ProcfsMetrics(0, 0, 0, 0, 0, 0)\n+\n+    if (!isCacheValid) {\n+      this.synchronized {\n+        if (isCacheValid) {\n+          return cachedAllMetric\n+        }\n+        val pids = computeProcessTree\n+        var allMetrics = ProcfsMetrics(0, 0, 0, 0, 0, 0)\n+        for (p <- pids) {\n+          allMetrics = addProcfsMetricsFromOneProcess(allMetrics, p)\n+          // if we had an error getting any of the metrics, we don't\n+          // want to report partial metrics, as that would be misleading.\n+          if (!isAvailable) {\n+            lastimeMetricsComputed = System.currentTimeMillis\n+            cachedAllMetric = ProcfsMetrics(0, 0, 0, 0, 0, 0)\n+            return cachedAllMetric\n+          }\n+        }\n+        lastimeMetricsComputed = System.currentTimeMillis\n+        cachedAllMetric = allMetrics\n+        allMetrics\n       }\n     }"
  }],
  "prId": 23306
}]