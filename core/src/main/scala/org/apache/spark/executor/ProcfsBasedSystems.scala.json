[{
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "nit: each arg on its own line, just a double indent (4 spaces)",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-03T16:03:55Z",
    "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(jvmVmemTotal: Long,\n+                                             jvmRSSTotal: Long,"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "rather than making this a `var` for testing, you can make it an argument to the constructor, with a default value",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-03T16:05:24Z",
    "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(jvmVmemTotal: Long,\n+                                             jvmRSSTotal: Long,\n+                                             pythonVmemTotal: Long,\n+                                             pythonRSSTotal: Long,\n+                                             otherVmemTotal: Long,\n+                                             otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems extends Logging {\n+  var procfsDir = \"/proc/\""
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "can be a `val`.   Also I don't love the name `isItProcfsBased`, how about `isProcfsAvailable`.",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-03T16:11:33Z",
    "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(jvmVmemTotal: Long,\n+                                             jvmRSSTotal: Long,\n+                                             pythonVmemTotal: Long,\n+                                             pythonRSSTotal: Long,\n+                                             otherVmemTotal: Long,\n+                                             otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems extends Logging {\n+  var procfsDir = \"/proc/\"\n+  val procfsStatFile = \"stat\"\n+  var pageSize = 0\n+  var isAvailable: Boolean = isItProcfsBased"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "Thank you Imran, for the reviews. I will apply your coments. \r\nIt needs to be a var for the case that for some reason we can't compute process tree any more.",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-03T20:15:27Z",
    "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(jvmVmemTotal: Long,\n+                                             jvmRSSTotal: Long,\n+                                             pythonVmemTotal: Long,\n+                                             pythonRSSTotal: Long,\n+                                             otherVmemTotal: Long,\n+                                             otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems extends Logging {\n+  var procfsDir = \"/proc/\"\n+  val procfsStatFile = \"stat\"\n+  var pageSize = 0\n+  var isAvailable: Boolean = isItProcfsBased"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "include the exception as the second argument to `logDebug`, so we get the full stack trace.  Also the two cases are basically the same, I'd combine them, no real value in a separate msg just for IOException",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-03T16:14:08Z",
    "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(jvmVmemTotal: Long,\n+                                             jvmRSSTotal: Long,\n+                                             pythonVmemTotal: Long,\n+                                             pythonRSSTotal: Long,\n+                                             otherVmemTotal: Long,\n+                                             otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems extends Logging {\n+  var procfsDir = \"/proc/\"\n+  val procfsStatFile = \"stat\"\n+  var pageSize = 0\n+  var isAvailable: Boolean = isItProcfsBased\n+  private val pid: Int = computePid()\n+  private val ptree: scala.collection.mutable.Map[ Int, Set[Int]] =\n+    scala.collection.mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal: Long = 0\n+  private var latestJVMRSSTotal: Long = 0\n+  private var latestPythonVmemTotal: Long = 0\n+  private var latestPythonRSSTotal: Long = 0\n+  private var latestOtherVmemTotal: Long = 0\n+  private var latestOtherRSSTotal: Long = 0\n+\n+  computeProcessTree()\n+\n+  private def isItProcfsBased: Boolean = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out: Array[Byte] = Array.fill[Byte](length)(0)\n+      Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+      val pid = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+      return pid;\n+    }\n+    catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return -1\n+      case _ => logDebug(\"Some exception occurred when trying to compute process tree. \" +\n+        \"As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "pretty sure this can't change in the middle so you can just get it once.",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-03T16:59:49Z",
    "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(jvmVmemTotal: Long,\n+                                             jvmRSSTotal: Long,\n+                                             pythonVmemTotal: Long,\n+                                             pythonRSSTotal: Long,\n+                                             otherVmemTotal: Long,\n+                                             otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems extends Logging {\n+  var procfsDir = \"/proc/\"\n+  val procfsStatFile = \"stat\"\n+  var pageSize = 0\n+  var isAvailable: Boolean = isItProcfsBased\n+  private val pid: Int = computePid()\n+  private val ptree: scala.collection.mutable.Map[ Int, Set[Int]] =\n+    scala.collection.mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal: Long = 0\n+  private var latestJVMRSSTotal: Long = 0\n+  private var latestPythonVmemTotal: Long = 0\n+  private var latestPythonRSSTotal: Long = 0\n+  private var latestOtherVmemTotal: Long = 0\n+  private var latestOtherRSSTotal: Long = 0\n+\n+  computeProcessTree()\n+\n+  private def isItProcfsBased: Boolean = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out: Array[Byte] = Array.fill[Byte](length)(0)\n+      Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+      val pid = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+      return pid;\n+    }\n+    catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return -1\n+      case _ => logDebug(\"Some exception occurred when trying to compute process tree. \" +\n+        \"As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Unit = {\n+    val cmd = Array(\"getconf\", \"PAGESIZE\")\n+    val out: Array[Byte] = Array.fill[Byte](10)(0)\n+    Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+    pageSize = Integer.parseInt(new String(out, \"UTF-8\").trim)"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "super nit: choose consistent capitalization for \"pid\", in particular whether or not you will capitalize the 'i'.  personally I prefer lower 'i', `getChildPids`",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-03T17:01:35Z",
    "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(jvmVmemTotal: Long,\n+                                             jvmRSSTotal: Long,\n+                                             pythonVmemTotal: Long,\n+                                             pythonRSSTotal: Long,\n+                                             otherVmemTotal: Long,\n+                                             otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems extends Logging {\n+  var procfsDir = \"/proc/\"\n+  val procfsStatFile = \"stat\"\n+  var pageSize = 0\n+  var isAvailable: Boolean = isItProcfsBased\n+  private val pid: Int = computePid()\n+  private val ptree: scala.collection.mutable.Map[ Int, Set[Int]] =\n+    scala.collection.mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal: Long = 0\n+  private var latestJVMRSSTotal: Long = 0\n+  private var latestPythonVmemTotal: Long = 0\n+  private var latestPythonRSSTotal: Long = 0\n+  private var latestOtherVmemTotal: Long = 0\n+  private var latestOtherRSSTotal: Long = 0\n+\n+  computeProcessTree()\n+\n+  private def isItProcfsBased: Boolean = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out: Array[Byte] = Array.fill[Byte](length)(0)\n+      Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+      val pid = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+      return pid;\n+    }\n+    catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return -1\n+      case _ => logDebug(\"Some exception occurred when trying to compute process tree. \" +\n+        \"As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Unit = {\n+    val cmd = Array(\"getconf\", \"PAGESIZE\")\n+    val out: Array[Byte] = Array.fill[Byte](10)(0)\n+    Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+    pageSize = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+  }\n+\n+  private def computeProcessTree(): Unit = {\n+    if (!isAvailable) {\n+      return\n+    }\n+    computePageSize\n+    val queue: Queue[Int] = new Queue[Int]()\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPIds(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree += (p -> c.toSet)\n+      }\n+      else {\n+        ptree += (p -> Set[Int]())\n+      }\n+    }\n+  }\n+\n+  private def getChildPIds(pid: Int): ArrayBuffer[Int] = {"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "this is a bit a dangerous if there is ever any error output, since you're not consuming it, things may just hang.  I'm not sure what you would do with stderr (maybe just logError it?)  but we certainly don't want to ignore it.  I'd suggest using ProcessBuilder.  Eg. see \r\nhttps://github.com/apache/spark/blob/master/common/network-common/src/main/java/org/apache/spark/network/util/JavaUtils.java#L149-L173\r\n\r\nAlso scala provides some nicer ways to read the input line-by-line that will simplify this a lot -- `Source.fromInputStream(proc.getInputStream).getLines()`\r\n\r\n",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-03T17:20:22Z",
    "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(jvmVmemTotal: Long,\n+                                             jvmRSSTotal: Long,\n+                                             pythonVmemTotal: Long,\n+                                             pythonRSSTotal: Long,\n+                                             otherVmemTotal: Long,\n+                                             otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems extends Logging {\n+  var procfsDir = \"/proc/\"\n+  val procfsStatFile = \"stat\"\n+  var pageSize = 0\n+  var isAvailable: Boolean = isItProcfsBased\n+  private val pid: Int = computePid()\n+  private val ptree: scala.collection.mutable.Map[ Int, Set[Int]] =\n+    scala.collection.mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal: Long = 0\n+  private var latestJVMRSSTotal: Long = 0\n+  private var latestPythonVmemTotal: Long = 0\n+  private var latestPythonRSSTotal: Long = 0\n+  private var latestOtherVmemTotal: Long = 0\n+  private var latestOtherRSSTotal: Long = 0\n+\n+  computeProcessTree()\n+\n+  private def isItProcfsBased: Boolean = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out: Array[Byte] = Array.fill[Byte](length)(0)\n+      Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+      val pid = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+      return pid;\n+    }\n+    catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return -1\n+      case _ => logDebug(\"Some exception occurred when trying to compute process tree. \" +\n+        \"As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Unit = {\n+    val cmd = Array(\"getconf\", \"PAGESIZE\")\n+    val out: Array[Byte] = Array.fill[Byte](10)(0)\n+    Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+    pageSize = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+  }\n+\n+  private def computeProcessTree(): Unit = {\n+    if (!isAvailable) {\n+      return\n+    }\n+    computePageSize\n+    val queue: Queue[Int] = new Queue[Int]()\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPIds(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree += (p -> c.toSet)\n+      }\n+      else {\n+        ptree += (p -> Set[Int]())\n+      }\n+    }\n+  }\n+\n+  private def getChildPIds(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val input = Runtime.getRuntime.exec(cmd).getInputStream"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "I will think about this. Thank you for bringing it up.",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-03T21:33:55Z",
    "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(jvmVmemTotal: Long,\n+                                             jvmRSSTotal: Long,\n+                                             pythonVmemTotal: Long,\n+                                             pythonRSSTotal: Long,\n+                                             otherVmemTotal: Long,\n+                                             otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems extends Logging {\n+  var procfsDir = \"/proc/\"\n+  val procfsStatFile = \"stat\"\n+  var pageSize = 0\n+  var isAvailable: Boolean = isItProcfsBased\n+  private val pid: Int = computePid()\n+  private val ptree: scala.collection.mutable.Map[ Int, Set[Int]] =\n+    scala.collection.mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal: Long = 0\n+  private var latestJVMRSSTotal: Long = 0\n+  private var latestPythonVmemTotal: Long = 0\n+  private var latestPythonRSSTotal: Long = 0\n+  private var latestOtherVmemTotal: Long = 0\n+  private var latestOtherRSSTotal: Long = 0\n+\n+  computeProcessTree()\n+\n+  private def isItProcfsBased: Boolean = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out: Array[Byte] = Array.fill[Byte](length)(0)\n+      Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+      val pid = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+      return pid;\n+    }\n+    catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return -1\n+      case _ => logDebug(\"Some exception occurred when trying to compute process tree. \" +\n+        \"As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Unit = {\n+    val cmd = Array(\"getconf\", \"PAGESIZE\")\n+    val out: Array[Byte] = Array.fill[Byte](10)(0)\n+    Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+    pageSize = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+  }\n+\n+  private def computeProcessTree(): Unit = {\n+    if (!isAvailable) {\n+      return\n+    }\n+    computePageSize\n+    val queue: Queue[Int] = new Queue[Int]()\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPIds(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree += (p -> c.toSet)\n+      }\n+      else {\n+        ptree += (p -> Set[Int]())\n+      }\n+    }\n+  }\n+\n+  private def getChildPIds(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val input = Runtime.getRuntime.exec(cmd).getInputStream"
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "The helper functions in `Utils` use process builders.",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-03T22:14:27Z",
    "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(jvmVmemTotal: Long,\n+                                             jvmRSSTotal: Long,\n+                                             pythonVmemTotal: Long,\n+                                             pythonRSSTotal: Long,\n+                                             otherVmemTotal: Long,\n+                                             otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems extends Logging {\n+  var procfsDir = \"/proc/\"\n+  val procfsStatFile = \"stat\"\n+  var pageSize = 0\n+  var isAvailable: Boolean = isItProcfsBased\n+  private val pid: Int = computePid()\n+  private val ptree: scala.collection.mutable.Map[ Int, Set[Int]] =\n+    scala.collection.mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal: Long = 0\n+  private var latestJVMRSSTotal: Long = 0\n+  private var latestPythonVmemTotal: Long = 0\n+  private var latestPythonRSSTotal: Long = 0\n+  private var latestOtherVmemTotal: Long = 0\n+  private var latestOtherRSSTotal: Long = 0\n+\n+  computeProcessTree()\n+\n+  private def isItProcfsBased: Boolean = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out: Array[Byte] = Array.fill[Byte](length)(0)\n+      Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+      val pid = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+      return pid;\n+    }\n+    catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return -1\n+      case _ => logDebug(\"Some exception occurred when trying to compute process tree. \" +\n+        \"As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Unit = {\n+    val cmd = Array(\"getconf\", \"PAGESIZE\")\n+    val out: Array[Byte] = Array.fill[Byte](10)(0)\n+    Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+    pageSize = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+  }\n+\n+  private def computeProcessTree(): Unit = {\n+    if (!isAvailable) {\n+      return\n+    }\n+    computePageSize\n+    val queue: Queue[Int] = new Queue[Int]()\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPIds(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree += (p -> c.toSet)\n+      }\n+      else {\n+        ptree += (p -> Set[Int]())\n+      }\n+    }\n+  }\n+\n+  private def getChildPIds(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val input = Runtime.getRuntime.exec(cmd).getInputStream"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "Can you expand on what you found was not correct during tests?  is that covered by the unit tests you've added?\r\n\r\nand super nit: this is more of a regular implementation comment, not a docstring -- so `/*` not `/**`, and also maybe move it inside the method",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-03T17:24:41Z",
    "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(jvmVmemTotal: Long,\n+                                             jvmRSSTotal: Long,\n+                                             pythonVmemTotal: Long,\n+                                             pythonRSSTotal: Long,\n+                                             otherVmemTotal: Long,\n+                                             otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems extends Logging {\n+  var procfsDir = \"/proc/\"\n+  val procfsStatFile = \"stat\"\n+  var pageSize = 0\n+  var isAvailable: Boolean = isItProcfsBased\n+  private val pid: Int = computePid()\n+  private val ptree: scala.collection.mutable.Map[ Int, Set[Int]] =\n+    scala.collection.mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal: Long = 0\n+  private var latestJVMRSSTotal: Long = 0\n+  private var latestPythonVmemTotal: Long = 0\n+  private var latestPythonRSSTotal: Long = 0\n+  private var latestOtherVmemTotal: Long = 0\n+  private var latestOtherRSSTotal: Long = 0\n+\n+  computeProcessTree()\n+\n+  private def isItProcfsBased: Boolean = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out: Array[Byte] = Array.fill[Byte](length)(0)\n+      Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+      val pid = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+      return pid;\n+    }\n+    catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return -1\n+      case _ => logDebug(\"Some exception occurred when trying to compute process tree. \" +\n+        \"As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Unit = {\n+    val cmd = Array(\"getconf\", \"PAGESIZE\")\n+    val out: Array[Byte] = Array.fill[Byte](10)(0)\n+    Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+    pageSize = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+  }\n+\n+  private def computeProcessTree(): Unit = {\n+    if (!isAvailable) {\n+      return\n+    }\n+    computePageSize\n+    val queue: Queue[Int] = new Queue[Int]()\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPIds(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree += (p -> c.toSet)\n+      }\n+      else {\n+        ptree += (p -> Set[Int]())\n+      }\n+    }\n+  }\n+\n+  private def getChildPIds(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val input = Runtime.getRuntime.exec(cmd).getInputStream\n+      val childPidsInByte: mutable.ArrayBuffer[Byte] = new mutable.ArrayBuffer()\n+      var d = input.read()\n+      while (d != -1) {\n+        childPidsInByte.append(d.asInstanceOf[Byte])\n+        d = input.read()\n+      }\n+      input.close()\n+      val childPids = new String(childPidsInByte.toArray, \"UTF-8\").split(\"\\n\")\n+      val childPidsInInt: ArrayBuffer[Int] = new ArrayBuffer[Int]()\n+      for (p <- childPids) {\n+        if (p != \"\") {\n+          childPidsInInt += Integer.parseInt(p)\n+        }\n+      }\n+      childPidsInInt\n+    } catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return new mutable.ArrayBuffer()\n+      case _ => logDebug(\"Some exception occurred when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return new mutable.ArrayBuffer()\n+    }\n+  }\n+\n+  /**\n+   * Hadoop ProcfsBasedProcessTree class used regex and pattern matching to retrive the memory\n+   * info. I tried that but found it not correct during tests, so I used normal string analysis\n+   * instead. The computation of RSS and Vmem are based on proc(5):"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "Yeah, the unit test cover this. I noticed that the it is hard to verify whether the regex match with the proc(5). ",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-03T21:37:57Z",
    "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(jvmVmemTotal: Long,\n+                                             jvmRSSTotal: Long,\n+                                             pythonVmemTotal: Long,\n+                                             pythonRSSTotal: Long,\n+                                             otherVmemTotal: Long,\n+                                             otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems extends Logging {\n+  var procfsDir = \"/proc/\"\n+  val procfsStatFile = \"stat\"\n+  var pageSize = 0\n+  var isAvailable: Boolean = isItProcfsBased\n+  private val pid: Int = computePid()\n+  private val ptree: scala.collection.mutable.Map[ Int, Set[Int]] =\n+    scala.collection.mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal: Long = 0\n+  private var latestJVMRSSTotal: Long = 0\n+  private var latestPythonVmemTotal: Long = 0\n+  private var latestPythonRSSTotal: Long = 0\n+  private var latestOtherVmemTotal: Long = 0\n+  private var latestOtherRSSTotal: Long = 0\n+\n+  computeProcessTree()\n+\n+  private def isItProcfsBased: Boolean = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out: Array[Byte] = Array.fill[Byte](length)(0)\n+      Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+      val pid = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+      return pid;\n+    }\n+    catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return -1\n+      case _ => logDebug(\"Some exception occurred when trying to compute process tree. \" +\n+        \"As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Unit = {\n+    val cmd = Array(\"getconf\", \"PAGESIZE\")\n+    val out: Array[Byte] = Array.fill[Byte](10)(0)\n+    Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+    pageSize = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+  }\n+\n+  private def computeProcessTree(): Unit = {\n+    if (!isAvailable) {\n+      return\n+    }\n+    computePageSize\n+    val queue: Queue[Int] = new Queue[Int]()\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPIds(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree += (p -> c.toSet)\n+      }\n+      else {\n+        ptree += (p -> Set[Int]())\n+      }\n+    }\n+  }\n+\n+  private def getChildPIds(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val input = Runtime.getRuntime.exec(cmd).getInputStream\n+      val childPidsInByte: mutable.ArrayBuffer[Byte] = new mutable.ArrayBuffer()\n+      var d = input.read()\n+      while (d != -1) {\n+        childPidsInByte.append(d.asInstanceOf[Byte])\n+        d = input.read()\n+      }\n+      input.close()\n+      val childPids = new String(childPidsInByte.toArray, \"UTF-8\").split(\"\\n\")\n+      val childPidsInInt: ArrayBuffer[Int] = new ArrayBuffer[Int]()\n+      for (p <- childPids) {\n+        if (p != \"\") {\n+          childPidsInInt += Integer.parseInt(p)\n+        }\n+      }\n+      childPidsInInt\n+    } catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return new mutable.ArrayBuffer()\n+      case _ => logDebug(\"Some exception occurred when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return new mutable.ArrayBuffer()\n+    }\n+  }\n+\n+  /**\n+   * Hadoop ProcfsBasedProcessTree class used regex and pattern matching to retrive the memory\n+   * info. I tried that but found it not correct during tests, so I used normal string analysis\n+   * instead. The computation of RSS and Vmem are based on proc(5):"
  }, {
    "author": {
      "login": "squito"
    },
    "body": "I can't find the old question about what exactly you meant about the \"not correct\" part anymore, but I believe you said that you didn't find any bugs in the hadoop version, just that it was harder to follow.  If so, then the comment here is not correct and should be updated.  I'd just remove the reference to the hadoop version, and just leave the pointer to man page.",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-11-16T20:51:52Z",
    "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(jvmVmemTotal: Long,\n+                                             jvmRSSTotal: Long,\n+                                             pythonVmemTotal: Long,\n+                                             pythonRSSTotal: Long,\n+                                             otherVmemTotal: Long,\n+                                             otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems extends Logging {\n+  var procfsDir = \"/proc/\"\n+  val procfsStatFile = \"stat\"\n+  var pageSize = 0\n+  var isAvailable: Boolean = isItProcfsBased\n+  private val pid: Int = computePid()\n+  private val ptree: scala.collection.mutable.Map[ Int, Set[Int]] =\n+    scala.collection.mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal: Long = 0\n+  private var latestJVMRSSTotal: Long = 0\n+  private var latestPythonVmemTotal: Long = 0\n+  private var latestPythonRSSTotal: Long = 0\n+  private var latestOtherVmemTotal: Long = 0\n+  private var latestOtherRSSTotal: Long = 0\n+\n+  computeProcessTree()\n+\n+  private def isItProcfsBased: Boolean = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out: Array[Byte] = Array.fill[Byte](length)(0)\n+      Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+      val pid = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+      return pid;\n+    }\n+    catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return -1\n+      case _ => logDebug(\"Some exception occurred when trying to compute process tree. \" +\n+        \"As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Unit = {\n+    val cmd = Array(\"getconf\", \"PAGESIZE\")\n+    val out: Array[Byte] = Array.fill[Byte](10)(0)\n+    Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+    pageSize = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+  }\n+\n+  private def computeProcessTree(): Unit = {\n+    if (!isAvailable) {\n+      return\n+    }\n+    computePageSize\n+    val queue: Queue[Int] = new Queue[Int]()\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPIds(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree += (p -> c.toSet)\n+      }\n+      else {\n+        ptree += (p -> Set[Int]())\n+      }\n+    }\n+  }\n+\n+  private def getChildPIds(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val input = Runtime.getRuntime.exec(cmd).getInputStream\n+      val childPidsInByte: mutable.ArrayBuffer[Byte] = new mutable.ArrayBuffer()\n+      var d = input.read()\n+      while (d != -1) {\n+        childPidsInByte.append(d.asInstanceOf[Byte])\n+        d = input.read()\n+      }\n+      input.close()\n+      val childPids = new String(childPidsInByte.toArray, \"UTF-8\").split(\"\\n\")\n+      val childPidsInInt: ArrayBuffer[Int] = new ArrayBuffer[Int]()\n+      for (p <- childPids) {\n+        if (p != \"\") {\n+          childPidsInInt += Integer.parseInt(p)\n+        }\n+      }\n+      childPidsInInt\n+    } catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return new mutable.ArrayBuffer()\n+      case _ => logDebug(\"Some exception occurred when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return new mutable.ArrayBuffer()\n+    }\n+  }\n+\n+  /**\n+   * Hadoop ProcfsBasedProcessTree class used regex and pattern matching to retrive the memory\n+   * info. I tried that but found it not correct during tests, so I used normal string analysis\n+   * instead. The computation of RSS and Vmem are based on proc(5):"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "you could avoid a bit of the duplicated code if you first did\r\n\r\n```scala\r\nvmem = procInfoSplit(22).toLong\r\nrssPages = procInfoSplit(23).toLong)\r\nif (procInfoSplit(1) ...\r\n```",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-03T17:29:26Z",
    "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(jvmVmemTotal: Long,\n+                                             jvmRSSTotal: Long,\n+                                             pythonVmemTotal: Long,\n+                                             pythonRSSTotal: Long,\n+                                             otherVmemTotal: Long,\n+                                             otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems extends Logging {\n+  var procfsDir = \"/proc/\"\n+  val procfsStatFile = \"stat\"\n+  var pageSize = 0\n+  var isAvailable: Boolean = isItProcfsBased\n+  private val pid: Int = computePid()\n+  private val ptree: scala.collection.mutable.Map[ Int, Set[Int]] =\n+    scala.collection.mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal: Long = 0\n+  private var latestJVMRSSTotal: Long = 0\n+  private var latestPythonVmemTotal: Long = 0\n+  private var latestPythonRSSTotal: Long = 0\n+  private var latestOtherVmemTotal: Long = 0\n+  private var latestOtherRSSTotal: Long = 0\n+\n+  computeProcessTree()\n+\n+  private def isItProcfsBased: Boolean = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out: Array[Byte] = Array.fill[Byte](length)(0)\n+      Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+      val pid = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+      return pid;\n+    }\n+    catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return -1\n+      case _ => logDebug(\"Some exception occurred when trying to compute process tree. \" +\n+        \"As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Unit = {\n+    val cmd = Array(\"getconf\", \"PAGESIZE\")\n+    val out: Array[Byte] = Array.fill[Byte](10)(0)\n+    Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+    pageSize = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+  }\n+\n+  private def computeProcessTree(): Unit = {\n+    if (!isAvailable) {\n+      return\n+    }\n+    computePageSize\n+    val queue: Queue[Int] = new Queue[Int]()\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPIds(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree += (p -> c.toSet)\n+      }\n+      else {\n+        ptree += (p -> Set[Int]())\n+      }\n+    }\n+  }\n+\n+  private def getChildPIds(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val input = Runtime.getRuntime.exec(cmd).getInputStream\n+      val childPidsInByte: mutable.ArrayBuffer[Byte] = new mutable.ArrayBuffer()\n+      var d = input.read()\n+      while (d != -1) {\n+        childPidsInByte.append(d.asInstanceOf[Byte])\n+        d = input.read()\n+      }\n+      input.close()\n+      val childPids = new String(childPidsInByte.toArray, \"UTF-8\").split(\"\\n\")\n+      val childPidsInInt: ArrayBuffer[Int] = new ArrayBuffer[Int]()\n+      for (p <- childPids) {\n+        if (p != \"\") {\n+          childPidsInInt += Integer.parseInt(p)\n+        }\n+      }\n+      childPidsInInt\n+    } catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return new mutable.ArrayBuffer()\n+      case _ => logDebug(\"Some exception occurred when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return new mutable.ArrayBuffer()\n+    }\n+  }\n+\n+  /**\n+   * Hadoop ProcfsBasedProcessTree class used regex and pattern matching to retrive the memory\n+   * info. I tried that but found it not correct during tests, so I used normal string analysis\n+   * instead. The computation of RSS and Vmem are based on proc(5):\n+   * http://man7.org/linux/man-pages/man5/proc.5.html\n+   */\n+  def getProcessInfo(pid: Int): Unit = {\n+    try {\n+      val pidDir: File = new File(procfsDir, pid.toString)\n+      val fReader = new InputStreamReader(\n+        new FileInputStream(\n+          new File(pidDir, procfsStatFile)), Charset.forName(\"UTF-8\"))\n+      val in: BufferedReader = new BufferedReader(fReader)\n+      val procInfo = in.readLine\n+      in.close\n+      fReader.close\n+      val procInfoSplit = procInfo.split(\" \")\n+      if ( procInfoSplit != null ) {\n+        if (procInfoSplit(1).toLowerCase.contains(\"java\")) {\n+          latestJVMVmemTotal += procInfoSplit(22).toLong\n+          latestJVMRSSTotal += procInfoSplit(23).toLong"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "why would it ever be null?",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-03T17:29:34Z",
    "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(jvmVmemTotal: Long,\n+                                             jvmRSSTotal: Long,\n+                                             pythonVmemTotal: Long,\n+                                             pythonRSSTotal: Long,\n+                                             otherVmemTotal: Long,\n+                                             otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems extends Logging {\n+  var procfsDir = \"/proc/\"\n+  val procfsStatFile = \"stat\"\n+  var pageSize = 0\n+  var isAvailable: Boolean = isItProcfsBased\n+  private val pid: Int = computePid()\n+  private val ptree: scala.collection.mutable.Map[ Int, Set[Int]] =\n+    scala.collection.mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal: Long = 0\n+  private var latestJVMRSSTotal: Long = 0\n+  private var latestPythonVmemTotal: Long = 0\n+  private var latestPythonRSSTotal: Long = 0\n+  private var latestOtherVmemTotal: Long = 0\n+  private var latestOtherRSSTotal: Long = 0\n+\n+  computeProcessTree()\n+\n+  private def isItProcfsBased: Boolean = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out: Array[Byte] = Array.fill[Byte](length)(0)\n+      Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+      val pid = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+      return pid;\n+    }\n+    catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return -1\n+      case _ => logDebug(\"Some exception occurred when trying to compute process tree. \" +\n+        \"As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Unit = {\n+    val cmd = Array(\"getconf\", \"PAGESIZE\")\n+    val out: Array[Byte] = Array.fill[Byte](10)(0)\n+    Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+    pageSize = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+  }\n+\n+  private def computeProcessTree(): Unit = {\n+    if (!isAvailable) {\n+      return\n+    }\n+    computePageSize\n+    val queue: Queue[Int] = new Queue[Int]()\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPIds(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree += (p -> c.toSet)\n+      }\n+      else {\n+        ptree += (p -> Set[Int]())\n+      }\n+    }\n+  }\n+\n+  private def getChildPIds(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val input = Runtime.getRuntime.exec(cmd).getInputStream\n+      val childPidsInByte: mutable.ArrayBuffer[Byte] = new mutable.ArrayBuffer()\n+      var d = input.read()\n+      while (d != -1) {\n+        childPidsInByte.append(d.asInstanceOf[Byte])\n+        d = input.read()\n+      }\n+      input.close()\n+      val childPids = new String(childPidsInByte.toArray, \"UTF-8\").split(\"\\n\")\n+      val childPidsInInt: ArrayBuffer[Int] = new ArrayBuffer[Int]()\n+      for (p <- childPids) {\n+        if (p != \"\") {\n+          childPidsInInt += Integer.parseInt(p)\n+        }\n+      }\n+      childPidsInInt\n+    } catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return new mutable.ArrayBuffer()\n+      case _ => logDebug(\"Some exception occurred when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return new mutable.ArrayBuffer()\n+    }\n+  }\n+\n+  /**\n+   * Hadoop ProcfsBasedProcessTree class used regex and pattern matching to retrive the memory\n+   * info. I tried that but found it not correct during tests, so I used normal string analysis\n+   * instead. The computation of RSS and Vmem are based on proc(5):\n+   * http://man7.org/linux/man-pages/man5/proc.5.html\n+   */\n+  def getProcessInfo(pid: Int): Unit = {\n+    try {\n+      val pidDir: File = new File(procfsDir, pid.toString)\n+      val fReader = new InputStreamReader(\n+        new FileInputStream(\n+          new File(pidDir, procfsStatFile)), Charset.forName(\"UTF-8\"))\n+      val in: BufferedReader = new BufferedReader(fReader)\n+      val procInfo = in.readLine\n+      in.close\n+      fReader.close\n+      val procInfoSplit = procInfo.split(\" \")\n+      if ( procInfoSplit != null ) {"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "I don't remember whether this really happened or just I checked to be really cautious. I will remove the check if I noticed this is just being super cautious",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-03T20:22:20Z",
    "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(jvmVmemTotal: Long,\n+                                             jvmRSSTotal: Long,\n+                                             pythonVmemTotal: Long,\n+                                             pythonRSSTotal: Long,\n+                                             otherVmemTotal: Long,\n+                                             otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems extends Logging {\n+  var procfsDir = \"/proc/\"\n+  val procfsStatFile = \"stat\"\n+  var pageSize = 0\n+  var isAvailable: Boolean = isItProcfsBased\n+  private val pid: Int = computePid()\n+  private val ptree: scala.collection.mutable.Map[ Int, Set[Int]] =\n+    scala.collection.mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal: Long = 0\n+  private var latestJVMRSSTotal: Long = 0\n+  private var latestPythonVmemTotal: Long = 0\n+  private var latestPythonRSSTotal: Long = 0\n+  private var latestOtherVmemTotal: Long = 0\n+  private var latestOtherRSSTotal: Long = 0\n+\n+  computeProcessTree()\n+\n+  private def isItProcfsBased: Boolean = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out: Array[Byte] = Array.fill[Byte](length)(0)\n+      Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+      val pid = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+      return pid;\n+    }\n+    catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return -1\n+      case _ => logDebug(\"Some exception occurred when trying to compute process tree. \" +\n+        \"As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Unit = {\n+    val cmd = Array(\"getconf\", \"PAGESIZE\")\n+    val out: Array[Byte] = Array.fill[Byte](10)(0)\n+    Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+    pageSize = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+  }\n+\n+  private def computeProcessTree(): Unit = {\n+    if (!isAvailable) {\n+      return\n+    }\n+    computePageSize\n+    val queue: Queue[Int] = new Queue[Int]()\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPIds(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree += (p -> c.toSet)\n+      }\n+      else {\n+        ptree += (p -> Set[Int]())\n+      }\n+    }\n+  }\n+\n+  private def getChildPIds(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val input = Runtime.getRuntime.exec(cmd).getInputStream\n+      val childPidsInByte: mutable.ArrayBuffer[Byte] = new mutable.ArrayBuffer()\n+      var d = input.read()\n+      while (d != -1) {\n+        childPidsInByte.append(d.asInstanceOf[Byte])\n+        d = input.read()\n+      }\n+      input.close()\n+      val childPids = new String(childPidsInByte.toArray, \"UTF-8\").split(\"\\n\")\n+      val childPidsInInt: ArrayBuffer[Int] = new ArrayBuffer[Int]()\n+      for (p <- childPids) {\n+        if (p != \"\") {\n+          childPidsInInt += Integer.parseInt(p)\n+        }\n+      }\n+      childPidsInInt\n+    } catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return new mutable.ArrayBuffer()\n+      case _ => logDebug(\"Some exception occurred when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return new mutable.ArrayBuffer()\n+    }\n+  }\n+\n+  /**\n+   * Hadoop ProcfsBasedProcessTree class used regex and pattern matching to retrive the memory\n+   * info. I tried that but found it not correct during tests, so I used normal string analysis\n+   * instead. The computation of RSS and Vmem are based on proc(5):\n+   * http://man7.org/linux/man-pages/man5/proc.5.html\n+   */\n+  def getProcessInfo(pid: Int): Unit = {\n+    try {\n+      val pidDir: File = new File(procfsDir, pid.toString)\n+      val fReader = new InputStreamReader(\n+        new FileInputStream(\n+          new File(pidDir, procfsStatFile)), Charset.forName(\"UTF-8\"))\n+      val in: BufferedReader = new BufferedReader(fReader)\n+      val procInfo = in.readLine\n+      in.close\n+      fReader.close\n+      val procInfoSplit = procInfo.split(\" \")\n+      if ( procInfoSplit != null ) {"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "logDebug at least?",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-03T17:29:56Z",
    "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(jvmVmemTotal: Long,\n+                                             jvmRSSTotal: Long,\n+                                             pythonVmemTotal: Long,\n+                                             pythonRSSTotal: Long,\n+                                             otherVmemTotal: Long,\n+                                             otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems extends Logging {\n+  var procfsDir = \"/proc/\"\n+  val procfsStatFile = \"stat\"\n+  var pageSize = 0\n+  var isAvailable: Boolean = isItProcfsBased\n+  private val pid: Int = computePid()\n+  private val ptree: scala.collection.mutable.Map[ Int, Set[Int]] =\n+    scala.collection.mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal: Long = 0\n+  private var latestJVMRSSTotal: Long = 0\n+  private var latestPythonVmemTotal: Long = 0\n+  private var latestPythonRSSTotal: Long = 0\n+  private var latestOtherVmemTotal: Long = 0\n+  private var latestOtherRSSTotal: Long = 0\n+\n+  computeProcessTree()\n+\n+  private def isItProcfsBased: Boolean = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out: Array[Byte] = Array.fill[Byte](length)(0)\n+      Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+      val pid = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+      return pid;\n+    }\n+    catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return -1\n+      case _ => logDebug(\"Some exception occurred when trying to compute process tree. \" +\n+        \"As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Unit = {\n+    val cmd = Array(\"getconf\", \"PAGESIZE\")\n+    val out: Array[Byte] = Array.fill[Byte](10)(0)\n+    Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+    pageSize = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+  }\n+\n+  private def computeProcessTree(): Unit = {\n+    if (!isAvailable) {\n+      return\n+    }\n+    computePageSize\n+    val queue: Queue[Int] = new Queue[Int]()\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPIds(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree += (p -> c.toSet)\n+      }\n+      else {\n+        ptree += (p -> Set[Int]())\n+      }\n+    }\n+  }\n+\n+  private def getChildPIds(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val input = Runtime.getRuntime.exec(cmd).getInputStream\n+      val childPidsInByte: mutable.ArrayBuffer[Byte] = new mutable.ArrayBuffer()\n+      var d = input.read()\n+      while (d != -1) {\n+        childPidsInByte.append(d.asInstanceOf[Byte])\n+        d = input.read()\n+      }\n+      input.close()\n+      val childPids = new String(childPidsInByte.toArray, \"UTF-8\").split(\"\\n\")\n+      val childPidsInInt: ArrayBuffer[Int] = new ArrayBuffer[Int]()\n+      for (p <- childPids) {\n+        if (p != \"\") {\n+          childPidsInInt += Integer.parseInt(p)\n+        }\n+      }\n+      childPidsInInt\n+    } catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return new mutable.ArrayBuffer()\n+      case _ => logDebug(\"Some exception occurred when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return new mutable.ArrayBuffer()\n+    }\n+  }\n+\n+  /**\n+   * Hadoop ProcfsBasedProcessTree class used regex and pattern matching to retrive the memory\n+   * info. I tried that but found it not correct during tests, so I used normal string analysis\n+   * instead. The computation of RSS and Vmem are based on proc(5):\n+   * http://man7.org/linux/man-pages/man5/proc.5.html\n+   */\n+  def getProcessInfo(pid: Int): Unit = {\n+    try {\n+      val pidDir: File = new File(procfsDir, pid.toString)\n+      val fReader = new InputStreamReader(\n+        new FileInputStream(\n+          new File(pidDir, procfsStatFile)), Charset.forName(\"UTF-8\"))\n+      val in: BufferedReader = new BufferedReader(fReader)\n+      val procInfo = in.readLine\n+      in.close\n+      fReader.close\n+      val procInfoSplit = procInfo.split(\" \")\n+      if ( procInfoSplit != null ) {\n+        if (procInfoSplit(1).toLowerCase.contains(\"java\")) {\n+          latestJVMVmemTotal += procInfoSplit(22).toLong\n+          latestJVMRSSTotal += procInfoSplit(23).toLong\n+        }\n+        else if (procInfoSplit(1).toLowerCase.contains(\"python\")) {\n+          latestPythonVmemTotal += procInfoSplit(22).toLong\n+          latestPythonRSSTotal += procInfoSplit(23).toLong\n+        }\n+        else {\n+        latestOtherVmemTotal += procInfoSplit(22).toLong\n+        latestOtherRSSTotal += procInfoSplit(23).toLong }\n+      }\n+    } catch {\n+      case f: FileNotFoundException =>"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "see my later comment about another way to expose multiple metrics in one go.  But there certainly isn't any point in storing the metrics twice, once in `allMetrics` and again as individual variables here, sorry I guess I was unclear in my previous comments.",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-03T17:45:50Z",
    "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(jvmVmemTotal: Long,\n+                                             jvmRSSTotal: Long,\n+                                             pythonVmemTotal: Long,\n+                                             pythonRSSTotal: Long,\n+                                             otherVmemTotal: Long,\n+                                             otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems extends Logging {\n+  var procfsDir = \"/proc/\"\n+  val procfsStatFile = \"stat\"\n+  var pageSize = 0\n+  var isAvailable: Boolean = isItProcfsBased\n+  private val pid: Int = computePid()\n+  private val ptree: scala.collection.mutable.Map[ Int, Set[Int]] =\n+    scala.collection.mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal: Long = 0\n+  private var latestJVMRSSTotal: Long = 0\n+  private var latestPythonVmemTotal: Long = 0\n+  private var latestPythonRSSTotal: Long = 0\n+  private var latestOtherVmemTotal: Long = 0\n+  private var latestOtherRSSTotal: Long = 0\n+\n+  computeProcessTree()\n+\n+  private def isItProcfsBased: Boolean = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out: Array[Byte] = Array.fill[Byte](length)(0)\n+      Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+      val pid = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+      return pid;\n+    }\n+    catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return -1\n+      case _ => logDebug(\"Some exception occurred when trying to compute process tree. \" +\n+        \"As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Unit = {\n+    val cmd = Array(\"getconf\", \"PAGESIZE\")\n+    val out: Array[Byte] = Array.fill[Byte](10)(0)\n+    Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+    pageSize = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+  }\n+\n+  private def computeProcessTree(): Unit = {\n+    if (!isAvailable) {\n+      return\n+    }\n+    computePageSize\n+    val queue: Queue[Int] = new Queue[Int]()\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPIds(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree += (p -> c.toSet)\n+      }\n+      else {\n+        ptree += (p -> Set[Int]())\n+      }\n+    }\n+  }\n+\n+  private def getChildPIds(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val input = Runtime.getRuntime.exec(cmd).getInputStream\n+      val childPidsInByte: mutable.ArrayBuffer[Byte] = new mutable.ArrayBuffer()\n+      var d = input.read()\n+      while (d != -1) {\n+        childPidsInByte.append(d.asInstanceOf[Byte])\n+        d = input.read()\n+      }\n+      input.close()\n+      val childPids = new String(childPidsInByte.toArray, \"UTF-8\").split(\"\\n\")\n+      val childPidsInInt: ArrayBuffer[Int] = new ArrayBuffer[Int]()\n+      for (p <- childPids) {\n+        if (p != \"\") {\n+          childPidsInInt += Integer.parseInt(p)\n+        }\n+      }\n+      childPidsInInt\n+    } catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return new mutable.ArrayBuffer()\n+      case _ => logDebug(\"Some exception occurred when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return new mutable.ArrayBuffer()\n+    }\n+  }\n+\n+  /**\n+   * Hadoop ProcfsBasedProcessTree class used regex and pattern matching to retrive the memory\n+   * info. I tried that but found it not correct during tests, so I used normal string analysis\n+   * instead. The computation of RSS and Vmem are based on proc(5):\n+   * http://man7.org/linux/man-pages/man5/proc.5.html\n+   */\n+  def getProcessInfo(pid: Int): Unit = {\n+    try {\n+      val pidDir: File = new File(procfsDir, pid.toString)\n+      val fReader = new InputStreamReader(\n+        new FileInputStream(\n+          new File(pidDir, procfsStatFile)), Charset.forName(\"UTF-8\"))\n+      val in: BufferedReader = new BufferedReader(fReader)\n+      val procInfo = in.readLine\n+      in.close\n+      fReader.close\n+      val procInfoSplit = procInfo.split(\" \")\n+      if ( procInfoSplit != null ) {\n+        if (procInfoSplit(1).toLowerCase.contains(\"java\")) {\n+          latestJVMVmemTotal += procInfoSplit(22).toLong\n+          latestJVMRSSTotal += procInfoSplit(23).toLong\n+        }\n+        else if (procInfoSplit(1).toLowerCase.contains(\"python\")) {\n+          latestPythonVmemTotal += procInfoSplit(22).toLong\n+          latestPythonRSSTotal += procInfoSplit(23).toLong\n+        }\n+        else {\n+        latestOtherVmemTotal += procInfoSplit(22).toLong\n+        latestOtherRSSTotal += procInfoSplit(23).toLong }\n+      }\n+    } catch {\n+      case f: FileNotFoundException =>\n+    }\n+  }\n+\n+  def updateAllMetrics(): Unit = {\n+    allMetrics = computeAllMetrics\n+  }\n+\n+  private def computeAllMetrics(): ProcfsBasedSystemsMetrics = {\n+    if (!isAvailable) {\n+      return ProcfsBasedSystemsMetrics(-1, -1, -1, -1, -1, -1)\n+    }\n+    computeProcessTree\n+    val pids = ptree.keySet\n+    latestJVMRSSTotal = 0"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "the checks for java & python seem error prone (maybe you have some external program called \"pass-data-from-java-to-fizzbuzz-cpp\") ... but I'm also not sure what else you could do here.",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-03T17:48:19Z",
    "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(jvmVmemTotal: Long,\n+                                             jvmRSSTotal: Long,\n+                                             pythonVmemTotal: Long,\n+                                             pythonRSSTotal: Long,\n+                                             otherVmemTotal: Long,\n+                                             otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems extends Logging {\n+  var procfsDir = \"/proc/\"\n+  val procfsStatFile = \"stat\"\n+  var pageSize = 0\n+  var isAvailable: Boolean = isItProcfsBased\n+  private val pid: Int = computePid()\n+  private val ptree: scala.collection.mutable.Map[ Int, Set[Int]] =\n+    scala.collection.mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal: Long = 0\n+  private var latestJVMRSSTotal: Long = 0\n+  private var latestPythonVmemTotal: Long = 0\n+  private var latestPythonRSSTotal: Long = 0\n+  private var latestOtherVmemTotal: Long = 0\n+  private var latestOtherRSSTotal: Long = 0\n+\n+  computeProcessTree()\n+\n+  private def isItProcfsBased: Boolean = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out: Array[Byte] = Array.fill[Byte](length)(0)\n+      Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+      val pid = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+      return pid;\n+    }\n+    catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return -1\n+      case _ => logDebug(\"Some exception occurred when trying to compute process tree. \" +\n+        \"As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Unit = {\n+    val cmd = Array(\"getconf\", \"PAGESIZE\")\n+    val out: Array[Byte] = Array.fill[Byte](10)(0)\n+    Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+    pageSize = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+  }\n+\n+  private def computeProcessTree(): Unit = {\n+    if (!isAvailable) {\n+      return\n+    }\n+    computePageSize\n+    val queue: Queue[Int] = new Queue[Int]()\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPIds(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree += (p -> c.toSet)\n+      }\n+      else {\n+        ptree += (p -> Set[Int]())\n+      }\n+    }\n+  }\n+\n+  private def getChildPIds(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val input = Runtime.getRuntime.exec(cmd).getInputStream\n+      val childPidsInByte: mutable.ArrayBuffer[Byte] = new mutable.ArrayBuffer()\n+      var d = input.read()\n+      while (d != -1) {\n+        childPidsInByte.append(d.asInstanceOf[Byte])\n+        d = input.read()\n+      }\n+      input.close()\n+      val childPids = new String(childPidsInByte.toArray, \"UTF-8\").split(\"\\n\")\n+      val childPidsInInt: ArrayBuffer[Int] = new ArrayBuffer[Int]()\n+      for (p <- childPids) {\n+        if (p != \"\") {\n+          childPidsInInt += Integer.parseInt(p)\n+        }\n+      }\n+      childPidsInInt\n+    } catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return new mutable.ArrayBuffer()\n+      case _ => logDebug(\"Some exception occurred when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return new mutable.ArrayBuffer()\n+    }\n+  }\n+\n+  /**\n+   * Hadoop ProcfsBasedProcessTree class used regex and pattern matching to retrive the memory\n+   * info. I tried that but found it not correct during tests, so I used normal string analysis\n+   * instead. The computation of RSS and Vmem are based on proc(5):\n+   * http://man7.org/linux/man-pages/man5/proc.5.html\n+   */\n+  def getProcessInfo(pid: Int): Unit = {\n+    try {\n+      val pidDir: File = new File(procfsDir, pid.toString)\n+      val fReader = new InputStreamReader(\n+        new FileInputStream(\n+          new File(pidDir, procfsStatFile)), Charset.forName(\"UTF-8\"))\n+      val in: BufferedReader = new BufferedReader(fReader)\n+      val procInfo = in.readLine\n+      in.close\n+      fReader.close\n+      val procInfoSplit = procInfo.split(\" \")\n+      if ( procInfoSplit != null ) {\n+        if (procInfoSplit(1).toLowerCase.contains(\"java\")) {"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "So I'm checking for the child processes of a Spark process. And there you will see either java or pythonX.Y. I don't think that Spark will create a process with custom name ",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-03T21:43:00Z",
    "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(jvmVmemTotal: Long,\n+                                             jvmRSSTotal: Long,\n+                                             pythonVmemTotal: Long,\n+                                             pythonRSSTotal: Long,\n+                                             otherVmemTotal: Long,\n+                                             otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems extends Logging {\n+  var procfsDir = \"/proc/\"\n+  val procfsStatFile = \"stat\"\n+  var pageSize = 0\n+  var isAvailable: Boolean = isItProcfsBased\n+  private val pid: Int = computePid()\n+  private val ptree: scala.collection.mutable.Map[ Int, Set[Int]] =\n+    scala.collection.mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal: Long = 0\n+  private var latestJVMRSSTotal: Long = 0\n+  private var latestPythonVmemTotal: Long = 0\n+  private var latestPythonRSSTotal: Long = 0\n+  private var latestOtherVmemTotal: Long = 0\n+  private var latestOtherRSSTotal: Long = 0\n+\n+  computeProcessTree()\n+\n+  private def isItProcfsBased: Boolean = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out: Array[Byte] = Array.fill[Byte](length)(0)\n+      Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+      val pid = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+      return pid;\n+    }\n+    catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return -1\n+      case _ => logDebug(\"Some exception occurred when trying to compute process tree. \" +\n+        \"As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Unit = {\n+    val cmd = Array(\"getconf\", \"PAGESIZE\")\n+    val out: Array[Byte] = Array.fill[Byte](10)(0)\n+    Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+    pageSize = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+  }\n+\n+  private def computeProcessTree(): Unit = {\n+    if (!isAvailable) {\n+      return\n+    }\n+    computePageSize\n+    val queue: Queue[Int] = new Queue[Int]()\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPIds(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree += (p -> c.toSet)\n+      }\n+      else {\n+        ptree += (p -> Set[Int]())\n+      }\n+    }\n+  }\n+\n+  private def getChildPIds(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val input = Runtime.getRuntime.exec(cmd).getInputStream\n+      val childPidsInByte: mutable.ArrayBuffer[Byte] = new mutable.ArrayBuffer()\n+      var d = input.read()\n+      while (d != -1) {\n+        childPidsInByte.append(d.asInstanceOf[Byte])\n+        d = input.read()\n+      }\n+      input.close()\n+      val childPids = new String(childPidsInByte.toArray, \"UTF-8\").split(\"\\n\")\n+      val childPidsInInt: ArrayBuffer[Int] = new ArrayBuffer[Int]()\n+      for (p <- childPids) {\n+        if (p != \"\") {\n+          childPidsInInt += Integer.parseInt(p)\n+        }\n+      }\n+      childPidsInInt\n+    } catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return new mutable.ArrayBuffer()\n+      case _ => logDebug(\"Some exception occurred when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return new mutable.ArrayBuffer()\n+    }\n+  }\n+\n+  /**\n+   * Hadoop ProcfsBasedProcessTree class used regex and pattern matching to retrive the memory\n+   * info. I tried that but found it not correct during tests, so I used normal string analysis\n+   * instead. The computation of RSS and Vmem are based on proc(5):\n+   * http://man7.org/linux/man-pages/man5/proc.5.html\n+   */\n+  def getProcessInfo(pid: Int): Unit = {\n+    try {\n+      val pidDir: File = new File(procfsDir, pid.toString)\n+      val fReader = new InputStreamReader(\n+        new FileInputStream(\n+          new File(pidDir, procfsStatFile)), Charset.forName(\"UTF-8\"))\n+      val in: BufferedReader = new BufferedReader(fReader)\n+      val procInfo = in.readLine\n+      in.close\n+      fReader.close\n+      val procInfoSplit = procInfo.split(\" \")\n+      if ( procInfoSplit != null ) {\n+        if (procInfoSplit(1).toLowerCase.contains(\"java\")) {"
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "It might not be as uncommon to have arbitrary forked processes from Spark executors as we think. Recent discussions from Spark Summit 2018 to my knowledge propose sending off work to be done by specialized hardware or specialized software. I'm not an expert in these use cases so someone can correct this observation if they like. Mostly just flagging that for completeness.",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-03T22:30:47Z",
    "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(jvmVmemTotal: Long,\n+                                             jvmRSSTotal: Long,\n+                                             pythonVmemTotal: Long,\n+                                             pythonRSSTotal: Long,\n+                                             otherVmemTotal: Long,\n+                                             otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems extends Logging {\n+  var procfsDir = \"/proc/\"\n+  val procfsStatFile = \"stat\"\n+  var pageSize = 0\n+  var isAvailable: Boolean = isItProcfsBased\n+  private val pid: Int = computePid()\n+  private val ptree: scala.collection.mutable.Map[ Int, Set[Int]] =\n+    scala.collection.mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal: Long = 0\n+  private var latestJVMRSSTotal: Long = 0\n+  private var latestPythonVmemTotal: Long = 0\n+  private var latestPythonRSSTotal: Long = 0\n+  private var latestOtherVmemTotal: Long = 0\n+  private var latestOtherRSSTotal: Long = 0\n+\n+  computeProcessTree()\n+\n+  private def isItProcfsBased: Boolean = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out: Array[Byte] = Array.fill[Byte](length)(0)\n+      Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+      val pid = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+      return pid;\n+    }\n+    catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return -1\n+      case _ => logDebug(\"Some exception occurred when trying to compute process tree. \" +\n+        \"As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Unit = {\n+    val cmd = Array(\"getconf\", \"PAGESIZE\")\n+    val out: Array[Byte] = Array.fill[Byte](10)(0)\n+    Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+    pageSize = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+  }\n+\n+  private def computeProcessTree(): Unit = {\n+    if (!isAvailable) {\n+      return\n+    }\n+    computePageSize\n+    val queue: Queue[Int] = new Queue[Int]()\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPIds(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree += (p -> c.toSet)\n+      }\n+      else {\n+        ptree += (p -> Set[Int]())\n+      }\n+    }\n+  }\n+\n+  private def getChildPIds(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val input = Runtime.getRuntime.exec(cmd).getInputStream\n+      val childPidsInByte: mutable.ArrayBuffer[Byte] = new mutable.ArrayBuffer()\n+      var d = input.read()\n+      while (d != -1) {\n+        childPidsInByte.append(d.asInstanceOf[Byte])\n+        d = input.read()\n+      }\n+      input.close()\n+      val childPids = new String(childPidsInByte.toArray, \"UTF-8\").split(\"\\n\")\n+      val childPidsInInt: ArrayBuffer[Int] = new ArrayBuffer[Int]()\n+      for (p <- childPids) {\n+        if (p != \"\") {\n+          childPidsInInt += Integer.parseInt(p)\n+        }\n+      }\n+      childPidsInInt\n+    } catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return new mutable.ArrayBuffer()\n+      case _ => logDebug(\"Some exception occurred when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return new mutable.ArrayBuffer()\n+    }\n+  }\n+\n+  /**\n+   * Hadoop ProcfsBasedProcessTree class used regex and pattern matching to retrive the memory\n+   * info. I tried that but found it not correct during tests, so I used normal string analysis\n+   * instead. The computation of RSS and Vmem are based on proc(5):\n+   * http://man7.org/linux/man-pages/man5/proc.5.html\n+   */\n+  def getProcessInfo(pid: Int): Unit = {\n+    try {\n+      val pidDir: File = new File(procfsDir, pid.toString)\n+      val fReader = new InputStreamReader(\n+        new FileInputStream(\n+          new File(pidDir, procfsStatFile)), Charset.forName(\"UTF-8\"))\n+      val in: BufferedReader = new BufferedReader(fReader)\n+      val procInfo = in.readLine\n+      in.close\n+      fReader.close\n+      val procInfoSplit = procInfo.split(\" \")\n+      if ( procInfoSplit != null ) {\n+        if (procInfoSplit(1).toLowerCase.contains(\"java\")) {"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "mccheah"
    },
    "body": "These don't need type declarations, I think.\r\n\r\nIt might make sense for the constant fields to be separated into a companion `object` but in practice I don't think we've been consistent in our usage of companion objects for constants and one-off computes.",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-03T22:09:56Z",
    "diffHunk": "@@ -0,0 +1,274 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  var pageSize: Long = computePageSize()"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "Thank you very much for the review. Will apply some of your comments soon and others later.",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-03T22:58:50Z",
    "diffHunk": "@@ -0,0 +1,274 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  var pageSize: Long = computePageSize()"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "mccheah"
    },
    "body": "There's some utilities for executing commands in `Utils`. Think `Utils#executeCommand` and `Utils#executeAndGetOutput` are worth looking at.",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-03T22:12:10Z",
    "diffHunk": "@@ -0,0 +1,274 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  var pageSize: Long = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid: Int = computePid()\n+  private val ptree: scala.collection.mutable.Map[ Int, Set[Int]] =\n+    scala.collection.mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal: Long = 0\n+  private var latestJVMRSSTotal: Long = 0\n+  private var latestPythonVmemTotal: Long = 0\n+  private var latestPythonRSSTotal: Long = 0\n+  private var latestOtherVmemTotal: Long = 0\n+  private var latestOtherRSSTotal: Long = 0\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out: Array[Byte] = Array.fill[Byte](length)(0)\n+      Runtime.getRuntime.exec(cmd).getInputStream.read(out)"
  }, {
    "author": {
      "login": "dhruve"
    },
    "body": "Agree with @mccheah  here. We should use the Utils class. Infact I was thinking wouldn't it be better to move the logic to get the childPIDs to the utils class itself.",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-04T16:26:08Z",
    "diffHunk": "@@ -0,0 +1,274 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  var pageSize: Long = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid: Int = computePid()\n+  private val ptree: scala.collection.mutable.Map[ Int, Set[Int]] =\n+    scala.collection.mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal: Long = 0\n+  private var latestJVMRSSTotal: Long = 0\n+  private var latestPythonVmemTotal: Long = 0\n+  private var latestPythonRSSTotal: Long = 0\n+  private var latestOtherVmemTotal: Long = 0\n+  private var latestOtherRSSTotal: Long = 0\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out: Array[Byte] = Array.fill[Byte](length)(0)\n+      Runtime.getRuntime.exec(cmd).getInputStream.read(out)"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "Thank you for the review. I am working on replacing these with what we have in the Utils. I am testing right now and hopefully will update the PR tomorrow or early next week. ",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-04T19:03:13Z",
    "diffHunk": "@@ -0,0 +1,274 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  var pageSize: Long = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid: Int = computePid()\n+  private val ptree: scala.collection.mutable.Map[ Int, Set[Int]] =\n+    scala.collection.mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal: Long = 0\n+  private var latestJVMRSSTotal: Long = 0\n+  private var latestPythonVmemTotal: Long = 0\n+  private var latestPythonRSSTotal: Long = 0\n+  private var latestOtherVmemTotal: Long = 0\n+  private var latestOtherRSSTotal: Long = 0\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out: Array[Byte] = Array.fill[Byte](length)(0)\n+      Runtime.getRuntime.exec(cmd).getInputStream.read(out)"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "done",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-05T22:09:16Z",
    "diffHunk": "@@ -0,0 +1,274 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  var pageSize: Long = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid: Int = computePid()\n+  private val ptree: scala.collection.mutable.Map[ Int, Set[Int]] =\n+    scala.collection.mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal: Long = 0\n+  private var latestJVMRSSTotal: Long = 0\n+  private var latestPythonVmemTotal: Long = 0\n+  private var latestPythonRSSTotal: Long = 0\n+  private var latestOtherVmemTotal: Long = 0\n+  private var latestOtherRSSTotal: Long = 0\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out: Array[Byte] = Array.fill[Byte](length)(0)\n+      Runtime.getRuntime.exec(cmd).getInputStream.read(out)"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "mccheah"
    },
    "body": "Catching `Throwable` is generally scary, can this mask out of memory and errors of that sort? Can we scope down the exception type to handle here?",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-03T22:13:14Z",
    "diffHunk": "@@ -0,0 +1,274 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  var pageSize: Long = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid: Int = computePid()\n+  private val ptree: scala.collection.mutable.Map[ Int, Set[Int]] =\n+    scala.collection.mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal: Long = 0\n+  private var latestJVMRSSTotal: Long = 0\n+  private var latestPythonVmemTotal: Long = 0\n+  private var latestPythonRSSTotal: Long = 0\n+  private var latestOtherVmemTotal: Long = 0\n+  private var latestOtherRSSTotal: Long = 0\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out: Array[Byte] = Array.fill[Byte](length)(0)\n+      Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+      val pid = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+      return pid;\n+    }\n+    catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+      case t: Throwable => logDebug(\"Some exception occurred when trying to\" +"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "yeah, sorry. I didn't want to push this. Just add it to avoid a test failure. will remove it in the next patch ",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-03T22:59:08Z",
    "diffHunk": "@@ -0,0 +1,274 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  var pageSize: Long = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid: Int = computePid()\n+  private val ptree: scala.collection.mutable.Map[ Int, Set[Int]] =\n+    scala.collection.mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal: Long = 0\n+  private var latestJVMRSSTotal: Long = 0\n+  private var latestPythonVmemTotal: Long = 0\n+  private var latestPythonRSSTotal: Long = 0\n+  private var latestOtherVmemTotal: Long = 0\n+  private var latestOtherRSSTotal: Long = 0\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out: Array[Byte] = Array.fill[Byte](length)(0)\n+      Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+      val pid = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+      return pid;\n+    }\n+    catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+      case t: Throwable => logDebug(\"Some exception occurred when trying to\" +"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "mccheah"
    },
    "body": "Again, everywhere it should be possible to use the `Utils` functions.",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-03T22:13:38Z",
    "diffHunk": "@@ -0,0 +1,274 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  var pageSize: Long = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid: Int = computePid()\n+  private val ptree: scala.collection.mutable.Map[ Int, Set[Int]] =\n+    scala.collection.mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal: Long = 0\n+  private var latestJVMRSSTotal: Long = 0\n+  private var latestPythonVmemTotal: Long = 0\n+  private var latestPythonRSSTotal: Long = 0\n+  private var latestOtherVmemTotal: Long = 0\n+  private var latestOtherRSSTotal: Long = 0\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out: Array[Byte] = Array.fill[Byte](length)(0)\n+      Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+      val pid = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+      return pid;\n+    }\n+    catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+      case t: Throwable => logDebug(\"Some exception occurred when trying to\" +\n+        \" compute process tree. As a result reporting of ProcessTree metrics is stopped\", t)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return 0;\n+    }\n+    val cmd = Array(\"getconf\", \"PAGESIZE\")\n+    val out: Array[Byte] = Array.fill[Byte](10)(0)\n+    Runtime.getRuntime.exec(cmd).getInputStream.read(out)"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "mccheah"
    },
    "body": "Wrap all of these in `Utils#tryWithResource` calls.",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-03T22:16:06Z",
    "diffHunk": "@@ -0,0 +1,274 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  var pageSize: Long = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid: Int = computePid()\n+  private val ptree: scala.collection.mutable.Map[ Int, Set[Int]] =\n+    scala.collection.mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal: Long = 0\n+  private var latestJVMRSSTotal: Long = 0\n+  private var latestPythonVmemTotal: Long = 0\n+  private var latestPythonRSSTotal: Long = 0\n+  private var latestOtherVmemTotal: Long = 0\n+  private var latestOtherRSSTotal: Long = 0\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out: Array[Byte] = Array.fill[Byte](length)(0)\n+      Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+      val pid = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+      return pid;\n+    }\n+    catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+      case t: Throwable => logDebug(\"Some exception occurred when trying to\" +\n+        \" compute process tree. As a result reporting of ProcessTree metrics is stopped\", t)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return 0;\n+    }\n+    val cmd = Array(\"getconf\", \"PAGESIZE\")\n+    val out: Array[Byte] = Array.fill[Byte](10)(0)\n+    Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+    return Integer.parseInt(new String(out, \"UTF-8\").trim)\n+  }\n+\n+  private def computeProcessTree(): Unit = {\n+    if (!isAvailable) {\n+      return\n+    }\n+    val queue: Queue[Int] = new Queue[Int]()\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPids(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree += (p -> c.toSet)\n+      }\n+      else {\n+        ptree += (p -> Set[Int]())\n+      }\n+    }\n+  }\n+\n+  private def getChildPids(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val input = Runtime.getRuntime.exec(cmd).getInputStream\n+      val childPidsInByte: mutable.ArrayBuffer[Byte] = new mutable.ArrayBuffer()\n+      var d = input.read()\n+      while (d != -1) {\n+        childPidsInByte.append(d.asInstanceOf[Byte])\n+        d = input.read()\n+      }\n+      input.close()\n+      val childPids = new String(childPidsInByte.toArray, \"UTF-8\").split(\"\\n\")\n+      val childPidsInInt: ArrayBuffer[Int] = new ArrayBuffer[Int]()\n+      for (p <- childPids) {\n+        if (p != \"\") {\n+          childPidsInInt += Integer.parseInt(p)\n+        }\n+      }\n+      childPidsInInt\n+    } catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return new mutable.ArrayBuffer()\n+      case t: Throwable => logDebug(\"Some exception occurred when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", t)\n+        isAvailable = false\n+        return new mutable.ArrayBuffer()\n+    }\n+  }\n+\n+  def getProcessInfo(pid: Int): Unit = {\n+    /*\n+   * Hadoop ProcfsBasedProcessTree class used regex and pattern matching to retrive the memory\n+   * info. I tried that but found it not correct during tests, so I used normal string analysis\n+   * instead. The computation of RSS and Vmem are based on proc(5):\n+   * http://man7.org/linux/man-pages/man5/proc.5.html\n+   */\n+    try {\n+      val pidDir: File = new File(procfsDir, pid.toString)\n+      val fReader = new InputStreamReader("
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "mccheah"
    },
    "body": "No need to declare type. Can we check everywhere in the patch and verify we're declaring types as infrequently as possible?",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-03T22:25:19Z",
    "diffHunk": "@@ -0,0 +1,274 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  var pageSize: Long = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid: Int = computePid()\n+  private val ptree: scala.collection.mutable.Map[ Int, Set[Int]] =\n+    scala.collection.mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal: Long = 0\n+  private var latestJVMRSSTotal: Long = 0\n+  private var latestPythonVmemTotal: Long = 0\n+  private var latestPythonRSSTotal: Long = 0\n+  private var latestOtherVmemTotal: Long = 0\n+  private var latestOtherRSSTotal: Long = 0\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out: Array[Byte] = Array.fill[Byte](length)(0)\n+      Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+      val pid = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+      return pid;\n+    }\n+    catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+      case t: Throwable => logDebug(\"Some exception occurred when trying to\" +\n+        \" compute process tree. As a result reporting of ProcessTree metrics is stopped\", t)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return 0;\n+    }\n+    val cmd = Array(\"getconf\", \"PAGESIZE\")\n+    val out: Array[Byte] = Array.fill[Byte](10)(0)\n+    Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+    return Integer.parseInt(new String(out, \"UTF-8\").trim)\n+  }\n+\n+  private def computeProcessTree(): Unit = {\n+    if (!isAvailable) {\n+      return\n+    }\n+    val queue: Queue[Int] = new Queue[Int]()"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "done",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-04T03:16:20Z",
    "diffHunk": "@@ -0,0 +1,274 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  var pageSize: Long = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid: Int = computePid()\n+  private val ptree: scala.collection.mutable.Map[ Int, Set[Int]] =\n+    scala.collection.mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal: Long = 0\n+  private var latestJVMRSSTotal: Long = 0\n+  private var latestPythonVmemTotal: Long = 0\n+  private var latestPythonRSSTotal: Long = 0\n+  private var latestOtherVmemTotal: Long = 0\n+  private var latestOtherRSSTotal: Long = 0\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out: Array[Byte] = Array.fill[Byte](length)(0)\n+      Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+      val pid = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+      return pid;\n+    }\n+    catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+      case t: Throwable => logDebug(\"Some exception occurred when trying to\" +\n+        \" compute process tree. As a result reporting of ProcessTree metrics is stopped\", t)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return 0;\n+    }\n+    val cmd = Array(\"getconf\", \"PAGESIZE\")\n+    val out: Array[Byte] = Array.fill[Byte](10)(0)\n+    Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+    return Integer.parseInt(new String(out, \"UTF-8\").trim)\n+  }\n+\n+  private def computeProcessTree(): Unit = {\n+    if (!isAvailable) {\n+      return\n+    }\n+    val queue: Queue[Int] = new Queue[Int]()"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "mccheah"
    },
    "body": "Minor detail, but `mutable.Buffer.empty` here and below. I think in general, around this file we can replace `new X` with `X.empty[<type>]` most of the time. Though I don't think we've been consistent with that everywhere.",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-03T22:27:17Z",
    "diffHunk": "@@ -0,0 +1,274 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  var pageSize: Long = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid: Int = computePid()\n+  private val ptree: scala.collection.mutable.Map[ Int, Set[Int]] =\n+    scala.collection.mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal: Long = 0\n+  private var latestJVMRSSTotal: Long = 0\n+  private var latestPythonVmemTotal: Long = 0\n+  private var latestPythonRSSTotal: Long = 0\n+  private var latestOtherVmemTotal: Long = 0\n+  private var latestOtherRSSTotal: Long = 0\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out: Array[Byte] = Array.fill[Byte](length)(0)\n+      Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+      val pid = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+      return pid;\n+    }\n+    catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+      case t: Throwable => logDebug(\"Some exception occurred when trying to\" +\n+        \" compute process tree. As a result reporting of ProcessTree metrics is stopped\", t)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return 0;\n+    }\n+    val cmd = Array(\"getconf\", \"PAGESIZE\")\n+    val out: Array[Byte] = Array.fill[Byte](10)(0)\n+    Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+    return Integer.parseInt(new String(out, \"UTF-8\").trim)\n+  }\n+\n+  private def computeProcessTree(): Unit = {\n+    if (!isAvailable) {\n+      return\n+    }\n+    val queue: Queue[Int] = new Queue[Int]()\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPids(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree += (p -> c.toSet)\n+      }\n+      else {\n+        ptree += (p -> Set[Int]())\n+      }\n+    }\n+  }\n+\n+  private def getChildPids(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val input = Runtime.getRuntime.exec(cmd).getInputStream\n+      val childPidsInByte: mutable.ArrayBuffer[Byte] = new mutable.ArrayBuffer()\n+      var d = input.read()\n+      while (d != -1) {\n+        childPidsInByte.append(d.asInstanceOf[Byte])\n+        d = input.read()\n+      }\n+      input.close()\n+      val childPids = new String(childPidsInByte.toArray, \"UTF-8\").split(\"\\n\")\n+      val childPidsInInt: ArrayBuffer[Int] = new ArrayBuffer[Int]()\n+      for (p <- childPids) {\n+        if (p != \"\") {\n+          childPidsInInt += Integer.parseInt(p)\n+        }\n+      }\n+      childPidsInInt\n+    } catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return new mutable.ArrayBuffer()"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "done",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-04T03:16:28Z",
    "diffHunk": "@@ -0,0 +1,274 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  var pageSize: Long = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid: Int = computePid()\n+  private val ptree: scala.collection.mutable.Map[ Int, Set[Int]] =\n+    scala.collection.mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal: Long = 0\n+  private var latestJVMRSSTotal: Long = 0\n+  private var latestPythonVmemTotal: Long = 0\n+  private var latestPythonRSSTotal: Long = 0\n+  private var latestOtherVmemTotal: Long = 0\n+  private var latestOtherRSSTotal: Long = 0\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out: Array[Byte] = Array.fill[Byte](length)(0)\n+      Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+      val pid = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+      return pid;\n+    }\n+    catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+      case t: Throwable => logDebug(\"Some exception occurred when trying to\" +\n+        \" compute process tree. As a result reporting of ProcessTree metrics is stopped\", t)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return 0;\n+    }\n+    val cmd = Array(\"getconf\", \"PAGESIZE\")\n+    val out: Array[Byte] = Array.fill[Byte](10)(0)\n+    Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+    return Integer.parseInt(new String(out, \"UTF-8\").trim)\n+  }\n+\n+  private def computeProcessTree(): Unit = {\n+    if (!isAvailable) {\n+      return\n+    }\n+    val queue: Queue[Int] = new Queue[Int]()\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPids(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree += (p -> c.toSet)\n+      }\n+      else {\n+        ptree += (p -> Set[Int]())\n+      }\n+    }\n+  }\n+\n+  private def getChildPids(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val input = Runtime.getRuntime.exec(cmd).getInputStream\n+      val childPidsInByte: mutable.ArrayBuffer[Byte] = new mutable.ArrayBuffer()\n+      var d = input.read()\n+      while (d != -1) {\n+        childPidsInByte.append(d.asInstanceOf[Byte])\n+        d = input.read()\n+      }\n+      input.close()\n+      val childPids = new String(childPidsInByte.toArray, \"UTF-8\").split(\"\\n\")\n+      val childPidsInInt: ArrayBuffer[Int] = new ArrayBuffer[Int]()\n+      for (p <- childPids) {\n+        if (p != \"\") {\n+          childPidsInInt += Integer.parseInt(p)\n+        }\n+      }\n+      childPidsInInt\n+    } catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return new mutable.ArrayBuffer()"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "dhruve"
    },
    "body": "This can be converted to a lazy val. I can't think of a use case where this might change value over a period of time.",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-04T16:25:06Z",
    "diffHunk": "@@ -0,0 +1,266 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+  private val ptree = mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal = 0L\n+  private var latestJVMRSSTotal = 0L\n+  private var latestPythonVmemTotal = 0L\n+  private var latestPythonRSSTotal = 0L\n+  private var latestOtherVmemTotal = 0L\n+  private var latestOtherRSSTotal = 0L\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "I will do some more testing and will change this if the tests passes. ",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-05T22:08:59Z",
    "diffHunk": "@@ -0,0 +1,266 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+  private val ptree = mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal = 0L\n+  private var latestJVMRSSTotal = 0L\n+  private var latestPythonVmemTotal = 0L\n+  private var latestPythonRSSTotal = 0L\n+  private var latestOtherVmemTotal = 0L\n+  private var latestOtherRSSTotal = 0L\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {"
  }, {
    "author": {
      "login": "squito"
    },
    "body": "+1 on using a `val`",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-19T02:24:37Z",
    "diffHunk": "@@ -0,0 +1,266 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+  private val ptree = mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal = 0L\n+  private var latestJVMRSSTotal = 0L\n+  private var latestPythonVmemTotal = 0L\n+  private var latestPythonRSSTotal = 0L\n+  private var latestOtherVmemTotal = 0L\n+  private var latestOtherRSSTotal = 0L\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "I had a test case that this can change depending on the health of the node. I think it shouldn't be a val to be cautious.",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-19T03:07:06Z",
    "diffHunk": "@@ -0,0 +1,266 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+  private val ptree = mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal = 0L\n+  private var latestJVMRSSTotal = 0L\n+  private var latestPythonVmemTotal = 0L\n+  private var latestPythonRSSTotal = 0L\n+  private var latestOtherVmemTotal = 0L\n+  private var latestOtherRSSTotal = 0L\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {"
  }, {
    "author": {
      "login": "squito"
    },
    "body": "I don't understand what you mean.  Yes, there may be a problem later reading from this file, we have to be defensive about that, but you have to do that anyway in the call.  I think of this specific check as just a first cut -- is procfs available at all?  And that should not be changing at runtime.",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-11-08T15:11:38Z",
    "diffHunk": "@@ -0,0 +1,266 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+  private val ptree = mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal = 0L\n+  private var latestJVMRSSTotal = 0L\n+  private var latestPythonVmemTotal = 0L\n+  private var latestPythonRSSTotal = 0L\n+  private var latestOtherVmemTotal = 0L\n+  private var latestOtherRSSTotal = 0L\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "I think I misunderstood the comment. I was thinking that this is referring to\" isAvailable\". I will change isProcfsAvailable to lazy val. Sorry about my confusion.",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-11-13T22:53:07Z",
    "diffHunk": "@@ -0,0 +1,266 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+  private val ptree = mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal = 0L\n+  private var latestJVMRSSTotal = 0L\n+  private var latestPythonVmemTotal = 0L\n+  private var latestPythonRSSTotal = 0L\n+  private var latestOtherVmemTotal = 0L\n+  private var latestOtherRSSTotal = 0L\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "dhruve"
    },
    "body": "We can use a consistent debugging convention. You can replace `log.debug` with `logDebug`",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-04T16:27:06Z",
    "diffHunk": "@@ -0,0 +1,266 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+  private val ptree = mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal = 0L\n+  private var latestJVMRSSTotal = 0L\n+  private var latestPythonVmemTotal = 0L\n+  private var latestPythonRSSTotal = 0L\n+  private var latestOtherVmemTotal = 0L\n+  private var latestOtherRSSTotal = 0L\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out = Array.fill[Byte](length)(0)\n+      Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+      val pid = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+      return pid;\n+    }\n+    catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return 0;\n+    }\n+    val cmd = Array(\"getconf\", \"PAGESIZE\")\n+    val out = Array.fill[Byte](10)(0)\n+    Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+    return Integer.parseInt(new String(out, \"UTF-8\").trim)\n+  }\n+\n+  private def computeProcessTree(): Unit = {\n+    if (!isAvailable) {\n+      return\n+    }\n+    val queue = mutable.Queue.empty[Int]\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPids(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree += (p -> c.toSet)\n+      }\n+      else {\n+        ptree += (p -> Set[Int]())\n+      }\n+    }\n+  }\n+\n+  private def getChildPids(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val input = Runtime.getRuntime.exec(cmd).getInputStream\n+      val childPidsInByte = mutable.ArrayBuffer.empty[Byte]\n+      var d = input.read()\n+      while (d != -1) {\n+        childPidsInByte.append(d.asInstanceOf[Byte])\n+        d = input.read()\n+      }\n+      input.close()\n+      val childPids = new String(childPidsInByte.toArray, \"UTF-8\").split(\"\\n\")\n+      val childPidsInInt = mutable.ArrayBuffer.empty[Int]\n+      for (p <- childPids) {\n+        if (p != \"\") {\n+          childPidsInInt += Integer.parseInt(p)\n+        }\n+      }\n+      childPidsInInt\n+    } catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return mutable.ArrayBuffer.empty[Int]\n+    }\n+  }\n+\n+  def getProcessInfo(pid: Int): Unit = {\n+    /*\n+   * Hadoop ProcfsBasedProcessTree class used regex and pattern matching to retrive the memory\n+   * info. I tried that but found it not correct during tests, so I used normal string analysis\n+   * instead. The computation of RSS and Vmem are based on proc(5):\n+   * http://man7.org/linux/man-pages/man5/proc.5.html\n+   */\n+    try {\n+      val pidDir = new File(procfsDir, pid.toString)\n+      val fReader = new InputStreamReader(\n+        new FileInputStream(\n+          new File(pidDir, procfsStatFile)), Charset.forName(\"UTF-8\"))\n+      val in = new BufferedReader(fReader)\n+      val procInfo = in.readLine\n+      in.close\n+      fReader.close\n+      val procInfoSplit = procInfo.split(\" \")\n+      if ( procInfoSplit != null ) {\n+        val vmem = procInfoSplit(22).toLong\n+        val rssPages = procInfoSplit(23).toLong\n+        if (procInfoSplit(1).toLowerCase(Locale.US).contains(\"java\")) {\n+          latestJVMVmemTotal += vmem\n+          latestJVMRSSTotal += rssPages\n+        }\n+        else if (procInfoSplit(1).toLowerCase(Locale.US).contains(\"python\")) {\n+          latestPythonVmemTotal += vmem\n+          latestPythonRSSTotal += rssPages\n+        }\n+        else {\n+        latestOtherVmemTotal += vmem\n+        latestOtherRSSTotal += rssPages }\n+      }\n+    } catch {\n+      case f: FileNotFoundException => log.debug(\"There was a problem with reading\" +"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "dhruve"
    },
    "body": "These don't seem to be needed as far as we have ProcfsBasedSystemsMetrics. As others have pointed out and it makes more sense to have a Map of named metrics.",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-04T18:25:05Z",
    "diffHunk": "@@ -0,0 +1,266 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+  private val ptree = mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal = 0L"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "Here I was trying to use existing ExecutorMetric type and also making code more clear. I will update this based on the conversation about changing that API to be able to get a set of metrics at once.",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-04T19:05:36Z",
    "diffHunk": "@@ -0,0 +1,266 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+  private val ptree = mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal = 0L"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "dhruve"
    },
    "body": "getProcessInfo is not returning anything here. What you could do is to set it to perhaps setProcessMetrics and pass in a temporary ProcfsBasedSystemsMetrics. And after setting the metrics for all the childPIDs replace/update the metrics instance.\r\n\r\nMore like:\r\n``` scala\r\n\r\ndef setProcessMetrics(pid: Int, metrics: ProcfsBasedSystemsMetrics) {\r\n....\r\n    if (java) {\r\n       metrics.memUsage(rssMemory) += ....\r\n    } else if (python) {\r\n      metrics.memUsage(pythonRssMemory) += ....\r\n    }\r\n\r\n}\r\n```",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-04T18:32:58Z",
    "diffHunk": "@@ -0,0 +1,266 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+  private val ptree = mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal = 0L\n+  private var latestJVMRSSTotal = 0L\n+  private var latestPythonVmemTotal = 0L\n+  private var latestPythonRSSTotal = 0L\n+  private var latestOtherVmemTotal = 0L\n+  private var latestOtherRSSTotal = 0L\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out = Array.fill[Byte](length)(0)\n+      Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+      val pid = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+      return pid;\n+    }\n+    catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return 0;\n+    }\n+    val cmd = Array(\"getconf\", \"PAGESIZE\")\n+    val out = Array.fill[Byte](10)(0)\n+    Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+    return Integer.parseInt(new String(out, \"UTF-8\").trim)\n+  }\n+\n+  private def computeProcessTree(): Unit = {\n+    if (!isAvailable) {\n+      return\n+    }\n+    val queue = mutable.Queue.empty[Int]\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPids(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree += (p -> c.toSet)\n+      }\n+      else {\n+        ptree += (p -> Set[Int]())\n+      }\n+    }\n+  }\n+\n+  private def getChildPids(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val input = Runtime.getRuntime.exec(cmd).getInputStream\n+      val childPidsInByte = mutable.ArrayBuffer.empty[Byte]\n+      var d = input.read()\n+      while (d != -1) {\n+        childPidsInByte.append(d.asInstanceOf[Byte])\n+        d = input.read()\n+      }\n+      input.close()\n+      val childPids = new String(childPidsInByte.toArray, \"UTF-8\").split(\"\\n\")\n+      val childPidsInInt = mutable.ArrayBuffer.empty[Int]\n+      for (p <- childPids) {\n+        if (p != \"\") {\n+          childPidsInInt += Integer.parseInt(p)\n+        }\n+      }\n+      childPidsInInt\n+    } catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return mutable.ArrayBuffer.empty[Int]\n+    }\n+  }\n+\n+  def getProcessInfo(pid: Int): Unit = {"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "yeah, I may change this either in your suggested way or some other way after figured out the changes to ExecutorMetrics ",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-04T19:06:59Z",
    "diffHunk": "@@ -0,0 +1,266 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+  private val ptree = mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal = 0L\n+  private var latestJVMRSSTotal = 0L\n+  private var latestPythonVmemTotal = 0L\n+  private var latestPythonRSSTotal = 0L\n+  private var latestOtherVmemTotal = 0L\n+  private var latestOtherRSSTotal = 0L\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out = Array.fill[Byte](length)(0)\n+      Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+      val pid = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+      return pid;\n+    }\n+    catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return 0;\n+    }\n+    val cmd = Array(\"getconf\", \"PAGESIZE\")\n+    val out = Array.fill[Byte](10)(0)\n+    Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+    return Integer.parseInt(new String(out, \"UTF-8\").trim)\n+  }\n+\n+  private def computeProcessTree(): Unit = {\n+    if (!isAvailable) {\n+      return\n+    }\n+    val queue = mutable.Queue.empty[Int]\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPids(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree += (p -> c.toSet)\n+      }\n+      else {\n+        ptree += (p -> Set[Int]())\n+      }\n+    }\n+  }\n+\n+  private def getChildPids(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val input = Runtime.getRuntime.exec(cmd).getInputStream\n+      val childPidsInByte = mutable.ArrayBuffer.empty[Byte]\n+      var d = input.read()\n+      while (d != -1) {\n+        childPidsInByte.append(d.asInstanceOf[Byte])\n+        d = input.read()\n+      }\n+      input.close()\n+      val childPids = new String(childPidsInByte.toArray, \"UTF-8\").split(\"\\n\")\n+      val childPidsInInt = mutable.ArrayBuffer.empty[Int]\n+      for (p <- childPids) {\n+        if (p != \"\") {\n+          childPidsInInt += Integer.parseInt(p)\n+        }\n+      }\n+      childPidsInInt\n+    } catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return mutable.ArrayBuffer.empty[Int]\n+    }\n+  }\n+\n+  def getProcessInfo(pid: Int): Unit = {"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "dhruve"
    },
    "body": "Setting a 0 for all of them should be fine here and will also remove the need to check if a proc based file system is available.",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-04T18:38:42Z",
    "diffHunk": "@@ -0,0 +1,266 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+  private val ptree = mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal = 0L\n+  private var latestJVMRSSTotal = 0L\n+  private var latestPythonVmemTotal = 0L\n+  private var latestPythonRSSTotal = 0L\n+  private var latestOtherVmemTotal = 0L\n+  private var latestOtherRSSTotal = 0L\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out = Array.fill[Byte](length)(0)\n+      Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+      val pid = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+      return pid;\n+    }\n+    catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return 0;\n+    }\n+    val cmd = Array(\"getconf\", \"PAGESIZE\")\n+    val out = Array.fill[Byte](10)(0)\n+    Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+    return Integer.parseInt(new String(out, \"UTF-8\").trim)\n+  }\n+\n+  private def computeProcessTree(): Unit = {\n+    if (!isAvailable) {\n+      return\n+    }\n+    val queue = mutable.Queue.empty[Int]\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPids(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree += (p -> c.toSet)\n+      }\n+      else {\n+        ptree += (p -> Set[Int]())\n+      }\n+    }\n+  }\n+\n+  private def getChildPids(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val input = Runtime.getRuntime.exec(cmd).getInputStream\n+      val childPidsInByte = mutable.ArrayBuffer.empty[Byte]\n+      var d = input.read()\n+      while (d != -1) {\n+        childPidsInByte.append(d.asInstanceOf[Byte])\n+        d = input.read()\n+      }\n+      input.close()\n+      val childPids = new String(childPidsInByte.toArray, \"UTF-8\").split(\"\\n\")\n+      val childPidsInInt = mutable.ArrayBuffer.empty[Int]\n+      for (p <- childPids) {\n+        if (p != \"\") {\n+          childPidsInInt += Integer.parseInt(p)\n+        }\n+      }\n+      childPidsInInt\n+    } catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return mutable.ArrayBuffer.empty[Int]\n+    }\n+  }\n+\n+  def getProcessInfo(pid: Int): Unit = {\n+    /*\n+   * Hadoop ProcfsBasedProcessTree class used regex and pattern matching to retrive the memory\n+   * info. I tried that but found it not correct during tests, so I used normal string analysis\n+   * instead. The computation of RSS and Vmem are based on proc(5):\n+   * http://man7.org/linux/man-pages/man5/proc.5.html\n+   */\n+    try {\n+      val pidDir = new File(procfsDir, pid.toString)\n+      val fReader = new InputStreamReader(\n+        new FileInputStream(\n+          new File(pidDir, procfsStatFile)), Charset.forName(\"UTF-8\"))\n+      val in = new BufferedReader(fReader)\n+      val procInfo = in.readLine\n+      in.close\n+      fReader.close\n+      val procInfoSplit = procInfo.split(\" \")\n+      if ( procInfoSplit != null ) {\n+        val vmem = procInfoSplit(22).toLong\n+        val rssPages = procInfoSplit(23).toLong\n+        if (procInfoSplit(1).toLowerCase(Locale.US).contains(\"java\")) {\n+          latestJVMVmemTotal += vmem\n+          latestJVMRSSTotal += rssPages\n+        }\n+        else if (procInfoSplit(1).toLowerCase(Locale.US).contains(\"python\")) {\n+          latestPythonVmemTotal += vmem\n+          latestPythonRSSTotal += rssPages\n+        }\n+        else {\n+        latestOtherVmemTotal += vmem\n+        latestOtherRSSTotal += rssPages }\n+      }\n+    } catch {\n+      case f: FileNotFoundException => log.debug(\"There was a problem with reading\" +\n+        \" the stat file of the process\", f)\n+    }\n+  }\n+\n+  def updateAllMetrics(): Unit = {\n+    allMetrics = computeAllMetrics\n+  }\n+\n+  private def computeAllMetrics(): ProcfsBasedSystemsMetrics = {\n+    if (!isAvailable) {\n+      return ProcfsBasedSystemsMetrics(-1, -1, -1, -1, -1, -1)"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "dhruve"
    },
    "body": "These checks would be redundant if you do not have a proc based FS and you return a 0 instead of -1.",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-04T18:39:14Z",
    "diffHunk": "@@ -0,0 +1,266 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+  private val ptree = mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal = 0L\n+  private var latestJVMRSSTotal = 0L\n+  private var latestPythonVmemTotal = 0L\n+  private var latestPythonRSSTotal = 0L\n+  private var latestOtherVmemTotal = 0L\n+  private var latestOtherRSSTotal = 0L\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out = Array.fill[Byte](length)(0)\n+      Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+      val pid = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+      return pid;\n+    }\n+    catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return 0;\n+    }\n+    val cmd = Array(\"getconf\", \"PAGESIZE\")\n+    val out = Array.fill[Byte](10)(0)\n+    Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+    return Integer.parseInt(new String(out, \"UTF-8\").trim)\n+  }\n+\n+  private def computeProcessTree(): Unit = {\n+    if (!isAvailable) {\n+      return\n+    }\n+    val queue = mutable.Queue.empty[Int]\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPids(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree += (p -> c.toSet)\n+      }\n+      else {\n+        ptree += (p -> Set[Int]())\n+      }\n+    }\n+  }\n+\n+  private def getChildPids(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val input = Runtime.getRuntime.exec(cmd).getInputStream\n+      val childPidsInByte = mutable.ArrayBuffer.empty[Byte]\n+      var d = input.read()\n+      while (d != -1) {\n+        childPidsInByte.append(d.asInstanceOf[Byte])\n+        d = input.read()\n+      }\n+      input.close()\n+      val childPids = new String(childPidsInByte.toArray, \"UTF-8\").split(\"\\n\")\n+      val childPidsInInt = mutable.ArrayBuffer.empty[Int]\n+      for (p <- childPids) {\n+        if (p != \"\") {\n+          childPidsInInt += Integer.parseInt(p)\n+        }\n+      }\n+      childPidsInInt\n+    } catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return mutable.ArrayBuffer.empty[Int]\n+    }\n+  }\n+\n+  def getProcessInfo(pid: Int): Unit = {\n+    /*\n+   * Hadoop ProcfsBasedProcessTree class used regex and pattern matching to retrive the memory\n+   * info. I tried that but found it not correct during tests, so I used normal string analysis\n+   * instead. The computation of RSS and Vmem are based on proc(5):\n+   * http://man7.org/linux/man-pages/man5/proc.5.html\n+   */\n+    try {\n+      val pidDir = new File(procfsDir, pid.toString)\n+      val fReader = new InputStreamReader(\n+        new FileInputStream(\n+          new File(pidDir, procfsStatFile)), Charset.forName(\"UTF-8\"))\n+      val in = new BufferedReader(fReader)\n+      val procInfo = in.readLine\n+      in.close\n+      fReader.close\n+      val procInfoSplit = procInfo.split(\" \")\n+      if ( procInfoSplit != null ) {\n+        val vmem = procInfoSplit(22).toLong\n+        val rssPages = procInfoSplit(23).toLong\n+        if (procInfoSplit(1).toLowerCase(Locale.US).contains(\"java\")) {\n+          latestJVMVmemTotal += vmem\n+          latestJVMRSSTotal += rssPages\n+        }\n+        else if (procInfoSplit(1).toLowerCase(Locale.US).contains(\"python\")) {\n+          latestPythonVmemTotal += vmem\n+          latestPythonRSSTotal += rssPages\n+        }\n+        else {\n+        latestOtherVmemTotal += vmem\n+        latestOtherRSSTotal += rssPages }\n+      }\n+    } catch {\n+      case f: FileNotFoundException => log.debug(\"There was a problem with reading\" +\n+        \" the stat file of the process\", f)\n+    }\n+  }\n+\n+  def updateAllMetrics(): Unit = {\n+    allMetrics = computeAllMetrics\n+  }\n+\n+  private def computeAllMetrics(): ProcfsBasedSystemsMetrics = {\n+    if (!isAvailable) {\n+      return ProcfsBasedSystemsMetrics(-1, -1, -1, -1, -1, -1)\n+    }\n+    computeProcessTree\n+    val pids = ptree.keySet\n+    latestJVMRSSTotal = 0\n+    latestJVMVmemTotal = 0\n+    latestPythonRSSTotal = 0\n+    latestPythonVmemTotal = 0\n+    latestOtherRSSTotal = 0\n+    latestOtherVmemTotal = 0\n+    for (p <- pids) {\n+      getProcessInfo(p)\n+    }\n+    ProcfsBasedSystemsMetrics(\n+      getJVMVirtualMemInfo,\n+      getJVMRSSInfo,\n+      getPythonVirtualMemInfo,\n+      getPythonRSSInfo,\n+      getOtherVirtualMemInfo,\n+      getOtherRSSInfo)\n+\n+  }\n+\n+  def getOtherRSSInfo(): Long = {\n+    if (!isAvailable) {"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "dhruve"
    },
    "body": "We can remove the individual variables and directly return the data from the current metrics instance. Also these seem to belong more to the metrics class and not really to the file system.",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-04T18:40:33Z",
    "diffHunk": "@@ -0,0 +1,266 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.mutable.Queue\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.{config, Logging}\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+  private val ptree = mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal = 0L\n+  private var latestJVMRSSTotal = 0L\n+  private var latestPythonVmemTotal = 0L\n+  private var latestPythonRSSTotal = 0L\n+  private var latestOtherVmemTotal = 0L\n+  private var latestOtherRSSTotal = 0L\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out = Array.fill[Byte](length)(0)\n+      Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+      val pid = Integer.parseInt(new String(out, \"UTF-8\").trim)\n+      return pid;\n+    }\n+    catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+    if (testing) {\n+      return 0;\n+    }\n+    val cmd = Array(\"getconf\", \"PAGESIZE\")\n+    val out = Array.fill[Byte](10)(0)\n+    Runtime.getRuntime.exec(cmd).getInputStream.read(out)\n+    return Integer.parseInt(new String(out, \"UTF-8\").trim)\n+  }\n+\n+  private def computeProcessTree(): Unit = {\n+    if (!isAvailable) {\n+      return\n+    }\n+    val queue = mutable.Queue.empty[Int]\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPids(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree += (p -> c.toSet)\n+      }\n+      else {\n+        ptree += (p -> Set[Int]())\n+      }\n+    }\n+  }\n+\n+  private def getChildPids(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val input = Runtime.getRuntime.exec(cmd).getInputStream\n+      val childPidsInByte = mutable.ArrayBuffer.empty[Byte]\n+      var d = input.read()\n+      while (d != -1) {\n+        childPidsInByte.append(d.asInstanceOf[Byte])\n+        d = input.read()\n+      }\n+      input.close()\n+      val childPids = new String(childPidsInByte.toArray, \"UTF-8\").split(\"\\n\")\n+      val childPidsInInt = mutable.ArrayBuffer.empty[Int]\n+      for (p <- childPids) {\n+        if (p != \"\") {\n+          childPidsInInt += Integer.parseInt(p)\n+        }\n+      }\n+      childPidsInInt\n+    } catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return mutable.ArrayBuffer.empty[Int]\n+    }\n+  }\n+\n+  def getProcessInfo(pid: Int): Unit = {\n+    /*\n+   * Hadoop ProcfsBasedProcessTree class used regex and pattern matching to retrive the memory\n+   * info. I tried that but found it not correct during tests, so I used normal string analysis\n+   * instead. The computation of RSS and Vmem are based on proc(5):\n+   * http://man7.org/linux/man-pages/man5/proc.5.html\n+   */\n+    try {\n+      val pidDir = new File(procfsDir, pid.toString)\n+      val fReader = new InputStreamReader(\n+        new FileInputStream(\n+          new File(pidDir, procfsStatFile)), Charset.forName(\"UTF-8\"))\n+      val in = new BufferedReader(fReader)\n+      val procInfo = in.readLine\n+      in.close\n+      fReader.close\n+      val procInfoSplit = procInfo.split(\" \")\n+      if ( procInfoSplit != null ) {\n+        val vmem = procInfoSplit(22).toLong\n+        val rssPages = procInfoSplit(23).toLong\n+        if (procInfoSplit(1).toLowerCase(Locale.US).contains(\"java\")) {\n+          latestJVMVmemTotal += vmem\n+          latestJVMRSSTotal += rssPages\n+        }\n+        else if (procInfoSplit(1).toLowerCase(Locale.US).contains(\"python\")) {\n+          latestPythonVmemTotal += vmem\n+          latestPythonRSSTotal += rssPages\n+        }\n+        else {\n+        latestOtherVmemTotal += vmem\n+        latestOtherRSSTotal += rssPages }\n+      }\n+    } catch {\n+      case f: FileNotFoundException => log.debug(\"There was a problem with reading\" +\n+        \" the stat file of the process\", f)\n+    }\n+  }\n+\n+  def updateAllMetrics(): Unit = {\n+    allMetrics = computeAllMetrics\n+  }\n+\n+  private def computeAllMetrics(): ProcfsBasedSystemsMetrics = {\n+    if (!isAvailable) {\n+      return ProcfsBasedSystemsMetrics(-1, -1, -1, -1, -1, -1)\n+    }\n+    computeProcessTree\n+    val pids = ptree.keySet\n+    latestJVMRSSTotal = 0\n+    latestJVMVmemTotal = 0\n+    latestPythonRSSTotal = 0\n+    latestPythonVmemTotal = 0\n+    latestOtherRSSTotal = 0\n+    latestOtherVmemTotal = 0\n+    for (p <- pids) {\n+      getProcessInfo(p)\n+    }\n+    ProcfsBasedSystemsMetrics(\n+      getJVMVirtualMemInfo,\n+      getJVMRSSInfo,\n+      getPythonVirtualMemInfo,\n+      getPythonRSSInfo,\n+      getOtherVirtualMemInfo,\n+      getOtherRSSInfo)\n+\n+  }\n+\n+  def getOtherRSSInfo(): Long = {\n+    if (!isAvailable) {\n+      return -1\n+    }\n+    latestOtherRSSTotal*pageSize"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "edwinalu"
    },
    "body": "Could this just be vmem and rssPages, rather than splitting into JVM, Python, and other? Can you explain more about how the separate values would be used?",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-08T19:02:37Z",
    "diffHunk": "@@ -0,0 +1,272 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+  private val ptree = mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal = 0L\n+  private var latestJVMRSSTotal = 0L\n+  private var latestPythonVmemTotal = 0L\n+  private var latestPythonRSSTotal = 0L\n+  private var latestOtherVmemTotal = 0L\n+  private var latestOtherRSSTotal = 0L\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      val pid = Integer.parseInt(out2.split(\"\\n\")(0))\n+      return pid;\n+    }\n+    catch {\n+      case e: SparkException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    if (testing) {\n+      return 0;\n+    }\n+    val cmd = Array(\"getconf\", \"PAGESIZE\")\n+    val out2 = Utils.executeAndGetOutput(cmd)\n+    return Integer.parseInt(out2.split(\"\\n\")(0))\n+  }\n+\n+  private def computeProcessTree(): Unit = {\n+    if (!isAvailable || testing) {\n+      return\n+    }\n+    val queue = mutable.Queue.empty[Int]\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPids(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree += (p -> c.toSet)\n+      }\n+      else {\n+        ptree += (p -> Set[Int]())\n+      }\n+    }\n+  }\n+\n+  private def getChildPids(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val builder = new ProcessBuilder(\"pgrep\", \"-P\", pid.toString)\n+      val process = builder.start()\n+      val output = new StringBuilder()\n+      val threadName = \"read stdout for \" + \"pgrep\"\n+      def appendToOutput(s: String): Unit = output.append(s).append(\"\\n\")\n+      val stdoutThread = Utils.processStreamByLine(threadName,\n+        process.getInputStream, appendToOutput)\n+      val exitCode = process.waitFor()\n+      stdoutThread.join()\n+      // pgrep will have exit code of 1 if there are more than one child process\n+      // and it will have a exit code of 2 if there is no child process\n+      if (exitCode != 0 && exitCode > 2) {\n+        logError(s\"Process $cmd exited with code $exitCode: $output\")\n+        throw new SparkException(s\"Process $cmd exited with code $exitCode\")\n+      }\n+      val childPids = output.toString.split(\"\\n\")\n+      val childPidsInInt = mutable.ArrayBuffer.empty[Int]\n+      for (p <- childPids) {\n+        if (p != \"\") {\n+          logDebug(\"Found a child pid: \" + p)\n+          childPidsInInt += Integer.parseInt(p)\n+        }\n+      }\n+      childPidsInInt\n+    } catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return mutable.ArrayBuffer.empty[Int]\n+    }\n+  }\n+\n+  def computeProcessInfo(pid: Int): Unit = {\n+    /*\n+   * Hadoop ProcfsBasedProcessTree class used regex and pattern matching to retrive the memory\n+   * info. I tried that but found it not correct during tests, so I used normal string analysis\n+   * instead. The computation of RSS and Vmem are based on proc(5):\n+   * http://man7.org/linux/man-pages/man5/proc.5.html\n+   */\n+    try {\n+      val pidDir = new File(procfsDir, pid.toString)\n+      Utils.tryWithResource( new InputStreamReader(\n+        new FileInputStream(\n+          new File(pidDir, procfsStatFile)), Charset.forName(\"UTF-8\"))) { fReader =>\n+        Utils.tryWithResource( new BufferedReader(fReader)) { in =>\n+          val procInfo = in.readLine\n+          val procInfoSplit = procInfo.split(\" \")\n+          if (procInfoSplit != null) {\n+            val vmem = procInfoSplit(22).toLong\n+            val rssPages = procInfoSplit(23).toLong\n+            if (procInfoSplit(1).toLowerCase(Locale.US).contains(\"java\")) {"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "This is separated since it turns out knowing main actors like jvm in seperation can have some value for the user. We just consider jvm (case of pur scala) and python (case of using pyspark). Other stuff can be added per interest in future, but for now we consider everything else under \"Other\" category",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-08T20:21:31Z",
    "diffHunk": "@@ -0,0 +1,272 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+  private val ptree = mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal = 0L\n+  private var latestJVMRSSTotal = 0L\n+  private var latestPythonVmemTotal = 0L\n+  private var latestPythonRSSTotal = 0L\n+  private var latestOtherVmemTotal = 0L\n+  private var latestOtherRSSTotal = 0L\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      val pid = Integer.parseInt(out2.split(\"\\n\")(0))\n+      return pid;\n+    }\n+    catch {\n+      case e: SparkException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    if (testing) {\n+      return 0;\n+    }\n+    val cmd = Array(\"getconf\", \"PAGESIZE\")\n+    val out2 = Utils.executeAndGetOutput(cmd)\n+    return Integer.parseInt(out2.split(\"\\n\")(0))\n+  }\n+\n+  private def computeProcessTree(): Unit = {\n+    if (!isAvailable || testing) {\n+      return\n+    }\n+    val queue = mutable.Queue.empty[Int]\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPids(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree += (p -> c.toSet)\n+      }\n+      else {\n+        ptree += (p -> Set[Int]())\n+      }\n+    }\n+  }\n+\n+  private def getChildPids(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val builder = new ProcessBuilder(\"pgrep\", \"-P\", pid.toString)\n+      val process = builder.start()\n+      val output = new StringBuilder()\n+      val threadName = \"read stdout for \" + \"pgrep\"\n+      def appendToOutput(s: String): Unit = output.append(s).append(\"\\n\")\n+      val stdoutThread = Utils.processStreamByLine(threadName,\n+        process.getInputStream, appendToOutput)\n+      val exitCode = process.waitFor()\n+      stdoutThread.join()\n+      // pgrep will have exit code of 1 if there are more than one child process\n+      // and it will have a exit code of 2 if there is no child process\n+      if (exitCode != 0 && exitCode > 2) {\n+        logError(s\"Process $cmd exited with code $exitCode: $output\")\n+        throw new SparkException(s\"Process $cmd exited with code $exitCode\")\n+      }\n+      val childPids = output.toString.split(\"\\n\")\n+      val childPidsInInt = mutable.ArrayBuffer.empty[Int]\n+      for (p <- childPids) {\n+        if (p != \"\") {\n+          logDebug(\"Found a child pid: \" + p)\n+          childPidsInInt += Integer.parseInt(p)\n+        }\n+      }\n+      childPidsInInt\n+    } catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return mutable.ArrayBuffer.empty[Int]\n+    }\n+  }\n+\n+  def computeProcessInfo(pid: Int): Unit = {\n+    /*\n+   * Hadoop ProcfsBasedProcessTree class used regex and pattern matching to retrive the memory\n+   * info. I tried that but found it not correct during tests, so I used normal string analysis\n+   * instead. The computation of RSS and Vmem are based on proc(5):\n+   * http://man7.org/linux/man-pages/man5/proc.5.html\n+   */\n+    try {\n+      val pidDir = new File(procfsDir, pid.toString)\n+      Utils.tryWithResource( new InputStreamReader(\n+        new FileInputStream(\n+          new File(pidDir, procfsStatFile)), Charset.forName(\"UTF-8\"))) { fReader =>\n+        Utils.tryWithResource( new BufferedReader(fReader)) { in =>\n+          val procInfo = in.readLine\n+          val procInfoSplit = procInfo.split(\" \")\n+          if (procInfoSplit != null) {\n+            val vmem = procInfoSplit(22).toLong\n+            val rssPages = procInfoSplit(23).toLong\n+            if (procInfoSplit(1).toLowerCase(Locale.US).contains(\"java\")) {"
  }, {
    "author": {
      "login": "dhruve"
    },
    "body": "@edwinalu It would be nice to have a break up of the total memory being consumed. Its easier to tune the parameters knowing what is consuming all the memory. For example if your container died OOMing - it helps to know if it was because of python or JVM. Also R fits in the other category so it makes sense to have all 3 of them as of now. ",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-09T15:04:55Z",
    "diffHunk": "@@ -0,0 +1,272 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+  private val ptree = mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal = 0L\n+  private var latestJVMRSSTotal = 0L\n+  private var latestPythonVmemTotal = 0L\n+  private var latestPythonRSSTotal = 0L\n+  private var latestOtherVmemTotal = 0L\n+  private var latestOtherRSSTotal = 0L\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      val pid = Integer.parseInt(out2.split(\"\\n\")(0))\n+      return pid;\n+    }\n+    catch {\n+      case e: SparkException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    if (testing) {\n+      return 0;\n+    }\n+    val cmd = Array(\"getconf\", \"PAGESIZE\")\n+    val out2 = Utils.executeAndGetOutput(cmd)\n+    return Integer.parseInt(out2.split(\"\\n\")(0))\n+  }\n+\n+  private def computeProcessTree(): Unit = {\n+    if (!isAvailable || testing) {\n+      return\n+    }\n+    val queue = mutable.Queue.empty[Int]\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPids(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree += (p -> c.toSet)\n+      }\n+      else {\n+        ptree += (p -> Set[Int]())\n+      }\n+    }\n+  }\n+\n+  private def getChildPids(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val builder = new ProcessBuilder(\"pgrep\", \"-P\", pid.toString)\n+      val process = builder.start()\n+      val output = new StringBuilder()\n+      val threadName = \"read stdout for \" + \"pgrep\"\n+      def appendToOutput(s: String): Unit = output.append(s).append(\"\\n\")\n+      val stdoutThread = Utils.processStreamByLine(threadName,\n+        process.getInputStream, appendToOutput)\n+      val exitCode = process.waitFor()\n+      stdoutThread.join()\n+      // pgrep will have exit code of 1 if there are more than one child process\n+      // and it will have a exit code of 2 if there is no child process\n+      if (exitCode != 0 && exitCode > 2) {\n+        logError(s\"Process $cmd exited with code $exitCode: $output\")\n+        throw new SparkException(s\"Process $cmd exited with code $exitCode\")\n+      }\n+      val childPids = output.toString.split(\"\\n\")\n+      val childPidsInInt = mutable.ArrayBuffer.empty[Int]\n+      for (p <- childPids) {\n+        if (p != \"\") {\n+          logDebug(\"Found a child pid: \" + p)\n+          childPidsInInt += Integer.parseInt(p)\n+        }\n+      }\n+      childPidsInInt\n+    } catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return mutable.ArrayBuffer.empty[Int]\n+    }\n+  }\n+\n+  def computeProcessInfo(pid: Int): Unit = {\n+    /*\n+   * Hadoop ProcfsBasedProcessTree class used regex and pattern matching to retrive the memory\n+   * info. I tried that but found it not correct during tests, so I used normal string analysis\n+   * instead. The computation of RSS and Vmem are based on proc(5):\n+   * http://man7.org/linux/man-pages/man5/proc.5.html\n+   */\n+    try {\n+      val pidDir = new File(procfsDir, pid.toString)\n+      Utils.tryWithResource( new InputStreamReader(\n+        new FileInputStream(\n+          new File(pidDir, procfsStatFile)), Charset.forName(\"UTF-8\"))) { fReader =>\n+        Utils.tryWithResource( new BufferedReader(fReader)) { in =>\n+          val procInfo = in.readLine\n+          val procInfoSplit = procInfo.split(\" \")\n+          if (procInfoSplit != null) {\n+            val vmem = procInfoSplit(22).toLong\n+            val rssPages = procInfoSplit(23).toLong\n+            if (procInfoSplit(1).toLowerCase(Locale.US).contains(\"java\")) {"
  }, {
    "author": {
      "login": "edwinalu"
    },
    "body": "We don't have much pyspark ourselves, but yes, it seems useful to have the breakdown, and it's easy to sum the values for the total.",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-09T16:07:49Z",
    "diffHunk": "@@ -0,0 +1,272 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+  private val ptree = mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+  private var latestJVMVmemTotal = 0L\n+  private var latestJVMRSSTotal = 0L\n+  private var latestPythonVmemTotal = 0L\n+  private var latestPythonRSSTotal = 0L\n+  private var latestOtherVmemTotal = 0L\n+  private var latestOtherRSSTotal = 0L\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      val pid = Integer.parseInt(out2.split(\"\\n\")(0))\n+      return pid;\n+    }\n+    catch {\n+      case e: SparkException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    if (testing) {\n+      return 0;\n+    }\n+    val cmd = Array(\"getconf\", \"PAGESIZE\")\n+    val out2 = Utils.executeAndGetOutput(cmd)\n+    return Integer.parseInt(out2.split(\"\\n\")(0))\n+  }\n+\n+  private def computeProcessTree(): Unit = {\n+    if (!isAvailable || testing) {\n+      return\n+    }\n+    val queue = mutable.Queue.empty[Int]\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPids(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree += (p -> c.toSet)\n+      }\n+      else {\n+        ptree += (p -> Set[Int]())\n+      }\n+    }\n+  }\n+\n+  private def getChildPids(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val builder = new ProcessBuilder(\"pgrep\", \"-P\", pid.toString)\n+      val process = builder.start()\n+      val output = new StringBuilder()\n+      val threadName = \"read stdout for \" + \"pgrep\"\n+      def appendToOutput(s: String): Unit = output.append(s).append(\"\\n\")\n+      val stdoutThread = Utils.processStreamByLine(threadName,\n+        process.getInputStream, appendToOutput)\n+      val exitCode = process.waitFor()\n+      stdoutThread.join()\n+      // pgrep will have exit code of 1 if there are more than one child process\n+      // and it will have a exit code of 2 if there is no child process\n+      if (exitCode != 0 && exitCode > 2) {\n+        logError(s\"Process $cmd exited with code $exitCode: $output\")\n+        throw new SparkException(s\"Process $cmd exited with code $exitCode\")\n+      }\n+      val childPids = output.toString.split(\"\\n\")\n+      val childPidsInInt = mutable.ArrayBuffer.empty[Int]\n+      for (p <- childPids) {\n+        if (p != \"\") {\n+          logDebug(\"Found a child pid: \" + p)\n+          childPidsInInt += Integer.parseInt(p)\n+        }\n+      }\n+      childPidsInInt\n+    } catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return mutable.ArrayBuffer.empty[Int]\n+    }\n+  }\n+\n+  def computeProcessInfo(pid: Int): Unit = {\n+    /*\n+   * Hadoop ProcfsBasedProcessTree class used regex and pattern matching to retrive the memory\n+   * info. I tried that but found it not correct during tests, so I used normal string analysis\n+   * instead. The computation of RSS and Vmem are based on proc(5):\n+   * http://man7.org/linux/man-pages/man5/proc.5.html\n+   */\n+    try {\n+      val pidDir = new File(procfsDir, pid.toString)\n+      Utils.tryWithResource( new InputStreamReader(\n+        new FileInputStream(\n+          new File(pidDir, procfsStatFile)), Charset.forName(\"UTF-8\"))) { fReader =>\n+        Utils.tryWithResource( new BufferedReader(fReader)) { in =>\n+          val procInfo = in.readLine\n+          val procInfoSplit = procInfo.split(\" \")\n+          if (procInfoSplit != null) {\n+            val vmem = procInfoSplit(22).toLong\n+            val rssPages = procInfoSplit(23).toLong\n+            if (procInfoSplit(1).toLowerCase(Locale.US).contains(\"java\")) {"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "why only SparkException, not any Exception?  also the msg shouldn't say \"IO Exception\".\r\n\r\nshould probably be logWarn",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-19T02:26:12Z",
    "diffHunk": "@@ -0,0 +1,226 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+  private val ptree = mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      val pid = Integer.parseInt(out2.split(\"\\n\")(0))\n+      return pid;\n+    }\n+    catch {\n+      case e: SparkException => logDebug(\"IO Exception when trying to compute process tree.\" +"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "Let me double check I thought there was a comment before that I should just get SparkException, but you are right. it doesn't make sense. Probably a mistake on my side. I was just caring about IOException here.",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-19T03:09:26Z",
    "diffHunk": "@@ -0,0 +1,226 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+  private val ptree = mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      val pid = Integer.parseInt(out2.split(\"\\n\")(0))\n+      return pid;\n+    }\n+    catch {\n+      case e: SparkException => logDebug(\"IO Exception when trying to compute process tree.\" +"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "oh it seems there wasn't a mistake here and I jut forgot the reason here. I caught SparkException since executeAndGetOutput may throw such an exception. I will remove the IOException",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-19T03:25:36Z",
    "diffHunk": "@@ -0,0 +1,226 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+  private val ptree = mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      val pid = Integer.parseInt(out2.split(\"\\n\")(0))\n+      return pid;\n+    }\n+    catch {\n+      case e: SparkException => logDebug(\"IO Exception when trying to compute process tree.\" +"
  }, {
    "author": {
      "login": "squito"
    },
    "body": "well, executeAndGetOutput might throw a SparkException ... but are you sure nothing else will get thrown?  Eg. what if you get some weird output and then the `Integer.parseInt` failse?  Is there some reason you wouldn't want the same error handling for any exception here?",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-22T16:58:36Z",
    "diffHunk": "@@ -0,0 +1,226 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+  private val ptree = mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      val pid = Integer.parseInt(out2.split(\"\\n\")(0))\n+      return pid;\n+    }\n+    catch {\n+      case e: SparkException => logDebug(\"IO Exception when trying to compute process tree.\" +"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "At first I was getting all throwables. Then I thought it can be dangerous. There was also a review comment about that. So Not sure what is the correct way of handling this. Is it better to just take care of exceptions that we know can be thrown or catch all throwables? ",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-22T17:11:07Z",
    "diffHunk": "@@ -0,0 +1,226 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+  private val ptree = mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      val pid = Integer.parseInt(out2.split(\"\\n\")(0))\n+      return pid;\n+    }\n+    catch {\n+      case e: SparkException => logDebug(\"IO Exception when trying to compute process tree.\" +"
  }, {
    "author": {
      "login": "squito"
    },
    "body": "there's a distinction between `Throwable` and `Exception` -- `Throwable` includes Errors which are fatal to the JVM, you probably can't do anything.\r\n\r\nIn general its a good question whether you should catch specific exceptions or everything.  Here, you're calling an external program, and I don't feel super confident that we know how it always behaves, so I think we should be a little extra cautious.  An unhandled exception here would lead to not sending any heartbeats, which would be really bad.  Except for JVM errors, I think we just want to turn off this particular metric and keep going.",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-22T18:17:23Z",
    "diffHunk": "@@ -0,0 +1,226 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+  private val ptree = mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      val pid = Integer.parseInt(out2.split(\"\\n\")(0))\n+      return pid;\n+    }\n+    catch {\n+      case e: SparkException => logDebug(\"IO Exception when trying to compute process tree.\" +"
  }, {
    "author": {
      "login": "squito"
    },
    "body": "found the old comment from @mccheah \r\n\r\n> Catching Throwable is generally scary, can this mask out of memory and errors of that sort? Can we scope down the exception type to handle here?\r\n\r\nI think this (partially) agrees with what I said above, we dont' want to catch `Throwable` because that can mask other stuff where the jvm is hosed.  But I still think `Exception` is the right thing to catch.  sound ok @mccheah ?\r\n\r\nif you really do want more specific exceptions, we should look through this more carefully to come up with a more exhaustive list, eg. I certainly don't want to fail the heartbeater because we dont' get an int out of the external call for some reason.",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-22T18:23:29Z",
    "diffHunk": "@@ -0,0 +1,226 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+  private val ptree = mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      val pid = Integer.parseInt(out2.split(\"\\n\")(0))\n+      return pid;\n+    }\n+    catch {\n+      case e: SparkException => logDebug(\"IO Exception when trying to compute process tree.\" +"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "if this fails in some unexpected way, you want to set `isAvailable=false` but not interfere with anything else, right?  so you should do similar error handling here.",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-19T02:27:52Z",
    "diffHunk": "@@ -0,0 +1,226 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+  private val ptree = mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      val pid = Integer.parseInt(out2.split(\"\\n\")(0))\n+      return pid;\n+    }\n+    catch {\n+      case e: SparkException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    if (testing) {\n+      return 0;\n+    }\n+    val cmd = Array(\"getconf\", \"PAGESIZE\")\n+    val out2 = Utils.executeAndGetOutput(cmd)\n+    return Integer.parseInt(out2.split(\"\\n\")(0))"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "yes, will fix it.",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-19T03:10:26Z",
    "diffHunk": "@@ -0,0 +1,226 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+  private val ptree = mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      val pid = Integer.parseInt(out2.split(\"\\n\")(0))\n+      return pid;\n+    }\n+    catch {\n+      case e: SparkException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    if (testing) {\n+      return 0;\n+    }\n+    val cmd = Array(\"getconf\", \"PAGESIZE\")\n+    val out2 = Utils.executeAndGetOutput(cmd)\n+    return Integer.parseInt(out2.split(\"\\n\")(0))"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "why only IOException?  you throw a SparkException above, or the integer parsing may fail -- I assume you want the same error handling.\r\n\r\nAlso you're ignoring stderr, probably at least want to log it?",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-19T02:28:44Z",
    "diffHunk": "@@ -0,0 +1,226 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+  private val ptree = mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      val pid = Integer.parseInt(out2.split(\"\\n\")(0))\n+      return pid;\n+    }\n+    catch {\n+      case e: SparkException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    if (testing) {\n+      return 0;\n+    }\n+    val cmd = Array(\"getconf\", \"PAGESIZE\")\n+    val out2 = Utils.executeAndGetOutput(cmd)\n+    return Integer.parseInt(out2.split(\"\\n\")(0))\n+  }\n+\n+  private def computeProcessTree(): Unit = {\n+    if (!isAvailable || testing) {\n+      return\n+    }\n+    val queue = mutable.Queue.empty[Int]\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPids(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree += (p -> c.toSet)\n+      }\n+      else {\n+        ptree += (p -> Set[Int]())\n+      }\n+    }\n+  }\n+\n+  private def getChildPids(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val builder = new ProcessBuilder(\"pgrep\", \"-P\", pid.toString)\n+      val process = builder.start()\n+      val output = new StringBuilder()\n+      val threadName = \"read stdout for \" + \"pgrep\"\n+      def appendToOutput(s: String): Unit = output.append(s).append(\"\\n\")\n+      val stdoutThread = Utils.processStreamByLine(threadName,\n+        process.getInputStream, appendToOutput)\n+      val exitCode = process.waitFor()\n+      stdoutThread.join()\n+      // pgrep will have exit code of 1 if there are more than one child process\n+      // and it will have a exit code of 2 if there is no child process\n+      if (exitCode != 0 && exitCode > 2) {\n+        logError(s\"Process $cmd exited with code $exitCode: $output\")\n+        throw new SparkException(s\"Process $cmd exited with code $exitCode\")\n+      }\n+      val childPids = output.toString.split(\"\\n\")\n+      val childPidsInInt = mutable.ArrayBuffer.empty[Int]\n+      for (p <- childPids) {\n+        if (p != \"\") {\n+          logDebug(\"Found a child pid: \" + p)\n+          childPidsInInt += Integer.parseInt(p)\n+        }\n+      }\n+      childPidsInInt\n+    } catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "I think the previous was a mistake, since I just care about IOExceptions. I will double check to make sure that this was a mistake before  ",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-19T03:13:24Z",
    "diffHunk": "@@ -0,0 +1,226 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+  private val ptree = mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      val pid = Integer.parseInt(out2.split(\"\\n\")(0))\n+      return pid;\n+    }\n+    catch {\n+      case e: SparkException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    if (testing) {\n+      return 0;\n+    }\n+    val cmd = Array(\"getconf\", \"PAGESIZE\")\n+    val out2 = Utils.executeAndGetOutput(cmd)\n+    return Integer.parseInt(out2.split(\"\\n\")(0))\n+  }\n+\n+  private def computeProcessTree(): Unit = {\n+    if (!isAvailable || testing) {\n+      return\n+    }\n+    val queue = mutable.Queue.empty[Int]\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPids(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree += (p -> c.toSet)\n+      }\n+      else {\n+        ptree += (p -> Set[Int]())\n+      }\n+    }\n+  }\n+\n+  private def getChildPids(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val builder = new ProcessBuilder(\"pgrep\", \"-P\", pid.toString)\n+      val process = builder.start()\n+      val output = new StringBuilder()\n+      val threadName = \"read stdout for \" + \"pgrep\"\n+      def appendToOutput(s: String): Unit = output.append(s).append(\"\\n\")\n+      val stdoutThread = Utils.processStreamByLine(threadName,\n+        process.getInputStream, appendToOutput)\n+      val exitCode = process.waitFor()\n+      stdoutThread.join()\n+      // pgrep will have exit code of 1 if there are more than one child process\n+      // and it will have a exit code of 2 if there is no child process\n+      if (exitCode != 0 && exitCode > 2) {\n+        logError(s\"Process $cmd exited with code $exitCode: $output\")\n+        throw new SparkException(s\"Process $cmd exited with code $exitCode\")\n+      }\n+      val childPids = output.toString.split(\"\\n\")\n+      val childPidsInInt = mutable.ArrayBuffer.empty[Int]\n+      for (p <- childPids) {\n+        if (p != \"\") {\n+          logDebug(\"Found a child pid: \" + p)\n+          childPidsInInt += Integer.parseInt(p)\n+        }\n+      }\n+      childPidsInInt\n+    } catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "I will change this to just catch the SparkException",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-19T03:29:22Z",
    "diffHunk": "@@ -0,0 +1,226 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+  private val ptree = mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      val pid = Integer.parseInt(out2.split(\"\\n\")(0))\n+      return pid;\n+    }\n+    catch {\n+      case e: SparkException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    if (testing) {\n+      return 0;\n+    }\n+    val cmd = Array(\"getconf\", \"PAGESIZE\")\n+    val out2 = Utils.executeAndGetOutput(cmd)\n+    return Integer.parseInt(out2.split(\"\\n\")(0))\n+  }\n+\n+  private def computeProcessTree(): Unit = {\n+    if (!isAvailable || testing) {\n+      return\n+    }\n+    val queue = mutable.Queue.empty[Int]\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPids(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree += (p -> c.toSet)\n+      }\n+      else {\n+        ptree += (p -> Set[Int]())\n+      }\n+    }\n+  }\n+\n+  private def getChildPids(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val builder = new ProcessBuilder(\"pgrep\", \"-P\", pid.toString)\n+      val process = builder.start()\n+      val output = new StringBuilder()\n+      val threadName = \"read stdout for \" + \"pgrep\"\n+      def appendToOutput(s: String): Unit = output.append(s).append(\"\\n\")\n+      val stdoutThread = Utils.processStreamByLine(threadName,\n+        process.getInputStream, appendToOutput)\n+      val exitCode = process.waitFor()\n+      stdoutThread.join()\n+      // pgrep will have exit code of 1 if there are more than one child process\n+      // and it will have a exit code of 2 if there is no child process\n+      if (exitCode != 0 && exitCode > 2) {\n+        logError(s\"Process $cmd exited with code $exitCode: $output\")\n+        throw new SparkException(s\"Process $cmd exited with code $exitCode\")\n+      }\n+      val childPids = output.toString.split(\"\\n\")\n+      val childPidsInInt = mutable.ArrayBuffer.empty[Int]\n+      for (p <- childPids) {\n+        if (p != \"\") {\n+          logDebug(\"Found a child pid: \" + p)\n+          childPidsInInt += Integer.parseInt(p)\n+        }\n+      }\n+      childPidsInInt\n+    } catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +"
  }, {
    "author": {
      "login": "squito"
    },
    "body": "as mentioned above, I think you really want to catch all of `Exception`",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-22T16:59:16Z",
    "diffHunk": "@@ -0,0 +1,226 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+  private val ptree = mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      val pid = Integer.parseInt(out2.split(\"\\n\")(0))\n+      return pid;\n+    }\n+    catch {\n+      case e: SparkException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    if (testing) {\n+      return 0;\n+    }\n+    val cmd = Array(\"getconf\", \"PAGESIZE\")\n+    val out2 = Utils.executeAndGetOutput(cmd)\n+    return Integer.parseInt(out2.split(\"\\n\")(0))\n+  }\n+\n+  private def computeProcessTree(): Unit = {\n+    if (!isAvailable || testing) {\n+      return\n+    }\n+    val queue = mutable.Queue.empty[Int]\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPids(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree += (p -> c.toSet)\n+      }\n+      else {\n+        ptree += (p -> Set[Int]())\n+      }\n+    }\n+  }\n+\n+  private def getChildPids(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val builder = new ProcessBuilder(\"pgrep\", \"-P\", pid.toString)\n+      val process = builder.start()\n+      val output = new StringBuilder()\n+      val threadName = \"read stdout for \" + \"pgrep\"\n+      def appendToOutput(s: String): Unit = output.append(s).append(\"\\n\")\n+      val stdoutThread = Utils.processStreamByLine(threadName,\n+        process.getInputStream, appendToOutput)\n+      val exitCode = process.waitFor()\n+      stdoutThread.join()\n+      // pgrep will have exit code of 1 if there are more than one child process\n+      // and it will have a exit code of 2 if there is no child process\n+      if (exitCode != 0 && exitCode > 2) {\n+        logError(s\"Process $cmd exited with code $exitCode: $output\")\n+        throw new SparkException(s\"Process $cmd exited with code $exitCode\")\n+      }\n+      val childPids = output.toString.split(\"\\n\")\n+      val childPidsInInt = mutable.ArrayBuffer.empty[Int]\n+      for (p <- childPids) {\n+        if (p != \"\") {\n+          logDebug(\"Found a child pid: \" + p)\n+          childPidsInInt += Integer.parseInt(p)\n+        }\n+      }\n+      childPidsInInt\n+    } catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "`cmd` is an array, its toString isn't nice.  maybe you want `cmd.mkString(\" \")`, or just say \"pgrep failed to find children of $pid\".\r\n\r\n```scala\r\nscala> val cmd = Array(\"foo\", \"bar\")\r\ncmd: Array[String] = Array(foo, bar)\r\n\r\nscala> s\"Process $cmd failed\"\r\nres0: String = Process [Ljava.lang.String;@76130a29 failed\r\n\r\nscala> s\"Process '${cmd.mkString(\" \")}' failed\"\r\nres3: String = Process 'foo bar' failed\r\n```",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-19T02:29:54Z",
    "diffHunk": "@@ -0,0 +1,226 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+  private val ptree = mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      val pid = Integer.parseInt(out2.split(\"\\n\")(0))\n+      return pid;\n+    }\n+    catch {\n+      case e: SparkException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    if (testing) {\n+      return 0;\n+    }\n+    val cmd = Array(\"getconf\", \"PAGESIZE\")\n+    val out2 = Utils.executeAndGetOutput(cmd)\n+    return Integer.parseInt(out2.split(\"\\n\")(0))\n+  }\n+\n+  private def computeProcessTree(): Unit = {\n+    if (!isAvailable || testing) {\n+      return\n+    }\n+    val queue = mutable.Queue.empty[Int]\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPids(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree += (p -> c.toSet)\n+      }\n+      else {\n+        ptree += (p -> Set[Int]())\n+      }\n+    }\n+  }\n+\n+  private def getChildPids(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val builder = new ProcessBuilder(\"pgrep\", \"-P\", pid.toString)\n+      val process = builder.start()\n+      val output = new StringBuilder()\n+      val threadName = \"read stdout for \" + \"pgrep\"\n+      def appendToOutput(s: String): Unit = output.append(s).append(\"\\n\")\n+      val stdoutThread = Utils.processStreamByLine(threadName,\n+        process.getInputStream, appendToOutput)\n+      val exitCode = process.waitFor()\n+      stdoutThread.join()\n+      // pgrep will have exit code of 1 if there are more than one child process\n+      // and it will have a exit code of 2 if there is no child process\n+      if (exitCode != 0 && exitCode > 2) {\n+        logError(s\"Process $cmd exited with code $exitCode: $output\")\n+        throw new SparkException(s\"Process $cmd exited with code $exitCode\")"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "you can simplify this if you just directly add to childPids in the function you pass to `processStreamByLine`\r\n\r\n```scala\r\nval childPids = ArrayBuffer.empty[Int]\r\ndef appendChildPid(s: String): Unit = {\r\n  if (s != \"\") {\r\n    logDebug(\"Found a child pid:\" + s)\r\n    childPids += s.toInt\r\n  }\r\n}\r\nval stdoutThread = Utils.processStreamByLine(threadName,\r\n        process.getInputStream, appendChildPids)\r\n```",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-19T02:32:36Z",
    "diffHunk": "@@ -0,0 +1,226 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+  private val ptree = mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      val pid = Integer.parseInt(out2.split(\"\\n\")(0))\n+      return pid;\n+    }\n+    catch {\n+      case e: SparkException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    if (testing) {\n+      return 0;\n+    }\n+    val cmd = Array(\"getconf\", \"PAGESIZE\")\n+    val out2 = Utils.executeAndGetOutput(cmd)\n+    return Integer.parseInt(out2.split(\"\\n\")(0))\n+  }\n+\n+  private def computeProcessTree(): Unit = {\n+    if (!isAvailable || testing) {\n+      return\n+    }\n+    val queue = mutable.Queue.empty[Int]\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPids(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree += (p -> c.toSet)\n+      }\n+      else {\n+        ptree += (p -> Set[Int]())\n+      }\n+    }\n+  }\n+\n+  private def getChildPids(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val builder = new ProcessBuilder(\"pgrep\", \"-P\", pid.toString)\n+      val process = builder.start()\n+      val output = new StringBuilder()\n+      val threadName = \"read stdout for \" + \"pgrep\"\n+      def appendToOutput(s: String): Unit = output.append(s).append(\"\\n\")\n+      val stdoutThread = Utils.processStreamByLine(threadName,\n+        process.getInputStream, appendToOutput)\n+      val exitCode = process.waitFor()\n+      stdoutThread.join()\n+      // pgrep will have exit code of 1 if there are more than one child process\n+      // and it will have a exit code of 2 if there is no child process\n+      if (exitCode != 0 && exitCode > 2) {\n+        logError(s\"Process $cmd exited with code $exitCode: $output\")\n+        throw new SparkException(s\"Process $cmd exited with code $exitCode\")\n+      }\n+      val childPids = output.toString.split(\"\\n\")\n+      val childPidsInInt = mutable.ArrayBuffer.empty[Int]\n+      for (p <- childPids) {\n+        if (p != \"\") {\n+          logDebug(\"Found a child pid: \" + p)\n+          childPidsInInt += Integer.parseInt(p)\n+        }\n+      }\n+      childPidsInInt"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "case classes have a `copy` method which helps with this\r\n\r\n```scala\r\nval rssMem = procInfoSplit(23).toLong * pageSize\r\n...\r\nallMetrics = allMetrics.copy(\r\n  jvmVmemTotal = allMetrics.jvmVMemTotal + vmem, \r\n  jvmRSSTotal = allMetrics.rssTotal + rssMem\r\n)\r\n```\r\n\r\n\r\n(also maybe easier if values were written directly into result array, as I mention below)",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-10-19T02:34:24Z",
    "diffHunk": "@@ -0,0 +1,226 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+  private val ptree = mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      val pid = Integer.parseInt(out2.split(\"\\n\")(0))\n+      return pid;\n+    }\n+    catch {\n+      case e: SparkException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    if (testing) {\n+      return 0;\n+    }\n+    val cmd = Array(\"getconf\", \"PAGESIZE\")\n+    val out2 = Utils.executeAndGetOutput(cmd)\n+    return Integer.parseInt(out2.split(\"\\n\")(0))\n+  }\n+\n+  private def computeProcessTree(): Unit = {\n+    if (!isAvailable || testing) {\n+      return\n+    }\n+    val queue = mutable.Queue.empty[Int]\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPids(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree += (p -> c.toSet)\n+      }\n+      else {\n+        ptree += (p -> Set[Int]())\n+      }\n+    }\n+  }\n+\n+  private def getChildPids(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val builder = new ProcessBuilder(\"pgrep\", \"-P\", pid.toString)\n+      val process = builder.start()\n+      val output = new StringBuilder()\n+      val threadName = \"read stdout for \" + \"pgrep\"\n+      def appendToOutput(s: String): Unit = output.append(s).append(\"\\n\")\n+      val stdoutThread = Utils.processStreamByLine(threadName,\n+        process.getInputStream, appendToOutput)\n+      val exitCode = process.waitFor()\n+      stdoutThread.join()\n+      // pgrep will have exit code of 1 if there are more than one child process\n+      // and it will have a exit code of 2 if there is no child process\n+      if (exitCode != 0 && exitCode > 2) {\n+        logError(s\"Process $cmd exited with code $exitCode: $output\")\n+        throw new SparkException(s\"Process $cmd exited with code $exitCode\")\n+      }\n+      val childPids = output.toString.split(\"\\n\")\n+      val childPidsInInt = mutable.ArrayBuffer.empty[Int]\n+      for (p <- childPids) {\n+        if (p != \"\") {\n+          logDebug(\"Found a child pid: \" + p)\n+          childPidsInInt += Integer.parseInt(p)\n+        }\n+      }\n+      childPidsInInt\n+    } catch {\n+      case e: IOException => logDebug(\"IO Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return mutable.ArrayBuffer.empty[Int]\n+    }\n+  }\n+\n+  def computeProcessInfo(pid: Int): Unit = {\n+    /*\n+   * Hadoop ProcfsBasedProcessTree class used regex and pattern matching to retrive the memory\n+   * info. I tried that but found it not correct during tests, so I used normal string analysis\n+   * instead. The computation of RSS and Vmem are based on proc(5):\n+   * http://man7.org/linux/man-pages/man5/proc.5.html\n+   */\n+    try {\n+      val pidDir = new File(procfsDir, pid.toString)\n+      Utils.tryWithResource( new InputStreamReader(\n+        new FileInputStream(\n+          new File(pidDir, procfsStatFile)), Charset.forName(\"UTF-8\"))) { fReader =>\n+        Utils.tryWithResource( new BufferedReader(fReader)) { in =>\n+          val procInfo = in.readLine\n+          val procInfoSplit = procInfo.split(\" \")\n+          if (procInfoSplit != null) {\n+            val vmem = procInfoSplit(22).toLong\n+            val rssPages = procInfoSplit(23).toLong\n+            if (procInfoSplit(1).toLowerCase(Locale.US).contains(\"java\")) {\n+              allMetrics = ProcfsBasedSystemsMetrics(\n+                allMetrics.jvmVmemTotal + vmem,\n+                allMetrics.jvmRSSTotal + (rssPages*pageSize),\n+                allMetrics.pythonVmemTotal,\n+                allMetrics.pythonRSSTotal,\n+                allMetrics.otherVmemTotal,\n+                allMetrics.otherRSSTotal\n+              )"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "unused",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-11-08T15:07:57Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+  private var ptree = mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "style nit: if the `case` block is multiline, don't put anything after the `=>`, move it to the next line (here and elsewhere)",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-11-08T16:59:56Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+  private var ptree = mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      val pid = Integer.parseInt(out2.split(\"\\n\")(0))\n+      return pid;\n+    }\n+    catch {\n+      case e: SparkException => logWarning(\"Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "can just be `out` instead of `out2`",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-11-08T17:01:38Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+  private var ptree = mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      val pid = Integer.parseInt(out2.split(\"\\n\")(0))\n+      return pid;\n+    }\n+    catch {\n+      case e: SparkException => logWarning(\"Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    if (testing) {\n+      return 0;\n+    }\n+    try {\n+      val cmd = Array(\"getconf\", \"PAGESIZE\")\n+      val out2 = Utils.executeAndGetOutput(cmd)"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "indentation is wrong",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-11-08T17:05:17Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+  private var ptree = mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      val pid = Integer.parseInt(out2.split(\"\\n\")(0))\n+      return pid;\n+    }\n+    catch {\n+      case e: SparkException => logWarning(\"Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    if (testing) {\n+      return 0;\n+    }\n+    try {\n+      val cmd = Array(\"getconf\", \"PAGESIZE\")\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      return Integer.parseInt(out2.split(\"\\n\")(0))\n+    } catch {\n+      case e: Exception => logWarning(\"Exception when trying to compute pagesize, as a\" +\n+        \" result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return 0\n+    }\n+  }\n+\n+  private def computeProcessTree(): Unit = {\n+    if (!isAvailable || testing) {\n+      return\n+    }\n+    ptree = mutable.Map[ Int, Set[Int]]()\n+    val queue = mutable.Queue.empty[Int]\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPids(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree += (p -> c.toSet)\n+      }\n+      else {\n+        ptree += (p -> Set[Int]())\n+      }\n+    }\n+  }\n+\n+  private def getChildPids(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      // val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val builder = new ProcessBuilder(\"pgrep\", \"-P\", pid.toString)\n+      val process = builder.start()\n+      // val output = new StringBuilder()\n+      val threadName = \"read stdout for \" + \"pgrep\"\n+      val childPidsInInt = mutable.ArrayBuffer.empty[Int]\n+      def appendChildPid(s: String): Unit = {\n+        if (s != \"\") {\n+          logDebug(\"Found a child pid:\" + s)\n+          childPidsInInt += Integer.parseInt(s)\n+        }\n+      }\n+      val stdoutThread = Utils.processStreamByLine(threadName,\n+        process.getInputStream, appendChildPid)\n+      val exitCode = process.waitFor()\n+      stdoutThread.join()\n+      // pgrep will have exit code of 1 if there are more than one child process\n+      // and it will have a exit code of 2 if there is no child process\n+      if (exitCode != 0 && exitCode > 2) {\n+        val cmd = builder.command().toArray.mkString(\" \")\n+        logWarning(s\"Process $cmd\" +\n+          s\" exited with code $exitCode, with stderr:\" + s\"${process.getErrorStream} \")\n+        throw new SparkException(s\"Process $cmd exited with code $exitCode\")\n+      }\n+      childPidsInInt\n+    } catch {\n+      case e: Exception => logWarning(\"Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped.\", e)\n+        isAvailable = false\n+        return mutable.ArrayBuffer.empty[Int]\n+    }\n+  }\n+\n+  def computeProcessInfo(pid: Int): Unit = {\n+    /*\n+   * Hadoop ProcfsBasedProcessTree class used regex and pattern matching to retrive the memory"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "the state used here is a little trickier than it needs to be. \r\n\r\n `computeProcessTree` is updating a member variable, even though its only used locally -- it would be easier to follow if instead it just returned the process tree, and then you passed it around.  Also I dont' think you actually care about the tree, just the set of pids?\r\n\r\nsimilarly for `allMetrics`.  it doesn't really need to be a member variable, since its use is entirely contained within this function, you could just pass it around.\r\n\r\n```scala\r\nval pids = discoverPids()\r\nval allMetrics = ...\r\nfor (p <- pids) {\r\n  allMetrics = updateMetricsForProcess(allMetrics, p)\r\n}\r\n```",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-11-09T11:18:41Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+  private var ptree = mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      val pid = Integer.parseInt(out2.split(\"\\n\")(0))\n+      return pid;\n+    }\n+    catch {\n+      case e: SparkException => logWarning(\"Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    if (testing) {\n+      return 0;\n+    }\n+    try {\n+      val cmd = Array(\"getconf\", \"PAGESIZE\")\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      return Integer.parseInt(out2.split(\"\\n\")(0))\n+    } catch {\n+      case e: Exception => logWarning(\"Exception when trying to compute pagesize, as a\" +\n+        \" result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return 0\n+    }\n+  }\n+\n+  private def computeProcessTree(): Unit = {\n+    if (!isAvailable || testing) {\n+      return\n+    }\n+    ptree = mutable.Map[ Int, Set[Int]]()\n+    val queue = mutable.Queue.empty[Int]\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPids(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree += (p -> c.toSet)\n+      }\n+      else {\n+        ptree += (p -> Set[Int]())\n+      }\n+    }\n+  }\n+\n+  private def getChildPids(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      // val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val builder = new ProcessBuilder(\"pgrep\", \"-P\", pid.toString)\n+      val process = builder.start()\n+      // val output = new StringBuilder()\n+      val threadName = \"read stdout for \" + \"pgrep\"\n+      val childPidsInInt = mutable.ArrayBuffer.empty[Int]\n+      def appendChildPid(s: String): Unit = {\n+        if (s != \"\") {\n+          logDebug(\"Found a child pid:\" + s)\n+          childPidsInInt += Integer.parseInt(s)\n+        }\n+      }\n+      val stdoutThread = Utils.processStreamByLine(threadName,\n+        process.getInputStream, appendChildPid)\n+      val exitCode = process.waitFor()\n+      stdoutThread.join()\n+      // pgrep will have exit code of 1 if there are more than one child process\n+      // and it will have a exit code of 2 if there is no child process\n+      if (exitCode != 0 && exitCode > 2) {\n+        val cmd = builder.command().toArray.mkString(\" \")\n+        logWarning(s\"Process $cmd\" +\n+          s\" exited with code $exitCode, with stderr:\" + s\"${process.getErrorStream} \")\n+        throw new SparkException(s\"Process $cmd exited with code $exitCode\")\n+      }\n+      childPidsInInt\n+    } catch {\n+      case e: Exception => logWarning(\"Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped.\", e)\n+        isAvailable = false\n+        return mutable.ArrayBuffer.empty[Int]\n+    }\n+  }\n+\n+  def computeProcessInfo(pid: Int): Unit = {\n+    /*\n+   * Hadoop ProcfsBasedProcessTree class used regex and pattern matching to retrive the memory\n+   * info. I tried that but found it not correct during tests, so I used normal string analysis\n+   * instead. The computation of RSS and Vmem are based on proc(5):\n+   * http://man7.org/linux/man-pages/man5/proc.5.html\n+   */\n+    try {\n+      val pidDir = new File(procfsDir, pid.toString)\n+      Utils.tryWithResource( new InputStreamReader(\n+        new FileInputStream(\n+          new File(pidDir, procfsStatFile)), Charset.forName(\"UTF-8\"))) { fReader =>\n+        Utils.tryWithResource( new BufferedReader(fReader)) { in =>\n+          val procInfo = in.readLine\n+          val procInfoSplit = procInfo.split(\" \")\n+          if (procInfoSplit != null) {\n+            val vmem = procInfoSplit(22).toLong\n+            val rssPages = procInfoSplit(23).toLong\n+            if (procInfoSplit(1).toLowerCase(Locale.US).contains(\"java\")) {\n+              allMetrics = allMetrics.copy(\n+                jvmVmemTotal = allMetrics.jvmVmemTotal + vmem,\n+                jvmRSSTotal = allMetrics.jvmRSSTotal + (rssPages*pageSize)\n+              )\n+            }\n+            else if (procInfoSplit(1).toLowerCase(Locale.US).contains(\"python\")) {\n+              allMetrics = allMetrics.copy(\n+                pythonVmemTotal = allMetrics.pythonVmemTotal + vmem,\n+                pythonRSSTotal = allMetrics.pythonRSSTotal + (rssPages*pageSize)\n+              )\n+            }\n+            else {\n+              allMetrics = allMetrics.copy(\n+                otherVmemTotal = allMetrics.otherVmemTotal + vmem,\n+                otherRSSTotal = allMetrics.otherRSSTotal + (rssPages*pageSize)\n+              )\n+            }\n+          }\n+        }\n+      }\n+    } catch {\n+      case f: FileNotFoundException => logWarning(\"There was a problem with reading\" +\n+        \" the stat file of the process. \", f)\n+    }\n+  }\n+\n+  private[spark] def computeAllMetrics(): ProcfsBasedSystemsMetrics = {\n+    if (!isAvailable) {\n+      return ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+    }\n+    computeProcessTree\n+    val pids = ptree.keySet\n+    allMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+    for (p <- pids) {\n+      computeProcessInfo(p)"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "The tree was there in case we want to do some other stuff with it, but I guess we can have a tree structure when we actually need it. Right now as you mentioned we don't need it. So I will change it.\r\nthe allMetrics was there for testing, but I can change the test anyway.",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-11-13T22:36:44Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+  private var ptree = mutable.Map[ Int, Set[Int]]()\n+\n+  var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+\n+  computeProcessTree()\n+\n+  private def isProcfsAvailable: Boolean = {\n+    if (testing) {\n+      return true\n+    }\n+    try {\n+      if (!Files.exists(Paths.get(procfsDir))) {\n+        return false\n+      }\n+    }\n+    catch {\n+      case f: FileNotFoundException => return false\n+    }\n+    val shouldLogStageExecutorMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+    val shouldLogStageExecutorProcessTreeMetrics =\n+      SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+    shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val length = 10\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      val pid = Integer.parseInt(out2.split(\"\\n\")(0))\n+      return pid;\n+    }\n+    catch {\n+      case e: SparkException => logWarning(\"Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    if (testing) {\n+      return 0;\n+    }\n+    try {\n+      val cmd = Array(\"getconf\", \"PAGESIZE\")\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      return Integer.parseInt(out2.split(\"\\n\")(0))\n+    } catch {\n+      case e: Exception => logWarning(\"Exception when trying to compute pagesize, as a\" +\n+        \" result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return 0\n+    }\n+  }\n+\n+  private def computeProcessTree(): Unit = {\n+    if (!isAvailable || testing) {\n+      return\n+    }\n+    ptree = mutable.Map[ Int, Set[Int]]()\n+    val queue = mutable.Queue.empty[Int]\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPids(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree += (p -> c.toSet)\n+      }\n+      else {\n+        ptree += (p -> Set[Int]())\n+      }\n+    }\n+  }\n+\n+  private def getChildPids(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      // val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val builder = new ProcessBuilder(\"pgrep\", \"-P\", pid.toString)\n+      val process = builder.start()\n+      // val output = new StringBuilder()\n+      val threadName = \"read stdout for \" + \"pgrep\"\n+      val childPidsInInt = mutable.ArrayBuffer.empty[Int]\n+      def appendChildPid(s: String): Unit = {\n+        if (s != \"\") {\n+          logDebug(\"Found a child pid:\" + s)\n+          childPidsInInt += Integer.parseInt(s)\n+        }\n+      }\n+      val stdoutThread = Utils.processStreamByLine(threadName,\n+        process.getInputStream, appendChildPid)\n+      val exitCode = process.waitFor()\n+      stdoutThread.join()\n+      // pgrep will have exit code of 1 if there are more than one child process\n+      // and it will have a exit code of 2 if there is no child process\n+      if (exitCode != 0 && exitCode > 2) {\n+        val cmd = builder.command().toArray.mkString(\" \")\n+        logWarning(s\"Process $cmd\" +\n+          s\" exited with code $exitCode, with stderr:\" + s\"${process.getErrorStream} \")\n+        throw new SparkException(s\"Process $cmd exited with code $exitCode\")\n+      }\n+      childPidsInInt\n+    } catch {\n+      case e: Exception => logWarning(\"Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped.\", e)\n+        isAvailable = false\n+        return mutable.ArrayBuffer.empty[Int]\n+    }\n+  }\n+\n+  def computeProcessInfo(pid: Int): Unit = {\n+    /*\n+   * Hadoop ProcfsBasedProcessTree class used regex and pattern matching to retrive the memory\n+   * info. I tried that but found it not correct during tests, so I used normal string analysis\n+   * instead. The computation of RSS and Vmem are based on proc(5):\n+   * http://man7.org/linux/man-pages/man5/proc.5.html\n+   */\n+    try {\n+      val pidDir = new File(procfsDir, pid.toString)\n+      Utils.tryWithResource( new InputStreamReader(\n+        new FileInputStream(\n+          new File(pidDir, procfsStatFile)), Charset.forName(\"UTF-8\"))) { fReader =>\n+        Utils.tryWithResource( new BufferedReader(fReader)) { in =>\n+          val procInfo = in.readLine\n+          val procInfoSplit = procInfo.split(\" \")\n+          if (procInfoSplit != null) {\n+            val vmem = procInfoSplit(22).toLong\n+            val rssPages = procInfoSplit(23).toLong\n+            if (procInfoSplit(1).toLowerCase(Locale.US).contains(\"java\")) {\n+              allMetrics = allMetrics.copy(\n+                jvmVmemTotal = allMetrics.jvmVmemTotal + vmem,\n+                jvmRSSTotal = allMetrics.jvmRSSTotal + (rssPages*pageSize)\n+              )\n+            }\n+            else if (procInfoSplit(1).toLowerCase(Locale.US).contains(\"python\")) {\n+              allMetrics = allMetrics.copy(\n+                pythonVmemTotal = allMetrics.pythonVmemTotal + vmem,\n+                pythonRSSTotal = allMetrics.pythonRSSTotal + (rssPages*pageSize)\n+              )\n+            }\n+            else {\n+              allMetrics = allMetrics.copy(\n+                otherVmemTotal = allMetrics.otherVmemTotal + vmem,\n+                otherRSSTotal = allMetrics.otherRSSTotal + (rssPages*pageSize)\n+              )\n+            }\n+          }\n+        }\n+      }\n+    } catch {\n+      case f: FileNotFoundException => logWarning(\"There was a problem with reading\" +\n+        \" the stat file of the process. \", f)\n+    }\n+  }\n+\n+  private[spark] def computeAllMetrics(): ProcfsBasedSystemsMetrics = {\n+    if (!isAvailable) {\n+      return ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+    }\n+    computeProcessTree\n+    val pids = ptree.keySet\n+    allMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+    for (p <- pids) {\n+      computeProcessInfo(p)"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "delete",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-11-16T20:25:22Z",
    "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+\n+  // var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "not needed anymroe",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-11-16T20:25:28Z",
    "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+\n+  // var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+\n+  computeProcessTree()"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "`pageSize` is only a `var` for testing -- instead just optionally pass it in to the constructor\r\n\r\nalso I think all of these can be `private`.",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-11-16T20:28:06Z",
    "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "I think I can't call computePageSize() in the constructor signature to compute the default value. Another solution is to check for testing inside computePageSize and if we are testing assign a value to it that is provided in the constructor (default to 4096).",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-11-21T21:23:45Z",
    "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()"
  }, {
    "author": {
      "login": "squito"
    },
    "body": "You can't put it as a default value, but if you make it a static method, then you can provide an overloaded method which uses it, see https://github.com/squito/spark/commit/cf008355e8b9ce9faeab267cd0763a3859a5ccc9\r\n\r\nBut, I think your other proposal is even better, if its testing just give it a fixed value (no need to even make it an argument to the constructor at all).",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-11-27T17:23:29Z",
    "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "this can be simplified to\r\n\r\n```scala\r\nval procDirExists = Try(Files.exists(Paths.get(procfsDir)).recover {\r\n  case ioe: IOException =>\r\n    logWarning(\"Exception checking for procfs dir\", f)\r\n    false\r\n}\r\n```\r\n\r\nor maybe even ditch that warning msg completely ...",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-11-16T20:33:51Z",
    "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+\n+  // var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+\n+  computeProcessTree()\n+\n+  private lazy val isProcfsAvailable: Boolean = {\n+    if (testing) {\n+       true\n+    }\n+    else {\n+      var procDirExists = true\n+      try {\n+        if (!Files.exists(Paths.get(procfsDir))) {\n+          procDirExists = false\n+        }\n+      }\n+      catch {\n+        case f: IOException =>\n+          logWarning(\"It seems that procfs isn't supported\", f)\n+          procDirExists = false\n+      }"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "can be `out` instead of `out2`",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-11-16T20:34:37Z",
    "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+\n+  // var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+\n+  computeProcessTree()\n+\n+  private lazy val isProcfsAvailable: Boolean = {\n+    if (testing) {\n+       true\n+    }\n+    else {\n+      var procDirExists = true\n+      try {\n+        if (!Files.exists(Paths.get(procfsDir))) {\n+          procDirExists = false\n+        }\n+      }\n+      catch {\n+        case f: IOException =>\n+          logWarning(\"It seems that procfs isn't supported\", f)\n+          procDirExists = false\n+      }\n+      val shouldLogStageExecutorMetrics =\n+        SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+      val shouldLogStageExecutorProcessTreeMetrics =\n+        SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+      procDirExists && shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+    }\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val out2 = Utils.executeAndGetOutput(cmd)"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "nit: indent the 2nd line. ",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-11-16T20:35:23Z",
    "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+\n+  // var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+\n+  computeProcessTree()\n+\n+  private lazy val isProcfsAvailable: Boolean = {\n+    if (testing) {\n+       true\n+    }\n+    else {\n+      var procDirExists = true\n+      try {\n+        if (!Files.exists(Paths.get(procfsDir))) {\n+          procDirExists = false\n+        }\n+      }\n+      catch {\n+        case f: IOException =>\n+          logWarning(\"It seems that procfs isn't supported\", f)\n+          procDirExists = false\n+      }\n+      val shouldLogStageExecutorMetrics =\n+        SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+      val shouldLogStageExecutorProcessTreeMetrics =\n+        SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+      procDirExists && shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+    }\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      val pid = Integer.parseInt(out2.split(\"\\n\")(0))\n+      return pid;\n+    }\n+    catch {\n+      case e: SparkException =>\n+        logWarning(\"Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "nit: nothing else on line after `=>`, and extra indent for second line of log msg",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-11-16T20:36:14Z",
    "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+\n+  // var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+\n+  computeProcessTree()\n+\n+  private lazy val isProcfsAvailable: Boolean = {\n+    if (testing) {\n+       true\n+    }\n+    else {\n+      var procDirExists = true\n+      try {\n+        if (!Files.exists(Paths.get(procfsDir))) {\n+          procDirExists = false\n+        }\n+      }\n+      catch {\n+        case f: IOException =>\n+          logWarning(\"It seems that procfs isn't supported\", f)\n+          procDirExists = false\n+      }\n+      val shouldLogStageExecutorMetrics =\n+        SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+      val shouldLogStageExecutorProcessTreeMetrics =\n+        SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+      procDirExists && shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+    }\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      val pid = Integer.parseInt(out2.split(\"\\n\")(0))\n+      return pid;\n+    }\n+    catch {\n+      case e: SparkException =>\n+        logWarning(\"Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    if (testing) {\n+      return 0;\n+    }\n+    try {\n+      val cmd = Array(\"getconf\", \"PAGESIZE\")\n+      val out = Utils.executeAndGetOutput(cmd)\n+      return Integer.parseInt(out.split(\"\\n\")(0))\n+    } catch {\n+      case e: Exception => logWarning(\"Exception when trying to compute pagesize, as a\" +\n+        \" result reporting of ProcessTree metrics is stopped\")"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "return isn't necessary here",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-11-16T20:36:40Z",
    "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+\n+  // var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+\n+  computeProcessTree()\n+\n+  private lazy val isProcfsAvailable: Boolean = {\n+    if (testing) {\n+       true\n+    }\n+    else {\n+      var procDirExists = true\n+      try {\n+        if (!Files.exists(Paths.get(procfsDir))) {\n+          procDirExists = false\n+        }\n+      }\n+      catch {\n+        case f: IOException =>\n+          logWarning(\"It seems that procfs isn't supported\", f)\n+          procDirExists = false\n+      }\n+      val shouldLogStageExecutorMetrics =\n+        SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+      val shouldLogStageExecutorProcessTreeMetrics =\n+        SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+      procDirExists && shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+    }\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      val pid = Integer.parseInt(out2.split(\"\\n\")(0))\n+      return pid;\n+    }\n+    catch {\n+      case e: SparkException =>\n+        logWarning(\"Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    if (testing) {\n+      return 0;\n+    }\n+    try {\n+      val cmd = Array(\"getconf\", \"PAGESIZE\")\n+      val out = Utils.executeAndGetOutput(cmd)\n+      return Integer.parseInt(out.split(\"\\n\")(0))"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "return isn't necessary",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-11-16T20:36:48Z",
    "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+\n+  // var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+\n+  computeProcessTree()\n+\n+  private lazy val isProcfsAvailable: Boolean = {\n+    if (testing) {\n+       true\n+    }\n+    else {\n+      var procDirExists = true\n+      try {\n+        if (!Files.exists(Paths.get(procfsDir))) {\n+          procDirExists = false\n+        }\n+      }\n+      catch {\n+        case f: IOException =>\n+          logWarning(\"It seems that procfs isn't supported\", f)\n+          procDirExists = false\n+      }\n+      val shouldLogStageExecutorMetrics =\n+        SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+      val shouldLogStageExecutorProcessTreeMetrics =\n+        SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+      procDirExists && shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+    }\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      val pid = Integer.parseInt(out2.split(\"\\n\")(0))\n+      return pid;\n+    }\n+    catch {\n+      case e: SparkException =>\n+        logWarning(\"Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    if (testing) {\n+      return 0;\n+    }\n+    try {\n+      val cmd = Array(\"getconf\", \"PAGESIZE\")\n+      val out = Utils.executeAndGetOutput(cmd)\n+      return Integer.parseInt(out.split(\"\\n\")(0))\n+    } catch {\n+      case e: Exception => logWarning(\"Exception when trying to compute pagesize, as a\" +\n+        \" result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return 0"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "delete",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-11-16T20:37:09Z",
    "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+\n+  // var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+\n+  computeProcessTree()\n+\n+  private lazy val isProcfsAvailable: Boolean = {\n+    if (testing) {\n+       true\n+    }\n+    else {\n+      var procDirExists = true\n+      try {\n+        if (!Files.exists(Paths.get(procfsDir))) {\n+          procDirExists = false\n+        }\n+      }\n+      catch {\n+        case f: IOException =>\n+          logWarning(\"It seems that procfs isn't supported\", f)\n+          procDirExists = false\n+      }\n+      val shouldLogStageExecutorMetrics =\n+        SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+      val shouldLogStageExecutorProcessTreeMetrics =\n+        SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+      procDirExists && shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+    }\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      val pid = Integer.parseInt(out2.split(\"\\n\")(0))\n+      return pid;\n+    }\n+    catch {\n+      case e: SparkException =>\n+        logWarning(\"Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    if (testing) {\n+      return 0;\n+    }\n+    try {\n+      val cmd = Array(\"getconf\", \"PAGESIZE\")\n+      val out = Utils.executeAndGetOutput(cmd)\n+      return Integer.parseInt(out.split(\"\\n\")(0))\n+    } catch {\n+      case e: Exception => logWarning(\"Exception when trying to compute pagesize, as a\" +\n+        \" result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return 0\n+    }\n+  }\n+\n+  private def computeProcessTree(): Set[Int] = {\n+    if (!isAvailable || testing) {\n+      return Set()\n+    }\n+    var ptree: Set[Int] = Set()\n+    ptree += pid\n+    val queue = mutable.Queue.empty[Int]\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPids(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree ++= c.toSet\n+      }\n+    }\n+    ptree\n+  }\n+\n+  private def getChildPids(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      // val cmd = Array(\"pgrep\", \"-P\", pid.toString)"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "I dont' think `process.getErrorStream` will have a useful `toString`.  I think you need to read all the data.  You probably also have to do that before `process.waitFor()`, otherwise I think its possible that the process blocks forever waiting for something to read stderr.",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-11-16T20:43:28Z",
    "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+\n+  // var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+\n+  computeProcessTree()\n+\n+  private lazy val isProcfsAvailable: Boolean = {\n+    if (testing) {\n+       true\n+    }\n+    else {\n+      var procDirExists = true\n+      try {\n+        if (!Files.exists(Paths.get(procfsDir))) {\n+          procDirExists = false\n+        }\n+      }\n+      catch {\n+        case f: IOException =>\n+          logWarning(\"It seems that procfs isn't supported\", f)\n+          procDirExists = false\n+      }\n+      val shouldLogStageExecutorMetrics =\n+        SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+      val shouldLogStageExecutorProcessTreeMetrics =\n+        SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+      procDirExists && shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+    }\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      val pid = Integer.parseInt(out2.split(\"\\n\")(0))\n+      return pid;\n+    }\n+    catch {\n+      case e: SparkException =>\n+        logWarning(\"Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    if (testing) {\n+      return 0;\n+    }\n+    try {\n+      val cmd = Array(\"getconf\", \"PAGESIZE\")\n+      val out = Utils.executeAndGetOutput(cmd)\n+      return Integer.parseInt(out.split(\"\\n\")(0))\n+    } catch {\n+      case e: Exception => logWarning(\"Exception when trying to compute pagesize, as a\" +\n+        \" result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return 0\n+    }\n+  }\n+\n+  private def computeProcessTree(): Set[Int] = {\n+    if (!isAvailable || testing) {\n+      return Set()\n+    }\n+    var ptree: Set[Int] = Set()\n+    ptree += pid\n+    val queue = mutable.Queue.empty[Int]\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPids(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree ++= c.toSet\n+      }\n+    }\n+    ptree\n+  }\n+\n+  private def getChildPids(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      // val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val builder = new ProcessBuilder(\"pgrep\", \"-P\", pid.toString)\n+      val process = builder.start()\n+      // val output = new StringBuilder()\n+      val threadName = \"read stdout for \" + \"pgrep\"\n+      val childPidsInInt = mutable.ArrayBuffer.empty[Int]\n+      def appendChildPid(s: String): Unit = {\n+        if (s != \"\") {\n+          logDebug(\"Found a child pid:\" + s)\n+          childPidsInInt += Integer.parseInt(s)\n+        }\n+      }\n+      val stdoutThread = Utils.processStreamByLine(threadName,\n+        process.getInputStream, appendChildPid)\n+      val exitCode = process.waitFor()\n+      stdoutThread.join()\n+      // pgrep will have exit code of 1 if there are more than one child process\n+      // and it will have a exit code of 2 if there is no child process\n+      if (exitCode != 0 && exitCode > 2) {\n+        val cmd = builder.command().toArray.mkString(\" \")\n+        logWarning(s\"Process $cmd\" +\n+          s\" exited with code $exitCode, with stderr:\" + s\"${process.getErrorStream} \")"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "I changed this in the new patch, but not sure if I addressed your concern. Please let me know",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-11-27T20:11:27Z",
    "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+\n+  // var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+\n+  computeProcessTree()\n+\n+  private lazy val isProcfsAvailable: Boolean = {\n+    if (testing) {\n+       true\n+    }\n+    else {\n+      var procDirExists = true\n+      try {\n+        if (!Files.exists(Paths.get(procfsDir))) {\n+          procDirExists = false\n+        }\n+      }\n+      catch {\n+        case f: IOException =>\n+          logWarning(\"It seems that procfs isn't supported\", f)\n+          procDirExists = false\n+      }\n+      val shouldLogStageExecutorMetrics =\n+        SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+      val shouldLogStageExecutorProcessTreeMetrics =\n+        SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+      procDirExists && shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+    }\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      val pid = Integer.parseInt(out2.split(\"\\n\")(0))\n+      return pid;\n+    }\n+    catch {\n+      case e: SparkException =>\n+        logWarning(\"Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    if (testing) {\n+      return 0;\n+    }\n+    try {\n+      val cmd = Array(\"getconf\", \"PAGESIZE\")\n+      val out = Utils.executeAndGetOutput(cmd)\n+      return Integer.parseInt(out.split(\"\\n\")(0))\n+    } catch {\n+      case e: Exception => logWarning(\"Exception when trying to compute pagesize, as a\" +\n+        \" result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return 0\n+    }\n+  }\n+\n+  private def computeProcessTree(): Set[Int] = {\n+    if (!isAvailable || testing) {\n+      return Set()\n+    }\n+    var ptree: Set[Int] = Set()\n+    ptree += pid\n+    val queue = mutable.Queue.empty[Int]\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPids(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree ++= c.toSet\n+      }\n+    }\n+    ptree\n+  }\n+\n+  private def getChildPids(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      // val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val builder = new ProcessBuilder(\"pgrep\", \"-P\", pid.toString)\n+      val process = builder.start()\n+      // val output = new StringBuilder()\n+      val threadName = \"read stdout for \" + \"pgrep\"\n+      val childPidsInInt = mutable.ArrayBuffer.empty[Int]\n+      def appendChildPid(s: String): Unit = {\n+        if (s != \"\") {\n+          logDebug(\"Found a child pid:\" + s)\n+          childPidsInInt += Integer.parseInt(s)\n+        }\n+      }\n+      val stdoutThread = Utils.processStreamByLine(threadName,\n+        process.getInputStream, appendChildPid)\n+      val exitCode = process.waitFor()\n+      stdoutThread.join()\n+      // pgrep will have exit code of 1 if there are more than one child process\n+      // and it will have a exit code of 2 if there is no child process\n+      if (exitCode != 0 && exitCode > 2) {\n+        val cmd = builder.command().toArray.mkString(\" \")\n+        logWarning(s\"Process $cmd\" +\n+          s\" exited with code $exitCode, with stderr:\" + s\"${process.getErrorStream} \")"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "seems like too much logging even for debug level -- should it be logTrace?",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-11-16T20:44:05Z",
    "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+\n+  // var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+\n+  computeProcessTree()\n+\n+  private lazy val isProcfsAvailable: Boolean = {\n+    if (testing) {\n+       true\n+    }\n+    else {\n+      var procDirExists = true\n+      try {\n+        if (!Files.exists(Paths.get(procfsDir))) {\n+          procDirExists = false\n+        }\n+      }\n+      catch {\n+        case f: IOException =>\n+          logWarning(\"It seems that procfs isn't supported\", f)\n+          procDirExists = false\n+      }\n+      val shouldLogStageExecutorMetrics =\n+        SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+      val shouldLogStageExecutorProcessTreeMetrics =\n+        SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+      procDirExists && shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+    }\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      val pid = Integer.parseInt(out2.split(\"\\n\")(0))\n+      return pid;\n+    }\n+    catch {\n+      case e: SparkException =>\n+        logWarning(\"Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    if (testing) {\n+      return 0;\n+    }\n+    try {\n+      val cmd = Array(\"getconf\", \"PAGESIZE\")\n+      val out = Utils.executeAndGetOutput(cmd)\n+      return Integer.parseInt(out.split(\"\\n\")(0))\n+    } catch {\n+      case e: Exception => logWarning(\"Exception when trying to compute pagesize, as a\" +\n+        \" result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return 0\n+    }\n+  }\n+\n+  private def computeProcessTree(): Set[Int] = {\n+    if (!isAvailable || testing) {\n+      return Set()\n+    }\n+    var ptree: Set[Int] = Set()\n+    ptree += pid\n+    val queue = mutable.Queue.empty[Int]\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPids(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree ++= c.toSet\n+      }\n+    }\n+    ptree\n+  }\n+\n+  private def getChildPids(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      // val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val builder = new ProcessBuilder(\"pgrep\", \"-P\", pid.toString)\n+      val process = builder.start()\n+      // val output = new StringBuilder()\n+      val threadName = \"read stdout for \" + \"pgrep\"\n+      val childPidsInInt = mutable.ArrayBuffer.empty[Int]\n+      def appendChildPid(s: String): Unit = {\n+        if (s != \"\") {\n+          logDebug(\"Found a child pid:\" + s)"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "you could inline `threadName` here (and skip the extra `+` on the string)",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-11-16T20:46:37Z",
    "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+\n+  // var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+\n+  computeProcessTree()\n+\n+  private lazy val isProcfsAvailable: Boolean = {\n+    if (testing) {\n+       true\n+    }\n+    else {\n+      var procDirExists = true\n+      try {\n+        if (!Files.exists(Paths.get(procfsDir))) {\n+          procDirExists = false\n+        }\n+      }\n+      catch {\n+        case f: IOException =>\n+          logWarning(\"It seems that procfs isn't supported\", f)\n+          procDirExists = false\n+      }\n+      val shouldLogStageExecutorMetrics =\n+        SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+      val shouldLogStageExecutorProcessTreeMetrics =\n+        SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+      procDirExists && shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+    }\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      val pid = Integer.parseInt(out2.split(\"\\n\")(0))\n+      return pid;\n+    }\n+    catch {\n+      case e: SparkException =>\n+        logWarning(\"Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    if (testing) {\n+      return 0;\n+    }\n+    try {\n+      val cmd = Array(\"getconf\", \"PAGESIZE\")\n+      val out = Utils.executeAndGetOutput(cmd)\n+      return Integer.parseInt(out.split(\"\\n\")(0))\n+    } catch {\n+      case e: Exception => logWarning(\"Exception when trying to compute pagesize, as a\" +\n+        \" result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return 0\n+    }\n+  }\n+\n+  private def computeProcessTree(): Set[Int] = {\n+    if (!isAvailable || testing) {\n+      return Set()\n+    }\n+    var ptree: Set[Int] = Set()\n+    ptree += pid\n+    val queue = mutable.Queue.empty[Int]\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPids(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree ++= c.toSet\n+      }\n+    }\n+    ptree\n+  }\n+\n+  private def getChildPids(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      // val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val builder = new ProcessBuilder(\"pgrep\", \"-P\", pid.toString)\n+      val process = builder.start()\n+      // val output = new StringBuilder()\n+      val threadName = \"read stdout for \" + \"pgrep\"\n+      val childPidsInInt = mutable.ArrayBuffer.empty[Int]\n+      def appendChildPid(s: String): Unit = {\n+        if (s != \"\") {\n+          logDebug(\"Found a child pid:\" + s)\n+          childPidsInInt += Integer.parseInt(s)\n+        }\n+      }\n+      val stdoutThread = Utils.processStreamByLine(threadName,"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "nit: if the method signature is multiline, each arg on its own line, double-indented",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-11-16T20:46:57Z",
    "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+\n+  // var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+\n+  computeProcessTree()\n+\n+  private lazy val isProcfsAvailable: Boolean = {\n+    if (testing) {\n+       true\n+    }\n+    else {\n+      var procDirExists = true\n+      try {\n+        if (!Files.exists(Paths.get(procfsDir))) {\n+          procDirExists = false\n+        }\n+      }\n+      catch {\n+        case f: IOException =>\n+          logWarning(\"It seems that procfs isn't supported\", f)\n+          procDirExists = false\n+      }\n+      val shouldLogStageExecutorMetrics =\n+        SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+      val shouldLogStageExecutorProcessTreeMetrics =\n+        SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+      procDirExists && shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+    }\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      val pid = Integer.parseInt(out2.split(\"\\n\")(0))\n+      return pid;\n+    }\n+    catch {\n+      case e: SparkException =>\n+        logWarning(\"Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    if (testing) {\n+      return 0;\n+    }\n+    try {\n+      val cmd = Array(\"getconf\", \"PAGESIZE\")\n+      val out = Utils.executeAndGetOutput(cmd)\n+      return Integer.parseInt(out.split(\"\\n\")(0))\n+    } catch {\n+      case e: Exception => logWarning(\"Exception when trying to compute pagesize, as a\" +\n+        \" result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return 0\n+    }\n+  }\n+\n+  private def computeProcessTree(): Set[Int] = {\n+    if (!isAvailable || testing) {\n+      return Set()\n+    }\n+    var ptree: Set[Int] = Set()\n+    ptree += pid\n+    val queue = mutable.Queue.empty[Int]\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPids(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree ++= c.toSet\n+      }\n+    }\n+    ptree\n+  }\n+\n+  private def getChildPids(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      // val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val builder = new ProcessBuilder(\"pgrep\", \"-P\", pid.toString)\n+      val process = builder.start()\n+      // val output = new StringBuilder()\n+      val threadName = \"read stdout for \" + \"pgrep\"\n+      val childPidsInInt = mutable.ArrayBuffer.empty[Int]\n+      def appendChildPid(s: String): Unit = {\n+        if (s != \"\") {\n+          logDebug(\"Found a child pid:\" + s)\n+          childPidsInInt += Integer.parseInt(s)\n+        }\n+      }\n+      val stdoutThread = Utils.processStreamByLine(threadName,\n+        process.getInputStream, appendChildPid)\n+      val exitCode = process.waitFor()\n+      stdoutThread.join()\n+      // pgrep will have exit code of 1 if there are more than one child process\n+      // and it will have a exit code of 2 if there is no child process\n+      if (exitCode != 0 && exitCode > 2) {\n+        val cmd = builder.command().toArray.mkString(\" \")\n+        logWarning(s\"Process $cmd\" +\n+          s\" exited with code $exitCode, with stderr:\" + s\"${process.getErrorStream} \")\n+        throw new SparkException(s\"Process $cmd exited with code $exitCode\")\n+      }\n+      childPidsInInt\n+    } catch {\n+      case e: Exception => logWarning(\"Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped.\", e)\n+        isAvailable = false\n+        return mutable.ArrayBuffer.empty[Int]\n+    }\n+  }\n+\n+  def computeProcessInfo(allMetrics: ProcfsBasedSystemsMetrics, pid: Int):\n+  ProcfsBasedSystemsMetrics = {"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "indentation is still off here.",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-11-16T20:47:27Z",
    "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+\n+  // var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+\n+  computeProcessTree()\n+\n+  private lazy val isProcfsAvailable: Boolean = {\n+    if (testing) {\n+       true\n+    }\n+    else {\n+      var procDirExists = true\n+      try {\n+        if (!Files.exists(Paths.get(procfsDir))) {\n+          procDirExists = false\n+        }\n+      }\n+      catch {\n+        case f: IOException =>\n+          logWarning(\"It seems that procfs isn't supported\", f)\n+          procDirExists = false\n+      }\n+      val shouldLogStageExecutorMetrics =\n+        SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+      val shouldLogStageExecutorProcessTreeMetrics =\n+        SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+      procDirExists && shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+    }\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      val pid = Integer.parseInt(out2.split(\"\\n\")(0))\n+      return pid;\n+    }\n+    catch {\n+      case e: SparkException =>\n+        logWarning(\"Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    if (testing) {\n+      return 0;\n+    }\n+    try {\n+      val cmd = Array(\"getconf\", \"PAGESIZE\")\n+      val out = Utils.executeAndGetOutput(cmd)\n+      return Integer.parseInt(out.split(\"\\n\")(0))\n+    } catch {\n+      case e: Exception => logWarning(\"Exception when trying to compute pagesize, as a\" +\n+        \" result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return 0\n+    }\n+  }\n+\n+  private def computeProcessTree(): Set[Int] = {\n+    if (!isAvailable || testing) {\n+      return Set()\n+    }\n+    var ptree: Set[Int] = Set()\n+    ptree += pid\n+    val queue = mutable.Queue.empty[Int]\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPids(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree ++= c.toSet\n+      }\n+    }\n+    ptree\n+  }\n+\n+  private def getChildPids(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      // val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val builder = new ProcessBuilder(\"pgrep\", \"-P\", pid.toString)\n+      val process = builder.start()\n+      // val output = new StringBuilder()\n+      val threadName = \"read stdout for \" + \"pgrep\"\n+      val childPidsInInt = mutable.ArrayBuffer.empty[Int]\n+      def appendChildPid(s: String): Unit = {\n+        if (s != \"\") {\n+          logDebug(\"Found a child pid:\" + s)\n+          childPidsInInt += Integer.parseInt(s)\n+        }\n+      }\n+      val stdoutThread = Utils.processStreamByLine(threadName,\n+        process.getInputStream, appendChildPid)\n+      val exitCode = process.waitFor()\n+      stdoutThread.join()\n+      // pgrep will have exit code of 1 if there are more than one child process\n+      // and it will have a exit code of 2 if there is no child process\n+      if (exitCode != 0 && exitCode > 2) {\n+        val cmd = builder.command().toArray.mkString(\" \")\n+        logWarning(s\"Process $cmd\" +\n+          s\" exited with code $exitCode, with stderr:\" + s\"${process.getErrorStream} \")\n+        throw new SparkException(s\"Process $cmd exited with code $exitCode\")\n+      }\n+      childPidsInInt\n+    } catch {\n+      case e: Exception => logWarning(\"Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped.\", e)\n+        isAvailable = false\n+        return mutable.ArrayBuffer.empty[Int]\n+    }\n+  }\n+\n+  def computeProcessInfo(allMetrics: ProcfsBasedSystemsMetrics, pid: Int):\n+  ProcfsBasedSystemsMetrics = {\n+  /*\n+   * Hadoop ProcfsBasedProcessTree class used regex and pattern matching to retrive the memory\n+   * info. I tried that but found it not correct during tests, so I used normal string analysis\n+   * instead. The computation of RSS and Vmem are based on proc(5):\n+   * http://man7.org/linux/man-pages/man5/proc.5.html\n+   */"
  }, {
    "author": {
      "login": "rezasafi"
    },
    "body": "I'm not sure how should be indentation here since it is just a multil line comment and not a scala doc",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-11-19T23:04:05Z",
    "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+\n+  // var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+\n+  computeProcessTree()\n+\n+  private lazy val isProcfsAvailable: Boolean = {\n+    if (testing) {\n+       true\n+    }\n+    else {\n+      var procDirExists = true\n+      try {\n+        if (!Files.exists(Paths.get(procfsDir))) {\n+          procDirExists = false\n+        }\n+      }\n+      catch {\n+        case f: IOException =>\n+          logWarning(\"It seems that procfs isn't supported\", f)\n+          procDirExists = false\n+      }\n+      val shouldLogStageExecutorMetrics =\n+        SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+      val shouldLogStageExecutorProcessTreeMetrics =\n+        SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+      procDirExists && shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+    }\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      val pid = Integer.parseInt(out2.split(\"\\n\")(0))\n+      return pid;\n+    }\n+    catch {\n+      case e: SparkException =>\n+        logWarning(\"Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    if (testing) {\n+      return 0;\n+    }\n+    try {\n+      val cmd = Array(\"getconf\", \"PAGESIZE\")\n+      val out = Utils.executeAndGetOutput(cmd)\n+      return Integer.parseInt(out.split(\"\\n\")(0))\n+    } catch {\n+      case e: Exception => logWarning(\"Exception when trying to compute pagesize, as a\" +\n+        \" result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return 0\n+    }\n+  }\n+\n+  private def computeProcessTree(): Set[Int] = {\n+    if (!isAvailable || testing) {\n+      return Set()\n+    }\n+    var ptree: Set[Int] = Set()\n+    ptree += pid\n+    val queue = mutable.Queue.empty[Int]\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPids(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree ++= c.toSet\n+      }\n+    }\n+    ptree\n+  }\n+\n+  private def getChildPids(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      // val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val builder = new ProcessBuilder(\"pgrep\", \"-P\", pid.toString)\n+      val process = builder.start()\n+      // val output = new StringBuilder()\n+      val threadName = \"read stdout for \" + \"pgrep\"\n+      val childPidsInInt = mutable.ArrayBuffer.empty[Int]\n+      def appendChildPid(s: String): Unit = {\n+        if (s != \"\") {\n+          logDebug(\"Found a child pid:\" + s)\n+          childPidsInInt += Integer.parseInt(s)\n+        }\n+      }\n+      val stdoutThread = Utils.processStreamByLine(threadName,\n+        process.getInputStream, appendChildPid)\n+      val exitCode = process.waitFor()\n+      stdoutThread.join()\n+      // pgrep will have exit code of 1 if there are more than one child process\n+      // and it will have a exit code of 2 if there is no child process\n+      if (exitCode != 0 && exitCode > 2) {\n+        val cmd = builder.command().toArray.mkString(\" \")\n+        logWarning(s\"Process $cmd\" +\n+          s\" exited with code $exitCode, with stderr:\" + s\"${process.getErrorStream} \")\n+        throw new SparkException(s\"Process $cmd exited with code $exitCode\")\n+      }\n+      childPidsInInt\n+    } catch {\n+      case e: Exception => logWarning(\"Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped.\", e)\n+        isAvailable = false\n+        return mutable.ArrayBuffer.empty[Int]\n+    }\n+  }\n+\n+  def computeProcessInfo(allMetrics: ProcfsBasedSystemsMetrics, pid: Int):\n+  ProcfsBasedSystemsMetrics = {\n+  /*\n+   * Hadoop ProcfsBasedProcessTree class used regex and pattern matching to retrive the memory\n+   * info. I tried that but found it not correct during tests, so I used normal string analysis\n+   * instead. The computation of RSS and Vmem are based on proc(5):\n+   * http://man7.org/linux/man-pages/man5/proc.5.html\n+   */"
  }, {
    "author": {
      "login": "squito"
    },
    "body": "I mean the comment should be indented the same as the rest of the body of method.  Eg the very first '/' needs to be indented to the same level as the 'try' just after this comment.  And the rest of the comment should be indented like that slash.\r\n\r\nAlternatively, just prefix each line with '//'.  Same indentation rule, but indentation might look better, and I guess is actually what is done more in spark code ( eg. https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/objects.scala#L47-L51)",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-11-20T18:44:52Z",
    "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+\n+  // var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+\n+  computeProcessTree()\n+\n+  private lazy val isProcfsAvailable: Boolean = {\n+    if (testing) {\n+       true\n+    }\n+    else {\n+      var procDirExists = true\n+      try {\n+        if (!Files.exists(Paths.get(procfsDir))) {\n+          procDirExists = false\n+        }\n+      }\n+      catch {\n+        case f: IOException =>\n+          logWarning(\"It seems that procfs isn't supported\", f)\n+          procDirExists = false\n+      }\n+      val shouldLogStageExecutorMetrics =\n+        SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+      val shouldLogStageExecutorProcessTreeMetrics =\n+        SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+      procDirExists && shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+    }\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      val pid = Integer.parseInt(out2.split(\"\\n\")(0))\n+      return pid;\n+    }\n+    catch {\n+      case e: SparkException =>\n+        logWarning(\"Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    if (testing) {\n+      return 0;\n+    }\n+    try {\n+      val cmd = Array(\"getconf\", \"PAGESIZE\")\n+      val out = Utils.executeAndGetOutput(cmd)\n+      return Integer.parseInt(out.split(\"\\n\")(0))\n+    } catch {\n+      case e: Exception => logWarning(\"Exception when trying to compute pagesize, as a\" +\n+        \" result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return 0\n+    }\n+  }\n+\n+  private def computeProcessTree(): Set[Int] = {\n+    if (!isAvailable || testing) {\n+      return Set()\n+    }\n+    var ptree: Set[Int] = Set()\n+    ptree += pid\n+    val queue = mutable.Queue.empty[Int]\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPids(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree ++= c.toSet\n+      }\n+    }\n+    ptree\n+  }\n+\n+  private def getChildPids(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      // val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val builder = new ProcessBuilder(\"pgrep\", \"-P\", pid.toString)\n+      val process = builder.start()\n+      // val output = new StringBuilder()\n+      val threadName = \"read stdout for \" + \"pgrep\"\n+      val childPidsInInt = mutable.ArrayBuffer.empty[Int]\n+      def appendChildPid(s: String): Unit = {\n+        if (s != \"\") {\n+          logDebug(\"Found a child pid:\" + s)\n+          childPidsInInt += Integer.parseInt(s)\n+        }\n+      }\n+      val stdoutThread = Utils.processStreamByLine(threadName,\n+        process.getInputStream, appendChildPid)\n+      val exitCode = process.waitFor()\n+      stdoutThread.join()\n+      // pgrep will have exit code of 1 if there are more than one child process\n+      // and it will have a exit code of 2 if there is no child process\n+      if (exitCode != 0 && exitCode > 2) {\n+        val cmd = builder.command().toArray.mkString(\" \")\n+        logWarning(s\"Process $cmd\" +\n+          s\" exited with code $exitCode, with stderr:\" + s\"${process.getErrorStream} \")\n+        throw new SparkException(s\"Process $cmd exited with code $exitCode\")\n+      }\n+      childPidsInInt\n+    } catch {\n+      case e: Exception => logWarning(\"Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped.\", e)\n+        isAvailable = false\n+        return mutable.ArrayBuffer.empty[Int]\n+    }\n+  }\n+\n+  def computeProcessInfo(allMetrics: ProcfsBasedSystemsMetrics, pid: Int):\n+  ProcfsBasedSystemsMetrics = {\n+  /*\n+   * Hadoop ProcfsBasedProcessTree class used regex and pattern matching to retrive the memory\n+   * info. I tried that but found it not correct during tests, so I used normal string analysis\n+   * instead. The computation of RSS and Vmem are based on proc(5):\n+   * http://man7.org/linux/man-pages/man5/proc.5.html\n+   */"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "nit: nothing after the =>, indent the continuation of the log msg",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-11-16T20:48:49Z",
    "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+\n+  // var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+\n+  computeProcessTree()\n+\n+  private lazy val isProcfsAvailable: Boolean = {\n+    if (testing) {\n+       true\n+    }\n+    else {\n+      var procDirExists = true\n+      try {\n+        if (!Files.exists(Paths.get(procfsDir))) {\n+          procDirExists = false\n+        }\n+      }\n+      catch {\n+        case f: IOException =>\n+          logWarning(\"It seems that procfs isn't supported\", f)\n+          procDirExists = false\n+      }\n+      val shouldLogStageExecutorMetrics =\n+        SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+      val shouldLogStageExecutorProcessTreeMetrics =\n+        SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+      procDirExists && shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+    }\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      val pid = Integer.parseInt(out2.split(\"\\n\")(0))\n+      return pid;\n+    }\n+    catch {\n+      case e: SparkException =>\n+        logWarning(\"Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    if (testing) {\n+      return 0;\n+    }\n+    try {\n+      val cmd = Array(\"getconf\", \"PAGESIZE\")\n+      val out = Utils.executeAndGetOutput(cmd)\n+      return Integer.parseInt(out.split(\"\\n\")(0))\n+    } catch {\n+      case e: Exception => logWarning(\"Exception when trying to compute pagesize, as a\" +\n+        \" result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return 0\n+    }\n+  }\n+\n+  private def computeProcessTree(): Set[Int] = {\n+    if (!isAvailable || testing) {\n+      return Set()\n+    }\n+    var ptree: Set[Int] = Set()\n+    ptree += pid\n+    val queue = mutable.Queue.empty[Int]\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPids(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree ++= c.toSet\n+      }\n+    }\n+    ptree\n+  }\n+\n+  private def getChildPids(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      // val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val builder = new ProcessBuilder(\"pgrep\", \"-P\", pid.toString)\n+      val process = builder.start()\n+      // val output = new StringBuilder()\n+      val threadName = \"read stdout for \" + \"pgrep\"\n+      val childPidsInInt = mutable.ArrayBuffer.empty[Int]\n+      def appendChildPid(s: String): Unit = {\n+        if (s != \"\") {\n+          logDebug(\"Found a child pid:\" + s)\n+          childPidsInInt += Integer.parseInt(s)\n+        }\n+      }\n+      val stdoutThread = Utils.processStreamByLine(threadName,\n+        process.getInputStream, appendChildPid)\n+      val exitCode = process.waitFor()\n+      stdoutThread.join()\n+      // pgrep will have exit code of 1 if there are more than one child process\n+      // and it will have a exit code of 2 if there is no child process\n+      if (exitCode != 0 && exitCode > 2) {\n+        val cmd = builder.command().toArray.mkString(\" \")\n+        logWarning(s\"Process $cmd\" +\n+          s\" exited with code $exitCode, with stderr:\" + s\"${process.getErrorStream} \")\n+        throw new SparkException(s\"Process $cmd exited with code $exitCode\")\n+      }\n+      childPidsInInt\n+    } catch {\n+      case e: Exception => logWarning(\"Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped.\", e)\n+        isAvailable = false\n+        return mutable.ArrayBuffer.empty[Int]\n+    }\n+  }\n+\n+  def computeProcessInfo(allMetrics: ProcfsBasedSystemsMetrics, pid: Int):\n+  ProcfsBasedSystemsMetrics = {\n+  /*\n+   * Hadoop ProcfsBasedProcessTree class used regex and pattern matching to retrive the memory\n+   * info. I tried that but found it not correct during tests, so I used normal string analysis\n+   * instead. The computation of RSS and Vmem are based on proc(5):\n+   * http://man7.org/linux/man-pages/man5/proc.5.html\n+   */\n+    try {\n+      val pidDir = new File(procfsDir, pid.toString)\n+      var allMetricsUpdated = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+      Utils.tryWithResource( new InputStreamReader(\n+        new FileInputStream(\n+          new File(pidDir, procfsStatFile)), Charset.forName(\"UTF-8\"))) { fReader =>\n+        Utils.tryWithResource( new BufferedReader(fReader)) { in =>\n+          val procInfo = in.readLine\n+          val procInfoSplit = procInfo.split(\" \")\n+          if (procInfoSplit != null) {\n+            val vmem = procInfoSplit(22).toLong\n+            val rssPages = procInfoSplit(23).toLong\n+            if (procInfoSplit(1).toLowerCase(Locale.US).contains(\"java\")) {\n+              allMetricsUpdated = allMetrics.copy(\n+                jvmVmemTotal = allMetrics.jvmVmemTotal + vmem,\n+                jvmRSSTotal = allMetrics.jvmRSSTotal + (rssPages*pageSize)\n+              )\n+            }\n+            else if (procInfoSplit(1).toLowerCase(Locale.US).contains(\"python\")) {\n+              allMetricsUpdated = allMetrics.copy(\n+                pythonVmemTotal = allMetrics.pythonVmemTotal + vmem,\n+                pythonRSSTotal = allMetrics.pythonRSSTotal + (rssPages*pageSize)\n+              )\n+            }\n+            else {\n+              allMetricsUpdated = allMetrics.copy(\n+                otherVmemTotal = allMetrics.otherVmemTotal + vmem,\n+                otherRSSTotal = allMetrics.otherRSSTotal + (rssPages*pageSize)\n+              )\n+            }\n+          }\n+        }\n+      }\n+      allMetricsUpdated\n+    } catch {\n+      case f: FileNotFoundException => logWarning(\"There was a problem with reading\" +\n+        \" the stat file of the process. \", f)"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "you don't actually need this at all -- just directly return the result of your update from `allMetrics.copy(...)`",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-11-16T20:49:53Z",
    "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+\n+  // var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+\n+  computeProcessTree()\n+\n+  private lazy val isProcfsAvailable: Boolean = {\n+    if (testing) {\n+       true\n+    }\n+    else {\n+      var procDirExists = true\n+      try {\n+        if (!Files.exists(Paths.get(procfsDir))) {\n+          procDirExists = false\n+        }\n+      }\n+      catch {\n+        case f: IOException =>\n+          logWarning(\"It seems that procfs isn't supported\", f)\n+          procDirExists = false\n+      }\n+      val shouldLogStageExecutorMetrics =\n+        SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+      val shouldLogStageExecutorProcessTreeMetrics =\n+        SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+      procDirExists && shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+    }\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      val pid = Integer.parseInt(out2.split(\"\\n\")(0))\n+      return pid;\n+    }\n+    catch {\n+      case e: SparkException =>\n+        logWarning(\"Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    if (testing) {\n+      return 0;\n+    }\n+    try {\n+      val cmd = Array(\"getconf\", \"PAGESIZE\")\n+      val out = Utils.executeAndGetOutput(cmd)\n+      return Integer.parseInt(out.split(\"\\n\")(0))\n+    } catch {\n+      case e: Exception => logWarning(\"Exception when trying to compute pagesize, as a\" +\n+        \" result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return 0\n+    }\n+  }\n+\n+  private def computeProcessTree(): Set[Int] = {\n+    if (!isAvailable || testing) {\n+      return Set()\n+    }\n+    var ptree: Set[Int] = Set()\n+    ptree += pid\n+    val queue = mutable.Queue.empty[Int]\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPids(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree ++= c.toSet\n+      }\n+    }\n+    ptree\n+  }\n+\n+  private def getChildPids(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      // val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val builder = new ProcessBuilder(\"pgrep\", \"-P\", pid.toString)\n+      val process = builder.start()\n+      // val output = new StringBuilder()\n+      val threadName = \"read stdout for \" + \"pgrep\"\n+      val childPidsInInt = mutable.ArrayBuffer.empty[Int]\n+      def appendChildPid(s: String): Unit = {\n+        if (s != \"\") {\n+          logDebug(\"Found a child pid:\" + s)\n+          childPidsInInt += Integer.parseInt(s)\n+        }\n+      }\n+      val stdoutThread = Utils.processStreamByLine(threadName,\n+        process.getInputStream, appendChildPid)\n+      val exitCode = process.waitFor()\n+      stdoutThread.join()\n+      // pgrep will have exit code of 1 if there are more than one child process\n+      // and it will have a exit code of 2 if there is no child process\n+      if (exitCode != 0 && exitCode > 2) {\n+        val cmd = builder.command().toArray.mkString(\" \")\n+        logWarning(s\"Process $cmd\" +\n+          s\" exited with code $exitCode, with stderr:\" + s\"${process.getErrorStream} \")\n+        throw new SparkException(s\"Process $cmd exited with code $exitCode\")\n+      }\n+      childPidsInInt\n+    } catch {\n+      case e: Exception => logWarning(\"Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped.\", e)\n+        isAvailable = false\n+        return mutable.ArrayBuffer.empty[Int]\n+    }\n+  }\n+\n+  def computeProcessInfo(allMetrics: ProcfsBasedSystemsMetrics, pid: Int):\n+  ProcfsBasedSystemsMetrics = {\n+  /*\n+   * Hadoop ProcfsBasedProcessTree class used regex and pattern matching to retrive the memory\n+   * info. I tried that but found it not correct during tests, so I used normal string analysis\n+   * instead. The computation of RSS and Vmem are based on proc(5):\n+   * http://man7.org/linux/man-pages/man5/proc.5.html\n+   */\n+    try {\n+      val pidDir = new File(procfsDir, pid.toString)\n+      var allMetricsUpdated = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "I think now this method would be better called something like `addProcfsMetricsFromOneProcess`",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-11-16T20:53:35Z",
    "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {\n+  val procfsStatFile = \"stat\"\n+  val testing = sys.env.contains(\"SPARK_TESTING\") || sys.props.contains(\"spark.testing\")\n+  var pageSize = computePageSize()\n+  var isAvailable: Boolean = isProcfsAvailable\n+  private val pid = computePid()\n+\n+  // var allMetrics: ProcfsBasedSystemsMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+\n+  computeProcessTree()\n+\n+  private lazy val isProcfsAvailable: Boolean = {\n+    if (testing) {\n+       true\n+    }\n+    else {\n+      var procDirExists = true\n+      try {\n+        if (!Files.exists(Paths.get(procfsDir))) {\n+          procDirExists = false\n+        }\n+      }\n+      catch {\n+        case f: IOException =>\n+          logWarning(\"It seems that procfs isn't supported\", f)\n+          procDirExists = false\n+      }\n+      val shouldLogStageExecutorMetrics =\n+        SparkEnv.get.conf.get(config.EVENT_LOG_STAGE_EXECUTOR_METRICS)\n+      val shouldLogStageExecutorProcessTreeMetrics =\n+        SparkEnv.get.conf.get(config.EVENT_LOG_PROCESS_TREE_METRICS)\n+      procDirExists && shouldLogStageExecutorProcessTreeMetrics && shouldLogStageExecutorMetrics\n+    }\n+  }\n+\n+  private def computePid(): Int = {\n+    if (!isAvailable || testing) {\n+      return -1;\n+    }\n+    try {\n+      // This can be simplified in java9:\n+      // https://docs.oracle.com/javase/9/docs/api/java/lang/ProcessHandle.html\n+      val cmd = Array(\"bash\", \"-c\", \"echo $PPID\")\n+      val out2 = Utils.executeAndGetOutput(cmd)\n+      val pid = Integer.parseInt(out2.split(\"\\n\")(0))\n+      return pid;\n+    }\n+    catch {\n+      case e: SparkException =>\n+        logWarning(\"Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped\", e)\n+        isAvailable = false\n+        return -1\n+    }\n+  }\n+\n+  private def computePageSize(): Long = {\n+    if (testing) {\n+      return 0;\n+    }\n+    try {\n+      val cmd = Array(\"getconf\", \"PAGESIZE\")\n+      val out = Utils.executeAndGetOutput(cmd)\n+      return Integer.parseInt(out.split(\"\\n\")(0))\n+    } catch {\n+      case e: Exception => logWarning(\"Exception when trying to compute pagesize, as a\" +\n+        \" result reporting of ProcessTree metrics is stopped\")\n+        isAvailable = false\n+        return 0\n+    }\n+  }\n+\n+  private def computeProcessTree(): Set[Int] = {\n+    if (!isAvailable || testing) {\n+      return Set()\n+    }\n+    var ptree: Set[Int] = Set()\n+    ptree += pid\n+    val queue = mutable.Queue.empty[Int]\n+    queue += pid\n+    while( !queue.isEmpty ) {\n+      val p = queue.dequeue()\n+      val c = getChildPids(p)\n+      if(!c.isEmpty) {\n+        queue ++= c\n+        ptree ++= c.toSet\n+      }\n+    }\n+    ptree\n+  }\n+\n+  private def getChildPids(pid: Int): ArrayBuffer[Int] = {\n+    try {\n+      // val cmd = Array(\"pgrep\", \"-P\", pid.toString)\n+      val builder = new ProcessBuilder(\"pgrep\", \"-P\", pid.toString)\n+      val process = builder.start()\n+      // val output = new StringBuilder()\n+      val threadName = \"read stdout for \" + \"pgrep\"\n+      val childPidsInInt = mutable.ArrayBuffer.empty[Int]\n+      def appendChildPid(s: String): Unit = {\n+        if (s != \"\") {\n+          logDebug(\"Found a child pid:\" + s)\n+          childPidsInInt += Integer.parseInt(s)\n+        }\n+      }\n+      val stdoutThread = Utils.processStreamByLine(threadName,\n+        process.getInputStream, appendChildPid)\n+      val exitCode = process.waitFor()\n+      stdoutThread.join()\n+      // pgrep will have exit code of 1 if there are more than one child process\n+      // and it will have a exit code of 2 if there is no child process\n+      if (exitCode != 0 && exitCode > 2) {\n+        val cmd = builder.command().toArray.mkString(\" \")\n+        logWarning(s\"Process $cmd\" +\n+          s\" exited with code $exitCode, with stderr:\" + s\"${process.getErrorStream} \")\n+        throw new SparkException(s\"Process $cmd exited with code $exitCode\")\n+      }\n+      childPidsInInt\n+    } catch {\n+      case e: Exception => logWarning(\"Exception when trying to compute process tree.\" +\n+        \" As a result reporting of ProcessTree metrics is stopped.\", e)\n+        isAvailable = false\n+        return mutable.ArrayBuffer.empty[Int]\n+    }\n+  }\n+\n+  def computeProcessInfo(allMetrics: ProcfsBasedSystemsMetrics, pid: Int):\n+  ProcfsBasedSystemsMetrics = {\n+  /*\n+   * Hadoop ProcfsBasedProcessTree class used regex and pattern matching to retrive the memory\n+   * info. I tried that but found it not correct during tests, so I used normal string analysis\n+   * instead. The computation of RSS and Vmem are based on proc(5):\n+   * http://man7.org/linux/man-pages/man5/proc.5.html\n+   */\n+    try {\n+      val pidDir = new File(procfsDir, pid.toString)\n+      var allMetricsUpdated = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+      Utils.tryWithResource( new InputStreamReader(\n+        new FileInputStream(\n+          new File(pidDir, procfsStatFile)), Charset.forName(\"UTF-8\"))) { fReader =>\n+        Utils.tryWithResource( new BufferedReader(fReader)) { in =>\n+          val procInfo = in.readLine\n+          val procInfoSplit = procInfo.split(\" \")\n+          if (procInfoSplit != null) {\n+            val vmem = procInfoSplit(22).toLong\n+            val rssPages = procInfoSplit(23).toLong\n+            if (procInfoSplit(1).toLowerCase(Locale.US).contains(\"java\")) {\n+              allMetricsUpdated = allMetrics.copy(\n+                jvmVmemTotal = allMetrics.jvmVmemTotal + vmem,\n+                jvmRSSTotal = allMetrics.jvmRSSTotal + (rssPages*pageSize)\n+              )\n+            }\n+            else if (procInfoSplit(1).toLowerCase(Locale.US).contains(\"python\")) {\n+              allMetricsUpdated = allMetrics.copy(\n+                pythonVmemTotal = allMetrics.pythonVmemTotal + vmem,\n+                pythonRSSTotal = allMetrics.pythonRSSTotal + (rssPages*pageSize)\n+              )\n+            }\n+            else {\n+              allMetricsUpdated = allMetrics.copy(\n+                otherVmemTotal = allMetrics.otherVmemTotal + vmem,\n+                otherRSSTotal = allMetrics.otherRSSTotal + (rssPages*pageSize)\n+              )\n+            }\n+          }\n+        }\n+      }\n+      allMetricsUpdated\n+    } catch {\n+      case f: FileNotFoundException => logWarning(\"There was a problem with reading\" +\n+        \" the stat file of the process. \", f)\n+        ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+    }\n+  }\n+\n+  private[spark] def computeAllMetrics(): ProcfsBasedSystemsMetrics = {\n+    if (!isAvailable) {\n+      return ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+    }\n+    val pids = computeProcessTree\n+    var allMetrics = ProcfsBasedSystemsMetrics(0, 0, 0, 0, 0, 0)\n+    for (p <- pids) {\n+      allMetrics = computeProcessInfo(allMetrics, p)"
  }],
  "prId": 22612
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "I don't love these names, though I also suck at coming up with good ones.  I think in particular the \"Systems\" part is too ambiguous to be useful.  how about\r\n\r\n`ProcfsBasedSystemsMetrics` -> `ProcfsMetrics`\r\n`ProcfsBasedSystems` -> `ProcfsMetricsGetter`",
    "commit": "6eab31587052796d997d9272803f2616e0eb3cba",
    "createdAt": "2018-11-16T21:10:46Z",
    "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.io._\n+import java.nio.charset.Charset\n+import java.nio.file.{Files, Paths}\n+import java.util.Locale\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, SparkException}\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ProcfsBasedSystemsMetrics(\n+    jvmVmemTotal: Long,\n+    jvmRSSTotal: Long,\n+    pythonVmemTotal: Long,\n+    pythonRSSTotal: Long,\n+    otherVmemTotal: Long,\n+    otherRSSTotal: Long)\n+\n+// Some of the ideas here are taken from the ProcfsBasedProcessTree class in hadoop\n+// project.\n+private[spark] class ProcfsBasedSystems(val procfsDir: String = \"/proc/\") extends Logging {"
  }],
  "prId": 22612
}]