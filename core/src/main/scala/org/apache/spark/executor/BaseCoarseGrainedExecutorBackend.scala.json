[{
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "This is completely same as original `CoarseGrainedExecutorBackend` (so please consider it as renaming) but added abstract and let derived classes implement extractLogUrls and extractAttributes.\r\n\r\nI'd like to make clear abstract class has its prefix to determine whether it is abstract class, but I'm open to other option like keeping this as `CoarseGrainedExecutorBackend` and rename new `CoarseGrainedExecutorBackend` as `DefaultCoarseGrainedExecutorBackend`.",
    "commit": "7992091c9a45ae3f234fd85e13e0deb1a5f4282f",
    "createdAt": "2019-01-30T23:28:49Z",
    "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.net.URL\n+import java.nio.ByteBuffer\n+import java.util.Locale\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import scala.util.{Failure, Success}\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SecurityManager, SparkConf, SparkEnv}\n+import org.apache.spark.TaskState.TaskState\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.worker.WorkerWatcher\n+import org.apache.spark.executor.CoarseGrainedExecutorBackend.log\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcAddress, RpcEndpointRef, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{ExecutorLossReason, TaskDescription}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages._\n+import org.apache.spark.serializer.SerializerInstance\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+private[spark] abstract class BaseCoarseGrainedExecutorBackend("
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "Why touch `CoarseGrainedExecutorBackend` at all? Can't you keep it as is, and override what you need in the YARN version?",
    "commit": "7992091c9a45ae3f234fd85e13e0deb1a5f4282f",
    "createdAt": "2019-02-12T18:04:09Z",
    "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.net.URL\n+import java.nio.ByteBuffer\n+import java.util.Locale\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import scala.util.{Failure, Success}\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SecurityManager, SparkConf, SparkEnv}\n+import org.apache.spark.TaskState.TaskState\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.worker.WorkerWatcher\n+import org.apache.spark.executor.CoarseGrainedExecutorBackend.log\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcAddress, RpcEndpointRef, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{ExecutorLossReason, TaskDescription}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages._\n+import org.apache.spark.serializer.SerializerInstance\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+private[spark] abstract class BaseCoarseGrainedExecutorBackend("
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Nope, just wanted to guarantee `executor log url` and `attributes` are overridable. Looks like we would want to have minimized diff, then I'll just let YARN executor backend override them.",
    "commit": "7992091c9a45ae3f234fd85e13e0deb1a5f4282f",
    "createdAt": "2019-02-12T23:30:24Z",
    "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.net.URL\n+import java.nio.ByteBuffer\n+import java.util.Locale\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import scala.util.{Failure, Success}\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SecurityManager, SparkConf, SparkEnv}\n+import org.apache.spark.TaskState.TaskState\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.worker.WorkerWatcher\n+import org.apache.spark.executor.CoarseGrainedExecutorBackend.log\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcAddress, RpcEndpointRef, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{ExecutorLossReason, TaskDescription}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages._\n+import org.apache.spark.serializer.SerializerInstance\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+private[spark] abstract class BaseCoarseGrainedExecutorBackend("
  }],
  "prId": 23706
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Do you actually need different command line parsing for the YARN version? Up to now they've been the same, so it seems to me they should remain the same.\r\n\r\nSo if instead of this, you add `main(args, backendCreateFn)` to `CoarseGrainedExecutorBackend`, you could share more code.",
    "commit": "7992091c9a45ae3f234fd85e13e0deb1a5f4282f",
    "createdAt": "2019-02-12T18:06:06Z",
    "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+import java.net.URL\n+import java.nio.ByteBuffer\n+import java.util.Locale\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import scala.util.{Failure, Success}\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SecurityManager, SparkConf, SparkEnv}\n+import org.apache.spark.TaskState.TaskState\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.worker.WorkerWatcher\n+import org.apache.spark.executor.CoarseGrainedExecutorBackend.log\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcAddress, RpcEndpointRef, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{ExecutorLossReason, TaskDescription}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages._\n+import org.apache.spark.serializer.SerializerInstance\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+\n+private[spark] abstract class BaseCoarseGrainedExecutorBackend(\n+    override val rpcEnv: RpcEnv,\n+    driverUrl: String,\n+    executorId: String,\n+    hostname: String,\n+    cores: Int,\n+    userClassPath: Seq[URL],\n+    env: SparkEnv)\n+  extends ThreadSafeRpcEndpoint with ExecutorBackend with Logging {\n+\n+  private[this] val stopping = new AtomicBoolean(false)\n+  var executor: Executor = null\n+  @volatile var driver: Option[RpcEndpointRef] = None\n+\n+  // If this CoarseGrainedExecutorBackend is changed to support multiple threads, then this may need\n+  // to be changed so that we don't share the serializer instance across threads\n+  private[this] val ser: SerializerInstance = env.closureSerializer.newInstance()\n+\n+  override def onStart() {\n+    logInfo(\"Connecting to driver: \" + driverUrl)\n+    rpcEnv.asyncSetupEndpointRefByURI(driverUrl).flatMap { ref =>\n+      // This is a very fast action so we can use \"ThreadUtils.sameThread\"\n+      driver = Some(ref)\n+      ref.ask[Boolean](RegisterExecutor(executorId, self, hostname, cores, extractLogUrls,\n+        extractAttributes))\n+    }(ThreadUtils.sameThread).onComplete {\n+      // This is a very fast action so we can use \"ThreadUtils.sameThread\"\n+      case Success(msg) =>\n+      // Always receive `true`. Just ignore it\n+      case Failure(e) =>\n+        exitExecutor(1, s\"Cannot register with driver: $driverUrl\", e, notifyDriver = false)\n+    }(ThreadUtils.sameThread)\n+  }\n+\n+  override def receive: PartialFunction[Any, Unit] = {\n+    case RegisteredExecutor =>\n+      logInfo(\"Successfully registered with driver\")\n+      try {\n+        executor = new Executor(executorId, hostname, env, userClassPath, isLocal = false)\n+      } catch {\n+        case NonFatal(e) =>\n+          exitExecutor(1, \"Unable to create executor due to \" + e.getMessage, e)\n+      }\n+\n+    case RegisterExecutorFailed(message) =>\n+      exitExecutor(1, \"Slave registration failed: \" + message)\n+\n+    case LaunchTask(data) =>\n+      if (executor == null) {\n+        exitExecutor(1, \"Received LaunchTask command but executor was null\")\n+      } else {\n+        val taskDesc = TaskDescription.decode(data.value)\n+        logInfo(\"Got assigned task \" + taskDesc.taskId)\n+        executor.launchTask(this, taskDesc)\n+      }\n+\n+    case KillTask(taskId, _, interruptThread, reason) =>\n+      if (executor == null) {\n+        exitExecutor(1, \"Received KillTask command but executor was null\")\n+      } else {\n+        executor.killTask(taskId, interruptThread, reason)\n+      }\n+\n+    case StopExecutor =>\n+      stopping.set(true)\n+      logInfo(\"Driver commanded a shutdown\")\n+      // Cannot shutdown here because an ack may need to be sent back to the caller. So send\n+      // a message to self to actually do the shutdown.\n+      self.send(Shutdown)\n+\n+    case Shutdown =>\n+      stopping.set(true)\n+      new Thread(\"CoarseGrainedExecutorBackend-stop-executor\") {\n+        override def run(): Unit = {\n+          // executor.stop() will call `SparkEnv.stop()` which waits until RpcEnv stops totally.\n+          // However, if `executor.stop()` runs in some thread of RpcEnv, RpcEnv won't be able to\n+          // stop until `executor.stop()` returns, which becomes a dead-lock (See SPARK-14180).\n+          // Therefore, we put this line in a new thread.\n+          executor.stop()\n+        }\n+      }.start()\n+\n+    case UpdateDelegationTokens(tokenBytes) =>\n+      logInfo(s\"Received tokens of ${tokenBytes.length} bytes\")\n+      SparkHadoopUtil.get.addDelegationTokens(tokenBytes, env.conf)\n+  }\n+\n+  override def onDisconnected(remoteAddress: RpcAddress): Unit = {\n+    if (stopping.get()) {\n+      logInfo(s\"Driver from $remoteAddress disconnected during shutdown\")\n+    } else if (driver.exists(_.address == remoteAddress)) {\n+      exitExecutor(1, s\"Driver $remoteAddress disassociated! Shutting down.\", null,\n+        notifyDriver = false)\n+    } else {\n+      logWarning(s\"An unknown ($remoteAddress) driver disconnected.\")\n+    }\n+  }\n+\n+  override def statusUpdate(taskId: Long, state: TaskState, data: ByteBuffer) {\n+    val msg = StatusUpdate(executorId, taskId, state, data)\n+    driver match {\n+      case Some(driverRef) => driverRef.send(msg)\n+      case None => logWarning(s\"Drop $msg because has not yet connected to driver\")\n+    }\n+  }\n+\n+  def extractLogUrls: Map[String, String]\n+\n+  def extractAttributes: Map[String, String]\n+\n+  /**\n+   * This function can be overloaded by other child classes to handle\n+   * executor exits differently. For e.g. when an executor goes down,\n+   * back-end may not want to take the parent process down.\n+   */\n+  protected def exitExecutor(\n+      code: Int,\n+      reason: String,\n+      throwable: Throwable = null,\n+      notifyDriver: Boolean = true) = {\n+    val message = \"Executor self-exiting due to : \" + reason\n+    if (throwable != null) {\n+      logError(message, throwable)\n+    } else {\n+      logError(message)\n+    }\n+\n+    if (notifyDriver && driver.nonEmpty) {\n+      driver.get.send(RemoveExecutor(executorId, new ExecutorLossReason(reason)))\n+    }\n+\n+    System.exit(code)\n+  }\n+}\n+\n+object BaseCoarseGrainedExecutorBackend {\n+  private[spark] def run("
  }],
  "prId": 23706
}]