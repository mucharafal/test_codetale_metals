[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "This is an example shows a weakness of the new API: we can't `setValue`.  For this example, we have the final output and we wanna set the value of accumulator so that it can produce the same output.  With the new API, we can't guarantee that all accumulators can implement `setValue`, e.g. the average accumulator.  I'm still thinking about how to fix it or work around it,  @rxin any ideas?\n",
    "commit": "124568b3eeb7e0a657b2fbe4f54bb85543b7ffa3",
    "createdAt": "2016-04-24T21:20:24Z",
    "diffHunk": "@@ -287,11 +258,10 @@ private[spark] object TaskMetrics extends Logging {\n   def fromAccumulatorUpdates(accumUpdates: Seq[AccumulableInfo]): TaskMetrics = {\n     val definedAccumUpdates = accumUpdates.filter(_.update.isDefined)\n     val metrics = new ListenerTaskMetrics(definedAccumUpdates)\n-    // We don't register this [[ListenerTaskMetrics]] for cleanup, and this is only used to post\n-    // event, we should un-register all accumulators immediately.\n-    metrics.internalAccums.foreach(acc => Accumulators.remove(acc.id))\n-    definedAccumUpdates.filter(_.internal).foreach { accum =>\n-      metrics.internalAccums.find(_.name == accum.name).foreach(_.setValueAny(accum.update.get))\n+    definedAccumUpdates.filter(_.internal).foreach { accInfo =>\n+      metrics.internalAccums.find(_.name == accInfo.name).foreach { acc =>\n+        acc.asInstanceOf[Accumulator[Any, Any]].add(accInfo.update.get)"
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "Just have a reset and \"add\"? \n\nI'd argue it doesn't make sense to call setValue, since \"set\" action is not algebraic (i.e. you cannot compose/merge set operations).\n",
    "commit": "124568b3eeb7e0a657b2fbe4f54bb85543b7ffa3",
    "createdAt": "2016-04-25T03:51:03Z",
    "diffHunk": "@@ -287,11 +258,10 @@ private[spark] object TaskMetrics extends Logging {\n   def fromAccumulatorUpdates(accumUpdates: Seq[AccumulableInfo]): TaskMetrics = {\n     val definedAccumUpdates = accumUpdates.filter(_.update.isDefined)\n     val metrics = new ListenerTaskMetrics(definedAccumUpdates)\n-    // We don't register this [[ListenerTaskMetrics]] for cleanup, and this is only used to post\n-    // event, we should un-register all accumulators immediately.\n-    metrics.internalAccums.foreach(acc => Accumulators.remove(acc.id))\n-    definedAccumUpdates.filter(_.internal).foreach { accum =>\n-      metrics.internalAccums.find(_.name == accum.name).foreach(_.setValueAny(accum.update.get))\n+    definedAccumUpdates.filter(_.internal).foreach { accInfo =>\n+      metrics.internalAccums.find(_.name == accInfo.name).foreach { acc =>\n+        acc.asInstanceOf[Accumulator[Any, Any]].add(accInfo.update.get)"
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "Actually i don't think we need this if we send accumulators back to the driver.\n",
    "commit": "124568b3eeb7e0a657b2fbe4f54bb85543b7ffa3",
    "createdAt": "2016-04-25T03:58:48Z",
    "diffHunk": "@@ -287,11 +258,10 @@ private[spark] object TaskMetrics extends Logging {\n   def fromAccumulatorUpdates(accumUpdates: Seq[AccumulableInfo]): TaskMetrics = {\n     val definedAccumUpdates = accumUpdates.filter(_.update.isDefined)\n     val metrics = new ListenerTaskMetrics(definedAccumUpdates)\n-    // We don't register this [[ListenerTaskMetrics]] for cleanup, and this is only used to post\n-    // event, we should un-register all accumulators immediately.\n-    metrics.internalAccums.foreach(acc => Accumulators.remove(acc.id))\n-    definedAccumUpdates.filter(_.internal).foreach { accum =>\n-      metrics.internalAccums.find(_.name == accum.name).foreach(_.setValueAny(accum.update.get))\n+    definedAccumUpdates.filter(_.internal).foreach { accInfo =>\n+      metrics.internalAccums.find(_.name == accInfo.name).foreach { acc =>\n+        acc.asInstanceOf[Accumulator[Any, Any]].add(accInfo.update.get)"
  }],
  "prId": 12612
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "this is just the ListAccumulator?\n",
    "commit": "124568b3eeb7e0a657b2fbe4f54bb85543b7ffa3",
    "createdAt": "2016-04-28T03:40:21Z",
    "diffHunk": "@@ -175,124 +172,143 @@ class TaskMetrics private[spark] () extends Serializable {\n   }\n \n   // Only used for test\n-  private[spark] val testAccum =\n-    sys.props.get(\"spark.testing\").map(_ => TaskMetrics.createLongAccum(TEST_ACCUM))\n-\n-  @transient private[spark] lazy val internalAccums: Seq[Accumulable[_, _]] = {\n-    val in = inputMetrics\n-    val out = outputMetrics\n-    val sr = shuffleReadMetrics\n-    val sw = shuffleWriteMetrics\n-    Seq(_executorDeserializeTime, _executorRunTime, _resultSize, _jvmGCTime,\n-      _resultSerializationTime, _memoryBytesSpilled, _diskBytesSpilled, _peakExecutionMemory,\n-      _updatedBlockStatuses, sr._remoteBlocksFetched, sr._localBlocksFetched, sr._remoteBytesRead,\n-      sr._localBytesRead, sr._fetchWaitTime, sr._recordsRead, sw._bytesWritten, sw._recordsWritten,\n-      sw._writeTime, in._bytesRead, in._recordsRead, out._bytesWritten, out._recordsWritten) ++\n-      testAccum\n-  }\n+  private[spark] val testAccum = sys.props.get(\"spark.testing\").map(_ => new LongAccumulator)\n+\n+\n+  import InternalAccumulator._\n+  @transient private[spark] lazy val nameToAccums = LinkedHashMap(\n+    EXECUTOR_DESERIALIZE_TIME -> _executorDeserializeTime,\n+    EXECUTOR_RUN_TIME -> _executorRunTime,\n+    RESULT_SIZE -> _resultSize,\n+    JVM_GC_TIME -> _jvmGCTime,\n+    RESULT_SERIALIZATION_TIME -> _resultSerializationTime,\n+    MEMORY_BYTES_SPILLED -> _memoryBytesSpilled,\n+    DISK_BYTES_SPILLED -> _diskBytesSpilled,\n+    PEAK_EXECUTION_MEMORY -> _peakExecutionMemory,\n+    UPDATED_BLOCK_STATUSES -> _updatedBlockStatuses,\n+    shuffleRead.REMOTE_BLOCKS_FETCHED -> shuffleReadMetrics._remoteBlocksFetched,\n+    shuffleRead.LOCAL_BLOCKS_FETCHED -> shuffleReadMetrics._localBlocksFetched,\n+    shuffleRead.REMOTE_BYTES_READ -> shuffleReadMetrics._remoteBytesRead,\n+    shuffleRead.LOCAL_BYTES_READ -> shuffleReadMetrics._localBytesRead,\n+    shuffleRead.FETCH_WAIT_TIME -> shuffleReadMetrics._fetchWaitTime,\n+    shuffleRead.RECORDS_READ -> shuffleReadMetrics._recordsRead,\n+    shuffleWrite.BYTES_WRITTEN -> shuffleWriteMetrics._bytesWritten,\n+    shuffleWrite.RECORDS_WRITTEN -> shuffleWriteMetrics._recordsWritten,\n+    shuffleWrite.WRITE_TIME -> shuffleWriteMetrics._writeTime,\n+    input.BYTES_READ -> inputMetrics._bytesRead,\n+    input.RECORDS_READ -> inputMetrics._recordsRead,\n+    output.BYTES_WRITTEN -> outputMetrics._bytesWritten,\n+    output.RECORDS_WRITTEN -> outputMetrics._recordsWritten\n+  ) ++ testAccum.map(TEST_ACCUM -> _)\n+\n+  @transient private[spark] lazy val internalAccums: Seq[NewAccumulator[_, _]] =\n+    nameToAccums.values.toIndexedSeq\n \n   /* ========================== *\n    |        OTHER THINGS        |\n    * ========================== */\n \n-  private[spark] def registerForCleanup(sc: SparkContext): Unit = {\n-    internalAccums.foreach { accum =>\n-      sc.cleaner.foreach(_.registerAccumulatorForCleanup(accum))\n+  private[spark] def register(sc: SparkContext): Unit = {\n+    nameToAccums.foreach {\n+      case (name, acc) => acc.register(sc, name = Some(name), countFailedValues = true)\n     }\n   }\n \n   /**\n    * External accumulators registered with this task.\n    */\n-  @transient private lazy val externalAccums = new ArrayBuffer[Accumulable[_, _]]\n+  @transient private lazy val externalAccums = new ArrayBuffer[NewAccumulator[_, _]]\n \n-  private[spark] def registerAccumulator(a: Accumulable[_, _]): Unit = {\n+  private[spark] def registerAccumulator(a: NewAccumulator[_, _]): Unit = {\n     externalAccums += a\n   }\n \n-  /**\n-   * Return the latest updates of accumulators in this task.\n-   *\n-   * The [[AccumulableInfo.update]] field is always defined and the [[AccumulableInfo.value]]\n-   * field is always empty, since this represents the partial updates recorded in this task,\n-   * not the aggregated value across multiple tasks.\n-   */\n-  def accumulatorUpdates(): Seq[AccumulableInfo] = {\n-    (internalAccums ++ externalAccums).map { a => a.toInfo(Some(a.localValue), None) }\n-  }\n+  private[spark] def accumulators(): Seq[NewAccumulator[_, _]] = internalAccums ++ externalAccums\n }\n \n-/**\n- * Internal subclass of [[TaskMetrics]] which is used only for posting events to listeners.\n- * Its purpose is to obviate the need for the driver to reconstruct the original accumulators,\n- * which might have been garbage-collected. See SPARK-13407 for more details.\n- *\n- * Instances of this class should be considered read-only and users should not call `inc*()` or\n- * `set*()` methods. While we could override the setter methods to throw\n- * UnsupportedOperationException, we choose not to do so because the overrides would quickly become\n- * out-of-date when new metrics are added.\n- */\n-private[spark] class ListenerTaskMetrics(accumUpdates: Seq[AccumulableInfo]) extends TaskMetrics {\n-\n-  override def accumulatorUpdates(): Seq[AccumulableInfo] = accumUpdates\n-\n-  override private[spark] def registerAccumulator(a: Accumulable[_, _]): Unit = {\n-    throw new UnsupportedOperationException(\"This TaskMetrics is read-only\")\n-  }\n-}\n \n private[spark] object TaskMetrics extends Logging {\n+  import InternalAccumulator._\n \n   /**\n    * Create an empty task metrics that doesn't register its accumulators.\n    */\n   def empty: TaskMetrics = {\n-    val metrics = new TaskMetrics\n-    metrics.internalAccums.foreach(acc => Accumulators.remove(acc.id))\n-    metrics\n+    val tm = new TaskMetrics\n+    tm.nameToAccums.foreach { case (name, acc) =>\n+      acc.metadata = AccumulatorMetadata(AccumulatorContext.newId(), Some(name), true)\n+    }\n+    tm\n+  }\n+\n+  def registered: TaskMetrics = {\n+    val tm = empty\n+    tm.internalAccums.foreach(AccumulatorContext.register)\n+    tm\n   }\n \n   /**\n-   * Create a new accumulator representing an internal task metric.\n+   * Construct a [[TaskMetrics]] object from a list of [[AccumulableInfo]], called on driver only.\n+   * The returned [[TaskMetrics]] is only used to get some internal metrics, we don't need to take\n+   * care of external accumulator info passed in.\n    */\n-  private def newMetric[T](\n-      initialValue: T,\n-      name: String,\n-      param: AccumulatorParam[T]): Accumulator[T] = {\n-    new Accumulator[T](initialValue, param, Some(name), countFailedValues = true)\n+  def fromAccumulatorInfos(infos: Seq[AccumulableInfo]): TaskMetrics = {\n+    val tm = new TaskMetrics\n+    infos.filter(info => info.name.isDefined && info.update.isDefined).foreach { info =>\n+      val name = info.name.get\n+      val value = info.update.get\n+      if (name == UPDATED_BLOCK_STATUSES) {\n+        tm.setUpdatedBlockStatuses(value.asInstanceOf[Seq[(BlockId, BlockStatus)]])\n+      } else {\n+        tm.nameToAccums.get(name).foreach(\n+          _.asInstanceOf[LongAccumulator].setValue(value.asInstanceOf[Long])\n+        )\n+      }\n+    }\n+    tm\n   }\n \n-  def createLongAccum(name: String): Accumulator[Long] = {\n-    newMetric(0L, name, LongAccumulatorParam)\n-  }\n+  /**\n+   * Construct a [[TaskMetrics]] object from a list of accumulator updates, called on driver only.\n+   */\n+  def fromAccumulators(accums: Seq[NewAccumulator[_, _]]): TaskMetrics = {\n+    val tm = new TaskMetrics\n+    val (internalAccums, externalAccums) =\n+      accums.partition(a => a.name.isDefined && tm.nameToAccums.contains(a.name.get))\n+\n+    internalAccums.foreach { acc =>\n+      val tmAcc = tm.nameToAccums(acc.name.get).asInstanceOf[NewAccumulator[Any, Any]]\n+      tmAcc.metadata = acc.metadata\n+      tmAcc.merge(acc.asInstanceOf[NewAccumulator[Any, Any]])\n+    }\n \n-  def createIntAccum(name: String): Accumulator[Int] = {\n-    newMetric(0, name, IntAccumulatorParam)\n+    tm.externalAccums ++= externalAccums\n+    tm\n   }\n+}\n+\n+\n+private[spark] class BlockStatusesAccumulator",
    "line": 282
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "I created this to avoid a lot of conversion between java list and scala seq.\n",
    "commit": "124568b3eeb7e0a657b2fbe4f54bb85543b7ffa3",
    "createdAt": "2016-04-28T05:21:18Z",
    "diffHunk": "@@ -175,124 +172,143 @@ class TaskMetrics private[spark] () extends Serializable {\n   }\n \n   // Only used for test\n-  private[spark] val testAccum =\n-    sys.props.get(\"spark.testing\").map(_ => TaskMetrics.createLongAccum(TEST_ACCUM))\n-\n-  @transient private[spark] lazy val internalAccums: Seq[Accumulable[_, _]] = {\n-    val in = inputMetrics\n-    val out = outputMetrics\n-    val sr = shuffleReadMetrics\n-    val sw = shuffleWriteMetrics\n-    Seq(_executorDeserializeTime, _executorRunTime, _resultSize, _jvmGCTime,\n-      _resultSerializationTime, _memoryBytesSpilled, _diskBytesSpilled, _peakExecutionMemory,\n-      _updatedBlockStatuses, sr._remoteBlocksFetched, sr._localBlocksFetched, sr._remoteBytesRead,\n-      sr._localBytesRead, sr._fetchWaitTime, sr._recordsRead, sw._bytesWritten, sw._recordsWritten,\n-      sw._writeTime, in._bytesRead, in._recordsRead, out._bytesWritten, out._recordsWritten) ++\n-      testAccum\n-  }\n+  private[spark] val testAccum = sys.props.get(\"spark.testing\").map(_ => new LongAccumulator)\n+\n+\n+  import InternalAccumulator._\n+  @transient private[spark] lazy val nameToAccums = LinkedHashMap(\n+    EXECUTOR_DESERIALIZE_TIME -> _executorDeserializeTime,\n+    EXECUTOR_RUN_TIME -> _executorRunTime,\n+    RESULT_SIZE -> _resultSize,\n+    JVM_GC_TIME -> _jvmGCTime,\n+    RESULT_SERIALIZATION_TIME -> _resultSerializationTime,\n+    MEMORY_BYTES_SPILLED -> _memoryBytesSpilled,\n+    DISK_BYTES_SPILLED -> _diskBytesSpilled,\n+    PEAK_EXECUTION_MEMORY -> _peakExecutionMemory,\n+    UPDATED_BLOCK_STATUSES -> _updatedBlockStatuses,\n+    shuffleRead.REMOTE_BLOCKS_FETCHED -> shuffleReadMetrics._remoteBlocksFetched,\n+    shuffleRead.LOCAL_BLOCKS_FETCHED -> shuffleReadMetrics._localBlocksFetched,\n+    shuffleRead.REMOTE_BYTES_READ -> shuffleReadMetrics._remoteBytesRead,\n+    shuffleRead.LOCAL_BYTES_READ -> shuffleReadMetrics._localBytesRead,\n+    shuffleRead.FETCH_WAIT_TIME -> shuffleReadMetrics._fetchWaitTime,\n+    shuffleRead.RECORDS_READ -> shuffleReadMetrics._recordsRead,\n+    shuffleWrite.BYTES_WRITTEN -> shuffleWriteMetrics._bytesWritten,\n+    shuffleWrite.RECORDS_WRITTEN -> shuffleWriteMetrics._recordsWritten,\n+    shuffleWrite.WRITE_TIME -> shuffleWriteMetrics._writeTime,\n+    input.BYTES_READ -> inputMetrics._bytesRead,\n+    input.RECORDS_READ -> inputMetrics._recordsRead,\n+    output.BYTES_WRITTEN -> outputMetrics._bytesWritten,\n+    output.RECORDS_WRITTEN -> outputMetrics._recordsWritten\n+  ) ++ testAccum.map(TEST_ACCUM -> _)\n+\n+  @transient private[spark] lazy val internalAccums: Seq[NewAccumulator[_, _]] =\n+    nameToAccums.values.toIndexedSeq\n \n   /* ========================== *\n    |        OTHER THINGS        |\n    * ========================== */\n \n-  private[spark] def registerForCleanup(sc: SparkContext): Unit = {\n-    internalAccums.foreach { accum =>\n-      sc.cleaner.foreach(_.registerAccumulatorForCleanup(accum))\n+  private[spark] def register(sc: SparkContext): Unit = {\n+    nameToAccums.foreach {\n+      case (name, acc) => acc.register(sc, name = Some(name), countFailedValues = true)\n     }\n   }\n \n   /**\n    * External accumulators registered with this task.\n    */\n-  @transient private lazy val externalAccums = new ArrayBuffer[Accumulable[_, _]]\n+  @transient private lazy val externalAccums = new ArrayBuffer[NewAccumulator[_, _]]\n \n-  private[spark] def registerAccumulator(a: Accumulable[_, _]): Unit = {\n+  private[spark] def registerAccumulator(a: NewAccumulator[_, _]): Unit = {\n     externalAccums += a\n   }\n \n-  /**\n-   * Return the latest updates of accumulators in this task.\n-   *\n-   * The [[AccumulableInfo.update]] field is always defined and the [[AccumulableInfo.value]]\n-   * field is always empty, since this represents the partial updates recorded in this task,\n-   * not the aggregated value across multiple tasks.\n-   */\n-  def accumulatorUpdates(): Seq[AccumulableInfo] = {\n-    (internalAccums ++ externalAccums).map { a => a.toInfo(Some(a.localValue), None) }\n-  }\n+  private[spark] def accumulators(): Seq[NewAccumulator[_, _]] = internalAccums ++ externalAccums\n }\n \n-/**\n- * Internal subclass of [[TaskMetrics]] which is used only for posting events to listeners.\n- * Its purpose is to obviate the need for the driver to reconstruct the original accumulators,\n- * which might have been garbage-collected. See SPARK-13407 for more details.\n- *\n- * Instances of this class should be considered read-only and users should not call `inc*()` or\n- * `set*()` methods. While we could override the setter methods to throw\n- * UnsupportedOperationException, we choose not to do so because the overrides would quickly become\n- * out-of-date when new metrics are added.\n- */\n-private[spark] class ListenerTaskMetrics(accumUpdates: Seq[AccumulableInfo]) extends TaskMetrics {\n-\n-  override def accumulatorUpdates(): Seq[AccumulableInfo] = accumUpdates\n-\n-  override private[spark] def registerAccumulator(a: Accumulable[_, _]): Unit = {\n-    throw new UnsupportedOperationException(\"This TaskMetrics is read-only\")\n-  }\n-}\n \n private[spark] object TaskMetrics extends Logging {\n+  import InternalAccumulator._\n \n   /**\n    * Create an empty task metrics that doesn't register its accumulators.\n    */\n   def empty: TaskMetrics = {\n-    val metrics = new TaskMetrics\n-    metrics.internalAccums.foreach(acc => Accumulators.remove(acc.id))\n-    metrics\n+    val tm = new TaskMetrics\n+    tm.nameToAccums.foreach { case (name, acc) =>\n+      acc.metadata = AccumulatorMetadata(AccumulatorContext.newId(), Some(name), true)\n+    }\n+    tm\n+  }\n+\n+  def registered: TaskMetrics = {\n+    val tm = empty\n+    tm.internalAccums.foreach(AccumulatorContext.register)\n+    tm\n   }\n \n   /**\n-   * Create a new accumulator representing an internal task metric.\n+   * Construct a [[TaskMetrics]] object from a list of [[AccumulableInfo]], called on driver only.\n+   * The returned [[TaskMetrics]] is only used to get some internal metrics, we don't need to take\n+   * care of external accumulator info passed in.\n    */\n-  private def newMetric[T](\n-      initialValue: T,\n-      name: String,\n-      param: AccumulatorParam[T]): Accumulator[T] = {\n-    new Accumulator[T](initialValue, param, Some(name), countFailedValues = true)\n+  def fromAccumulatorInfos(infos: Seq[AccumulableInfo]): TaskMetrics = {\n+    val tm = new TaskMetrics\n+    infos.filter(info => info.name.isDefined && info.update.isDefined).foreach { info =>\n+      val name = info.name.get\n+      val value = info.update.get\n+      if (name == UPDATED_BLOCK_STATUSES) {\n+        tm.setUpdatedBlockStatuses(value.asInstanceOf[Seq[(BlockId, BlockStatus)]])\n+      } else {\n+        tm.nameToAccums.get(name).foreach(\n+          _.asInstanceOf[LongAccumulator].setValue(value.asInstanceOf[Long])\n+        )\n+      }\n+    }\n+    tm\n   }\n \n-  def createLongAccum(name: String): Accumulator[Long] = {\n-    newMetric(0L, name, LongAccumulatorParam)\n-  }\n+  /**\n+   * Construct a [[TaskMetrics]] object from a list of accumulator updates, called on driver only.\n+   */\n+  def fromAccumulators(accums: Seq[NewAccumulator[_, _]]): TaskMetrics = {\n+    val tm = new TaskMetrics\n+    val (internalAccums, externalAccums) =\n+      accums.partition(a => a.name.isDefined && tm.nameToAccums.contains(a.name.get))\n+\n+    internalAccums.foreach { acc =>\n+      val tmAcc = tm.nameToAccums(acc.name.get).asInstanceOf[NewAccumulator[Any, Any]]\n+      tmAcc.metadata = acc.metadata\n+      tmAcc.merge(acc.asInstanceOf[NewAccumulator[Any, Any]])\n+    }\n \n-  def createIntAccum(name: String): Accumulator[Int] = {\n-    newMetric(0, name, IntAccumulatorParam)\n+    tm.externalAccums ++= externalAccums\n+    tm\n   }\n+}\n+\n+\n+private[spark] class BlockStatusesAccumulator",
    "line": 282
  }],
  "prId": 12612
}]