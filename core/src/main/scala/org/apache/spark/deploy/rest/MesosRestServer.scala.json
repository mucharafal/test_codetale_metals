[{
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "these should all be `private[something]`\n",
    "commit": "069e9463b377d1a0c7e4c35619e06213eae07fc2",
    "createdAt": "2015-03-25T21:14:53Z",
    "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.rest\n+\n+import java.io.File\n+import javax.servlet.http.HttpServletResponse\n+\n+import org.apache.spark.deploy.DriverDescription\n+import org.apache.spark.deploy.ClientArguments._\n+import org.apache.spark.deploy.Command\n+import org.apache.spark.{SparkConf, SPARK_VERSION => sparkVersion}\n+import org.apache.spark.util.Utils\n+import org.apache.spark.scheduler.cluster.mesos.ClusterScheduler\n+import scala.collection.mutable\n+import org.apache.spark.deploy.mesos.MesosDriverDescription\n+\n+/**\n+ * A server that responds to requests submitted by the [[RestClient]].\n+ * This is intended to be used in Mesos cluster mode only.\n+ *\n+ * This server responds with different HTTP codes depending on the situation:\n+ *   200 OK - Request was processed successfully\n+ *   400 BAD REQUEST - Request was malformed, not successfully validated, or of unexpected type\n+ *   468 UNKNOWN PROTOCOL VERSION - Request specified a protocol this server does not understand\n+ *   500 INTERNAL SERVER ERROR - Server throws an exception internally while processing the request\n+ *\n+ * The server always includes a JSON representation of the relevant [[SubmitRestProtocolResponse]]\n+ * in the HTTP body. If an error occurs, however, the server will include an [[ErrorResponse]]\n+ * instead of the one expected by the client. If the construction of this error response itself\n+ * fails, the response will consist of an empty body with a response code that indicates internal\n+ * server error.\n+ *\n+ * @param host the address this server should bind to\n+ * @param requestedPort the port this server will attempt to bind to\n+ * @param masterConf the conf used by the Master\n+ * @param scheduler the scheduler that handles driver requests\n+ */\n+private [spark] class MesosRestServer(\n+    host: String,\n+    requestedPort: Int,\n+    masterConf: SparkConf,\n+    scheduler: ClusterScheduler)\n+  extends RestServer(\n+    host,\n+    requestedPort,\n+    masterConf,\n+    new MesosSubmitRequestServlet(scheduler, masterConf),\n+    new MesosKillRequestServlet(scheduler, masterConf),\n+    new MesosStatusRequestServlet(scheduler, masterConf)) {}\n+\n+class MesosSubmitRequestServlet(\n+    scheduler: ClusterScheduler,\n+    conf: SparkConf)\n+  extends SubmitRequestServlet {\n+\n+  /**\n+   * Build a driver description from the fields specified in the submit request.\n+   *\n+   * This involves constructing a command that launches a mesos framework for the job.\n+   * This does not currently consider fields used by python applications since python\n+   * is not supported in mesos cluster mode yet.\n+   */\n+  private def buildDriverDescription(request: CreateSubmissionRequest): MesosDriverDescription = {\n+    // Required fields, including the main class because python is not yet supported\n+    val appResource = Option(request.appResource).getOrElse {\n+      throw new SubmitRestMissingFieldException(\"Application jar is missing.\")\n+    }\n+    val mainClass = Option(request.mainClass).getOrElse {\n+      throw new SubmitRestMissingFieldException(\"Main class is missing.\")\n+    }\n+\n+    // Optional fields\n+    val sparkProperties = request.sparkProperties\n+    val driverExtraJavaOptions = sparkProperties.get(\"spark.driver.extraJavaOptions\")\n+    val driverExtraClassPath = sparkProperties.get(\"spark.driver.extraClassPath\")\n+    val driverExtraLibraryPath = sparkProperties.get(\"spark.driver.extraLibraryPath\")\n+    val superviseDriver = sparkProperties.get(\"spark.driver.supervise\")\n+    val driverMemory = sparkProperties.get(\"spark.driver.memory\")\n+    val driverCores = sparkProperties.get(\"spark.driver.cores\")\n+    val appArgs = request.appArgs\n+    val environmentVariables = request.environmentVariables\n+    val schedulerProperties = new mutable.HashMap[String, String]\n+    // Store Spark submit specific arguments here to pass to the scheduler.\n+    schedulerProperties(\"spark.app.name\") = sparkProperties.getOrElse(\"spark.app.name\", mainClass)\n+\n+    sparkProperties.get(\"spark.executor.memory\").foreach {\n+      v => schedulerProperties(\"spark.executor.memory\") = v\n+    }\n+\n+    sparkProperties.get(\"spark.cores.max\").foreach {\n+      v => schedulerProperties(\"spark.cores.max\") = v\n+    }\n+\n+    sparkProperties.get(\"spark.executor.uri\").foreach {\n+      v => schedulerProperties(\"spark.executor.uri\") = v\n+    }\n+\n+    sparkProperties.get(\"spark.mesos.executor.home\").foreach {\n+      v => schedulerProperties(\"spark.mesos.executor.home\") = v\n+    }\n+\n+    // Construct driver description\n+    val conf = new SparkConf(false)\n+      .setAll(sparkProperties)\n+\n+    val extraClassPath = driverExtraClassPath.toSeq.flatMap(_.split(File.pathSeparator))\n+    val extraLibraryPath = driverExtraLibraryPath.toSeq.flatMap(_.split(File.pathSeparator))\n+    val extraJavaOpts = driverExtraJavaOptions.map(Utils.splitCommandString).getOrElse(Seq.empty)\n+    val sparkJavaOpts = Utils.sparkJavaOpts(conf)\n+    val javaOpts = sparkJavaOpts ++ extraJavaOpts\n+    val command = new Command(\n+      mainClass, appArgs, environmentVariables, extraClassPath, extraLibraryPath, javaOpts)\n+    val actualSuperviseDriver = superviseDriver.map(_.toBoolean).getOrElse(DEFAULT_SUPERVISE)\n+    val actualDriverMemory = driverMemory.map(Utils.memoryStringToMb).getOrElse(DEFAULT_MEMORY)\n+    val actualDriverCores = driverCores.map(_.toInt).getOrElse(DEFAULT_CORES)\n+\n+    val desc = new DriverDescription(\n+      appResource, actualDriverMemory, actualDriverCores, actualSuperviseDriver, command)\n+\n+    new MesosDriverDescription(desc, schedulerProperties)\n+  }\n+\n+  protected override def handleSubmit(\n+      requestMessageJson: String,\n+      requestMessage: SubmitRestProtocolMessage,\n+      responseServlet: HttpServletResponse): SubmitRestProtocolResponse = {\n+    requestMessage match {\n+      case submitRequest: CreateSubmissionRequest =>\n+        val driverDescription = buildDriverDescription(submitRequest)\n+        val response = scheduler.submitDriver(driverDescription)\n+        val submitResponse = new CreateSubmissionResponse\n+        submitResponse.serverSparkVersion = sparkVersion\n+        submitResponse.message = response.message\n+        submitResponse.success = response.success\n+        submitResponse.submissionId = response.id\n+        val unknownFields = findUnknownFields(requestMessageJson, requestMessage)\n+        if (unknownFields.nonEmpty) {\n+          // If there are fields that the server does not know about, warn the client\n+          submitResponse.unknownFields = unknownFields\n+        }\n+        submitResponse\n+      case unexpected =>\n+        responseServlet.setStatus(HttpServletResponse.SC_BAD_REQUEST)\n+        handleError(s\"Received message of unexpected type ${unexpected.messageType}.\")\n+    }\n+  }\n+}\n+\n+class MesosKillRequestServlet(scheduler: ClusterScheduler, conf: SparkConf)\n+  extends KillRequestServlet {\n+  protected override def handleKill(submissionId: String): KillSubmissionResponse = {\n+    val response = scheduler.killDriver(submissionId)\n+    val k = new KillSubmissionResponse\n+    k.serverSparkVersion = sparkVersion\n+    k.message = response.message\n+    k.submissionId = submissionId\n+    k.success = response.success\n+    k\n+  }\n+}\n+\n+class MesosStatusRequestServlet(scheduler: ClusterScheduler, conf: SparkConf)"
  }],
  "prId": 5144
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "We should avoid duplicating this java doc from the parent abstract class. We can just point the user to the docs there instead.\n",
    "commit": "069e9463b377d1a0c7e4c35619e06213eae07fc2",
    "createdAt": "2015-03-25T21:37:37Z",
    "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.rest\n+\n+import java.io.File\n+import javax.servlet.http.HttpServletResponse\n+\n+import org.apache.spark.deploy.DriverDescription\n+import org.apache.spark.deploy.ClientArguments._\n+import org.apache.spark.deploy.Command\n+import org.apache.spark.{SparkConf, SPARK_VERSION => sparkVersion}\n+import org.apache.spark.util.Utils\n+import org.apache.spark.scheduler.cluster.mesos.ClusterScheduler\n+import scala.collection.mutable\n+import org.apache.spark.deploy.mesos.MesosDriverDescription\n+\n+/**\n+ * A server that responds to requests submitted by the [[RestClient]].\n+ * This is intended to be used in Mesos cluster mode only.\n+ *\n+ * This server responds with different HTTP codes depending on the situation:\n+ *   200 OK - Request was processed successfully\n+ *   400 BAD REQUEST - Request was malformed, not successfully validated, or of unexpected type\n+ *   468 UNKNOWN PROTOCOL VERSION - Request specified a protocol this server does not understand\n+ *   500 INTERNAL SERVER ERROR - Server throws an exception internally while processing the request\n+ *\n+ * The server always includes a JSON representation of the relevant [[SubmitRestProtocolResponse]]\n+ * in the HTTP body. If an error occurs, however, the server will include an [[ErrorResponse]]\n+ * instead of the one expected by the client. If the construction of this error response itself\n+ * fails, the response will consist of an empty body with a response code that indicates internal\n+ * server error."
  }],
  "prId": 5144
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "I'm opposed to just reaching in to the `standalone` space and grabbing default values from there. Mesos should have its own default values for these things. This is a common theme with this patch that I am not super comfortable with. If we want to reuse code then we should put them in a common place (as you have done by refactoring the `RestServer`) instead of just reaching into the namespace of different packages.\n",
    "commit": "069e9463b377d1a0c7e4c35619e06213eae07fc2",
    "createdAt": "2015-03-25T21:41:07Z",
    "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.rest\n+\n+import java.io.File\n+import javax.servlet.http.HttpServletResponse\n+\n+import org.apache.spark.deploy.DriverDescription\n+import org.apache.spark.deploy.ClientArguments._"
  }],
  "prId": 5144
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "by this way there is actually nothing that restricts Mesos to follow standalone mode's way submitting a `DriverDescription`. In our case it might make more sense to merge `MesosDriverDescription` with `DriverSubmission` since the latter contains the former anyway. Then you can just express `schedulerProperties` as another field in `DriverSubmission` instead of worrying about how to extend `DriverDescription` in a clean way.\n",
    "commit": "069e9463b377d1a0c7e4c35619e06213eae07fc2",
    "createdAt": "2015-03-25T22:38:08Z",
    "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.rest\n+\n+import java.io.File\n+import javax.servlet.http.HttpServletResponse\n+\n+import org.apache.spark.deploy.DriverDescription\n+import org.apache.spark.deploy.ClientArguments._\n+import org.apache.spark.deploy.Command\n+import org.apache.spark.{SparkConf, SPARK_VERSION => sparkVersion}\n+import org.apache.spark.util.Utils\n+import org.apache.spark.scheduler.cluster.mesos.ClusterScheduler\n+import scala.collection.mutable\n+import org.apache.spark.deploy.mesos.MesosDriverDescription\n+\n+/**\n+ * A server that responds to requests submitted by the [[RestClient]].\n+ * This is intended to be used in Mesos cluster mode only.\n+ *\n+ * This server responds with different HTTP codes depending on the situation:\n+ *   200 OK - Request was processed successfully\n+ *   400 BAD REQUEST - Request was malformed, not successfully validated, or of unexpected type\n+ *   468 UNKNOWN PROTOCOL VERSION - Request specified a protocol this server does not understand\n+ *   500 INTERNAL SERVER ERROR - Server throws an exception internally while processing the request\n+ *\n+ * The server always includes a JSON representation of the relevant [[SubmitRestProtocolResponse]]\n+ * in the HTTP body. If an error occurs, however, the server will include an [[ErrorResponse]]\n+ * instead of the one expected by the client. If the construction of this error response itself\n+ * fails, the response will consist of an empty body with a response code that indicates internal\n+ * server error.\n+ *\n+ * @param host the address this server should bind to\n+ * @param requestedPort the port this server will attempt to bind to\n+ * @param masterConf the conf used by the Master\n+ * @param scheduler the scheduler that handles driver requests\n+ */\n+private [spark] class MesosRestServer(\n+    host: String,\n+    requestedPort: Int,\n+    masterConf: SparkConf,\n+    scheduler: ClusterScheduler)\n+  extends RestServer(\n+    host,\n+    requestedPort,\n+    masterConf,\n+    new MesosSubmitRequestServlet(scheduler, masterConf),\n+    new MesosKillRequestServlet(scheduler, masterConf),\n+    new MesosStatusRequestServlet(scheduler, masterConf)) {}\n+\n+class MesosSubmitRequestServlet(\n+    scheduler: ClusterScheduler,\n+    conf: SparkConf)\n+  extends SubmitRequestServlet {\n+\n+  /**\n+   * Build a driver description from the fields specified in the submit request.\n+   *\n+   * This involves constructing a command that launches a mesos framework for the job.\n+   * This does not currently consider fields used by python applications since python\n+   * is not supported in mesos cluster mode yet.\n+   */\n+  private def buildDriverDescription(request: CreateSubmissionRequest): MesosDriverDescription = {"
  }],
  "prId": 5144
}]