[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "Is this the only reason we need to make spark core depend on hive? My concern is that, previously it's possible for users to build spark without hive, but now it's impossible.\r\n\r\nCan we move this file to hive module and use reflection in `ConfigurableCredentialManager`? so that if users build spark without hive and wanna use `HiveCredentialProvider`, we throw an exception.\r\n\r\n",
    "commit": "c4149ddb940c32285fde8b1c08a5b212fced5d66",
    "createdAt": "2017-05-31T00:01:23Z",
    "diffHunk": "@@ -29,11 +29,10 @@ import org.apache.hadoop.io.Text\n import org.apache.hadoop.security.{Credentials, UserGroupInformation}\n import org.apache.hadoop.security.token.Token\n \n-import org.apache.spark.SparkConf\n import org.apache.spark.internal.Logging\n import org.apache.spark.util.Utils\n \n-private[security] class HiveCredentialProvider extends ServiceCredentialProvider with Logging {\n+private[security] class HiveCredentialProvider extends HadoopDelegationTokenProvider with Logging {"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "> build spark without hive\r\n\r\nIt's still possible to build (as in package) a Spark distribution without Hive. This code already uses reflection, and the new dependencies should be test-scope only (aside from any issues that still need to be solved).\r\n\r\nSo you need Hive to run the tests, but you don't need Hive at runtime.",
    "commit": "c4149ddb940c32285fde8b1c08a5b212fced5d66",
    "createdAt": "2017-05-31T00:14:54Z",
    "diffHunk": "@@ -29,11 +29,10 @@ import org.apache.hadoop.io.Text\n import org.apache.hadoop.security.{Credentials, UserGroupInformation}\n import org.apache.hadoop.security.token.Token\n \n-import org.apache.spark.SparkConf\n import org.apache.spark.internal.Logging\n import org.apache.spark.util.Utils\n \n-private[security] class HiveCredentialProvider extends ServiceCredentialProvider with Logging {\n+private[security] class HiveCredentialProvider extends HadoopDelegationTokenProvider with Logging {"
  }, {
    "author": {
      "login": "mgummelt"
    },
    "body": "Right, those deps should be in test scope, but I ran into trouble doing that: https://github.com/apache/spark/pull/17723/files#r112788867\r\n\r\nI'll go back and try again.  Suggestions welcome.",
    "commit": "c4149ddb940c32285fde8b1c08a5b212fced5d66",
    "createdAt": "2017-05-31T00:34:28Z",
    "diffHunk": "@@ -29,11 +29,10 @@ import org.apache.hadoop.io.Text\n import org.apache.hadoop.security.{Credentials, UserGroupInformation}\n import org.apache.hadoop.security.token.Token\n \n-import org.apache.spark.SparkConf\n import org.apache.spark.internal.Logging\n import org.apache.spark.util.Utils\n \n-private[security] class HiveCredentialProvider extends ServiceCredentialProvider with Logging {\n+private[security] class HiveCredentialProvider extends HadoopDelegationTokenProvider with Logging {"
  }],
  "prId": 17723
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Hmmm... the user is not attempting, Spark does it by default if for some reason there's a Hive config available somewhere. So I think `logWarning` and not throwing an error would be a more user-friendly behavior.",
    "commit": "c4149ddb940c32285fde8b1c08a5b212fced5d66",
    "createdAt": "2017-06-07T22:16:28Z",
    "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import java.lang.reflect.UndeclaredThrowableException\n+import java.security.PrivilegedExceptionAction\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier\n+import org.apache.hadoop.hive.conf.HiveConf\n+import org.apache.hadoop.hive.ql.metadata.Hive\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.security.{Credentials, UserGroupInformation}\n+import org.apache.hadoop.security.token.Token\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+private[security] class HiveCredentialProvider extends HadoopDelegationTokenProvider with Logging {\n+\n+  override def serviceName: String = \"hive\"\n+\n+  private val classNotFoundErrorStr = \"You are attempting to use the HiveCredentialProvider,\" +\n+    \"but your Spark distribution is not built with Hive libraries.\"\n+\n+  private def hiveConf(hadoopConf: Configuration): Configuration = {\n+    try {\n+      new HiveConf(hadoopConf, classOf[HiveConf])\n+    } catch {\n+      case NonFatal(e) =>\n+        logDebug(\"Fail to create Hive Configuration\", e)\n+        hadoopConf\n+      case e: ClassNotFoundException =>\n+        logError(classNotFoundErrorStr)\n+        throw e"
  }, {
    "author": {
      "login": "mgummelt"
    },
    "body": "The client to this class (`ConfigurableCredentialProvider`) will assume it can fetch delegation tokens if this Hive config is set, though.  So if the necessary classes aren't loaded, client code will fail when it calls `obtainCredentials`.",
    "commit": "c4149ddb940c32285fde8b1c08a5b212fced5d66",
    "createdAt": "2017-06-08T21:22:48Z",
    "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import java.lang.reflect.UndeclaredThrowableException\n+import java.security.PrivilegedExceptionAction\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier\n+import org.apache.hadoop.hive.conf.HiveConf\n+import org.apache.hadoop.hive.ql.metadata.Hive\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.security.{Credentials, UserGroupInformation}\n+import org.apache.hadoop.security.token.Token\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+private[security] class HiveCredentialProvider extends HadoopDelegationTokenProvider with Logging {\n+\n+  override def serviceName: String = \"hive\"\n+\n+  private val classNotFoundErrorStr = \"You are attempting to use the HiveCredentialProvider,\" +\n+    \"but your Spark distribution is not built with Hive libraries.\"\n+\n+  private def hiveConf(hadoopConf: Configuration): Configuration = {\n+    try {\n+      new HiveConf(hadoopConf, classOf[HiveConf])\n+    } catch {\n+      case NonFatal(e) =>\n+        logDebug(\"Fail to create Hive Configuration\", e)\n+        hadoopConf\n+      case e: ClassNotFoundException =>\n+        logError(classNotFoundErrorStr)\n+        throw e"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "This is what the current code does:\r\n\r\n```\r\n      case NonFatal(e) =>\r\n        logDebug(\"Fail to create Hive Configuration\", e)\r\n        hadoopConf\r\n```\r\n\r\nSo, it does not throw an error if Hive classes are not available, even if the configs exist (because it won't load `HiveConf` and thus won't load `hive-site.xml`).\r\n",
    "commit": "c4149ddb940c32285fde8b1c08a5b212fced5d66",
    "createdAt": "2017-06-08T21:30:54Z",
    "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import java.lang.reflect.UndeclaredThrowableException\n+import java.security.PrivilegedExceptionAction\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier\n+import org.apache.hadoop.hive.conf.HiveConf\n+import org.apache.hadoop.hive.ql.metadata.Hive\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.security.{Credentials, UserGroupInformation}\n+import org.apache.hadoop.security.token.Token\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+private[security] class HiveCredentialProvider extends HadoopDelegationTokenProvider with Logging {\n+\n+  override def serviceName: String = \"hive\"\n+\n+  private val classNotFoundErrorStr = \"You are attempting to use the HiveCredentialProvider,\" +\n+    \"but your Spark distribution is not built with Hive libraries.\"\n+\n+  private def hiveConf(hadoopConf: Configuration): Configuration = {\n+    try {\n+      new HiveConf(hadoopConf, classOf[HiveConf])\n+    } catch {\n+      case NonFatal(e) =>\n+        logDebug(\"Fail to create Hive Configuration\", e)\n+        hadoopConf\n+      case e: ClassNotFoundException =>\n+        logError(classNotFoundErrorStr)\n+        throw e"
  }, {
    "author": {
      "login": "mgummelt"
    },
    "body": "But then client code calls `obtainCredentials`, which will fail with a `NoClassDefFoundError`.",
    "commit": "c4149ddb940c32285fde8b1c08a5b212fced5d66",
    "createdAt": "2017-06-08T21:45:18Z",
    "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import java.lang.reflect.UndeclaredThrowableException\n+import java.security.PrivilegedExceptionAction\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier\n+import org.apache.hadoop.hive.conf.HiveConf\n+import org.apache.hadoop.hive.ql.metadata.Hive\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.security.{Credentials, UserGroupInformation}\n+import org.apache.hadoop.security.token.Token\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+private[security] class HiveCredentialProvider extends HadoopDelegationTokenProvider with Logging {\n+\n+  override def serviceName: String = \"hive\"\n+\n+  private val classNotFoundErrorStr = \"You are attempting to use the HiveCredentialProvider,\" +\n+    \"but your Spark distribution is not built with Hive libraries.\"\n+\n+  private def hiveConf(hadoopConf: Configuration): Configuration = {\n+    try {\n+      new HiveConf(hadoopConf, classOf[HiveConf])\n+    } catch {\n+      case NonFatal(e) =>\n+        logDebug(\"Fail to create Hive Configuration\", e)\n+        hadoopConf\n+      case e: ClassNotFoundException =>\n+        logError(classNotFoundErrorStr)\n+        throw e"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "Which you also need to handle, since the original code handled the original error (`ClassNotFoundException`).",
    "commit": "c4149ddb940c32285fde8b1c08a5b212fced5d66",
    "createdAt": "2017-06-08T21:46:21Z",
    "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import java.lang.reflect.UndeclaredThrowableException\n+import java.security.PrivilegedExceptionAction\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier\n+import org.apache.hadoop.hive.conf.HiveConf\n+import org.apache.hadoop.hive.ql.metadata.Hive\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.security.{Credentials, UserGroupInformation}\n+import org.apache.hadoop.security.token.Token\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+private[security] class HiveCredentialProvider extends HadoopDelegationTokenProvider with Logging {\n+\n+  override def serviceName: String = \"hive\"\n+\n+  private val classNotFoundErrorStr = \"You are attempting to use the HiveCredentialProvider,\" +\n+    \"but your Spark distribution is not built with Hive libraries.\"\n+\n+  private def hiveConf(hadoopConf: Configuration): Configuration = {\n+    try {\n+      new HiveConf(hadoopConf, classOf[HiveConf])\n+    } catch {\n+      case NonFatal(e) =>\n+        logDebug(\"Fail to create Hive Configuration\", e)\n+        hadoopConf\n+      case e: ClassNotFoundException =>\n+        logError(classNotFoundErrorStr)\n+        throw e"
  }, {
    "author": {
      "login": "mgummelt"
    },
    "body": "The original code handled it in `hiveConf()`, but not `obtainCredentials()`.  It looks like the original code, too, would have failed if a user called `obtainCredentials()` without Hive classes loaded.",
    "commit": "c4149ddb940c32285fde8b1c08a5b212fced5d66",
    "createdAt": "2017-06-08T21:51:10Z",
    "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import java.lang.reflect.UndeclaredThrowableException\n+import java.security.PrivilegedExceptionAction\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier\n+import org.apache.hadoop.hive.conf.HiveConf\n+import org.apache.hadoop.hive.ql.metadata.Hive\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.security.{Credentials, UserGroupInformation}\n+import org.apache.hadoop.security.token.Token\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+private[security] class HiveCredentialProvider extends HadoopDelegationTokenProvider with Logging {\n+\n+  override def serviceName: String = \"hive\"\n+\n+  private val classNotFoundErrorStr = \"You are attempting to use the HiveCredentialProvider,\" +\n+    \"but your Spark distribution is not built with Hive libraries.\"\n+\n+  private def hiveConf(hadoopConf: Configuration): Configuration = {\n+    try {\n+      new HiveConf(hadoopConf, classOf[HiveConf])\n+    } catch {\n+      case NonFatal(e) =>\n+        logDebug(\"Fail to create Hive Configuration\", e)\n+        hadoopConf\n+      case e: ClassNotFoundException =>\n+        logError(classNotFoundErrorStr)\n+        throw e"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "Consider it a bug fix then. This is the 1.6 code (or at least the code before the 2.1 refactoring):\r\n\r\n```\r\n  def obtainTokenForHiveMetastore(conf: Configuration): Option[Token[DelegationTokenIdentifier]] = {\r\n    try {\r\n      obtainTokenForHiveMetastoreInner(conf)\r\n    } catch {\r\n      case e: ClassNotFoundException =>\r\n        logInfo(s\"Hive class not found $e\")\r\n        logDebug(\"Hive class not found\", e)\r\n        None\r\n    }\r\n  }\r\n```",
    "commit": "c4149ddb940c32285fde8b1c08a5b212fced5d66",
    "createdAt": "2017-06-08T21:54:05Z",
    "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import java.lang.reflect.UndeclaredThrowableException\n+import java.security.PrivilegedExceptionAction\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier\n+import org.apache.hadoop.hive.conf.HiveConf\n+import org.apache.hadoop.hive.ql.metadata.Hive\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.security.{Credentials, UserGroupInformation}\n+import org.apache.hadoop.security.token.Token\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+private[security] class HiveCredentialProvider extends HadoopDelegationTokenProvider with Logging {\n+\n+  override def serviceName: String = \"hive\"\n+\n+  private val classNotFoundErrorStr = \"You are attempting to use the HiveCredentialProvider,\" +\n+    \"but your Spark distribution is not built with Hive libraries.\"\n+\n+  private def hiveConf(hadoopConf: Configuration): Configuration = {\n+    try {\n+      new HiveConf(hadoopConf, classOf[HiveConf])\n+    } catch {\n+      case NonFatal(e) =>\n+        logDebug(\"Fail to create Hive Configuration\", e)\n+        hadoopConf\n+      case e: ClassNotFoundException =>\n+        logError(classNotFoundErrorStr)\n+        throw e"
  }, {
    "author": {
      "login": "mgummelt"
    },
    "body": "OK.  fixed.",
    "commit": "c4149ddb940c32285fde8b1c08a5b212fced5d66",
    "createdAt": "2017-06-08T21:56:04Z",
    "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import java.lang.reflect.UndeclaredThrowableException\n+import java.security.PrivilegedExceptionAction\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier\n+import org.apache.hadoop.hive.conf.HiveConf\n+import org.apache.hadoop.hive.ql.metadata.Hive\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.security.{Credentials, UserGroupInformation}\n+import org.apache.hadoop.security.token.Token\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+private[security] class HiveCredentialProvider extends HadoopDelegationTokenProvider with Logging {\n+\n+  override def serviceName: String = \"hive\"\n+\n+  private val classNotFoundErrorStr = \"You are attempting to use the HiveCredentialProvider,\" +\n+    \"but your Spark distribution is not built with Hive libraries.\"\n+\n+  private def hiveConf(hadoopConf: Configuration): Configuration = {\n+    try {\n+      new HiveConf(hadoopConf, classOf[HiveConf])\n+    } catch {\n+      case NonFatal(e) =>\n+        logDebug(\"Fail to create Hive Configuration\", e)\n+        hadoopConf\n+      case e: ClassNotFoundException =>\n+        logError(classNotFoundErrorStr)\n+        throw e"
  }],
  "prId": 17723
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "This is the wrong exception to catch; this worked with reflection, but not with regular code:\r\n\r\n```\r\njava.lang.NoClassDefFoundError: org/apache/hadoop/hive/conf/HiveConf\r\n  at org.apache.spark.deploy.security.ConfigurableCredentialManager.getCredentialProviders(ConfigurableCredentialManager.scala:59)\r\n  at org.apache.spark.deploy.security.ConfigurableCredentialManager.<init>(ConfigurableCredentialManager.scala:54)\r\n  at org.apache.spark.deploy.yarn.security.YARNConfigurableCredentialManager.<init>(YARNConfigurableCredentialManager.scala:45)\r\n  at org.apache.spark.deploy.yarn.Client.<init>(Client.scala:124)\r\n```",
    "commit": "c4149ddb940c32285fde8b1c08a5b212fced5d66",
    "createdAt": "2017-06-07T23:24:09Z",
    "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import java.lang.reflect.UndeclaredThrowableException\n+import java.security.PrivilegedExceptionAction\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier\n+import org.apache.hadoop.hive.conf.HiveConf\n+import org.apache.hadoop.hive.ql.metadata.Hive\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.security.{Credentials, UserGroupInformation}\n+import org.apache.hadoop.security.token.Token\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+private[security] class HiveCredentialProvider extends HadoopDelegationTokenProvider with Logging {\n+\n+  override def serviceName: String = \"hive\"\n+\n+  private val classNotFoundErrorStr = \"You are attempting to use the HiveCredentialProvider,\" +\n+    \"but your Spark distribution is not built with Hive libraries.\"\n+\n+  private def hiveConf(hadoopConf: Configuration): Configuration = {\n+    try {\n+      new HiveConf(hadoopConf, classOf[HiveConf])\n+    } catch {\n+      case NonFatal(e) =>\n+        logDebug(\"Fail to create Hive Configuration\", e)\n+        hadoopConf\n+      case e: ClassNotFoundException =>"
  }, {
    "author": {
      "login": "mgummelt"
    },
    "body": "fixed",
    "commit": "c4149ddb940c32285fde8b1c08a5b212fced5d66",
    "createdAt": "2017-06-08T21:19:07Z",
    "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import java.lang.reflect.UndeclaredThrowableException\n+import java.security.PrivilegedExceptionAction\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier\n+import org.apache.hadoop.hive.conf.HiveConf\n+import org.apache.hadoop.hive.ql.metadata.Hive\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.security.{Credentials, UserGroupInformation}\n+import org.apache.hadoop.security.token.Token\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+private[security] class HiveCredentialProvider extends HadoopDelegationTokenProvider with Logging {\n+\n+  override def serviceName: String = \"hive\"\n+\n+  private val classNotFoundErrorStr = \"You are attempting to use the HiveCredentialProvider,\" +\n+    \"but your Spark distribution is not built with Hive libraries.\"\n+\n+  private def hiveConf(hadoopConf: Configuration): Configuration = {\n+    try {\n+      new HiveConf(hadoopConf, classOf[HiveConf])\n+    } catch {\n+      case NonFatal(e) =>\n+        logDebug(\"Fail to create Hive Configuration\", e)\n+        hadoopConf\n+      case e: ClassNotFoundException =>"
  }],
  "prId": 17723
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "Nit: `,\"` -> `, \"`",
    "commit": "c4149ddb940c32285fde8b1c08a5b212fced5d66",
    "createdAt": "2017-06-09T21:37:15Z",
    "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import java.lang.reflect.UndeclaredThrowableException\n+import java.security.PrivilegedExceptionAction\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier\n+import org.apache.hadoop.hive.conf.HiveConf\n+import org.apache.hadoop.hive.ql.metadata.Hive\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.security.{Credentials, UserGroupInformation}\n+import org.apache.hadoop.security.token.Token\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+private[security] class HiveCredentialProvider extends HadoopDelegationTokenProvider with Logging {\n+\n+  override def serviceName: String = \"hive\"\n+\n+  private val classNotFoundErrorStr = \"You are attempting to use the HiveCredentialProvider,\" +"
  }, {
    "author": {
      "login": "mgummelt"
    },
    "body": "fixed",
    "commit": "c4149ddb940c32285fde8b1c08a5b212fced5d66",
    "createdAt": "2017-06-12T17:43:09Z",
    "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import java.lang.reflect.UndeclaredThrowableException\n+import java.security.PrivilegedExceptionAction\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier\n+import org.apache.hadoop.hive.conf.HiveConf\n+import org.apache.hadoop.hive.ql.metadata.Hive\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.security.{Credentials, UserGroupInformation}\n+import org.apache.hadoop.security.token.Token\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+private[security] class HiveCredentialProvider extends HadoopDelegationTokenProvider with Logging {\n+\n+  override def serviceName: String = \"hive\"\n+\n+  private val classNotFoundErrorStr = \"You are attempting to use the HiveCredentialProvider,\" +"
  }],
  "prId": 17723
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Move this catch block to the end of the method?",
    "commit": "c4149ddb940c32285fde8b1c08a5b212fced5d66",
    "createdAt": "2017-06-09T22:40:48Z",
    "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import java.lang.reflect.UndeclaredThrowableException\n+import java.security.PrivilegedExceptionAction\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier\n+import org.apache.hadoop.hive.conf.HiveConf\n+import org.apache.hadoop.hive.ql.metadata.Hive\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.security.{Credentials, UserGroupInformation}\n+import org.apache.hadoop.security.token.Token\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+private[security] class HiveCredentialProvider extends HadoopDelegationTokenProvider with Logging {\n+\n+  override def serviceName: String = \"hive\"\n+\n+  private val classNotFoundErrorStr = \"You are attempting to use the HiveCredentialProvider,\" +\n+    \"but your Spark distribution is not built with Hive libraries.\"\n+\n+  private def hiveConf(hadoopConf: Configuration): Configuration = {\n+    try {\n+      new HiveConf(hadoopConf, classOf[HiveConf])\n+    } catch {\n+      case NonFatal(e) =>\n+        logDebug(\"Fail to create Hive Configuration\", e)\n+        hadoopConf\n+      case e: NoClassDefFoundError =>\n+        logWarning(classNotFoundErrorStr)\n+        hadoopConf\n+    }\n+  }\n+\n+  override def credentialsRequired(hadoopConf: Configuration): Boolean = {\n+    UserGroupInformation.isSecurityEnabled &&\n+      hiveConf(hadoopConf).getTrimmed(\"hive.metastore.uris\", \"\").nonEmpty\n+  }\n+\n+  override def obtainCredentials(\n+      hadoopConf: Configuration,\n+      creds: Credentials): Option[Long] = {\n+    try {\n+      val conf = hiveConf(hadoopConf)\n+\n+      val principalKey = \"hive.metastore.kerberos.principal\"\n+      val principal = conf.getTrimmed(principalKey, \"\")\n+      require(principal.nonEmpty, s\"Hive principal $principalKey undefined\")\n+      val metastoreUri = conf.getTrimmed(\"hive.metastore.uris\", \"\")\n+      require(metastoreUri.nonEmpty, \"Hive metastore uri undefined\")\n+\n+      val currentUser = UserGroupInformation.getCurrentUser()\n+      logDebug(s\"Getting Hive delegation token for ${currentUser.getUserName()} against \" +\n+        s\"$principal at $metastoreUri\")\n+\n+      try {\n+        doAsRealUser {\n+          val hive = Hive.get(conf, classOf[HiveConf])\n+          val tokenStr = hive.getDelegationToken(currentUser.getUserName(), principal)\n+\n+          val hive2Token = new Token[DelegationTokenIdentifier]()\n+          hive2Token.decodeFromUrlString(tokenStr)\n+          logInfo(s\"Get Token from hive metastore: ${hive2Token.toString}\")\n+          creds.addToken(new Text(\"hive.server2.delegation.token\"), hive2Token)\n+        }\n+      } catch {\n+        case NonFatal(e) =>"
  }, {
    "author": {
      "login": "mgummelt"
    },
    "body": "good catch.  fixed.",
    "commit": "c4149ddb940c32285fde8b1c08a5b212fced5d66",
    "createdAt": "2017-06-12T17:46:08Z",
    "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import java.lang.reflect.UndeclaredThrowableException\n+import java.security.PrivilegedExceptionAction\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier\n+import org.apache.hadoop.hive.conf.HiveConf\n+import org.apache.hadoop.hive.ql.metadata.Hive\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.security.{Credentials, UserGroupInformation}\n+import org.apache.hadoop.security.token.Token\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+private[security] class HiveCredentialProvider extends HadoopDelegationTokenProvider with Logging {\n+\n+  override def serviceName: String = \"hive\"\n+\n+  private val classNotFoundErrorStr = \"You are attempting to use the HiveCredentialProvider,\" +\n+    \"but your Spark distribution is not built with Hive libraries.\"\n+\n+  private def hiveConf(hadoopConf: Configuration): Configuration = {\n+    try {\n+      new HiveConf(hadoopConf, classOf[HiveConf])\n+    } catch {\n+      case NonFatal(e) =>\n+        logDebug(\"Fail to create Hive Configuration\", e)\n+        hadoopConf\n+      case e: NoClassDefFoundError =>\n+        logWarning(classNotFoundErrorStr)\n+        hadoopConf\n+    }\n+  }\n+\n+  override def credentialsRequired(hadoopConf: Configuration): Boolean = {\n+    UserGroupInformation.isSecurityEnabled &&\n+      hiveConf(hadoopConf).getTrimmed(\"hive.metastore.uris\", \"\").nonEmpty\n+  }\n+\n+  override def obtainCredentials(\n+      hadoopConf: Configuration,\n+      creds: Credentials): Option[Long] = {\n+    try {\n+      val conf = hiveConf(hadoopConf)\n+\n+      val principalKey = \"hive.metastore.kerberos.principal\"\n+      val principal = conf.getTrimmed(principalKey, \"\")\n+      require(principal.nonEmpty, s\"Hive principal $principalKey undefined\")\n+      val metastoreUri = conf.getTrimmed(\"hive.metastore.uris\", \"\")\n+      require(metastoreUri.nonEmpty, \"Hive metastore uri undefined\")\n+\n+      val currentUser = UserGroupInformation.getCurrentUser()\n+      logDebug(s\"Getting Hive delegation token for ${currentUser.getUserName()} against \" +\n+        s\"$principal at $metastoreUri\")\n+\n+      try {\n+        doAsRealUser {\n+          val hive = Hive.get(conf, classOf[HiveConf])\n+          val tokenStr = hive.getDelegationToken(currentUser.getUserName(), principal)\n+\n+          val hive2Token = new Token[DelegationTokenIdentifier]()\n+          hive2Token.decodeFromUrlString(tokenStr)\n+          logInfo(s\"Get Token from hive metastore: ${hive2Token.toString}\")\n+          creds.addToken(new Text(\"hive.server2.delegation.token\"), hive2Token)\n+        }\n+      } catch {\n+        case NonFatal(e) =>"
  }],
  "prId": 17723
}]