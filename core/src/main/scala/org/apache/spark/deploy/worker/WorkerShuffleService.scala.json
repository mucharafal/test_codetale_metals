[{
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "Should we have some node-level environment variable in addition to this? The point is that if the Worker is started on a different port then it would be good if the executors can just pick it up. Otherwise the executors have no way of figuring out which other port the service is started on unless they go check out the worker logs or something. In Yarn for instance we have shared Yarn config used by both the executors and the NM. Though this is probably fine for first-cut implementation. Any thoughts @pwendell \n",
    "commit": "3780bd704adfc738ea70bd6d311a2c824e3849c6",
    "createdAt": "2014-11-06T22:44:12Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.worker\n+\n+import org.apache.spark.{Logging, SparkConf, SecurityManager}\n+import org.apache.spark.network.TransportContext\n+import org.apache.spark.network.netty.SparkTransportConf\n+import org.apache.spark.network.sasl.SaslRpcHandler\n+import org.apache.spark.network.server.TransportServer\n+import org.apache.spark.network.shuffle.ExternalShuffleBlockHandler\n+\n+/**\n+ * Provides a server from which Executors can read shuffle files (rather than reading directly from\n+ * each other), to provide uninterrupted access to the files in the face of executors being turned\n+ * off or killed.\n+ *\n+ * Optionally requires SASL authentication in order to read. See [[SecurityManager]].\n+ */\n+private[worker]\n+class WorkerShuffleService(sparkConf: SparkConf, securityManager: SecurityManager) extends Logging {\n+\n+  private val enabled = sparkConf.getBoolean(\"spark.shuffle.service.enabled\", false)\n+  private val port = sparkConf.getInt(\"spark.shuffle.service.port\", 7337)"
  }],
  "prId": 3142
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "I wouldn't be opposed to this being called `StandaloneShuffleService` now that you renamed the other one `ExternalShuffleService`. This name would match `YarnShuffleService` more. The existing name is also fine.\n",
    "commit": "3780bd704adfc738ea70bd6d311a2c824e3849c6",
    "createdAt": "2014-11-06T22:47:01Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.worker\n+\n+import org.apache.spark.{Logging, SparkConf, SecurityManager}\n+import org.apache.spark.network.TransportContext\n+import org.apache.spark.network.netty.SparkTransportConf\n+import org.apache.spark.network.sasl.SaslRpcHandler\n+import org.apache.spark.network.server.TransportServer\n+import org.apache.spark.network.shuffle.ExternalShuffleBlockHandler\n+\n+/**\n+ * Provides a server from which Executors can read shuffle files (rather than reading directly from\n+ * each other), to provide uninterrupted access to the files in the face of executors being turned\n+ * off or killed.\n+ *\n+ * Optionally requires SASL authentication in order to read. See [[SecurityManager]].\n+ */\n+private[worker]\n+class WorkerShuffleService(sparkConf: SparkConf, securityManager: SecurityManager) extends Logging {"
  }],
  "prId": 3142
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "How about we do the enable check outside of this class and instantiate it only if it's enabled? Then we don't need to create all the RPC handler and stuff when we won't actually use them. Alternatively we could just move all of those things into this method.\n",
    "commit": "3780bd704adfc738ea70bd6d311a2c824e3849c6",
    "createdAt": "2014-11-06T22:54:04Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.worker\n+\n+import org.apache.spark.{Logging, SparkConf, SecurityManager}\n+import org.apache.spark.network.TransportContext\n+import org.apache.spark.network.netty.SparkTransportConf\n+import org.apache.spark.network.sasl.SaslRpcHandler\n+import org.apache.spark.network.server.TransportServer\n+import org.apache.spark.network.shuffle.ExternalShuffleBlockHandler\n+\n+/**\n+ * Provides a server from which Executors can read shuffle files (rather than reading directly from\n+ * each other), to provide uninterrupted access to the files in the face of executors being turned\n+ * off or killed.\n+ *\n+ * Optionally requires SASL authentication in order to read. See [[SecurityManager]].\n+ */\n+private[worker]\n+class WorkerShuffleService(sparkConf: SparkConf, securityManager: SecurityManager) extends Logging {\n+\n+  private val enabled = sparkConf.getBoolean(\"spark.shuffle.service.enabled\", false)\n+  private val port = sparkConf.getInt(\"spark.shuffle.service.port\", 7337)\n+  private val useSasl: Boolean = securityManager.isAuthenticationEnabled()\n+\n+  private val transportConf = SparkTransportConf.fromSparkConf(sparkConf)\n+  private val blockHandler = new ExternalShuffleBlockHandler()\n+  private val transportContext: TransportContext = {\n+    val handler = if (useSasl) new SaslRpcHandler(blockHandler, securityManager) else blockHandler\n+    new TransportContext(transportConf, handler)\n+  }\n+\n+  private var server: TransportServer = _\n+\n+  /** Starts the external shuffle service if the user has configured us to. */\n+  def startIfEnabled() {\n+    if (enabled) {\n+      require(server == null, \"Shuffle server already started\")\n+      logInfo(s\"Starting shuffle service on port $port with useSasl = $useSasl\")\n+      server = transportContext.createServer(port)\n+    }\n+  }"
  }, {
    "author": {
      "login": "aarondav"
    },
    "body": "Intentionally, the constructors of those objects do no work at all, only setting field variables. It should be generally reasonable to construct these objects and expect no allocation or threads to be started.\n",
    "commit": "3780bd704adfc738ea70bd6d311a2c824e3849c6",
    "createdAt": "2014-11-06T23:25:32Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.worker\n+\n+import org.apache.spark.{Logging, SparkConf, SecurityManager}\n+import org.apache.spark.network.TransportContext\n+import org.apache.spark.network.netty.SparkTransportConf\n+import org.apache.spark.network.sasl.SaslRpcHandler\n+import org.apache.spark.network.server.TransportServer\n+import org.apache.spark.network.shuffle.ExternalShuffleBlockHandler\n+\n+/**\n+ * Provides a server from which Executors can read shuffle files (rather than reading directly from\n+ * each other), to provide uninterrupted access to the files in the face of executors being turned\n+ * off or killed.\n+ *\n+ * Optionally requires SASL authentication in order to read. See [[SecurityManager]].\n+ */\n+private[worker]\n+class WorkerShuffleService(sparkConf: SparkConf, securityManager: SecurityManager) extends Logging {\n+\n+  private val enabled = sparkConf.getBoolean(\"spark.shuffle.service.enabled\", false)\n+  private val port = sparkConf.getInt(\"spark.shuffle.service.port\", 7337)\n+  private val useSasl: Boolean = securityManager.isAuthenticationEnabled()\n+\n+  private val transportConf = SparkTransportConf.fromSparkConf(sparkConf)\n+  private val blockHandler = new ExternalShuffleBlockHandler()\n+  private val transportContext: TransportContext = {\n+    val handler = if (useSasl) new SaslRpcHandler(blockHandler, securityManager) else blockHandler\n+    new TransportContext(transportConf, handler)\n+  }\n+\n+  private var server: TransportServer = _\n+\n+  /** Starts the external shuffle service if the user has configured us to. */\n+  def startIfEnabled() {\n+    if (enabled) {\n+      require(server == null, \"Shuffle server already started\")\n+      logInfo(s\"Starting shuffle service on port $port with useSasl = $useSasl\")\n+      server = transportContext.createServer(port)\n+    }\n+  }"
  }],
  "prId": 3142
}]