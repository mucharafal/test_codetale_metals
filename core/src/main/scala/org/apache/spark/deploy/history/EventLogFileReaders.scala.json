[{
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "This is not thread-safe, pretty sure this can be an issue in the SHS.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-26T00:16:59Z",
    "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.{BufferedInputStream, InputStream}\n+import java.util.zip.{ZipEntry, ZipOutputStream}\n+\n+import scala.collection.mutable\n+\n+import com.google.common.io.ByteStreams\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.apache.hadoop.hdfs.DFSInputStream\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.history.EventLogFileWriter.codecName\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/** The base class of reader which will read the information of event log file(s). */\n+abstract class EventLogFileReader(\n+    protected val fileSystem: FileSystem,\n+    val rootPath: Path) {\n+\n+  protected def fileSizeForDFS(path: Path): Option[Long] = {\n+    Utils.tryWithResource(fileSystem.open(path)) { in =>\n+      in.getWrappedStream match {\n+        case dfsIn: DFSInputStream => Some(dfsIn.getFileLength)\n+        case _ => None\n+      }\n+    }\n+  }\n+\n+  protected def addFileAsZipEntry(\n+      zipStream: ZipOutputStream,\n+      path: Path,\n+      entryName: String): Unit = {\n+    Utils.tryWithResource(fileSystem.open(path, 1 * 1024 * 1024)) { inputStream =>\n+      zipStream.putNextEntry(new ZipEntry(entryName))\n+      ByteStreams.copy(inputStream, zipStream)\n+      zipStream.closeEntry()\n+    }\n+  }\n+\n+  /** Returns the last index of event log files. None for single event log file. */\n+  def lastIndex: Option[Long]\n+\n+  /**\n+   * Returns the size of file for the last index of event log files. Returns its size for\n+   * single event log file.\n+   */\n+  def fileSizeForLastIndex: Long\n+\n+  /** Returns whether the application is completed. */\n+  def completed: Boolean\n+\n+  /**\n+   * Returns the size of file for the last index of event log files, only when\n+   * underlying input stream is DFSInputStream. Otherwise returns None.\n+   */\n+  def fileSizeForLastIndexForDFS: Option[Long]\n+\n+  /** Returns the modification time for the last sequence number of event log files. */\n+  def modificationTime: Long\n+\n+  /**\n+   * This method compresses the files passed in, and writes the compressed data out into the\n+   * ZipOutputStream passed in. Each file is written as a new ZipEntry with its name being\n+   * the name of the file being compressed.\n+   */\n+  def zipEventLogFiles(zipStream: ZipOutputStream): Unit\n+\n+  /** Returns all available event log files. */\n+  def listEventLogFiles: Seq[FileStatus]\n+\n+  /** Returns the short compression name if being used. None if it's uncompressed. */\n+  def compressionCodec: Option[String]\n+\n+  /** Returns the size of all event log files. */\n+  def totalSize: Long\n+}\n+\n+object EventLogFileReader {\n+  // A cache for compression codecs to avoid creating the same codec many times\n+  private val codecMap = mutable.HashMap.empty[String, CompressionCodec]\n+\n+  def apply(\n+      fs: FileSystem,\n+      path: Path,\n+      lastSequenceNumber: Option[Long]): EventLogFileReader = {\n+    lastSequenceNumber match {\n+      case Some(_) => new RollingEventLogFilesFileReader(fs, path)\n+      case None => new SingleFileEventLogFileReader(fs, path)\n+    }\n+  }\n+\n+  def apply(fs: FileSystem, path: Path): Option[EventLogFileReader] = {\n+    apply(fs, fs.getFileStatus(path))\n+  }\n+\n+  def apply(fs: FileSystem, status: FileStatus): Option[EventLogFileReader] = {\n+    if (isSingleEventLog(status)) {\n+      Some(new SingleFileEventLogFileReader(fs, status.getPath))\n+    } else if (isRollingEventLogs(status)) {\n+      Some(new RollingEventLogFilesFileReader(fs, status.getPath))\n+    } else {\n+      None\n+    }\n+  }\n+\n+  /**\n+   * Opens an event log file and returns an input stream that contains the event data.\n+   *\n+   * @return input stream that holds one JSON record per line.\n+   */\n+  def openEventLog(log: Path, fs: FileSystem): InputStream = {\n+    val in = new BufferedInputStream(fs.open(log))\n+    try {\n+      val codec = codecName(log).map { c =>",
    "line": 135
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Not sure what you mean here, but it should be \"status\".",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-26T00:17:55Z",
    "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.{BufferedInputStream, InputStream}\n+import java.util.zip.{ZipEntry, ZipOutputStream}\n+\n+import scala.collection.mutable\n+\n+import com.google.common.io.ByteStreams\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.apache.hadoop.hdfs.DFSInputStream\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.history.EventLogFileWriter.codecName\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/** The base class of reader which will read the information of event log file(s). */\n+abstract class EventLogFileReader(\n+    protected val fileSystem: FileSystem,\n+    val rootPath: Path) {\n+\n+  protected def fileSizeForDFS(path: Path): Option[Long] = {\n+    Utils.tryWithResource(fileSystem.open(path)) { in =>\n+      in.getWrappedStream match {\n+        case dfsIn: DFSInputStream => Some(dfsIn.getFileLength)\n+        case _ => None\n+      }\n+    }\n+  }\n+\n+  protected def addFileAsZipEntry(\n+      zipStream: ZipOutputStream,\n+      path: Path,\n+      entryName: String): Unit = {\n+    Utils.tryWithResource(fileSystem.open(path, 1 * 1024 * 1024)) { inputStream =>\n+      zipStream.putNextEntry(new ZipEntry(entryName))\n+      ByteStreams.copy(inputStream, zipStream)\n+      zipStream.closeEntry()\n+    }\n+  }\n+\n+  /** Returns the last index of event log files. None for single event log file. */\n+  def lastIndex: Option[Long]\n+\n+  /**\n+   * Returns the size of file for the last index of event log files. Returns its size for\n+   * single event log file.\n+   */\n+  def fileSizeForLastIndex: Long\n+\n+  /** Returns whether the application is completed. */\n+  def completed: Boolean\n+\n+  /**\n+   * Returns the size of file for the last index of event log files, only when\n+   * underlying input stream is DFSInputStream. Otherwise returns None.\n+   */\n+  def fileSizeForLastIndexForDFS: Option[Long]\n+\n+  /** Returns the modification time for the last sequence number of event log files. */\n+  def modificationTime: Long\n+\n+  /**\n+   * This method compresses the files passed in, and writes the compressed data out into the\n+   * ZipOutputStream passed in. Each file is written as a new ZipEntry with its name being\n+   * the name of the file being compressed.\n+   */\n+  def zipEventLogFiles(zipStream: ZipOutputStream): Unit\n+\n+  /** Returns all available event log files. */\n+  def listEventLogFiles: Seq[FileStatus]\n+\n+  /** Returns the short compression name if being used. None if it's uncompressed. */\n+  def compressionCodec: Option[String]\n+\n+  /** Returns the size of all event log files. */\n+  def totalSize: Long\n+}\n+\n+object EventLogFileReader {\n+  // A cache for compression codecs to avoid creating the same codec many times\n+  private val codecMap = mutable.HashMap.empty[String, CompressionCodec]\n+\n+  def apply(\n+      fs: FileSystem,\n+      path: Path,\n+      lastSequenceNumber: Option[Long]): EventLogFileReader = {\n+    lastSequenceNumber match {\n+      case Some(_) => new RollingEventLogFilesFileReader(fs, path)\n+      case None => new SingleFileEventLogFileReader(fs, path)\n+    }\n+  }\n+\n+  def apply(fs: FileSystem, path: Path): Option[EventLogFileReader] = {\n+    apply(fs, fs.getFileStatus(path))\n+  }\n+\n+  def apply(fs: FileSystem, status: FileStatus): Option[EventLogFileReader] = {\n+    if (isSingleEventLog(status)) {\n+      Some(new SingleFileEventLogFileReader(fs, status.getPath))\n+    } else if (isRollingEventLogs(status)) {\n+      Some(new RollingEventLogFilesFileReader(fs, status.getPath))\n+    } else {\n+      None\n+    }\n+  }\n+\n+  /**\n+   * Opens an event log file and returns an input stream that contains the event data.\n+   *\n+   * @return input stream that holds one JSON record per line.\n+   */\n+  def openEventLog(log: Path, fs: FileSystem): InputStream = {\n+    val in = new BufferedInputStream(fs.open(log))\n+    try {\n+      val codec = codecName(log).map { c =>\n+        codecMap.getOrElseUpdate(c, CompressionCodec.createCodec(new SparkConf, c))\n+      }\n+      codec.map(_.compressedContinuousInputStream(in)).getOrElse(in)\n+    } catch {\n+      case e: Throwable =>\n+        in.close()\n+        throw e\n+    }\n+  }\n+\n+  private def isSingleEventLog(status: FileStatus): Boolean = {\n+    !status.isDirectory &&\n+      // FsHistoryProvider used to generate a hidden file which can't be read.  Accidentally\n+      // reading a garbage file is safe, but we would log an error which can be scary to\n+      // the end-user.\n+      !status.getPath.getName.startsWith(\".\")\n+  }\n+\n+  private def isRollingEventLogs(status: FileStatus): Boolean = {\n+    status.isDirectory && RollingEventLogFilesWriter.isEventLogDir(status)\n+  }\n+}\n+\n+/** The reader which will read the information of single event log file. */\n+class SingleFileEventLogFileReader(\n+    fs: FileSystem,\n+    path: Path) extends EventLogFileReader(fs, path) {\n+  // TODO: get stats with constructor and only call if it's needed?"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "I'll just remove this comment for now. Left for TODO but not strictly needed.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-26T01:14:38Z",
    "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.{BufferedInputStream, InputStream}\n+import java.util.zip.{ZipEntry, ZipOutputStream}\n+\n+import scala.collection.mutable\n+\n+import com.google.common.io.ByteStreams\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.apache.hadoop.hdfs.DFSInputStream\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.history.EventLogFileWriter.codecName\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/** The base class of reader which will read the information of event log file(s). */\n+abstract class EventLogFileReader(\n+    protected val fileSystem: FileSystem,\n+    val rootPath: Path) {\n+\n+  protected def fileSizeForDFS(path: Path): Option[Long] = {\n+    Utils.tryWithResource(fileSystem.open(path)) { in =>\n+      in.getWrappedStream match {\n+        case dfsIn: DFSInputStream => Some(dfsIn.getFileLength)\n+        case _ => None\n+      }\n+    }\n+  }\n+\n+  protected def addFileAsZipEntry(\n+      zipStream: ZipOutputStream,\n+      path: Path,\n+      entryName: String): Unit = {\n+    Utils.tryWithResource(fileSystem.open(path, 1 * 1024 * 1024)) { inputStream =>\n+      zipStream.putNextEntry(new ZipEntry(entryName))\n+      ByteStreams.copy(inputStream, zipStream)\n+      zipStream.closeEntry()\n+    }\n+  }\n+\n+  /** Returns the last index of event log files. None for single event log file. */\n+  def lastIndex: Option[Long]\n+\n+  /**\n+   * Returns the size of file for the last index of event log files. Returns its size for\n+   * single event log file.\n+   */\n+  def fileSizeForLastIndex: Long\n+\n+  /** Returns whether the application is completed. */\n+  def completed: Boolean\n+\n+  /**\n+   * Returns the size of file for the last index of event log files, only when\n+   * underlying input stream is DFSInputStream. Otherwise returns None.\n+   */\n+  def fileSizeForLastIndexForDFS: Option[Long]\n+\n+  /** Returns the modification time for the last sequence number of event log files. */\n+  def modificationTime: Long\n+\n+  /**\n+   * This method compresses the files passed in, and writes the compressed data out into the\n+   * ZipOutputStream passed in. Each file is written as a new ZipEntry with its name being\n+   * the name of the file being compressed.\n+   */\n+  def zipEventLogFiles(zipStream: ZipOutputStream): Unit\n+\n+  /** Returns all available event log files. */\n+  def listEventLogFiles: Seq[FileStatus]\n+\n+  /** Returns the short compression name if being used. None if it's uncompressed. */\n+  def compressionCodec: Option[String]\n+\n+  /** Returns the size of all event log files. */\n+  def totalSize: Long\n+}\n+\n+object EventLogFileReader {\n+  // A cache for compression codecs to avoid creating the same codec many times\n+  private val codecMap = mutable.HashMap.empty[String, CompressionCodec]\n+\n+  def apply(\n+      fs: FileSystem,\n+      path: Path,\n+      lastSequenceNumber: Option[Long]): EventLogFileReader = {\n+    lastSequenceNumber match {\n+      case Some(_) => new RollingEventLogFilesFileReader(fs, path)\n+      case None => new SingleFileEventLogFileReader(fs, path)\n+    }\n+  }\n+\n+  def apply(fs: FileSystem, path: Path): Option[EventLogFileReader] = {\n+    apply(fs, fs.getFileStatus(path))\n+  }\n+\n+  def apply(fs: FileSystem, status: FileStatus): Option[EventLogFileReader] = {\n+    if (isSingleEventLog(status)) {\n+      Some(new SingleFileEventLogFileReader(fs, status.getPath))\n+    } else if (isRollingEventLogs(status)) {\n+      Some(new RollingEventLogFilesFileReader(fs, status.getPath))\n+    } else {\n+      None\n+    }\n+  }\n+\n+  /**\n+   * Opens an event log file and returns an input stream that contains the event data.\n+   *\n+   * @return input stream that holds one JSON record per line.\n+   */\n+  def openEventLog(log: Path, fs: FileSystem): InputStream = {\n+    val in = new BufferedInputStream(fs.open(log))\n+    try {\n+      val codec = codecName(log).map { c =>\n+        codecMap.getOrElseUpdate(c, CompressionCodec.createCodec(new SparkConf, c))\n+      }\n+      codec.map(_.compressedContinuousInputStream(in)).getOrElse(in)\n+    } catch {\n+      case e: Throwable =>\n+        in.close()\n+        throw e\n+    }\n+  }\n+\n+  private def isSingleEventLog(status: FileStatus): Boolean = {\n+    !status.isDirectory &&\n+      // FsHistoryProvider used to generate a hidden file which can't be read.  Accidentally\n+      // reading a garbage file is safe, but we would log an error which can be scary to\n+      // the end-user.\n+      !status.getPath.getName.startsWith(\".\")\n+  }\n+\n+  private def isRollingEventLogs(status: FileStatus): Boolean = {\n+    status.isDirectory && RollingEventLogFilesWriter.isEventLogDir(status)\n+  }\n+}\n+\n+/** The reader which will read the information of single event log file. */\n+class SingleFileEventLogFileReader(\n+    fs: FileSystem,\n+    path: Path) extends EventLogFileReader(fs, path) {\n+  // TODO: get stats with constructor and only call if it's needed?"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "status",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-26T00:19:52Z",
    "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.{BufferedInputStream, InputStream}\n+import java.util.zip.{ZipEntry, ZipOutputStream}\n+\n+import scala.collection.mutable\n+\n+import com.google.common.io.ByteStreams\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.apache.hadoop.hdfs.DFSInputStream\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.history.EventLogFileWriter.codecName\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/** The base class of reader which will read the information of event log file(s). */\n+abstract class EventLogFileReader(\n+    protected val fileSystem: FileSystem,\n+    val rootPath: Path) {\n+\n+  protected def fileSizeForDFS(path: Path): Option[Long] = {\n+    Utils.tryWithResource(fileSystem.open(path)) { in =>\n+      in.getWrappedStream match {\n+        case dfsIn: DFSInputStream => Some(dfsIn.getFileLength)\n+        case _ => None\n+      }\n+    }\n+  }\n+\n+  protected def addFileAsZipEntry(\n+      zipStream: ZipOutputStream,\n+      path: Path,\n+      entryName: String): Unit = {\n+    Utils.tryWithResource(fileSystem.open(path, 1 * 1024 * 1024)) { inputStream =>\n+      zipStream.putNextEntry(new ZipEntry(entryName))\n+      ByteStreams.copy(inputStream, zipStream)\n+      zipStream.closeEntry()\n+    }\n+  }\n+\n+  /** Returns the last index of event log files. None for single event log file. */\n+  def lastIndex: Option[Long]\n+\n+  /**\n+   * Returns the size of file for the last index of event log files. Returns its size for\n+   * single event log file.\n+   */\n+  def fileSizeForLastIndex: Long\n+\n+  /** Returns whether the application is completed. */\n+  def completed: Boolean\n+\n+  /**\n+   * Returns the size of file for the last index of event log files, only when\n+   * underlying input stream is DFSInputStream. Otherwise returns None.\n+   */\n+  def fileSizeForLastIndexForDFS: Option[Long]\n+\n+  /** Returns the modification time for the last sequence number of event log files. */\n+  def modificationTime: Long\n+\n+  /**\n+   * This method compresses the files passed in, and writes the compressed data out into the\n+   * ZipOutputStream passed in. Each file is written as a new ZipEntry with its name being\n+   * the name of the file being compressed.\n+   */\n+  def zipEventLogFiles(zipStream: ZipOutputStream): Unit\n+\n+  /** Returns all available event log files. */\n+  def listEventLogFiles: Seq[FileStatus]\n+\n+  /** Returns the short compression name if being used. None if it's uncompressed. */\n+  def compressionCodec: Option[String]\n+\n+  /** Returns the size of all event log files. */\n+  def totalSize: Long\n+}\n+\n+object EventLogFileReader {\n+  // A cache for compression codecs to avoid creating the same codec many times\n+  private val codecMap = mutable.HashMap.empty[String, CompressionCodec]\n+\n+  def apply(\n+      fs: FileSystem,\n+      path: Path,\n+      lastSequenceNumber: Option[Long]): EventLogFileReader = {\n+    lastSequenceNumber match {\n+      case Some(_) => new RollingEventLogFilesFileReader(fs, path)\n+      case None => new SingleFileEventLogFileReader(fs, path)\n+    }\n+  }\n+\n+  def apply(fs: FileSystem, path: Path): Option[EventLogFileReader] = {\n+    apply(fs, fs.getFileStatus(path))\n+  }\n+\n+  def apply(fs: FileSystem, status: FileStatus): Option[EventLogFileReader] = {\n+    if (isSingleEventLog(status)) {\n+      Some(new SingleFileEventLogFileReader(fs, status.getPath))\n+    } else if (isRollingEventLogs(status)) {\n+      Some(new RollingEventLogFilesFileReader(fs, status.getPath))\n+    } else {\n+      None\n+    }\n+  }\n+\n+  /**\n+   * Opens an event log file and returns an input stream that contains the event data.\n+   *\n+   * @return input stream that holds one JSON record per line.\n+   */\n+  def openEventLog(log: Path, fs: FileSystem): InputStream = {\n+    val in = new BufferedInputStream(fs.open(log))\n+    try {\n+      val codec = codecName(log).map { c =>\n+        codecMap.getOrElseUpdate(c, CompressionCodec.createCodec(new SparkConf, c))\n+      }\n+      codec.map(_.compressedContinuousInputStream(in)).getOrElse(in)\n+    } catch {\n+      case e: Throwable =>\n+        in.close()\n+        throw e\n+    }\n+  }\n+\n+  private def isSingleEventLog(status: FileStatus): Boolean = {\n+    !status.isDirectory &&\n+      // FsHistoryProvider used to generate a hidden file which can't be read.  Accidentally\n+      // reading a garbage file is safe, but we would log an error which can be scary to\n+      // the end-user.\n+      !status.getPath.getName.startsWith(\".\")\n+  }\n+\n+  private def isRollingEventLogs(status: FileStatus): Boolean = {\n+    status.isDirectory && RollingEventLogFilesWriter.isEventLogDir(status)\n+  }\n+}\n+\n+/** The reader which will read the information of single event log file. */\n+class SingleFileEventLogFileReader(\n+    fs: FileSystem,\n+    path: Path) extends EventLogFileReader(fs, path) {\n+  // TODO: get stats with constructor and only call if it's needed?\n+  private lazy val status = fileSystem.getFileStatus(rootPath)\n+\n+  override def lastIndex: Option[Long] = None\n+\n+  override def fileSizeForLastIndex: Long = status.getLen\n+\n+  override def completed: Boolean = !rootPath.getName.endsWith(EventLogFileWriter.IN_PROGRESS)\n+\n+  override def fileSizeForLastIndexForDFS: Option[Long] = {\n+    if (completed) {\n+      Some(fileSizeForLastIndex)\n+    } else {\n+      fileSizeForDFS(rootPath)\n+    }\n+  }\n+\n+  override def modificationTime: Long = status.getModificationTime\n+\n+  override def zipEventLogFiles(zipStream: ZipOutputStream): Unit =\n+    addFileAsZipEntry(zipStream, rootPath, rootPath.getName)\n+\n+  override def listEventLogFiles: Seq[FileStatus] = Seq(status)\n+\n+  override def compressionCodec: Option[String] = EventLogFileWriter.codecName(rootPath)\n+\n+  override def totalSize: Long = fileSizeForLastIndex\n+}\n+\n+/** The reader which will read the information of rolled multiple event log files. */\n+class RollingEventLogFilesFileReader(\n+    fs: FileSystem,\n+    path: Path) extends EventLogFileReader(fs, path) {\n+  import RollingEventLogFilesWriter._\n+\n+  private lazy val files: Seq[FileStatus] = {\n+    val ret = fs.listStatus(rootPath).toSeq\n+    require(ret.exists(isEventLogFile), \"Log directory must contain at least one event log file!\")\n+    require(ret.exists(isAppStatusFile), \"Log directory must contain an appstatus file!\")\n+    ret\n+  }\n+\n+  override def lastIndex: Option[Long] = {\n+    val maxSeq = files.filter(isEventLogFile)\n+      .map { stats => getSequence(stats.getPath.getName) }"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: use full method with braces for multi-line methods. (Also in a few other places.)",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-26T00:21:14Z",
    "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.{BufferedInputStream, InputStream}\n+import java.util.zip.{ZipEntry, ZipOutputStream}\n+\n+import scala.collection.mutable\n+\n+import com.google.common.io.ByteStreams\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.apache.hadoop.hdfs.DFSInputStream\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.history.EventLogFileWriter.codecName\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/** The base class of reader which will read the information of event log file(s). */\n+abstract class EventLogFileReader(\n+    protected val fileSystem: FileSystem,\n+    val rootPath: Path) {\n+\n+  protected def fileSizeForDFS(path: Path): Option[Long] = {\n+    Utils.tryWithResource(fileSystem.open(path)) { in =>\n+      in.getWrappedStream match {\n+        case dfsIn: DFSInputStream => Some(dfsIn.getFileLength)\n+        case _ => None\n+      }\n+    }\n+  }\n+\n+  protected def addFileAsZipEntry(\n+      zipStream: ZipOutputStream,\n+      path: Path,\n+      entryName: String): Unit = {\n+    Utils.tryWithResource(fileSystem.open(path, 1 * 1024 * 1024)) { inputStream =>\n+      zipStream.putNextEntry(new ZipEntry(entryName))\n+      ByteStreams.copy(inputStream, zipStream)\n+      zipStream.closeEntry()\n+    }\n+  }\n+\n+  /** Returns the last index of event log files. None for single event log file. */\n+  def lastIndex: Option[Long]\n+\n+  /**\n+   * Returns the size of file for the last index of event log files. Returns its size for\n+   * single event log file.\n+   */\n+  def fileSizeForLastIndex: Long\n+\n+  /** Returns whether the application is completed. */\n+  def completed: Boolean\n+\n+  /**\n+   * Returns the size of file for the last index of event log files, only when\n+   * underlying input stream is DFSInputStream. Otherwise returns None.\n+   */\n+  def fileSizeForLastIndexForDFS: Option[Long]\n+\n+  /** Returns the modification time for the last sequence number of event log files. */\n+  def modificationTime: Long\n+\n+  /**\n+   * This method compresses the files passed in, and writes the compressed data out into the\n+   * ZipOutputStream passed in. Each file is written as a new ZipEntry with its name being\n+   * the name of the file being compressed.\n+   */\n+  def zipEventLogFiles(zipStream: ZipOutputStream): Unit\n+\n+  /** Returns all available event log files. */\n+  def listEventLogFiles: Seq[FileStatus]\n+\n+  /** Returns the short compression name if being used. None if it's uncompressed. */\n+  def compressionCodec: Option[String]\n+\n+  /** Returns the size of all event log files. */\n+  def totalSize: Long\n+}\n+\n+object EventLogFileReader {\n+  // A cache for compression codecs to avoid creating the same codec many times\n+  private val codecMap = mutable.HashMap.empty[String, CompressionCodec]\n+\n+  def apply(\n+      fs: FileSystem,\n+      path: Path,\n+      lastSequenceNumber: Option[Long]): EventLogFileReader = {\n+    lastSequenceNumber match {\n+      case Some(_) => new RollingEventLogFilesFileReader(fs, path)\n+      case None => new SingleFileEventLogFileReader(fs, path)\n+    }\n+  }\n+\n+  def apply(fs: FileSystem, path: Path): Option[EventLogFileReader] = {\n+    apply(fs, fs.getFileStatus(path))\n+  }\n+\n+  def apply(fs: FileSystem, status: FileStatus): Option[EventLogFileReader] = {\n+    if (isSingleEventLog(status)) {\n+      Some(new SingleFileEventLogFileReader(fs, status.getPath))\n+    } else if (isRollingEventLogs(status)) {\n+      Some(new RollingEventLogFilesFileReader(fs, status.getPath))\n+    } else {\n+      None\n+    }\n+  }\n+\n+  /**\n+   * Opens an event log file and returns an input stream that contains the event data.\n+   *\n+   * @return input stream that holds one JSON record per line.\n+   */\n+  def openEventLog(log: Path, fs: FileSystem): InputStream = {\n+    val in = new BufferedInputStream(fs.open(log))\n+    try {\n+      val codec = codecName(log).map { c =>\n+        codecMap.getOrElseUpdate(c, CompressionCodec.createCodec(new SparkConf, c))\n+      }\n+      codec.map(_.compressedContinuousInputStream(in)).getOrElse(in)\n+    } catch {\n+      case e: Throwable =>\n+        in.close()\n+        throw e\n+    }\n+  }\n+\n+  private def isSingleEventLog(status: FileStatus): Boolean = {\n+    !status.isDirectory &&\n+      // FsHistoryProvider used to generate a hidden file which can't be read.  Accidentally\n+      // reading a garbage file is safe, but we would log an error which can be scary to\n+      // the end-user.\n+      !status.getPath.getName.startsWith(\".\")\n+  }\n+\n+  private def isRollingEventLogs(status: FileStatus): Boolean = {\n+    status.isDirectory && RollingEventLogFilesWriter.isEventLogDir(status)\n+  }\n+}\n+\n+/** The reader which will read the information of single event log file. */\n+class SingleFileEventLogFileReader(\n+    fs: FileSystem,\n+    path: Path) extends EventLogFileReader(fs, path) {\n+  // TODO: get stats with constructor and only call if it's needed?\n+  private lazy val status = fileSystem.getFileStatus(rootPath)\n+\n+  override def lastIndex: Option[Long] = None\n+\n+  override def fileSizeForLastIndex: Long = status.getLen\n+\n+  override def completed: Boolean = !rootPath.getName.endsWith(EventLogFileWriter.IN_PROGRESS)\n+\n+  override def fileSizeForLastIndexForDFS: Option[Long] = {\n+    if (completed) {\n+      Some(fileSizeForLastIndex)\n+    } else {\n+      fileSizeForDFS(rootPath)\n+    }\n+  }\n+\n+  override def modificationTime: Long = status.getModificationTime\n+\n+  override def zipEventLogFiles(zipStream: ZipOutputStream): Unit =\n+    addFileAsZipEntry(zipStream, rootPath, rootPath.getName)\n+\n+  override def listEventLogFiles: Seq[FileStatus] = Seq(status)\n+\n+  override def compressionCodec: Option[String] = EventLogFileWriter.codecName(rootPath)\n+\n+  override def totalSize: Long = fileSizeForLastIndex\n+}\n+\n+/** The reader which will read the information of rolled multiple event log files. */\n+class RollingEventLogFilesFileReader(\n+    fs: FileSystem,\n+    path: Path) extends EventLogFileReader(fs, path) {\n+  import RollingEventLogFilesWriter._\n+\n+  private lazy val files: Seq[FileStatus] = {\n+    val ret = fs.listStatus(rootPath).toSeq\n+    require(ret.exists(isEventLogFile), \"Log directory must contain at least one event log file!\")\n+    require(ret.exists(isAppStatusFile), \"Log directory must contain an appstatus file!\")\n+    ret\n+  }\n+\n+  override def lastIndex: Option[Long] = {\n+    val maxSeq = files.filter(isEventLogFile)\n+      .map { stats => getSequence(stats.getPath.getName) }\n+      .max\n+    Some(maxSeq)\n+  }\n+\n+  override def fileSizeForLastIndex: Long = lastEventLogFile.getLen\n+\n+  override def completed: Boolean = {\n+    val appStatsFile = files.find(isAppStatusFile)\n+    require(appStatsFile.isDefined)\n+    appStatsFile.exists(!_.getPath.getName.endsWith(EventLogFileWriter.IN_PROGRESS))\n+  }\n+\n+  override def fileSizeForLastIndexForDFS: Option[Long] = {\n+    if (completed) {\n+      Some(fileSizeForLastIndex)\n+    } else {\n+      fileSizeForDFS(lastEventLogFile.getPath)\n+    }\n+  }\n+\n+  override def modificationTime: Long = lastEventLogFile.getModificationTime\n+\n+  override def zipEventLogFiles(zipStream: ZipOutputStream): Unit = {\n+    val dirEntryName = rootPath.getName + \"/\"\n+    zipStream.putNextEntry(new ZipEntry(dirEntryName))\n+    files.foreach { file =>\n+      addFileAsZipEntry(zipStream, file.getPath, dirEntryName + file.getPath.getName)\n+    }\n+  }\n+\n+  override def listEventLogFiles: Seq[FileStatus] = eventLogFiles\n+\n+  override def compressionCodec: Option[String] = EventLogFileWriter.codecName("
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "status",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-26T00:21:23Z",
    "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.{BufferedInputStream, InputStream}\n+import java.util.zip.{ZipEntry, ZipOutputStream}\n+\n+import scala.collection.mutable\n+\n+import com.google.common.io.ByteStreams\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.apache.hadoop.hdfs.DFSInputStream\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.history.EventLogFileWriter.codecName\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/** The base class of reader which will read the information of event log file(s). */\n+abstract class EventLogFileReader(\n+    protected val fileSystem: FileSystem,\n+    val rootPath: Path) {\n+\n+  protected def fileSizeForDFS(path: Path): Option[Long] = {\n+    Utils.tryWithResource(fileSystem.open(path)) { in =>\n+      in.getWrappedStream match {\n+        case dfsIn: DFSInputStream => Some(dfsIn.getFileLength)\n+        case _ => None\n+      }\n+    }\n+  }\n+\n+  protected def addFileAsZipEntry(\n+      zipStream: ZipOutputStream,\n+      path: Path,\n+      entryName: String): Unit = {\n+    Utils.tryWithResource(fileSystem.open(path, 1 * 1024 * 1024)) { inputStream =>\n+      zipStream.putNextEntry(new ZipEntry(entryName))\n+      ByteStreams.copy(inputStream, zipStream)\n+      zipStream.closeEntry()\n+    }\n+  }\n+\n+  /** Returns the last index of event log files. None for single event log file. */\n+  def lastIndex: Option[Long]\n+\n+  /**\n+   * Returns the size of file for the last index of event log files. Returns its size for\n+   * single event log file.\n+   */\n+  def fileSizeForLastIndex: Long\n+\n+  /** Returns whether the application is completed. */\n+  def completed: Boolean\n+\n+  /**\n+   * Returns the size of file for the last index of event log files, only when\n+   * underlying input stream is DFSInputStream. Otherwise returns None.\n+   */\n+  def fileSizeForLastIndexForDFS: Option[Long]\n+\n+  /** Returns the modification time for the last sequence number of event log files. */\n+  def modificationTime: Long\n+\n+  /**\n+   * This method compresses the files passed in, and writes the compressed data out into the\n+   * ZipOutputStream passed in. Each file is written as a new ZipEntry with its name being\n+   * the name of the file being compressed.\n+   */\n+  def zipEventLogFiles(zipStream: ZipOutputStream): Unit\n+\n+  /** Returns all available event log files. */\n+  def listEventLogFiles: Seq[FileStatus]\n+\n+  /** Returns the short compression name if being used. None if it's uncompressed. */\n+  def compressionCodec: Option[String]\n+\n+  /** Returns the size of all event log files. */\n+  def totalSize: Long\n+}\n+\n+object EventLogFileReader {\n+  // A cache for compression codecs to avoid creating the same codec many times\n+  private val codecMap = mutable.HashMap.empty[String, CompressionCodec]\n+\n+  def apply(\n+      fs: FileSystem,\n+      path: Path,\n+      lastSequenceNumber: Option[Long]): EventLogFileReader = {\n+    lastSequenceNumber match {\n+      case Some(_) => new RollingEventLogFilesFileReader(fs, path)\n+      case None => new SingleFileEventLogFileReader(fs, path)\n+    }\n+  }\n+\n+  def apply(fs: FileSystem, path: Path): Option[EventLogFileReader] = {\n+    apply(fs, fs.getFileStatus(path))\n+  }\n+\n+  def apply(fs: FileSystem, status: FileStatus): Option[EventLogFileReader] = {\n+    if (isSingleEventLog(status)) {\n+      Some(new SingleFileEventLogFileReader(fs, status.getPath))\n+    } else if (isRollingEventLogs(status)) {\n+      Some(new RollingEventLogFilesFileReader(fs, status.getPath))\n+    } else {\n+      None\n+    }\n+  }\n+\n+  /**\n+   * Opens an event log file and returns an input stream that contains the event data.\n+   *\n+   * @return input stream that holds one JSON record per line.\n+   */\n+  def openEventLog(log: Path, fs: FileSystem): InputStream = {\n+    val in = new BufferedInputStream(fs.open(log))\n+    try {\n+      val codec = codecName(log).map { c =>\n+        codecMap.getOrElseUpdate(c, CompressionCodec.createCodec(new SparkConf, c))\n+      }\n+      codec.map(_.compressedContinuousInputStream(in)).getOrElse(in)\n+    } catch {\n+      case e: Throwable =>\n+        in.close()\n+        throw e\n+    }\n+  }\n+\n+  private def isSingleEventLog(status: FileStatus): Boolean = {\n+    !status.isDirectory &&\n+      // FsHistoryProvider used to generate a hidden file which can't be read.  Accidentally\n+      // reading a garbage file is safe, but we would log an error which can be scary to\n+      // the end-user.\n+      !status.getPath.getName.startsWith(\".\")\n+  }\n+\n+  private def isRollingEventLogs(status: FileStatus): Boolean = {\n+    status.isDirectory && RollingEventLogFilesWriter.isEventLogDir(status)\n+  }\n+}\n+\n+/** The reader which will read the information of single event log file. */\n+class SingleFileEventLogFileReader(\n+    fs: FileSystem,\n+    path: Path) extends EventLogFileReader(fs, path) {\n+  // TODO: get stats with constructor and only call if it's needed?\n+  private lazy val status = fileSystem.getFileStatus(rootPath)\n+\n+  override def lastIndex: Option[Long] = None\n+\n+  override def fileSizeForLastIndex: Long = status.getLen\n+\n+  override def completed: Boolean = !rootPath.getName.endsWith(EventLogFileWriter.IN_PROGRESS)\n+\n+  override def fileSizeForLastIndexForDFS: Option[Long] = {\n+    if (completed) {\n+      Some(fileSizeForLastIndex)\n+    } else {\n+      fileSizeForDFS(rootPath)\n+    }\n+  }\n+\n+  override def modificationTime: Long = status.getModificationTime\n+\n+  override def zipEventLogFiles(zipStream: ZipOutputStream): Unit =\n+    addFileAsZipEntry(zipStream, rootPath, rootPath.getName)\n+\n+  override def listEventLogFiles: Seq[FileStatus] = Seq(status)\n+\n+  override def compressionCodec: Option[String] = EventLogFileWriter.codecName(rootPath)\n+\n+  override def totalSize: Long = fileSizeForLastIndex\n+}\n+\n+/** The reader which will read the information of rolled multiple event log files. */\n+class RollingEventLogFilesFileReader(\n+    fs: FileSystem,\n+    path: Path) extends EventLogFileReader(fs, path) {\n+  import RollingEventLogFilesWriter._\n+\n+  private lazy val files: Seq[FileStatus] = {\n+    val ret = fs.listStatus(rootPath).toSeq\n+    require(ret.exists(isEventLogFile), \"Log directory must contain at least one event log file!\")\n+    require(ret.exists(isAppStatusFile), \"Log directory must contain an appstatus file!\")\n+    ret\n+  }\n+\n+  override def lastIndex: Option[Long] = {\n+    val maxSeq = files.filter(isEventLogFile)\n+      .map { stats => getSequence(stats.getPath.getName) }\n+      .max\n+    Some(maxSeq)\n+  }\n+\n+  override def fileSizeForLastIndex: Long = lastEventLogFile.getLen\n+\n+  override def completed: Boolean = {\n+    val appStatsFile = files.find(isAppStatusFile)\n+    require(appStatsFile.isDefined)\n+    appStatsFile.exists(!_.getPath.getName.endsWith(EventLogFileWriter.IN_PROGRESS))\n+  }\n+\n+  override def fileSizeForLastIndexForDFS: Option[Long] = {\n+    if (completed) {\n+      Some(fileSizeForLastIndex)\n+    } else {\n+      fileSizeForDFS(lastEventLogFile.getPath)\n+    }\n+  }\n+\n+  override def modificationTime: Long = lastEventLogFile.getModificationTime\n+\n+  override def zipEventLogFiles(zipStream: ZipOutputStream): Unit = {\n+    val dirEntryName = rootPath.getName + \"/\"\n+    zipStream.putNextEntry(new ZipEntry(dirEntryName))\n+    files.foreach { file =>\n+      addFileAsZipEntry(zipStream, file.getPath, dirEntryName + file.getPath.getName)\n+    }\n+  }\n+\n+  override def listEventLogFiles: Seq[FileStatus] = eventLogFiles\n+\n+  override def compressionCodec: Option[String] = EventLogFileWriter.codecName(\n+    eventLogFiles.head.getPath)\n+\n+  override def totalSize: Long = eventLogFiles.map(_.getLen).sum\n+\n+  private def eventLogFiles: Seq[FileStatus] = {\n+    files.filter(isEventLogFile).sortBy { stats => getSequence(stats.getPath.getName) }"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "Ngone51"
    },
    "body": "`sequence number` or `index` ?",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-26T14:24:14Z",
    "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.{BufferedInputStream, InputStream}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.zip.{ZipEntry, ZipOutputStream}\n+\n+import com.google.common.io.ByteStreams\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.apache.hadoop.hdfs.DFSInputStream\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.history.EventLogFileWriter.codecName\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/** The base class of reader which will read the information of event log file(s). */\n+abstract class EventLogFileReader(\n+    protected val fileSystem: FileSystem,\n+    val rootPath: Path) {\n+\n+  protected def fileSizeForDFS(path: Path): Option[Long] = {\n+    Utils.tryWithResource(fileSystem.open(path)) { in =>\n+      in.getWrappedStream match {\n+        case dfsIn: DFSInputStream => Some(dfsIn.getFileLength)\n+        case _ => None\n+      }\n+    }\n+  }\n+\n+  protected def addFileAsZipEntry(\n+      zipStream: ZipOutputStream,\n+      path: Path,\n+      entryName: String): Unit = {\n+    Utils.tryWithResource(fileSystem.open(path, 1 * 1024 * 1024)) { inputStream =>\n+      zipStream.putNextEntry(new ZipEntry(entryName))\n+      ByteStreams.copy(inputStream, zipStream)\n+      zipStream.closeEntry()\n+    }\n+  }\n+\n+  /** Returns the last index of event log files. None for single event log file. */\n+  def lastIndex: Option[Long]\n+\n+  /**\n+   * Returns the size of file for the last index of event log files. Returns its size for\n+   * single event log file.\n+   */\n+  def fileSizeForLastIndex: Long\n+\n+  /** Returns whether the application is completed. */\n+  def completed: Boolean\n+\n+  /**\n+   * Returns the size of file for the last index of event log files, only when\n+   * underlying input stream is DFSInputStream. Otherwise returns None.\n+   */\n+  def fileSizeForLastIndexForDFS: Option[Long]\n+\n+  /** Returns the modification time for the last sequence number of event log files. */"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "I guess I've searched but there're missing spots. Thanks for finding!",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-26T20:17:32Z",
    "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.{BufferedInputStream, InputStream}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.zip.{ZipEntry, ZipOutputStream}\n+\n+import com.google.common.io.ByteStreams\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.apache.hadoop.hdfs.DFSInputStream\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.history.EventLogFileWriter.codecName\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/** The base class of reader which will read the information of event log file(s). */\n+abstract class EventLogFileReader(\n+    protected val fileSystem: FileSystem,\n+    val rootPath: Path) {\n+\n+  protected def fileSizeForDFS(path: Path): Option[Long] = {\n+    Utils.tryWithResource(fileSystem.open(path)) { in =>\n+      in.getWrappedStream match {\n+        case dfsIn: DFSInputStream => Some(dfsIn.getFileLength)\n+        case _ => None\n+      }\n+    }\n+  }\n+\n+  protected def addFileAsZipEntry(\n+      zipStream: ZipOutputStream,\n+      path: Path,\n+      entryName: String): Unit = {\n+    Utils.tryWithResource(fileSystem.open(path, 1 * 1024 * 1024)) { inputStream =>\n+      zipStream.putNextEntry(new ZipEntry(entryName))\n+      ByteStreams.copy(inputStream, zipStream)\n+      zipStream.closeEntry()\n+    }\n+  }\n+\n+  /** Returns the last index of event log files. None for single event log file. */\n+  def lastIndex: Option[Long]\n+\n+  /**\n+   * Returns the size of file for the last index of event log files. Returns its size for\n+   * single event log file.\n+   */\n+  def fileSizeForLastIndex: Long\n+\n+  /** Returns whether the application is completed. */\n+  def completed: Boolean\n+\n+  /**\n+   * Returns the size of file for the last index of event log files, only when\n+   * underlying input stream is DFSInputStream. Otherwise returns None.\n+   */\n+  def fileSizeForLastIndexForDFS: Option[Long]\n+\n+  /** Returns the modification time for the last sequence number of event log files. */"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "Ngone51"
    },
    "body": "Shall we unify these two terminologies ? Index or Sequence ?",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-26T14:29:27Z",
    "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.{BufferedInputStream, InputStream}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.zip.{ZipEntry, ZipOutputStream}\n+\n+import com.google.common.io.ByteStreams\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.apache.hadoop.hdfs.DFSInputStream\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.history.EventLogFileWriter.codecName\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/** The base class of reader which will read the information of event log file(s). */\n+abstract class EventLogFileReader(\n+    protected val fileSystem: FileSystem,\n+    val rootPath: Path) {\n+\n+  protected def fileSizeForDFS(path: Path): Option[Long] = {\n+    Utils.tryWithResource(fileSystem.open(path)) { in =>\n+      in.getWrappedStream match {\n+        case dfsIn: DFSInputStream => Some(dfsIn.getFileLength)\n+        case _ => None\n+      }\n+    }\n+  }\n+\n+  protected def addFileAsZipEntry(\n+      zipStream: ZipOutputStream,\n+      path: Path,\n+      entryName: String): Unit = {\n+    Utils.tryWithResource(fileSystem.open(path, 1 * 1024 * 1024)) { inputStream =>\n+      zipStream.putNextEntry(new ZipEntry(entryName))\n+      ByteStreams.copy(inputStream, zipStream)\n+      zipStream.closeEntry()\n+    }\n+  }\n+\n+  /** Returns the last index of event log files. None for single event log file. */\n+  def lastIndex: Option[Long]\n+\n+  /**\n+   * Returns the size of file for the last index of event log files. Returns its size for\n+   * single event log file.\n+   */\n+  def fileSizeForLastIndex: Long\n+\n+  /** Returns whether the application is completed. */\n+  def completed: Boolean\n+\n+  /**\n+   * Returns the size of file for the last index of event log files, only when\n+   * underlying input stream is DFSInputStream. Otherwise returns None.\n+   */\n+  def fileSizeForLastIndexForDFS: Option[Long]\n+\n+  /** Returns the modification time for the last sequence number of event log files. */\n+  def modificationTime: Long\n+\n+  /**\n+   * This method compresses the files passed in, and writes the compressed data out into the\n+   * ZipOutputStream passed in. Each file is written as a new ZipEntry with its name being\n+   * the name of the file being compressed.\n+   */\n+  def zipEventLogFiles(zipStream: ZipOutputStream): Unit\n+\n+  /** Returns all available event log files. */\n+  def listEventLogFiles: Seq[FileStatus]\n+\n+  /** Returns the short compression name if being used. None if it's uncompressed. */\n+  def compressionCodec: Option[String]\n+\n+  /** Returns the size of all event log files. */\n+  def totalSize: Long\n+}\n+\n+object EventLogFileReader {\n+  // A cache for compression codecs to avoid creating the same codec many times\n+  private val codecMap = new ConcurrentHashMap[String, CompressionCodec]()\n+\n+  def apply(\n+      fs: FileSystem,\n+      path: Path,\n+      lastIndex: Option[Long]): EventLogFileReader = {\n+    lastIndex match {\n+      case Some(_) => new RollingEventLogFilesFileReader(fs, path)\n+      case None => new SingleFileEventLogFileReader(fs, path)\n+    }\n+  }\n+\n+  def apply(fs: FileSystem, path: Path): Option[EventLogFileReader] = {\n+    apply(fs, fs.getFileStatus(path))\n+  }\n+\n+  def apply(fs: FileSystem, status: FileStatus): Option[EventLogFileReader] = {\n+    if (isSingleEventLog(status)) {\n+      Some(new SingleFileEventLogFileReader(fs, status.getPath))\n+    } else if (isRollingEventLogs(status)) {\n+      Some(new RollingEventLogFilesFileReader(fs, status.getPath))\n+    } else {\n+      None\n+    }\n+  }\n+\n+  /**\n+   * Opens an event log file and returns an input stream that contains the event data.\n+   *\n+   * @return input stream that holds one JSON record per line.\n+   */\n+  def openEventLog(log: Path, fs: FileSystem): InputStream = {\n+    val in = new BufferedInputStream(fs.open(log))\n+    try {\n+      val codec = codecName(log).map { c =>\n+        codecMap.computeIfAbsent(c, CompressionCodec.createCodec(new SparkConf, _))\n+      }\n+      codec.map(_.compressedContinuousInputStream(in)).getOrElse(in)\n+    } catch {\n+      case e: Throwable =>\n+        in.close()\n+        throw e\n+    }\n+  }\n+\n+  private def isSingleEventLog(status: FileStatus): Boolean = {\n+    !status.isDirectory &&\n+      // FsHistoryProvider used to generate a hidden file which can't be read.  Accidentally\n+      // reading a garbage file is safe, but we would log an error which can be scary to\n+      // the end-user.\n+      !status.getPath.getName.startsWith(\".\")\n+  }\n+\n+  private def isRollingEventLogs(status: FileStatus): Boolean = {\n+    status.isDirectory && RollingEventLogFilesWriter.isEventLogDir(status)\n+  }\n+}\n+\n+/** The reader which will read the information of single event log file. */\n+class SingleFileEventLogFileReader(\n+    fs: FileSystem,\n+    path: Path) extends EventLogFileReader(fs, path) {\n+  private lazy val status = fileSystem.getFileStatus(rootPath)\n+\n+  override def lastIndex: Option[Long] = None\n+\n+  override def fileSizeForLastIndex: Long = status.getLen\n+\n+  override def completed: Boolean = !rootPath.getName.endsWith(EventLogFileWriter.IN_PROGRESS)\n+\n+  override def fileSizeForLastIndexForDFS: Option[Long] = {\n+    if (completed) {\n+      Some(fileSizeForLastIndex)\n+    } else {\n+      fileSizeForDFS(rootPath)\n+    }\n+  }\n+\n+  override def modificationTime: Long = status.getModificationTime\n+\n+  override def zipEventLogFiles(zipStream: ZipOutputStream): Unit = {\n+    addFileAsZipEntry(zipStream, rootPath, rootPath.getName)\n+  }\n+\n+  override def listEventLogFiles: Seq[FileStatus] = Seq(status)\n+\n+  override def compressionCodec: Option[String] = EventLogFileWriter.codecName(rootPath)\n+\n+  override def totalSize: Long = fileSizeForLastIndex\n+}\n+\n+/** The reader which will read the information of rolled multiple event log files. */\n+class RollingEventLogFilesFileReader(\n+    fs: FileSystem,\n+    path: Path) extends EventLogFileReader(fs, path) {\n+  import RollingEventLogFilesWriter._\n+\n+  private lazy val files: Seq[FileStatus] = {\n+    val ret = fs.listStatus(rootPath).toSeq\n+    require(ret.exists(isEventLogFile), \"Log directory must contain at least one event log file!\")\n+    require(ret.exists(isAppStatusFile), \"Log directory must contain an appstatus file!\")\n+    ret\n+  }\n+\n+  override def lastIndex: Option[Long] = {\n+    val maxSeq = files.filter(isEventLogFile)\n+      .map { status => getSequence(status.getPath.getName) }"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Same here; will fix.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-26T20:18:52Z",
    "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.{BufferedInputStream, InputStream}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.zip.{ZipEntry, ZipOutputStream}\n+\n+import com.google.common.io.ByteStreams\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.apache.hadoop.hdfs.DFSInputStream\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.history.EventLogFileWriter.codecName\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/** The base class of reader which will read the information of event log file(s). */\n+abstract class EventLogFileReader(\n+    protected val fileSystem: FileSystem,\n+    val rootPath: Path) {\n+\n+  protected def fileSizeForDFS(path: Path): Option[Long] = {\n+    Utils.tryWithResource(fileSystem.open(path)) { in =>\n+      in.getWrappedStream match {\n+        case dfsIn: DFSInputStream => Some(dfsIn.getFileLength)\n+        case _ => None\n+      }\n+    }\n+  }\n+\n+  protected def addFileAsZipEntry(\n+      zipStream: ZipOutputStream,\n+      path: Path,\n+      entryName: String): Unit = {\n+    Utils.tryWithResource(fileSystem.open(path, 1 * 1024 * 1024)) { inputStream =>\n+      zipStream.putNextEntry(new ZipEntry(entryName))\n+      ByteStreams.copy(inputStream, zipStream)\n+      zipStream.closeEntry()\n+    }\n+  }\n+\n+  /** Returns the last index of event log files. None for single event log file. */\n+  def lastIndex: Option[Long]\n+\n+  /**\n+   * Returns the size of file for the last index of event log files. Returns its size for\n+   * single event log file.\n+   */\n+  def fileSizeForLastIndex: Long\n+\n+  /** Returns whether the application is completed. */\n+  def completed: Boolean\n+\n+  /**\n+   * Returns the size of file for the last index of event log files, only when\n+   * underlying input stream is DFSInputStream. Otherwise returns None.\n+   */\n+  def fileSizeForLastIndexForDFS: Option[Long]\n+\n+  /** Returns the modification time for the last sequence number of event log files. */\n+  def modificationTime: Long\n+\n+  /**\n+   * This method compresses the files passed in, and writes the compressed data out into the\n+   * ZipOutputStream passed in. Each file is written as a new ZipEntry with its name being\n+   * the name of the file being compressed.\n+   */\n+  def zipEventLogFiles(zipStream: ZipOutputStream): Unit\n+\n+  /** Returns all available event log files. */\n+  def listEventLogFiles: Seq[FileStatus]\n+\n+  /** Returns the short compression name if being used. None if it's uncompressed. */\n+  def compressionCodec: Option[String]\n+\n+  /** Returns the size of all event log files. */\n+  def totalSize: Long\n+}\n+\n+object EventLogFileReader {\n+  // A cache for compression codecs to avoid creating the same codec many times\n+  private val codecMap = new ConcurrentHashMap[String, CompressionCodec]()\n+\n+  def apply(\n+      fs: FileSystem,\n+      path: Path,\n+      lastIndex: Option[Long]): EventLogFileReader = {\n+    lastIndex match {\n+      case Some(_) => new RollingEventLogFilesFileReader(fs, path)\n+      case None => new SingleFileEventLogFileReader(fs, path)\n+    }\n+  }\n+\n+  def apply(fs: FileSystem, path: Path): Option[EventLogFileReader] = {\n+    apply(fs, fs.getFileStatus(path))\n+  }\n+\n+  def apply(fs: FileSystem, status: FileStatus): Option[EventLogFileReader] = {\n+    if (isSingleEventLog(status)) {\n+      Some(new SingleFileEventLogFileReader(fs, status.getPath))\n+    } else if (isRollingEventLogs(status)) {\n+      Some(new RollingEventLogFilesFileReader(fs, status.getPath))\n+    } else {\n+      None\n+    }\n+  }\n+\n+  /**\n+   * Opens an event log file and returns an input stream that contains the event data.\n+   *\n+   * @return input stream that holds one JSON record per line.\n+   */\n+  def openEventLog(log: Path, fs: FileSystem): InputStream = {\n+    val in = new BufferedInputStream(fs.open(log))\n+    try {\n+      val codec = codecName(log).map { c =>\n+        codecMap.computeIfAbsent(c, CompressionCodec.createCodec(new SparkConf, _))\n+      }\n+      codec.map(_.compressedContinuousInputStream(in)).getOrElse(in)\n+    } catch {\n+      case e: Throwable =>\n+        in.close()\n+        throw e\n+    }\n+  }\n+\n+  private def isSingleEventLog(status: FileStatus): Boolean = {\n+    !status.isDirectory &&\n+      // FsHistoryProvider used to generate a hidden file which can't be read.  Accidentally\n+      // reading a garbage file is safe, but we would log an error which can be scary to\n+      // the end-user.\n+      !status.getPath.getName.startsWith(\".\")\n+  }\n+\n+  private def isRollingEventLogs(status: FileStatus): Boolean = {\n+    status.isDirectory && RollingEventLogFilesWriter.isEventLogDir(status)\n+  }\n+}\n+\n+/** The reader which will read the information of single event log file. */\n+class SingleFileEventLogFileReader(\n+    fs: FileSystem,\n+    path: Path) extends EventLogFileReader(fs, path) {\n+  private lazy val status = fileSystem.getFileStatus(rootPath)\n+\n+  override def lastIndex: Option[Long] = None\n+\n+  override def fileSizeForLastIndex: Long = status.getLen\n+\n+  override def completed: Boolean = !rootPath.getName.endsWith(EventLogFileWriter.IN_PROGRESS)\n+\n+  override def fileSizeForLastIndexForDFS: Option[Long] = {\n+    if (completed) {\n+      Some(fileSizeForLastIndex)\n+    } else {\n+      fileSizeForDFS(rootPath)\n+    }\n+  }\n+\n+  override def modificationTime: Long = status.getModificationTime\n+\n+  override def zipEventLogFiles(zipStream: ZipOutputStream): Unit = {\n+    addFileAsZipEntry(zipStream, rootPath, rootPath.getName)\n+  }\n+\n+  override def listEventLogFiles: Seq[FileStatus] = Seq(status)\n+\n+  override def compressionCodec: Option[String] = EventLogFileWriter.codecName(rootPath)\n+\n+  override def totalSize: Long = fileSizeForLastIndex\n+}\n+\n+/** The reader which will read the information of rolled multiple event log files. */\n+class RollingEventLogFilesFileReader(\n+    fs: FileSystem,\n+    path: Path) extends EventLogFileReader(fs, path) {\n+  import RollingEventLogFilesWriter._\n+\n+  private lazy val files: Seq[FileStatus] = {\n+    val ret = fs.listStatus(rootPath).toSeq\n+    require(ret.exists(isEventLogFile), \"Log directory must contain at least one event log file!\")\n+    require(ret.exists(isAppStatusFile), \"Log directory must contain an appstatus file!\")\n+    ret\n+  }\n+\n+  override def lastIndex: Option[Long] = {\n+    val maxSeq = files.filter(isEventLogFile)\n+      .map { status => getSequence(status.getPath.getName) }"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "Ngone51"
    },
    "body": "Why not save appStatsFile to avoid searching it every time when we need it ?\r\n\r\nI think we can do it when initialize `files`.\r\n\r\nAfter I read `FsHistoryProvider` I think \"save\" may not necessary.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-26T14:49:14Z",
    "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.{BufferedInputStream, InputStream}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.zip.{ZipEntry, ZipOutputStream}\n+\n+import com.google.common.io.ByteStreams\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.apache.hadoop.hdfs.DFSInputStream\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.history.EventLogFileWriter.codecName\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/** The base class of reader which will read the information of event log file(s). */\n+abstract class EventLogFileReader(\n+    protected val fileSystem: FileSystem,\n+    val rootPath: Path) {\n+\n+  protected def fileSizeForDFS(path: Path): Option[Long] = {\n+    Utils.tryWithResource(fileSystem.open(path)) { in =>\n+      in.getWrappedStream match {\n+        case dfsIn: DFSInputStream => Some(dfsIn.getFileLength)\n+        case _ => None\n+      }\n+    }\n+  }\n+\n+  protected def addFileAsZipEntry(\n+      zipStream: ZipOutputStream,\n+      path: Path,\n+      entryName: String): Unit = {\n+    Utils.tryWithResource(fileSystem.open(path, 1 * 1024 * 1024)) { inputStream =>\n+      zipStream.putNextEntry(new ZipEntry(entryName))\n+      ByteStreams.copy(inputStream, zipStream)\n+      zipStream.closeEntry()\n+    }\n+  }\n+\n+  /** Returns the last index of event log files. None for single event log file. */\n+  def lastIndex: Option[Long]\n+\n+  /**\n+   * Returns the size of file for the last index of event log files. Returns its size for\n+   * single event log file.\n+   */\n+  def fileSizeForLastIndex: Long\n+\n+  /** Returns whether the application is completed. */\n+  def completed: Boolean\n+\n+  /**\n+   * Returns the size of file for the last index of event log files, only when\n+   * underlying input stream is DFSInputStream. Otherwise returns None.\n+   */\n+  def fileSizeForLastIndexForDFS: Option[Long]\n+\n+  /** Returns the modification time for the last sequence number of event log files. */\n+  def modificationTime: Long\n+\n+  /**\n+   * This method compresses the files passed in, and writes the compressed data out into the\n+   * ZipOutputStream passed in. Each file is written as a new ZipEntry with its name being\n+   * the name of the file being compressed.\n+   */\n+  def zipEventLogFiles(zipStream: ZipOutputStream): Unit\n+\n+  /** Returns all available event log files. */\n+  def listEventLogFiles: Seq[FileStatus]\n+\n+  /** Returns the short compression name if being used. None if it's uncompressed. */\n+  def compressionCodec: Option[String]\n+\n+  /** Returns the size of all event log files. */\n+  def totalSize: Long\n+}\n+\n+object EventLogFileReader {\n+  // A cache for compression codecs to avoid creating the same codec many times\n+  private val codecMap = new ConcurrentHashMap[String, CompressionCodec]()\n+\n+  def apply(\n+      fs: FileSystem,\n+      path: Path,\n+      lastIndex: Option[Long]): EventLogFileReader = {\n+    lastIndex match {\n+      case Some(_) => new RollingEventLogFilesFileReader(fs, path)\n+      case None => new SingleFileEventLogFileReader(fs, path)\n+    }\n+  }\n+\n+  def apply(fs: FileSystem, path: Path): Option[EventLogFileReader] = {\n+    apply(fs, fs.getFileStatus(path))\n+  }\n+\n+  def apply(fs: FileSystem, status: FileStatus): Option[EventLogFileReader] = {\n+    if (isSingleEventLog(status)) {\n+      Some(new SingleFileEventLogFileReader(fs, status.getPath))\n+    } else if (isRollingEventLogs(status)) {\n+      Some(new RollingEventLogFilesFileReader(fs, status.getPath))\n+    } else {\n+      None\n+    }\n+  }\n+\n+  /**\n+   * Opens an event log file and returns an input stream that contains the event data.\n+   *\n+   * @return input stream that holds one JSON record per line.\n+   */\n+  def openEventLog(log: Path, fs: FileSystem): InputStream = {\n+    val in = new BufferedInputStream(fs.open(log))\n+    try {\n+      val codec = codecName(log).map { c =>\n+        codecMap.computeIfAbsent(c, CompressionCodec.createCodec(new SparkConf, _))\n+      }\n+      codec.map(_.compressedContinuousInputStream(in)).getOrElse(in)\n+    } catch {\n+      case e: Throwable =>\n+        in.close()\n+        throw e\n+    }\n+  }\n+\n+  private def isSingleEventLog(status: FileStatus): Boolean = {\n+    !status.isDirectory &&\n+      // FsHistoryProvider used to generate a hidden file which can't be read.  Accidentally\n+      // reading a garbage file is safe, but we would log an error which can be scary to\n+      // the end-user.\n+      !status.getPath.getName.startsWith(\".\")\n+  }\n+\n+  private def isRollingEventLogs(status: FileStatus): Boolean = {\n+    status.isDirectory && RollingEventLogFilesWriter.isEventLogDir(status)\n+  }\n+}\n+\n+/** The reader which will read the information of single event log file. */\n+class SingleFileEventLogFileReader(\n+    fs: FileSystem,\n+    path: Path) extends EventLogFileReader(fs, path) {\n+  private lazy val status = fileSystem.getFileStatus(rootPath)\n+\n+  override def lastIndex: Option[Long] = None\n+\n+  override def fileSizeForLastIndex: Long = status.getLen\n+\n+  override def completed: Boolean = !rootPath.getName.endsWith(EventLogFileWriter.IN_PROGRESS)\n+\n+  override def fileSizeForLastIndexForDFS: Option[Long] = {\n+    if (completed) {\n+      Some(fileSizeForLastIndex)\n+    } else {\n+      fileSizeForDFS(rootPath)\n+    }\n+  }\n+\n+  override def modificationTime: Long = status.getModificationTime\n+\n+  override def zipEventLogFiles(zipStream: ZipOutputStream): Unit = {\n+    addFileAsZipEntry(zipStream, rootPath, rootPath.getName)\n+  }\n+\n+  override def listEventLogFiles: Seq[FileStatus] = Seq(status)\n+\n+  override def compressionCodec: Option[String] = EventLogFileWriter.codecName(rootPath)\n+\n+  override def totalSize: Long = fileSizeForLastIndex\n+}\n+\n+/** The reader which will read the information of rolled multiple event log files. */\n+class RollingEventLogFilesFileReader(\n+    fs: FileSystem,\n+    path: Path) extends EventLogFileReader(fs, path) {\n+  import RollingEventLogFilesWriter._\n+\n+  private lazy val files: Seq[FileStatus] = {\n+    val ret = fs.listStatus(rootPath).toSeq\n+    require(ret.exists(isEventLogFile), \"Log directory must contain at least one event log file!\")\n+    require(ret.exists(isAppStatusFile), \"Log directory must contain an appstatus file!\")\n+    ret\n+  }\n+\n+  override def lastIndex: Option[Long] = {\n+    val maxSeq = files.filter(isEventLogFile)\n+      .map { status => getSequence(status.getPath.getName) }\n+      .max\n+    Some(maxSeq)\n+  }\n+\n+  override def fileSizeForLastIndex: Long = lastEventLogFile.getLen\n+\n+  override def completed: Boolean = {\n+    val appStatsFile = files.find(isAppStatusFile)"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "I'll save appStatusFile as we don't update the list of files. Thanks for suggestion.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-26T20:31:51Z",
    "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.{BufferedInputStream, InputStream}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.zip.{ZipEntry, ZipOutputStream}\n+\n+import com.google.common.io.ByteStreams\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.apache.hadoop.hdfs.DFSInputStream\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.history.EventLogFileWriter.codecName\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/** The base class of reader which will read the information of event log file(s). */\n+abstract class EventLogFileReader(\n+    protected val fileSystem: FileSystem,\n+    val rootPath: Path) {\n+\n+  protected def fileSizeForDFS(path: Path): Option[Long] = {\n+    Utils.tryWithResource(fileSystem.open(path)) { in =>\n+      in.getWrappedStream match {\n+        case dfsIn: DFSInputStream => Some(dfsIn.getFileLength)\n+        case _ => None\n+      }\n+    }\n+  }\n+\n+  protected def addFileAsZipEntry(\n+      zipStream: ZipOutputStream,\n+      path: Path,\n+      entryName: String): Unit = {\n+    Utils.tryWithResource(fileSystem.open(path, 1 * 1024 * 1024)) { inputStream =>\n+      zipStream.putNextEntry(new ZipEntry(entryName))\n+      ByteStreams.copy(inputStream, zipStream)\n+      zipStream.closeEntry()\n+    }\n+  }\n+\n+  /** Returns the last index of event log files. None for single event log file. */\n+  def lastIndex: Option[Long]\n+\n+  /**\n+   * Returns the size of file for the last index of event log files. Returns its size for\n+   * single event log file.\n+   */\n+  def fileSizeForLastIndex: Long\n+\n+  /** Returns whether the application is completed. */\n+  def completed: Boolean\n+\n+  /**\n+   * Returns the size of file for the last index of event log files, only when\n+   * underlying input stream is DFSInputStream. Otherwise returns None.\n+   */\n+  def fileSizeForLastIndexForDFS: Option[Long]\n+\n+  /** Returns the modification time for the last sequence number of event log files. */\n+  def modificationTime: Long\n+\n+  /**\n+   * This method compresses the files passed in, and writes the compressed data out into the\n+   * ZipOutputStream passed in. Each file is written as a new ZipEntry with its name being\n+   * the name of the file being compressed.\n+   */\n+  def zipEventLogFiles(zipStream: ZipOutputStream): Unit\n+\n+  /** Returns all available event log files. */\n+  def listEventLogFiles: Seq[FileStatus]\n+\n+  /** Returns the short compression name if being used. None if it's uncompressed. */\n+  def compressionCodec: Option[String]\n+\n+  /** Returns the size of all event log files. */\n+  def totalSize: Long\n+}\n+\n+object EventLogFileReader {\n+  // A cache for compression codecs to avoid creating the same codec many times\n+  private val codecMap = new ConcurrentHashMap[String, CompressionCodec]()\n+\n+  def apply(\n+      fs: FileSystem,\n+      path: Path,\n+      lastIndex: Option[Long]): EventLogFileReader = {\n+    lastIndex match {\n+      case Some(_) => new RollingEventLogFilesFileReader(fs, path)\n+      case None => new SingleFileEventLogFileReader(fs, path)\n+    }\n+  }\n+\n+  def apply(fs: FileSystem, path: Path): Option[EventLogFileReader] = {\n+    apply(fs, fs.getFileStatus(path))\n+  }\n+\n+  def apply(fs: FileSystem, status: FileStatus): Option[EventLogFileReader] = {\n+    if (isSingleEventLog(status)) {\n+      Some(new SingleFileEventLogFileReader(fs, status.getPath))\n+    } else if (isRollingEventLogs(status)) {\n+      Some(new RollingEventLogFilesFileReader(fs, status.getPath))\n+    } else {\n+      None\n+    }\n+  }\n+\n+  /**\n+   * Opens an event log file and returns an input stream that contains the event data.\n+   *\n+   * @return input stream that holds one JSON record per line.\n+   */\n+  def openEventLog(log: Path, fs: FileSystem): InputStream = {\n+    val in = new BufferedInputStream(fs.open(log))\n+    try {\n+      val codec = codecName(log).map { c =>\n+        codecMap.computeIfAbsent(c, CompressionCodec.createCodec(new SparkConf, _))\n+      }\n+      codec.map(_.compressedContinuousInputStream(in)).getOrElse(in)\n+    } catch {\n+      case e: Throwable =>\n+        in.close()\n+        throw e\n+    }\n+  }\n+\n+  private def isSingleEventLog(status: FileStatus): Boolean = {\n+    !status.isDirectory &&\n+      // FsHistoryProvider used to generate a hidden file which can't be read.  Accidentally\n+      // reading a garbage file is safe, but we would log an error which can be scary to\n+      // the end-user.\n+      !status.getPath.getName.startsWith(\".\")\n+  }\n+\n+  private def isRollingEventLogs(status: FileStatus): Boolean = {\n+    status.isDirectory && RollingEventLogFilesWriter.isEventLogDir(status)\n+  }\n+}\n+\n+/** The reader which will read the information of single event log file. */\n+class SingleFileEventLogFileReader(\n+    fs: FileSystem,\n+    path: Path) extends EventLogFileReader(fs, path) {\n+  private lazy val status = fileSystem.getFileStatus(rootPath)\n+\n+  override def lastIndex: Option[Long] = None\n+\n+  override def fileSizeForLastIndex: Long = status.getLen\n+\n+  override def completed: Boolean = !rootPath.getName.endsWith(EventLogFileWriter.IN_PROGRESS)\n+\n+  override def fileSizeForLastIndexForDFS: Option[Long] = {\n+    if (completed) {\n+      Some(fileSizeForLastIndex)\n+    } else {\n+      fileSizeForDFS(rootPath)\n+    }\n+  }\n+\n+  override def modificationTime: Long = status.getModificationTime\n+\n+  override def zipEventLogFiles(zipStream: ZipOutputStream): Unit = {\n+    addFileAsZipEntry(zipStream, rootPath, rootPath.getName)\n+  }\n+\n+  override def listEventLogFiles: Seq[FileStatus] = Seq(status)\n+\n+  override def compressionCodec: Option[String] = EventLogFileWriter.codecName(rootPath)\n+\n+  override def totalSize: Long = fileSizeForLastIndex\n+}\n+\n+/** The reader which will read the information of rolled multiple event log files. */\n+class RollingEventLogFilesFileReader(\n+    fs: FileSystem,\n+    path: Path) extends EventLogFileReader(fs, path) {\n+  import RollingEventLogFilesWriter._\n+\n+  private lazy val files: Seq[FileStatus] = {\n+    val ret = fs.listStatus(rootPath).toSeq\n+    require(ret.exists(isEventLogFile), \"Log directory must contain at least one event log file!\")\n+    require(ret.exists(isAppStatusFile), \"Log directory must contain an appstatus file!\")\n+    ret\n+  }\n+\n+  override def lastIndex: Option[Long] = {\n+    val maxSeq = files.filter(isEventLogFile)\n+      .map { status => getSequence(status.getPath.getName) }\n+      .max\n+    Some(maxSeq)\n+  }\n+\n+  override def fileSizeForLastIndex: Long = lastEventLogFile.getLen\n+\n+  override def completed: Boolean = {\n+    val appStatsFile = files.find(isAppStatusFile)"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`eventLogFiles.last`",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-10-02T16:21:44Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.{BufferedInputStream, InputStream}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.zip.{ZipEntry, ZipOutputStream}\n+\n+import com.google.common.io.ByteStreams\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.apache.hadoop.hdfs.DFSInputStream\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.history.EventLogFileWriter.codecName\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/** The base class of reader which will read the information of event log file(s). */\n+abstract class EventLogFileReader(\n+    protected val fileSystem: FileSystem,\n+    val rootPath: Path) {\n+\n+  protected def fileSizeForDFS(path: Path): Option[Long] = {\n+    Utils.tryWithResource(fileSystem.open(path)) { in =>\n+      in.getWrappedStream match {\n+        case dfsIn: DFSInputStream => Some(dfsIn.getFileLength)\n+        case _ => None\n+      }\n+    }\n+  }\n+\n+  protected def addFileAsZipEntry(\n+      zipStream: ZipOutputStream,\n+      path: Path,\n+      entryName: String): Unit = {\n+    Utils.tryWithResource(fileSystem.open(path, 1 * 1024 * 1024)) { inputStream =>\n+      zipStream.putNextEntry(new ZipEntry(entryName))\n+      ByteStreams.copy(inputStream, zipStream)\n+      zipStream.closeEntry()\n+    }\n+  }\n+\n+  /** Returns the last index of event log files. None for single event log file. */\n+  def lastIndex: Option[Long]\n+\n+  /**\n+   * Returns the size of file for the last index of event log files. Returns its size for\n+   * single event log file.\n+   */\n+  def fileSizeForLastIndex: Long\n+\n+  /** Returns whether the application is completed. */\n+  def completed: Boolean\n+\n+  /**\n+   * Returns the size of file for the last index of event log files, only when\n+   * underlying input stream is DFSInputStream. Otherwise returns None.\n+   */\n+  def fileSizeForLastIndexForDFS: Option[Long]\n+\n+  /** Returns the modification time for the last index of event log files. */\n+  def modificationTime: Long\n+\n+  /**\n+   * This method compresses the files passed in, and writes the compressed data out into the\n+   * ZipOutputStream passed in. Each file is written as a new ZipEntry with its name being\n+   * the name of the file being compressed.\n+   */\n+  def zipEventLogFiles(zipStream: ZipOutputStream): Unit\n+\n+  /** Returns all available event log files. */\n+  def listEventLogFiles: Seq[FileStatus]\n+\n+  /** Returns the short compression name if being used. None if it's uncompressed. */\n+  def compressionCodec: Option[String]\n+\n+  /** Returns the size of all event log files. */\n+  def totalSize: Long\n+}\n+\n+object EventLogFileReader {\n+  // A cache for compression codecs to avoid creating the same codec many times\n+  private val codecMap = new ConcurrentHashMap[String, CompressionCodec]()\n+\n+  def apply(\n+      fs: FileSystem,\n+      path: Path,\n+      lastIndex: Option[Long]): EventLogFileReader = {\n+    lastIndex match {\n+      case Some(_) => new RollingEventLogFilesFileReader(fs, path)\n+      case None => new SingleFileEventLogFileReader(fs, path)\n+    }\n+  }\n+\n+  def apply(fs: FileSystem, path: Path): Option[EventLogFileReader] = {\n+    apply(fs, fs.getFileStatus(path))\n+  }\n+\n+  def apply(fs: FileSystem, status: FileStatus): Option[EventLogFileReader] = {\n+    if (isSingleEventLog(status)) {\n+      Some(new SingleFileEventLogFileReader(fs, status.getPath))\n+    } else if (isRollingEventLogs(status)) {\n+      Some(new RollingEventLogFilesFileReader(fs, status.getPath))\n+    } else {\n+      None\n+    }\n+  }\n+\n+  /**\n+   * Opens an event log file and returns an input stream that contains the event data.\n+   *\n+   * @return input stream that holds one JSON record per line.\n+   */\n+  def openEventLog(log: Path, fs: FileSystem): InputStream = {\n+    val in = new BufferedInputStream(fs.open(log))\n+    try {\n+      val codec = codecName(log).map { c =>\n+        codecMap.computeIfAbsent(c, CompressionCodec.createCodec(new SparkConf, _))\n+      }\n+      codec.map(_.compressedContinuousInputStream(in)).getOrElse(in)\n+    } catch {\n+      case e: Throwable =>\n+        in.close()\n+        throw e\n+    }\n+  }\n+\n+  private def isSingleEventLog(status: FileStatus): Boolean = {\n+    !status.isDirectory &&\n+      // FsHistoryProvider used to generate a hidden file which can't be read.  Accidentally\n+      // reading a garbage file is safe, but we would log an error which can be scary to\n+      // the end-user.\n+      !status.getPath.getName.startsWith(\".\")\n+  }\n+\n+  private def isRollingEventLogs(status: FileStatus): Boolean = {\n+    status.isDirectory && RollingEventLogFilesWriter.isEventLogDir(status)\n+  }\n+}\n+\n+/**\n+ * The reader which will read the information of single event log file.\n+ *\n+ * This reader gets the status of event log file only once when required;\n+ * It may not give \"live\" status of file that could be changing concurrently, and\n+ * FileNotFoundException could occur if the log file is renamed before getting the\n+ * status of log file.\n+ */\n+class SingleFileEventLogFileReader(\n+    fs: FileSystem,\n+    path: Path) extends EventLogFileReader(fs, path) {\n+  private lazy val status = fileSystem.getFileStatus(rootPath)\n+\n+  override def lastIndex: Option[Long] = None\n+\n+  override def fileSizeForLastIndex: Long = status.getLen\n+\n+  override def completed: Boolean = !rootPath.getName.endsWith(EventLogFileWriter.IN_PROGRESS)\n+\n+  override def fileSizeForLastIndexForDFS: Option[Long] = {\n+    if (completed) {\n+      Some(fileSizeForLastIndex)\n+    } else {\n+      fileSizeForDFS(rootPath)\n+    }\n+  }\n+\n+  override def modificationTime: Long = status.getModificationTime\n+\n+  override def zipEventLogFiles(zipStream: ZipOutputStream): Unit = {\n+    addFileAsZipEntry(zipStream, rootPath, rootPath.getName)\n+  }\n+\n+  override def listEventLogFiles: Seq[FileStatus] = Seq(status)\n+\n+  override def compressionCodec: Option[String] = EventLogFileWriter.codecName(rootPath)\n+\n+  override def totalSize: Long = fileSizeForLastIndex\n+}\n+\n+/**\n+ * The reader which will read the information of rolled multiple event log files.\n+ *\n+ * This reader lists the files only once; if caller would like to play with updated list,\n+ * it needs to create another reader instance.\n+ */\n+class RollingEventLogFilesFileReader(\n+    fs: FileSystem,\n+    path: Path) extends EventLogFileReader(fs, path) {\n+  import RollingEventLogFilesWriter._\n+\n+  private lazy val files: Seq[FileStatus] = {\n+    val ret = fs.listStatus(rootPath).toSeq\n+    require(ret.exists(isEventLogFile), \"Log directory must contain at least one event log file!\")\n+    require(ret.exists(isAppStatusFile), \"Log directory must contain an appstatus file!\")\n+    ret\n+  }\n+\n+  private lazy val appStatusFile = files.find(isAppStatusFile).get\n+\n+  override def lastIndex: Option[Long] = {\n+    val maxSeq = files.filter(isEventLogFile)\n+      .map { status => getIndex(status.getPath.getName) }\n+      .max\n+    Some(maxSeq)\n+  }\n+\n+  override def fileSizeForLastIndex: Long = lastEventLogFile.getLen\n+\n+  override def completed: Boolean = {\n+    !appStatusFile.getPath.getName.endsWith(EventLogFileWriter.IN_PROGRESS)\n+  }\n+\n+  override def fileSizeForLastIndexForDFS: Option[Long] = {\n+    if (completed) {\n+      Some(fileSizeForLastIndex)\n+    } else {\n+      fileSizeForDFS(lastEventLogFile.getPath)\n+    }\n+  }\n+\n+  override def modificationTime: Long = lastEventLogFile.getModificationTime\n+\n+  override def zipEventLogFiles(zipStream: ZipOutputStream): Unit = {\n+    val dirEntryName = rootPath.getName + \"/\"\n+    zipStream.putNextEntry(new ZipEntry(dirEntryName))\n+    files.foreach { file =>\n+      addFileAsZipEntry(zipStream, file.getPath, dirEntryName + file.getPath.getName)\n+    }\n+  }\n+\n+  override def listEventLogFiles: Seq[FileStatus] = eventLogFiles\n+\n+  override def compressionCodec: Option[String] = {\n+    EventLogFileWriter.codecName(eventLogFiles.head.getPath)\n+  }\n+\n+  override def totalSize: Long = eventLogFiles.map(_.getLen).sum\n+\n+  private def eventLogFiles: Seq[FileStatus] = {\n+    files.filter(isEventLogFile).sortBy { status => getIndex(status.getPath.getName) }\n+  }\n+\n+  private def lastEventLogFile: FileStatus = eventLogFiles.reverse.head"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "Ngone51"
    },
    "body": "`Returns the modification time for the last index of event log files or single event log file` ?",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-10-08T12:10:46Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.{BufferedInputStream, InputStream}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.zip.{ZipEntry, ZipOutputStream}\n+\n+import com.google.common.io.ByteStreams\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.apache.hadoop.hdfs.DFSInputStream\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.history.EventLogFileWriter.codecName\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/** The base class of reader which will read the information of event log file(s). */\n+abstract class EventLogFileReader(\n+    protected val fileSystem: FileSystem,\n+    val rootPath: Path) {\n+\n+  protected def fileSizeForDFS(path: Path): Option[Long] = {\n+    Utils.tryWithResource(fileSystem.open(path)) { in =>\n+      in.getWrappedStream match {\n+        case dfsIn: DFSInputStream => Some(dfsIn.getFileLength)\n+        case _ => None\n+      }\n+    }\n+  }\n+\n+  protected def addFileAsZipEntry(\n+      zipStream: ZipOutputStream,\n+      path: Path,\n+      entryName: String): Unit = {\n+    Utils.tryWithResource(fileSystem.open(path, 1 * 1024 * 1024)) { inputStream =>\n+      zipStream.putNextEntry(new ZipEntry(entryName))\n+      ByteStreams.copy(inputStream, zipStream)\n+      zipStream.closeEntry()\n+    }\n+  }\n+\n+  /** Returns the last index of event log files. None for single event log file. */\n+  def lastIndex: Option[Long]\n+\n+  /**\n+   * Returns the size of file for the last index of event log files. Returns its size for\n+   * single event log file.\n+   */\n+  def fileSizeForLastIndex: Long\n+\n+  /** Returns whether the application is completed. */\n+  def completed: Boolean\n+\n+  /**\n+   * Returns the size of file for the last index of event log files, only when\n+   * underlying input stream is DFSInputStream. Otherwise returns None.\n+   */\n+  def fileSizeForLastIndexForDFS: Option[Long]\n+\n+  /** Returns the modification time for the last index of event log files. */"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "It seems to be better to add how \"the last index\" is applied for single event log file, as it's not easy to apply this suggestion to `fileSizeForLastIndexForDFS`.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-10-08T21:41:41Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.{BufferedInputStream, InputStream}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.zip.{ZipEntry, ZipOutputStream}\n+\n+import com.google.common.io.ByteStreams\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.apache.hadoop.hdfs.DFSInputStream\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.history.EventLogFileWriter.codecName\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/** The base class of reader which will read the information of event log file(s). */\n+abstract class EventLogFileReader(\n+    protected val fileSystem: FileSystem,\n+    val rootPath: Path) {\n+\n+  protected def fileSizeForDFS(path: Path): Option[Long] = {\n+    Utils.tryWithResource(fileSystem.open(path)) { in =>\n+      in.getWrappedStream match {\n+        case dfsIn: DFSInputStream => Some(dfsIn.getFileLength)\n+        case _ => None\n+      }\n+    }\n+  }\n+\n+  protected def addFileAsZipEntry(\n+      zipStream: ZipOutputStream,\n+      path: Path,\n+      entryName: String): Unit = {\n+    Utils.tryWithResource(fileSystem.open(path, 1 * 1024 * 1024)) { inputStream =>\n+      zipStream.putNextEntry(new ZipEntry(entryName))\n+      ByteStreams.copy(inputStream, zipStream)\n+      zipStream.closeEntry()\n+    }\n+  }\n+\n+  /** Returns the last index of event log files. None for single event log file. */\n+  def lastIndex: Option[Long]\n+\n+  /**\n+   * Returns the size of file for the last index of event log files. Returns its size for\n+   * single event log file.\n+   */\n+  def fileSizeForLastIndex: Long\n+\n+  /** Returns whether the application is completed. */\n+  def completed: Boolean\n+\n+  /**\n+   * Returns the size of file for the last index of event log files, only when\n+   * underlying input stream is DFSInputStream. Otherwise returns None.\n+   */\n+  def fileSizeForLastIndexForDFS: Option[Long]\n+\n+  /** Returns the modification time for the last index of event log files. */"
  }],
  "prId": 25670
}]