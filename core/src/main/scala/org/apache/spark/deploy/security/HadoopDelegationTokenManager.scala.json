[{
  "comments": [{
    "author": {
      "login": "arunmahadevan"
    },
    "body": "Update the class docs of which providers are loaded by default or better set the default for `spark.security.credentials.kafka.enabled` to false. ",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-10-25T20:24:11Z",
    "diffHunk": "@@ -66,7 +66,8 @@ private[spark] class HadoopDelegationTokenManager(\n   private def getDelegationTokenProviders: Map[String, HadoopDelegationTokenProvider] = {\n     val providers = Seq(new HadoopFSDelegationTokenProvider(fileSystems)) ++\n       safeCreateProvider(new HiveDelegationTokenProvider) ++\n-      safeCreateProvider(new HBaseDelegationTokenProvider)\n+      safeCreateProvider(new HBaseDelegationTokenProvider) ++\n+      safeCreateProvider(new KafkaDelegationTokenProvider)"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "It shouldn't be disabled by default.\r\n\r\nI think it's better to just remove the list of provides from the scaladoc. That's not really helpful in any case.",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-10-26T21:07:45Z",
    "diffHunk": "@@ -66,7 +66,8 @@ private[spark] class HadoopDelegationTokenManager(\n   private def getDelegationTokenProviders: Map[String, HadoopDelegationTokenProvider] = {\n     val providers = Seq(new HadoopFSDelegationTokenProvider(fileSystems)) ++\n       safeCreateProvider(new HiveDelegationTokenProvider) ++\n-      safeCreateProvider(new HBaseDelegationTokenProvider)\n+      safeCreateProvider(new HBaseDelegationTokenProvider) ++\n+      safeCreateProvider(new KafkaDelegationTokenProvider)"
  }, {
    "author": {
      "login": "arunmahadevan"
    },
    "body": "Why I thought disabling by default might make sense - \r\n\r\nThe tokens fetch would be attempted if just \"spark.kafka.bootstrap.servers\" is defined. And if this config is set the spark-sql-kafka libraries needs to be in the class path as well. Better mention these in the docs. \r\n\r\nWe could also consider prefixing all the configs with spark.security.credentials.kafka instead of spark.kafka (like spark.security.credentials.kafka.bootstrap.servers) to make it explicit that these are security related settings required for fetching kafka delegation tokens.",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-10-26T21:32:23Z",
    "diffHunk": "@@ -66,7 +66,8 @@ private[spark] class HadoopDelegationTokenManager(\n   private def getDelegationTokenProviders: Map[String, HadoopDelegationTokenProvider] = {\n     val providers = Seq(new HadoopFSDelegationTokenProvider(fileSystems)) ++\n       safeCreateProvider(new HiveDelegationTokenProvider) ++\n-      safeCreateProvider(new HBaseDelegationTokenProvider)\n+      safeCreateProvider(new HBaseDelegationTokenProvider) ++\n+      safeCreateProvider(new KafkaDelegationTokenProvider)"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": ">  if this config is set the spark-sql-kafka libraries needs to be in the class path as well\r\n\r\nThat's actually an argument against disabling it by default. If you set that config and don't have the libs in the classpath, you should either:\r\n\r\n- get nothing (e.g. current HBase behavior)\r\n- get an error because the libraries are not present\r\n\r\nBut disabling it by default just means you'd have 3 different things to do to enable this, instead of two.",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-10-26T21:46:57Z",
    "diffHunk": "@@ -66,7 +66,8 @@ private[spark] class HadoopDelegationTokenManager(\n   private def getDelegationTokenProviders: Map[String, HadoopDelegationTokenProvider] = {\n     val providers = Seq(new HadoopFSDelegationTokenProvider(fileSystems)) ++\n       safeCreateProvider(new HiveDelegationTokenProvider) ++\n-      safeCreateProvider(new HBaseDelegationTokenProvider)\n+      safeCreateProvider(new HBaseDelegationTokenProvider) ++\n+      safeCreateProvider(new KafkaDelegationTokenProvider)"
  }, {
    "author": {
      "login": "arunmahadevan"
    },
    "body": "yes, I think the best we can do is to document the configs and throw some useful error messages to make the user aware of the \"bootstrapservers\" config (in case they accidently left it) when the spark-sql-kafka libraries are not in the classpath.",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-10-26T22:33:43Z",
    "diffHunk": "@@ -66,7 +66,8 @@ private[spark] class HadoopDelegationTokenManager(\n   private def getDelegationTokenProviders: Map[String, HadoopDelegationTokenProvider] = {\n     val providers = Seq(new HadoopFSDelegationTokenProvider(fileSystems)) ++\n       safeCreateProvider(new HiveDelegationTokenProvider) ++\n-      safeCreateProvider(new HBaseDelegationTokenProvider)\n+      safeCreateProvider(new HBaseDelegationTokenProvider) ++\n+      safeCreateProvider(new KafkaDelegationTokenProvider)"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "We've considered turning off by default and came to the conclusion what Marcelo described which I think still stands.\r\n\r\nThe PR says documentation is not covered because design can change. My plan is to add it in a separate PR when the feature merged.\r\n\r\nRelated what to document I think kafka integration guide should cover all the things. There it's already covered that the jar should be on the path.\r\n",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-11-05T09:40:04Z",
    "diffHunk": "@@ -66,7 +66,8 @@ private[spark] class HadoopDelegationTokenManager(\n   private def getDelegationTokenProviders: Map[String, HadoopDelegationTokenProvider] = {\n     val providers = Seq(new HadoopFSDelegationTokenProvider(fileSystems)) ++\n       safeCreateProvider(new HiveDelegationTokenProvider) ++\n-      safeCreateProvider(new HBaseDelegationTokenProvider)\n+      safeCreateProvider(new HBaseDelegationTokenProvider) ++\n+      safeCreateProvider(new KafkaDelegationTokenProvider)"
  }],
  "prId": 22598
}]