[{
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "`private[spark]`\n",
    "commit": "fadff896194886854c0bae6657a4897f9a48ce21",
    "createdAt": "2015-08-01T07:50:10Z",
    "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.mesos\n+\n+import java.util.concurrent.CountDownLatch\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.deploy.ExternalShuffleService\n+import org.apache.spark.rpc.{RpcAddress, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SecurityManager, SparkConf}\n+\n+private[mesos] class MesosExternalShuffleService(conf: SparkConf, securityManager: SecurityManager)\n+  extends ExternalShuffleService(conf, securityManager) {\n+\n+  private var rpcEnv: RpcEnv = _\n+\n+  override def start(): Unit = {\n+    super.start()\n+    val port = conf.getInt(\"spark.mesos.shuffle.service.port\", 7327)\n+    logInfo(s\"Starting Mesos external shuffle service endpoint on port $port\")\n+    rpcEnv = RpcEnv.create(\n+      MesosExternalShuffleService.SYSTEM_NAME,\n+      Utils.localHostName(),\n+      port,\n+      conf,\n+      securityManager)\n+    val endpoint = new MesosExternalShuffleServiceEndpoint(rpcEnv)\n+    rpcEnv.setupEndpoint(MesosExternalShuffleService.ENDPOINT_NAME, endpoint)\n+  }\n+\n+  override def stop(): Unit = {\n+    rpcEnv.shutdown()\n+    rpcEnv.awaitTermination()\n+    super.stop()\n+  }\n+\n+  private class MesosExternalShuffleServiceEndpoint(override val rpcEnv: RpcEnv)\n+    extends ThreadSafeRpcEndpoint with Logging {\n+\n+    // Stores a map of driver rpc addresses to app ids\n+    private val connectedApps = new mutable.HashMap[RpcAddress, String]\n+\n+    override def receive: PartialFunction[Any, Unit] = {\n+      case RegisterMesosDriver(appId, address) =>\n+        logDebug(s\"Received registration request from app $appId, address $address\")\n+        if (connectedApps.contains(address)) {\n+          val existingAppId: String = connectedApps(address)\n+          if (!existingAppId.equals(appId)) {\n+            logError(s\"A new app id $appId has connected to existing address $address\" +\n+                     s\", removing registered app $existingAppId\")\n+            blockHandler.applicationRemoved(existingAppId, true)\n+          }\n+        } else {\n+          connectedApps(address) = appId\n+        }\n+    }\n+\n+    override def onDisconnected(remoteAddress: RpcAddress): Unit = {\n+      logDebug(s\"Received disconnect from $remoteAddress\")\n+      if (connectedApps.contains(remoteAddress)) {\n+        blockHandler.applicationRemoved(connectedApps(remoteAddress), true)\n+        connectedApps.remove(remoteAddress)\n+      } else {\n+        logWarning(s\"Address $remoteAddress not found in mesos shuffle service\")\n+      }\n+    }\n+  }\n+}\n+\n+case class RegisterMesosDriver(appId: String, address: RpcAddress)"
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "also please add a simple java doc, even something like `A message sent from the driver to register with this shuffle service.` would do.\n",
    "commit": "fadff896194886854c0bae6657a4897f9a48ce21",
    "createdAt": "2015-08-01T08:13:37Z",
    "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.mesos\n+\n+import java.util.concurrent.CountDownLatch\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.deploy.ExternalShuffleService\n+import org.apache.spark.rpc.{RpcAddress, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SecurityManager, SparkConf}\n+\n+private[mesos] class MesosExternalShuffleService(conf: SparkConf, securityManager: SecurityManager)\n+  extends ExternalShuffleService(conf, securityManager) {\n+\n+  private var rpcEnv: RpcEnv = _\n+\n+  override def start(): Unit = {\n+    super.start()\n+    val port = conf.getInt(\"spark.mesos.shuffle.service.port\", 7327)\n+    logInfo(s\"Starting Mesos external shuffle service endpoint on port $port\")\n+    rpcEnv = RpcEnv.create(\n+      MesosExternalShuffleService.SYSTEM_NAME,\n+      Utils.localHostName(),\n+      port,\n+      conf,\n+      securityManager)\n+    val endpoint = new MesosExternalShuffleServiceEndpoint(rpcEnv)\n+    rpcEnv.setupEndpoint(MesosExternalShuffleService.ENDPOINT_NAME, endpoint)\n+  }\n+\n+  override def stop(): Unit = {\n+    rpcEnv.shutdown()\n+    rpcEnv.awaitTermination()\n+    super.stop()\n+  }\n+\n+  private class MesosExternalShuffleServiceEndpoint(override val rpcEnv: RpcEnv)\n+    extends ThreadSafeRpcEndpoint with Logging {\n+\n+    // Stores a map of driver rpc addresses to app ids\n+    private val connectedApps = new mutable.HashMap[RpcAddress, String]\n+\n+    override def receive: PartialFunction[Any, Unit] = {\n+      case RegisterMesosDriver(appId, address) =>\n+        logDebug(s\"Received registration request from app $appId, address $address\")\n+        if (connectedApps.contains(address)) {\n+          val existingAppId: String = connectedApps(address)\n+          if (!existingAppId.equals(appId)) {\n+            logError(s\"A new app id $appId has connected to existing address $address\" +\n+                     s\", removing registered app $existingAppId\")\n+            blockHandler.applicationRemoved(existingAppId, true)\n+          }\n+        } else {\n+          connectedApps(address) = appId\n+        }\n+    }\n+\n+    override def onDisconnected(remoteAddress: RpcAddress): Unit = {\n+      logDebug(s\"Received disconnect from $remoteAddress\")\n+      if (connectedApps.contains(remoteAddress)) {\n+        blockHandler.applicationRemoved(connectedApps(remoteAddress), true)\n+        connectedApps.remove(remoteAddress)\n+      } else {\n+        logWarning(s\"Address $remoteAddress not found in mesos shuffle service\")\n+      }\n+    }\n+  }\n+}\n+\n+case class RegisterMesosDriver(appId: String, address: RpcAddress)"
  }],
  "prId": 7820
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "no need to make a new config for this one, just reuse `spark.shuffle.service.port` at 7337\n",
    "commit": "fadff896194886854c0bae6657a4897f9a48ce21",
    "createdAt": "2015-08-01T07:50:58Z",
    "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.mesos\n+\n+import java.util.concurrent.CountDownLatch\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.deploy.ExternalShuffleService\n+import org.apache.spark.rpc.{RpcAddress, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SecurityManager, SparkConf}\n+\n+private[mesos] class MesosExternalShuffleService(conf: SparkConf, securityManager: SecurityManager)\n+  extends ExternalShuffleService(conf, securityManager) {\n+\n+  private var rpcEnv: RpcEnv = _\n+\n+  override def start(): Unit = {\n+    super.start()\n+    val port = conf.getInt(\"spark.mesos.shuffle.service.port\", 7327)"
  }, {
    "author": {
      "login": "tnachen"
    },
    "body": "It won't clash on the same port?\n",
    "commit": "fadff896194886854c0bae6657a4897f9a48ce21",
    "createdAt": "2015-08-01T17:13:23Z",
    "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.mesos\n+\n+import java.util.concurrent.CountDownLatch\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.deploy.ExternalShuffleService\n+import org.apache.spark.rpc.{RpcAddress, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SecurityManager, SparkConf}\n+\n+private[mesos] class MesosExternalShuffleService(conf: SparkConf, securityManager: SecurityManager)\n+  extends ExternalShuffleService(conf, securityManager) {\n+\n+  private var rpcEnv: RpcEnv = _\n+\n+  override def start(): Unit = {\n+    super.start()\n+    val port = conf.getInt(\"spark.mesos.shuffle.service.port\", 7327)"
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "why would you have both shuffle services on the same cluster? Your new shuffle service is a replacement for the old one, no?\n",
    "commit": "fadff896194886854c0bae6657a4897f9a48ce21",
    "createdAt": "2015-08-01T18:59:08Z",
    "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.mesos\n+\n+import java.util.concurrent.CountDownLatch\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.deploy.ExternalShuffleService\n+import org.apache.spark.rpc.{RpcAddress, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SecurityManager, SparkConf}\n+\n+private[mesos] class MesosExternalShuffleService(conf: SparkConf, securityManager: SecurityManager)\n+  extends ExternalShuffleService(conf, securityManager) {\n+\n+  private var rpcEnv: RpcEnv = _\n+\n+  override def start(): Unit = {\n+    super.start()\n+    val port = conf.getInt(\"spark.mesos.shuffle.service.port\", 7327)"
  }, {
    "author": {
      "login": "tnachen"
    },
    "body": "It's actually a additional endpoint on a different port that is listening for the driver.\n\nLet me take a look if I can easily manage to use the same, but it looks like it's not using the endpoint mechanism.\n\n> On Aug 1, 2015, at 11:59 AM, andrewor14 notifications@github.com wrote:\n> \n> In core/src/main/scala/org/apache/spark/deploy/mesos/MesosExternalShuffleService.scala:\n> \n> > +\n> > +import scala.collection.mutable\n> > +\n> > +import org.apache.spark.deploy.ExternalShuffleService\n> > +import org.apache.spark.rpc.{RpcAddress, RpcEnv, ThreadSafeRpcEndpoint}\n> > +import org.apache.spark.util.Utils\n> > +import org.apache.spark.{Logging, SecurityManager, SparkConf}\n> > +\n> > +private[mesos] class MesosExternalShuffleService(conf: SparkConf, securityManager: SecurityManager)\n> > -  extends ExternalShuffleService(conf, securityManager) {\n> >   +\n> > -  private var rpcEnv: RpcEnv = _\n> >   +\n> > -  override def start(): Unit = {\n> > -    super.start()\n> > -    val port = conf.getInt(\"spark.mesos.shuffle.service.port\", 7327)\n> >   why would you have both shuffle services on the same cluster? Your new shuffle service is a replacement for the old one, no?\n> \n> â€”\n> Reply to this email directly or view it on GitHub.\n",
    "commit": "fadff896194886854c0bae6657a4897f9a48ce21",
    "createdAt": "2015-08-01T21:27:06Z",
    "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.mesos\n+\n+import java.util.concurrent.CountDownLatch\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.deploy.ExternalShuffleService\n+import org.apache.spark.rpc.{RpcAddress, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SecurityManager, SparkConf}\n+\n+private[mesos] class MesosExternalShuffleService(conf: SparkConf, securityManager: SecurityManager)\n+  extends ExternalShuffleService(conf, securityManager) {\n+\n+  private var rpcEnv: RpcEnv = _\n+\n+  override def start(): Unit = {\n+    super.start()\n+    val port = conf.getInt(\"spark.mesos.shuffle.service.port\", 7327)"
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "oh I see, currently 7337 is for executors while 7327 is for the driver. Yeah I think it would be very valuable to avoid another config.\n",
    "commit": "fadff896194886854c0bae6657a4897f9a48ce21",
    "createdAt": "2015-08-01T22:02:25Z",
    "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.mesos\n+\n+import java.util.concurrent.CountDownLatch\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.deploy.ExternalShuffleService\n+import org.apache.spark.rpc.{RpcAddress, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SecurityManager, SparkConf}\n+\n+private[mesos] class MesosExternalShuffleService(conf: SparkConf, securityManager: SecurityManager)\n+  extends ExternalShuffleService(conf, securityManager) {\n+\n+  private var rpcEnv: RpcEnv = _\n+\n+  override def start(): Unit = {\n+    super.start()\n+    val port = conf.getInt(\"spark.mesos.shuffle.service.port\", 7327)"
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "One thing you could do is have a `MesosExternalShuffleBlockHandler` that extends `ExternalShuffleBlockHandler`. It would override the `receive` method to also accept a `RegisterDriver` message. Note that because it extends `RpcHandler` there is also a `connectionTerminated` method, so we can try using that one instead.\n\nIn other words, use the network RPC framework rather than the Spark core one. You might need to add something that looks like the `ExternalShuffleClient` on the driver.\n",
    "commit": "fadff896194886854c0bae6657a4897f9a48ce21",
    "createdAt": "2015-08-01T22:07:13Z",
    "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.mesos\n+\n+import java.util.concurrent.CountDownLatch\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.deploy.ExternalShuffleService\n+import org.apache.spark.rpc.{RpcAddress, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SecurityManager, SparkConf}\n+\n+private[mesos] class MesosExternalShuffleService(conf: SparkConf, securityManager: SecurityManager)\n+  extends ExternalShuffleService(conf, securityManager) {\n+\n+  private var rpcEnv: RpcEnv = _\n+\n+  override def start(): Unit = {\n+    super.start()\n+    val port = conf.getInt(\"spark.mesos.shuffle.service.port\", 7327)"
  }],
  "prId": 7820
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "This is missing a top level javadoc. You need to explain why Mesos needs a special shuffle service, i.e. it needs to detect when drivers exit, and the way we do it is through a separate RPC channel between the driver and each shuffle service.\n",
    "commit": "fadff896194886854c0bae6657a4897f9a48ce21",
    "createdAt": "2015-08-01T07:52:18Z",
    "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.mesos\n+\n+import java.util.concurrent.CountDownLatch\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.deploy.ExternalShuffleService\n+import org.apache.spark.rpc.{RpcAddress, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SecurityManager, SparkConf}\n+\n+private[mesos] class MesosExternalShuffleService(conf: SparkConf, securityManager: SecurityManager)"
  }],
  "prId": 7820
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "This isn't a Mesos driver right? This is a Spark driver. I would just call this `RegisterDriver`\n",
    "commit": "fadff896194886854c0bae6657a4897f9a48ce21",
    "createdAt": "2015-08-01T07:53:32Z",
    "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.mesos\n+\n+import java.util.concurrent.CountDownLatch\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.deploy.ExternalShuffleService\n+import org.apache.spark.rpc.{RpcAddress, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SecurityManager, SparkConf}\n+\n+private[mesos] class MesosExternalShuffleService(conf: SparkConf, securityManager: SecurityManager)\n+  extends ExternalShuffleService(conf, securityManager) {\n+\n+  private var rpcEnv: RpcEnv = _\n+\n+  override def start(): Unit = {\n+    super.start()\n+    val port = conf.getInt(\"spark.mesos.shuffle.service.port\", 7327)\n+    logInfo(s\"Starting Mesos external shuffle service endpoint on port $port\")\n+    rpcEnv = RpcEnv.create(\n+      MesosExternalShuffleService.SYSTEM_NAME,\n+      Utils.localHostName(),\n+      port,\n+      conf,\n+      securityManager)\n+    val endpoint = new MesosExternalShuffleServiceEndpoint(rpcEnv)\n+    rpcEnv.setupEndpoint(MesosExternalShuffleService.ENDPOINT_NAME, endpoint)\n+  }\n+\n+  override def stop(): Unit = {\n+    rpcEnv.shutdown()\n+    rpcEnv.awaitTermination()\n+    super.stop()\n+  }\n+\n+  private class MesosExternalShuffleServiceEndpoint(override val rpcEnv: RpcEnv)\n+    extends ThreadSafeRpcEndpoint with Logging {\n+\n+    // Stores a map of driver rpc addresses to app ids\n+    private val connectedApps = new mutable.HashMap[RpcAddress, String]\n+\n+    override def receive: PartialFunction[Any, Unit] = {\n+      case RegisterMesosDriver(appId, address) =>"
  }],
  "prId": 7820
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "can you add a java doc for this class:\n\n```\nAn RPC endpoint for talking to the Spark driver.\n```\n",
    "commit": "fadff896194886854c0bae6657a4897f9a48ce21",
    "createdAt": "2015-08-01T07:54:05Z",
    "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.mesos\n+\n+import java.util.concurrent.CountDownLatch\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.deploy.ExternalShuffleService\n+import org.apache.spark.rpc.{RpcAddress, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SecurityManager, SparkConf}\n+\n+private[mesos] class MesosExternalShuffleService(conf: SparkConf, securityManager: SecurityManager)\n+  extends ExternalShuffleService(conf, securityManager) {\n+\n+  private var rpcEnv: RpcEnv = _\n+\n+  override def start(): Unit = {\n+    super.start()\n+    val port = conf.getInt(\"spark.mesos.shuffle.service.port\", 7327)\n+    logInfo(s\"Starting Mesos external shuffle service endpoint on port $port\")\n+    rpcEnv = RpcEnv.create(\n+      MesosExternalShuffleService.SYSTEM_NAME,\n+      Utils.localHostName(),\n+      port,\n+      conf,\n+      securityManager)\n+    val endpoint = new MesosExternalShuffleServiceEndpoint(rpcEnv)\n+    rpcEnv.setupEndpoint(MesosExternalShuffleService.ENDPOINT_NAME, endpoint)\n+  }\n+\n+  override def stop(): Unit = {\n+    rpcEnv.shutdown()\n+    rpcEnv.awaitTermination()\n+    super.stop()\n+  }\n+\n+  private class MesosExternalShuffleServiceEndpoint(override val rpcEnv: RpcEnv)"
  }],
  "prId": 7820
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "should this be an else case? In L68 we removed the existing application ID if there's a duplicate. Shouldn't we put the updated one in `connectedApps`?\n",
    "commit": "fadff896194886854c0bae6657a4897f9a48ce21",
    "createdAt": "2015-08-01T07:55:38Z",
    "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.mesos\n+\n+import java.util.concurrent.CountDownLatch\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.deploy.ExternalShuffleService\n+import org.apache.spark.rpc.{RpcAddress, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SecurityManager, SparkConf}\n+\n+private[mesos] class MesosExternalShuffleService(conf: SparkConf, securityManager: SecurityManager)\n+  extends ExternalShuffleService(conf, securityManager) {\n+\n+  private var rpcEnv: RpcEnv = _\n+\n+  override def start(): Unit = {\n+    super.start()\n+    val port = conf.getInt(\"spark.mesos.shuffle.service.port\", 7327)\n+    logInfo(s\"Starting Mesos external shuffle service endpoint on port $port\")\n+    rpcEnv = RpcEnv.create(\n+      MesosExternalShuffleService.SYSTEM_NAME,\n+      Utils.localHostName(),\n+      port,\n+      conf,\n+      securityManager)\n+    val endpoint = new MesosExternalShuffleServiceEndpoint(rpcEnv)\n+    rpcEnv.setupEndpoint(MesosExternalShuffleService.ENDPOINT_NAME, endpoint)\n+  }\n+\n+  override def stop(): Unit = {\n+    rpcEnv.shutdown()\n+    rpcEnv.awaitTermination()\n+    super.stop()\n+  }\n+\n+  private class MesosExternalShuffleServiceEndpoint(override val rpcEnv: RpcEnv)\n+    extends ThreadSafeRpcEndpoint with Logging {\n+\n+    // Stores a map of driver rpc addresses to app ids\n+    private val connectedApps = new mutable.HashMap[RpcAddress, String]\n+\n+    override def receive: PartialFunction[Any, Unit] = {\n+      case RegisterMesosDriver(appId, address) =>\n+        logDebug(s\"Received registration request from app $appId, address $address\")\n+        if (connectedApps.contains(address)) {\n+          val existingAppId: String = connectedApps(address)\n+          if (!existingAppId.equals(appId)) {\n+            logError(s\"A new app id $appId has connected to existing address $address\" +\n+                     s\", removing registered app $existingAppId\")\n+            blockHandler.applicationRemoved(existingAppId, true)\n+          }\n+        } else {\n+          connectedApps(address) = appId\n+        }"
  }],
  "prId": 7820
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "this should be at least `logInfo`. Also I would rephrase this:\n\n```\ns\"Application $appId disconnected (address was $remoteAddress)\"\n```\n",
    "commit": "fadff896194886854c0bae6657a4897f9a48ce21",
    "createdAt": "2015-08-01T08:15:06Z",
    "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.mesos\n+\n+import java.util.concurrent.CountDownLatch\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.deploy.ExternalShuffleService\n+import org.apache.spark.rpc.{RpcAddress, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SecurityManager, SparkConf}\n+\n+private[mesos] class MesosExternalShuffleService(conf: SparkConf, securityManager: SecurityManager)\n+  extends ExternalShuffleService(conf, securityManager) {\n+\n+  private var rpcEnv: RpcEnv = _\n+\n+  override def start(): Unit = {\n+    super.start()\n+    val port = conf.getInt(\"spark.mesos.shuffle.service.port\", 7327)\n+    logInfo(s\"Starting Mesos external shuffle service endpoint on port $port\")\n+    rpcEnv = RpcEnv.create(\n+      MesosExternalShuffleService.SYSTEM_NAME,\n+      Utils.localHostName(),\n+      port,\n+      conf,\n+      securityManager)\n+    val endpoint = new MesosExternalShuffleServiceEndpoint(rpcEnv)\n+    rpcEnv.setupEndpoint(MesosExternalShuffleService.ENDPOINT_NAME, endpoint)\n+  }\n+\n+  override def stop(): Unit = {\n+    rpcEnv.shutdown()\n+    rpcEnv.awaitTermination()\n+    super.stop()\n+  }\n+\n+  private class MesosExternalShuffleServiceEndpoint(override val rpcEnv: RpcEnv)\n+    extends ThreadSafeRpcEndpoint with Logging {\n+\n+    // Stores a map of driver rpc addresses to app ids\n+    private val connectedApps = new mutable.HashMap[RpcAddress, String]\n+\n+    override def receive: PartialFunction[Any, Unit] = {\n+      case RegisterMesosDriver(appId, address) =>\n+        logDebug(s\"Received registration request from app $appId, address $address\")\n+        if (connectedApps.contains(address)) {\n+          val existingAppId: String = connectedApps(address)\n+          if (!existingAppId.equals(appId)) {\n+            logError(s\"A new app id $appId has connected to existing address $address\" +\n+                     s\", removing registered app $existingAppId\")\n+            blockHandler.applicationRemoved(existingAppId, true)\n+          }\n+        } else {\n+          connectedApps(address) = appId\n+        }\n+    }\n+\n+    override def onDisconnected(remoteAddress: RpcAddress): Unit = {\n+      logDebug(s\"Received disconnect from $remoteAddress\")"
  }],
  "prId": 7820
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "as mentioned below, this would become `receiveAndReply` once you add the `DriverRegistered` response\n",
    "commit": "fadff896194886854c0bae6657a4897f9a48ce21",
    "createdAt": "2015-08-01T08:15:30Z",
    "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.mesos\n+\n+import java.util.concurrent.CountDownLatch\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.deploy.ExternalShuffleService\n+import org.apache.spark.rpc.{RpcAddress, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SecurityManager, SparkConf}\n+\n+private[mesos] class MesosExternalShuffleService(conf: SparkConf, securityManager: SecurityManager)\n+  extends ExternalShuffleService(conf, securityManager) {\n+\n+  private var rpcEnv: RpcEnv = _\n+\n+  override def start(): Unit = {\n+    super.start()\n+    val port = conf.getInt(\"spark.mesos.shuffle.service.port\", 7327)\n+    logInfo(s\"Starting Mesos external shuffle service endpoint on port $port\")\n+    rpcEnv = RpcEnv.create(\n+      MesosExternalShuffleService.SYSTEM_NAME,\n+      Utils.localHostName(),\n+      port,\n+      conf,\n+      securityManager)\n+    val endpoint = new MesosExternalShuffleServiceEndpoint(rpcEnv)\n+    rpcEnv.setupEndpoint(MesosExternalShuffleService.ENDPOINT_NAME, endpoint)\n+  }\n+\n+  override def stop(): Unit = {\n+    rpcEnv.shutdown()\n+    rpcEnv.awaitTermination()\n+    super.stop()\n+  }\n+\n+  private class MesosExternalShuffleServiceEndpoint(override val rpcEnv: RpcEnv)\n+    extends ThreadSafeRpcEndpoint with Logging {\n+\n+    // Stores a map of driver rpc addresses to app ids\n+    private val connectedApps = new mutable.HashMap[RpcAddress, String]\n+\n+    override def receive: PartialFunction[Any, Unit] = {"
  }],
  "prId": 7820
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "nit: just start with `An RPC endpoint that...`. It's kinda verbose right now.\n",
    "commit": "fadff896194886854c0bae6657a4897f9a48ce21",
    "createdAt": "2015-08-02T22:59:15Z",
    "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.mesos\n+\n+import java.net.SocketAddress\n+import java.util.concurrent.CountDownLatch\n+\n+import org.apache.spark.deploy.ExternalShuffleService\n+import org.apache.spark.network.client.{RpcResponseCallback, TransportClient}\n+import org.apache.spark.network.netty.SparkTransportConf\n+import org.apache.spark.network.shuffle.ExternalShuffleBlockHandler\n+import org.apache.spark.network.shuffle.protocol.BlockTransferMessage\n+import org.apache.spark.network.shuffle.protocol.mesos.RegisterDriver\n+import org.apache.spark.network.util.TransportConf\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SecurityManager, SparkConf}\n+\n+import scala.collection.mutable\n+\n+\n+/**\n+ * MesosExternalShuffleServiceEndpoint is a RPC endpoint that receives",
    "line": 37
  }],
  "prId": 7820
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "can you name this boolean parameter, e.g. `cleanup = true`\n",
    "commit": "fadff896194886854c0bae6657a4897f9a48ce21",
    "createdAt": "2015-08-02T22:59:45Z",
    "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.mesos\n+\n+import java.net.SocketAddress\n+import java.util.concurrent.CountDownLatch\n+\n+import org.apache.spark.deploy.ExternalShuffleService\n+import org.apache.spark.network.client.{RpcResponseCallback, TransportClient}\n+import org.apache.spark.network.netty.SparkTransportConf\n+import org.apache.spark.network.shuffle.ExternalShuffleBlockHandler\n+import org.apache.spark.network.shuffle.protocol.BlockTransferMessage\n+import org.apache.spark.network.shuffle.protocol.mesos.RegisterDriver\n+import org.apache.spark.network.util.TransportConf\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SecurityManager, SparkConf}\n+\n+import scala.collection.mutable\n+\n+\n+/**\n+ * MesosExternalShuffleServiceEndpoint is a RPC endpoint that receives\n+ * registration requests from Spark drivers launched with Mesos.\n+ * It detects driver termination and calls the cleanup callback to [[ExternalShuffleService]]\n+ */\n+private[mesos] class MesosExternalShuffleBlockHandler(transportConf: TransportConf)\n+  extends ExternalShuffleBlockHandler(transportConf) with Logging {\n+\n+  // Stores a map of driver socket addresses to app ids\n+  private val connectedApps = new mutable.HashMap[SocketAddress, String]\n+\n+  protected override def handleMessage(\n+      message: BlockTransferMessage,\n+      client: TransportClient,\n+      callback: RpcResponseCallback): Unit = {\n+    message match {\n+      case RegisterDriverParam(appId) =>\n+        val address = client.getSocketAddress()\n+        logDebug(s\"Received registration request from app $appId, address $address\")\n+        if (connectedApps.contains(address)) {\n+          val existingAppId: String = connectedApps(address)\n+          if (!existingAppId.equals(appId)) {\n+            logError(s\"A new app id $appId has connected to existing address $address\" +\n+              s\", removing registered app $existingAppId\")\n+            applicationRemoved(existingAppId, true)",
    "line": 60
  }],
  "prId": 7820
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "this could be strictly `private`. It could even go into the `MesosShuffleBlockHandler` class since it's only used there\n",
    "commit": "fadff896194886854c0bae6657a4897f9a48ce21",
    "createdAt": "2015-08-02T23:00:25Z",
    "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.mesos\n+\n+import java.net.SocketAddress\n+import java.util.concurrent.CountDownLatch\n+\n+import org.apache.spark.deploy.ExternalShuffleService\n+import org.apache.spark.network.client.{RpcResponseCallback, TransportClient}\n+import org.apache.spark.network.netty.SparkTransportConf\n+import org.apache.spark.network.shuffle.ExternalShuffleBlockHandler\n+import org.apache.spark.network.shuffle.protocol.BlockTransferMessage\n+import org.apache.spark.network.shuffle.protocol.mesos.RegisterDriver\n+import org.apache.spark.network.util.TransportConf\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SecurityManager, SparkConf}\n+\n+import scala.collection.mutable\n+\n+\n+/**\n+ * MesosExternalShuffleServiceEndpoint is a RPC endpoint that receives\n+ * registration requests from Spark drivers launched with Mesos.\n+ * It detects driver termination and calls the cleanup callback to [[ExternalShuffleService]]\n+ */\n+private[mesos] class MesosExternalShuffleBlockHandler(transportConf: TransportConf)\n+  extends ExternalShuffleBlockHandler(transportConf) with Logging {\n+\n+  // Stores a map of driver socket addresses to app ids\n+  private val connectedApps = new mutable.HashMap[SocketAddress, String]\n+\n+  protected override def handleMessage(\n+      message: BlockTransferMessage,\n+      client: TransportClient,\n+      callback: RpcResponseCallback): Unit = {\n+    message match {\n+      case RegisterDriverParam(appId) =>\n+        val address = client.getSocketAddress()\n+        logDebug(s\"Received registration request from app $appId, address $address\")\n+        if (connectedApps.contains(address)) {\n+          val existingAppId: String = connectedApps(address)\n+          if (!existingAppId.equals(appId)) {\n+            logError(s\"A new app id $appId has connected to existing address $address\" +\n+              s\", removing registered app $existingAppId\")\n+            applicationRemoved(existingAppId, true)\n+          }\n+        }\n+        connectedApps(address) = appId\n+        callback.onSuccess(new Array[Byte](0))\n+      case _ => super.handleMessage(message, client, callback)\n+    }\n+  }\n+\n+  override def connectionTerminated(client: TransportClient): Unit = {\n+    val address = client.getSocketAddress()\n+    if (connectedApps.contains(address)) {\n+      val appId = connectedApps(address)\n+      logInfo(s\"Application $appId disconnected (address was $address)\")\n+      applicationRemoved(appId, true)\n+      connectedApps.remove(address)\n+    } else {\n+      logWarning(s\"Address $address not found in mesos shuffle service\")\n+    }\n+  }\n+}\n+\n+/**\n+ * An extractor object for matching RegisterDriver message.\n+ */\n+private[mesos] object RegisterDriverParam {",
    "line": 85
  }],
  "prId": 7820
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "technically incorrect: shuffle data is not cached. Just say the `clean up the shuffle files`\n",
    "commit": "fadff896194886854c0bae6657a4897f9a48ce21",
    "createdAt": "2015-08-02T23:01:25Z",
    "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.mesos\n+\n+import java.net.SocketAddress\n+import java.util.concurrent.CountDownLatch\n+\n+import org.apache.spark.deploy.ExternalShuffleService\n+import org.apache.spark.network.client.{RpcResponseCallback, TransportClient}\n+import org.apache.spark.network.netty.SparkTransportConf\n+import org.apache.spark.network.shuffle.ExternalShuffleBlockHandler\n+import org.apache.spark.network.shuffle.protocol.BlockTransferMessage\n+import org.apache.spark.network.shuffle.protocol.mesos.RegisterDriver\n+import org.apache.spark.network.util.TransportConf\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SecurityManager, SparkConf}\n+\n+import scala.collection.mutable\n+\n+\n+/**\n+ * MesosExternalShuffleServiceEndpoint is a RPC endpoint that receives\n+ * registration requests from Spark drivers launched with Mesos.\n+ * It detects driver termination and calls the cleanup callback to [[ExternalShuffleService]]\n+ */\n+private[mesos] class MesosExternalShuffleBlockHandler(transportConf: TransportConf)\n+  extends ExternalShuffleBlockHandler(transportConf) with Logging {\n+\n+  // Stores a map of driver socket addresses to app ids\n+  private val connectedApps = new mutable.HashMap[SocketAddress, String]\n+\n+  protected override def handleMessage(\n+      message: BlockTransferMessage,\n+      client: TransportClient,\n+      callback: RpcResponseCallback): Unit = {\n+    message match {\n+      case RegisterDriverParam(appId) =>\n+        val address = client.getSocketAddress()\n+        logDebug(s\"Received registration request from app $appId, address $address\")\n+        if (connectedApps.contains(address)) {\n+          val existingAppId: String = connectedApps(address)\n+          if (!existingAppId.equals(appId)) {\n+            logError(s\"A new app id $appId has connected to existing address $address\" +\n+              s\", removing registered app $existingAppId\")\n+            applicationRemoved(existingAppId, true)\n+          }\n+        }\n+        connectedApps(address) = appId\n+        callback.onSuccess(new Array[Byte](0))\n+      case _ => super.handleMessage(message, client, callback)\n+    }\n+  }\n+\n+  override def connectionTerminated(client: TransportClient): Unit = {\n+    val address = client.getSocketAddress()\n+    if (connectedApps.contains(address)) {\n+      val appId = connectedApps(address)\n+      logInfo(s\"Application $appId disconnected (address was $address)\")\n+      applicationRemoved(appId, true)\n+      connectedApps.remove(address)\n+    } else {\n+      logWarning(s\"Address $address not found in mesos shuffle service\")\n+    }\n+  }\n+}\n+\n+/**\n+ * An extractor object for matching RegisterDriver message.\n+ */\n+private[mesos] object RegisterDriverParam {\n+  def unapply(r: RegisterDriver): Option[String] = Some(r.getAppId())\n+}\n+\n+/**\n+ * MesosExternalShuffleService wraps [[ExternalShuffleService]] which provides an additional\n+ * endpoint for drivers to associate with. This allows the shuffle service to detect when\n+ * a driver is terminated and can further clean up the cached shuffle data.",
    "line": 92
  }],
  "prId": 7820
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "not used, please clean up the code after an update\n",
    "commit": "fadff896194886854c0bae6657a4897f9a48ce21",
    "createdAt": "2015-08-02T23:02:45Z",
    "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.mesos\n+\n+import java.net.SocketAddress\n+import java.util.concurrent.CountDownLatch\n+\n+import org.apache.spark.deploy.ExternalShuffleService\n+import org.apache.spark.network.client.{RpcResponseCallback, TransportClient}\n+import org.apache.spark.network.netty.SparkTransportConf\n+import org.apache.spark.network.shuffle.ExternalShuffleBlockHandler\n+import org.apache.spark.network.shuffle.protocol.BlockTransferMessage\n+import org.apache.spark.network.shuffle.protocol.mesos.RegisterDriver\n+import org.apache.spark.network.util.TransportConf\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SecurityManager, SparkConf}\n+\n+import scala.collection.mutable\n+\n+\n+/**\n+ * MesosExternalShuffleServiceEndpoint is a RPC endpoint that receives\n+ * registration requests from Spark drivers launched with Mesos.\n+ * It detects driver termination and calls the cleanup callback to [[ExternalShuffleService]]\n+ */\n+private[mesos] class MesosExternalShuffleBlockHandler(transportConf: TransportConf)\n+  extends ExternalShuffleBlockHandler(transportConf) with Logging {\n+\n+  // Stores a map of driver socket addresses to app ids\n+  private val connectedApps = new mutable.HashMap[SocketAddress, String]\n+\n+  protected override def handleMessage(\n+      message: BlockTransferMessage,\n+      client: TransportClient,\n+      callback: RpcResponseCallback): Unit = {\n+    message match {\n+      case RegisterDriverParam(appId) =>\n+        val address = client.getSocketAddress()\n+        logDebug(s\"Received registration request from app $appId, address $address\")\n+        if (connectedApps.contains(address)) {\n+          val existingAppId: String = connectedApps(address)\n+          if (!existingAppId.equals(appId)) {\n+            logError(s\"A new app id $appId has connected to existing address $address\" +\n+              s\", removing registered app $existingAppId\")\n+            applicationRemoved(existingAppId, true)\n+          }\n+        }\n+        connectedApps(address) = appId\n+        callback.onSuccess(new Array[Byte](0))\n+      case _ => super.handleMessage(message, client, callback)\n+    }\n+  }\n+\n+  override def connectionTerminated(client: TransportClient): Unit = {\n+    val address = client.getSocketAddress()\n+    if (connectedApps.contains(address)) {\n+      val appId = connectedApps(address)\n+      logInfo(s\"Application $appId disconnected (address was $address)\")\n+      applicationRemoved(appId, true)\n+      connectedApps.remove(address)\n+    } else {\n+      logWarning(s\"Address $address not found in mesos shuffle service\")\n+    }\n+  }\n+}\n+\n+/**\n+ * An extractor object for matching RegisterDriver message.\n+ */\n+private[mesos] object RegisterDriverParam {\n+  def unapply(r: RegisterDriver): Option[String] = Some(r.getAppId())\n+}\n+\n+/**\n+ * MesosExternalShuffleService wraps [[ExternalShuffleService]] which provides an additional\n+ * endpoint for drivers to associate with. This allows the shuffle service to detect when\n+ * a driver is terminated and can further clean up the cached shuffle data.\n+ */\n+private[mesos] class MesosExternalShuffleService(\n+    conf: SparkConf,\n+    securityManager: SecurityManager,\n+    transportConf: TransportConf)\n+  extends ExternalShuffleService(\n+    conf, securityManager, transportConf, new MesosExternalShuffleBlockHandler(transportConf)) {\n+}\n+\n+private[spark] object MesosExternalShuffleService extends Logging {\n+  val SYSTEM_NAME = \"mesosExternalShuffleService\"\n+  val ENDPOINT_NAME = \"mesosExternalShuffleServiceEndpoint\"",
    "line": 104
  }],
  "prId": 7820
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "this whole thing is duplicated from `ExternalShuffleService`. We should at least attempt to reuse some of the code... I think if you make this object inherit the `ExternalShuffleService` object we do it.\n",
    "commit": "fadff896194886854c0bae6657a4897f9a48ce21",
    "createdAt": "2015-08-02T23:05:41Z",
    "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.mesos\n+\n+import java.net.SocketAddress\n+import java.util.concurrent.CountDownLatch\n+\n+import org.apache.spark.deploy.ExternalShuffleService\n+import org.apache.spark.network.client.{RpcResponseCallback, TransportClient}\n+import org.apache.spark.network.netty.SparkTransportConf\n+import org.apache.spark.network.shuffle.ExternalShuffleBlockHandler\n+import org.apache.spark.network.shuffle.protocol.BlockTransferMessage\n+import org.apache.spark.network.shuffle.protocol.mesos.RegisterDriver\n+import org.apache.spark.network.util.TransportConf\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SecurityManager, SparkConf}\n+\n+import scala.collection.mutable\n+\n+\n+/**\n+ * MesosExternalShuffleServiceEndpoint is a RPC endpoint that receives\n+ * registration requests from Spark drivers launched with Mesos.\n+ * It detects driver termination and calls the cleanup callback to [[ExternalShuffleService]]\n+ */\n+private[mesos] class MesosExternalShuffleBlockHandler(transportConf: TransportConf)\n+  extends ExternalShuffleBlockHandler(transportConf) with Logging {\n+\n+  // Stores a map of driver socket addresses to app ids\n+  private val connectedApps = new mutable.HashMap[SocketAddress, String]\n+\n+  protected override def handleMessage(\n+      message: BlockTransferMessage,\n+      client: TransportClient,\n+      callback: RpcResponseCallback): Unit = {\n+    message match {\n+      case RegisterDriverParam(appId) =>\n+        val address = client.getSocketAddress()\n+        logDebug(s\"Received registration request from app $appId, address $address\")\n+        if (connectedApps.contains(address)) {\n+          val existingAppId: String = connectedApps(address)\n+          if (!existingAppId.equals(appId)) {\n+            logError(s\"A new app id $appId has connected to existing address $address\" +\n+              s\", removing registered app $existingAppId\")\n+            applicationRemoved(existingAppId, true)\n+          }\n+        }\n+        connectedApps(address) = appId\n+        callback.onSuccess(new Array[Byte](0))\n+      case _ => super.handleMessage(message, client, callback)\n+    }\n+  }\n+\n+  override def connectionTerminated(client: TransportClient): Unit = {\n+    val address = client.getSocketAddress()\n+    if (connectedApps.contains(address)) {\n+      val appId = connectedApps(address)\n+      logInfo(s\"Application $appId disconnected (address was $address)\")\n+      applicationRemoved(appId, true)\n+      connectedApps.remove(address)\n+    } else {\n+      logWarning(s\"Address $address not found in mesos shuffle service\")\n+    }\n+  }\n+}\n+\n+/**\n+ * An extractor object for matching RegisterDriver message.\n+ */\n+private[mesos] object RegisterDriverParam {\n+  def unapply(r: RegisterDriver): Option[String] = Some(r.getAppId())\n+}\n+\n+/**\n+ * MesosExternalShuffleService wraps [[ExternalShuffleService]] which provides an additional\n+ * endpoint for drivers to associate with. This allows the shuffle service to detect when\n+ * a driver is terminated and can further clean up the cached shuffle data.\n+ */\n+private[mesos] class MesosExternalShuffleService(\n+    conf: SparkConf,\n+    securityManager: SecurityManager,\n+    transportConf: TransportConf)\n+  extends ExternalShuffleService(\n+    conf, securityManager, transportConf, new MesosExternalShuffleBlockHandler(transportConf)) {\n+}\n+\n+private[spark] object MesosExternalShuffleService extends Logging {",
    "line": 102
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "never mind you can't extend objects.\n",
    "commit": "fadff896194886854c0bae6657a4897f9a48ce21",
    "createdAt": "2015-08-02T23:43:00Z",
    "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.mesos\n+\n+import java.net.SocketAddress\n+import java.util.concurrent.CountDownLatch\n+\n+import org.apache.spark.deploy.ExternalShuffleService\n+import org.apache.spark.network.client.{RpcResponseCallback, TransportClient}\n+import org.apache.spark.network.netty.SparkTransportConf\n+import org.apache.spark.network.shuffle.ExternalShuffleBlockHandler\n+import org.apache.spark.network.shuffle.protocol.BlockTransferMessage\n+import org.apache.spark.network.shuffle.protocol.mesos.RegisterDriver\n+import org.apache.spark.network.util.TransportConf\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SecurityManager, SparkConf}\n+\n+import scala.collection.mutable\n+\n+\n+/**\n+ * MesosExternalShuffleServiceEndpoint is a RPC endpoint that receives\n+ * registration requests from Spark drivers launched with Mesos.\n+ * It detects driver termination and calls the cleanup callback to [[ExternalShuffleService]]\n+ */\n+private[mesos] class MesosExternalShuffleBlockHandler(transportConf: TransportConf)\n+  extends ExternalShuffleBlockHandler(transportConf) with Logging {\n+\n+  // Stores a map of driver socket addresses to app ids\n+  private val connectedApps = new mutable.HashMap[SocketAddress, String]\n+\n+  protected override def handleMessage(\n+      message: BlockTransferMessage,\n+      client: TransportClient,\n+      callback: RpcResponseCallback): Unit = {\n+    message match {\n+      case RegisterDriverParam(appId) =>\n+        val address = client.getSocketAddress()\n+        logDebug(s\"Received registration request from app $appId, address $address\")\n+        if (connectedApps.contains(address)) {\n+          val existingAppId: String = connectedApps(address)\n+          if (!existingAppId.equals(appId)) {\n+            logError(s\"A new app id $appId has connected to existing address $address\" +\n+              s\", removing registered app $existingAppId\")\n+            applicationRemoved(existingAppId, true)\n+          }\n+        }\n+        connectedApps(address) = appId\n+        callback.onSuccess(new Array[Byte](0))\n+      case _ => super.handleMessage(message, client, callback)\n+    }\n+  }\n+\n+  override def connectionTerminated(client: TransportClient): Unit = {\n+    val address = client.getSocketAddress()\n+    if (connectedApps.contains(address)) {\n+      val appId = connectedApps(address)\n+      logInfo(s\"Application $appId disconnected (address was $address)\")\n+      applicationRemoved(appId, true)\n+      connectedApps.remove(address)\n+    } else {\n+      logWarning(s\"Address $address not found in mesos shuffle service\")\n+    }\n+  }\n+}\n+\n+/**\n+ * An extractor object for matching RegisterDriver message.\n+ */\n+private[mesos] object RegisterDriverParam {\n+  def unapply(r: RegisterDriver): Option[String] = Some(r.getAppId())\n+}\n+\n+/**\n+ * MesosExternalShuffleService wraps [[ExternalShuffleService]] which provides an additional\n+ * endpoint for drivers to associate with. This allows the shuffle service to detect when\n+ * a driver is terminated and can further clean up the cached shuffle data.\n+ */\n+private[mesos] class MesosExternalShuffleService(\n+    conf: SparkConf,\n+    securityManager: SecurityManager,\n+    transportConf: TransportConf)\n+  extends ExternalShuffleService(\n+    conf, securityManager, transportConf, new MesosExternalShuffleBlockHandler(transportConf)) {\n+}\n+\n+private[spark] object MesosExternalShuffleService extends Logging {",
    "line": 102
  }],
  "prId": 7820
}]