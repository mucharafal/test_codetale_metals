[{
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "I would structure that goal as a TODO that appears internally in the class rather than in the public docstring.\n",
    "commit": "bed4bdff74e941f791080e9407fc5295af10f677",
    "createdAt": "2014-08-19T04:00:01Z",
    "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io.File\n+\n+import scala.collection.JavaConversions._\n+\n+import org.apache.spark.util.{RedirectThread, Utils}\n+\n+/**\n+ * Wrapper of `bin/spark-class` that prepares the launch environment of the child JVM properly.\n+ * This is currently only used for running Spark submit in client mode. The goal moving forward"
  }],
  "prId": 1845
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "I can't see a case where this would ever not be \"true\" - to keep it simple and understandable it might be good to just omit this command line argument and the associated logic.\n",
    "commit": "bed4bdff74e941f791080e9407fc5295af10f677",
    "createdAt": "2014-08-19T04:41:20Z",
    "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io.File\n+\n+import scala.collection.JavaConversions._\n+\n+import org.apache.spark.util.{RedirectThread, Utils}\n+\n+/**\n+ * Wrapper of `bin/spark-class` that prepares the launch environment of the child JVM properly.\n+ * This is currently only used for running Spark submit in client mode. The goal moving forward\n+ * is to use this class for all use cases of `bin/spark-class`.\n+ */\n+object SparkClassLauncher {\n+\n+  /**\n+   * Launch a Spark class with the given class paths, library paths, java options and memory.\n+   * If we are launching an application through Spark submit in client mode, we must also\n+   * take into account special `spark.driver.*` properties needed to start the driver JVM.\n+   */\n+  def main(args: Array[String]): Unit = {\n+    if (args.size < 8) {\n+      System.err.println(\n+        \"\"\"\n+          |Usage: org.apache.spark.deploy.SparkClassLauncher\n+          |\n+          |  [properties file]    - path to your Spark properties file\n+          |  [java runner]        - command to launch the child JVM\n+          |  [java class paths]   - class paths to pass to the child JVM\n+          |  [java library paths] - library paths to pass to the child JVM\n+          |  [java opts]          - java options to pass to the child JVM\n+          |  [java memory]        - memory used to launch the child JVM\n+          |  [client mode]        - whether the child JVM will run the Spark driver\n+          |  [main class]         - main class to run in the child JVM\n+          |  <main args>          - arguments passed to this main class\n+          |\n+          |Example:\n+          |  org.apache.spark.deploy.SparkClassLauncher.SparkClassLauncher\n+          |    conf/spark-defaults.conf java /classpath1:/classpath2 /librarypath1:/librarypath2\n+          |    \"-XX:-UseParallelGC -Dsome=property\" 5g true org.apache.spark.deploy.SparkSubmit\n+          |    --master local --class org.apache.spark.examples.SparkPi 10\n+        \"\"\".stripMargin)\n+      System.exit(1)\n+    }\n+    val propertiesFile = args(0)\n+    val javaRunner = args(1)\n+    val clClassPaths = args(2)\n+    val clLibraryPaths = args(3)\n+    val clJavaOpts = Utils.splitCommandString(args(4))\n+    val clJavaMemory = args(5)\n+    val clientMode = args(6) == \"true\""
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "This is anticipating the future where all usages of `bin/spark-class` are routed through this class. I was hesitant on removing this because it would technically break backward compatibility if we decide to add a new flag in the future, since this is not `private[spark]` or anything. Should I just add a warning saying this should be called only from `bin/spark-class` instead?\n",
    "commit": "bed4bdff74e941f791080e9407fc5295af10f677",
    "createdAt": "2014-08-19T05:07:05Z",
    "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io.File\n+\n+import scala.collection.JavaConversions._\n+\n+import org.apache.spark.util.{RedirectThread, Utils}\n+\n+/**\n+ * Wrapper of `bin/spark-class` that prepares the launch environment of the child JVM properly.\n+ * This is currently only used for running Spark submit in client mode. The goal moving forward\n+ * is to use this class for all use cases of `bin/spark-class`.\n+ */\n+object SparkClassLauncher {\n+\n+  /**\n+   * Launch a Spark class with the given class paths, library paths, java options and memory.\n+   * If we are launching an application through Spark submit in client mode, we must also\n+   * take into account special `spark.driver.*` properties needed to start the driver JVM.\n+   */\n+  def main(args: Array[String]): Unit = {\n+    if (args.size < 8) {\n+      System.err.println(\n+        \"\"\"\n+          |Usage: org.apache.spark.deploy.SparkClassLauncher\n+          |\n+          |  [properties file]    - path to your Spark properties file\n+          |  [java runner]        - command to launch the child JVM\n+          |  [java class paths]   - class paths to pass to the child JVM\n+          |  [java library paths] - library paths to pass to the child JVM\n+          |  [java opts]          - java options to pass to the child JVM\n+          |  [java memory]        - memory used to launch the child JVM\n+          |  [client mode]        - whether the child JVM will run the Spark driver\n+          |  [main class]         - main class to run in the child JVM\n+          |  <main args>          - arguments passed to this main class\n+          |\n+          |Example:\n+          |  org.apache.spark.deploy.SparkClassLauncher.SparkClassLauncher\n+          |    conf/spark-defaults.conf java /classpath1:/classpath2 /librarypath1:/librarypath2\n+          |    \"-XX:-UseParallelGC -Dsome=property\" 5g true org.apache.spark.deploy.SparkSubmit\n+          |    --master local --class org.apache.spark.examples.SparkPi 10\n+        \"\"\".stripMargin)\n+      System.exit(1)\n+    }\n+    val propertiesFile = args(0)\n+    val javaRunner = args(1)\n+    val clClassPaths = args(2)\n+    val clLibraryPaths = args(3)\n+    val clJavaOpts = Utils.splitCommandString(args(4))\n+    val clJavaMemory = args(5)\n+    val clientMode = args(6) == \"true\""
  }, {
    "author": {
      "login": "pwendell"
    },
    "body": "This is an internal tool - it doesn't need to have compatibility at all.\n",
    "commit": "bed4bdff74e941f791080e9407fc5295af10f677",
    "createdAt": "2014-08-19T05:25:23Z",
    "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io.File\n+\n+import scala.collection.JavaConversions._\n+\n+import org.apache.spark.util.{RedirectThread, Utils}\n+\n+/**\n+ * Wrapper of `bin/spark-class` that prepares the launch environment of the child JVM properly.\n+ * This is currently only used for running Spark submit in client mode. The goal moving forward\n+ * is to use this class for all use cases of `bin/spark-class`.\n+ */\n+object SparkClassLauncher {\n+\n+  /**\n+   * Launch a Spark class with the given class paths, library paths, java options and memory.\n+   * If we are launching an application through Spark submit in client mode, we must also\n+   * take into account special `spark.driver.*` properties needed to start the driver JVM.\n+   */\n+  def main(args: Array[String]): Unit = {\n+    if (args.size < 8) {\n+      System.err.println(\n+        \"\"\"\n+          |Usage: org.apache.spark.deploy.SparkClassLauncher\n+          |\n+          |  [properties file]    - path to your Spark properties file\n+          |  [java runner]        - command to launch the child JVM\n+          |  [java class paths]   - class paths to pass to the child JVM\n+          |  [java library paths] - library paths to pass to the child JVM\n+          |  [java opts]          - java options to pass to the child JVM\n+          |  [java memory]        - memory used to launch the child JVM\n+          |  [client mode]        - whether the child JVM will run the Spark driver\n+          |  [main class]         - main class to run in the child JVM\n+          |  <main args>          - arguments passed to this main class\n+          |\n+          |Example:\n+          |  org.apache.spark.deploy.SparkClassLauncher.SparkClassLauncher\n+          |    conf/spark-defaults.conf java /classpath1:/classpath2 /librarypath1:/librarypath2\n+          |    \"-XX:-UseParallelGC -Dsome=property\" 5g true org.apache.spark.deploy.SparkSubmit\n+          |    --master local --class org.apache.spark.examples.SparkPi 10\n+        \"\"\".stripMargin)\n+      System.exit(1)\n+    }\n+    val propertiesFile = args(0)\n+    val javaRunner = args(1)\n+    val clClassPaths = args(2)\n+    val clLibraryPaths = args(3)\n+    val clJavaOpts = Utils.splitCommandString(args(4))\n+    val clJavaMemory = args(5)\n+    val clientMode = args(6) == \"true\""
  }],
  "prId": 1845
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "To keep it simpler for now, rather than having a bunch of command line arguments, why not just directly read the environment variables set in `spark-class`? Then you could just have this script take the main class and the arguments and you could just directly read `RUNNER`, `PROPERTEIS_FILE`, `CLASSPATH`, `SPARK_SUBMIT_LIBRARY_PATH`, `JAVA_OPTS`, and `OUR_JAVA_MEM`. This is one fewer levels of interpretation/parsing to worry about for now and would overall make this patch smaller.\n",
    "commit": "bed4bdff74e941f791080e9407fc5295af10f677",
    "createdAt": "2014-08-19T05:06:10Z",
    "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io.File\n+\n+import scala.collection.JavaConversions._\n+\n+import org.apache.spark.util.{RedirectThread, Utils}\n+\n+/**\n+ * Wrapper of `bin/spark-class` that prepares the launch environment of the child JVM properly.\n+ * This is currently only used for running Spark submit in client mode. The goal moving forward\n+ * is to use this class for all use cases of `bin/spark-class`.\n+ */\n+object SparkClassLauncher {\n+\n+  /**\n+   * Launch a Spark class with the given class paths, library paths, java options and memory.\n+   * If we are launching an application through Spark submit in client mode, we must also\n+   * take into account special `spark.driver.*` properties needed to start the driver JVM.\n+   */\n+  def main(args: Array[String]): Unit = {\n+    if (args.size < 8) {\n+      System.err.println(\n+        \"\"\"\n+          |Usage: org.apache.spark.deploy.SparkClassLauncher"
  }],
  "prId": 1845
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "does this mean if I put `--driver-memory` as a flag and I also include `spark.driver.memory` it in the config file that the config file will take precedence?\n",
    "commit": "bed4bdff74e941f791080e9407fc5295af10f677",
    "createdAt": "2014-08-19T05:20:13Z",
    "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io.File\n+\n+import scala.collection.JavaConversions._\n+\n+import org.apache.spark.util.{RedirectThread, Utils}\n+\n+/**\n+ * Wrapper of `bin/spark-class` that prepares the launch environment of the child JVM properly.\n+ * This is currently only used for running Spark submit in client mode. The goal moving forward\n+ * is to use this class for all use cases of `bin/spark-class`.\n+ */\n+object SparkClassLauncher {\n+\n+  /**\n+   * Launch a Spark class with the given class paths, library paths, java options and memory.\n+   * If we are launching an application through Spark submit in client mode, we must also\n+   * take into account special `spark.driver.*` properties needed to start the driver JVM.\n+   */\n+  def main(args: Array[String]): Unit = {\n+    if (args.size < 8) {\n+      System.err.println(\n+        \"\"\"\n+          |Usage: org.apache.spark.deploy.SparkClassLauncher\n+          |\n+          |  [properties file]    - path to your Spark properties file\n+          |  [java runner]        - command to launch the child JVM\n+          |  [java class paths]   - class paths to pass to the child JVM\n+          |  [java library paths] - library paths to pass to the child JVM\n+          |  [java opts]          - java options to pass to the child JVM\n+          |  [java memory]        - memory used to launch the child JVM\n+          |  [client mode]        - whether the child JVM will run the Spark driver\n+          |  [main class]         - main class to run in the child JVM\n+          |  <main args>          - arguments passed to this main class\n+          |\n+          |Example:\n+          |  org.apache.spark.deploy.SparkClassLauncher.SparkClassLauncher\n+          |    conf/spark-defaults.conf java /classpath1:/classpath2 /librarypath1:/librarypath2\n+          |    \"-XX:-UseParallelGC -Dsome=property\" 5g true org.apache.spark.deploy.SparkSubmit\n+          |    --master local --class org.apache.spark.examples.SparkPi 10\n+        \"\"\".stripMargin)\n+      System.exit(1)\n+    }\n+    val propertiesFile = args(0)\n+    val javaRunner = args(1)\n+    val clClassPaths = args(2)\n+    val clLibraryPaths = args(3)\n+    val clJavaOpts = Utils.splitCommandString(args(4))\n+    val clJavaMemory = args(5)\n+    val clientMode = args(6) == \"true\"\n+    val mainClass = args(7)\n+\n+    // In client deploy mode, parse the properties file for certain `spark.driver.*` configs.\n+    // These configs encode java options, class paths, and library paths needed to launch the JVM.\n+    val properties =\n+      if (clientMode) {\n+        SparkSubmitArguments.getPropertiesFromFile(new File(propertiesFile)).toMap\n+      } else {\n+        Map[String, String]()\n+      }\n+    val confDriverMemory = properties.get(\"spark.driver.memory\")\n+    val confClassPaths = properties.get(\"spark.driver.extraClassPath\")\n+    val confLibraryPaths = properties.get(\"spark.driver.extraLibraryPath\")\n+    val confJavaOpts = properties.get(\"spark.driver.extraJavaOptions\")\n+\n+    // Merge relevant command line values with the config equivalents, if any\n+    val javaMemory =\n+      if (clientMode) {\n+        confDriverMemory.getOrElse(clJavaMemory)\n+      } else {\n+        clJavaMemory\n+      }\n+    val pathSeparator = sys.props(\"path.separator\")\n+    val classPaths = clClassPaths + confClassPaths.map(pathSeparator + _).getOrElse(\"\")\n+    val libraryPaths = clLibraryPaths + confLibraryPaths.map(pathSeparator + _).getOrElse(\"\")\n+    val javaOpts = clJavaOpts ++ confJavaOpts.map(Utils.splitCommandString).getOrElse(Seq.empty)\n+    val filteredJavaOpts = javaOpts.distinct.filterNot { opt =>\n+      opt.startsWith(\"-Djava.library.path\") || opt.startsWith(\"-Xms\") || opt.startsWith(\"-Xmx\")"
  }],
  "prId": 1845
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "rather than append one to the other - I think that users setting `--driver-library-path` should simply take precedence over any `driver.library.path` defined in the conf. That's the behavior we have in general for configs... it could cause some confusion to merge them here.\n",
    "commit": "bed4bdff74e941f791080e9407fc5295af10f677",
    "createdAt": "2014-08-19T05:23:52Z",
    "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io.File\n+\n+import scala.collection.JavaConversions._\n+\n+import org.apache.spark.util.{RedirectThread, Utils}\n+\n+/**\n+ * Wrapper of `bin/spark-class` that prepares the launch environment of the child JVM properly.\n+ * This is currently only used for running Spark submit in client mode. The goal moving forward\n+ * is to use this class for all use cases of `bin/spark-class`.\n+ */\n+object SparkClassLauncher {\n+\n+  /**\n+   * Launch a Spark class with the given class paths, library paths, java options and memory.\n+   * If we are launching an application through Spark submit in client mode, we must also\n+   * take into account special `spark.driver.*` properties needed to start the driver JVM.\n+   */\n+  def main(args: Array[String]): Unit = {\n+    if (args.size < 8) {\n+      System.err.println(\n+        \"\"\"\n+          |Usage: org.apache.spark.deploy.SparkClassLauncher\n+          |\n+          |  [properties file]    - path to your Spark properties file\n+          |  [java runner]        - command to launch the child JVM\n+          |  [java class paths]   - class paths to pass to the child JVM\n+          |  [java library paths] - library paths to pass to the child JVM\n+          |  [java opts]          - java options to pass to the child JVM\n+          |  [java memory]        - memory used to launch the child JVM\n+          |  [client mode]        - whether the child JVM will run the Spark driver\n+          |  [main class]         - main class to run in the child JVM\n+          |  <main args>          - arguments passed to this main class\n+          |\n+          |Example:\n+          |  org.apache.spark.deploy.SparkClassLauncher.SparkClassLauncher\n+          |    conf/spark-defaults.conf java /classpath1:/classpath2 /librarypath1:/librarypath2\n+          |    \"-XX:-UseParallelGC -Dsome=property\" 5g true org.apache.spark.deploy.SparkSubmit\n+          |    --master local --class org.apache.spark.examples.SparkPi 10\n+        \"\"\".stripMargin)\n+      System.exit(1)\n+    }\n+    val propertiesFile = args(0)\n+    val javaRunner = args(1)\n+    val clClassPaths = args(2)\n+    val clLibraryPaths = args(3)\n+    val clJavaOpts = Utils.splitCommandString(args(4))\n+    val clJavaMemory = args(5)\n+    val clientMode = args(6) == \"true\"\n+    val mainClass = args(7)\n+\n+    // In client deploy mode, parse the properties file for certain `spark.driver.*` configs.\n+    // These configs encode java options, class paths, and library paths needed to launch the JVM.\n+    val properties =\n+      if (clientMode) {\n+        SparkSubmitArguments.getPropertiesFromFile(new File(propertiesFile)).toMap\n+      } else {\n+        Map[String, String]()\n+      }\n+    val confDriverMemory = properties.get(\"spark.driver.memory\")\n+    val confClassPaths = properties.get(\"spark.driver.extraClassPath\")\n+    val confLibraryPaths = properties.get(\"spark.driver.extraLibraryPath\")\n+    val confJavaOpts = properties.get(\"spark.driver.extraJavaOptions\")\n+\n+    // Merge relevant command line values with the config equivalents, if any\n+    val javaMemory =\n+      if (clientMode) {\n+        confDriverMemory.getOrElse(clJavaMemory)\n+      } else {\n+        clJavaMemory\n+      }\n+    val pathSeparator = sys.props(\"path.separator\")\n+    val classPaths = clClassPaths + confClassPaths.map(pathSeparator + _).getOrElse(\"\")\n+    val libraryPaths = clLibraryPaths + confLibraryPaths.map(pathSeparator + _).getOrElse(\"\")"
  }],
  "prId": 1845
}]