[{
  "comments": [{
    "author": {
      "login": "attilapiros"
    },
    "body": "This method is very similar to HBaseDelegationTokenProvider#obtainDelegationTokens() mostly the loaded class name is the only difference (and some log lines where the serviceName can be used). Does it makes sense to extract this?",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-10-01T12:14:13Z",
    "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import scala.reflect.runtime.universe\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.Credentials\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.{KAFKA_DELEGATION_TOKEN_ENABLED, KAFKA_SECURITY_PROTOCOL}\n+import org.apache.spark.util.Utils\n+\n+private[security] class KafkaDelegationTokenProvider\n+  extends HadoopDelegationTokenProvider with Logging {\n+\n+  override def serviceName: String = \"kafka\"\n+\n+  override def obtainDelegationTokens(",
    "line": 36
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "I see ~5-10 lines of duplication. Creating a base class for this would be an overkill. How do you think it could be done exactly?",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-10-01T13:34:22Z",
    "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import scala.reflect.runtime.universe\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.Credentials\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.{KAFKA_DELEGATION_TOKEN_ENABLED, KAFKA_SECURITY_PROTOCOL}\n+import org.apache.spark.util.Utils\n+\n+private[security] class KafkaDelegationTokenProvider\n+  extends HadoopDelegationTokenProvider with Logging {\n+\n+  override def serviceName: String = \"kafka\"\n+\n+  override def obtainDelegationTokens(",
    "line": 36
  }, {
    "author": {
      "login": "attilapiros"
    },
    "body": "Yes, my idea was a base class which in the future could be the bases for other reflection based delegation token providers as well. ",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-10-02T04:19:28Z",
    "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import scala.reflect.runtime.universe\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.Credentials\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.{KAFKA_DELEGATION_TOKEN_ENABLED, KAFKA_SECURITY_PROTOCOL}\n+import org.apache.spark.util.Utils\n+\n+private[security] class KafkaDelegationTokenProvider\n+  extends HadoopDelegationTokenProvider with Logging {\n+\n+  override def serviceName: String = \"kafka\"\n+\n+  override def obtainDelegationTokens(",
    "line": 36
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "I've tried it out but still think it's an overkill.",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-10-02T19:31:13Z",
    "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import scala.reflect.runtime.universe\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.Credentials\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.{KAFKA_DELEGATION_TOKEN_ENABLED, KAFKA_SECURITY_PROTOCOL}\n+import org.apache.spark.util.Utils\n+\n+private[security] class KafkaDelegationTokenProvider\n+  extends HadoopDelegationTokenProvider with Logging {\n+\n+  override def serviceName: String = \"kafka\"\n+\n+  override def obtainDelegationTokens(",
    "line": 36
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Having an utility trait or utility singleton object could reduce the overkill, but personally I'd be OK on allowing 5~10 lines of duplication. If we are likely to leverage Scala reflection other than catalyst continuously (HBaseDelegationTokenProvider does it for two times), we could have utility class for that.",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-10-11T05:46:25Z",
    "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import scala.reflect.runtime.universe\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.Credentials\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.{KAFKA_DELEGATION_TOKEN_ENABLED, KAFKA_SECURITY_PROTOCOL}\n+import org.apache.spark.util.Utils\n+\n+private[security] class KafkaDelegationTokenProvider\n+  extends HadoopDelegationTokenProvider with Logging {\n+\n+  override def serviceName: String = \"kafka\"\n+\n+  override def obtainDelegationTokens(",
    "line": 36
  }],
  "prId": 22598
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Don't log tokens at info level; debug is better even if still a little sketchy.\r\n\r\n(Yeah I know the HBase one is doing that, but it shouldn't.)\r\n\r\nYou're also printing debug info in the `TokenUtil` class, which is a better place for it in any case; so no need to log it here.",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-10-04T18:06:45Z",
    "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import scala.reflect.runtime.universe\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.Credentials\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.{KAFKA_DELEGATION_TOKEN_ENABLED, KAFKA_SECURITY_PROTOCOL}\n+import org.apache.spark.util.Utils\n+\n+private[security] class KafkaDelegationTokenProvider\n+  extends HadoopDelegationTokenProvider with Logging {\n+\n+  override def serviceName: String = \"kafka\"\n+\n+  override def obtainDelegationTokens(\n+      hadoopConf: Configuration,\n+      sparkConf: SparkConf,\n+      creds: Credentials): Option[Long] = {\n+    try {\n+      val mirror = universe.runtimeMirror(Utils.getContextOrSparkClassLoader)\n+      val obtainToken = mirror.classLoader.\n+        loadClass(\"org.apache.spark.sql.kafka010.TokenUtil\").\n+        getMethod(\"obtainToken\", classOf[SparkConf])\n+\n+      logDebug(\"Attempting to fetch Kafka security token.\")\n+      val token = obtainToken.invoke(null, sparkConf)\n+        .asInstanceOf[Token[_ <: TokenIdentifier]]\n+      logInfo(s\"Get token from Kafka: ${token.toString}\")"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Fixed.",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-10-08T12:56:45Z",
    "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import scala.reflect.runtime.universe\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.Credentials\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.{KAFKA_DELEGATION_TOKEN_ENABLED, KAFKA_SECURITY_PROTOCOL}\n+import org.apache.spark.util.Utils\n+\n+private[security] class KafkaDelegationTokenProvider\n+  extends HadoopDelegationTokenProvider with Logging {\n+\n+  override def serviceName: String = \"kafka\"\n+\n+  override def obtainDelegationTokens(\n+      hadoopConf: Configuration,\n+      sparkConf: SparkConf,\n+      creds: Credentials): Option[Long] = {\n+    try {\n+      val mirror = universe.runtimeMirror(Utils.getContextOrSparkClassLoader)\n+      val obtainToken = mirror.classLoader.\n+        loadClass(\"org.apache.spark.sql.kafka010.TokenUtil\").\n+        getMethod(\"obtainToken\", classOf[SparkConf])\n+\n+      logDebug(\"Attempting to fetch Kafka security token.\")\n+      val token = obtainToken.invoke(null, sparkConf)\n+        .asInstanceOf[Token[_ <: TokenIdentifier]]\n+      logInfo(s\"Get token from Kafka: ${token.toString}\")"
  }],
  "prId": 22598
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "This seems like it should be at least info, since in this case you're explicitly asking for tokens by setting the config values. (Yes I know this is the same in the HBase provider.)",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-10-04T18:07:45Z",
    "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import scala.reflect.runtime.universe\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.Credentials\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.{KAFKA_DELEGATION_TOKEN_ENABLED, KAFKA_SECURITY_PROTOCOL}\n+import org.apache.spark.util.Utils\n+\n+private[security] class KafkaDelegationTokenProvider\n+  extends HadoopDelegationTokenProvider with Logging {\n+\n+  override def serviceName: String = \"kafka\"\n+\n+  override def obtainDelegationTokens(\n+      hadoopConf: Configuration,\n+      sparkConf: SparkConf,\n+      creds: Credentials): Option[Long] = {\n+    try {\n+      val mirror = universe.runtimeMirror(Utils.getContextOrSparkClassLoader)\n+      val obtainToken = mirror.classLoader.\n+        loadClass(\"org.apache.spark.sql.kafka010.TokenUtil\").\n+        getMethod(\"obtainToken\", classOf[SparkConf])\n+\n+      logDebug(\"Attempting to fetch Kafka security token.\")\n+      val token = obtainToken.invoke(null, sparkConf)\n+        .asInstanceOf[Token[_ <: TokenIdentifier]]\n+      logInfo(s\"Get token from Kafka: ${token.toString}\")\n+      creds.addToken(token.getService, token)\n+    } catch {\n+      case NonFatal(e) =>\n+        logDebug(s\"Failed to get token from service $serviceName\", e)"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Fixed.",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-10-08T12:56:58Z",
    "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import scala.reflect.runtime.universe\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.Credentials\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.{KAFKA_DELEGATION_TOKEN_ENABLED, KAFKA_SECURITY_PROTOCOL}\n+import org.apache.spark.util.Utils\n+\n+private[security] class KafkaDelegationTokenProvider\n+  extends HadoopDelegationTokenProvider with Logging {\n+\n+  override def serviceName: String = \"kafka\"\n+\n+  override def obtainDelegationTokens(\n+      hadoopConf: Configuration,\n+      sparkConf: SparkConf,\n+      creds: Credentials): Option[Long] = {\n+    try {\n+      val mirror = universe.runtimeMirror(Utils.getContextOrSparkClassLoader)\n+      val obtainToken = mirror.classLoader.\n+        loadClass(\"org.apache.spark.sql.kafka010.TokenUtil\").\n+        getMethod(\"obtainToken\", classOf[SparkConf])\n+\n+      logDebug(\"Attempting to fetch Kafka security token.\")\n+      val token = obtainToken.invoke(null, sparkConf)\n+        .asInstanceOf[Token[_ <: TokenIdentifier]]\n+      logInfo(s\"Get token from Kafka: ${token.toString}\")\n+      creds.addToken(token.getService, token)\n+    } catch {\n+      case NonFatal(e) =>\n+        logDebug(s\"Failed to get token from service $serviceName\", e)"
  }],
  "prId": 22598
}, {
  "comments": [{
    "author": {
      "login": "steveloughran"
    },
    "body": "OK: so this asks for DTs even if UGI says the cluster is insecure? \r\n\r\nNothing wrong with that...I've been wondering what would happen if `HadoopFSDelegationTokenProvider` did the same thing: asked filesystems for their tokens even if in an insecure cluster, as it would let DT support in object stores (HADOOP-14556...) work without kerberos.\r\n\r\nI'd test to make sure that everything gets through OK. AFAIK YARN is happy to pass round credentials in an insecure cluster (it get the AM/RM token to the AM this way); its more a matter of making sure the launcher chain is all ready fo it.",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-10-18T09:30:27Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import scala.reflect.runtime.universe\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.Credentials\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.{KAFKA_BOOTSTRAP_SERVERS, KAFKA_SECURITY_PROTOCOL}\n+import org.apache.spark.util.Utils\n+\n+private[security] class KafkaDelegationTokenProvider\n+  extends HadoopDelegationTokenProvider with Logging {\n+\n+  override def serviceName: String = \"kafka\"\n+\n+  override def obtainDelegationTokens(\n+      hadoopConf: Configuration,\n+      sparkConf: SparkConf,\n+      creds: Credentials): Option[Long] = {\n+    try {\n+      val mirror = universe.runtimeMirror(Utils.getContextOrSparkClassLoader)\n+      val obtainToken = mirror.classLoader.\n+        loadClass(\"org.apache.spark.sql.kafka010.TokenUtil\").\n+        getMethod(\"obtainToken\", classOf[SparkConf])\n+\n+      logDebug(\"Attempting to fetch Kafka security token.\")\n+      val token = obtainToken.invoke(null, sparkConf)\n+        .asInstanceOf[Token[_ <: TokenIdentifier]]\n+      creds.addToken(token.getService, token)\n+    } catch {\n+      case NonFatal(e) =>\n+        logInfo(s\"Failed to get token from service $serviceName\", e)\n+    }\n+\n+    None\n+  }\n+\n+  override def delegationTokensRequired(",
    "line": 52
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Are there news on this?",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-11-06T09:04:31Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import scala.reflect.runtime.universe\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.Credentials\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.{KAFKA_BOOTSTRAP_SERVERS, KAFKA_SECURITY_PROTOCOL}\n+import org.apache.spark.util.Utils\n+\n+private[security] class KafkaDelegationTokenProvider\n+  extends HadoopDelegationTokenProvider with Logging {\n+\n+  override def serviceName: String = \"kafka\"\n+\n+  override def obtainDelegationTokens(\n+      hadoopConf: Configuration,\n+      sparkConf: SparkConf,\n+      creds: Credentials): Option[Long] = {\n+    try {\n+      val mirror = universe.runtimeMirror(Utils.getContextOrSparkClassLoader)\n+      val obtainToken = mirror.classLoader.\n+        loadClass(\"org.apache.spark.sql.kafka010.TokenUtil\").\n+        getMethod(\"obtainToken\", classOf[SparkConf])\n+\n+      logDebug(\"Attempting to fetch Kafka security token.\")\n+      val token = obtainToken.invoke(null, sparkConf)\n+        .asInstanceOf[Token[_ <: TokenIdentifier]]\n+      creds.addToken(token.getService, token)\n+    } catch {\n+      case NonFatal(e) =>\n+        logInfo(s\"Failed to get token from service $serviceName\", e)\n+    }\n+\n+    None\n+  }\n+\n+  override def delegationTokensRequired(",
    "line": 52
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "Although in theory we could fix up MR, distp, spark etc to say \"always ask for DTs\", it may just encourage people to run with Kerberos off, which is never something they should be doing. I don't want to do that & am not actively playing with this approach. ",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-11-15T14:46:52Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import scala.reflect.runtime.universe\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.Credentials\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.{KAFKA_BOOTSTRAP_SERVERS, KAFKA_SECURITY_PROTOCOL}\n+import org.apache.spark.util.Utils\n+\n+private[security] class KafkaDelegationTokenProvider\n+  extends HadoopDelegationTokenProvider with Logging {\n+\n+  override def serviceName: String = \"kafka\"\n+\n+  override def obtainDelegationTokens(\n+      hadoopConf: Configuration,\n+      sparkConf: SparkConf,\n+      creds: Credentials): Option[Long] = {\n+    try {\n+      val mirror = universe.runtimeMirror(Utils.getContextOrSparkClassLoader)\n+      val obtainToken = mirror.classLoader.\n+        loadClass(\"org.apache.spark.sql.kafka010.TokenUtil\").\n+        getMethod(\"obtainToken\", classOf[SparkConf])\n+\n+      logDebug(\"Attempting to fetch Kafka security token.\")\n+      val token = obtainToken.invoke(null, sparkConf)\n+        .asInstanceOf[Token[_ <: TokenIdentifier]]\n+      creds.addToken(token.getService, token)\n+    } catch {\n+      case NonFatal(e) =>\n+        logInfo(s\"Failed to get token from service $serviceName\", e)\n+    }\n+\n+    None\n+  }\n+\n+  override def delegationTokensRequired(",
    "line": 52
  }],
  "prId": 22598
}, {
  "comments": [{
    "author": {
      "login": "soenkeliebau"
    },
    "body": "Was this a conscious decision to not use delegation tokens when using ssl authentication? That might also be useful to avoid spreading keys all over the cluster, even if load on the kdc is not an issue in that case.",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-10-25T11:14:12Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import scala.reflect.runtime.universe\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.Credentials\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.{KAFKA_BOOTSTRAP_SERVERS, KAFKA_SECURITY_PROTOCOL}\n+import org.apache.spark.util.Utils\n+\n+private[security] class KafkaDelegationTokenProvider\n+  extends HadoopDelegationTokenProvider with Logging {\n+\n+  override def serviceName: String = \"kafka\"\n+\n+  override def obtainDelegationTokens(\n+      hadoopConf: Configuration,\n+      sparkConf: SparkConf,\n+      creds: Credentials): Option[Long] = {\n+    try {\n+      val mirror = universe.runtimeMirror(Utils.getContextOrSparkClassLoader)\n+      val obtainToken = mirror.classLoader.\n+        loadClass(\"org.apache.spark.sql.kafka010.TokenUtil\").\n+        getMethod(\"obtainToken\", classOf[SparkConf])\n+\n+      logDebug(\"Attempting to fetch Kafka security token.\")\n+      val token = obtainToken.invoke(null, sparkConf)\n+        .asInstanceOf[Token[_ <: TokenIdentifier]]\n+      creds.addToken(token.getService, token)\n+    } catch {\n+      case NonFatal(e) =>\n+        logInfo(s\"Failed to get token from service $serviceName\", e)\n+    }\n+\n+    None\n+  }\n+\n+  override def delegationTokensRequired(\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): Boolean = {\n+    sparkConf.get(KAFKA_BOOTSTRAP_SERVERS).isDefined &&\n+      sparkConf.get(KAFKA_SECURITY_PROTOCOL).startsWith(\"SASL\")"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "This condition means\r\n`if (bootstrap servers configured and (protocol == SASL_PLAINTEXT or protocol == SASL_SSL))`\r\nWhy do you think ssl not covered?\r\n",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-11-05T09:29:26Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import scala.reflect.runtime.universe\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.Credentials\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.{KAFKA_BOOTSTRAP_SERVERS, KAFKA_SECURITY_PROTOCOL}\n+import org.apache.spark.util.Utils\n+\n+private[security] class KafkaDelegationTokenProvider\n+  extends HadoopDelegationTokenProvider with Logging {\n+\n+  override def serviceName: String = \"kafka\"\n+\n+  override def obtainDelegationTokens(\n+      hadoopConf: Configuration,\n+      sparkConf: SparkConf,\n+      creds: Credentials): Option[Long] = {\n+    try {\n+      val mirror = universe.runtimeMirror(Utils.getContextOrSparkClassLoader)\n+      val obtainToken = mirror.classLoader.\n+        loadClass(\"org.apache.spark.sql.kafka010.TokenUtil\").\n+        getMethod(\"obtainToken\", classOf[SparkConf])\n+\n+      logDebug(\"Attempting to fetch Kafka security token.\")\n+      val token = obtainToken.invoke(null, sparkConf)\n+        .asInstanceOf[Token[_ <: TokenIdentifier]]\n+      creds.addToken(token.getService, token)\n+    } catch {\n+      case NonFatal(e) =>\n+        logInfo(s\"Failed to get token from service $serviceName\", e)\n+    }\n+\n+    None\n+  }\n+\n+  override def delegationTokensRequired(\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): Boolean = {\n+    sparkConf.get(KAFKA_BOOTSTRAP_SERVERS).isDefined &&\n+      sparkConf.get(KAFKA_SECURITY_PROTOCOL).startsWith(\"SASL\")"
  }, {
    "author": {
      "login": "soenkeliebau"
    },
    "body": "Looking at the possible protocol [values](https://github.com/apache/kafka/blob/068ab9cefae301f3187ea885d645c425955e77d2/clients/src/main/java/org/apache/kafka/common/security/auth/SecurityProtocol.java#L26) this condition misses the value \"SSL\" unless I am mistaken. If the user presents a client certificate during the SSL handshake for a SSL connection he is allowed to obtain delegation tokens as well. If a client cert was not provided then Kafka will reject the token request, but that can should be covered in error handling I guess.\r\nWould it maybe be an option to move this check into the KafkaTokenUtil class and compare with the actual values of the SecurityProtocol enum instead of relying on hard coded Strings here?",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-11-21T13:54:43Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import scala.reflect.runtime.universe\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.Credentials\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.{KAFKA_BOOTSTRAP_SERVERS, KAFKA_SECURITY_PROTOCOL}\n+import org.apache.spark.util.Utils\n+\n+private[security] class KafkaDelegationTokenProvider\n+  extends HadoopDelegationTokenProvider with Logging {\n+\n+  override def serviceName: String = \"kafka\"\n+\n+  override def obtainDelegationTokens(\n+      hadoopConf: Configuration,\n+      sparkConf: SparkConf,\n+      creds: Credentials): Option[Long] = {\n+    try {\n+      val mirror = universe.runtimeMirror(Utils.getContextOrSparkClassLoader)\n+      val obtainToken = mirror.classLoader.\n+        loadClass(\"org.apache.spark.sql.kafka010.TokenUtil\").\n+        getMethod(\"obtainToken\", classOf[SparkConf])\n+\n+      logDebug(\"Attempting to fetch Kafka security token.\")\n+      val token = obtainToken.invoke(null, sparkConf)\n+        .asInstanceOf[Token[_ <: TokenIdentifier]]\n+      creds.addToken(token.getService, token)\n+    } catch {\n+      case NonFatal(e) =>\n+        logInfo(s\"Failed to get token from service $serviceName\", e)\n+    }\n+\n+    None\n+  }\n+\n+  override def delegationTokensRequired(\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): Boolean = {\n+    sparkConf.get(KAFKA_BOOTSTRAP_SERVERS).isDefined &&\n+      sparkConf.get(KAFKA_SECURITY_PROTOCOL).startsWith(\"SASL\")"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Yeah, SSL 2-way authentication was not covered though it's not that hard to add.\r\nUsing the enum is definitely worth but not found until you've linked here. I've adapted `KafkaDelegationTokenProvider` and `KafkaTokenUtil` to use them.",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-11-22T16:26:23Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import scala.reflect.runtime.universe\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.Credentials\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.{KAFKA_BOOTSTRAP_SERVERS, KAFKA_SECURITY_PROTOCOL}\n+import org.apache.spark.util.Utils\n+\n+private[security] class KafkaDelegationTokenProvider\n+  extends HadoopDelegationTokenProvider with Logging {\n+\n+  override def serviceName: String = \"kafka\"\n+\n+  override def obtainDelegationTokens(\n+      hadoopConf: Configuration,\n+      sparkConf: SparkConf,\n+      creds: Credentials): Option[Long] = {\n+    try {\n+      val mirror = universe.runtimeMirror(Utils.getContextOrSparkClassLoader)\n+      val obtainToken = mirror.classLoader.\n+        loadClass(\"org.apache.spark.sql.kafka010.TokenUtil\").\n+        getMethod(\"obtainToken\", classOf[SparkConf])\n+\n+      logDebug(\"Attempting to fetch Kafka security token.\")\n+      val token = obtainToken.invoke(null, sparkConf)\n+        .asInstanceOf[Token[_ <: TokenIdentifier]]\n+      creds.addToken(token.getService, token)\n+    } catch {\n+      case NonFatal(e) =>\n+        logInfo(s\"Failed to get token from service $serviceName\", e)\n+    }\n+\n+    None\n+  }\n+\n+  override def delegationTokensRequired(\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): Boolean = {\n+    sparkConf.get(KAFKA_BOOTSTRAP_SERVERS).isDefined &&\n+      sparkConf.get(KAFKA_SECURITY_PROTOCOL).startsWith(\"SASL\")"
  }],
  "prId": 22598
}, {
  "comments": [{
    "author": {
      "login": "arunmahadevan"
    },
    "body": "Shouldn't this return the time of the next renewal? Otherwise how does the token manager know when should it be renewed or recreated ?",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-10-25T21:24:43Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import scala.reflect.runtime.universe\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.Credentials\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.{KAFKA_BOOTSTRAP_SERVERS, KAFKA_SECURITY_PROTOCOL}\n+import org.apache.spark.util.Utils\n+\n+private[security] class KafkaDelegationTokenProvider\n+  extends HadoopDelegationTokenProvider with Logging {\n+\n+  override def serviceName: String = \"kafka\"\n+\n+  override def obtainDelegationTokens(\n+      hadoopConf: Configuration,\n+      sparkConf: SparkConf,\n+      creds: Credentials): Option[Long] = {\n+    try {\n+      val mirror = universe.runtimeMirror(Utils.getContextOrSparkClassLoader)\n+      val obtainToken = mirror.classLoader.\n+        loadClass(\"org.apache.spark.sql.kafka010.TokenUtil\").\n+        getMethod(\"obtainToken\", classOf[SparkConf])\n+\n+      logDebug(\"Attempting to fetch Kafka security token.\")\n+      val token = obtainToken.invoke(null, sparkConf)\n+        .asInstanceOf[Token[_ <: TokenIdentifier]]\n+      creds.addToken(token.getService, token)\n+    } catch {\n+      case NonFatal(e) =>\n+        logInfo(s\"Failed to get token from service $serviceName\", e)\n+    }\n+\n+    None",
    "line": 49
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "If Kafka tokens expose that information it should be returned here. (HBase tokens, at least, don't...)",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-10-26T21:09:49Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import scala.reflect.runtime.universe\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.Credentials\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.{KAFKA_BOOTSTRAP_SERVERS, KAFKA_SECURITY_PROTOCOL}\n+import org.apache.spark.util.Utils\n+\n+private[security] class KafkaDelegationTokenProvider\n+  extends HadoopDelegationTokenProvider with Logging {\n+\n+  override def serviceName: String = \"kafka\"\n+\n+  override def obtainDelegationTokens(\n+      hadoopConf: Configuration,\n+      sparkConf: SparkConf,\n+      creds: Credentials): Option[Long] = {\n+    try {\n+      val mirror = universe.runtimeMirror(Utils.getContextOrSparkClassLoader)\n+      val obtainToken = mirror.classLoader.\n+        loadClass(\"org.apache.spark.sql.kafka010.TokenUtil\").\n+        getMethod(\"obtainToken\", classOf[SparkConf])\n+\n+      logDebug(\"Attempting to fetch Kafka security token.\")\n+      val token = obtainToken.invoke(null, sparkConf)\n+        .asInstanceOf[Token[_ <: TokenIdentifier]]\n+      creds.addToken(token.getService, token)\n+    } catch {\n+      case NonFatal(e) =>\n+        logInfo(s\"Failed to get token from service $serviceName\", e)\n+    }\n+\n+    None",
    "line": 49
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "That's a nice catch, fixing it.",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-11-05T09:25:52Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import scala.reflect.runtime.universe\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.Credentials\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.{KAFKA_BOOTSTRAP_SERVERS, KAFKA_SECURITY_PROTOCOL}\n+import org.apache.spark.util.Utils\n+\n+private[security] class KafkaDelegationTokenProvider\n+  extends HadoopDelegationTokenProvider with Logging {\n+\n+  override def serviceName: String = \"kafka\"\n+\n+  override def obtainDelegationTokens(\n+      hadoopConf: Configuration,\n+      sparkConf: SparkConf,\n+      creds: Credentials): Option[Long] = {\n+    try {\n+      val mirror = universe.runtimeMirror(Utils.getContextOrSparkClassLoader)\n+      val obtainToken = mirror.classLoader.\n+        loadClass(\"org.apache.spark.sql.kafka010.TokenUtil\").\n+        getMethod(\"obtainToken\", classOf[SparkConf])\n+\n+      logDebug(\"Attempting to fetch Kafka security token.\")\n+      val token = obtainToken.invoke(null, sparkConf)\n+        .asInstanceOf[Token[_ <: TokenIdentifier]]\n+      creds.addToken(token.getService, token)\n+    } catch {\n+      case NonFatal(e) =>\n+        logInfo(s\"Failed to get token from service $serviceName\", e)\n+    }\n+\n+    None",
    "line": 49
  }],
  "prId": 22598
}, {
  "comments": [{
    "author": {
      "login": "soenkeliebau"
    },
    "body": "Is there a specific reason to dynamically load this class? This code looks quite similar to [HBaseDelegationTokenProvider](https://github.com/apache/spark/blob/5264164a67df498b73facae207eda12ee133be7d/core/src/main/scala/org/apache/spark/deploy/security/HBaseDelegationTokenProvider.scala#L41), but I think there it was done to avoid creating a hard dependency on the HBase libs of which TokenUtil is a part. In this case the (Kafka)TokenUtil is within the Spark project, so we could just use it directly, could we not?",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-10-26T15:45:52Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import scala.reflect.runtime.universe\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.Credentials\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.{KAFKA_BOOTSTRAP_SERVERS, KAFKA_SECURITY_PROTOCOL}\n+import org.apache.spark.util.Utils\n+\n+private[security] class KafkaDelegationTokenProvider\n+  extends HadoopDelegationTokenProvider with Logging {\n+\n+  override def serviceName: String = \"kafka\"\n+\n+  override def obtainDelegationTokens(\n+      hadoopConf: Configuration,\n+      sparkConf: SparkConf,\n+      creds: Credentials): Option[Long] = {\n+    try {\n+      val mirror = universe.runtimeMirror(Utils.getContextOrSparkClassLoader)\n+      val obtainToken = mirror.classLoader.\n+        loadClass(\"org.apache.spark.sql.kafka010.TokenUtil\").\n+        getMethod(\"obtainToken\", classOf[SparkConf])"
  }, {
    "author": {
      "login": "arunmahadevan"
    },
    "body": "spark-core does not have a dependency on spark-sql-kafka so this is needed.",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-10-26T16:07:18Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import scala.reflect.runtime.universe\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.Credentials\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.{KAFKA_BOOTSTRAP_SERVERS, KAFKA_SECURITY_PROTOCOL}\n+import org.apache.spark.util.Utils\n+\n+private[security] class KafkaDelegationTokenProvider\n+  extends HadoopDelegationTokenProvider with Logging {\n+\n+  override def serviceName: String = \"kafka\"\n+\n+  override def obtainDelegationTokens(\n+      hadoopConf: Configuration,\n+      sparkConf: SparkConf,\n+      creds: Credentials): Option[Long] = {\n+    try {\n+      val mirror = universe.runtimeMirror(Utils.getContextOrSparkClassLoader)\n+      val obtainToken = mirror.classLoader.\n+        loadClass(\"org.apache.spark.sql.kafka010.TokenUtil\").\n+        getMethod(\"obtainToken\", classOf[SparkConf])"
  }, {
    "author": {
      "login": "soenkeliebau"
    },
    "body": "You are right, missed that. Sorry",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-10-30T09:48:18Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import scala.reflect.runtime.universe\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.Credentials\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.{KAFKA_BOOTSTRAP_SERVERS, KAFKA_SECURITY_PROTOCOL}\n+import org.apache.spark.util.Utils\n+\n+private[security] class KafkaDelegationTokenProvider\n+  extends HadoopDelegationTokenProvider with Logging {\n+\n+  override def serviceName: String = \"kafka\"\n+\n+  override def obtainDelegationTokens(\n+      hadoopConf: Configuration,\n+      sparkConf: SparkConf,\n+      creds: Credentials): Option[Long] = {\n+    try {\n+      val mirror = universe.runtimeMirror(Utils.getContextOrSparkClassLoader)\n+      val obtainToken = mirror.classLoader.\n+        loadClass(\"org.apache.spark.sql.kafka010.TokenUtil\").\n+        getMethod(\"obtainToken\", classOf[SparkConf])"
  }],
  "prId": 22598
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Another alternative that may be cleaner than this is to actually have this class in the spark-sql-kafka module, and instantiate it in `HadoopDelegationTokenManager` using `Utils.classForName(...).newInstance()`.\r\n\r\nThen this particular code does not need to use reflection at all, and you may be able to clean some code up on the `TokenUtils` side.",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-10-26T21:13:01Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import scala.reflect.runtime.universe\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.Credentials\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.{KAFKA_BOOTSTRAP_SERVERS, KAFKA_SECURITY_PROTOCOL}\n+import org.apache.spark.util.Utils\n+\n+private[security] class KafkaDelegationTokenProvider",
    "line": 31
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Checking it...",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-11-06T08:59:27Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import scala.reflect.runtime.universe\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.Credentials\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.{KAFKA_BOOTSTRAP_SERVERS, KAFKA_SECURITY_PROTOCOL}\n+import org.apache.spark.util.Utils\n+\n+private[security] class KafkaDelegationTokenProvider",
    "line": 31
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "That will make `KafkaDelegationTokenProvider` more clean but has a couple of drawbacks:\r\n * Just moves the reflection into `HadoopDelegationTokenManager`\r\n * Has to be copied when delegation token will be introduced in DStreams\r\n",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-11-06T13:46:15Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import scala.reflect.runtime.universe\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.Credentials\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.{KAFKA_BOOTSTRAP_SERVERS, KAFKA_SECURITY_PROTOCOL}\n+import org.apache.spark.util.Utils\n+\n+private[security] class KafkaDelegationTokenProvider",
    "line": 31
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "> Just moves the reflection into HadoopDelegationTokenManager\r\n\r\nYou can't escape reflection unless you use `java.util.ServiceLoader` here, and it was decided not to use that when this code was first written. My argument is that it is a lot less reflection - it's just `Class.forName` instead of this whole class here.\r\n\r\n> Has to be copied when delegation token will be introduced in DStreams\r\n \r\nAnd how is that different from what you have here? The code you have won't work for dstreams since it's not even available when you pull in just the dstream connector.",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-11-06T19:07:48Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import scala.reflect.runtime.universe\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.Credentials\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.{KAFKA_BOOTSTRAP_SERVERS, KAFKA_SECURITY_PROTOCOL}\n+import org.apache.spark.util.Utils\n+\n+private[security] class KafkaDelegationTokenProvider",
    "line": 31
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "In general if we're not moving this class into kafka area the only change which has to be done is to add another package from where the `TokenUtil` can be loaded. Otherwise `KafkaDelegationTokenProvider` has to be copied from `spark-sql-kafka...` to `spark-streaming-kafka...`. I think leaving as it is makes it more simple.",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-11-07T10:48:45Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import scala.reflect.runtime.universe\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.Credentials\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.{KAFKA_BOOTSTRAP_SERVERS, KAFKA_SECURITY_PROTOCOL}\n+import org.apache.spark.util.Utils\n+\n+private[security] class KafkaDelegationTokenProvider",
    "line": 31
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "> add another package from where the TokenUtil can be loaded.\r\n\r\nSeems like what you're saying is that to support dstreams you'd have two pretty much identical TokenUtil classes in different modules? That sounds sub-optimal. At that point, it might be better to move the logic in that class to core, and just go full-reflection, or even add kafka as a `provided` dependency and mimic the Hive provider.",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-11-07T16:58:59Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import scala.reflect.runtime.universe\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.Credentials\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.{KAFKA_BOOTSTRAP_SERVERS, KAFKA_SECURITY_PROTOCOL}\n+import org.apache.spark.util.Utils\n+\n+private[security] class KafkaDelegationTokenProvider",
    "line": 31
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Using full-reflection would make the code hell complicated but your other suggestion about provided scope is really good. This way the copy-paste can be avoided. Changed accordingly.",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-11-08T19:35:24Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import scala.reflect.runtime.universe\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.Credentials\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.{KAFKA_BOOTSTRAP_SERVERS, KAFKA_SECURITY_PROTOCOL}\n+import org.apache.spark.util.Utils\n+\n+private[security] class KafkaDelegationTokenProvider",
    "line": 31
  }],
  "prId": 22598
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`sparkConf.contains(KAFKA_BOOTSTRAP_SERVERS)`",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-11-14T21:31:55Z",
    "diffHunk": "@@ -0,0 +1,57 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import scala.language.existentials\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.Credentials\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.{KAFKA_BOOTSTRAP_SERVERS, KAFKA_SECURITY_PROTOCOL}\n+\n+private[security] class KafkaDelegationTokenProvider\n+  extends HadoopDelegationTokenProvider with Logging {\n+\n+  override def serviceName: String = \"kafka\"\n+\n+  override def obtainDelegationTokens(\n+      hadoopConf: Configuration,\n+      sparkConf: SparkConf,\n+      creds: Credentials): Option[Long] = {\n+    try {\n+      logDebug(\"Attempting to fetch Kafka security token.\")\n+      val (token, nextRenewalDate) = KafkaTokenUtil.obtainToken(sparkConf)\n+      creds.addToken(token.getService, token)\n+      return Some(nextRenewalDate)\n+    } catch {\n+      case NonFatal(e) =>\n+        logInfo(s\"Failed to get token from service $serviceName\", e)\n+    }\n+    None\n+  }\n+\n+  override def delegationTokensRequired(\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): Boolean = {\n+    sparkConf.get(KAFKA_BOOTSTRAP_SERVERS).isDefined &&"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Fixed.",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-11-15T14:20:14Z",
    "diffHunk": "@@ -0,0 +1,57 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import scala.language.existentials\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.Credentials\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.{KAFKA_BOOTSTRAP_SERVERS, KAFKA_SECURITY_PROTOCOL}\n+\n+private[security] class KafkaDelegationTokenProvider\n+  extends HadoopDelegationTokenProvider with Logging {\n+\n+  override def serviceName: String = \"kafka\"\n+\n+  override def obtainDelegationTokens(\n+      hadoopConf: Configuration,\n+      sparkConf: SparkConf,\n+      creds: Credentials): Option[Long] = {\n+    try {\n+      logDebug(\"Attempting to fetch Kafka security token.\")\n+      val (token, nextRenewalDate) = KafkaTokenUtil.obtainToken(sparkConf)\n+      creds.addToken(token.getService, token)\n+      return Some(nextRenewalDate)\n+    } catch {\n+      case NonFatal(e) =>\n+        logInfo(s\"Failed to get token from service $serviceName\", e)\n+    }\n+    None\n+  }\n+\n+  override def delegationTokensRequired(\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): Boolean = {\n+    sparkConf.get(KAFKA_BOOTSTRAP_SERVERS).isDefined &&"
  }],
  "prId": 22598
}]