[{
  "comments": [{
    "author": {
      "login": "Ngone51"
    },
    "body": "This kind of misleading when I see it at first time. Maybe, just given `_`  instead of `logDirForAppPath` ?",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-25T09:03:01Z",
    "diffHunk": "@@ -0,0 +1,420 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io._\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[history] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path): (Option[FSDataOutputStream], OutputStream) = {\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    var hadoopDataStream: Option[FSDataOutputStream] = None\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(\n+          SparkHadoopUtil.createFile(fileSystem, path, sparkConf.get(EVENT_LOG_ALLOW_EC)))\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+      (hadoopDataStream, bstream)\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it, -and which support setTimes(); it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  /** initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def apply(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  override val logPath: String = SingleEventLogFileWriter.getLogPath(logBaseDir, appId,\n+    appAttemptId, compressionCodecName)\n+\n+  private val inProgressPath = logPath + EventLogFileWriter.IN_PROGRESS\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+\n+  private var writer: Option[PrintWriter] = None\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    val (hadoopStream, outputStream) = initLogFile(new Path(inProgressPath))\n+    hadoopDataStream = hadoopStream\n+    writer = Some(new PrintWriter(new OutputStreamWriter(outputStream, StandardCharsets.UTF_8)))\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    // scalastyle:off println\n+    writer.foreach(_.println(eventJson))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  /**\n+   * Stop logging events. The event log file will be renamed so that it loses the\n+   * \".inprogress\" suffix.\n+   */\n+  override def stop(): Unit = {\n+    writer.foreach(_.close())\n+    renameFile(new Path(inProgressPath), new Path(logPath), shouldOverwrite)\n+  }\n+}\n+\n+object SingleEventLogFileWriter {\n+  /**\n+   * Return a file-system-safe path to the log file for the given application.\n+   *\n+   * Note that because we currently only create a single log file for each application,\n+   * we must encode all the information needed to parse this event log in the file name\n+   * instead of within the file itself. Otherwise, if the file is compressed, for instance,\n+   * we won't know which codec to use to decompress the metadata needed to open the file in\n+   * the first place.\n+   *\n+   * The log file name will identify the compression codec used for the contents, if any.\n+   * For example, app_123 for an uncompressed log, app_123.lzf for an LZF-compressed log.\n+   *\n+   * @param logBaseDir Directory where the log file will be written.\n+   * @param appId A unique app ID.\n+   * @param appAttemptId A unique attempt id of appId. May be the empty string.\n+   * @param compressionCodecName Name to identify the codec used to compress the contents\n+   *                             of the log, or None if compression is not enabled.\n+   * @return A path which consists of file-system-safe characters.\n+   */\n+  def getLogPath(\n+      logBaseDir: URI,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      compressionCodecName: Option[String] = None): String = {\n+    val codec = compressionCodecName.map(\".\" + _).getOrElse(\"\")\n+    new Path(logBaseDir).toString.stripSuffix(\"/\") + \"/\" +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId) + codec\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into multiple log files, rolled over via configured size.\n+ *\n+ * The class creates one directory per application, and stores event log files as well as\n+ * metadata files. The name of directory and files in the directory would follow:\n+ *\n+ * - The name of directory: eventlog_v2_appId(_[appAttemptId])\n+ * - The prefix of name on event files: events_[index]_[appId](_[appAttemptId])(.[codec])\n+ *   - \"index\" would be monotonically increasing value (say, sequence)\n+ * - The name of metadata (app. status) file name: appstatus_[appId](_[appAttemptId])(.inprogress)\n+ *\n+ * The writer will roll over the event log file when configured size is reached. Note that the\n+ * writer doesn't check the size on file being open for write: the writer tracks the count of bytes\n+ * written before compression is applied.\n+ *\n+ * For metadata files, the class will leverage zero-byte file, as it provides minimized cost.\n+ */\n+class RollingEventLogFilesWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  import RollingEventLogFilesWriter._\n+\n+  private val eventFileMaxLengthKiB = sparkConf.get(EVENT_LOG_ROLLED_EVENT_LOG_MAX_FILE_SIZE)\n+\n+  private val logDirForAppPath = getAppEventLogDirPath(logBaseDir, appId, appAttemptId)\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+  private var countingOutputStream: Option[CountingOutputStream] = None\n+  private var writer: Option[PrintWriter] = None\n+\n+  // seq and event log path will be updated soon in rollEventLogFile, which `start` will call\n+  private var index: Long = 0L\n+  private var currentEventLogFilePath: Path = logDirForAppPath"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "Ngone51"
    },
    "body": "Maybe, we could extract this part into abstract `EventLogFilesWriter`'s `writeEvent()` and call `super.writeEvent` here ? And then,  `SingleEventLogFileWriter` doesn't need to implement its own `writeEvent()`, either.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-25T09:07:01Z",
    "diffHunk": "@@ -0,0 +1,420 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io._\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[history] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path): (Option[FSDataOutputStream], OutputStream) = {\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    var hadoopDataStream: Option[FSDataOutputStream] = None\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(\n+          SparkHadoopUtil.createFile(fileSystem, path, sparkConf.get(EVENT_LOG_ALLOW_EC)))\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+      (hadoopDataStream, bstream)\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it, -and which support setTimes(); it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  /** initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def apply(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  override val logPath: String = SingleEventLogFileWriter.getLogPath(logBaseDir, appId,\n+    appAttemptId, compressionCodecName)\n+\n+  private val inProgressPath = logPath + EventLogFileWriter.IN_PROGRESS\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+\n+  private var writer: Option[PrintWriter] = None\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    val (hadoopStream, outputStream) = initLogFile(new Path(inProgressPath))\n+    hadoopDataStream = hadoopStream\n+    writer = Some(new PrintWriter(new OutputStreamWriter(outputStream, StandardCharsets.UTF_8)))\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    // scalastyle:off println\n+    writer.foreach(_.println(eventJson))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  /**\n+   * Stop logging events. The event log file will be renamed so that it loses the\n+   * \".inprogress\" suffix.\n+   */\n+  override def stop(): Unit = {\n+    writer.foreach(_.close())\n+    renameFile(new Path(inProgressPath), new Path(logPath), shouldOverwrite)\n+  }\n+}\n+\n+object SingleEventLogFileWriter {\n+  /**\n+   * Return a file-system-safe path to the log file for the given application.\n+   *\n+   * Note that because we currently only create a single log file for each application,\n+   * we must encode all the information needed to parse this event log in the file name\n+   * instead of within the file itself. Otherwise, if the file is compressed, for instance,\n+   * we won't know which codec to use to decompress the metadata needed to open the file in\n+   * the first place.\n+   *\n+   * The log file name will identify the compression codec used for the contents, if any.\n+   * For example, app_123 for an uncompressed log, app_123.lzf for an LZF-compressed log.\n+   *\n+   * @param logBaseDir Directory where the log file will be written.\n+   * @param appId A unique app ID.\n+   * @param appAttemptId A unique attempt id of appId. May be the empty string.\n+   * @param compressionCodecName Name to identify the codec used to compress the contents\n+   *                             of the log, or None if compression is not enabled.\n+   * @return A path which consists of file-system-safe characters.\n+   */\n+  def getLogPath(\n+      logBaseDir: URI,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      compressionCodecName: Option[String] = None): String = {\n+    val codec = compressionCodecName.map(\".\" + _).getOrElse(\"\")\n+    new Path(logBaseDir).toString.stripSuffix(\"/\") + \"/\" +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId) + codec\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into multiple log files, rolled over via configured size.\n+ *\n+ * The class creates one directory per application, and stores event log files as well as\n+ * metadata files. The name of directory and files in the directory would follow:\n+ *\n+ * - The name of directory: eventlog_v2_appId(_[appAttemptId])\n+ * - The prefix of name on event files: events_[index]_[appId](_[appAttemptId])(.[codec])\n+ *   - \"index\" would be monotonically increasing value (say, sequence)\n+ * - The name of metadata (app. status) file name: appstatus_[appId](_[appAttemptId])(.inprogress)\n+ *\n+ * The writer will roll over the event log file when configured size is reached. Note that the\n+ * writer doesn't check the size on file being open for write: the writer tracks the count of bytes\n+ * written before compression is applied.\n+ *\n+ * For metadata files, the class will leverage zero-byte file, as it provides minimized cost.\n+ */\n+class RollingEventLogFilesWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  import RollingEventLogFilesWriter._\n+\n+  private val eventFileMaxLengthKiB = sparkConf.get(EVENT_LOG_ROLLED_EVENT_LOG_MAX_FILE_SIZE)\n+\n+  private val logDirForAppPath = getAppEventLogDirPath(logBaseDir, appId, appAttemptId)\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+  private var countingOutputStream: Option[CountingOutputStream] = None\n+  private var writer: Option[PrintWriter] = None\n+\n+  // seq and event log path will be updated soon in rollEventLogFile, which `start` will call\n+  private var index: Long = 0L\n+  private var currentEventLogFilePath: Path = logDirForAppPath\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    if (fileSystem.exists(logDirForAppPath) && shouldOverwrite) {\n+      fileSystem.delete(logDirForAppPath, true)\n+    }\n+\n+    if (fileSystem.exists(logDirForAppPath)) {\n+      throw new IOException(s\"Target log directory already exists ($logDirForAppPath)\")\n+    }\n+\n+    fileSystem.mkdirs(logDirForAppPath, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+    createAppStatusFile(inProgress = true)\n+    rollEventLogFile()\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    writer.foreach { w =>\n+      val currentLen = countingOutputStream.get.getBytesWritten\n+      if (currentLen + eventJson.length > eventFileMaxLengthKiB * 1024) {\n+        rollEventLogFile()\n+      }\n+    }\n+\n+    // scalastyle:off println\n+    writer.foreach(_.println(eventJson))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "That's good idea! Though I just would like to leave abstract method as it is, as calling `super.xxx()` is easy to be missed when you override the method. Extracted into separate method and let implementations call it.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-25T12:47:58Z",
    "diffHunk": "@@ -0,0 +1,420 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io._\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[history] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path): (Option[FSDataOutputStream], OutputStream) = {\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    var hadoopDataStream: Option[FSDataOutputStream] = None\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(\n+          SparkHadoopUtil.createFile(fileSystem, path, sparkConf.get(EVENT_LOG_ALLOW_EC)))\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+      (hadoopDataStream, bstream)\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it, -and which support setTimes(); it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  /** initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def apply(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  override val logPath: String = SingleEventLogFileWriter.getLogPath(logBaseDir, appId,\n+    appAttemptId, compressionCodecName)\n+\n+  private val inProgressPath = logPath + EventLogFileWriter.IN_PROGRESS\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+\n+  private var writer: Option[PrintWriter] = None\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    val (hadoopStream, outputStream) = initLogFile(new Path(inProgressPath))\n+    hadoopDataStream = hadoopStream\n+    writer = Some(new PrintWriter(new OutputStreamWriter(outputStream, StandardCharsets.UTF_8)))\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    // scalastyle:off println\n+    writer.foreach(_.println(eventJson))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  /**\n+   * Stop logging events. The event log file will be renamed so that it loses the\n+   * \".inprogress\" suffix.\n+   */\n+  override def stop(): Unit = {\n+    writer.foreach(_.close())\n+    renameFile(new Path(inProgressPath), new Path(logPath), shouldOverwrite)\n+  }\n+}\n+\n+object SingleEventLogFileWriter {\n+  /**\n+   * Return a file-system-safe path to the log file for the given application.\n+   *\n+   * Note that because we currently only create a single log file for each application,\n+   * we must encode all the information needed to parse this event log in the file name\n+   * instead of within the file itself. Otherwise, if the file is compressed, for instance,\n+   * we won't know which codec to use to decompress the metadata needed to open the file in\n+   * the first place.\n+   *\n+   * The log file name will identify the compression codec used for the contents, if any.\n+   * For example, app_123 for an uncompressed log, app_123.lzf for an LZF-compressed log.\n+   *\n+   * @param logBaseDir Directory where the log file will be written.\n+   * @param appId A unique app ID.\n+   * @param appAttemptId A unique attempt id of appId. May be the empty string.\n+   * @param compressionCodecName Name to identify the codec used to compress the contents\n+   *                             of the log, or None if compression is not enabled.\n+   * @return A path which consists of file-system-safe characters.\n+   */\n+  def getLogPath(\n+      logBaseDir: URI,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      compressionCodecName: Option[String] = None): String = {\n+    val codec = compressionCodecName.map(\".\" + _).getOrElse(\"\")\n+    new Path(logBaseDir).toString.stripSuffix(\"/\") + \"/\" +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId) + codec\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into multiple log files, rolled over via configured size.\n+ *\n+ * The class creates one directory per application, and stores event log files as well as\n+ * metadata files. The name of directory and files in the directory would follow:\n+ *\n+ * - The name of directory: eventlog_v2_appId(_[appAttemptId])\n+ * - The prefix of name on event files: events_[index]_[appId](_[appAttemptId])(.[codec])\n+ *   - \"index\" would be monotonically increasing value (say, sequence)\n+ * - The name of metadata (app. status) file name: appstatus_[appId](_[appAttemptId])(.inprogress)\n+ *\n+ * The writer will roll over the event log file when configured size is reached. Note that the\n+ * writer doesn't check the size on file being open for write: the writer tracks the count of bytes\n+ * written before compression is applied.\n+ *\n+ * For metadata files, the class will leverage zero-byte file, as it provides minimized cost.\n+ */\n+class RollingEventLogFilesWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  import RollingEventLogFilesWriter._\n+\n+  private val eventFileMaxLengthKiB = sparkConf.get(EVENT_LOG_ROLLED_EVENT_LOG_MAX_FILE_SIZE)\n+\n+  private val logDirForAppPath = getAppEventLogDirPath(logBaseDir, appId, appAttemptId)\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+  private var countingOutputStream: Option[CountingOutputStream] = None\n+  private var writer: Option[PrintWriter] = None\n+\n+  // seq and event log path will be updated soon in rollEventLogFile, which `start` will call\n+  private var index: Long = 0L\n+  private var currentEventLogFilePath: Path = logDirForAppPath\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    if (fileSystem.exists(logDirForAppPath) && shouldOverwrite) {\n+      fileSystem.delete(logDirForAppPath, true)\n+    }\n+\n+    if (fileSystem.exists(logDirForAppPath)) {\n+      throw new IOException(s\"Target log directory already exists ($logDirForAppPath)\")\n+    }\n+\n+    fileSystem.mkdirs(logDirForAppPath, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+    createAppStatusFile(inProgress = true)\n+    rollEventLogFile()\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    writer.foreach { w =>\n+      val currentLen = countingOutputStream.get.getBytesWritten\n+      if (currentLen + eventJson.length > eventFileMaxLengthKiB * 1024) {\n+        rollEventLogFile()\n+      }\n+    }\n+\n+    // scalastyle:off println\n+    writer.foreach(_.println(eventJson))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }"
  }, {
    "author": {
      "login": "Ngone51"
    },
    "body": "make sense.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-26T12:52:30Z",
    "diffHunk": "@@ -0,0 +1,420 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io._\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[history] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path): (Option[FSDataOutputStream], OutputStream) = {\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    var hadoopDataStream: Option[FSDataOutputStream] = None\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(\n+          SparkHadoopUtil.createFile(fileSystem, path, sparkConf.get(EVENT_LOG_ALLOW_EC)))\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+      (hadoopDataStream, bstream)\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it, -and which support setTimes(); it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  /** initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def apply(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  override val logPath: String = SingleEventLogFileWriter.getLogPath(logBaseDir, appId,\n+    appAttemptId, compressionCodecName)\n+\n+  private val inProgressPath = logPath + EventLogFileWriter.IN_PROGRESS\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+\n+  private var writer: Option[PrintWriter] = None\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    val (hadoopStream, outputStream) = initLogFile(new Path(inProgressPath))\n+    hadoopDataStream = hadoopStream\n+    writer = Some(new PrintWriter(new OutputStreamWriter(outputStream, StandardCharsets.UTF_8)))\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    // scalastyle:off println\n+    writer.foreach(_.println(eventJson))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  /**\n+   * Stop logging events. The event log file will be renamed so that it loses the\n+   * \".inprogress\" suffix.\n+   */\n+  override def stop(): Unit = {\n+    writer.foreach(_.close())\n+    renameFile(new Path(inProgressPath), new Path(logPath), shouldOverwrite)\n+  }\n+}\n+\n+object SingleEventLogFileWriter {\n+  /**\n+   * Return a file-system-safe path to the log file for the given application.\n+   *\n+   * Note that because we currently only create a single log file for each application,\n+   * we must encode all the information needed to parse this event log in the file name\n+   * instead of within the file itself. Otherwise, if the file is compressed, for instance,\n+   * we won't know which codec to use to decompress the metadata needed to open the file in\n+   * the first place.\n+   *\n+   * The log file name will identify the compression codec used for the contents, if any.\n+   * For example, app_123 for an uncompressed log, app_123.lzf for an LZF-compressed log.\n+   *\n+   * @param logBaseDir Directory where the log file will be written.\n+   * @param appId A unique app ID.\n+   * @param appAttemptId A unique attempt id of appId. May be the empty string.\n+   * @param compressionCodecName Name to identify the codec used to compress the contents\n+   *                             of the log, or None if compression is not enabled.\n+   * @return A path which consists of file-system-safe characters.\n+   */\n+  def getLogPath(\n+      logBaseDir: URI,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      compressionCodecName: Option[String] = None): String = {\n+    val codec = compressionCodecName.map(\".\" + _).getOrElse(\"\")\n+    new Path(logBaseDir).toString.stripSuffix(\"/\") + \"/\" +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId) + codec\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into multiple log files, rolled over via configured size.\n+ *\n+ * The class creates one directory per application, and stores event log files as well as\n+ * metadata files. The name of directory and files in the directory would follow:\n+ *\n+ * - The name of directory: eventlog_v2_appId(_[appAttemptId])\n+ * - The prefix of name on event files: events_[index]_[appId](_[appAttemptId])(.[codec])\n+ *   - \"index\" would be monotonically increasing value (say, sequence)\n+ * - The name of metadata (app. status) file name: appstatus_[appId](_[appAttemptId])(.inprogress)\n+ *\n+ * The writer will roll over the event log file when configured size is reached. Note that the\n+ * writer doesn't check the size on file being open for write: the writer tracks the count of bytes\n+ * written before compression is applied.\n+ *\n+ * For metadata files, the class will leverage zero-byte file, as it provides minimized cost.\n+ */\n+class RollingEventLogFilesWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  import RollingEventLogFilesWriter._\n+\n+  private val eventFileMaxLengthKiB = sparkConf.get(EVENT_LOG_ROLLED_EVENT_LOG_MAX_FILE_SIZE)\n+\n+  private val logDirForAppPath = getAppEventLogDirPath(logBaseDir, appId, appAttemptId)\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+  private var countingOutputStream: Option[CountingOutputStream] = None\n+  private var writer: Option[PrintWriter] = None\n+\n+  // seq and event log path will be updated soon in rollEventLogFile, which `start` will call\n+  private var index: Long = 0L\n+  private var currentEventLogFilePath: Path = logDirForAppPath\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    if (fileSystem.exists(logDirForAppPath) && shouldOverwrite) {\n+      fileSystem.delete(logDirForAppPath, true)\n+    }\n+\n+    if (fileSystem.exists(logDirForAppPath)) {\n+      throw new IOException(s\"Target log directory already exists ($logDirForAppPath)\")\n+    }\n+\n+    fileSystem.mkdirs(logDirForAppPath, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+    createAppStatusFile(inProgress = true)\n+    rollEventLogFile()\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    writer.foreach { w =>\n+      val currentLen = countingOutputStream.get.getBytesWritten\n+      if (currentLen + eventJson.length > eventFileMaxLengthKiB * 1024) {\n+        rollEventLogFile()\n+      }\n+    }\n+\n+    // scalastyle:off println\n+    writer.foreach(_.println(eventJson))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "Ngone51"
    },
    "body": "I'm feeling the same that `hadoopDataStream = hadoopStream` could move into `initLogFile()` and `hadoopDataStream` could also move into abstract `EventLogFilesWriter`, in order to reduce duplicate code.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-25T09:09:46Z",
    "diffHunk": "@@ -0,0 +1,420 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io._\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[history] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path): (Option[FSDataOutputStream], OutputStream) = {\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    var hadoopDataStream: Option[FSDataOutputStream] = None\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(\n+          SparkHadoopUtil.createFile(fileSystem, path, sparkConf.get(EVENT_LOG_ALLOW_EC)))\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+      (hadoopDataStream, bstream)\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it, -and which support setTimes(); it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  /** initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def apply(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  override val logPath: String = SingleEventLogFileWriter.getLogPath(logBaseDir, appId,\n+    appAttemptId, compressionCodecName)\n+\n+  private val inProgressPath = logPath + EventLogFileWriter.IN_PROGRESS\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+\n+  private var writer: Option[PrintWriter] = None\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    val (hadoopStream, outputStream) = initLogFile(new Path(inProgressPath))\n+    hadoopDataStream = hadoopStream\n+    writer = Some(new PrintWriter(new OutputStreamWriter(outputStream, StandardCharsets.UTF_8)))\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    // scalastyle:off println\n+    writer.foreach(_.println(eventJson))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  /**\n+   * Stop logging events. The event log file will be renamed so that it loses the\n+   * \".inprogress\" suffix.\n+   */\n+  override def stop(): Unit = {\n+    writer.foreach(_.close())\n+    renameFile(new Path(inProgressPath), new Path(logPath), shouldOverwrite)\n+  }\n+}\n+\n+object SingleEventLogFileWriter {\n+  /**\n+   * Return a file-system-safe path to the log file for the given application.\n+   *\n+   * Note that because we currently only create a single log file for each application,\n+   * we must encode all the information needed to parse this event log in the file name\n+   * instead of within the file itself. Otherwise, if the file is compressed, for instance,\n+   * we won't know which codec to use to decompress the metadata needed to open the file in\n+   * the first place.\n+   *\n+   * The log file name will identify the compression codec used for the contents, if any.\n+   * For example, app_123 for an uncompressed log, app_123.lzf for an LZF-compressed log.\n+   *\n+   * @param logBaseDir Directory where the log file will be written.\n+   * @param appId A unique app ID.\n+   * @param appAttemptId A unique attempt id of appId. May be the empty string.\n+   * @param compressionCodecName Name to identify the codec used to compress the contents\n+   *                             of the log, or None if compression is not enabled.\n+   * @return A path which consists of file-system-safe characters.\n+   */\n+  def getLogPath(\n+      logBaseDir: URI,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      compressionCodecName: Option[String] = None): String = {\n+    val codec = compressionCodecName.map(\".\" + _).getOrElse(\"\")\n+    new Path(logBaseDir).toString.stripSuffix(\"/\") + \"/\" +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId) + codec\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into multiple log files, rolled over via configured size.\n+ *\n+ * The class creates one directory per application, and stores event log files as well as\n+ * metadata files. The name of directory and files in the directory would follow:\n+ *\n+ * - The name of directory: eventlog_v2_appId(_[appAttemptId])\n+ * - The prefix of name on event files: events_[index]_[appId](_[appAttemptId])(.[codec])\n+ *   - \"index\" would be monotonically increasing value (say, sequence)\n+ * - The name of metadata (app. status) file name: appstatus_[appId](_[appAttemptId])(.inprogress)\n+ *\n+ * The writer will roll over the event log file when configured size is reached. Note that the\n+ * writer doesn't check the size on file being open for write: the writer tracks the count of bytes\n+ * written before compression is applied.\n+ *\n+ * For metadata files, the class will leverage zero-byte file, as it provides minimized cost.\n+ */\n+class RollingEventLogFilesWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  import RollingEventLogFilesWriter._\n+\n+  private val eventFileMaxLengthKiB = sparkConf.get(EVENT_LOG_ROLLED_EVENT_LOG_MAX_FILE_SIZE)\n+\n+  private val logDirForAppPath = getAppEventLogDirPath(logBaseDir, appId, appAttemptId)\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+  private var countingOutputStream: Option[CountingOutputStream] = None\n+  private var writer: Option[PrintWriter] = None\n+\n+  // seq and event log path will be updated soon in rollEventLogFile, which `start` will call\n+  private var index: Long = 0L\n+  private var currentEventLogFilePath: Path = logDirForAppPath\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    if (fileSystem.exists(logDirForAppPath) && shouldOverwrite) {\n+      fileSystem.delete(logDirForAppPath, true)\n+    }\n+\n+    if (fileSystem.exists(logDirForAppPath)) {\n+      throw new IOException(s\"Target log directory already exists ($logDirForAppPath)\")\n+    }\n+\n+    fileSystem.mkdirs(logDirForAppPath, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+    createAppStatusFile(inProgress = true)\n+    rollEventLogFile()\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    writer.foreach { w =>\n+      val currentLen = countingOutputStream.get.getBytesWritten\n+      if (currentLen + eventJson.length > eventFileMaxLengthKiB * 1024) {\n+        rollEventLogFile()\n+      }\n+    }\n+\n+    // scalastyle:off println\n+    writer.foreach(_.println(eventJson))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  private def rollEventLogFile(): Unit = {\n+    writer.foreach(_.close())\n+\n+    index += 1\n+    currentEventLogFilePath = getEventLogFilePath(logDirForAppPath, appId, appAttemptId, index,\n+      compressionCodecName)\n+\n+    val (hadoopStream, outputStream) = initLogFile(currentEventLogFilePath)\n+    hadoopDataStream = hadoopStream"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Nice idea! Addressed.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-25T12:48:16Z",
    "diffHunk": "@@ -0,0 +1,420 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io._\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[history] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path): (Option[FSDataOutputStream], OutputStream) = {\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    var hadoopDataStream: Option[FSDataOutputStream] = None\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(\n+          SparkHadoopUtil.createFile(fileSystem, path, sparkConf.get(EVENT_LOG_ALLOW_EC)))\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+      (hadoopDataStream, bstream)\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it, -and which support setTimes(); it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  /** initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def apply(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  override val logPath: String = SingleEventLogFileWriter.getLogPath(logBaseDir, appId,\n+    appAttemptId, compressionCodecName)\n+\n+  private val inProgressPath = logPath + EventLogFileWriter.IN_PROGRESS\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+\n+  private var writer: Option[PrintWriter] = None\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    val (hadoopStream, outputStream) = initLogFile(new Path(inProgressPath))\n+    hadoopDataStream = hadoopStream\n+    writer = Some(new PrintWriter(new OutputStreamWriter(outputStream, StandardCharsets.UTF_8)))\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    // scalastyle:off println\n+    writer.foreach(_.println(eventJson))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  /**\n+   * Stop logging events. The event log file will be renamed so that it loses the\n+   * \".inprogress\" suffix.\n+   */\n+  override def stop(): Unit = {\n+    writer.foreach(_.close())\n+    renameFile(new Path(inProgressPath), new Path(logPath), shouldOverwrite)\n+  }\n+}\n+\n+object SingleEventLogFileWriter {\n+  /**\n+   * Return a file-system-safe path to the log file for the given application.\n+   *\n+   * Note that because we currently only create a single log file for each application,\n+   * we must encode all the information needed to parse this event log in the file name\n+   * instead of within the file itself. Otherwise, if the file is compressed, for instance,\n+   * we won't know which codec to use to decompress the metadata needed to open the file in\n+   * the first place.\n+   *\n+   * The log file name will identify the compression codec used for the contents, if any.\n+   * For example, app_123 for an uncompressed log, app_123.lzf for an LZF-compressed log.\n+   *\n+   * @param logBaseDir Directory where the log file will be written.\n+   * @param appId A unique app ID.\n+   * @param appAttemptId A unique attempt id of appId. May be the empty string.\n+   * @param compressionCodecName Name to identify the codec used to compress the contents\n+   *                             of the log, or None if compression is not enabled.\n+   * @return A path which consists of file-system-safe characters.\n+   */\n+  def getLogPath(\n+      logBaseDir: URI,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      compressionCodecName: Option[String] = None): String = {\n+    val codec = compressionCodecName.map(\".\" + _).getOrElse(\"\")\n+    new Path(logBaseDir).toString.stripSuffix(\"/\") + \"/\" +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId) + codec\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into multiple log files, rolled over via configured size.\n+ *\n+ * The class creates one directory per application, and stores event log files as well as\n+ * metadata files. The name of directory and files in the directory would follow:\n+ *\n+ * - The name of directory: eventlog_v2_appId(_[appAttemptId])\n+ * - The prefix of name on event files: events_[index]_[appId](_[appAttemptId])(.[codec])\n+ *   - \"index\" would be monotonically increasing value (say, sequence)\n+ * - The name of metadata (app. status) file name: appstatus_[appId](_[appAttemptId])(.inprogress)\n+ *\n+ * The writer will roll over the event log file when configured size is reached. Note that the\n+ * writer doesn't check the size on file being open for write: the writer tracks the count of bytes\n+ * written before compression is applied.\n+ *\n+ * For metadata files, the class will leverage zero-byte file, as it provides minimized cost.\n+ */\n+class RollingEventLogFilesWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  import RollingEventLogFilesWriter._\n+\n+  private val eventFileMaxLengthKiB = sparkConf.get(EVENT_LOG_ROLLED_EVENT_LOG_MAX_FILE_SIZE)\n+\n+  private val logDirForAppPath = getAppEventLogDirPath(logBaseDir, appId, appAttemptId)\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+  private var countingOutputStream: Option[CountingOutputStream] = None\n+  private var writer: Option[PrintWriter] = None\n+\n+  // seq and event log path will be updated soon in rollEventLogFile, which `start` will call\n+  private var index: Long = 0L\n+  private var currentEventLogFilePath: Path = logDirForAppPath\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    if (fileSystem.exists(logDirForAppPath) && shouldOverwrite) {\n+      fileSystem.delete(logDirForAppPath, true)\n+    }\n+\n+    if (fileSystem.exists(logDirForAppPath)) {\n+      throw new IOException(s\"Target log directory already exists ($logDirForAppPath)\")\n+    }\n+\n+    fileSystem.mkdirs(logDirForAppPath, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+    createAppStatusFile(inProgress = true)\n+    rollEventLogFile()\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    writer.foreach { w =>\n+      val currentLen = countingOutputStream.get.getBytesWritten\n+      if (currentLen + eventJson.length > eventFileMaxLengthKiB * 1024) {\n+        rollEventLogFile()\n+      }\n+    }\n+\n+    // scalastyle:off println\n+    writer.foreach(_.println(eventJson))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  private def rollEventLogFile(): Unit = {\n+    writer.foreach(_.close())\n+\n+    index += 1\n+    currentEventLogFilePath = getEventLogFilePath(logDirForAppPath, appId, appAttemptId, index,\n+      compressionCodecName)\n+\n+    val (hadoopStream, outputStream) = initLogFile(currentEventLogFilePath)\n+    hadoopDataStream = hadoopStream"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "Ngone51"
    },
    "body": "Using constant variable for such prefix ?",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-25T09:26:46Z",
    "diffHunk": "@@ -0,0 +1,420 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io._\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[history] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path): (Option[FSDataOutputStream], OutputStream) = {\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    var hadoopDataStream: Option[FSDataOutputStream] = None\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(\n+          SparkHadoopUtil.createFile(fileSystem, path, sparkConf.get(EVENT_LOG_ALLOW_EC)))\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+      (hadoopDataStream, bstream)\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it, -and which support setTimes(); it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  /** initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def apply(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  override val logPath: String = SingleEventLogFileWriter.getLogPath(logBaseDir, appId,\n+    appAttemptId, compressionCodecName)\n+\n+  private val inProgressPath = logPath + EventLogFileWriter.IN_PROGRESS\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+\n+  private var writer: Option[PrintWriter] = None\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    val (hadoopStream, outputStream) = initLogFile(new Path(inProgressPath))\n+    hadoopDataStream = hadoopStream\n+    writer = Some(new PrintWriter(new OutputStreamWriter(outputStream, StandardCharsets.UTF_8)))\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    // scalastyle:off println\n+    writer.foreach(_.println(eventJson))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  /**\n+   * Stop logging events. The event log file will be renamed so that it loses the\n+   * \".inprogress\" suffix.\n+   */\n+  override def stop(): Unit = {\n+    writer.foreach(_.close())\n+    renameFile(new Path(inProgressPath), new Path(logPath), shouldOverwrite)\n+  }\n+}\n+\n+object SingleEventLogFileWriter {\n+  /**\n+   * Return a file-system-safe path to the log file for the given application.\n+   *\n+   * Note that because we currently only create a single log file for each application,\n+   * we must encode all the information needed to parse this event log in the file name\n+   * instead of within the file itself. Otherwise, if the file is compressed, for instance,\n+   * we won't know which codec to use to decompress the metadata needed to open the file in\n+   * the first place.\n+   *\n+   * The log file name will identify the compression codec used for the contents, if any.\n+   * For example, app_123 for an uncompressed log, app_123.lzf for an LZF-compressed log.\n+   *\n+   * @param logBaseDir Directory where the log file will be written.\n+   * @param appId A unique app ID.\n+   * @param appAttemptId A unique attempt id of appId. May be the empty string.\n+   * @param compressionCodecName Name to identify the codec used to compress the contents\n+   *                             of the log, or None if compression is not enabled.\n+   * @return A path which consists of file-system-safe characters.\n+   */\n+  def getLogPath(\n+      logBaseDir: URI,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      compressionCodecName: Option[String] = None): String = {\n+    val codec = compressionCodecName.map(\".\" + _).getOrElse(\"\")\n+    new Path(logBaseDir).toString.stripSuffix(\"/\") + \"/\" +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId) + codec\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into multiple log files, rolled over via configured size.\n+ *\n+ * The class creates one directory per application, and stores event log files as well as\n+ * metadata files. The name of directory and files in the directory would follow:\n+ *\n+ * - The name of directory: eventlog_v2_appId(_[appAttemptId])\n+ * - The prefix of name on event files: events_[index]_[appId](_[appAttemptId])(.[codec])\n+ *   - \"index\" would be monotonically increasing value (say, sequence)\n+ * - The name of metadata (app. status) file name: appstatus_[appId](_[appAttemptId])(.inprogress)\n+ *\n+ * The writer will roll over the event log file when configured size is reached. Note that the\n+ * writer doesn't check the size on file being open for write: the writer tracks the count of bytes\n+ * written before compression is applied.\n+ *\n+ * For metadata files, the class will leverage zero-byte file, as it provides minimized cost.\n+ */\n+class RollingEventLogFilesWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  import RollingEventLogFilesWriter._\n+\n+  private val eventFileMaxLengthKiB = sparkConf.get(EVENT_LOG_ROLLED_EVENT_LOG_MAX_FILE_SIZE)\n+\n+  private val logDirForAppPath = getAppEventLogDirPath(logBaseDir, appId, appAttemptId)\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+  private var countingOutputStream: Option[CountingOutputStream] = None\n+  private var writer: Option[PrintWriter] = None\n+\n+  // seq and event log path will be updated soon in rollEventLogFile, which `start` will call\n+  private var index: Long = 0L\n+  private var currentEventLogFilePath: Path = logDirForAppPath\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    if (fileSystem.exists(logDirForAppPath) && shouldOverwrite) {\n+      fileSystem.delete(logDirForAppPath, true)\n+    }\n+\n+    if (fileSystem.exists(logDirForAppPath)) {\n+      throw new IOException(s\"Target log directory already exists ($logDirForAppPath)\")\n+    }\n+\n+    fileSystem.mkdirs(logDirForAppPath, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+    createAppStatusFile(inProgress = true)\n+    rollEventLogFile()\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    writer.foreach { w =>\n+      val currentLen = countingOutputStream.get.getBytesWritten\n+      if (currentLen + eventJson.length > eventFileMaxLengthKiB * 1024) {\n+        rollEventLogFile()\n+      }\n+    }\n+\n+    // scalastyle:off println\n+    writer.foreach(_.println(eventJson))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  private def rollEventLogFile(): Unit = {\n+    writer.foreach(_.close())\n+\n+    index += 1\n+    currentEventLogFilePath = getEventLogFilePath(logDirForAppPath, appId, appAttemptId, index,\n+      compressionCodecName)\n+\n+    val (hadoopStream, outputStream) = initLogFile(currentEventLogFilePath)\n+    hadoopDataStream = hadoopStream\n+    countingOutputStream = Some(new CountingOutputStream(outputStream))\n+    writer = Some(new PrintWriter(\n+      new OutputStreamWriter(countingOutputStream.get, StandardCharsets.UTF_8)))\n+  }\n+\n+  override def stop(): Unit = {\n+    writer.foreach(_.close())\n+    val appStatusPathIncomplete = getAppStatusFilePath(logDirForAppPath, appId, appAttemptId,\n+      inProgress = true)\n+    val appStatusPathComplete = getAppStatusFilePath(logDirForAppPath, appId, appAttemptId,\n+      inProgress = false)\n+    renameFile(appStatusPathIncomplete, appStatusPathComplete, overwrite = true)\n+  }\n+\n+  override def logPath: String = logDirForAppPath.toString\n+\n+  private def createAppStatusFile(inProgress: Boolean): Unit = {\n+    val appStatusPath = getAppStatusFilePath(logDirForAppPath, appId, appAttemptId, inProgress)\n+    val outputStream = fileSystem.create(appStatusPath)\n+    // we intentionally create zero-byte file to minimize the cost\n+    outputStream.close()\n+  }\n+}\n+\n+object RollingEventLogFilesWriter {\n+  def getAppEventLogDirPath(logBaseDir: URI, appId: String, appAttemptId: Option[String]): Path =\n+    new Path(new Path(logBaseDir), \"eventlog_v2_\" +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId))\n+\n+  def getAppStatusFilePath(\n+      appLogDir: Path,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      inProgress: Boolean): Path = {\n+    val base = \"appstatus_\" + EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId)\n+    val name = if (inProgress) base + EventLogFileWriter.IN_PROGRESS else base\n+    new Path(appLogDir, name)\n+  }\n+\n+  def getEventLogFilePath(\n+      appLogDir: Path,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      index: Long,\n+      codecName: Option[String]): Path = {\n+    val base = s\"events_${index}_\" + EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId)\n+    val codec = codecName.map(\".\" + _).getOrElse(\"\")\n+    new Path(appLogDir, base + codec)\n+  }\n+\n+  def isEventLogDir(status: FileStatus): Boolean = {\n+    status.isDirectory && status.getPath.getName.startsWith(\"eventlog_v2_\")\n+  }\n+\n+  def isEventLogFile(status: FileStatus): Boolean = {\n+    status.isFile && isEventLogFile(status.getPath)\n+  }\n+\n+  def isEventLogFile(path: Path): Boolean = {\n+    path.getName.startsWith(\"events_\")\n+  }\n+\n+  def isAppStatusFile(status: FileStatus): Boolean = {\n+    status.isFile && isAppStatusFile(status.getPath)\n+  }\n+\n+  def isAppStatusFile(path: Path): Boolean = {\n+    path.getName.startsWith(\"appstatus\")\n+  }\n+\n+  def getSequence(eventLogFileName: String): Long = {\n+    require(eventLogFileName.startsWith(\"events_\"), \"Not a event log file!\")"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Nice suggestion! Will address.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-25T12:14:02Z",
    "diffHunk": "@@ -0,0 +1,420 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io._\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[history] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path): (Option[FSDataOutputStream], OutputStream) = {\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    var hadoopDataStream: Option[FSDataOutputStream] = None\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(\n+          SparkHadoopUtil.createFile(fileSystem, path, sparkConf.get(EVENT_LOG_ALLOW_EC)))\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+      (hadoopDataStream, bstream)\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it, -and which support setTimes(); it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  /** initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def apply(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  override val logPath: String = SingleEventLogFileWriter.getLogPath(logBaseDir, appId,\n+    appAttemptId, compressionCodecName)\n+\n+  private val inProgressPath = logPath + EventLogFileWriter.IN_PROGRESS\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+\n+  private var writer: Option[PrintWriter] = None\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    val (hadoopStream, outputStream) = initLogFile(new Path(inProgressPath))\n+    hadoopDataStream = hadoopStream\n+    writer = Some(new PrintWriter(new OutputStreamWriter(outputStream, StandardCharsets.UTF_8)))\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    // scalastyle:off println\n+    writer.foreach(_.println(eventJson))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  /**\n+   * Stop logging events. The event log file will be renamed so that it loses the\n+   * \".inprogress\" suffix.\n+   */\n+  override def stop(): Unit = {\n+    writer.foreach(_.close())\n+    renameFile(new Path(inProgressPath), new Path(logPath), shouldOverwrite)\n+  }\n+}\n+\n+object SingleEventLogFileWriter {\n+  /**\n+   * Return a file-system-safe path to the log file for the given application.\n+   *\n+   * Note that because we currently only create a single log file for each application,\n+   * we must encode all the information needed to parse this event log in the file name\n+   * instead of within the file itself. Otherwise, if the file is compressed, for instance,\n+   * we won't know which codec to use to decompress the metadata needed to open the file in\n+   * the first place.\n+   *\n+   * The log file name will identify the compression codec used for the contents, if any.\n+   * For example, app_123 for an uncompressed log, app_123.lzf for an LZF-compressed log.\n+   *\n+   * @param logBaseDir Directory where the log file will be written.\n+   * @param appId A unique app ID.\n+   * @param appAttemptId A unique attempt id of appId. May be the empty string.\n+   * @param compressionCodecName Name to identify the codec used to compress the contents\n+   *                             of the log, or None if compression is not enabled.\n+   * @return A path which consists of file-system-safe characters.\n+   */\n+  def getLogPath(\n+      logBaseDir: URI,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      compressionCodecName: Option[String] = None): String = {\n+    val codec = compressionCodecName.map(\".\" + _).getOrElse(\"\")\n+    new Path(logBaseDir).toString.stripSuffix(\"/\") + \"/\" +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId) + codec\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into multiple log files, rolled over via configured size.\n+ *\n+ * The class creates one directory per application, and stores event log files as well as\n+ * metadata files. The name of directory and files in the directory would follow:\n+ *\n+ * - The name of directory: eventlog_v2_appId(_[appAttemptId])\n+ * - The prefix of name on event files: events_[index]_[appId](_[appAttemptId])(.[codec])\n+ *   - \"index\" would be monotonically increasing value (say, sequence)\n+ * - The name of metadata (app. status) file name: appstatus_[appId](_[appAttemptId])(.inprogress)\n+ *\n+ * The writer will roll over the event log file when configured size is reached. Note that the\n+ * writer doesn't check the size on file being open for write: the writer tracks the count of bytes\n+ * written before compression is applied.\n+ *\n+ * For metadata files, the class will leverage zero-byte file, as it provides minimized cost.\n+ */\n+class RollingEventLogFilesWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  import RollingEventLogFilesWriter._\n+\n+  private val eventFileMaxLengthKiB = sparkConf.get(EVENT_LOG_ROLLED_EVENT_LOG_MAX_FILE_SIZE)\n+\n+  private val logDirForAppPath = getAppEventLogDirPath(logBaseDir, appId, appAttemptId)\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+  private var countingOutputStream: Option[CountingOutputStream] = None\n+  private var writer: Option[PrintWriter] = None\n+\n+  // seq and event log path will be updated soon in rollEventLogFile, which `start` will call\n+  private var index: Long = 0L\n+  private var currentEventLogFilePath: Path = logDirForAppPath\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    if (fileSystem.exists(logDirForAppPath) && shouldOverwrite) {\n+      fileSystem.delete(logDirForAppPath, true)\n+    }\n+\n+    if (fileSystem.exists(logDirForAppPath)) {\n+      throw new IOException(s\"Target log directory already exists ($logDirForAppPath)\")\n+    }\n+\n+    fileSystem.mkdirs(logDirForAppPath, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+    createAppStatusFile(inProgress = true)\n+    rollEventLogFile()\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    writer.foreach { w =>\n+      val currentLen = countingOutputStream.get.getBytesWritten\n+      if (currentLen + eventJson.length > eventFileMaxLengthKiB * 1024) {\n+        rollEventLogFile()\n+      }\n+    }\n+\n+    // scalastyle:off println\n+    writer.foreach(_.println(eventJson))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  private def rollEventLogFile(): Unit = {\n+    writer.foreach(_.close())\n+\n+    index += 1\n+    currentEventLogFilePath = getEventLogFilePath(logDirForAppPath, appId, appAttemptId, index,\n+      compressionCodecName)\n+\n+    val (hadoopStream, outputStream) = initLogFile(currentEventLogFilePath)\n+    hadoopDataStream = hadoopStream\n+    countingOutputStream = Some(new CountingOutputStream(outputStream))\n+    writer = Some(new PrintWriter(\n+      new OutputStreamWriter(countingOutputStream.get, StandardCharsets.UTF_8)))\n+  }\n+\n+  override def stop(): Unit = {\n+    writer.foreach(_.close())\n+    val appStatusPathIncomplete = getAppStatusFilePath(logDirForAppPath, appId, appAttemptId,\n+      inProgress = true)\n+    val appStatusPathComplete = getAppStatusFilePath(logDirForAppPath, appId, appAttemptId,\n+      inProgress = false)\n+    renameFile(appStatusPathIncomplete, appStatusPathComplete, overwrite = true)\n+  }\n+\n+  override def logPath: String = logDirForAppPath.toString\n+\n+  private def createAppStatusFile(inProgress: Boolean): Unit = {\n+    val appStatusPath = getAppStatusFilePath(logDirForAppPath, appId, appAttemptId, inProgress)\n+    val outputStream = fileSystem.create(appStatusPath)\n+    // we intentionally create zero-byte file to minimize the cost\n+    outputStream.close()\n+  }\n+}\n+\n+object RollingEventLogFilesWriter {\n+  def getAppEventLogDirPath(logBaseDir: URI, appId: String, appAttemptId: Option[String]): Path =\n+    new Path(new Path(logBaseDir), \"eventlog_v2_\" +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId))\n+\n+  def getAppStatusFilePath(\n+      appLogDir: Path,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      inProgress: Boolean): Path = {\n+    val base = \"appstatus_\" + EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId)\n+    val name = if (inProgress) base + EventLogFileWriter.IN_PROGRESS else base\n+    new Path(appLogDir, name)\n+  }\n+\n+  def getEventLogFilePath(\n+      appLogDir: Path,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      index: Long,\n+      codecName: Option[String]): Path = {\n+    val base = s\"events_${index}_\" + EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId)\n+    val codec = codecName.map(\".\" + _).getOrElse(\"\")\n+    new Path(appLogDir, base + codec)\n+  }\n+\n+  def isEventLogDir(status: FileStatus): Boolean = {\n+    status.isDirectory && status.getPath.getName.startsWith(\"eventlog_v2_\")\n+  }\n+\n+  def isEventLogFile(status: FileStatus): Boolean = {\n+    status.isFile && isEventLogFile(status.getPath)\n+  }\n+\n+  def isEventLogFile(path: Path): Boolean = {\n+    path.getName.startsWith(\"events_\")\n+  }\n+\n+  def isAppStatusFile(status: FileStatus): Boolean = {\n+    status.isFile && isAppStatusFile(status.getPath)\n+  }\n+\n+  def isAppStatusFile(path: Path): Boolean = {\n+    path.getName.startsWith(\"appstatus\")\n+  }\n+\n+  def getSequence(eventLogFileName: String): Long = {\n+    require(eventLogFileName.startsWith(\"events_\"), \"Not a event log file!\")"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "unused",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-26T00:24:26Z",
    "diffHunk": "@@ -0,0 +1,422 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io._\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "\"-and\"?",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-26T00:30:02Z",
    "diffHunk": "@@ -0,0 +1,422 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io._\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[history] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  // Only defined if the file system scheme is not local\n+  protected var hadoopDataStream: Option[FSDataOutputStream] = None\n+  protected var writer: Option[PrintWriter] = None\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path, fnSetupWriter: OutputStream => PrintWriter): Unit = {\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(\n+          SparkHadoopUtil.createFile(fileSystem, path, sparkConf.get(EVENT_LOG_ALLOW_EC)))\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+      writer = Some(fnSetupWriter(bstream))\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def writeJson(json: String, flushLogger: Boolean = false): Unit = {\n+    // scalastyle:off println\n+    writer.foreach(_.println(json))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  protected def closeWriter(): Unit = {\n+    writer.foreach(_.close())\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it, -and which support setTimes(); it's a no-op on most object stores"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Will fix it. I guess the comment was already there and I'm unsure I made a mistake.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-26T01:18:06Z",
    "diffHunk": "@@ -0,0 +1,422 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io._\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[history] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  // Only defined if the file system scheme is not local\n+  protected var hadoopDataStream: Option[FSDataOutputStream] = None\n+  protected var writer: Option[PrintWriter] = None\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path, fnSetupWriter: OutputStream => PrintWriter): Unit = {\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(\n+          SparkHadoopUtil.createFile(fileSystem, path, sparkConf.get(EVENT_LOG_ALLOW_EC)))\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+      writer = Some(fnSetupWriter(bstream))\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def writeJson(json: String, flushLogger: Boolean = false): Unit = {\n+    // scalastyle:off println\n+    writer.foreach(_.println(json))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  protected def closeWriter(): Unit = {\n+    writer.foreach(_.close())\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it, -and which support setTimes(); it's a no-op on most object stores"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "It was there and even I remove `-` I feel it's not clear. I'll refine it a bit.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-26T01:44:57Z",
    "diffHunk": "@@ -0,0 +1,422 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io._\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[history] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  // Only defined if the file system scheme is not local\n+  protected var hadoopDataStream: Option[FSDataOutputStream] = None\n+  protected var writer: Option[PrintWriter] = None\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path, fnSetupWriter: OutputStream => PrintWriter): Unit = {\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(\n+          SparkHadoopUtil.createFile(fileSystem, path, sparkConf.get(EVENT_LOG_ALLOW_EC)))\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+      writer = Some(fnSetupWriter(bstream))\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def writeJson(json: String, flushLogger: Boolean = false): Unit = {\n+    // scalastyle:off println\n+    writer.foreach(_.println(json))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  protected def closeWriter(): Unit = {\n+    writer.foreach(_.close())\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it, -and which support setTimes(); it's a no-op on most object stores"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "This looks nicer with currying.\r\n\r\n```\r\ninitLogFile(path) { stream =>\r\n  // wrap the stream as needed\r\n}\r\n```",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-26T00:34:11Z",
    "diffHunk": "@@ -0,0 +1,422 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io._\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[history] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  // Only defined if the file system scheme is not local\n+  protected var hadoopDataStream: Option[FSDataOutputStream] = None\n+  protected var writer: Option[PrintWriter] = None\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path, fnSetupWriter: OutputStream => PrintWriter): Unit = {"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Why not store the max size field as bytes?",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-26T00:37:08Z",
    "diffHunk": "@@ -0,0 +1,422 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io._\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[history] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  // Only defined if the file system scheme is not local\n+  protected var hadoopDataStream: Option[FSDataOutputStream] = None\n+  protected var writer: Option[PrintWriter] = None\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path, fnSetupWriter: OutputStream => PrintWriter): Unit = {\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(\n+          SparkHadoopUtil.createFile(fileSystem, path, sparkConf.get(EVENT_LOG_ALLOW_EC)))\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+      writer = Some(fnSetupWriter(bstream))\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def writeJson(json: String, flushLogger: Boolean = false): Unit = {\n+    // scalastyle:off println\n+    writer.foreach(_.println(json))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  protected def closeWriter(): Unit = {\n+    writer.foreach(_.close())\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it, -and which support setTimes(); it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  /** initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def apply(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  override val logPath: String = SingleEventLogFileWriter.getLogPath(logBaseDir, appId,\n+    appAttemptId, compressionCodecName)\n+\n+  private val inProgressPath = logPath + EventLogFileWriter.IN_PROGRESS\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    initLogFile(new Path(inProgressPath),\n+      ostream => new PrintWriter(new OutputStreamWriter(ostream, StandardCharsets.UTF_8)))\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    writeJson(eventJson, flushLogger)\n+  }\n+\n+  /**\n+   * Stop logging events. The event log file will be renamed so that it loses the\n+   * \".inprogress\" suffix.\n+   */\n+  override def stop(): Unit = {\n+    closeWriter()\n+    renameFile(new Path(inProgressPath), new Path(logPath), shouldOverwrite)\n+  }\n+}\n+\n+object SingleEventLogFileWriter {\n+  /**\n+   * Return a file-system-safe path to the log file for the given application.\n+   *\n+   * Note that because we currently only create a single log file for each application,\n+   * we must encode all the information needed to parse this event log in the file name\n+   * instead of within the file itself. Otherwise, if the file is compressed, for instance,\n+   * we won't know which codec to use to decompress the metadata needed to open the file in\n+   * the first place.\n+   *\n+   * The log file name will identify the compression codec used for the contents, if any.\n+   * For example, app_123 for an uncompressed log, app_123.lzf for an LZF-compressed log.\n+   *\n+   * @param logBaseDir Directory where the log file will be written.\n+   * @param appId A unique app ID.\n+   * @param appAttemptId A unique attempt id of appId. May be the empty string.\n+   * @param compressionCodecName Name to identify the codec used to compress the contents\n+   *                             of the log, or None if compression is not enabled.\n+   * @return A path which consists of file-system-safe characters.\n+   */\n+  def getLogPath(\n+      logBaseDir: URI,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      compressionCodecName: Option[String] = None): String = {\n+    val codec = compressionCodecName.map(\".\" + _).getOrElse(\"\")\n+    new Path(logBaseDir).toString.stripSuffix(\"/\") + \"/\" +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId) + codec\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into multiple log files, rolled over via configured size.\n+ *\n+ * The class creates one directory per application, and stores event log files as well as\n+ * metadata files. The name of directory and files in the directory would follow:\n+ *\n+ * - The name of directory: eventlog_v2_appId(_[appAttemptId])\n+ * - The prefix of name on event files: events_[index]_[appId](_[appAttemptId])(.[codec])\n+ *   - \"index\" would be monotonically increasing value (say, sequence)\n+ * - The name of metadata (app. status) file name: appstatus_[appId](_[appAttemptId])(.inprogress)\n+ *\n+ * The writer will roll over the event log file when configured size is reached. Note that the\n+ * writer doesn't check the size on file being open for write: the writer tracks the count of bytes\n+ * written before compression is applied.\n+ *\n+ * For metadata files, the class will leverage zero-byte file, as it provides minimized cost.\n+ */\n+class RollingEventLogFilesWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  import RollingEventLogFilesWriter._\n+\n+  private val eventFileMaxLengthKiB = sparkConf.get(EVENT_LOG_ROLLED_EVENT_LOG_MAX_FILE_SIZE)\n+\n+  private val logDirForAppPath = getAppEventLogDirPath(logBaseDir, appId, appAttemptId)\n+\n+  private var countingOutputStream: Option[CountingOutputStream] = None\n+\n+  // seq and event log path will be updated soon in rollEventLogFile, which `start` will call\n+  private var index: Long = 0L\n+  private var currentEventLogFilePath: Path = _\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    if (fileSystem.exists(logDirForAppPath) && shouldOverwrite) {\n+      fileSystem.delete(logDirForAppPath, true)\n+    }\n+\n+    if (fileSystem.exists(logDirForAppPath)) {\n+      throw new IOException(s\"Target log directory already exists ($logDirForAppPath)\")\n+    }\n+\n+    fileSystem.mkdirs(logDirForAppPath, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+    createAppStatusFile(inProgress = true)\n+    rollEventLogFile()\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    writer.foreach { w =>\n+      val currentLen = countingOutputStream.get.getBytesWritten\n+      if (currentLen + eventJson.length > eventFileMaxLengthKiB * 1024) {"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Yeah it was MiB and changed to KiB, but once end users will specify unit then no matter what the unit is. I'll change it as bytes.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-26T01:19:08Z",
    "diffHunk": "@@ -0,0 +1,422 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io._\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[history] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  // Only defined if the file system scheme is not local\n+  protected var hadoopDataStream: Option[FSDataOutputStream] = None\n+  protected var writer: Option[PrintWriter] = None\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path, fnSetupWriter: OutputStream => PrintWriter): Unit = {\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(\n+          SparkHadoopUtil.createFile(fileSystem, path, sparkConf.get(EVENT_LOG_ALLOW_EC)))\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+      writer = Some(fnSetupWriter(bstream))\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def writeJson(json: String, flushLogger: Boolean = false): Unit = {\n+    // scalastyle:off println\n+    writer.foreach(_.println(json))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  protected def closeWriter(): Unit = {\n+    writer.foreach(_.close())\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it, -and which support setTimes(); it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  /** initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def apply(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  override val logPath: String = SingleEventLogFileWriter.getLogPath(logBaseDir, appId,\n+    appAttemptId, compressionCodecName)\n+\n+  private val inProgressPath = logPath + EventLogFileWriter.IN_PROGRESS\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    initLogFile(new Path(inProgressPath),\n+      ostream => new PrintWriter(new OutputStreamWriter(ostream, StandardCharsets.UTF_8)))\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    writeJson(eventJson, flushLogger)\n+  }\n+\n+  /**\n+   * Stop logging events. The event log file will be renamed so that it loses the\n+   * \".inprogress\" suffix.\n+   */\n+  override def stop(): Unit = {\n+    closeWriter()\n+    renameFile(new Path(inProgressPath), new Path(logPath), shouldOverwrite)\n+  }\n+}\n+\n+object SingleEventLogFileWriter {\n+  /**\n+   * Return a file-system-safe path to the log file for the given application.\n+   *\n+   * Note that because we currently only create a single log file for each application,\n+   * we must encode all the information needed to parse this event log in the file name\n+   * instead of within the file itself. Otherwise, if the file is compressed, for instance,\n+   * we won't know which codec to use to decompress the metadata needed to open the file in\n+   * the first place.\n+   *\n+   * The log file name will identify the compression codec used for the contents, if any.\n+   * For example, app_123 for an uncompressed log, app_123.lzf for an LZF-compressed log.\n+   *\n+   * @param logBaseDir Directory where the log file will be written.\n+   * @param appId A unique app ID.\n+   * @param appAttemptId A unique attempt id of appId. May be the empty string.\n+   * @param compressionCodecName Name to identify the codec used to compress the contents\n+   *                             of the log, or None if compression is not enabled.\n+   * @return A path which consists of file-system-safe characters.\n+   */\n+  def getLogPath(\n+      logBaseDir: URI,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      compressionCodecName: Option[String] = None): String = {\n+    val codec = compressionCodecName.map(\".\" + _).getOrElse(\"\")\n+    new Path(logBaseDir).toString.stripSuffix(\"/\") + \"/\" +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId) + codec\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into multiple log files, rolled over via configured size.\n+ *\n+ * The class creates one directory per application, and stores event log files as well as\n+ * metadata files. The name of directory and files in the directory would follow:\n+ *\n+ * - The name of directory: eventlog_v2_appId(_[appAttemptId])\n+ * - The prefix of name on event files: events_[index]_[appId](_[appAttemptId])(.[codec])\n+ *   - \"index\" would be monotonically increasing value (say, sequence)\n+ * - The name of metadata (app. status) file name: appstatus_[appId](_[appAttemptId])(.inprogress)\n+ *\n+ * The writer will roll over the event log file when configured size is reached. Note that the\n+ * writer doesn't check the size on file being open for write: the writer tracks the count of bytes\n+ * written before compression is applied.\n+ *\n+ * For metadata files, the class will leverage zero-byte file, as it provides minimized cost.\n+ */\n+class RollingEventLogFilesWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  import RollingEventLogFilesWriter._\n+\n+  private val eventFileMaxLengthKiB = sparkConf.get(EVENT_LOG_ROLLED_EVENT_LOG_MAX_FILE_SIZE)\n+\n+  private val logDirForAppPath = getAppEventLogDirPath(logBaseDir, appId, appAttemptId)\n+\n+  private var countingOutputStream: Option[CountingOutputStream] = None\n+\n+  // seq and event log path will be updated soon in rollEventLogFile, which `start` will call\n+  private var index: Long = 0L\n+  private var currentEventLogFilePath: Path = _\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    if (fileSystem.exists(logDirForAppPath) && shouldOverwrite) {\n+      fileSystem.delete(logDirForAppPath, true)\n+    }\n+\n+    if (fileSystem.exists(logDirForAppPath)) {\n+      throw new IOException(s\"Target log directory already exists ($logDirForAppPath)\")\n+    }\n+\n+    fileSystem.mkdirs(logDirForAppPath, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+    createAppStatusFile(inProgress = true)\n+    rollEventLogFile()\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    writer.foreach { w =>\n+      val currentLen = countingOutputStream.get.getBytesWritten\n+      if (currentLen + eventJson.length > eventFileMaxLengthKiB * 1024) {"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Is there any place where this one is called? Unless I missed something, all the calls I saw had a `FileStatus` object as the argument. Probably similar question for `isAppStatusFile`.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-26T00:42:14Z",
    "diffHunk": "@@ -0,0 +1,422 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io._\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[history] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  // Only defined if the file system scheme is not local\n+  protected var hadoopDataStream: Option[FSDataOutputStream] = None\n+  protected var writer: Option[PrintWriter] = None\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path, fnSetupWriter: OutputStream => PrintWriter): Unit = {\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(\n+          SparkHadoopUtil.createFile(fileSystem, path, sparkConf.get(EVENT_LOG_ALLOW_EC)))\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+      writer = Some(fnSetupWriter(bstream))\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def writeJson(json: String, flushLogger: Boolean = false): Unit = {\n+    // scalastyle:off println\n+    writer.foreach(_.println(json))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  protected def closeWriter(): Unit = {\n+    writer.foreach(_.close())\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it, -and which support setTimes(); it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  /** initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def apply(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  override val logPath: String = SingleEventLogFileWriter.getLogPath(logBaseDir, appId,\n+    appAttemptId, compressionCodecName)\n+\n+  private val inProgressPath = logPath + EventLogFileWriter.IN_PROGRESS\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    initLogFile(new Path(inProgressPath),\n+      ostream => new PrintWriter(new OutputStreamWriter(ostream, StandardCharsets.UTF_8)))\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    writeJson(eventJson, flushLogger)\n+  }\n+\n+  /**\n+   * Stop logging events. The event log file will be renamed so that it loses the\n+   * \".inprogress\" suffix.\n+   */\n+  override def stop(): Unit = {\n+    closeWriter()\n+    renameFile(new Path(inProgressPath), new Path(logPath), shouldOverwrite)\n+  }\n+}\n+\n+object SingleEventLogFileWriter {\n+  /**\n+   * Return a file-system-safe path to the log file for the given application.\n+   *\n+   * Note that because we currently only create a single log file for each application,\n+   * we must encode all the information needed to parse this event log in the file name\n+   * instead of within the file itself. Otherwise, if the file is compressed, for instance,\n+   * we won't know which codec to use to decompress the metadata needed to open the file in\n+   * the first place.\n+   *\n+   * The log file name will identify the compression codec used for the contents, if any.\n+   * For example, app_123 for an uncompressed log, app_123.lzf for an LZF-compressed log.\n+   *\n+   * @param logBaseDir Directory where the log file will be written.\n+   * @param appId A unique app ID.\n+   * @param appAttemptId A unique attempt id of appId. May be the empty string.\n+   * @param compressionCodecName Name to identify the codec used to compress the contents\n+   *                             of the log, or None if compression is not enabled.\n+   * @return A path which consists of file-system-safe characters.\n+   */\n+  def getLogPath(\n+      logBaseDir: URI,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      compressionCodecName: Option[String] = None): String = {\n+    val codec = compressionCodecName.map(\".\" + _).getOrElse(\"\")\n+    new Path(logBaseDir).toString.stripSuffix(\"/\") + \"/\" +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId) + codec\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into multiple log files, rolled over via configured size.\n+ *\n+ * The class creates one directory per application, and stores event log files as well as\n+ * metadata files. The name of directory and files in the directory would follow:\n+ *\n+ * - The name of directory: eventlog_v2_appId(_[appAttemptId])\n+ * - The prefix of name on event files: events_[index]_[appId](_[appAttemptId])(.[codec])\n+ *   - \"index\" would be monotonically increasing value (say, sequence)\n+ * - The name of metadata (app. status) file name: appstatus_[appId](_[appAttemptId])(.inprogress)\n+ *\n+ * The writer will roll over the event log file when configured size is reached. Note that the\n+ * writer doesn't check the size on file being open for write: the writer tracks the count of bytes\n+ * written before compression is applied.\n+ *\n+ * For metadata files, the class will leverage zero-byte file, as it provides minimized cost.\n+ */\n+class RollingEventLogFilesWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  import RollingEventLogFilesWriter._\n+\n+  private val eventFileMaxLengthKiB = sparkConf.get(EVENT_LOG_ROLLED_EVENT_LOG_MAX_FILE_SIZE)\n+\n+  private val logDirForAppPath = getAppEventLogDirPath(logBaseDir, appId, appAttemptId)\n+\n+  private var countingOutputStream: Option[CountingOutputStream] = None\n+\n+  // seq and event log path will be updated soon in rollEventLogFile, which `start` will call\n+  private var index: Long = 0L\n+  private var currentEventLogFilePath: Path = _\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    if (fileSystem.exists(logDirForAppPath) && shouldOverwrite) {\n+      fileSystem.delete(logDirForAppPath, true)\n+    }\n+\n+    if (fileSystem.exists(logDirForAppPath)) {\n+      throw new IOException(s\"Target log directory already exists ($logDirForAppPath)\")\n+    }\n+\n+    fileSystem.mkdirs(logDirForAppPath, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+    createAppStatusFile(inProgress = true)\n+    rollEventLogFile()\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    writer.foreach { w =>\n+      val currentLen = countingOutputStream.get.getBytesWritten\n+      if (currentLen + eventJson.length > eventFileMaxLengthKiB * 1024) {\n+        rollEventLogFile()\n+      }\n+    }\n+\n+    writeJson(eventJson, flushLogger)\n+  }\n+\n+  private def rollEventLogFile(): Unit = {\n+    closeWriter()\n+\n+    index += 1\n+    currentEventLogFilePath = getEventLogFilePath(logDirForAppPath, appId, appAttemptId, index,\n+      compressionCodecName)\n+\n+    initLogFile(currentEventLogFilePath, ostream => {\n+      countingOutputStream = Some(new CountingOutputStream(ostream))\n+      new PrintWriter(\n+        new OutputStreamWriter(countingOutputStream.get, StandardCharsets.UTF_8))\n+    })\n+  }\n+\n+  override def stop(): Unit = {\n+    closeWriter()\n+    val appStatusPathIncomplete = getAppStatusFilePath(logDirForAppPath, appId, appAttemptId,\n+      inProgress = true)\n+    val appStatusPathComplete = getAppStatusFilePath(logDirForAppPath, appId, appAttemptId,\n+      inProgress = false)\n+    renameFile(appStatusPathIncomplete, appStatusPathComplete, overwrite = true)\n+  }\n+\n+  override def logPath: String = logDirForAppPath.toString\n+\n+  private def createAppStatusFile(inProgress: Boolean): Unit = {\n+    val appStatusPath = getAppStatusFilePath(logDirForAppPath, appId, appAttemptId, inProgress)\n+    val outputStream = fileSystem.create(appStatusPath)\n+    // we intentionally create zero-byte file to minimize the cost\n+    outputStream.close()\n+  }\n+}\n+\n+object RollingEventLogFilesWriter {\n+  private val EVENT_LOG_DIR_NAME_PREFIX = \"eventlog_v2_\"\n+  private val EVENT_LOG_FILE_NAME_PREFIX = \"events_\"\n+  private val APPSTATUS_FILE_NAME_PREFIX = \"appstatus_\"\n+\n+  def getAppEventLogDirPath(logBaseDir: URI, appId: String, appAttemptId: Option[String]): Path =\n+    new Path(new Path(logBaseDir), EVENT_LOG_DIR_NAME_PREFIX +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId))\n+\n+  def getAppStatusFilePath(\n+      appLogDir: Path,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      inProgress: Boolean): Path = {\n+    val base = APPSTATUS_FILE_NAME_PREFIX +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId)\n+    val name = if (inProgress) base + EventLogFileWriter.IN_PROGRESS else base\n+    new Path(appLogDir, name)\n+  }\n+\n+  def getEventLogFilePath(\n+      appLogDir: Path,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      index: Long,\n+      codecName: Option[String]): Path = {\n+    val base = s\"${EVENT_LOG_FILE_NAME_PREFIX}${index}_\" +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId)\n+    val codec = codecName.map(\".\" + _).getOrElse(\"\")\n+    new Path(appLogDir, base + codec)\n+  }\n+\n+  def isEventLogDir(status: FileStatus): Boolean = {\n+    status.isDirectory && status.getPath.getName.startsWith(EVENT_LOG_DIR_NAME_PREFIX)\n+  }\n+\n+  def isEventLogFile(status: FileStatus): Boolean = {\n+    status.isFile && isEventLogFile(status.getPath)\n+  }\n+\n+  def isEventLogFile(path: Path): Boolean = {"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "OK I see what you mean. Will consolidate methods into one.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-26T01:20:38Z",
    "diffHunk": "@@ -0,0 +1,422 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io._\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[history] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  // Only defined if the file system scheme is not local\n+  protected var hadoopDataStream: Option[FSDataOutputStream] = None\n+  protected var writer: Option[PrintWriter] = None\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path, fnSetupWriter: OutputStream => PrintWriter): Unit = {\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(\n+          SparkHadoopUtil.createFile(fileSystem, path, sparkConf.get(EVENT_LOG_ALLOW_EC)))\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+      writer = Some(fnSetupWriter(bstream))\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def writeJson(json: String, flushLogger: Boolean = false): Unit = {\n+    // scalastyle:off println\n+    writer.foreach(_.println(json))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  protected def closeWriter(): Unit = {\n+    writer.foreach(_.close())\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it, -and which support setTimes(); it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  /** initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def apply(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  override val logPath: String = SingleEventLogFileWriter.getLogPath(logBaseDir, appId,\n+    appAttemptId, compressionCodecName)\n+\n+  private val inProgressPath = logPath + EventLogFileWriter.IN_PROGRESS\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    initLogFile(new Path(inProgressPath),\n+      ostream => new PrintWriter(new OutputStreamWriter(ostream, StandardCharsets.UTF_8)))\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    writeJson(eventJson, flushLogger)\n+  }\n+\n+  /**\n+   * Stop logging events. The event log file will be renamed so that it loses the\n+   * \".inprogress\" suffix.\n+   */\n+  override def stop(): Unit = {\n+    closeWriter()\n+    renameFile(new Path(inProgressPath), new Path(logPath), shouldOverwrite)\n+  }\n+}\n+\n+object SingleEventLogFileWriter {\n+  /**\n+   * Return a file-system-safe path to the log file for the given application.\n+   *\n+   * Note that because we currently only create a single log file for each application,\n+   * we must encode all the information needed to parse this event log in the file name\n+   * instead of within the file itself. Otherwise, if the file is compressed, for instance,\n+   * we won't know which codec to use to decompress the metadata needed to open the file in\n+   * the first place.\n+   *\n+   * The log file name will identify the compression codec used for the contents, if any.\n+   * For example, app_123 for an uncompressed log, app_123.lzf for an LZF-compressed log.\n+   *\n+   * @param logBaseDir Directory where the log file will be written.\n+   * @param appId A unique app ID.\n+   * @param appAttemptId A unique attempt id of appId. May be the empty string.\n+   * @param compressionCodecName Name to identify the codec used to compress the contents\n+   *                             of the log, or None if compression is not enabled.\n+   * @return A path which consists of file-system-safe characters.\n+   */\n+  def getLogPath(\n+      logBaseDir: URI,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      compressionCodecName: Option[String] = None): String = {\n+    val codec = compressionCodecName.map(\".\" + _).getOrElse(\"\")\n+    new Path(logBaseDir).toString.stripSuffix(\"/\") + \"/\" +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId) + codec\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into multiple log files, rolled over via configured size.\n+ *\n+ * The class creates one directory per application, and stores event log files as well as\n+ * metadata files. The name of directory and files in the directory would follow:\n+ *\n+ * - The name of directory: eventlog_v2_appId(_[appAttemptId])\n+ * - The prefix of name on event files: events_[index]_[appId](_[appAttemptId])(.[codec])\n+ *   - \"index\" would be monotonically increasing value (say, sequence)\n+ * - The name of metadata (app. status) file name: appstatus_[appId](_[appAttemptId])(.inprogress)\n+ *\n+ * The writer will roll over the event log file when configured size is reached. Note that the\n+ * writer doesn't check the size on file being open for write: the writer tracks the count of bytes\n+ * written before compression is applied.\n+ *\n+ * For metadata files, the class will leverage zero-byte file, as it provides minimized cost.\n+ */\n+class RollingEventLogFilesWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  import RollingEventLogFilesWriter._\n+\n+  private val eventFileMaxLengthKiB = sparkConf.get(EVENT_LOG_ROLLED_EVENT_LOG_MAX_FILE_SIZE)\n+\n+  private val logDirForAppPath = getAppEventLogDirPath(logBaseDir, appId, appAttemptId)\n+\n+  private var countingOutputStream: Option[CountingOutputStream] = None\n+\n+  // seq and event log path will be updated soon in rollEventLogFile, which `start` will call\n+  private var index: Long = 0L\n+  private var currentEventLogFilePath: Path = _\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    if (fileSystem.exists(logDirForAppPath) && shouldOverwrite) {\n+      fileSystem.delete(logDirForAppPath, true)\n+    }\n+\n+    if (fileSystem.exists(logDirForAppPath)) {\n+      throw new IOException(s\"Target log directory already exists ($logDirForAppPath)\")\n+    }\n+\n+    fileSystem.mkdirs(logDirForAppPath, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+    createAppStatusFile(inProgress = true)\n+    rollEventLogFile()\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    writer.foreach { w =>\n+      val currentLen = countingOutputStream.get.getBytesWritten\n+      if (currentLen + eventJson.length > eventFileMaxLengthKiB * 1024) {\n+        rollEventLogFile()\n+      }\n+    }\n+\n+    writeJson(eventJson, flushLogger)\n+  }\n+\n+  private def rollEventLogFile(): Unit = {\n+    closeWriter()\n+\n+    index += 1\n+    currentEventLogFilePath = getEventLogFilePath(logDirForAppPath, appId, appAttemptId, index,\n+      compressionCodecName)\n+\n+    initLogFile(currentEventLogFilePath, ostream => {\n+      countingOutputStream = Some(new CountingOutputStream(ostream))\n+      new PrintWriter(\n+        new OutputStreamWriter(countingOutputStream.get, StandardCharsets.UTF_8))\n+    })\n+  }\n+\n+  override def stop(): Unit = {\n+    closeWriter()\n+    val appStatusPathIncomplete = getAppStatusFilePath(logDirForAppPath, appId, appAttemptId,\n+      inProgress = true)\n+    val appStatusPathComplete = getAppStatusFilePath(logDirForAppPath, appId, appAttemptId,\n+      inProgress = false)\n+    renameFile(appStatusPathIncomplete, appStatusPathComplete, overwrite = true)\n+  }\n+\n+  override def logPath: String = logDirForAppPath.toString\n+\n+  private def createAppStatusFile(inProgress: Boolean): Unit = {\n+    val appStatusPath = getAppStatusFilePath(logDirForAppPath, appId, appAttemptId, inProgress)\n+    val outputStream = fileSystem.create(appStatusPath)\n+    // we intentionally create zero-byte file to minimize the cost\n+    outputStream.close()\n+  }\n+}\n+\n+object RollingEventLogFilesWriter {\n+  private val EVENT_LOG_DIR_NAME_PREFIX = \"eventlog_v2_\"\n+  private val EVENT_LOG_FILE_NAME_PREFIX = \"events_\"\n+  private val APPSTATUS_FILE_NAME_PREFIX = \"appstatus_\"\n+\n+  def getAppEventLogDirPath(logBaseDir: URI, appId: String, appAttemptId: Option[String]): Path =\n+    new Path(new Path(logBaseDir), EVENT_LOG_DIR_NAME_PREFIX +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId))\n+\n+  def getAppStatusFilePath(\n+      appLogDir: Path,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      inProgress: Boolean): Path = {\n+    val base = APPSTATUS_FILE_NAME_PREFIX +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId)\n+    val name = if (inProgress) base + EventLogFileWriter.IN_PROGRESS else base\n+    new Path(appLogDir, name)\n+  }\n+\n+  def getEventLogFilePath(\n+      appLogDir: Path,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      index: Long,\n+      codecName: Option[String]): Path = {\n+    val base = s\"${EVENT_LOG_FILE_NAME_PREFIX}${index}_\" +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId)\n+    val codec = codecName.map(\".\" + _).getOrElse(\"\")\n+    new Path(appLogDir, base + codec)\n+  }\n+\n+  def isEventLogDir(status: FileStatus): Boolean = {\n+    status.isDirectory && status.getPath.getName.startsWith(EVENT_LOG_DIR_NAME_PREFIX)\n+  }\n+\n+  def isEventLogFile(status: FileStatus): Boolean = {\n+    status.isFile && isEventLogFile(status.getPath)\n+  }\n+\n+  def isEventLogFile(path: Path): Boolean = {"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "Ngone51"
    },
    "body": "Where is this method used ?",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-26T16:19:36Z",
    "diffHunk": "@@ -0,0 +1,414 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io._\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[history] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  // Only defined if the file system scheme is not local\n+  protected var hadoopDataStream: Option[FSDataOutputStream] = None\n+  protected var writer: Option[PrintWriter] = None\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path)(fnSetupWriter: OutputStream => PrintWriter): Unit = {\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(\n+          SparkHadoopUtil.createFile(fileSystem, path, sparkConf.get(EVENT_LOG_ALLOW_EC)))\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+      writer = Some(fnSetupWriter(bstream))\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def writeJson(json: String, flushLogger: Boolean = false): Unit = {\n+    // scalastyle:off println\n+    writer.foreach(_.println(json))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  protected def closeWriter(): Unit = {\n+    writer.foreach(_.close())\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it but support setTimes() instead; it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  /** initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def apply(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  override val logPath: String = SingleEventLogFileWriter.getLogPath(logBaseDir, appId,\n+    appAttemptId, compressionCodecName)\n+\n+  private val inProgressPath = logPath + EventLogFileWriter.IN_PROGRESS\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    initLogFile(new Path(inProgressPath)) { os =>\n+      new PrintWriter(new OutputStreamWriter(os, StandardCharsets.UTF_8))\n+    }\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    writeJson(eventJson, flushLogger)\n+  }\n+\n+  /**\n+   * Stop logging events. The event log file will be renamed so that it loses the\n+   * \".inprogress\" suffix.\n+   */\n+  override def stop(): Unit = {\n+    closeWriter()\n+    renameFile(new Path(inProgressPath), new Path(logPath), shouldOverwrite)\n+  }\n+}\n+\n+object SingleEventLogFileWriter {\n+  /**\n+   * Return a file-system-safe path to the log file for the given application.\n+   *\n+   * Note that because we currently only create a single log file for each application,\n+   * we must encode all the information needed to parse this event log in the file name\n+   * instead of within the file itself. Otherwise, if the file is compressed, for instance,\n+   * we won't know which codec to use to decompress the metadata needed to open the file in\n+   * the first place.\n+   *\n+   * The log file name will identify the compression codec used for the contents, if any.\n+   * For example, app_123 for an uncompressed log, app_123.lzf for an LZF-compressed log.\n+   *\n+   * @param logBaseDir Directory where the log file will be written.\n+   * @param appId A unique app ID.\n+   * @param appAttemptId A unique attempt id of appId. May be the empty string.\n+   * @param compressionCodecName Name to identify the codec used to compress the contents\n+   *                             of the log, or None if compression is not enabled.\n+   * @return A path which consists of file-system-safe characters.\n+   */\n+  def getLogPath(\n+      logBaseDir: URI,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      compressionCodecName: Option[String] = None): String = {\n+    val codec = compressionCodecName.map(\".\" + _).getOrElse(\"\")\n+    new Path(logBaseDir).toString.stripSuffix(\"/\") + \"/\" +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId) + codec\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into multiple log files, rolled over via configured size.\n+ *\n+ * The class creates one directory per application, and stores event log files as well as\n+ * metadata files. The name of directory and files in the directory would follow:\n+ *\n+ * - The name of directory: eventlog_v2_appId(_[appAttemptId])\n+ * - The prefix of name on event files: events_[index]_[appId](_[appAttemptId])(.[codec])\n+ *   - \"index\" would be monotonically increasing value (say, sequence)\n+ * - The name of metadata (app. status) file name: appstatus_[appId](_[appAttemptId])(.inprogress)\n+ *\n+ * The writer will roll over the event log file when configured size is reached. Note that the\n+ * writer doesn't check the size on file being open for write: the writer tracks the count of bytes\n+ * written before compression is applied.\n+ *\n+ * For metadata files, the class will leverage zero-byte file, as it provides minimized cost.\n+ */\n+class RollingEventLogFilesWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  import RollingEventLogFilesWriter._\n+\n+  private val eventFileMaxLength = sparkConf.get(EVENT_LOG_ROLLED_EVENT_LOG_MAX_FILE_SIZE)\n+\n+  private val logDirForAppPath = getAppEventLogDirPath(logBaseDir, appId, appAttemptId)\n+\n+  private var countingOutputStream: Option[CountingOutputStream] = None\n+\n+  // seq and event log path will be updated soon in rollEventLogFile, which `start` will call\n+  private var index: Long = 0L\n+  private var currentEventLogFilePath: Path = _\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    if (fileSystem.exists(logDirForAppPath) && shouldOverwrite) {\n+      fileSystem.delete(logDirForAppPath, true)\n+    }\n+\n+    if (fileSystem.exists(logDirForAppPath)) {\n+      throw new IOException(s\"Target log directory already exists ($logDirForAppPath)\")\n+    }\n+\n+    fileSystem.mkdirs(logDirForAppPath, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+    createAppStatusFile(inProgress = true)\n+    rollEventLogFile()\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    writer.foreach { w =>\n+      val currentLen = countingOutputStream.get.getBytesWritten\n+      if (currentLen + eventJson.length > eventFileMaxLength) {\n+        rollEventLogFile()\n+      }\n+    }\n+\n+    writeJson(eventJson, flushLogger)\n+  }\n+\n+  private def rollEventLogFile(): Unit = {\n+    closeWriter()\n+\n+    index += 1\n+    currentEventLogFilePath = getEventLogFilePath(logDirForAppPath, appId, appAttemptId, index,\n+      compressionCodecName)\n+\n+    initLogFile(currentEventLogFilePath) { os =>\n+      countingOutputStream = Some(new CountingOutputStream(os))\n+      new PrintWriter(\n+        new OutputStreamWriter(countingOutputStream.get, StandardCharsets.UTF_8))\n+    }\n+  }\n+\n+  override def stop(): Unit = {\n+    closeWriter()\n+    val appStatusPathIncomplete = getAppStatusFilePath(logDirForAppPath, appId, appAttemptId,\n+      inProgress = true)\n+    val appStatusPathComplete = getAppStatusFilePath(logDirForAppPath, appId, appAttemptId,\n+      inProgress = false)\n+    renameFile(appStatusPathIncomplete, appStatusPathComplete, overwrite = true)\n+  }\n+\n+  override def logPath: String = logDirForAppPath.toString",
    "line": 356
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "They're used in test suites - we need to provide unified way to get log path between single file reader and rolling files reader.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-26T20:35:50Z",
    "diffHunk": "@@ -0,0 +1,414 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io._\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[history] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  // Only defined if the file system scheme is not local\n+  protected var hadoopDataStream: Option[FSDataOutputStream] = None\n+  protected var writer: Option[PrintWriter] = None\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path)(fnSetupWriter: OutputStream => PrintWriter): Unit = {\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(\n+          SparkHadoopUtil.createFile(fileSystem, path, sparkConf.get(EVENT_LOG_ALLOW_EC)))\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+      writer = Some(fnSetupWriter(bstream))\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def writeJson(json: String, flushLogger: Boolean = false): Unit = {\n+    // scalastyle:off println\n+    writer.foreach(_.println(json))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  protected def closeWriter(): Unit = {\n+    writer.foreach(_.close())\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it but support setTimes() instead; it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  /** initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def apply(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  override val logPath: String = SingleEventLogFileWriter.getLogPath(logBaseDir, appId,\n+    appAttemptId, compressionCodecName)\n+\n+  private val inProgressPath = logPath + EventLogFileWriter.IN_PROGRESS\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    initLogFile(new Path(inProgressPath)) { os =>\n+      new PrintWriter(new OutputStreamWriter(os, StandardCharsets.UTF_8))\n+    }\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    writeJson(eventJson, flushLogger)\n+  }\n+\n+  /**\n+   * Stop logging events. The event log file will be renamed so that it loses the\n+   * \".inprogress\" suffix.\n+   */\n+  override def stop(): Unit = {\n+    closeWriter()\n+    renameFile(new Path(inProgressPath), new Path(logPath), shouldOverwrite)\n+  }\n+}\n+\n+object SingleEventLogFileWriter {\n+  /**\n+   * Return a file-system-safe path to the log file for the given application.\n+   *\n+   * Note that because we currently only create a single log file for each application,\n+   * we must encode all the information needed to parse this event log in the file name\n+   * instead of within the file itself. Otherwise, if the file is compressed, for instance,\n+   * we won't know which codec to use to decompress the metadata needed to open the file in\n+   * the first place.\n+   *\n+   * The log file name will identify the compression codec used for the contents, if any.\n+   * For example, app_123 for an uncompressed log, app_123.lzf for an LZF-compressed log.\n+   *\n+   * @param logBaseDir Directory where the log file will be written.\n+   * @param appId A unique app ID.\n+   * @param appAttemptId A unique attempt id of appId. May be the empty string.\n+   * @param compressionCodecName Name to identify the codec used to compress the contents\n+   *                             of the log, or None if compression is not enabled.\n+   * @return A path which consists of file-system-safe characters.\n+   */\n+  def getLogPath(\n+      logBaseDir: URI,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      compressionCodecName: Option[String] = None): String = {\n+    val codec = compressionCodecName.map(\".\" + _).getOrElse(\"\")\n+    new Path(logBaseDir).toString.stripSuffix(\"/\") + \"/\" +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId) + codec\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into multiple log files, rolled over via configured size.\n+ *\n+ * The class creates one directory per application, and stores event log files as well as\n+ * metadata files. The name of directory and files in the directory would follow:\n+ *\n+ * - The name of directory: eventlog_v2_appId(_[appAttemptId])\n+ * - The prefix of name on event files: events_[index]_[appId](_[appAttemptId])(.[codec])\n+ *   - \"index\" would be monotonically increasing value (say, sequence)\n+ * - The name of metadata (app. status) file name: appstatus_[appId](_[appAttemptId])(.inprogress)\n+ *\n+ * The writer will roll over the event log file when configured size is reached. Note that the\n+ * writer doesn't check the size on file being open for write: the writer tracks the count of bytes\n+ * written before compression is applied.\n+ *\n+ * For metadata files, the class will leverage zero-byte file, as it provides minimized cost.\n+ */\n+class RollingEventLogFilesWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  import RollingEventLogFilesWriter._\n+\n+  private val eventFileMaxLength = sparkConf.get(EVENT_LOG_ROLLED_EVENT_LOG_MAX_FILE_SIZE)\n+\n+  private val logDirForAppPath = getAppEventLogDirPath(logBaseDir, appId, appAttemptId)\n+\n+  private var countingOutputStream: Option[CountingOutputStream] = None\n+\n+  // seq and event log path will be updated soon in rollEventLogFile, which `start` will call\n+  private var index: Long = 0L\n+  private var currentEventLogFilePath: Path = _\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    if (fileSystem.exists(logDirForAppPath) && shouldOverwrite) {\n+      fileSystem.delete(logDirForAppPath, true)\n+    }\n+\n+    if (fileSystem.exists(logDirForAppPath)) {\n+      throw new IOException(s\"Target log directory already exists ($logDirForAppPath)\")\n+    }\n+\n+    fileSystem.mkdirs(logDirForAppPath, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+    createAppStatusFile(inProgress = true)\n+    rollEventLogFile()\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    writer.foreach { w =>\n+      val currentLen = countingOutputStream.get.getBytesWritten\n+      if (currentLen + eventJson.length > eventFileMaxLength) {\n+        rollEventLogFile()\n+      }\n+    }\n+\n+    writeJson(eventJson, flushLogger)\n+  }\n+\n+  private def rollEventLogFile(): Unit = {\n+    closeWriter()\n+\n+    index += 1\n+    currentEventLogFilePath = getEventLogFilePath(logDirForAppPath, appId, appAttemptId, index,\n+      compressionCodecName)\n+\n+    initLogFile(currentEventLogFilePath) { os =>\n+      countingOutputStream = Some(new CountingOutputStream(os))\n+      new PrintWriter(\n+        new OutputStreamWriter(countingOutputStream.get, StandardCharsets.UTF_8))\n+    }\n+  }\n+\n+  override def stop(): Unit = {\n+    closeWriter()\n+    val appStatusPathIncomplete = getAppStatusFilePath(logDirForAppPath, appId, appAttemptId,\n+      inProgress = true)\n+    val appStatusPathComplete = getAppStatusFilePath(logDirForAppPath, appId, appAttemptId,\n+      inProgress = false)\n+    renameFile(appStatusPathIncomplete, appStatusPathComplete, overwrite = true)\n+  }\n+\n+  override def logPath: String = logDirForAppPath.toString",
    "line": 356
  }, {
    "author": {
      "login": "Ngone51"
    },
    "body": "Then, would be better to mark as \"For tests only\"",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-27T01:23:35Z",
    "diffHunk": "@@ -0,0 +1,414 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io._\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[history] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  // Only defined if the file system scheme is not local\n+  protected var hadoopDataStream: Option[FSDataOutputStream] = None\n+  protected var writer: Option[PrintWriter] = None\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path)(fnSetupWriter: OutputStream => PrintWriter): Unit = {\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(\n+          SparkHadoopUtil.createFile(fileSystem, path, sparkConf.get(EVENT_LOG_ALLOW_EC)))\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+      writer = Some(fnSetupWriter(bstream))\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def writeJson(json: String, flushLogger: Boolean = false): Unit = {\n+    // scalastyle:off println\n+    writer.foreach(_.println(json))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  protected def closeWriter(): Unit = {\n+    writer.foreach(_.close())\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it but support setTimes() instead; it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  /** initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def apply(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  override val logPath: String = SingleEventLogFileWriter.getLogPath(logBaseDir, appId,\n+    appAttemptId, compressionCodecName)\n+\n+  private val inProgressPath = logPath + EventLogFileWriter.IN_PROGRESS\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    initLogFile(new Path(inProgressPath)) { os =>\n+      new PrintWriter(new OutputStreamWriter(os, StandardCharsets.UTF_8))\n+    }\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    writeJson(eventJson, flushLogger)\n+  }\n+\n+  /**\n+   * Stop logging events. The event log file will be renamed so that it loses the\n+   * \".inprogress\" suffix.\n+   */\n+  override def stop(): Unit = {\n+    closeWriter()\n+    renameFile(new Path(inProgressPath), new Path(logPath), shouldOverwrite)\n+  }\n+}\n+\n+object SingleEventLogFileWriter {\n+  /**\n+   * Return a file-system-safe path to the log file for the given application.\n+   *\n+   * Note that because we currently only create a single log file for each application,\n+   * we must encode all the information needed to parse this event log in the file name\n+   * instead of within the file itself. Otherwise, if the file is compressed, for instance,\n+   * we won't know which codec to use to decompress the metadata needed to open the file in\n+   * the first place.\n+   *\n+   * The log file name will identify the compression codec used for the contents, if any.\n+   * For example, app_123 for an uncompressed log, app_123.lzf for an LZF-compressed log.\n+   *\n+   * @param logBaseDir Directory where the log file will be written.\n+   * @param appId A unique app ID.\n+   * @param appAttemptId A unique attempt id of appId. May be the empty string.\n+   * @param compressionCodecName Name to identify the codec used to compress the contents\n+   *                             of the log, or None if compression is not enabled.\n+   * @return A path which consists of file-system-safe characters.\n+   */\n+  def getLogPath(\n+      logBaseDir: URI,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      compressionCodecName: Option[String] = None): String = {\n+    val codec = compressionCodecName.map(\".\" + _).getOrElse(\"\")\n+    new Path(logBaseDir).toString.stripSuffix(\"/\") + \"/\" +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId) + codec\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into multiple log files, rolled over via configured size.\n+ *\n+ * The class creates one directory per application, and stores event log files as well as\n+ * metadata files. The name of directory and files in the directory would follow:\n+ *\n+ * - The name of directory: eventlog_v2_appId(_[appAttemptId])\n+ * - The prefix of name on event files: events_[index]_[appId](_[appAttemptId])(.[codec])\n+ *   - \"index\" would be monotonically increasing value (say, sequence)\n+ * - The name of metadata (app. status) file name: appstatus_[appId](_[appAttemptId])(.inprogress)\n+ *\n+ * The writer will roll over the event log file when configured size is reached. Note that the\n+ * writer doesn't check the size on file being open for write: the writer tracks the count of bytes\n+ * written before compression is applied.\n+ *\n+ * For metadata files, the class will leverage zero-byte file, as it provides minimized cost.\n+ */\n+class RollingEventLogFilesWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  import RollingEventLogFilesWriter._\n+\n+  private val eventFileMaxLength = sparkConf.get(EVENT_LOG_ROLLED_EVENT_LOG_MAX_FILE_SIZE)\n+\n+  private val logDirForAppPath = getAppEventLogDirPath(logBaseDir, appId, appAttemptId)\n+\n+  private var countingOutputStream: Option[CountingOutputStream] = None\n+\n+  // seq and event log path will be updated soon in rollEventLogFile, which `start` will call\n+  private var index: Long = 0L\n+  private var currentEventLogFilePath: Path = _\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    if (fileSystem.exists(logDirForAppPath) && shouldOverwrite) {\n+      fileSystem.delete(logDirForAppPath, true)\n+    }\n+\n+    if (fileSystem.exists(logDirForAppPath)) {\n+      throw new IOException(s\"Target log directory already exists ($logDirForAppPath)\")\n+    }\n+\n+    fileSystem.mkdirs(logDirForAppPath, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+    createAppStatusFile(inProgress = true)\n+    rollEventLogFile()\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    writer.foreach { w =>\n+      val currentLen = countingOutputStream.get.getBytesWritten\n+      if (currentLen + eventJson.length > eventFileMaxLength) {\n+        rollEventLogFile()\n+      }\n+    }\n+\n+    writeJson(eventJson, flushLogger)\n+  }\n+\n+  private def rollEventLogFile(): Unit = {\n+    closeWriter()\n+\n+    index += 1\n+    currentEventLogFilePath = getEventLogFilePath(logDirForAppPath, appId, appAttemptId, index,\n+      compressionCodecName)\n+\n+    initLogFile(currentEventLogFilePath) { os =>\n+      countingOutputStream = Some(new CountingOutputStream(os))\n+      new PrintWriter(\n+        new OutputStreamWriter(countingOutputStream.get, StandardCharsets.UTF_8))\n+    }\n+  }\n+\n+  override def stop(): Unit = {\n+    closeWriter()\n+    val appStatusPathIncomplete = getAppStatusFilePath(logDirForAppPath, appId, appAttemptId,\n+      inProgress = true)\n+    val appStatusPathComplete = getAppStatusFilePath(logDirForAppPath, appId, appAttemptId,\n+      inProgress = false)\n+    renameFile(appStatusPathIncomplete, appStatusPathComplete, overwrite = true)\n+  }\n+\n+  override def logPath: String = logDirForAppPath.toString",
    "line": 356
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "This comes from the old code, but use `//` for these kind of comments.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-10-02T16:25:13Z",
    "diffHunk": "@@ -0,0 +1,414 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io._\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[history] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  // Only defined if the file system scheme is not local\n+  protected var hadoopDataStream: Option[FSDataOutputStream] = None\n+  protected var writer: Option[PrintWriter] = None\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path)(fnSetupWriter: OutputStream => PrintWriter): Unit = {\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844)."
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "This comes from the old code but seems to be somewhat incorrect. If the file exists and `shouldOverwrite` is false, this will not delete the file, as expected.\r\n\r\nBut later, when you do e.g. `new FileOutputStream(uri.getPath)`, the existing file will be overwritten on most file systems (NTFS being the exception, I think).\r\n\r\nNo need to fix that here, though. Also because it's unlikely to happen given how we name files.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-10-02T16:28:10Z",
    "diffHunk": "@@ -0,0 +1,414 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io._\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[history] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  // Only defined if the file system scheme is not local\n+  protected var hadoopDataStream: Option[FSDataOutputStream] = None\n+  protected var writer: Option[PrintWriter] = None\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path)(fnSetupWriter: OutputStream => PrintWriter): Unit = {\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {",
    "line": 83
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "That's a good point. Maybe we haven't defined proper behavior of this case: would we want to fail the application?\r\n\r\nThere might be similar case for if shouldOverwrite is true and fileSystem.delete() returns false. I see fileSystem.delete() will mostly throw IOException when it fails to delete, but at least in javadoc, having 'false' as return value when calling fileSystem.delete() may not only say the file doesn't exist. Javadoc doesn't guarantee that.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-10-02T22:56:39Z",
    "diffHunk": "@@ -0,0 +1,414 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io._\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[history] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  // Only defined if the file system scheme is not local\n+  protected var hadoopDataStream: Option[FSDataOutputStream] = None\n+  protected var writer: Option[PrintWriter] = None\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path)(fnSetupWriter: OutputStream => PrintWriter): Unit = {\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {",
    "line": 83
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "Ngone51"
    },
    "body": "`a` -> `an` ?",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-10-03T13:27:22Z",
    "diffHunk": "@@ -0,0 +1,414 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io._\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[history] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  // Only defined if the file system scheme is not local\n+  protected var hadoopDataStream: Option[FSDataOutputStream] = None\n+  protected var writer: Option[PrintWriter] = None\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path)(fnSetupWriter: OutputStream => PrintWriter): Unit = {\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(\n+          SparkHadoopUtil.createFile(fileSystem, path, sparkConf.get(EVENT_LOG_ALLOW_EC)))\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+      writer = Some(fnSetupWriter(bstream))\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def writeJson(json: String, flushLogger: Boolean = false): Unit = {\n+    // scalastyle:off println\n+    writer.foreach(_.println(json))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  protected def closeWriter(): Unit = {\n+    writer.foreach(_.close())\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it but support setTimes() instead; it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  /** initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log. for tests only. */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def apply(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  override val logPath: String = SingleEventLogFileWriter.getLogPath(logBaseDir, appId,\n+    appAttemptId, compressionCodecName)\n+\n+  private val inProgressPath = logPath + EventLogFileWriter.IN_PROGRESS\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    initLogFile(new Path(inProgressPath)) { os =>\n+      new PrintWriter(new OutputStreamWriter(os, StandardCharsets.UTF_8))\n+    }\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    writeJson(eventJson, flushLogger)\n+  }\n+\n+  /**\n+   * Stop logging events. The event log file will be renamed so that it loses the\n+   * \".inprogress\" suffix.\n+   */\n+  override def stop(): Unit = {\n+    closeWriter()\n+    renameFile(new Path(inProgressPath), new Path(logPath), shouldOverwrite)\n+  }\n+}\n+\n+object SingleEventLogFileWriter {\n+  /**\n+   * Return a file-system-safe path to the log file for the given application.\n+   *\n+   * Note that because we currently only create a single log file for each application,\n+   * we must encode all the information needed to parse this event log in the file name\n+   * instead of within the file itself. Otherwise, if the file is compressed, for instance,\n+   * we won't know which codec to use to decompress the metadata needed to open the file in\n+   * the first place.\n+   *\n+   * The log file name will identify the compression codec used for the contents, if any.\n+   * For example, app_123 for an uncompressed log, app_123.lzf for an LZF-compressed log.\n+   *\n+   * @param logBaseDir Directory where the log file will be written.\n+   * @param appId A unique app ID.\n+   * @param appAttemptId A unique attempt id of appId. May be the empty string.\n+   * @param compressionCodecName Name to identify the codec used to compress the contents\n+   *                             of the log, or None if compression is not enabled.\n+   * @return A path which consists of file-system-safe characters.\n+   */\n+  def getLogPath(\n+      logBaseDir: URI,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      compressionCodecName: Option[String] = None): String = {\n+    val codec = compressionCodecName.map(\".\" + _).getOrElse(\"\")\n+    new Path(logBaseDir).toString.stripSuffix(\"/\") + \"/\" +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId) + codec\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into multiple log files, rolled over via configured size.\n+ *\n+ * The class creates one directory per application, and stores event log files as well as\n+ * metadata files. The name of directory and files in the directory would follow:\n+ *\n+ * - The name of directory: eventlog_v2_appId(_[appAttemptId])\n+ * - The prefix of name on event files: events_[index]_[appId](_[appAttemptId])(.[codec])\n+ *   - \"index\" would be monotonically increasing value (say, sequence)\n+ * - The name of metadata (app. status) file name: appstatus_[appId](_[appAttemptId])(.inprogress)\n+ *\n+ * The writer will roll over the event log file when configured size is reached. Note that the\n+ * writer doesn't check the size on file being open for write: the writer tracks the count of bytes\n+ * written before compression is applied.\n+ *\n+ * For metadata files, the class will leverage zero-byte file, as it provides minimized cost.\n+ */\n+class RollingEventLogFilesWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  import RollingEventLogFilesWriter._\n+\n+  private val eventFileMaxLength = sparkConf.get(EVENT_LOG_ROLLED_EVENT_LOG_MAX_FILE_SIZE)\n+\n+  private val logDirForAppPath = getAppEventLogDirPath(logBaseDir, appId, appAttemptId)\n+\n+  private var countingOutputStream: Option[CountingOutputStream] = None\n+\n+  // seq and event log path will be updated soon in rollEventLogFile, which `start` will call\n+  private var index: Long = 0L\n+  private var currentEventLogFilePath: Path = _\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    if (fileSystem.exists(logDirForAppPath) && shouldOverwrite) {\n+      fileSystem.delete(logDirForAppPath, true)\n+    }\n+\n+    if (fileSystem.exists(logDirForAppPath)) {\n+      throw new IOException(s\"Target log directory already exists ($logDirForAppPath)\")\n+    }\n+\n+    fileSystem.mkdirs(logDirForAppPath, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+    createAppStatusFile(inProgress = true)\n+    rollEventLogFile()\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    writer.foreach { w =>\n+      val currentLen = countingOutputStream.get.getBytesWritten\n+      if (currentLen + eventJson.length > eventFileMaxLength) {\n+        rollEventLogFile()\n+      }\n+    }\n+\n+    writeJson(eventJson, flushLogger)\n+  }\n+\n+  private def rollEventLogFile(): Unit = {\n+    closeWriter()\n+\n+    index += 1\n+    currentEventLogFilePath = getEventLogFilePath(logDirForAppPath, appId, appAttemptId, index,\n+      compressionCodecName)\n+\n+    initLogFile(currentEventLogFilePath) { os =>\n+      countingOutputStream = Some(new CountingOutputStream(os))\n+      new PrintWriter(\n+        new OutputStreamWriter(countingOutputStream.get, StandardCharsets.UTF_8))\n+    }\n+  }\n+\n+  override def stop(): Unit = {\n+    closeWriter()\n+    val appStatusPathIncomplete = getAppStatusFilePath(logDirForAppPath, appId, appAttemptId,\n+      inProgress = true)\n+    val appStatusPathComplete = getAppStatusFilePath(logDirForAppPath, appId, appAttemptId,\n+      inProgress = false)\n+    renameFile(appStatusPathIncomplete, appStatusPathComplete, overwrite = true)\n+  }\n+\n+  override def logPath: String = logDirForAppPath.toString\n+\n+  private def createAppStatusFile(inProgress: Boolean): Unit = {\n+    val appStatusPath = getAppStatusFilePath(logDirForAppPath, appId, appAttemptId, inProgress)\n+    val outputStream = fileSystem.create(appStatusPath)\n+    // we intentionally create zero-byte file to minimize the cost\n+    outputStream.close()\n+  }\n+}\n+\n+object RollingEventLogFilesWriter {\n+  private val EVENT_LOG_DIR_NAME_PREFIX = \"eventlog_v2_\"\n+  private val EVENT_LOG_FILE_NAME_PREFIX = \"events_\"\n+  private val APPSTATUS_FILE_NAME_PREFIX = \"appstatus_\"\n+\n+  def getAppEventLogDirPath(logBaseDir: URI, appId: String, appAttemptId: Option[String]): Path =\n+    new Path(new Path(logBaseDir), EVENT_LOG_DIR_NAME_PREFIX +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId))\n+\n+  def getAppStatusFilePath(\n+      appLogDir: Path,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      inProgress: Boolean): Path = {\n+    val base = APPSTATUS_FILE_NAME_PREFIX +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId)\n+    val name = if (inProgress) base + EventLogFileWriter.IN_PROGRESS else base\n+    new Path(appLogDir, name)\n+  }\n+\n+  def getEventLogFilePath(\n+      appLogDir: Path,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      index: Long,\n+      codecName: Option[String]): Path = {\n+    val base = s\"${EVENT_LOG_FILE_NAME_PREFIX}${index}_\" +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId)\n+    val codec = codecName.map(\".\" + _).getOrElse(\"\")\n+    new Path(appLogDir, base + codec)\n+  }\n+\n+  def isEventLogDir(status: FileStatus): Boolean = {\n+    status.isDirectory && status.getPath.getName.startsWith(EVENT_LOG_DIR_NAME_PREFIX)\n+  }\n+\n+  def isEventLogFile(status: FileStatus): Boolean = {\n+    status.isFile && status.getPath.getName.startsWith(EVENT_LOG_FILE_NAME_PREFIX)\n+  }\n+\n+  def isAppStatusFile(status: FileStatus): Boolean = {\n+    status.isFile && status.getPath.getName.startsWith(APPSTATUS_FILE_NAME_PREFIX)\n+  }\n+\n+  def getIndex(eventLogFileName: String): Long = {\n+    require(eventLogFileName.startsWith(EVENT_LOG_FILE_NAME_PREFIX), \"Not a event log file!\")"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "Ngone51"
    },
    "body": "`seq` ?",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-10-03T13:32:45Z",
    "diffHunk": "@@ -0,0 +1,414 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io._\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[history] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  // Only defined if the file system scheme is not local\n+  protected var hadoopDataStream: Option[FSDataOutputStream] = None\n+  protected var writer: Option[PrintWriter] = None\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path)(fnSetupWriter: OutputStream => PrintWriter): Unit = {\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(\n+          SparkHadoopUtil.createFile(fileSystem, path, sparkConf.get(EVENT_LOG_ALLOW_EC)))\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+      writer = Some(fnSetupWriter(bstream))\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def writeJson(json: String, flushLogger: Boolean = false): Unit = {\n+    // scalastyle:off println\n+    writer.foreach(_.println(json))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  protected def closeWriter(): Unit = {\n+    writer.foreach(_.close())\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it but support setTimes() instead; it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  /** initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log. for tests only. */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def apply(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  override val logPath: String = SingleEventLogFileWriter.getLogPath(logBaseDir, appId,\n+    appAttemptId, compressionCodecName)\n+\n+  private val inProgressPath = logPath + EventLogFileWriter.IN_PROGRESS\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    initLogFile(new Path(inProgressPath)) { os =>\n+      new PrintWriter(new OutputStreamWriter(os, StandardCharsets.UTF_8))\n+    }\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    writeJson(eventJson, flushLogger)\n+  }\n+\n+  /**\n+   * Stop logging events. The event log file will be renamed so that it loses the\n+   * \".inprogress\" suffix.\n+   */\n+  override def stop(): Unit = {\n+    closeWriter()\n+    renameFile(new Path(inProgressPath), new Path(logPath), shouldOverwrite)\n+  }\n+}\n+\n+object SingleEventLogFileWriter {\n+  /**\n+   * Return a file-system-safe path to the log file for the given application.\n+   *\n+   * Note that because we currently only create a single log file for each application,\n+   * we must encode all the information needed to parse this event log in the file name\n+   * instead of within the file itself. Otherwise, if the file is compressed, for instance,\n+   * we won't know which codec to use to decompress the metadata needed to open the file in\n+   * the first place.\n+   *\n+   * The log file name will identify the compression codec used for the contents, if any.\n+   * For example, app_123 for an uncompressed log, app_123.lzf for an LZF-compressed log.\n+   *\n+   * @param logBaseDir Directory where the log file will be written.\n+   * @param appId A unique app ID.\n+   * @param appAttemptId A unique attempt id of appId. May be the empty string.\n+   * @param compressionCodecName Name to identify the codec used to compress the contents\n+   *                             of the log, or None if compression is not enabled.\n+   * @return A path which consists of file-system-safe characters.\n+   */\n+  def getLogPath(\n+      logBaseDir: URI,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      compressionCodecName: Option[String] = None): String = {\n+    val codec = compressionCodecName.map(\".\" + _).getOrElse(\"\")\n+    new Path(logBaseDir).toString.stripSuffix(\"/\") + \"/\" +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId) + codec\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into multiple log files, rolled over via configured size.\n+ *\n+ * The class creates one directory per application, and stores event log files as well as\n+ * metadata files. The name of directory and files in the directory would follow:\n+ *\n+ * - The name of directory: eventlog_v2_appId(_[appAttemptId])\n+ * - The prefix of name on event files: events_[index]_[appId](_[appAttemptId])(.[codec])\n+ *   - \"index\" would be monotonically increasing value (say, sequence)\n+ * - The name of metadata (app. status) file name: appstatus_[appId](_[appAttemptId])(.inprogress)\n+ *\n+ * The writer will roll over the event log file when configured size is reached. Note that the\n+ * writer doesn't check the size on file being open for write: the writer tracks the count of bytes\n+ * written before compression is applied.\n+ *\n+ * For metadata files, the class will leverage zero-byte file, as it provides minimized cost.\n+ */\n+class RollingEventLogFilesWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  import RollingEventLogFilesWriter._\n+\n+  private val eventFileMaxLength = sparkConf.get(EVENT_LOG_ROLLED_EVENT_LOG_MAX_FILE_SIZE)\n+\n+  private val logDirForAppPath = getAppEventLogDirPath(logBaseDir, appId, appAttemptId)\n+\n+  private var countingOutputStream: Option[CountingOutputStream] = None\n+\n+  // seq and event log path will be updated soon in rollEventLogFile, which `start` will call"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Sigh. Now I'm regretting I shouldn't either use abbr or change the term. Nice finding. Will change. ",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-10-03T20:40:07Z",
    "diffHunk": "@@ -0,0 +1,414 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io._\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[history] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  // Only defined if the file system scheme is not local\n+  protected var hadoopDataStream: Option[FSDataOutputStream] = None\n+  protected var writer: Option[PrintWriter] = None\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path)(fnSetupWriter: OutputStream => PrintWriter): Unit = {\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(\n+          SparkHadoopUtil.createFile(fileSystem, path, sparkConf.get(EVENT_LOG_ALLOW_EC)))\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+      writer = Some(fnSetupWriter(bstream))\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def writeJson(json: String, flushLogger: Boolean = false): Unit = {\n+    // scalastyle:off println\n+    writer.foreach(_.println(json))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  protected def closeWriter(): Unit = {\n+    writer.foreach(_.close())\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it but support setTimes() instead; it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  /** initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log. for tests only. */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def apply(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  override val logPath: String = SingleEventLogFileWriter.getLogPath(logBaseDir, appId,\n+    appAttemptId, compressionCodecName)\n+\n+  private val inProgressPath = logPath + EventLogFileWriter.IN_PROGRESS\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    initLogFile(new Path(inProgressPath)) { os =>\n+      new PrintWriter(new OutputStreamWriter(os, StandardCharsets.UTF_8))\n+    }\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    writeJson(eventJson, flushLogger)\n+  }\n+\n+  /**\n+   * Stop logging events. The event log file will be renamed so that it loses the\n+   * \".inprogress\" suffix.\n+   */\n+  override def stop(): Unit = {\n+    closeWriter()\n+    renameFile(new Path(inProgressPath), new Path(logPath), shouldOverwrite)\n+  }\n+}\n+\n+object SingleEventLogFileWriter {\n+  /**\n+   * Return a file-system-safe path to the log file for the given application.\n+   *\n+   * Note that because we currently only create a single log file for each application,\n+   * we must encode all the information needed to parse this event log in the file name\n+   * instead of within the file itself. Otherwise, if the file is compressed, for instance,\n+   * we won't know which codec to use to decompress the metadata needed to open the file in\n+   * the first place.\n+   *\n+   * The log file name will identify the compression codec used for the contents, if any.\n+   * For example, app_123 for an uncompressed log, app_123.lzf for an LZF-compressed log.\n+   *\n+   * @param logBaseDir Directory where the log file will be written.\n+   * @param appId A unique app ID.\n+   * @param appAttemptId A unique attempt id of appId. May be the empty string.\n+   * @param compressionCodecName Name to identify the codec used to compress the contents\n+   *                             of the log, or None if compression is not enabled.\n+   * @return A path which consists of file-system-safe characters.\n+   */\n+  def getLogPath(\n+      logBaseDir: URI,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      compressionCodecName: Option[String] = None): String = {\n+    val codec = compressionCodecName.map(\".\" + _).getOrElse(\"\")\n+    new Path(logBaseDir).toString.stripSuffix(\"/\") + \"/\" +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId) + codec\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into multiple log files, rolled over via configured size.\n+ *\n+ * The class creates one directory per application, and stores event log files as well as\n+ * metadata files. The name of directory and files in the directory would follow:\n+ *\n+ * - The name of directory: eventlog_v2_appId(_[appAttemptId])\n+ * - The prefix of name on event files: events_[index]_[appId](_[appAttemptId])(.[codec])\n+ *   - \"index\" would be monotonically increasing value (say, sequence)\n+ * - The name of metadata (app. status) file name: appstatus_[appId](_[appAttemptId])(.inprogress)\n+ *\n+ * The writer will roll over the event log file when configured size is reached. Note that the\n+ * writer doesn't check the size on file being open for write: the writer tracks the count of bytes\n+ * written before compression is applied.\n+ *\n+ * For metadata files, the class will leverage zero-byte file, as it provides minimized cost.\n+ */\n+class RollingEventLogFilesWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  import RollingEventLogFilesWriter._\n+\n+  private val eventFileMaxLength = sparkConf.get(EVENT_LOG_ROLLED_EVENT_LOG_MAX_FILE_SIZE)\n+\n+  private val logDirForAppPath = getAppEventLogDirPath(logBaseDir, appId, appAttemptId)\n+\n+  private var countingOutputStream: Option[CountingOutputStream] = None\n+\n+  // seq and event log path will be updated soon in rollEventLogFile, which `start` will call"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "Ngone51"
    },
    "body": "Still, `seq` or `index` ?",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-10-03T15:08:45Z",
    "diffHunk": "@@ -0,0 +1,414 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io._\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[history] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  // Only defined if the file system scheme is not local\n+  protected var hadoopDataStream: Option[FSDataOutputStream] = None\n+  protected var writer: Option[PrintWriter] = None\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path)(fnSetupWriter: OutputStream => PrintWriter): Unit = {\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(\n+          SparkHadoopUtil.createFile(fileSystem, path, sparkConf.get(EVENT_LOG_ALLOW_EC)))\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+      writer = Some(fnSetupWriter(bstream))\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def writeJson(json: String, flushLogger: Boolean = false): Unit = {\n+    // scalastyle:off println\n+    writer.foreach(_.println(json))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  protected def closeWriter(): Unit = {\n+    writer.foreach(_.close())\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it but support setTimes() instead; it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  /** initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log. for tests only. */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def apply(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  override val logPath: String = SingleEventLogFileWriter.getLogPath(logBaseDir, appId,\n+    appAttemptId, compressionCodecName)\n+\n+  private val inProgressPath = logPath + EventLogFileWriter.IN_PROGRESS\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    initLogFile(new Path(inProgressPath)) { os =>\n+      new PrintWriter(new OutputStreamWriter(os, StandardCharsets.UTF_8))\n+    }\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    writeJson(eventJson, flushLogger)\n+  }\n+\n+  /**\n+   * Stop logging events. The event log file will be renamed so that it loses the\n+   * \".inprogress\" suffix.\n+   */\n+  override def stop(): Unit = {\n+    closeWriter()\n+    renameFile(new Path(inProgressPath), new Path(logPath), shouldOverwrite)\n+  }\n+}\n+\n+object SingleEventLogFileWriter {\n+  /**\n+   * Return a file-system-safe path to the log file for the given application.\n+   *\n+   * Note that because we currently only create a single log file for each application,\n+   * we must encode all the information needed to parse this event log in the file name\n+   * instead of within the file itself. Otherwise, if the file is compressed, for instance,\n+   * we won't know which codec to use to decompress the metadata needed to open the file in\n+   * the first place.\n+   *\n+   * The log file name will identify the compression codec used for the contents, if any.\n+   * For example, app_123 for an uncompressed log, app_123.lzf for an LZF-compressed log.\n+   *\n+   * @param logBaseDir Directory where the log file will be written.\n+   * @param appId A unique app ID.\n+   * @param appAttemptId A unique attempt id of appId. May be the empty string.\n+   * @param compressionCodecName Name to identify the codec used to compress the contents\n+   *                             of the log, or None if compression is not enabled.\n+   * @return A path which consists of file-system-safe characters.\n+   */\n+  def getLogPath(\n+      logBaseDir: URI,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      compressionCodecName: Option[String] = None): String = {\n+    val codec = compressionCodecName.map(\".\" + _).getOrElse(\"\")\n+    new Path(logBaseDir).toString.stripSuffix(\"/\") + \"/\" +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId) + codec\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into multiple log files, rolled over via configured size.\n+ *\n+ * The class creates one directory per application, and stores event log files as well as\n+ * metadata files. The name of directory and files in the directory would follow:\n+ *\n+ * - The name of directory: eventlog_v2_appId(_[appAttemptId])\n+ * - The prefix of name on event files: events_[index]_[appId](_[appAttemptId])(.[codec])\n+ *   - \"index\" would be monotonically increasing value (say, sequence)\n+ * - The name of metadata (app. status) file name: appstatus_[appId](_[appAttemptId])(.inprogress)\n+ *\n+ * The writer will roll over the event log file when configured size is reached. Note that the\n+ * writer doesn't check the size on file being open for write: the writer tracks the count of bytes\n+ * written before compression is applied.\n+ *\n+ * For metadata files, the class will leverage zero-byte file, as it provides minimized cost.\n+ */\n+class RollingEventLogFilesWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  import RollingEventLogFilesWriter._\n+\n+  private val eventFileMaxLength = sparkConf.get(EVENT_LOG_ROLLED_EVENT_LOG_MAX_FILE_SIZE)\n+\n+  private val logDirForAppPath = getAppEventLogDirPath(logBaseDir, appId, appAttemptId)\n+\n+  private var countingOutputStream: Option[CountingOutputStream] = None\n+\n+  // seq and event log path will be updated soon in rollEventLogFile, which `start` will call\n+  private var index: Long = 0L\n+  private var currentEventLogFilePath: Path = _\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    if (fileSystem.exists(logDirForAppPath) && shouldOverwrite) {\n+      fileSystem.delete(logDirForAppPath, true)\n+    }\n+\n+    if (fileSystem.exists(logDirForAppPath)) {\n+      throw new IOException(s\"Target log directory already exists ($logDirForAppPath)\")\n+    }\n+\n+    fileSystem.mkdirs(logDirForAppPath, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+    createAppStatusFile(inProgress = true)\n+    rollEventLogFile()\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    writer.foreach { w =>\n+      val currentLen = countingOutputStream.get.getBytesWritten\n+      if (currentLen + eventJson.length > eventFileMaxLength) {\n+        rollEventLogFile()\n+      }\n+    }\n+\n+    writeJson(eventJson, flushLogger)\n+  }\n+\n+  private def rollEventLogFile(): Unit = {\n+    closeWriter()\n+\n+    index += 1\n+    currentEventLogFilePath = getEventLogFilePath(logDirForAppPath, appId, appAttemptId, index,\n+      compressionCodecName)\n+\n+    initLogFile(currentEventLogFilePath) { os =>\n+      countingOutputStream = Some(new CountingOutputStream(os))\n+      new PrintWriter(\n+        new OutputStreamWriter(countingOutputStream.get, StandardCharsets.UTF_8))\n+    }\n+  }\n+\n+  override def stop(): Unit = {\n+    closeWriter()\n+    val appStatusPathIncomplete = getAppStatusFilePath(logDirForAppPath, appId, appAttemptId,\n+      inProgress = true)\n+    val appStatusPathComplete = getAppStatusFilePath(logDirForAppPath, appId, appAttemptId,\n+      inProgress = false)\n+    renameFile(appStatusPathIncomplete, appStatusPathComplete, overwrite = true)\n+  }\n+\n+  override def logPath: String = logDirForAppPath.toString\n+\n+  private def createAppStatusFile(inProgress: Boolean): Unit = {\n+    val appStatusPath = getAppStatusFilePath(logDirForAppPath, appId, appAttemptId, inProgress)\n+    val outputStream = fileSystem.create(appStatusPath)\n+    // we intentionally create zero-byte file to minimize the cost\n+    outputStream.close()\n+  }\n+}\n+\n+object RollingEventLogFilesWriter {\n+  private val EVENT_LOG_DIR_NAME_PREFIX = \"eventlog_v2_\"\n+  private val EVENT_LOG_FILE_NAME_PREFIX = \"events_\"\n+  private val APPSTATUS_FILE_NAME_PREFIX = \"appstatus_\"\n+\n+  def getAppEventLogDirPath(logBaseDir: URI, appId: String, appAttemptId: Option[String]): Path =\n+    new Path(new Path(logBaseDir), EVENT_LOG_DIR_NAME_PREFIX +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId))\n+\n+  def getAppStatusFilePath(\n+      appLogDir: Path,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      inProgress: Boolean): Path = {\n+    val base = APPSTATUS_FILE_NAME_PREFIX +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId)\n+    val name = if (inProgress) base + EventLogFileWriter.IN_PROGRESS else base\n+    new Path(appLogDir, name)\n+  }\n+\n+  def getEventLogFilePath(\n+      appLogDir: Path,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      index: Long,\n+      codecName: Option[String]): Path = {\n+    val base = s\"${EVENT_LOG_FILE_NAME_PREFIX}${index}_\" +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId)\n+    val codec = codecName.map(\".\" + _).getOrElse(\"\")\n+    new Path(appLogDir, base + codec)\n+  }\n+\n+  def isEventLogDir(status: FileStatus): Boolean = {\n+    status.isDirectory && status.getPath.getName.startsWith(EVENT_LOG_DIR_NAME_PREFIX)\n+  }\n+\n+  def isEventLogFile(status: FileStatus): Boolean = {\n+    status.isFile && status.getPath.getName.startsWith(EVENT_LOG_FILE_NAME_PREFIX)\n+  }\n+\n+  def isAppStatusFile(status: FileStatus): Boolean = {\n+    status.isFile && status.getPath.getName.startsWith(APPSTATUS_FILE_NAME_PREFIX)\n+  }\n+\n+  def getIndex(eventLogFileName: String): Long = {\n+    require(eventLogFileName.startsWith(EVENT_LOG_FILE_NAME_PREFIX), \"Not a event log file!\")\n+    val seq = eventLogFileName.stripPrefix(EVENT_LOG_FILE_NAME_PREFIX).split(\"_\")(0)\n+    seq.toLong"
  }],
  "prId": 25670
}]