[{
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "Hey one thing I realize while looking it this. I'm not sure it says anywhere in any docs that you are supposed to set the master to `yarn-standalone` when initializing a SparkContext for use on yarn. I realize this is unrelated to your PR, but it would be great if you could add that to the YARN documentation /cc @tgraves (maybe I missed it?). Also it's a little confusing calling it `yarn-standalone` - it might make sense to rename this to `yarn-cluster` and `yarn-client` since there is a name collision with \"The standalone cluster manager\". @tgraves do you mind if we rename this?\n",
    "commit": "d428d857971866aa5c8f75970a9655071bf016fd",
    "createdAt": "2014-03-06T03:52:10Z",
    "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io.BufferedReader\n+import java.io.InputStream\n+import java.io.InputStreamReader\n+import java.io.PrintStream\n+\n+import scala.collection.mutable.HashMap\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.JavaConverters._\n+\n+object SparkApp {\n+  val CLIENT = 1\n+  val CLUSTER = 2\n+  val YARN = 1\n+  val STANDALONE = 2\n+  val MESOS = 4\n+  val LOCAL = 8\n+  val ALL_CLUSTER_MGRS = YARN | STANDALONE | MESOS | LOCAL\n+\n+  var clusterManager: Int = LOCAL\n+\n+  def main(args: Array[String]) {\n+    println(\"args: \" + args.toList)\n+    val appArgs = new SparkAppArguments(args)\n+\n+    if (appArgs.master != null) {\n+      if (appArgs.master.startsWith(\"yarn\")) {"
  }, {
    "author": {
      "login": "sryza"
    },
    "body": "This has bothered me too.  Would love to make that change if it's not too late.\n",
    "commit": "d428d857971866aa5c8f75970a9655071bf016fd",
    "createdAt": "2014-03-06T04:10:13Z",
    "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io.BufferedReader\n+import java.io.InputStream\n+import java.io.InputStreamReader\n+import java.io.PrintStream\n+\n+import scala.collection.mutable.HashMap\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.JavaConverters._\n+\n+object SparkApp {\n+  val CLIENT = 1\n+  val CLUSTER = 2\n+  val YARN = 1\n+  val STANDALONE = 2\n+  val MESOS = 4\n+  val LOCAL = 8\n+  val ALL_CLUSTER_MGRS = YARN | STANDALONE | MESOS | LOCAL\n+\n+  var clusterManager: Int = LOCAL\n+\n+  def main(args: Array[String]) {\n+    println(\"args: \" + args.toList)\n+    val appArgs = new SparkAppArguments(args)\n+\n+    if (appArgs.master != null) {\n+      if (appArgs.master.startsWith(\"yarn\")) {"
  }, {
    "author": {
      "login": "pwendell"
    },
    "body": "It depends what @tgraves says but at a minimum I think we could silently support `yarn-standalone` for backwards compatibility but change all the docs to say `yarn-cluster`.\n",
    "commit": "d428d857971866aa5c8f75970a9655071bf016fd",
    "createdAt": "2014-03-06T04:18:38Z",
    "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io.BufferedReader\n+import java.io.InputStream\n+import java.io.InputStreamReader\n+import java.io.PrintStream\n+\n+import scala.collection.mutable.HashMap\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.JavaConverters._\n+\n+object SparkApp {\n+  val CLIENT = 1\n+  val CLUSTER = 2\n+  val YARN = 1\n+  val STANDALONE = 2\n+  val MESOS = 4\n+  val LOCAL = 8\n+  val ALL_CLUSTER_MGRS = YARN | STANDALONE | MESOS | LOCAL\n+\n+  var clusterManager: Int = LOCAL\n+\n+  def main(args: Array[String]) {\n+    println(\"args: \" + args.toList)\n+    val appArgs = new SparkAppArguments(args)\n+\n+    if (appArgs.master != null) {\n+      if (appArgs.master.startsWith(\"yarn\")) {"
  }, {
    "author": {
      "login": "tgravescs"
    },
    "body": "I agree with you the yarn-standalone name is a bit confusing. Its not used in to many places so if we can keep it for backwards compatibility but move to the new one that would be preferable.  \n",
    "commit": "d428d857971866aa5c8f75970a9655071bf016fd",
    "createdAt": "2014-03-06T04:56:55Z",
    "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io.BufferedReader\n+import java.io.InputStream\n+import java.io.InputStreamReader\n+import java.io.PrintStream\n+\n+import scala.collection.mutable.HashMap\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.JavaConverters._\n+\n+object SparkApp {\n+  val CLIENT = 1\n+  val CLUSTER = 2\n+  val YARN = 1\n+  val STANDALONE = 2\n+  val MESOS = 4\n+  val LOCAL = 8\n+  val ALL_CLUSTER_MGRS = YARN | STANDALONE | MESOS | LOCAL\n+\n+  var clusterManager: Int = LOCAL\n+\n+  def main(args: Array[String]) {\n+    println(\"args: \" + args.toList)\n+    val appArgs = new SparkAppArguments(args)\n+\n+    if (appArgs.master != null) {\n+      if (appArgs.master.startsWith(\"yarn\")) {"
  }, {
    "author": {
      "login": "mridulm"
    },
    "body": "yarn-standalone was probably named that way since it was modification of standalone cluster code to get it to run on yarn :-)\nyarn-cluster is slightly misleading as well - since even the yarn client mode runs on the cluster.\n\nI am not sure what is a good alternative - yarn-batch ? or something along those lines ?\n",
    "commit": "d428d857971866aa5c8f75970a9655071bf016fd",
    "createdAt": "2014-03-06T06:18:57Z",
    "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io.BufferedReader\n+import java.io.InputStream\n+import java.io.InputStreamReader\n+import java.io.PrintStream\n+\n+import scala.collection.mutable.HashMap\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.JavaConverters._\n+\n+object SparkApp {\n+  val CLIENT = 1\n+  val CLUSTER = 2\n+  val YARN = 1\n+  val STANDALONE = 2\n+  val MESOS = 4\n+  val LOCAL = 8\n+  val ALL_CLUSTER_MGRS = YARN | STANDALONE | MESOS | LOCAL\n+\n+  var clusterManager: Int = LOCAL\n+\n+  def main(args: Array[String]) {\n+    println(\"args: \" + args.toList)\n+    val appArgs = new SparkAppArguments(args)\n+\n+    if (appArgs.master != null) {\n+      if (appArgs.master.startsWith(\"yarn\")) {"
  }, {
    "author": {
      "login": "sryza"
    },
    "body": "Definitely open to improvements over \"yarn-cluster\", but \"yarn-batch\" doesn't sound right to me because this mode can be used to run long running apps as well like streaming.\n\nAs this is orthogonal to spark-app, and there are a few related changes I'd like to make, I submitted a separate pull request for this.  Let's continue the discussion on https://github.com/apache/spark/pull/95.\n",
    "commit": "d428d857971866aa5c8f75970a9655071bf016fd",
    "createdAt": "2014-03-06T20:40:10Z",
    "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io.BufferedReader\n+import java.io.InputStream\n+import java.io.InputStreamReader\n+import java.io.PrintStream\n+\n+import scala.collection.mutable.HashMap\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.JavaConverters._\n+\n+object SparkApp {\n+  val CLIENT = 1\n+  val CLUSTER = 2\n+  val YARN = 1\n+  val STANDALONE = 2\n+  val MESOS = 4\n+  val LOCAL = 8\n+  val ALL_CLUSTER_MGRS = YARN | STANDALONE | MESOS | LOCAL\n+\n+  var clusterManager: Int = LOCAL\n+\n+  def main(args: Array[String]) {\n+    println(\"args: \" + args.toList)\n+    val appArgs = new SparkAppArguments(args)\n+\n+    if (appArgs.master != null) {\n+      if (appArgs.master.startsWith(\"yarn\")) {"
  }],
  "prId": 86
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "These are created as flags, but since they are mutually exclusive would it make more sense for them to be an `Enumeration`?\nhttp://www.scala-lang.org/api/2.10.2/index.html#scala.Enumeration\n\nI.e. is it possible for something to be both YARN and LOCAL?\n",
    "commit": "d428d857971866aa5c8f75970a9655071bf016fd",
    "createdAt": "2014-03-06T03:55:02Z",
    "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io.BufferedReader\n+import java.io.InputStream\n+import java.io.InputStreamReader\n+import java.io.PrintStream\n+\n+import scala.collection.mutable.HashMap\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.JavaConverters._\n+\n+object SparkApp {\n+  val CLIENT = 1\n+  val CLUSTER = 2\n+  val YARN = 1"
  }, {
    "author": {
      "login": "sryza"
    },
    "body": "It's not possible for something to be both, but there are some options that we want to apply the same way for multiple different cluster managers.\n\nE.g. on both standalone and Mesos mode, --executor-cores should translate to setting the spark.executor.memory Java system property, and with the flags, we can express that with\n-      new Opt(appArgs.executorMemory, STANDALONE | MESOS, CLIENT, null, null, \"spark.executor.memory\"),\n\nI couldn't find a way to do this with Enumerations.  Do you know if there is one?\n",
    "commit": "d428d857971866aa5c8f75970a9655071bf016fd",
    "createdAt": "2014-03-06T04:29:32Z",
    "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io.BufferedReader\n+import java.io.InputStream\n+import java.io.InputStreamReader\n+import java.io.PrintStream\n+\n+import scala.collection.mutable.HashMap\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.JavaConverters._\n+\n+object SparkApp {\n+  val CLIENT = 1\n+  val CLUSTER = 2\n+  val YARN = 1"
  }, {
    "author": {
      "login": "mateiz"
    },
    "body": "At the very least though, separating these two sets of constants to make it clear that they cover different things would help. E.g. give them prefixes, like `MANAGER_LOCAL` and `MODE_CLIENT`, or maybe make the client vs cluster thing a boolean.\n",
    "commit": "d428d857971866aa5c8f75970a9655071bf016fd",
    "createdAt": "2014-03-06T06:12:45Z",
    "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io.BufferedReader\n+import java.io.InputStream\n+import java.io.InputStreamReader\n+import java.io.PrintStream\n+\n+import scala.collection.mutable.HashMap\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.JavaConverters._\n+\n+object SparkApp {\n+  val CLIENT = 1\n+  val CLUSTER = 2\n+  val YARN = 1"
  }],
  "prId": 86
}, {
  "comments": [{
    "author": {
      "login": "mateiz"
    },
    "body": "This should be startsWith because you can do local[4] and stuff like that\n",
    "commit": "d428d857971866aa5c8f75970a9655071bf016fd",
    "createdAt": "2014-03-06T06:08:26Z",
    "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io.BufferedReader\n+import java.io.InputStream\n+import java.io.InputStreamReader\n+import java.io.PrintStream\n+\n+import scala.collection.mutable.HashMap\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.JavaConverters._\n+\n+object SparkApp {\n+  val CLIENT = 1\n+  val CLUSTER = 2\n+  val YARN = 1\n+  val STANDALONE = 2\n+  val MESOS = 4\n+  val LOCAL = 8\n+  val ALL_CLUSTER_MGRS = YARN | STANDALONE | MESOS | LOCAL\n+\n+  var clusterManager: Int = LOCAL\n+\n+  def main(args: Array[String]) {\n+    println(\"args: \" + args.toList)\n+    val appArgs = new SparkAppArguments(args)\n+\n+    if (appArgs.master != null) {\n+      if (appArgs.master.startsWith(\"yarn\")) {\n+        clusterManager = YARN\n+      } else if (appArgs.master.startsWith(\"spark\")) {\n+        clusterManager = STANDALONE\n+      } else if (appArgs.master.startsWith(\"mesos\")) {\n+        clusterManager = MESOS\n+      } else if (appArgs.master == \"local\") {"
  }, {
    "author": {
      "login": "mridulm"
    },
    "body": "So local[<num>] wont work anymore ?\n",
    "commit": "d428d857971866aa5c8f75970a9655071bf016fd",
    "createdAt": "2014-03-06T06:20:27Z",
    "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io.BufferedReader\n+import java.io.InputStream\n+import java.io.InputStreamReader\n+import java.io.PrintStream\n+\n+import scala.collection.mutable.HashMap\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.JavaConverters._\n+\n+object SparkApp {\n+  val CLIENT = 1\n+  val CLUSTER = 2\n+  val YARN = 1\n+  val STANDALONE = 2\n+  val MESOS = 4\n+  val LOCAL = 8\n+  val ALL_CLUSTER_MGRS = YARN | STANDALONE | MESOS | LOCAL\n+\n+  var clusterManager: Int = LOCAL\n+\n+  def main(args: Array[String]) {\n+    println(\"args: \" + args.toList)\n+    val appArgs = new SparkAppArguments(args)\n+\n+    if (appArgs.master != null) {\n+      if (appArgs.master.startsWith(\"yarn\")) {\n+        clusterManager = YARN\n+      } else if (appArgs.master.startsWith(\"spark\")) {\n+        clusterManager = STANDALONE\n+      } else if (appArgs.master.startsWith(\"mesos\")) {\n+        clusterManager = MESOS\n+      } else if (appArgs.master == \"local\") {"
  }],
  "prId": 86
}, {
  "comments": [{
    "author": {
      "login": "mateiz"
    },
    "body": "These helper classes need to be marked `private[spark]`\n",
    "commit": "d428d857971866aa5c8f75970a9655071bf016fd",
    "createdAt": "2014-03-06T06:13:15Z",
    "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io.BufferedReader\n+import java.io.InputStream\n+import java.io.InputStreamReader\n+import java.io.PrintStream\n+\n+import scala.collection.mutable.HashMap\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.JavaConverters._\n+\n+object SparkApp {\n+  val CLIENT = 1\n+  val CLUSTER = 2\n+  val YARN = 1\n+  val STANDALONE = 2\n+  val MESOS = 4\n+  val LOCAL = 8\n+  val ALL_CLUSTER_MGRS = YARN | STANDALONE | MESOS | LOCAL\n+\n+  var clusterManager: Int = LOCAL\n+\n+  def main(args: Array[String]) {\n+    println(\"args: \" + args.toList)\n+    val appArgs = new SparkAppArguments(args)\n+\n+    if (appArgs.master != null) {\n+      if (appArgs.master.startsWith(\"yarn\")) {\n+        clusterManager = YARN\n+      } else if (appArgs.master.startsWith(\"spark\")) {\n+        clusterManager = STANDALONE\n+      } else if (appArgs.master.startsWith(\"mesos\")) {\n+        clusterManager = MESOS\n+      } else if (appArgs.master == \"local\") {\n+        clusterManager = LOCAL\n+      } else {\n+        System.err.println(\"master must start with yarn, mesos, spark, or be local\")\n+        System.exit(1)\n+      }\n+    }\n+\n+    val deployMode = if (appArgs.deployMode == \"client\") CLIENT else CLUSTER\n+    val childEnv = new HashMap[String, String]()\n+    val childClasspath = new ArrayBuffer[String]()\n+    val childArgs = new ArrayBuffer[String]()\n+    childArgs += System.getenv(\"SPARK_HOME\") + \"/bin/spark-class\"\n+\n+    if (clusterManager == MESOS && deployMode == CLUSTER) {\n+      System.err.println(\"Mesos does not support running the driver on the cluster\")\n+      System.exit(1)\n+    }\n+\n+    if (deployMode == CLUSTER && clusterManager == STANDALONE) {\n+      childArgs += \"org.apache.spark.deploy.Client\"\n+      childArgs += \"launch\"\n+      childArgs += appArgs.master\n+      childArgs += appArgs.primaryResource\n+      childArgs += appArgs.mainClass\n+    } else if (deployMode == CLUSTER && clusterManager == YARN) {\n+      childArgs += \"org.apache.spark.deploy.yarn.Client\"\n+      childArgs += \"--jar\"\n+      childArgs += appArgs.primaryResource\n+      childArgs += \"--class\"\n+      childArgs += appArgs.mainClass\n+    } else {\n+      childClasspath += appArgs.primaryResource\n+      childArgs += appArgs.mainClass\n+    }\n+\n+    // TODO: num-executors when not using YARN\n+    val options = List[Opt](\n+      new Opt(appArgs.driverMemory, YARN, CLUSTER, null, \"--master-memory\", null),\n+      new Opt(appArgs.name, YARN, CLUSTER, null, \"--name\", null),\n+      new Opt(appArgs.queue, YARN, CLUSTER, null, \"--queue\", null),\n+      new Opt(appArgs.queue, YARN, CLIENT, \"SPARK_YARN_QUEUE\", null, null),\n+      new Opt(appArgs.numExecutors, YARN, CLUSTER, null, \"--num-workers\", null),\n+      new Opt(appArgs.executorMemory, YARN, CLIENT, \"SPARK_WORKER_MEMORY\", null, null),\n+      new Opt(appArgs.executorMemory, YARN, CLUSTER, null, \"--worker-memory\", null),\n+      new Opt(appArgs.executorMemory, STANDALONE, CLUSTER, null, \"--memory\", null),\n+      new Opt(appArgs.executorMemory, STANDALONE | MESOS, CLIENT, null, null, \"spark.executor.memory\"),\n+      new Opt(appArgs.executorCores, YARN, CLIENT, \"SPARK_WORKER_CORES\", null, null),\n+      new Opt(appArgs.executorCores, YARN, CLUSTER, null, \"--worker-cores\", null),\n+      new Opt(appArgs.executorCores, STANDALONE, CLUSTER, null, \"--cores\", null),\n+      new Opt(appArgs.executorCores, STANDALONE | MESOS, CLIENT, null, null, \"spark.cores.max\"),\n+      new Opt(appArgs.files, YARN, CLIENT, \"SPARK_YARN_DIST_FILES\", null, null),\n+      new Opt(appArgs.files, YARN, CLUSTER, null, \"--files\", null),\n+      new Opt(appArgs.archives, YARN, CLIENT, \"SPARK_YARN_DIST_ARCHIVES\", null, null),\n+      new Opt(appArgs.archives, YARN, CLUSTER, null, \"--archives\", null),\n+      new Opt(appArgs.moreJars, YARN, CLUSTER, null, \"--addJars\", null)\n+    )\n+\n+    // args\n+    if (deployMode == CLIENT || clusterManager == STANDALONE) {\n+      childArgs ++= appArgs.childArgs\n+    } else if (clusterManager == YARN) {\n+      for (arg <- appArgs.childArgs) {\n+        childArgs += \"--args\"\n+        childArgs += arg\n+      }\n+    }\n+\n+    // client memory\n+    if (appArgs.driverMemory != null && deployMode == CLIENT) {\n+      childArgs += \"-Xmx\" + appArgs.driverMemory\n+    }\n+\n+    for (opt <- options) {\n+      if (opt.value != null && deployMode == opt.deployMode &&\n+        (clusterManager & opt.clusterManager) != 0) {\n+        if (opt.clOption != null) {\n+          childArgs += opt.clOption\n+          childArgs += opt.value\n+        } else if (opt.envVar != null) {\n+          childEnv.put(opt.envVar, opt.value)\n+        } else if (opt.jvmSystemProperty != null) {\n+          childArgs += \"-D\" + opt.jvmSystemProperty + \"=\" + opt.value\n+        }\n+      }\n+    }\n+\n+    childEnv.put(\"SPARK_CLASSPATH\", childClasspath.mkString(\",\"))\n+    launch(childEnv, childArgs)\n+  }\n+\n+  def launch(childEnv: Map[String, String], childArgs: ArrayBuffer[String]) {\n+    val pb = new ProcessBuilder()\n+    pb.environment().putAll(childEnv.asJava)\n+    pb.command(childArgs.asJava)\n+    val proc = pb.start()\n+    redirectInputStream(proc.getInputStream, System.out)\n+    redirectInputStream(proc.getErrorStream, System.err)\n+    proc.waitFor()\n+  }\n+\n+  def redirectInputStream(is: InputStream, ps: PrintStream) {\n+    new Thread(new Runnable {\n+      def run() {\n+        val br = new BufferedReader(new InputStreamReader(is))\n+        var line = br.readLine()\n+        while (line != null) {\n+          ps.println(line)\n+          line = br.readLine()\n+        }\n+      }\n+    }).start()\n+  }\n+}\n+\n+class LaunchContext(val env: Map[String, String],"
  }],
  "prId": 86
}, {
  "comments": [{
    "author": {
      "login": "mridulm"
    },
    "body": "This needs to be used passed on for yarn standalone case.\n",
    "commit": "d428d857971866aa5c8f75970a9655071bf016fd",
    "createdAt": "2014-03-06T06:14:06Z",
    "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io.BufferedReader\n+import java.io.InputStream\n+import java.io.InputStreamReader\n+import java.io.PrintStream\n+\n+import scala.collection.mutable.HashMap\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.JavaConverters._\n+\n+object SparkApp {\n+  val CLIENT = 1\n+  val CLUSTER = 2\n+  val YARN = 1\n+  val STANDALONE = 2\n+  val MESOS = 4\n+  val LOCAL = 8\n+  val ALL_CLUSTER_MGRS = YARN | STANDALONE | MESOS | LOCAL\n+\n+  var clusterManager: Int = LOCAL\n+\n+  def main(args: Array[String]) {\n+    println(\"args: \" + args.toList)\n+    val appArgs = new SparkAppArguments(args)\n+\n+    if (appArgs.master != null) {\n+      if (appArgs.master.startsWith(\"yarn\")) {\n+        clusterManager = YARN\n+      } else if (appArgs.master.startsWith(\"spark\")) {\n+        clusterManager = STANDALONE\n+      } else if (appArgs.master.startsWith(\"mesos\")) {\n+        clusterManager = MESOS\n+      } else if (appArgs.master == \"local\") {\n+        clusterManager = LOCAL\n+      } else {\n+        System.err.println(\"master must start with yarn, mesos, spark, or be local\")\n+        System.exit(1)\n+      }\n+    }\n+\n+    val deployMode = if (appArgs.deployMode == \"client\") CLIENT else CLUSTER\n+    val childEnv = new HashMap[String, String]()\n+    val childClasspath = new ArrayBuffer[String]()\n+    val childArgs = new ArrayBuffer[String]()\n+    childArgs += System.getenv(\"SPARK_HOME\") + \"/bin/spark-class\"\n+\n+    if (clusterManager == MESOS && deployMode == CLUSTER) {\n+      System.err.println(\"Mesos does not support running the driver on the cluster\")\n+      System.exit(1)\n+    }\n+\n+    if (deployMode == CLUSTER && clusterManager == STANDALONE) {\n+      childArgs += \"org.apache.spark.deploy.Client\"\n+      childArgs += \"launch\"\n+      childArgs += appArgs.master\n+      childArgs += appArgs.primaryResource\n+      childArgs += appArgs.mainClass\n+    } else if (deployMode == CLUSTER && clusterManager == YARN) {\n+      childArgs += \"org.apache.spark.deploy.yarn.Client\"\n+      childArgs += \"--jar\"\n+      childArgs += appArgs.primaryResource\n+      childArgs += \"--class\"\n+      childArgs += appArgs.mainClass\n+    } else {\n+      childClasspath += appArgs.primaryResource\n+      childArgs += appArgs.mainClass\n+    }\n+\n+    // TODO: num-executors when not using YARN\n+    val options = List[Opt](\n+      new Opt(appArgs.driverMemory, YARN, CLUSTER, null, \"--master-memory\", null),\n+      new Opt(appArgs.name, YARN, CLUSTER, null, \"--name\", null),\n+      new Opt(appArgs.queue, YARN, CLUSTER, null, \"--queue\", null),\n+      new Opt(appArgs.queue, YARN, CLIENT, \"SPARK_YARN_QUEUE\", null, null),\n+      new Opt(appArgs.numExecutors, YARN, CLUSTER, null, \"--num-workers\", null),\n+      new Opt(appArgs.executorMemory, YARN, CLIENT, \"SPARK_WORKER_MEMORY\", null, null),\n+      new Opt(appArgs.executorMemory, YARN, CLUSTER, null, \"--worker-memory\", null),\n+      new Opt(appArgs.executorMemory, STANDALONE, CLUSTER, null, \"--memory\", null),\n+      new Opt(appArgs.executorMemory, STANDALONE | MESOS, CLIENT, null, null, \"spark.executor.memory\"),\n+      new Opt(appArgs.executorCores, YARN, CLIENT, \"SPARK_WORKER_CORES\", null, null),\n+      new Opt(appArgs.executorCores, YARN, CLUSTER, null, \"--worker-cores\", null),\n+      new Opt(appArgs.executorCores, STANDALONE, CLUSTER, null, \"--cores\", null),\n+      new Opt(appArgs.executorCores, STANDALONE | MESOS, CLIENT, null, null, \"spark.cores.max\"),\n+      new Opt(appArgs.files, YARN, CLIENT, \"SPARK_YARN_DIST_FILES\", null, null),\n+      new Opt(appArgs.files, YARN, CLUSTER, null, \"--files\", null),\n+      new Opt(appArgs.archives, YARN, CLIENT, \"SPARK_YARN_DIST_ARCHIVES\", null, null),\n+      new Opt(appArgs.archives, YARN, CLUSTER, null, \"--archives\", null),\n+      new Opt(appArgs.moreJars, YARN, CLUSTER, null, \"--addJars\", null)\n+    )\n+\n+    // args\n+    if (deployMode == CLIENT || clusterManager == STANDALONE) {\n+      childArgs ++= appArgs.childArgs\n+    } else if (clusterManager == YARN) {\n+      for (arg <- appArgs.childArgs) {\n+        childArgs += \"--args\"\n+        childArgs += arg\n+      }\n+    }\n+\n+    // client memory\n+    if (appArgs.driverMemory != null && deployMode == CLIENT) {\n+      childArgs += \"-Xmx\" + appArgs.driverMemory\n+    }"
  }, {
    "author": {
      "login": "sryza"
    },
    "body": "Unless I'm missing something, the yarn-standalone case is handled in the else block.\n",
    "commit": "d428d857971866aa5c8f75970a9655071bf016fd",
    "createdAt": "2014-03-06T20:42:08Z",
    "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io.BufferedReader\n+import java.io.InputStream\n+import java.io.InputStreamReader\n+import java.io.PrintStream\n+\n+import scala.collection.mutable.HashMap\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.JavaConverters._\n+\n+object SparkApp {\n+  val CLIENT = 1\n+  val CLUSTER = 2\n+  val YARN = 1\n+  val STANDALONE = 2\n+  val MESOS = 4\n+  val LOCAL = 8\n+  val ALL_CLUSTER_MGRS = YARN | STANDALONE | MESOS | LOCAL\n+\n+  var clusterManager: Int = LOCAL\n+\n+  def main(args: Array[String]) {\n+    println(\"args: \" + args.toList)\n+    val appArgs = new SparkAppArguments(args)\n+\n+    if (appArgs.master != null) {\n+      if (appArgs.master.startsWith(\"yarn\")) {\n+        clusterManager = YARN\n+      } else if (appArgs.master.startsWith(\"spark\")) {\n+        clusterManager = STANDALONE\n+      } else if (appArgs.master.startsWith(\"mesos\")) {\n+        clusterManager = MESOS\n+      } else if (appArgs.master == \"local\") {\n+        clusterManager = LOCAL\n+      } else {\n+        System.err.println(\"master must start with yarn, mesos, spark, or be local\")\n+        System.exit(1)\n+      }\n+    }\n+\n+    val deployMode = if (appArgs.deployMode == \"client\") CLIENT else CLUSTER\n+    val childEnv = new HashMap[String, String]()\n+    val childClasspath = new ArrayBuffer[String]()\n+    val childArgs = new ArrayBuffer[String]()\n+    childArgs += System.getenv(\"SPARK_HOME\") + \"/bin/spark-class\"\n+\n+    if (clusterManager == MESOS && deployMode == CLUSTER) {\n+      System.err.println(\"Mesos does not support running the driver on the cluster\")\n+      System.exit(1)\n+    }\n+\n+    if (deployMode == CLUSTER && clusterManager == STANDALONE) {\n+      childArgs += \"org.apache.spark.deploy.Client\"\n+      childArgs += \"launch\"\n+      childArgs += appArgs.master\n+      childArgs += appArgs.primaryResource\n+      childArgs += appArgs.mainClass\n+    } else if (deployMode == CLUSTER && clusterManager == YARN) {\n+      childArgs += \"org.apache.spark.deploy.yarn.Client\"\n+      childArgs += \"--jar\"\n+      childArgs += appArgs.primaryResource\n+      childArgs += \"--class\"\n+      childArgs += appArgs.mainClass\n+    } else {\n+      childClasspath += appArgs.primaryResource\n+      childArgs += appArgs.mainClass\n+    }\n+\n+    // TODO: num-executors when not using YARN\n+    val options = List[Opt](\n+      new Opt(appArgs.driverMemory, YARN, CLUSTER, null, \"--master-memory\", null),\n+      new Opt(appArgs.name, YARN, CLUSTER, null, \"--name\", null),\n+      new Opt(appArgs.queue, YARN, CLUSTER, null, \"--queue\", null),\n+      new Opt(appArgs.queue, YARN, CLIENT, \"SPARK_YARN_QUEUE\", null, null),\n+      new Opt(appArgs.numExecutors, YARN, CLUSTER, null, \"--num-workers\", null),\n+      new Opt(appArgs.executorMemory, YARN, CLIENT, \"SPARK_WORKER_MEMORY\", null, null),\n+      new Opt(appArgs.executorMemory, YARN, CLUSTER, null, \"--worker-memory\", null),\n+      new Opt(appArgs.executorMemory, STANDALONE, CLUSTER, null, \"--memory\", null),\n+      new Opt(appArgs.executorMemory, STANDALONE | MESOS, CLIENT, null, null, \"spark.executor.memory\"),\n+      new Opt(appArgs.executorCores, YARN, CLIENT, \"SPARK_WORKER_CORES\", null, null),\n+      new Opt(appArgs.executorCores, YARN, CLUSTER, null, \"--worker-cores\", null),\n+      new Opt(appArgs.executorCores, STANDALONE, CLUSTER, null, \"--cores\", null),\n+      new Opt(appArgs.executorCores, STANDALONE | MESOS, CLIENT, null, null, \"spark.cores.max\"),\n+      new Opt(appArgs.files, YARN, CLIENT, \"SPARK_YARN_DIST_FILES\", null, null),\n+      new Opt(appArgs.files, YARN, CLUSTER, null, \"--files\", null),\n+      new Opt(appArgs.archives, YARN, CLIENT, \"SPARK_YARN_DIST_ARCHIVES\", null, null),\n+      new Opt(appArgs.archives, YARN, CLUSTER, null, \"--archives\", null),\n+      new Opt(appArgs.moreJars, YARN, CLUSTER, null, \"--addJars\", null)\n+    )\n+\n+    // args\n+    if (deployMode == CLIENT || clusterManager == STANDALONE) {\n+      childArgs ++= appArgs.childArgs\n+    } else if (clusterManager == YARN) {\n+      for (arg <- appArgs.childArgs) {\n+        childArgs += \"--args\"\n+        childArgs += arg\n+      }\n+    }\n+\n+    // client memory\n+    if (appArgs.driverMemory != null && deployMode == CLIENT) {\n+      childArgs += \"-Xmx\" + appArgs.driverMemory\n+    }"
  }, {
    "author": {
      "login": "mridulm"
    },
    "body": "To clarify, I was referring to driver memory.\n\nRegards\nMridul\nOn Mar 7, 2014 2:12 AM, \"Sandy Ryza\" notifications@github.com wrote:\n\n> In core/src/main/scala/org/apache/spark/deploy/SparkApp.scala:\n> \n> > -    )\n> >   +\n> > -    // args\n> > -    if (deployMode == CLIENT || clusterManager == STANDALONE) {\n> > -      childArgs ++= appArgs.childArgs\n> > -    } else if (clusterManager == YARN) {\n> > -      for (arg <- appArgs.childArgs) {\n> > -        childArgs += \"--args\"\n> > -        childArgs += arg\n> > -      }\n> > -    }\n> >   +\n> > -    // client memory\n> > -    if (appArgs.driverMemory != null && deployMode == CLIENT) {\n> > -      childArgs += \"-Xmx\" + appArgs.driverMemory\n> > -    }\n> \n> Unless I'm missing something, the yarn-standalone case is handled in the\n> else block.\n> \n> ## \n> \n> Reply to this email directly or view it on GitHubhttps://github.com/apache/spark/pull/86/files#r10359569\n> .\n",
    "commit": "d428d857971866aa5c8f75970a9655071bf016fd",
    "createdAt": "2014-03-07T03:15:35Z",
    "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io.BufferedReader\n+import java.io.InputStream\n+import java.io.InputStreamReader\n+import java.io.PrintStream\n+\n+import scala.collection.mutable.HashMap\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.JavaConverters._\n+\n+object SparkApp {\n+  val CLIENT = 1\n+  val CLUSTER = 2\n+  val YARN = 1\n+  val STANDALONE = 2\n+  val MESOS = 4\n+  val LOCAL = 8\n+  val ALL_CLUSTER_MGRS = YARN | STANDALONE | MESOS | LOCAL\n+\n+  var clusterManager: Int = LOCAL\n+\n+  def main(args: Array[String]) {\n+    println(\"args: \" + args.toList)\n+    val appArgs = new SparkAppArguments(args)\n+\n+    if (appArgs.master != null) {\n+      if (appArgs.master.startsWith(\"yarn\")) {\n+        clusterManager = YARN\n+      } else if (appArgs.master.startsWith(\"spark\")) {\n+        clusterManager = STANDALONE\n+      } else if (appArgs.master.startsWith(\"mesos\")) {\n+        clusterManager = MESOS\n+      } else if (appArgs.master == \"local\") {\n+        clusterManager = LOCAL\n+      } else {\n+        System.err.println(\"master must start with yarn, mesos, spark, or be local\")\n+        System.exit(1)\n+      }\n+    }\n+\n+    val deployMode = if (appArgs.deployMode == \"client\") CLIENT else CLUSTER\n+    val childEnv = new HashMap[String, String]()\n+    val childClasspath = new ArrayBuffer[String]()\n+    val childArgs = new ArrayBuffer[String]()\n+    childArgs += System.getenv(\"SPARK_HOME\") + \"/bin/spark-class\"\n+\n+    if (clusterManager == MESOS && deployMode == CLUSTER) {\n+      System.err.println(\"Mesos does not support running the driver on the cluster\")\n+      System.exit(1)\n+    }\n+\n+    if (deployMode == CLUSTER && clusterManager == STANDALONE) {\n+      childArgs += \"org.apache.spark.deploy.Client\"\n+      childArgs += \"launch\"\n+      childArgs += appArgs.master\n+      childArgs += appArgs.primaryResource\n+      childArgs += appArgs.mainClass\n+    } else if (deployMode == CLUSTER && clusterManager == YARN) {\n+      childArgs += \"org.apache.spark.deploy.yarn.Client\"\n+      childArgs += \"--jar\"\n+      childArgs += appArgs.primaryResource\n+      childArgs += \"--class\"\n+      childArgs += appArgs.mainClass\n+    } else {\n+      childClasspath += appArgs.primaryResource\n+      childArgs += appArgs.mainClass\n+    }\n+\n+    // TODO: num-executors when not using YARN\n+    val options = List[Opt](\n+      new Opt(appArgs.driverMemory, YARN, CLUSTER, null, \"--master-memory\", null),\n+      new Opt(appArgs.name, YARN, CLUSTER, null, \"--name\", null),\n+      new Opt(appArgs.queue, YARN, CLUSTER, null, \"--queue\", null),\n+      new Opt(appArgs.queue, YARN, CLIENT, \"SPARK_YARN_QUEUE\", null, null),\n+      new Opt(appArgs.numExecutors, YARN, CLUSTER, null, \"--num-workers\", null),\n+      new Opt(appArgs.executorMemory, YARN, CLIENT, \"SPARK_WORKER_MEMORY\", null, null),\n+      new Opt(appArgs.executorMemory, YARN, CLUSTER, null, \"--worker-memory\", null),\n+      new Opt(appArgs.executorMemory, STANDALONE, CLUSTER, null, \"--memory\", null),\n+      new Opt(appArgs.executorMemory, STANDALONE | MESOS, CLIENT, null, null, \"spark.executor.memory\"),\n+      new Opt(appArgs.executorCores, YARN, CLIENT, \"SPARK_WORKER_CORES\", null, null),\n+      new Opt(appArgs.executorCores, YARN, CLUSTER, null, \"--worker-cores\", null),\n+      new Opt(appArgs.executorCores, STANDALONE, CLUSTER, null, \"--cores\", null),\n+      new Opt(appArgs.executorCores, STANDALONE | MESOS, CLIENT, null, null, \"spark.cores.max\"),\n+      new Opt(appArgs.files, YARN, CLIENT, \"SPARK_YARN_DIST_FILES\", null, null),\n+      new Opt(appArgs.files, YARN, CLUSTER, null, \"--files\", null),\n+      new Opt(appArgs.archives, YARN, CLIENT, \"SPARK_YARN_DIST_ARCHIVES\", null, null),\n+      new Opt(appArgs.archives, YARN, CLUSTER, null, \"--archives\", null),\n+      new Opt(appArgs.moreJars, YARN, CLUSTER, null, \"--addJars\", null)\n+    )\n+\n+    // args\n+    if (deployMode == CLIENT || clusterManager == STANDALONE) {\n+      childArgs ++= appArgs.childArgs\n+    } else if (clusterManager == YARN) {\n+      for (arg <- appArgs.childArgs) {\n+        childArgs += \"--args\"\n+        childArgs += arg\n+      }\n+    }\n+\n+    // client memory\n+    if (appArgs.driverMemory != null && deployMode == CLIENT) {\n+      childArgs += \"-Xmx\" + appArgs.driverMemory\n+    }"
  }],
  "prId": 86
}, {
  "comments": [{
    "author": {
      "login": "mateiz"
    },
    "body": "I'm not sure Opt is the best name for this, it's almost something like an OptionAssigner. Also as a tip you can use default parameters here to avoid passing lots of nulls above (which is hard to read). Something like this:\n\n```\nclass Opt(val value: String, val deployModes: Int, val clusterManagers: Int, val envVar: String = null, ...)\n```\n\nThen you can call:\n\n```\nnew OptionAssigner(foo, YARN, CLIENT, envVar=\"SPARK_FOO\")\n```\n\nYou can also make this a case class to avoid the `new`.\n",
    "commit": "d428d857971866aa5c8f75970a9655071bf016fd",
    "createdAt": "2014-03-06T06:16:50Z",
    "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io.BufferedReader\n+import java.io.InputStream\n+import java.io.InputStreamReader\n+import java.io.PrintStream\n+\n+import scala.collection.mutable.HashMap\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.JavaConverters._\n+\n+object SparkApp {\n+  val CLIENT = 1\n+  val CLUSTER = 2\n+  val YARN = 1\n+  val STANDALONE = 2\n+  val MESOS = 4\n+  val LOCAL = 8\n+  val ALL_CLUSTER_MGRS = YARN | STANDALONE | MESOS | LOCAL\n+\n+  var clusterManager: Int = LOCAL\n+\n+  def main(args: Array[String]) {\n+    println(\"args: \" + args.toList)\n+    val appArgs = new SparkAppArguments(args)\n+\n+    if (appArgs.master != null) {\n+      if (appArgs.master.startsWith(\"yarn\")) {\n+        clusterManager = YARN\n+      } else if (appArgs.master.startsWith(\"spark\")) {\n+        clusterManager = STANDALONE\n+      } else if (appArgs.master.startsWith(\"mesos\")) {\n+        clusterManager = MESOS\n+      } else if (appArgs.master == \"local\") {\n+        clusterManager = LOCAL\n+      } else {\n+        System.err.println(\"master must start with yarn, mesos, spark, or be local\")\n+        System.exit(1)\n+      }\n+    }\n+\n+    val deployMode = if (appArgs.deployMode == \"client\") CLIENT else CLUSTER\n+    val childEnv = new HashMap[String, String]()\n+    val childClasspath = new ArrayBuffer[String]()\n+    val childArgs = new ArrayBuffer[String]()\n+    childArgs += System.getenv(\"SPARK_HOME\") + \"/bin/spark-class\"\n+\n+    if (clusterManager == MESOS && deployMode == CLUSTER) {\n+      System.err.println(\"Mesos does not support running the driver on the cluster\")\n+      System.exit(1)\n+    }\n+\n+    if (deployMode == CLUSTER && clusterManager == STANDALONE) {\n+      childArgs += \"org.apache.spark.deploy.Client\"\n+      childArgs += \"launch\"\n+      childArgs += appArgs.master\n+      childArgs += appArgs.primaryResource\n+      childArgs += appArgs.mainClass\n+    } else if (deployMode == CLUSTER && clusterManager == YARN) {\n+      childArgs += \"org.apache.spark.deploy.yarn.Client\"\n+      childArgs += \"--jar\"\n+      childArgs += appArgs.primaryResource\n+      childArgs += \"--class\"\n+      childArgs += appArgs.mainClass\n+    } else {\n+      childClasspath += appArgs.primaryResource\n+      childArgs += appArgs.mainClass\n+    }\n+\n+    // TODO: num-executors when not using YARN\n+    val options = List[Opt](\n+      new Opt(appArgs.driverMemory, YARN, CLUSTER, null, \"--master-memory\", null),\n+      new Opt(appArgs.name, YARN, CLUSTER, null, \"--name\", null),\n+      new Opt(appArgs.queue, YARN, CLUSTER, null, \"--queue\", null),\n+      new Opt(appArgs.queue, YARN, CLIENT, \"SPARK_YARN_QUEUE\", null, null),\n+      new Opt(appArgs.numExecutors, YARN, CLUSTER, null, \"--num-workers\", null),\n+      new Opt(appArgs.executorMemory, YARN, CLIENT, \"SPARK_WORKER_MEMORY\", null, null),\n+      new Opt(appArgs.executorMemory, YARN, CLUSTER, null, \"--worker-memory\", null),\n+      new Opt(appArgs.executorMemory, STANDALONE, CLUSTER, null, \"--memory\", null),\n+      new Opt(appArgs.executorMemory, STANDALONE | MESOS, CLIENT, null, null, \"spark.executor.memory\"),\n+      new Opt(appArgs.executorCores, YARN, CLIENT, \"SPARK_WORKER_CORES\", null, null),\n+      new Opt(appArgs.executorCores, YARN, CLUSTER, null, \"--worker-cores\", null),\n+      new Opt(appArgs.executorCores, STANDALONE, CLUSTER, null, \"--cores\", null),\n+      new Opt(appArgs.executorCores, STANDALONE | MESOS, CLIENT, null, null, \"spark.cores.max\"),\n+      new Opt(appArgs.files, YARN, CLIENT, \"SPARK_YARN_DIST_FILES\", null, null),\n+      new Opt(appArgs.files, YARN, CLUSTER, null, \"--files\", null),\n+      new Opt(appArgs.archives, YARN, CLIENT, \"SPARK_YARN_DIST_ARCHIVES\", null, null),\n+      new Opt(appArgs.archives, YARN, CLUSTER, null, \"--archives\", null),\n+      new Opt(appArgs.moreJars, YARN, CLUSTER, null, \"--addJars\", null)\n+    )\n+\n+    // args\n+    if (deployMode == CLIENT || clusterManager == STANDALONE) {\n+      childArgs ++= appArgs.childArgs\n+    } else if (clusterManager == YARN) {\n+      for (arg <- appArgs.childArgs) {\n+        childArgs += \"--args\"\n+        childArgs += arg\n+      }\n+    }\n+\n+    // client memory\n+    if (appArgs.driverMemory != null && deployMode == CLIENT) {\n+      childArgs += \"-Xmx\" + appArgs.driverMemory\n+    }\n+\n+    for (opt <- options) {\n+      if (opt.value != null && deployMode == opt.deployMode &&\n+        (clusterManager & opt.clusterManager) != 0) {\n+        if (opt.clOption != null) {\n+          childArgs += opt.clOption\n+          childArgs += opt.value\n+        } else if (opt.envVar != null) {\n+          childEnv.put(opt.envVar, opt.value)\n+        } else if (opt.jvmSystemProperty != null) {\n+          childArgs += \"-D\" + opt.jvmSystemProperty + \"=\" + opt.value\n+        }\n+      }\n+    }\n+\n+    childEnv.put(\"SPARK_CLASSPATH\", childClasspath.mkString(\",\"))\n+    launch(childEnv, childArgs)\n+  }\n+\n+  def launch(childEnv: Map[String, String], childArgs: ArrayBuffer[String]) {\n+    val pb = new ProcessBuilder()\n+    pb.environment().putAll(childEnv.asJava)\n+    pb.command(childArgs.asJava)\n+    val proc = pb.start()\n+    redirectInputStream(proc.getInputStream, System.out)\n+    redirectInputStream(proc.getErrorStream, System.err)\n+    proc.waitFor()\n+  }\n+\n+  def redirectInputStream(is: InputStream, ps: PrintStream) {\n+    new Thread(new Runnable {\n+      def run() {\n+        val br = new BufferedReader(new InputStreamReader(is))\n+        var line = br.readLine()\n+        while (line != null) {\n+          ps.println(line)\n+          line = br.readLine()\n+        }\n+      }\n+    }).start()\n+  }\n+}\n+\n+class LaunchContext(val env: Map[String, String],\n+  val args: ArrayBuffer[String],\n+  val classpath: ArrayBuffer[String]\n+) {}\n+\n+class Opt(val value: String,"
  }],
  "prId": 86
}, {
  "comments": [{
    "author": {
      "login": "mateiz"
    },
    "body": "Aren't you passing the deployMode and clusterManager in the wrong order in here? That's one of the problems with these being Ints, no type-checking.\n",
    "commit": "d428d857971866aa5c8f75970a9655071bf016fd",
    "createdAt": "2014-03-06T06:17:16Z",
    "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io.BufferedReader\n+import java.io.InputStream\n+import java.io.InputStreamReader\n+import java.io.PrintStream\n+\n+import scala.collection.mutable.HashMap\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.JavaConverters._\n+\n+object SparkApp {\n+  val CLIENT = 1\n+  val CLUSTER = 2\n+  val YARN = 1\n+  val STANDALONE = 2\n+  val MESOS = 4\n+  val LOCAL = 8\n+  val ALL_CLUSTER_MGRS = YARN | STANDALONE | MESOS | LOCAL\n+\n+  var clusterManager: Int = LOCAL\n+\n+  def main(args: Array[String]) {\n+    println(\"args: \" + args.toList)\n+    val appArgs = new SparkAppArguments(args)\n+\n+    if (appArgs.master != null) {\n+      if (appArgs.master.startsWith(\"yarn\")) {\n+        clusterManager = YARN\n+      } else if (appArgs.master.startsWith(\"spark\")) {\n+        clusterManager = STANDALONE\n+      } else if (appArgs.master.startsWith(\"mesos\")) {\n+        clusterManager = MESOS\n+      } else if (appArgs.master == \"local\") {\n+        clusterManager = LOCAL\n+      } else {\n+        System.err.println(\"master must start with yarn, mesos, spark, or be local\")\n+        System.exit(1)\n+      }\n+    }\n+\n+    val deployMode = if (appArgs.deployMode == \"client\") CLIENT else CLUSTER\n+    val childEnv = new HashMap[String, String]()\n+    val childClasspath = new ArrayBuffer[String]()\n+    val childArgs = new ArrayBuffer[String]()\n+    childArgs += System.getenv(\"SPARK_HOME\") + \"/bin/spark-class\"\n+\n+    if (clusterManager == MESOS && deployMode == CLUSTER) {\n+      System.err.println(\"Mesos does not support running the driver on the cluster\")\n+      System.exit(1)\n+    }\n+\n+    if (deployMode == CLUSTER && clusterManager == STANDALONE) {\n+      childArgs += \"org.apache.spark.deploy.Client\"\n+      childArgs += \"launch\"\n+      childArgs += appArgs.master\n+      childArgs += appArgs.primaryResource\n+      childArgs += appArgs.mainClass\n+    } else if (deployMode == CLUSTER && clusterManager == YARN) {\n+      childArgs += \"org.apache.spark.deploy.yarn.Client\"\n+      childArgs += \"--jar\"\n+      childArgs += appArgs.primaryResource\n+      childArgs += \"--class\"\n+      childArgs += appArgs.mainClass\n+    } else {\n+      childClasspath += appArgs.primaryResource\n+      childArgs += appArgs.mainClass\n+    }\n+\n+    // TODO: num-executors when not using YARN\n+    val options = List[Opt](\n+      new Opt(appArgs.driverMemory, YARN, CLUSTER, null, \"--master-memory\", null),\n+      new Opt(appArgs.name, YARN, CLUSTER, null, \"--name\", null),\n+      new Opt(appArgs.queue, YARN, CLUSTER, null, \"--queue\", null),\n+      new Opt(appArgs.queue, YARN, CLIENT, \"SPARK_YARN_QUEUE\", null, null),\n+      new Opt(appArgs.numExecutors, YARN, CLUSTER, null, \"--num-workers\", null),\n+      new Opt(appArgs.executorMemory, YARN, CLIENT, \"SPARK_WORKER_MEMORY\", null, null),\n+      new Opt(appArgs.executorMemory, YARN, CLUSTER, null, \"--worker-memory\", null),\n+      new Opt(appArgs.executorMemory, STANDALONE, CLUSTER, null, \"--memory\", null),\n+      new Opt(appArgs.executorMemory, STANDALONE | MESOS, CLIENT, null, null, \"spark.executor.memory\"),\n+      new Opt(appArgs.executorCores, YARN, CLIENT, \"SPARK_WORKER_CORES\", null, null),\n+      new Opt(appArgs.executorCores, YARN, CLUSTER, null, \"--worker-cores\", null),\n+      new Opt(appArgs.executorCores, STANDALONE, CLUSTER, null, \"--cores\", null),\n+      new Opt(appArgs.executorCores, STANDALONE | MESOS, CLIENT, null, null, \"spark.cores.max\"),\n+      new Opt(appArgs.files, YARN, CLIENT, \"SPARK_YARN_DIST_FILES\", null, null),\n+      new Opt(appArgs.files, YARN, CLUSTER, null, \"--files\", null),\n+      new Opt(appArgs.archives, YARN, CLIENT, \"SPARK_YARN_DIST_ARCHIVES\", null, null),\n+      new Opt(appArgs.archives, YARN, CLUSTER, null, \"--archives\", null),\n+      new Opt(appArgs.moreJars, YARN, CLUSTER, null, \"--addJars\", null)"
  }],
  "prId": 86
}]