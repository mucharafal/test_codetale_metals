[{
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "If we only want to track live executors, we need to address this kind of FIXME. If we also want to track dead executors, FIXME can be removed.",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-06T21:42:20Z",
    "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.scheduler._\n+\n+class BasicEventFilterBuilder extends SparkListener with EventFilterBuilder {\n+  val liveJobToStages = new mutable.HashMap[Int, Seq[Int]]\n+  val stageToTasks = new mutable.HashMap[Int, mutable.Set[Long]]\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    liveJobToStages += jobStart.jobId -> jobStart.stageIds\n+  }\n+\n+  override def onJobEnd(jobEnd: SparkListenerJobEnd): Unit = {\n+    val stages = liveJobToStages.getOrElse(jobEnd.jobId, Seq.empty[Int])\n+    liveJobToStages -= jobEnd.jobId\n+    stages.foreach { stage => stageToTasks -= stage }\n+  }\n+\n+  override def onTaskStart(taskStart: SparkListenerTaskStart): Unit = {\n+    val curTasks = stageToTasks.getOrElseUpdate(taskStart.stageId,\n+      mutable.HashSet[Long]())\n+    curTasks += taskStart.taskInfo.taskId\n+  }\n+\n+  override def createFilter(): EventFilter = new BasicEventFilter(this)\n+}\n+\n+class BasicEventFilter(trackListener: BasicEventFilterBuilder) extends EventFilter with Logging {\n+\n+  private val liveTasks: Set[Long] = trackListener.stageToTasks.values match {\n+    case xs if xs.isEmpty => Set.empty[Long]\n+    case xs => xs.reduce(_ ++ _).toSet\n+  }\n+\n+  if (log.isDebugEnabled) {\n+    logDebug(s\"live jobs : ${trackListener.liveJobToStages.keySet}\")\n+    logDebug(s\"stages in jobs : ${trackListener.liveJobToStages.values.flatten}\")\n+    logDebug(s\"stages : ${trackListener.stageToTasks.keySet}\")\n+    logDebug(s\"tasks in stages : ${trackListener.stageToTasks.values.flatten}\")\n+  }\n+\n+  override def filterStageCompleted(event: SparkListenerStageCompleted): Option[Boolean] = {\n+    Some(trackListener.stageToTasks.contains(event.stageInfo.stageId))\n+  }\n+\n+  override def filterStageSubmitted(event: SparkListenerStageSubmitted): Option[Boolean] = {\n+    Some(trackListener.stageToTasks.contains(event.stageInfo.stageId))\n+  }\n+\n+  override def filterTaskStart(event: SparkListenerTaskStart): Option[Boolean] = {\n+    Some(liveTasks.contains(event.taskInfo.taskId))\n+  }\n+\n+  override def filterTaskGettingResult(event: SparkListenerTaskGettingResult): Option[Boolean] = {\n+    Some(liveTasks.contains(event.taskInfo.taskId))\n+  }\n+\n+  override def filterTaskEnd(event: SparkListenerTaskEnd): Option[Boolean] = {\n+    Some(liveTasks.contains(event.taskInfo.taskId))\n+  }\n+\n+  override def filterJobStart(event: SparkListenerJobStart): Option[Boolean] = {\n+    Some(trackListener.liveJobToStages.contains(event.jobId))\n+  }\n+\n+  override def filterJobEnd(event: SparkListenerJobEnd): Option[Boolean] = {\n+    Some(trackListener.liveJobToStages.contains(event.jobId))\n+  }\n+\n+  override def filterUnpersistRDD(event: SparkListenerUnpersistRDD): Option[Boolean] = {\n+    // FIXME: need to track rdd ids?\n+    None\n+  }\n+\n+  override def filterExecutorMetricsUpdate(\n+      event: SparkListenerExecutorMetricsUpdate): Option[Boolean] = {\n+    // FIXME: need to track live executors?"
  }],
  "prId": 26416
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "I may need to rethink on this as I'm not fully sure whether it's safe to leave when it's not live RDD or vice versa.",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-06T21:44:24Z",
    "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.scheduler._\n+\n+class BasicEventFilterBuilder extends SparkListener with EventFilterBuilder {\n+  val liveJobToStages = new mutable.HashMap[Int, Seq[Int]]\n+  val stageToTasks = new mutable.HashMap[Int, mutable.Set[Long]]\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    liveJobToStages += jobStart.jobId -> jobStart.stageIds\n+  }\n+\n+  override def onJobEnd(jobEnd: SparkListenerJobEnd): Unit = {\n+    val stages = liveJobToStages.getOrElse(jobEnd.jobId, Seq.empty[Int])\n+    liveJobToStages -= jobEnd.jobId\n+    stages.foreach { stage => stageToTasks -= stage }\n+  }\n+\n+  override def onTaskStart(taskStart: SparkListenerTaskStart): Unit = {\n+    val curTasks = stageToTasks.getOrElseUpdate(taskStart.stageId,\n+      mutable.HashSet[Long]())\n+    curTasks += taskStart.taskInfo.taskId\n+  }\n+\n+  override def createFilter(): EventFilter = new BasicEventFilter(this)\n+}\n+\n+class BasicEventFilter(trackListener: BasicEventFilterBuilder) extends EventFilter with Logging {\n+\n+  private val liveTasks: Set[Long] = trackListener.stageToTasks.values match {\n+    case xs if xs.isEmpty => Set.empty[Long]\n+    case xs => xs.reduce(_ ++ _).toSet\n+  }\n+\n+  if (log.isDebugEnabled) {\n+    logDebug(s\"live jobs : ${trackListener.liveJobToStages.keySet}\")\n+    logDebug(s\"stages in jobs : ${trackListener.liveJobToStages.values.flatten}\")\n+    logDebug(s\"stages : ${trackListener.stageToTasks.keySet}\")\n+    logDebug(s\"tasks in stages : ${trackListener.stageToTasks.values.flatten}\")\n+  }\n+\n+  override def filterStageCompleted(event: SparkListenerStageCompleted): Option[Boolean] = {\n+    Some(trackListener.stageToTasks.contains(event.stageInfo.stageId))\n+  }\n+\n+  override def filterStageSubmitted(event: SparkListenerStageSubmitted): Option[Boolean] = {\n+    Some(trackListener.stageToTasks.contains(event.stageInfo.stageId))\n+  }\n+\n+  override def filterTaskStart(event: SparkListenerTaskStart): Option[Boolean] = {\n+    Some(liveTasks.contains(event.taskInfo.taskId))\n+  }\n+\n+  override def filterTaskGettingResult(event: SparkListenerTaskGettingResult): Option[Boolean] = {\n+    Some(liveTasks.contains(event.taskInfo.taskId))\n+  }\n+\n+  override def filterTaskEnd(event: SparkListenerTaskEnd): Option[Boolean] = {\n+    Some(liveTasks.contains(event.taskInfo.taskId))\n+  }\n+\n+  override def filterJobStart(event: SparkListenerJobStart): Option[Boolean] = {\n+    Some(trackListener.liveJobToStages.contains(event.jobId))\n+  }\n+\n+  override def filterJobEnd(event: SparkListenerJobEnd): Option[Boolean] = {\n+    Some(trackListener.liveJobToStages.contains(event.jobId))\n+  }\n+\n+  override def filterUnpersistRDD(event: SparkListenerUnpersistRDD): Option[Boolean] = {\n+    // FIXME: need to track rdd ids?"
  }],
  "prId": 26416
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "I think it's a weird construct. `BasicEventFilterBuilder` should build a filter which is `SparkListener`. Same applies to the other builder.",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-11T10:38:23Z",
    "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.scheduler._\n+\n+class BasicEventFilterBuilder extends SparkListener with EventFilterBuilder {"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Agreed. The Builder was lately introduced so that is, but should be better to swap twos.",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-13T00:32:17Z",
    "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.scheduler._\n+\n+class BasicEventFilterBuilder extends SparkListener with EventFilterBuilder {"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "So no luck, as SparkListener is not a trait, but an abstract class.",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-13T10:36:36Z",
    "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.scheduler._\n+\n+class BasicEventFilterBuilder extends SparkListener with EventFilterBuilder {"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "I've invested time to understand your point. The following worked for me:\r\n```\r\n...\r\nprivate[spark] class BasicEventFilterBuilder extends EventFilterBuilder\r\n...\r\nprivate[spark] trait EventFilter extends SparkListenerInterface\r\n...\r\nprivate[spark] class BasicEventFilter extends SparkListener with EventFilter with Logging\r\n...\r\n```\r\nTesting would be a bit harder though...\r\n",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-13T14:31:58Z",
    "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.scheduler._\n+\n+class BasicEventFilterBuilder extends SparkListener with EventFilterBuilder {"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "BasicEventFilterBuilder should extend SparkListener; it is optional for BasicEventFilter to extend SparkListener(Interface) but it is intended for BasicEventFilterBuilder to extend SparkListener. BasicEventFilterBuilder and BasicEventFilter are working for different purposes.",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-13T22:17:52Z",
    "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.scheduler._\n+\n+class BasicEventFilterBuilder extends SparkListener with EventFilterBuilder {"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "> BasicEventFilterBuilder should extend SparkListener\r\n\r\nWhy? Not yet see your point. Is it not working what I've suggested?\r\n\r\nIf we consider responsibilities:\r\n* `SparkListener` is input\r\n* `EventFilterBuilder` builds `EventFilter`\r\n* `EventFilter` filters input to output\r\n\r\nAdding `EventFilterBuilder` input functionality is side effect.\r\n",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-14T12:43:41Z",
    "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.scheduler._\n+\n+class BasicEventFilterBuilder extends SparkListener with EventFilterBuilder {"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Asking it differently why `EventFilter` can't build up itself from `SparkListener` events and why can't be this list of `EventFilter` passed to `FilteredEventLogFileRewriter`?",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-14T13:04:15Z",
    "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.scheduler._\n+\n+class BasicEventFilterBuilder extends SparkListener with EventFilterBuilder {"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "> why EventFilter can't build up itself from SparkListener events\r\n\r\nIt's possible but then the filter does too many things and UTs will be harder to implement. Please refer how I craft UTs for filters - it passes the information which is expected to be passed from filter builder. If we unify two classes, not intuitive to inject the test information.",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-15T04:47:55Z",
    "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.scheduler._\n+\n+class BasicEventFilterBuilder extends SparkListener with EventFilterBuilder {"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Back to the origin question, SparkListener interface defines \"what to do\" when the event comes in. EventFilter interface defines the \"predicate\" whether the event should be filtered or not when the event comes in.\r\n\r\n```\r\n...\r\nprivate[spark] class BasicEventFilterBuilder extends EventFilterBuilder\r\n...\r\nprivate[spark] trait EventFilter extends SparkListenerInterface\r\n...\r\nprivate[spark] class BasicEventFilter extends SparkListener with EventFilter with Logging\r\n...\r\n```\r\n\r\nSo `private[spark] trait EventFilter extends SparkListenerInterface` is not same as current one and doesn't match with my intention. Hope this clarify my intention.",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-15T04:53:57Z",
    "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.scheduler._\n+\n+class BasicEventFilterBuilder extends SparkListener with EventFilterBuilder {"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "> It's possible but then the filter does too many things\r\n\r\nI can relate to this.\r\n\r\nNow I see your point and I have the feeling both are right.\r\n`Builder` is needed to create type specific constructs.\r\n`EventFilter` is needed as predicate.\r\nMaybe we can create `EventSource` (or whatever name which makes more sense).\r\n\r\n```\r\nprivate[spark] class BasicEventFilterBuilder extends EventFilterBuilder\r\nprivate[spark] trait EventSource extends SparkListener\r\nprivate[spark] trait EventFilter {\r\n  val eventSource: EventSource\r\n}\r\nprivate[spark] class BasicEventSource extends EventSource\r\nprivate[spark] class BasicEventFilter extends EventFilter with Logging\r\n```\r\n",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-15T16:00:36Z",
    "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.scheduler._\n+\n+class BasicEventFilterBuilder extends SparkListener with EventFilterBuilder {"
  }],
  "prId": 26416
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Redundant since `logDebug` already contains is.",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-14T13:50:16Z",
    "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.scheduler._\n+\n+/**\n+ * This class tracks both live jobs and live executors, and pass the list to the\n+ * [[BasicEventFilter]] to help BasicEventFilter to filter out finished jobs\n+ * (+ stages/tasks/RDDs) and dead executors.\n+ */\n+private[spark] class BasicEventFilterBuilder extends SparkListener with EventFilterBuilder {\n+  private val _liveJobToStages = new mutable.HashMap[Int, Seq[Int]]\n+  private val _stageToTasks = new mutable.HashMap[Int, mutable.Set[Long]]\n+  private val _stageToRDDs = new mutable.HashMap[Int, Seq[Int]]\n+  private val _liveExecutors = new mutable.HashSet[String]\n+\n+  def liveJobToStages: Map[Int, Seq[Int]] = _liveJobToStages.toMap\n+  def stageToTasks: Map[Int, Set[Long]] = _stageToTasks.mapValues(_.toSet).toMap\n+  def stageToRDDs: Map[Int, Seq[Int]] = _stageToRDDs.toMap\n+  def liveExecutors: Set[String] = _liveExecutors.toSet\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    _liveJobToStages += jobStart.jobId -> jobStart.stageIds\n+  }\n+\n+  override def onJobEnd(jobEnd: SparkListenerJobEnd): Unit = {\n+    val stages = _liveJobToStages.getOrElse(jobEnd.jobId, Seq.empty[Int])\n+    _liveJobToStages -= jobEnd.jobId\n+    _stageToTasks --= stages\n+    _stageToRDDs --= stages\n+  }\n+\n+  override def onStageSubmitted(stageSubmitted: SparkListenerStageSubmitted): Unit = {\n+    _stageToRDDs.getOrElseUpdate(stageSubmitted.stageInfo.stageId,\n+      stageSubmitted.stageInfo.rddInfos.map(_.id))\n+  }\n+\n+  override def onTaskStart(taskStart: SparkListenerTaskStart): Unit = {\n+    val curTasks = _stageToTasks.getOrElseUpdate(taskStart.stageId,\n+      mutable.HashSet[Long]())\n+    curTasks += taskStart.taskInfo.taskId\n+  }\n+\n+  override def onExecutorAdded(executorAdded: SparkListenerExecutorAdded): Unit = {\n+    _liveExecutors += executorAdded.executorId\n+  }\n+\n+  override def onExecutorRemoved(executorRemoved: SparkListenerExecutorRemoved): Unit = {\n+    _liveExecutors -= executorRemoved.executorId\n+  }\n+\n+  override def createFilter(): EventFilter = BasicEventFilter(this)\n+}\n+\n+/**\n+ * This class provides the functionality to filter out events which are related to the finished\n+ * jobs based on the given information. This class only deals with job related events, and returns\n+ * either Some(true) or Some(false) - successors should override the methods if they don't want to\n+ * return Some(false) for finished jobs and related events.\n+ */\n+private[spark] abstract class JobEventFilter(\n+    jobToStages: Map[Int, Seq[Int]],\n+    stageToTasks: Map[Int, Set[Long]],\n+    stageToRDDs: Map[Int, Seq[Int]]) extends EventFilter with Logging {\n+\n+  private val liveTasks: Set[Long] = stageToTasks.values match {\n+    case xs if xs.isEmpty => Set.empty[Long]\n+    case xs => xs.reduce(_ ++ _).toSet\n+  }\n+\n+  private val liveRDDs: Set[Int] = stageToRDDs.values match {\n+    case xs if xs.isEmpty => Set.empty[Int]\n+    case xs => xs.reduce(_ ++ _).toSet\n+  }\n+\n+  if (log.isDebugEnabled) {"
  }],
  "prId": 26416
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Redundant since `logDebug` already contains is.",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-14T13:50:38Z",
    "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.scheduler._\n+\n+/**\n+ * This class tracks both live jobs and live executors, and pass the list to the\n+ * [[BasicEventFilter]] to help BasicEventFilter to filter out finished jobs\n+ * (+ stages/tasks/RDDs) and dead executors.\n+ */\n+private[spark] class BasicEventFilterBuilder extends SparkListener with EventFilterBuilder {\n+  private val _liveJobToStages = new mutable.HashMap[Int, Seq[Int]]\n+  private val _stageToTasks = new mutable.HashMap[Int, mutable.Set[Long]]\n+  private val _stageToRDDs = new mutable.HashMap[Int, Seq[Int]]\n+  private val _liveExecutors = new mutable.HashSet[String]\n+\n+  def liveJobToStages: Map[Int, Seq[Int]] = _liveJobToStages.toMap\n+  def stageToTasks: Map[Int, Set[Long]] = _stageToTasks.mapValues(_.toSet).toMap\n+  def stageToRDDs: Map[Int, Seq[Int]] = _stageToRDDs.toMap\n+  def liveExecutors: Set[String] = _liveExecutors.toSet\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    _liveJobToStages += jobStart.jobId -> jobStart.stageIds\n+  }\n+\n+  override def onJobEnd(jobEnd: SparkListenerJobEnd): Unit = {\n+    val stages = _liveJobToStages.getOrElse(jobEnd.jobId, Seq.empty[Int])\n+    _liveJobToStages -= jobEnd.jobId\n+    _stageToTasks --= stages\n+    _stageToRDDs --= stages\n+  }\n+\n+  override def onStageSubmitted(stageSubmitted: SparkListenerStageSubmitted): Unit = {\n+    _stageToRDDs.getOrElseUpdate(stageSubmitted.stageInfo.stageId,\n+      stageSubmitted.stageInfo.rddInfos.map(_.id))\n+  }\n+\n+  override def onTaskStart(taskStart: SparkListenerTaskStart): Unit = {\n+    val curTasks = _stageToTasks.getOrElseUpdate(taskStart.stageId,\n+      mutable.HashSet[Long]())\n+    curTasks += taskStart.taskInfo.taskId\n+  }\n+\n+  override def onExecutorAdded(executorAdded: SparkListenerExecutorAdded): Unit = {\n+    _liveExecutors += executorAdded.executorId\n+  }\n+\n+  override def onExecutorRemoved(executorRemoved: SparkListenerExecutorRemoved): Unit = {\n+    _liveExecutors -= executorRemoved.executorId\n+  }\n+\n+  override def createFilter(): EventFilter = BasicEventFilter(this)\n+}\n+\n+/**\n+ * This class provides the functionality to filter out events which are related to the finished\n+ * jobs based on the given information. This class only deals with job related events, and returns\n+ * either Some(true) or Some(false) - successors should override the methods if they don't want to\n+ * return Some(false) for finished jobs and related events.\n+ */\n+private[spark] abstract class JobEventFilter(\n+    jobToStages: Map[Int, Seq[Int]],\n+    stageToTasks: Map[Int, Set[Long]],\n+    stageToRDDs: Map[Int, Seq[Int]]) extends EventFilter with Logging {\n+\n+  private val liveTasks: Set[Long] = stageToTasks.values match {\n+    case xs if xs.isEmpty => Set.empty[Long]\n+    case xs => xs.reduce(_ ++ _).toSet\n+  }\n+\n+  private val liveRDDs: Set[Int] = stageToRDDs.values match {\n+    case xs if xs.isEmpty => Set.empty[Int]\n+    case xs => xs.reduce(_ ++ _).toSet\n+  }\n+\n+  if (log.isDebugEnabled) {\n+    logDebug(s\"jobs : ${jobToStages.keySet}\")\n+    logDebug(s\"stages in jobs : ${jobToStages.values.flatten}\")\n+    logDebug(s\"stages : ${stageToTasks.keySet}\")\n+    logDebug(s\"tasks in stages : ${stageToTasks.values.flatten}\")\n+    logDebug(s\"RDDs in stages : ${stageToRDDs.values.flatten}\")\n+  }\n+\n+  override def filterStageCompleted(event: SparkListenerStageCompleted): Option[Boolean] = {\n+    Some(stageToTasks.contains(event.stageInfo.stageId))\n+  }\n+\n+  override def filterStageSubmitted(event: SparkListenerStageSubmitted): Option[Boolean] = {\n+    Some(stageToTasks.contains(event.stageInfo.stageId))\n+  }\n+\n+  override def filterTaskStart(event: SparkListenerTaskStart): Option[Boolean] = {\n+    Some(liveTasks.contains(event.taskInfo.taskId))\n+  }\n+\n+  override def filterTaskGettingResult(event: SparkListenerTaskGettingResult): Option[Boolean] = {\n+    Some(liveTasks.contains(event.taskInfo.taskId))\n+  }\n+\n+  override def filterTaskEnd(event: SparkListenerTaskEnd): Option[Boolean] = {\n+    Some(liveTasks.contains(event.taskInfo.taskId))\n+  }\n+\n+  override def filterJobStart(event: SparkListenerJobStart): Option[Boolean] = {\n+    Some(jobToStages.contains(event.jobId))\n+  }\n+\n+  override def filterJobEnd(event: SparkListenerJobEnd): Option[Boolean] = {\n+    Some(jobToStages.contains(event.jobId))\n+  }\n+\n+  override def filterUnpersistRDD(event: SparkListenerUnpersistRDD): Option[Boolean] = {\n+    Some(liveRDDs.contains(event.rddId))\n+  }\n+\n+  override def filterExecutorMetricsUpdate(\n+      event: SparkListenerExecutorMetricsUpdate): Option[Boolean] = {\n+    Some(event.accumUpdates.exists { case (_, stageId, _, _) =>\n+      stageToTasks.contains(stageId)\n+    })\n+  }\n+\n+  override def filterSpeculativeTaskSubmitted(\n+      event: SparkListenerSpeculativeTaskSubmitted): Option[Boolean] = {\n+    Some(stageToTasks.contains(event.stageId))\n+  }\n+}\n+\n+/**\n+ * This class filters out events which are related to the finished jobs or dead executors,\n+ * based on the given information. The events which are not related to the job and executor\n+ * will be considered as \"Don't mind\".\n+ */\n+private[spark] class BasicEventFilter(\n+    _liveJobToStages: Map[Int, Seq[Int]],\n+    _stageToTasks: Map[Int, Set[Long]],\n+    _stageToRDDs: Map[Int, Seq[Int]],\n+    liveExecutors: Set[String])\n+  extends JobEventFilter(_liveJobToStages, _stageToTasks, _stageToRDDs) with Logging {\n+\n+  if (log.isDebugEnabled) {"
  }],
  "prId": 26416
}]