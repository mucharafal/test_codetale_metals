[{
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "This will throw if `spark.master` is not set. I'm pretty sure it's set at this point, but just in case, it's safer to call `get(\"spark.master\", null)`.",
    "commit": "1c87238cb28fd28b46f1b7362a20a7538a617ba5",
    "createdAt": "2019-01-30T21:17:49Z",
    "diffHunk": "@@ -133,3 +133,45 @@ private[deploy] class HadoopFSDelegationTokenProvider\n     if (renewIntervals.isEmpty) None else Some(renewIntervals.min)\n   }\n }\n+\n+private[deploy] object HadoopFSDelegationTokenProvider {\n+  def hadoopFSsToAccess(\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): Set[FileSystem] = {\n+    val filesystemsToAccess = sparkConf.get(FILESYSTEMS_TO_ACCESS)\n+    val requestAllDelegationTokens = filesystemsToAccess.isEmpty\n+\n+    val master = sparkConf.get(\"spark.master\")"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Yeah, originally double checked this and it's a required param but doesn't hurt and can be added.",
    "commit": "1c87238cb28fd28b46f1b7362a20a7538a617ba5",
    "createdAt": "2019-01-31T13:03:13Z",
    "diffHunk": "@@ -133,3 +133,45 @@ private[deploy] class HadoopFSDelegationTokenProvider\n     if (renewIntervals.isEmpty) None else Some(renewIntervals.min)\n   }\n }\n+\n+private[deploy] object HadoopFSDelegationTokenProvider {\n+  def hadoopFSsToAccess(\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): Set[FileSystem] = {\n+    val filesystemsToAccess = sparkConf.get(FILESYSTEMS_TO_ACCESS)\n+    val requestAllDelegationTokens = filesystemsToAccess.isEmpty\n+\n+    val master = sparkConf.get(\"spark.master\")"
  }],
  "prId": 23698
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "I'd either not have this variable, or have it also reflect the `viewfs` check (and have the comment from L153 attached to it). I know it comes from old code, but since you're already modifying the logic here a bit...",
    "commit": "1c87238cb28fd28b46f1b7362a20a7538a617ba5",
    "createdAt": "2019-01-30T21:20:41Z",
    "diffHunk": "@@ -133,3 +133,45 @@ private[deploy] class HadoopFSDelegationTokenProvider\n     if (renewIntervals.isEmpty) None else Some(renewIntervals.min)\n   }\n }\n+\n+private[deploy] object HadoopFSDelegationTokenProvider {\n+  def hadoopFSsToAccess(\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): Set[FileSystem] = {\n+    val filesystemsToAccess = sparkConf.get(FILESYSTEMS_TO_ACCESS)\n+    val requestAllDelegationTokens = filesystemsToAccess.isEmpty"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Removed.",
    "commit": "1c87238cb28fd28b46f1b7362a20a7538a617ba5",
    "createdAt": "2019-01-31T13:08:26Z",
    "diffHunk": "@@ -133,3 +133,45 @@ private[deploy] class HadoopFSDelegationTokenProvider\n     if (renewIntervals.isEmpty) None else Some(renewIntervals.min)\n   }\n }\n+\n+private[deploy] object HadoopFSDelegationTokenProvider {\n+  def hadoopFSsToAccess(\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): Set[FileSystem] = {\n+    val filesystemsToAccess = sparkConf.get(FILESYSTEMS_TO_ACCESS)\n+    val requestAllDelegationTokens = filesystemsToAccess.isEmpty"
  }],
  "prId": 23698
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "The docs for `FILESYSTEMS_TO_ACCESS` mention that `fs,defaultFS` does not need to be added, which means that instead of this else, you should always add the default FS to the list.\r\n\r\nThis was a bug in the previous code, probably never caught because it's very, very unusual to store the staging dir in a different fs.",
    "commit": "1c87238cb28fd28b46f1b7362a20a7538a617ba5",
    "createdAt": "2019-01-30T21:30:50Z",
    "diffHunk": "@@ -133,3 +133,45 @@ private[deploy] class HadoopFSDelegationTokenProvider\n     if (renewIntervals.isEmpty) None else Some(renewIntervals.min)\n   }\n }\n+\n+private[deploy] object HadoopFSDelegationTokenProvider {\n+  def hadoopFSsToAccess(\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): Set[FileSystem] = {\n+    val filesystemsToAccess = sparkConf.get(FILESYSTEMS_TO_ACCESS)\n+    val requestAllDelegationTokens = filesystemsToAccess.isEmpty\n+\n+    val master = sparkConf.get(\"spark.master\")\n+    val stagingFS = if (master != null && master.contains(\"yarn\")) {\n+      sparkConf.get(STAGING_DIR)\n+        .map(new Path(_).getFileSystem(hadoopConf))\n+        .getOrElse(FileSystem.get(hadoopConf))\n+    } else {",
    "line": 35
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Nice catch, fixed.",
    "commit": "1c87238cb28fd28b46f1b7362a20a7538a617ba5",
    "createdAt": "2019-01-31T15:48:43Z",
    "diffHunk": "@@ -133,3 +133,45 @@ private[deploy] class HadoopFSDelegationTokenProvider\n     if (renewIntervals.isEmpty) None else Some(renewIntervals.min)\n   }\n }\n+\n+private[deploy] object HadoopFSDelegationTokenProvider {\n+  def hadoopFSsToAccess(\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): Set[FileSystem] = {\n+    val filesystemsToAccess = sparkConf.get(FILESYSTEMS_TO_ACCESS)\n+    val requestAllDelegationTokens = filesystemsToAccess.isEmpty\n+\n+    val master = sparkConf.get(\"spark.master\")\n+    val stagingFS = if (master != null && master.contains(\"yarn\")) {\n+      sparkConf.get(STAGING_DIR)\n+        .map(new Path(_).getFileSystem(hadoopConf))\n+        .getOrElse(FileSystem.get(hadoopConf))\n+    } else {",
    "line": 35
  }],
  "prId": 23698
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Feels weird to have this fall back to `defaultFS` when the default FS is treated separately. An `Option` seems better.",
    "commit": "1c87238cb28fd28b46f1b7362a20a7538a617ba5",
    "createdAt": "2019-01-31T22:13:37Z",
    "diffHunk": "@@ -133,3 +133,45 @@ private[deploy] class HadoopFSDelegationTokenProvider\n     if (renewIntervals.isEmpty) None else Some(renewIntervals.min)\n   }\n }\n+\n+private[deploy] object HadoopFSDelegationTokenProvider {\n+  def hadoopFSsToAccess(\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): Set[FileSystem] = {\n+    val filesystemsToAccess = sparkConf.get(KERBEROS_FILESYSTEMS_TO_ACCESS)\n+\n+    val defaultFS = FileSystem.get(hadoopConf)\n+    val master = sparkConf.get(\"spark.master\", null)\n+    val stagingFS = if (master != null && master.contains(\"yarn\")) {\n+      sparkConf.get(STAGING_DIR)\n+        .map(new Path(_).getFileSystem(hadoopConf))\n+        .getOrElse(defaultFS)"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "What do you mean exactly here? `defaultFS` is `viewfs` in one of the tests. Not checking that would be a behaviour change.",
    "commit": "1c87238cb28fd28b46f1b7362a20a7538a617ba5",
    "createdAt": "2019-02-01T16:43:07Z",
    "diffHunk": "@@ -133,3 +133,45 @@ private[deploy] class HadoopFSDelegationTokenProvider\n     if (renewIntervals.isEmpty) None else Some(renewIntervals.min)\n   }\n }\n+\n+private[deploy] object HadoopFSDelegationTokenProvider {\n+  def hadoopFSsToAccess(\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): Set[FileSystem] = {\n+    val filesystemsToAccess = sparkConf.get(KERBEROS_FILESYSTEMS_TO_ACCESS)\n+\n+    val defaultFS = FileSystem.get(hadoopConf)\n+    val master = sparkConf.get(\"spark.master\", null)\n+    val stagingFS = if (master != null && master.contains(\"yarn\")) {\n+      sparkConf.get(STAGING_DIR)\n+        .map(new Path(_).getFileSystem(hadoopConf))\n+        .getOrElse(defaultFS)"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "You're already adding `defaultFS` explicitly to the final list. So if `STAGING_DIR` is not defined, you'll have `defaultFS` twice. It's ok because the final list is a set, but why would you do that?",
    "commit": "1c87238cb28fd28b46f1b7362a20a7538a617ba5",
    "createdAt": "2019-02-06T17:52:39Z",
    "diffHunk": "@@ -133,3 +133,45 @@ private[deploy] class HadoopFSDelegationTokenProvider\n     if (renewIntervals.isEmpty) None else Some(renewIntervals.min)\n   }\n }\n+\n+private[deploy] object HadoopFSDelegationTokenProvider {\n+  def hadoopFSsToAccess(\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): Set[FileSystem] = {\n+    val filesystemsToAccess = sparkConf.get(KERBEROS_FILESYSTEMS_TO_ACCESS)\n+\n+    val defaultFS = FileSystem.get(hadoopConf)\n+    val master = sparkConf.get(\"spark.master\", null)\n+    val stagingFS = if (master != null && master.contains(\"yarn\")) {\n+      sparkConf.get(STAGING_DIR)\n+        .map(new Path(_).getFileSystem(hadoopConf))\n+        .getOrElse(defaultFS)"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "I see, fixed.",
    "commit": "1c87238cb28fd28b46f1b7362a20a7538a617ba5",
    "createdAt": "2019-02-07T14:30:52Z",
    "diffHunk": "@@ -133,3 +133,45 @@ private[deploy] class HadoopFSDelegationTokenProvider\n     if (renewIntervals.isEmpty) None else Some(renewIntervals.min)\n   }\n }\n+\n+private[deploy] object HadoopFSDelegationTokenProvider {\n+  def hadoopFSsToAccess(\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): Set[FileSystem] = {\n+    val filesystemsToAccess = sparkConf.get(KERBEROS_FILESYSTEMS_TO_ACCESS)\n+\n+    val defaultFS = FileSystem.get(hadoopConf)\n+    val master = sparkConf.get(\"spark.master\", null)\n+    val stagingFS = if (master != null && master.contains(\"yarn\")) {\n+      sparkConf.get(STAGING_DIR)\n+        .map(new Path(_).getFileSystem(hadoopConf))\n+        .getOrElse(defaultFS)"
  }],
  "prId": 23698
}]