[{
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "and some non Hadoop compliant as well, like Kafka.",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-02T13:43:29Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demistify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they",
    "line": 10
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "I think listing all services that use DTs is kinda pointless. Otherwise, let's also includ Solr, Kudu, and who knows what else.",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-03T19:08:42Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demistify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they",
    "line": 10
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "The point here not the list but the fact there are services [which are] + [which aren't] in the Hadoop ecosystem. It's a minor thing so no strong opinion.",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-04T14:57:26Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demistify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they",
    "line": 10
  }],
  "prId": 23348
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Don't really feel the difference between `lifetime` and `renewable life` from these lines.\r\nAs I understand in one case token can be renewed and the content doesn't change.\r\nIn the other case the token is more like re-created because it changes. Is it right?",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-02T16:00:07Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demistify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application so they can authenticate\n+to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g.\n+Spark executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates\n+unnecessary extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used to replace Kerberos authentication in\n+distributed applications, although there is nothing (aside from maybe implementation details) that\n+ties them to Kerberos.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and\n+use them. While it would be possible to create a shared API or even a shared service to manage the\n+creation and use of DTs, that doesn't currently exist, and retrofitting such a system would be a\n+huge change in a bunch of different services.\n+\n+Spark works around this by having a (somewhat) pluggable DT creation API. Support for new\n+services can be added by implementing a \"DT provider\" that is then called by Spark when\n+generating delegation tokens for an application. Spark distributes tokens to executors using\n+the `UserGroupInformation` Hadoop API, and it's up to the DT provider and the respective\n+client library to agree on how to use those tokens.\n+\n+Once they are created, the semantics of how DTs operate are also service-specific. But, in general,\n+they try to follow the semantics of Kerberos tokens:\n+\n+* A \"lifetime\" which is for how long the DT is valid before it requires renewal."
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "There's an example in the next topic if this is not clear. This is the naming used by `kinit` (see its man page).",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-03T19:09:24Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demistify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application so they can authenticate\n+to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g.\n+Spark executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates\n+unnecessary extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used to replace Kerberos authentication in\n+distributed applications, although there is nothing (aside from maybe implementation details) that\n+ties them to Kerberos.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and\n+use them. While it would be possible to create a shared API or even a shared service to manage the\n+creation and use of DTs, that doesn't currently exist, and retrofitting such a system would be a\n+huge change in a bunch of different services.\n+\n+Spark works around this by having a (somewhat) pluggable DT creation API. Support for new\n+services can be added by implementing a \"DT provider\" that is then called by Spark when\n+generating delegation tokens for an application. Spark distributes tokens to executors using\n+the `UserGroupInformation` Hadoop API, and it's up to the DT provider and the respective\n+client library to agree on how to use those tokens.\n+\n+Once they are created, the semantics of how DTs operate are also service-specific. But, in general,\n+they try to follow the semantics of Kerberos tokens:\n+\n+* A \"lifetime\" which is for how long the DT is valid before it requires renewal."
  }, {
    "author": {
      "login": "jerryshao"
    },
    "body": "Yes, I think we should differentiate renew and recreate, AFAIK in the Spark side we always create a new DT before the renewal interval.",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-04T02:05:55Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demistify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application so they can authenticate\n+to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g.\n+Spark executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates\n+unnecessary extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used to replace Kerberos authentication in\n+distributed applications, although there is nothing (aside from maybe implementation details) that\n+ties them to Kerberos.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and\n+use them. While it would be possible to create a shared API or even a shared service to manage the\n+creation and use of DTs, that doesn't currently exist, and retrofitting such a system would be a\n+huge change in a bunch of different services.\n+\n+Spark works around this by having a (somewhat) pluggable DT creation API. Support for new\n+services can be added by implementing a \"DT provider\" that is then called by Spark when\n+generating delegation tokens for an application. Spark distributes tokens to executors using\n+the `UserGroupInformation` Hadoop API, and it's up to the DT provider and the respective\n+client library to agree on how to use those tokens.\n+\n+Once they are created, the semantics of how DTs operate are also service-specific. But, in general,\n+they try to follow the semantics of Kerberos tokens:\n+\n+* A \"lifetime\" which is for how long the DT is valid before it requires renewal."
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "The names are fine. AFAIK after lifetime reached the token can't be renewed but has to be recreated.",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-04T15:32:49Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demistify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application so they can authenticate\n+to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g.\n+Spark executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates\n+unnecessary extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used to replace Kerberos authentication in\n+distributed applications, although there is nothing (aside from maybe implementation details) that\n+ties them to Kerberos.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and\n+use them. While it would be possible to create a shared API or even a shared service to manage the\n+creation and use of DTs, that doesn't currently exist, and retrofitting such a system would be a\n+huge change in a bunch of different services.\n+\n+Spark works around this by having a (somewhat) pluggable DT creation API. Support for new\n+services can be added by implementing a \"DT provider\" that is then called by Spark when\n+generating delegation tokens for an application. Spark distributes tokens to executors using\n+the `UserGroupInformation` Hadoop API, and it's up to the DT provider and the respective\n+client library to agree on how to use those tokens.\n+\n+Once they are created, the semantics of how DTs operate are also service-specific. But, in general,\n+they try to follow the semantics of Kerberos tokens:\n+\n+* A \"lifetime\" which is for how long the DT is valid before it requires renewal."
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "> after lifetime reached the token can't be renewed \r\n\r\nNo, that's the renewable life.",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-04T19:16:24Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demistify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application so they can authenticate\n+to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g.\n+Spark executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates\n+unnecessary extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used to replace Kerberos authentication in\n+distributed applications, although there is nothing (aside from maybe implementation details) that\n+ties them to Kerberos.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and\n+use them. While it would be possible to create a shared API or even a shared service to manage the\n+creation and use of DTs, that doesn't currently exist, and retrofitting such a system would be a\n+huge change in a bunch of different services.\n+\n+Spark works around this by having a (somewhat) pluggable DT creation API. Support for new\n+services can be added by implementing a \"DT provider\" that is then called by Spark when\n+generating delegation tokens for an application. Spark distributes tokens to executors using\n+the `UserGroupInformation` Hadoop API, and it's up to the DT provider and the respective\n+client library to agree on how to use those tokens.\n+\n+Once they are created, the semantics of how DTs operate are also service-specific. But, in general,\n+they try to follow the semantics of Kerberos tokens:\n+\n+* A \"lifetime\" which is for how long the DT is valid before it requires renewal."
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Yeah, written the wrong name. The point is such case worth to say recreate because renew is misleading.",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-07T10:15:11Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demistify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application so they can authenticate\n+to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g.\n+Spark executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates\n+unnecessary extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used to replace Kerberos authentication in\n+distributed applications, although there is nothing (aside from maybe implementation details) that\n+ties them to Kerberos.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and\n+use them. While it would be possible to create a shared API or even a shared service to manage the\n+creation and use of DTs, that doesn't currently exist, and retrofitting such a system would be a\n+huge change in a bunch of different services.\n+\n+Spark works around this by having a (somewhat) pluggable DT creation API. Support for new\n+services can be added by implementing a \"DT provider\" that is then called by Spark when\n+generating delegation tokens for an application. Spark distributes tokens to executors using\n+the `UserGroupInformation` Hadoop API, and it's up to the DT provider and the respective\n+client library to agree on how to use those tokens.\n+\n+Once they are created, the semantics of how DTs operate are also service-specific. But, in general,\n+they try to follow the semantics of Kerberos tokens:\n+\n+* A \"lifetime\" which is for how long the DT is valid before it requires renewal."
  }, {
    "author": {
      "login": "tgravescs"
    },
    "body": "perhaps you should use max lifetime and expiration time, it has to be renewed before the expiration time but it can only be renewed up to the max lifetime",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-07T14:35:24Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demistify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application so they can authenticate\n+to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g.\n+Spark executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates\n+unnecessary extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used to replace Kerberos authentication in\n+distributed applications, although there is nothing (aside from maybe implementation details) that\n+ties them to Kerberos.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and\n+use them. While it would be possible to create a shared API or even a shared service to manage the\n+creation and use of DTs, that doesn't currently exist, and retrofitting such a system would be a\n+huge change in a bunch of different services.\n+\n+Spark works around this by having a (somewhat) pluggable DT creation API. Support for new\n+services can be added by implementing a \"DT provider\" that is then called by Spark when\n+generating delegation tokens for an application. Spark distributes tokens to executors using\n+the `UserGroupInformation` Hadoop API, and it's up to the DT provider and the respective\n+client library to agree on how to use those tokens.\n+\n+Once they are created, the semantics of how DTs operate are also service-specific. But, in general,\n+they try to follow the semantics of Kerberos tokens:\n+\n+* A \"lifetime\" which is for how long the DT is valid before it requires renewal."
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "> recreate because renew is misleading\r\n\r\nIt's not. Here you *can* renew the token. The \"renewable life\" controls what you're saying.\r\n\r\n> perhaps you should use max lifetime and expiration time\r\n\r\nAgain, this is the same naming as kerberos documentation will give you, which I think is, in the end, less confusing. I'll use HDFS names also, just in case.",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-07T17:51:33Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demistify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application so they can authenticate\n+to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g.\n+Spark executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates\n+unnecessary extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used to replace Kerberos authentication in\n+distributed applications, although there is nothing (aside from maybe implementation details) that\n+ties them to Kerberos.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and\n+use them. While it would be possible to create a shared API or even a shared service to manage the\n+creation and use of DTs, that doesn't currently exist, and retrofitting such a system would be a\n+huge change in a bunch of different services.\n+\n+Spark works around this by having a (somewhat) pluggable DT creation API. Support for new\n+services can be added by implementing a \"DT provider\" that is then called by Spark when\n+generating delegation tokens for an application. Spark distributes tokens to executors using\n+the `UserGroupInformation` Hadoop API, and it's up to the DT provider and the respective\n+client library to agree on how to use those tokens.\n+\n+Once they are created, the semantics of how DTs operate are also service-specific. But, in general,\n+they try to follow the semantics of Kerberos tokens:\n+\n+* A \"lifetime\" which is for how long the DT is valid before it requires renewal."
  }],
  "prId": 23348
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "typo .. I believe demistify is demystify.",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-03T09:15:54Z",
    "diffHunk": "@@ -0,0 +1,237 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demistify delegation tokens as they are used by Spark, since"
  }],
  "prId": 23348
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Could we add Spark executor as an example as well?",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-03T09:25:39Z",
    "diffHunk": "@@ -0,0 +1,237 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demistify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application so they can authenticate"
  }],
  "prId": 23348
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "\"KDC (key distribution center)\" once maybe",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-03T09:31:16Z",
    "diffHunk": "@@ -0,0 +1,237 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demistify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application so they can authenticate\n+to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service",
    "line": 26
  }],
  "prId": 23348
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "I would leave a class name as well. BTW, are you referring `HadoopDelegationTokenProvider`? It's not an exposed API.",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-03T09:44:15Z",
    "diffHunk": "@@ -0,0 +1,237 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demistify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application so they can authenticate\n+to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g.\n+Spark executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates\n+unnecessary extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used to replace Kerberos authentication in\n+distributed applications, although there is nothing (aside from maybe implementation details) that\n+ties them to Kerberos.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and\n+use them. While it would be possible to create a shared API or even a shared service to manage the\n+creation and use of DTs, that doesn't currently exist, and retrofitting such a system would be a\n+huge change in a bunch of different services.\n+\n+Spark works around this by having a (somewhat) pluggable DT creation API. Support for new\n+services can be added by implementing a \"DT provider\" that is then called by Spark when"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "This document is in the same package as the API. It's targeted at developers changing / using that API.",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-03T19:04:37Z",
    "diffHunk": "@@ -0,0 +1,237 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demistify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application so they can authenticate\n+to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g.\n+Spark executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates\n+unnecessary extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used to replace Kerberos authentication in\n+distributed applications, although there is nothing (aside from maybe implementation details) that\n+ties them to Kerberos.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and\n+use them. While it would be possible to create a shared API or even a shared service to manage the\n+creation and use of DTs, that doesn't currently exist, and retrofitting such a system would be a\n+huge change in a bunch of different services.\n+\n+Spark works around this by having a (somewhat) pluggable DT creation API. Support for new\n+services can be added by implementing a \"DT provider\" that is then called by Spark when"
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "I see. How about adding the class name `HadoopDelegationTokenProvider` or explicitly mentioning it's an internal API? Maybe i'm too much caring but wonder if it could give an impression that we have an external API that support DT creation.",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-04T02:01:54Z",
    "diffHunk": "@@ -0,0 +1,237 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demistify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application so they can authenticate\n+to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g.\n+Spark executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates\n+unnecessary extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used to replace Kerberos authentication in\n+distributed applications, although there is nothing (aside from maybe implementation details) that\n+ties them to Kerberos.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and\n+use them. While it would be possible to create a shared API or even a shared service to manage the\n+creation and use of DTs, that doesn't currently exist, and retrofitting such a system would be a\n+huge change in a bunch of different services.\n+\n+Spark works around this by having a (somewhat) pluggable DT creation API. Support for new\n+services can be added by implementing a \"DT provider\" that is then called by Spark when"
  }],
  "prId": 23348
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "maybe we better say 1 day or 24 hours to be consistent.\r\n",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-03T09:52:25Z",
    "diffHunk": "@@ -0,0 +1,237 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demistify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application so they can authenticate\n+to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g.\n+Spark executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates\n+unnecessary extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used to replace Kerberos authentication in\n+distributed applications, although there is nothing (aside from maybe implementation details) that\n+ties them to Kerberos.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and\n+use them. While it would be possible to create a shared API or even a shared service to manage the\n+creation and use of DTs, that doesn't currently exist, and retrofitting such a system would be a\n+huge change in a bunch of different services.\n+\n+Spark works around this by having a (somewhat) pluggable DT creation API. Support for new\n+services can be added by implementing a \"DT provider\" that is then called by Spark when\n+generating delegation tokens for an application. Spark distributes tokens to executors using\n+the `UserGroupInformation` Hadoop API, and it's up to the DT provider and the respective\n+client library to agree on how to use those tokens.\n+\n+Once they are created, the semantics of how DTs operate are also service-specific. But, in general,\n+they try to follow the semantics of Kerberos tokens:\n+\n+* A \"lifetime\" which is for how long the DT is valid before it requires renewal.\n+* A \"renewable life\" which is for how long the DT can be renewed.\n+\n+Once the token reaches its \"renewable life\", a new one needs to be created by contacting the\n+appropriate service, restarting the above process.\n+\n+\n+## DT Renewal, Renewers, and YARN\n+\n+This is the most confusing part of DT handling, and part of it is because much of the system was\n+designed with MapReduce, and later YARN, in mind.\n+\n+As seen above, DTs need to be renewed periodically until they finally expire for good. An example\n+of this is the default configuration of HDFS services: delegation tokens are valid for up to 7\n+days, and need to be renewed every 24h. If 24h pass without the token being renewed, the token"
  }],
  "prId": 23348
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Should it be \", which means\" or \". It means\"?",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-03T09:54:49Z",
    "diffHunk": "@@ -0,0 +1,237 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demistify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application so they can authenticate\n+to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g.\n+Spark executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates\n+unnecessary extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used to replace Kerberos authentication in\n+distributed applications, although there is nothing (aside from maybe implementation details) that\n+ties them to Kerberos.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and\n+use them. While it would be possible to create a shared API or even a shared service to manage the\n+creation and use of DTs, that doesn't currently exist, and retrofitting such a system would be a\n+huge change in a bunch of different services.\n+\n+Spark works around this by having a (somewhat) pluggable DT creation API. Support for new\n+services can be added by implementing a \"DT provider\" that is then called by Spark when\n+generating delegation tokens for an application. Spark distributes tokens to executors using\n+the `UserGroupInformation` Hadoop API, and it's up to the DT provider and the respective\n+client library to agree on how to use those tokens.\n+\n+Once they are created, the semantics of how DTs operate are also service-specific. But, in general,\n+they try to follow the semantics of Kerberos tokens:\n+\n+* A \"lifetime\" which is for how long the DT is valid before it requires renewal.\n+* A \"renewable life\" which is for how long the DT can be renewed.\n+\n+Once the token reaches its \"renewable life\", a new one needs to be created by contacting the\n+appropriate service, restarting the above process.\n+\n+\n+## DT Renewal, Renewers, and YARN\n+\n+This is the most confusing part of DT handling, and part of it is because much of the system was\n+designed with MapReduce, and later YARN, in mind.\n+\n+As seen above, DTs need to be renewed periodically until they finally expire for good. An example\n+of this is the default configuration of HDFS services: delegation tokens are valid for up to 7\n+days, and need to be renewed every 24h. If 24h pass without the token being renewed, the token\n+cannot be used anymore. And the token cannot be renewed anymore after 7 days.\n+\n+This raises the question: who renews tokens? And for a long time the answer was YARN.\n+\n+When YARN applications are submitted, a set of DTs is also submitted with them. YARN takes care\n+of distributing these tokens to containers (using conventions set by the `UserGroupInformation`\n+API) and, also, keeping them renewed while the app is running. These tokens are used not just\n+by the application; they are also used by YARN itself to implement features like log collection\n+and aggregation.\n+\n+But this has a few caveats.\n+\n+\n+1. Who renews the tokens?\n+\n+This is handled mostly transparently by the Hadoop libraries in the case of YARN. Some services have\n+the concept of a token \"renewer\". This \"renewer\" is the name of the principal that is allowed to\n+renew the DT. When submitting to YARN, that will be the principal that the YARN service is running\n+as. Which means that the client application needs to know that information."
  }],
  "prId": 23348
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Ditto. Is it correct to say \". Which\"?",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-03T10:02:11Z",
    "diffHunk": "@@ -0,0 +1,237 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demistify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application so they can authenticate\n+to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g.\n+Spark executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates\n+unnecessary extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used to replace Kerberos authentication in\n+distributed applications, although there is nothing (aside from maybe implementation details) that\n+ties them to Kerberos.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and\n+use them. While it would be possible to create a shared API or even a shared service to manage the\n+creation and use of DTs, that doesn't currently exist, and retrofitting such a system would be a\n+huge change in a bunch of different services.\n+\n+Spark works around this by having a (somewhat) pluggable DT creation API. Support for new\n+services can be added by implementing a \"DT provider\" that is then called by Spark when\n+generating delegation tokens for an application. Spark distributes tokens to executors using\n+the `UserGroupInformation` Hadoop API, and it's up to the DT provider and the respective\n+client library to agree on how to use those tokens.\n+\n+Once they are created, the semantics of how DTs operate are also service-specific. But, in general,\n+they try to follow the semantics of Kerberos tokens:\n+\n+* A \"lifetime\" which is for how long the DT is valid before it requires renewal.\n+* A \"renewable life\" which is for how long the DT can be renewed.\n+\n+Once the token reaches its \"renewable life\", a new one needs to be created by contacting the\n+appropriate service, restarting the above process.\n+\n+\n+## DT Renewal, Renewers, and YARN\n+\n+This is the most confusing part of DT handling, and part of it is because much of the system was\n+designed with MapReduce, and later YARN, in mind.\n+\n+As seen above, DTs need to be renewed periodically until they finally expire for good. An example\n+of this is the default configuration of HDFS services: delegation tokens are valid for up to 7\n+days, and need to be renewed every 24h. If 24h pass without the token being renewed, the token\n+cannot be used anymore. And the token cannot be renewed anymore after 7 days.\n+\n+This raises the question: who renews tokens? And for a long time the answer was YARN.\n+\n+When YARN applications are submitted, a set of DTs is also submitted with them. YARN takes care\n+of distributing these tokens to containers (using conventions set by the `UserGroupInformation`\n+API) and, also, keeping them renewed while the app is running. These tokens are used not just\n+by the application; they are also used by YARN itself to implement features like log collection\n+and aggregation.\n+\n+But this has a few caveats.\n+\n+\n+1. Who renews the tokens?\n+\n+This is handled mostly transparently by the Hadoop libraries in the case of YARN. Some services have\n+the concept of a token \"renewer\". This \"renewer\" is the name of the principal that is allowed to\n+renew the DT. When submitting to YARN, that will be the principal that the YARN service is running\n+as. Which means that the client application needs to know that information.\n+\n+For other resource managers, the renewer mostly does not matter, since there is no service that\n+is doing the renewal. Except that it sometimes leaks into library code, such as in SPARK-20328.\n+\n+\n+2. What tokens are renewed?\n+\n+This is probably the biggest caveat.\n+\n+As discussed in the previous section, DTs are service-specific, and require service-specific\n+libraries for creation *and* renewal. This means that for YARN to be able to renew application\n+tokens, YARN needs:\n+\n+* The client libraries for all the services the application is using\n+* Information about how to connect to the services the application is using\n+* Permissions to connect to those services\n+\n+In reality, though, most of the time YARN has access to a single HDFS cluster, and that will be\n+the extent of its DT renewal features. Any other tokens sent to YARN will be distributed to\n+containers, but will not be renewed.\n+\n+This means that those tokens will expire way before their max lifetime, unless some other code\n+takes care of renewing them.\n+\n+And, to make matters worse, not all client libraries even implement token renewal. To use the\n+example of a service supported by Spark, the `renew()` method of HBase tokens is a no-op. So\n+the only way to \"renew\" an HBase token is to create a new one.\n+\n+\n+3. What happens when tokens expire for good?\n+\n+The final caveat is that DTs have a maximum life, regardless of renewal. And after that deadline\n+is met, you need to create new tokens to be able to connect to the services. Which means you need"
  }],
  "prId": 23348
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Sorry if i'm being stupid but what's \"OS users\"?",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-03T10:15:07Z",
    "diffHunk": "@@ -0,0 +1,237 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demistify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application so they can authenticate\n+to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g.\n+Spark executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates\n+unnecessary extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used to replace Kerberos authentication in\n+distributed applications, although there is nothing (aside from maybe implementation details) that\n+ties them to Kerberos.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and\n+use them. While it would be possible to create a shared API or even a shared service to manage the\n+creation and use of DTs, that doesn't currently exist, and retrofitting such a system would be a\n+huge change in a bunch of different services.\n+\n+Spark works around this by having a (somewhat) pluggable DT creation API. Support for new\n+services can be added by implementing a \"DT provider\" that is then called by Spark when\n+generating delegation tokens for an application. Spark distributes tokens to executors using\n+the `UserGroupInformation` Hadoop API, and it's up to the DT provider and the respective\n+client library to agree on how to use those tokens.\n+\n+Once they are created, the semantics of how DTs operate are also service-specific. But, in general,\n+they try to follow the semantics of Kerberos tokens:\n+\n+* A \"lifetime\" which is for how long the DT is valid before it requires renewal.\n+* A \"renewable life\" which is for how long the DT can be renewed.\n+\n+Once the token reaches its \"renewable life\", a new one needs to be created by contacting the\n+appropriate service, restarting the above process.\n+\n+\n+## DT Renewal, Renewers, and YARN\n+\n+This is the most confusing part of DT handling, and part of it is because much of the system was\n+designed with MapReduce, and later YARN, in mind.\n+\n+As seen above, DTs need to be renewed periodically until they finally expire for good. An example\n+of this is the default configuration of HDFS services: delegation tokens are valid for up to 7\n+days, and need to be renewed every 24h. If 24h pass without the token being renewed, the token\n+cannot be used anymore. And the token cannot be renewed anymore after 7 days.\n+\n+This raises the question: who renews tokens? And for a long time the answer was YARN.\n+\n+When YARN applications are submitted, a set of DTs is also submitted with them. YARN takes care\n+of distributing these tokens to containers (using conventions set by the `UserGroupInformation`\n+API) and, also, keeping them renewed while the app is running. These tokens are used not just\n+by the application; they are also used by YARN itself to implement features like log collection\n+and aggregation.\n+\n+But this has a few caveats.\n+\n+\n+1. Who renews the tokens?\n+\n+This is handled mostly transparently by the Hadoop libraries in the case of YARN. Some services have\n+the concept of a token \"renewer\". This \"renewer\" is the name of the principal that is allowed to\n+renew the DT. When submitting to YARN, that will be the principal that the YARN service is running\n+as. Which means that the client application needs to know that information.\n+\n+For other resource managers, the renewer mostly does not matter, since there is no service that\n+is doing the renewal. Except that it sometimes leaks into library code, such as in SPARK-20328.\n+\n+\n+2. What tokens are renewed?\n+\n+This is probably the biggest caveat.\n+\n+As discussed in the previous section, DTs are service-specific, and require service-specific\n+libraries for creation *and* renewal. This means that for YARN to be able to renew application\n+tokens, YARN needs:\n+\n+* The client libraries for all the services the application is using\n+* Information about how to connect to the services the application is using\n+* Permissions to connect to those services\n+\n+In reality, though, most of the time YARN has access to a single HDFS cluster, and that will be\n+the extent of its DT renewal features. Any other tokens sent to YARN will be distributed to\n+containers, but will not be renewed.\n+\n+This means that those tokens will expire way before their max lifetime, unless some other code\n+takes care of renewing them.\n+\n+And, to make matters worse, not all client libraries even implement token renewal. To use the\n+example of a service supported by Spark, the `renew()` method of HBase tokens is a no-op. So\n+the only way to \"renew\" an HBase token is to create a new one.\n+\n+\n+3. What happens when tokens expire for good?\n+\n+The final caveat is that DTs have a maximum life, regardless of renewal. And after that deadline\n+is met, you need to create new tokens to be able to connect to the services. Which means you need\n+the ability to connect to the service without a delegation token, which means you need Kerberos\n+credentials.\n+\n+This is especially important for long-running applications that run unsupervised. They must be\n+able to keep on going without having someone logging into a terminal and typing a password every\n+few days.\n+\n+\n+## DT Renewal in Spark\n+\n+Because of the issues explained above, Spark implements a different way of doing renewal. Spark's\n+solution is a compromise: it targets the lowest common denominator, which is services like HBase\n+that do not support actual token renewal.\n+\n+In Spark, DT \"renewal\" is enabled by giving the application a Kerberos keytab. A keytab is\n+basically your Kerberos password written into a plain text file, which is why it's so sensitive.\n+By having the keytab, Spark can indefinitely maintain a valid Kerberos TGT.\n+\n+With Kerberos credentials available, Spark will create new DTs for the configured services as old\n+ones expire. So Spark doesn't renew tokens as explained in the previous section: it will create new\n+tokens at every renewal interval instead, and distribute those tokens to executors.\n+\n+This also has another advantage on top of supporting services like HBase: it removes the dependency\n+on an external renewal service (like YARN). That way, Spark's renewal feature can be used with\n+resource managers that are not DT-aware, such as Mesos or Kubernetes, as long as the application\n+has access to a keytab.\n+\n+\n+## DTs and Proxy Users\n+\n+\"Proxy users\" is Hadoop-speak for impersonation. It allows user A to impersonate user B when\n+connecting to a service, if that service allows it.\n+\n+Spark allows impersonation when submitting applications, so that the whole application runs as\n+user B in the above example.\n+\n+Spark does not allow token renewal when impersonation is on. Impersonation was added in Spark\n+as a means for services (like Hive or Oozie) to start Spark applications on behalf of users.\n+That means that those services would provide the Spark launcher code with privileged credentials,\n+to submit applications and, potentially, user code to run as a regular user.\n+\n+In that situation, the service credentials should never be made available to the Spark application,\n+since that would be tantamount to giving your service credentials to unprivileged users.\n+\n+The above also implies that running impersonated applications in client mode can be a security\n+concern, since arbitrary user code would have access to the same local content as the privileged\n+user. But unlike token renewal, Spark does not prevent that configuration from running.\n+\n+When impersonating, the Spark launcher will create DTs for the \"proxy\" user. In the example\n+used above, that means that when code authenticates to a service using the DTs, the authenticated\n+user will be \"B\", not \"A\".\n+\n+Note that \"proxy user\" is a very Hadoop-specific concept. It does not apply to OS users (which",
    "line": 196
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Linux OS users and not kerberos users.",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-03T14:16:31Z",
    "diffHunk": "@@ -0,0 +1,237 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demistify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application so they can authenticate\n+to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g.\n+Spark executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates\n+unnecessary extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used to replace Kerberos authentication in\n+distributed applications, although there is nothing (aside from maybe implementation details) that\n+ties them to Kerberos.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and\n+use them. While it would be possible to create a shared API or even a shared service to manage the\n+creation and use of DTs, that doesn't currently exist, and retrofitting such a system would be a\n+huge change in a bunch of different services.\n+\n+Spark works around this by having a (somewhat) pluggable DT creation API. Support for new\n+services can be added by implementing a \"DT provider\" that is then called by Spark when\n+generating delegation tokens for an application. Spark distributes tokens to executors using\n+the `UserGroupInformation` Hadoop API, and it's up to the DT provider and the respective\n+client library to agree on how to use those tokens.\n+\n+Once they are created, the semantics of how DTs operate are also service-specific. But, in general,\n+they try to follow the semantics of Kerberos tokens:\n+\n+* A \"lifetime\" which is for how long the DT is valid before it requires renewal.\n+* A \"renewable life\" which is for how long the DT can be renewed.\n+\n+Once the token reaches its \"renewable life\", a new one needs to be created by contacting the\n+appropriate service, restarting the above process.\n+\n+\n+## DT Renewal, Renewers, and YARN\n+\n+This is the most confusing part of DT handling, and part of it is because much of the system was\n+designed with MapReduce, and later YARN, in mind.\n+\n+As seen above, DTs need to be renewed periodically until they finally expire for good. An example\n+of this is the default configuration of HDFS services: delegation tokens are valid for up to 7\n+days, and need to be renewed every 24h. If 24h pass without the token being renewed, the token\n+cannot be used anymore. And the token cannot be renewed anymore after 7 days.\n+\n+This raises the question: who renews tokens? And for a long time the answer was YARN.\n+\n+When YARN applications are submitted, a set of DTs is also submitted with them. YARN takes care\n+of distributing these tokens to containers (using conventions set by the `UserGroupInformation`\n+API) and, also, keeping them renewed while the app is running. These tokens are used not just\n+by the application; they are also used by YARN itself to implement features like log collection\n+and aggregation.\n+\n+But this has a few caveats.\n+\n+\n+1. Who renews the tokens?\n+\n+This is handled mostly transparently by the Hadoop libraries in the case of YARN. Some services have\n+the concept of a token \"renewer\". This \"renewer\" is the name of the principal that is allowed to\n+renew the DT. When submitting to YARN, that will be the principal that the YARN service is running\n+as. Which means that the client application needs to know that information.\n+\n+For other resource managers, the renewer mostly does not matter, since there is no service that\n+is doing the renewal. Except that it sometimes leaks into library code, such as in SPARK-20328.\n+\n+\n+2. What tokens are renewed?\n+\n+This is probably the biggest caveat.\n+\n+As discussed in the previous section, DTs are service-specific, and require service-specific\n+libraries for creation *and* renewal. This means that for YARN to be able to renew application\n+tokens, YARN needs:\n+\n+* The client libraries for all the services the application is using\n+* Information about how to connect to the services the application is using\n+* Permissions to connect to those services\n+\n+In reality, though, most of the time YARN has access to a single HDFS cluster, and that will be\n+the extent of its DT renewal features. Any other tokens sent to YARN will be distributed to\n+containers, but will not be renewed.\n+\n+This means that those tokens will expire way before their max lifetime, unless some other code\n+takes care of renewing them.\n+\n+And, to make matters worse, not all client libraries even implement token renewal. To use the\n+example of a service supported by Spark, the `renew()` method of HBase tokens is a no-op. So\n+the only way to \"renew\" an HBase token is to create a new one.\n+\n+\n+3. What happens when tokens expire for good?\n+\n+The final caveat is that DTs have a maximum life, regardless of renewal. And after that deadline\n+is met, you need to create new tokens to be able to connect to the services. Which means you need\n+the ability to connect to the service without a delegation token, which means you need Kerberos\n+credentials.\n+\n+This is especially important for long-running applications that run unsupervised. They must be\n+able to keep on going without having someone logging into a terminal and typing a password every\n+few days.\n+\n+\n+## DT Renewal in Spark\n+\n+Because of the issues explained above, Spark implements a different way of doing renewal. Spark's\n+solution is a compromise: it targets the lowest common denominator, which is services like HBase\n+that do not support actual token renewal.\n+\n+In Spark, DT \"renewal\" is enabled by giving the application a Kerberos keytab. A keytab is\n+basically your Kerberos password written into a plain text file, which is why it's so sensitive.\n+By having the keytab, Spark can indefinitely maintain a valid Kerberos TGT.\n+\n+With Kerberos credentials available, Spark will create new DTs for the configured services as old\n+ones expire. So Spark doesn't renew tokens as explained in the previous section: it will create new\n+tokens at every renewal interval instead, and distribute those tokens to executors.\n+\n+This also has another advantage on top of supporting services like HBase: it removes the dependency\n+on an external renewal service (like YARN). That way, Spark's renewal feature can be used with\n+resource managers that are not DT-aware, such as Mesos or Kubernetes, as long as the application\n+has access to a keytab.\n+\n+\n+## DTs and Proxy Users\n+\n+\"Proxy users\" is Hadoop-speak for impersonation. It allows user A to impersonate user B when\n+connecting to a service, if that service allows it.\n+\n+Spark allows impersonation when submitting applications, so that the whole application runs as\n+user B in the above example.\n+\n+Spark does not allow token renewal when impersonation is on. Impersonation was added in Spark\n+as a means for services (like Hive or Oozie) to start Spark applications on behalf of users.\n+That means that those services would provide the Spark launcher code with privileged credentials,\n+to submit applications and, potentially, user code to run as a regular user.\n+\n+In that situation, the service credentials should never be made available to the Spark application,\n+since that would be tantamount to giving your service credentials to unprivileged users.\n+\n+The above also implies that running impersonated applications in client mode can be a security\n+concern, since arbitrary user code would have access to the same local content as the privileged\n+user. But unlike token renewal, Spark does not prevent that configuration from running.\n+\n+When impersonating, the Spark launcher will create DTs for the \"proxy\" user. In the example\n+used above, that means that when code authenticates to a service using the DTs, the authenticated\n+user will be \"B\", not \"A\".\n+\n+Note that \"proxy user\" is a very Hadoop-specific concept. It does not apply to OS users (which",
    "line": 196
  }],
  "prId": 23348
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "`For this reason, certain DT\r\nproviders cannot provide a renewal period to the Spark code`\r\nJust for the understanding are these HBase + Hive, right?",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-03T14:07:10Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demistify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application so they can authenticate\n+to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g.\n+Spark executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates\n+unnecessary extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used to replace Kerberos authentication in\n+distributed applications, although there is nothing (aside from maybe implementation details) that\n+ties them to Kerberos.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and\n+use them. While it would be possible to create a shared API or even a shared service to manage the\n+creation and use of DTs, that doesn't currently exist, and retrofitting such a system would be a\n+huge change in a bunch of different services.\n+\n+Spark works around this by having a (somewhat) pluggable DT creation API. Support for new\n+services can be added by implementing a \"DT provider\" that is then called by Spark when\n+generating delegation tokens for an application. Spark distributes tokens to executors using\n+the `UserGroupInformation` Hadoop API, and it's up to the DT provider and the respective\n+client library to agree on how to use those tokens.\n+\n+Once they are created, the semantics of how DTs operate are also service-specific. But, in general,\n+they try to follow the semantics of Kerberos tokens:\n+\n+* A \"lifetime\" which is for how long the DT is valid before it requires renewal.\n+* A \"renewable life\" which is for how long the DT can be renewed.\n+\n+Once the token reaches its \"renewable life\", a new one needs to be created by contacting the\n+appropriate service, restarting the above process.\n+\n+\n+## DT Renewal, Renewers, and YARN\n+\n+This is the most confusing part of DT handling, and part of it is because much of the system was\n+designed with MapReduce, and later YARN, in mind.\n+\n+As seen above, DTs need to be renewed periodically until they finally expire for good. An example\n+of this is the default configuration of HDFS services: delegation tokens are valid for up to 7\n+days, and need to be renewed every 24h. If 24h pass without the token being renewed, the token\n+cannot be used anymore. And the token cannot be renewed anymore after 7 days.\n+\n+This raises the question: who renews tokens? And for a long time the answer was YARN.\n+\n+When YARN applications are submitted, a set of DTs is also submitted with them. YARN takes care\n+of distributing these tokens to containers (using conventions set by the `UserGroupInformation`\n+API) and, also, keeping them renewed while the app is running. These tokens are used not just\n+by the application; they are also used by YARN itself to implement features like log collection\n+and aggregation.\n+\n+But this has a few caveats.\n+\n+\n+1. Who renews the tokens?\n+\n+This is handled mostly transparently by the Hadoop libraries in the case of YARN. Some services have\n+the concept of a token \"renewer\". This \"renewer\" is the name of the principal that is allowed to\n+renew the DT. When submitting to YARN, that will be the principal that the YARN service is running\n+as. Which means that the client application needs to know that information.\n+\n+For other resource managers, the renewer mostly does not matter, since there is no service that\n+is doing the renewal. Except that it sometimes leaks into library code, such as in SPARK-20328.\n+\n+\n+2. What tokens are renewed?\n+\n+This is probably the biggest caveat.\n+\n+As discussed in the previous section, DTs are service-specific, and require service-specific\n+libraries for creation *and* renewal. This means that for YARN to be able to renew application\n+tokens, YARN needs:\n+\n+* The client libraries for all the services the application is using\n+* Information about how to connect to the services the application is using\n+* Permissions to connect to those services\n+\n+In reality, though, most of the time YARN has access to a single HDFS cluster, and that will be\n+the extent of its DT renewal features. Any other tokens sent to YARN will be distributed to\n+containers, but will not be renewed.\n+\n+This means that those tokens will expire way before their max lifetime, unless some other code\n+takes care of renewing them.\n+\n+And, to make matters worse, not all client libraries even implement token renewal. To use the\n+example of a service supported by Spark, the `renew()` method of HBase tokens is a no-op. So\n+the only way to \"renew\" an HBase token is to create a new one.\n+\n+\n+3. What happens when tokens expire for good?\n+\n+The final caveat is that DTs have a maximum life, regardless of renewal. And after that deadline\n+is met, you need to create new tokens to be able to connect to the services. Which means you need\n+the ability to connect to the service without a delegation token, which means you need Kerberos\n+credentials.\n+\n+This is especially important for long-running applications that run unsupervised. They must be\n+able to keep on going without having someone logging into a terminal and typing a password every\n+few days.\n+\n+\n+## DT Renewal in Spark\n+\n+Because of the issues explained above, Spark implements a different way of doing renewal. Spark's\n+solution is a compromise: it targets the lowest common denominator, which is services like HBase\n+that do not support actual token renewal.\n+\n+In Spark, DT \"renewal\" is enabled by giving the application a Kerberos keytab. A keytab is\n+basically your Kerberos password written into a plain text file, which is why it's so sensitive.\n+By having the keytab, Spark can indefinitely maintain a valid Kerberos TGT.\n+\n+With Kerberos credentials available, Spark will create new DTs for the configured services as old\n+ones expire. So Spark doesn't renew tokens as explained in the previous section: it will create new\n+tokens at every renewal interval instead, and distribute those tokens to executors.\n+\n+This also has another advantage on top of supporting services like HBase: it removes the dependency\n+on an external renewal service (like YARN). That way, Spark's renewal feature can be used with\n+resource managers that are not DT-aware, such as Mesos or Kubernetes, as long as the application\n+has access to a keytab.\n+\n+\n+## DTs and Proxy Users\n+\n+\"Proxy users\" is Hadoop-speak for impersonation. It allows user A to impersonate user B when\n+connecting to a service, if that service allows it.\n+\n+Spark allows impersonation when submitting applications, so that the whole application runs as\n+user B in the above example.\n+\n+Spark does not allow token renewal when impersonation is on. Impersonation was added in Spark\n+as a means for services (like Hive or Oozie) to start Spark applications on behalf of users.\n+That means that those services would provide the Spark launcher code with privileged credentials,\n+to submit applications and, potentially, user code to run as a regular user.\n+\n+In that situation, the service credentials should never be made available to the Spark application,\n+since that would be tantamount to giving your service credentials to unprivileged users.\n+\n+The above also implies that running impersonated applications in client mode can be a security\n+concern, since arbitrary user code would have access to the same local content as the privileged\n+user. But unlike token renewal, Spark does not prevent that configuration from running.\n+\n+When impersonating, the Spark launcher will create DTs for the \"proxy\" user. In the example\n+used above, that means that when code authenticates to a service using the DTs, the authenticated\n+user will be \"B\", not \"A\".\n+\n+Note that \"proxy user\" is a very Hadoop-specific concept. It does not apply to OS users (which\n+is why the client-mode case is an issue) and to services that do not authenticate using Hadoop's\n+`UserGroupInformation` system. It is generally used in the context of YARN - since an application\n+submitted as a proxy user will run as that particular user in the YARN cluster, obeying any\n+Hadoop-to-local-OS-user mapping configured for the service. But the overall support should work\n+for connecting to other services even when YARN is not being used.\n+\n+\n+## Limitations of DT support in Spark\n+\n+There are certain limitations to bear in mind when talking about DTs in Spark.\n+\n+The first one is that not all DTs actually expose their renewal period. This is generally a\n+service configuration that is not generally exposed to clients. For this reason, certain DT",
    "line": 228
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "That's the case in the current providers available in Spark. But I can't say those are the only examples. This is not meant to explain the Spark code, it's meant to explain DTs in general.",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-03T19:10:06Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demistify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application so they can authenticate\n+to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g.\n+Spark executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates\n+unnecessary extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used to replace Kerberos authentication in\n+distributed applications, although there is nothing (aside from maybe implementation details) that\n+ties them to Kerberos.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and\n+use them. While it would be possible to create a shared API or even a shared service to manage the\n+creation and use of DTs, that doesn't currently exist, and retrofitting such a system would be a\n+huge change in a bunch of different services.\n+\n+Spark works around this by having a (somewhat) pluggable DT creation API. Support for new\n+services can be added by implementing a \"DT provider\" that is then called by Spark when\n+generating delegation tokens for an application. Spark distributes tokens to executors using\n+the `UserGroupInformation` Hadoop API, and it's up to the DT provider and the respective\n+client library to agree on how to use those tokens.\n+\n+Once they are created, the semantics of how DTs operate are also service-specific. But, in general,\n+they try to follow the semantics of Kerberos tokens:\n+\n+* A \"lifetime\" which is for how long the DT is valid before it requires renewal.\n+* A \"renewable life\" which is for how long the DT can be renewed.\n+\n+Once the token reaches its \"renewable life\", a new one needs to be created by contacting the\n+appropriate service, restarting the above process.\n+\n+\n+## DT Renewal, Renewers, and YARN\n+\n+This is the most confusing part of DT handling, and part of it is because much of the system was\n+designed with MapReduce, and later YARN, in mind.\n+\n+As seen above, DTs need to be renewed periodically until they finally expire for good. An example\n+of this is the default configuration of HDFS services: delegation tokens are valid for up to 7\n+days, and need to be renewed every 24h. If 24h pass without the token being renewed, the token\n+cannot be used anymore. And the token cannot be renewed anymore after 7 days.\n+\n+This raises the question: who renews tokens? And for a long time the answer was YARN.\n+\n+When YARN applications are submitted, a set of DTs is also submitted with them. YARN takes care\n+of distributing these tokens to containers (using conventions set by the `UserGroupInformation`\n+API) and, also, keeping them renewed while the app is running. These tokens are used not just\n+by the application; they are also used by YARN itself to implement features like log collection\n+and aggregation.\n+\n+But this has a few caveats.\n+\n+\n+1. Who renews the tokens?\n+\n+This is handled mostly transparently by the Hadoop libraries in the case of YARN. Some services have\n+the concept of a token \"renewer\". This \"renewer\" is the name of the principal that is allowed to\n+renew the DT. When submitting to YARN, that will be the principal that the YARN service is running\n+as. Which means that the client application needs to know that information.\n+\n+For other resource managers, the renewer mostly does not matter, since there is no service that\n+is doing the renewal. Except that it sometimes leaks into library code, such as in SPARK-20328.\n+\n+\n+2. What tokens are renewed?\n+\n+This is probably the biggest caveat.\n+\n+As discussed in the previous section, DTs are service-specific, and require service-specific\n+libraries for creation *and* renewal. This means that for YARN to be able to renew application\n+tokens, YARN needs:\n+\n+* The client libraries for all the services the application is using\n+* Information about how to connect to the services the application is using\n+* Permissions to connect to those services\n+\n+In reality, though, most of the time YARN has access to a single HDFS cluster, and that will be\n+the extent of its DT renewal features. Any other tokens sent to YARN will be distributed to\n+containers, but will not be renewed.\n+\n+This means that those tokens will expire way before their max lifetime, unless some other code\n+takes care of renewing them.\n+\n+And, to make matters worse, not all client libraries even implement token renewal. To use the\n+example of a service supported by Spark, the `renew()` method of HBase tokens is a no-op. So\n+the only way to \"renew\" an HBase token is to create a new one.\n+\n+\n+3. What happens when tokens expire for good?\n+\n+The final caveat is that DTs have a maximum life, regardless of renewal. And after that deadline\n+is met, you need to create new tokens to be able to connect to the services. Which means you need\n+the ability to connect to the service without a delegation token, which means you need Kerberos\n+credentials.\n+\n+This is especially important for long-running applications that run unsupervised. They must be\n+able to keep on going without having someone logging into a terminal and typing a password every\n+few days.\n+\n+\n+## DT Renewal in Spark\n+\n+Because of the issues explained above, Spark implements a different way of doing renewal. Spark's\n+solution is a compromise: it targets the lowest common denominator, which is services like HBase\n+that do not support actual token renewal.\n+\n+In Spark, DT \"renewal\" is enabled by giving the application a Kerberos keytab. A keytab is\n+basically your Kerberos password written into a plain text file, which is why it's so sensitive.\n+By having the keytab, Spark can indefinitely maintain a valid Kerberos TGT.\n+\n+With Kerberos credentials available, Spark will create new DTs for the configured services as old\n+ones expire. So Spark doesn't renew tokens as explained in the previous section: it will create new\n+tokens at every renewal interval instead, and distribute those tokens to executors.\n+\n+This also has another advantage on top of supporting services like HBase: it removes the dependency\n+on an external renewal service (like YARN). That way, Spark's renewal feature can be used with\n+resource managers that are not DT-aware, such as Mesos or Kubernetes, as long as the application\n+has access to a keytab.\n+\n+\n+## DTs and Proxy Users\n+\n+\"Proxy users\" is Hadoop-speak for impersonation. It allows user A to impersonate user B when\n+connecting to a service, if that service allows it.\n+\n+Spark allows impersonation when submitting applications, so that the whole application runs as\n+user B in the above example.\n+\n+Spark does not allow token renewal when impersonation is on. Impersonation was added in Spark\n+as a means for services (like Hive or Oozie) to start Spark applications on behalf of users.\n+That means that those services would provide the Spark launcher code with privileged credentials,\n+to submit applications and, potentially, user code to run as a regular user.\n+\n+In that situation, the service credentials should never be made available to the Spark application,\n+since that would be tantamount to giving your service credentials to unprivileged users.\n+\n+The above also implies that running impersonated applications in client mode can be a security\n+concern, since arbitrary user code would have access to the same local content as the privileged\n+user. But unlike token renewal, Spark does not prevent that configuration from running.\n+\n+When impersonating, the Spark launcher will create DTs for the \"proxy\" user. In the example\n+used above, that means that when code authenticates to a service using the DTs, the authenticated\n+user will be \"B\", not \"A\".\n+\n+Note that \"proxy user\" is a very Hadoop-specific concept. It does not apply to OS users (which\n+is why the client-mode case is an issue) and to services that do not authenticate using Hadoop's\n+`UserGroupInformation` system. It is generally used in the context of YARN - since an application\n+submitted as a proxy user will run as that particular user in the YARN cluster, obeying any\n+Hadoop-to-local-OS-user mapping configured for the service. But the overall support should work\n+for connecting to other services even when YARN is not being used.\n+\n+\n+## Limitations of DT support in Spark\n+\n+There are certain limitations to bear in mind when talking about DTs in Spark.\n+\n+The first one is that not all DTs actually expose their renewal period. This is generally a\n+service configuration that is not generally exposed to clients. For this reason, certain DT",
    "line": 228
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Here I didn't expect any changes, just wanted to know which are fulfilling the criteria.",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-04T14:59:45Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demistify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application so they can authenticate\n+to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g.\n+Spark executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates\n+unnecessary extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used to replace Kerberos authentication in\n+distributed applications, although there is nothing (aside from maybe implementation details) that\n+ties them to Kerberos.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and\n+use them. While it would be possible to create a shared API or even a shared service to manage the\n+creation and use of DTs, that doesn't currently exist, and retrofitting such a system would be a\n+huge change in a bunch of different services.\n+\n+Spark works around this by having a (somewhat) pluggable DT creation API. Support for new\n+services can be added by implementing a \"DT provider\" that is then called by Spark when\n+generating delegation tokens for an application. Spark distributes tokens to executors using\n+the `UserGroupInformation` Hadoop API, and it's up to the DT provider and the respective\n+client library to agree on how to use those tokens.\n+\n+Once they are created, the semantics of how DTs operate are also service-specific. But, in general,\n+they try to follow the semantics of Kerberos tokens:\n+\n+* A \"lifetime\" which is for how long the DT is valid before it requires renewal.\n+* A \"renewable life\" which is for how long the DT can be renewed.\n+\n+Once the token reaches its \"renewable life\", a new one needs to be created by contacting the\n+appropriate service, restarting the above process.\n+\n+\n+## DT Renewal, Renewers, and YARN\n+\n+This is the most confusing part of DT handling, and part of it is because much of the system was\n+designed with MapReduce, and later YARN, in mind.\n+\n+As seen above, DTs need to be renewed periodically until they finally expire for good. An example\n+of this is the default configuration of HDFS services: delegation tokens are valid for up to 7\n+days, and need to be renewed every 24h. If 24h pass without the token being renewed, the token\n+cannot be used anymore. And the token cannot be renewed anymore after 7 days.\n+\n+This raises the question: who renews tokens? And for a long time the answer was YARN.\n+\n+When YARN applications are submitted, a set of DTs is also submitted with them. YARN takes care\n+of distributing these tokens to containers (using conventions set by the `UserGroupInformation`\n+API) and, also, keeping them renewed while the app is running. These tokens are used not just\n+by the application; they are also used by YARN itself to implement features like log collection\n+and aggregation.\n+\n+But this has a few caveats.\n+\n+\n+1. Who renews the tokens?\n+\n+This is handled mostly transparently by the Hadoop libraries in the case of YARN. Some services have\n+the concept of a token \"renewer\". This \"renewer\" is the name of the principal that is allowed to\n+renew the DT. When submitting to YARN, that will be the principal that the YARN service is running\n+as. Which means that the client application needs to know that information.\n+\n+For other resource managers, the renewer mostly does not matter, since there is no service that\n+is doing the renewal. Except that it sometimes leaks into library code, such as in SPARK-20328.\n+\n+\n+2. What tokens are renewed?\n+\n+This is probably the biggest caveat.\n+\n+As discussed in the previous section, DTs are service-specific, and require service-specific\n+libraries for creation *and* renewal. This means that for YARN to be able to renew application\n+tokens, YARN needs:\n+\n+* The client libraries for all the services the application is using\n+* Information about how to connect to the services the application is using\n+* Permissions to connect to those services\n+\n+In reality, though, most of the time YARN has access to a single HDFS cluster, and that will be\n+the extent of its DT renewal features. Any other tokens sent to YARN will be distributed to\n+containers, but will not be renewed.\n+\n+This means that those tokens will expire way before their max lifetime, unless some other code\n+takes care of renewing them.\n+\n+And, to make matters worse, not all client libraries even implement token renewal. To use the\n+example of a service supported by Spark, the `renew()` method of HBase tokens is a no-op. So\n+the only way to \"renew\" an HBase token is to create a new one.\n+\n+\n+3. What happens when tokens expire for good?\n+\n+The final caveat is that DTs have a maximum life, regardless of renewal. And after that deadline\n+is met, you need to create new tokens to be able to connect to the services. Which means you need\n+the ability to connect to the service without a delegation token, which means you need Kerberos\n+credentials.\n+\n+This is especially important for long-running applications that run unsupervised. They must be\n+able to keep on going without having someone logging into a terminal and typing a password every\n+few days.\n+\n+\n+## DT Renewal in Spark\n+\n+Because of the issues explained above, Spark implements a different way of doing renewal. Spark's\n+solution is a compromise: it targets the lowest common denominator, which is services like HBase\n+that do not support actual token renewal.\n+\n+In Spark, DT \"renewal\" is enabled by giving the application a Kerberos keytab. A keytab is\n+basically your Kerberos password written into a plain text file, which is why it's so sensitive.\n+By having the keytab, Spark can indefinitely maintain a valid Kerberos TGT.\n+\n+With Kerberos credentials available, Spark will create new DTs for the configured services as old\n+ones expire. So Spark doesn't renew tokens as explained in the previous section: it will create new\n+tokens at every renewal interval instead, and distribute those tokens to executors.\n+\n+This also has another advantage on top of supporting services like HBase: it removes the dependency\n+on an external renewal service (like YARN). That way, Spark's renewal feature can be used with\n+resource managers that are not DT-aware, such as Mesos or Kubernetes, as long as the application\n+has access to a keytab.\n+\n+\n+## DTs and Proxy Users\n+\n+\"Proxy users\" is Hadoop-speak for impersonation. It allows user A to impersonate user B when\n+connecting to a service, if that service allows it.\n+\n+Spark allows impersonation when submitting applications, so that the whole application runs as\n+user B in the above example.\n+\n+Spark does not allow token renewal when impersonation is on. Impersonation was added in Spark\n+as a means for services (like Hive or Oozie) to start Spark applications on behalf of users.\n+That means that those services would provide the Spark launcher code with privileged credentials,\n+to submit applications and, potentially, user code to run as a regular user.\n+\n+In that situation, the service credentials should never be made available to the Spark application,\n+since that would be tantamount to giving your service credentials to unprivileged users.\n+\n+The above also implies that running impersonated applications in client mode can be a security\n+concern, since arbitrary user code would have access to the same local content as the privileged\n+user. But unlike token renewal, Spark does not prevent that configuration from running.\n+\n+When impersonating, the Spark launcher will create DTs for the \"proxy\" user. In the example\n+used above, that means that when code authenticates to a service using the DTs, the authenticated\n+user will be \"B\", not \"A\".\n+\n+Note that \"proxy user\" is a very Hadoop-specific concept. It does not apply to OS users (which\n+is why the client-mode case is an issue) and to services that do not authenticate using Hadoop's\n+`UserGroupInformation` system. It is generally used in the context of YARN - since an application\n+submitted as a proxy user will run as that particular user in the YARN cluster, obeying any\n+Hadoop-to-local-OS-user mapping configured for the service. But the overall support should work\n+for connecting to other services even when YARN is not being used.\n+\n+\n+## Limitations of DT support in Spark\n+\n+There are certain limitations to bear in mind when talking about DTs in Spark.\n+\n+The first one is that not all DTs actually expose their renewal period. This is generally a\n+service configuration that is not generally exposed to clients. For this reason, certain DT",
    "line": 228
  }],
  "prId": 23348
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Maybe worth to mention the provider enable flag?",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-03T14:08:56Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demistify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application so they can authenticate\n+to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g.\n+Spark executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates\n+unnecessary extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used to replace Kerberos authentication in\n+distributed applications, although there is nothing (aside from maybe implementation details) that\n+ties them to Kerberos.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and\n+use them. While it would be possible to create a shared API or even a shared service to manage the\n+creation and use of DTs, that doesn't currently exist, and retrofitting such a system would be a\n+huge change in a bunch of different services.\n+\n+Spark works around this by having a (somewhat) pluggable DT creation API. Support for new\n+services can be added by implementing a \"DT provider\" that is then called by Spark when\n+generating delegation tokens for an application. Spark distributes tokens to executors using\n+the `UserGroupInformation` Hadoop API, and it's up to the DT provider and the respective\n+client library to agree on how to use those tokens.\n+\n+Once they are created, the semantics of how DTs operate are also service-specific. But, in general,\n+they try to follow the semantics of Kerberos tokens:\n+\n+* A \"lifetime\" which is for how long the DT is valid before it requires renewal.\n+* A \"renewable life\" which is for how long the DT can be renewed.\n+\n+Once the token reaches its \"renewable life\", a new one needs to be created by contacting the\n+appropriate service, restarting the above process.\n+\n+\n+## DT Renewal, Renewers, and YARN\n+\n+This is the most confusing part of DT handling, and part of it is because much of the system was\n+designed with MapReduce, and later YARN, in mind.\n+\n+As seen above, DTs need to be renewed periodically until they finally expire for good. An example\n+of this is the default configuration of HDFS services: delegation tokens are valid for up to 7\n+days, and need to be renewed every 24h. If 24h pass without the token being renewed, the token\n+cannot be used anymore. And the token cannot be renewed anymore after 7 days.\n+\n+This raises the question: who renews tokens? And for a long time the answer was YARN.\n+\n+When YARN applications are submitted, a set of DTs is also submitted with them. YARN takes care\n+of distributing these tokens to containers (using conventions set by the `UserGroupInformation`\n+API) and, also, keeping them renewed while the app is running. These tokens are used not just\n+by the application; they are also used by YARN itself to implement features like log collection\n+and aggregation.\n+\n+But this has a few caveats.\n+\n+\n+1. Who renews the tokens?\n+\n+This is handled mostly transparently by the Hadoop libraries in the case of YARN. Some services have\n+the concept of a token \"renewer\". This \"renewer\" is the name of the principal that is allowed to\n+renew the DT. When submitting to YARN, that will be the principal that the YARN service is running\n+as. Which means that the client application needs to know that information.\n+\n+For other resource managers, the renewer mostly does not matter, since there is no service that\n+is doing the renewal. Except that it sometimes leaks into library code, such as in SPARK-20328.\n+\n+\n+2. What tokens are renewed?\n+\n+This is probably the biggest caveat.\n+\n+As discussed in the previous section, DTs are service-specific, and require service-specific\n+libraries for creation *and* renewal. This means that for YARN to be able to renew application\n+tokens, YARN needs:\n+\n+* The client libraries for all the services the application is using\n+* Information about how to connect to the services the application is using\n+* Permissions to connect to those services\n+\n+In reality, though, most of the time YARN has access to a single HDFS cluster, and that will be\n+the extent of its DT renewal features. Any other tokens sent to YARN will be distributed to\n+containers, but will not be renewed.\n+\n+This means that those tokens will expire way before their max lifetime, unless some other code\n+takes care of renewing them.\n+\n+And, to make matters worse, not all client libraries even implement token renewal. To use the\n+example of a service supported by Spark, the `renew()` method of HBase tokens is a no-op. So\n+the only way to \"renew\" an HBase token is to create a new one.\n+\n+\n+3. What happens when tokens expire for good?\n+\n+The final caveat is that DTs have a maximum life, regardless of renewal. And after that deadline\n+is met, you need to create new tokens to be able to connect to the services. Which means you need\n+the ability to connect to the service without a delegation token, which means you need Kerberos\n+credentials.\n+\n+This is especially important for long-running applications that run unsupervised. They must be\n+able to keep on going without having someone logging into a terminal and typing a password every\n+few days.\n+\n+\n+## DT Renewal in Spark\n+\n+Because of the issues explained above, Spark implements a different way of doing renewal. Spark's\n+solution is a compromise: it targets the lowest common denominator, which is services like HBase\n+that do not support actual token renewal.\n+\n+In Spark, DT \"renewal\" is enabled by giving the application a Kerberos keytab. A keytab is\n+basically your Kerberos password written into a plain text file, which is why it's so sensitive.\n+By having the keytab, Spark can indefinitely maintain a valid Kerberos TGT.\n+\n+With Kerberos credentials available, Spark will create new DTs for the configured services as old\n+ones expire. So Spark doesn't renew tokens as explained in the previous section: it will create new\n+tokens at every renewal interval instead, and distribute those tokens to executors.\n+\n+This also has another advantage on top of supporting services like HBase: it removes the dependency\n+on an external renewal service (like YARN). That way, Spark's renewal feature can be used with\n+resource managers that are not DT-aware, such as Mesos or Kubernetes, as long as the application\n+has access to a keytab.\n+\n+\n+## DTs and Proxy Users\n+\n+\"Proxy users\" is Hadoop-speak for impersonation. It allows user A to impersonate user B when\n+connecting to a service, if that service allows it.\n+\n+Spark allows impersonation when submitting applications, so that the whole application runs as\n+user B in the above example.\n+\n+Spark does not allow token renewal when impersonation is on. Impersonation was added in Spark\n+as a means for services (like Hive or Oozie) to start Spark applications on behalf of users.\n+That means that those services would provide the Spark launcher code with privileged credentials,\n+to submit applications and, potentially, user code to run as a regular user.\n+\n+In that situation, the service credentials should never be made available to the Spark application,\n+since that would be tantamount to giving your service credentials to unprivileged users.\n+\n+The above also implies that running impersonated applications in client mode can be a security\n+concern, since arbitrary user code would have access to the same local content as the privileged\n+user. But unlike token renewal, Spark does not prevent that configuration from running.\n+\n+When impersonating, the Spark launcher will create DTs for the \"proxy\" user. In the example\n+used above, that means that when code authenticates to a service using the DTs, the authenticated\n+user will be \"B\", not \"A\".\n+\n+Note that \"proxy user\" is a very Hadoop-specific concept. It does not apply to OS users (which\n+is why the client-mode case is an issue) and to services that do not authenticate using Hadoop's\n+`UserGroupInformation` system. It is generally used in the context of YARN - since an application\n+submitted as a proxy user will run as that particular user in the YARN cluster, obeying any\n+Hadoop-to-local-OS-user mapping configured for the service. But the overall support should work\n+for connecting to other services even when YARN is not being used.\n+\n+\n+## Limitations of DT support in Spark\n+\n+There are certain limitations to bear in mind when talking about DTs in Spark.\n+\n+The first one is that not all DTs actually expose their renewal period. This is generally a\n+service configuration that is not generally exposed to clients. For this reason, certain DT\n+providers cannot provide a renewal period to the Spark code, thus requiring that the service's\n+configuration is in some way synchronized with another one that does provide that information.\n+\n+The HDFS service, which is generally available when DTs are needed in the first place, provides\n+that information, so in general it's a good idea for all services using DTs to use the same\n+configuration as HDFS for the renewal period.\n+\n+The second one is that Spark doesn't always know what delegation tokens will be needed. For\n+example, when submitting an application in cluster mode without a keytab, the launcher needs\n+to create DTs without knowing what the application code will actually be doing. This means that\n+Spark will try to get as many delegation tokens as is possible based on the configuration\n+available. That means that if an HBase configuration is available to the launcher but the app\n+doesn't actually use HBase, a DT will still be generated. The user would have to explicitly\n+opt out of generating HBase tokens in that case.",
    "line": 242
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "This is not user documentation, so not sure what benefit that would bring.",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-03T19:10:37Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demistify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application so they can authenticate\n+to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g.\n+Spark executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates\n+unnecessary extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used to replace Kerberos authentication in\n+distributed applications, although there is nothing (aside from maybe implementation details) that\n+ties them to Kerberos.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and\n+use them. While it would be possible to create a shared API or even a shared service to manage the\n+creation and use of DTs, that doesn't currently exist, and retrofitting such a system would be a\n+huge change in a bunch of different services.\n+\n+Spark works around this by having a (somewhat) pluggable DT creation API. Support for new\n+services can be added by implementing a \"DT provider\" that is then called by Spark when\n+generating delegation tokens for an application. Spark distributes tokens to executors using\n+the `UserGroupInformation` Hadoop API, and it's up to the DT provider and the respective\n+client library to agree on how to use those tokens.\n+\n+Once they are created, the semantics of how DTs operate are also service-specific. But, in general,\n+they try to follow the semantics of Kerberos tokens:\n+\n+* A \"lifetime\" which is for how long the DT is valid before it requires renewal.\n+* A \"renewable life\" which is for how long the DT can be renewed.\n+\n+Once the token reaches its \"renewable life\", a new one needs to be created by contacting the\n+appropriate service, restarting the above process.\n+\n+\n+## DT Renewal, Renewers, and YARN\n+\n+This is the most confusing part of DT handling, and part of it is because much of the system was\n+designed with MapReduce, and later YARN, in mind.\n+\n+As seen above, DTs need to be renewed periodically until they finally expire for good. An example\n+of this is the default configuration of HDFS services: delegation tokens are valid for up to 7\n+days, and need to be renewed every 24h. If 24h pass without the token being renewed, the token\n+cannot be used anymore. And the token cannot be renewed anymore after 7 days.\n+\n+This raises the question: who renews tokens? And for a long time the answer was YARN.\n+\n+When YARN applications are submitted, a set of DTs is also submitted with them. YARN takes care\n+of distributing these tokens to containers (using conventions set by the `UserGroupInformation`\n+API) and, also, keeping them renewed while the app is running. These tokens are used not just\n+by the application; they are also used by YARN itself to implement features like log collection\n+and aggregation.\n+\n+But this has a few caveats.\n+\n+\n+1. Who renews the tokens?\n+\n+This is handled mostly transparently by the Hadoop libraries in the case of YARN. Some services have\n+the concept of a token \"renewer\". This \"renewer\" is the name of the principal that is allowed to\n+renew the DT. When submitting to YARN, that will be the principal that the YARN service is running\n+as. Which means that the client application needs to know that information.\n+\n+For other resource managers, the renewer mostly does not matter, since there is no service that\n+is doing the renewal. Except that it sometimes leaks into library code, such as in SPARK-20328.\n+\n+\n+2. What tokens are renewed?\n+\n+This is probably the biggest caveat.\n+\n+As discussed in the previous section, DTs are service-specific, and require service-specific\n+libraries for creation *and* renewal. This means that for YARN to be able to renew application\n+tokens, YARN needs:\n+\n+* The client libraries for all the services the application is using\n+* Information about how to connect to the services the application is using\n+* Permissions to connect to those services\n+\n+In reality, though, most of the time YARN has access to a single HDFS cluster, and that will be\n+the extent of its DT renewal features. Any other tokens sent to YARN will be distributed to\n+containers, but will not be renewed.\n+\n+This means that those tokens will expire way before their max lifetime, unless some other code\n+takes care of renewing them.\n+\n+And, to make matters worse, not all client libraries even implement token renewal. To use the\n+example of a service supported by Spark, the `renew()` method of HBase tokens is a no-op. So\n+the only way to \"renew\" an HBase token is to create a new one.\n+\n+\n+3. What happens when tokens expire for good?\n+\n+The final caveat is that DTs have a maximum life, regardless of renewal. And after that deadline\n+is met, you need to create new tokens to be able to connect to the services. Which means you need\n+the ability to connect to the service without a delegation token, which means you need Kerberos\n+credentials.\n+\n+This is especially important for long-running applications that run unsupervised. They must be\n+able to keep on going without having someone logging into a terminal and typing a password every\n+few days.\n+\n+\n+## DT Renewal in Spark\n+\n+Because of the issues explained above, Spark implements a different way of doing renewal. Spark's\n+solution is a compromise: it targets the lowest common denominator, which is services like HBase\n+that do not support actual token renewal.\n+\n+In Spark, DT \"renewal\" is enabled by giving the application a Kerberos keytab. A keytab is\n+basically your Kerberos password written into a plain text file, which is why it's so sensitive.\n+By having the keytab, Spark can indefinitely maintain a valid Kerberos TGT.\n+\n+With Kerberos credentials available, Spark will create new DTs for the configured services as old\n+ones expire. So Spark doesn't renew tokens as explained in the previous section: it will create new\n+tokens at every renewal interval instead, and distribute those tokens to executors.\n+\n+This also has another advantage on top of supporting services like HBase: it removes the dependency\n+on an external renewal service (like YARN). That way, Spark's renewal feature can be used with\n+resource managers that are not DT-aware, such as Mesos or Kubernetes, as long as the application\n+has access to a keytab.\n+\n+\n+## DTs and Proxy Users\n+\n+\"Proxy users\" is Hadoop-speak for impersonation. It allows user A to impersonate user B when\n+connecting to a service, if that service allows it.\n+\n+Spark allows impersonation when submitting applications, so that the whole application runs as\n+user B in the above example.\n+\n+Spark does not allow token renewal when impersonation is on. Impersonation was added in Spark\n+as a means for services (like Hive or Oozie) to start Spark applications on behalf of users.\n+That means that those services would provide the Spark launcher code with privileged credentials,\n+to submit applications and, potentially, user code to run as a regular user.\n+\n+In that situation, the service credentials should never be made available to the Spark application,\n+since that would be tantamount to giving your service credentials to unprivileged users.\n+\n+The above also implies that running impersonated applications in client mode can be a security\n+concern, since arbitrary user code would have access to the same local content as the privileged\n+user. But unlike token renewal, Spark does not prevent that configuration from running.\n+\n+When impersonating, the Spark launcher will create DTs for the \"proxy\" user. In the example\n+used above, that means that when code authenticates to a service using the DTs, the authenticated\n+user will be \"B\", not \"A\".\n+\n+Note that \"proxy user\" is a very Hadoop-specific concept. It does not apply to OS users (which\n+is why the client-mode case is an issue) and to services that do not authenticate using Hadoop's\n+`UserGroupInformation` system. It is generally used in the context of YARN - since an application\n+submitted as a proxy user will run as that particular user in the YARN cluster, obeying any\n+Hadoop-to-local-OS-user mapping configured for the service. But the overall support should work\n+for connecting to other services even when YARN is not being used.\n+\n+\n+## Limitations of DT support in Spark\n+\n+There are certain limitations to bear in mind when talking about DTs in Spark.\n+\n+The first one is that not all DTs actually expose their renewal period. This is generally a\n+service configuration that is not generally exposed to clients. For this reason, certain DT\n+providers cannot provide a renewal period to the Spark code, thus requiring that the service's\n+configuration is in some way synchronized with another one that does provide that information.\n+\n+The HDFS service, which is generally available when DTs are needed in the first place, provides\n+that information, so in general it's a good idea for all services using DTs to use the same\n+configuration as HDFS for the renewal period.\n+\n+The second one is that Spark doesn't always know what delegation tokens will be needed. For\n+example, when submitting an application in cluster mode without a keytab, the launcher needs\n+to create DTs without knowing what the application code will actually be doing. This means that\n+Spark will try to get as many delegation tokens as is possible based on the configuration\n+available. That means that if an HBase configuration is available to the launcher but the app\n+doesn't actually use HBase, a DT will still be generated. The user would have to explicitly\n+opt out of generating HBase tokens in that case.",
    "line": 242
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Not to introduce a new flag for example. At least I've received such comments because it was not obvious to devs there are flags for this.",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-03T19:24:39Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demistify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application so they can authenticate\n+to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g.\n+Spark executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates\n+unnecessary extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used to replace Kerberos authentication in\n+distributed applications, although there is nothing (aside from maybe implementation details) that\n+ties them to Kerberos.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and\n+use them. While it would be possible to create a shared API or even a shared service to manage the\n+creation and use of DTs, that doesn't currently exist, and retrofitting such a system would be a\n+huge change in a bunch of different services.\n+\n+Spark works around this by having a (somewhat) pluggable DT creation API. Support for new\n+services can be added by implementing a \"DT provider\" that is then called by Spark when\n+generating delegation tokens for an application. Spark distributes tokens to executors using\n+the `UserGroupInformation` Hadoop API, and it's up to the DT provider and the respective\n+client library to agree on how to use those tokens.\n+\n+Once they are created, the semantics of how DTs operate are also service-specific. But, in general,\n+they try to follow the semantics of Kerberos tokens:\n+\n+* A \"lifetime\" which is for how long the DT is valid before it requires renewal.\n+* A \"renewable life\" which is for how long the DT can be renewed.\n+\n+Once the token reaches its \"renewable life\", a new one needs to be created by contacting the\n+appropriate service, restarting the above process.\n+\n+\n+## DT Renewal, Renewers, and YARN\n+\n+This is the most confusing part of DT handling, and part of it is because much of the system was\n+designed with MapReduce, and later YARN, in mind.\n+\n+As seen above, DTs need to be renewed periodically until they finally expire for good. An example\n+of this is the default configuration of HDFS services: delegation tokens are valid for up to 7\n+days, and need to be renewed every 24h. If 24h pass without the token being renewed, the token\n+cannot be used anymore. And the token cannot be renewed anymore after 7 days.\n+\n+This raises the question: who renews tokens? And for a long time the answer was YARN.\n+\n+When YARN applications are submitted, a set of DTs is also submitted with them. YARN takes care\n+of distributing these tokens to containers (using conventions set by the `UserGroupInformation`\n+API) and, also, keeping them renewed while the app is running. These tokens are used not just\n+by the application; they are also used by YARN itself to implement features like log collection\n+and aggregation.\n+\n+But this has a few caveats.\n+\n+\n+1. Who renews the tokens?\n+\n+This is handled mostly transparently by the Hadoop libraries in the case of YARN. Some services have\n+the concept of a token \"renewer\". This \"renewer\" is the name of the principal that is allowed to\n+renew the DT. When submitting to YARN, that will be the principal that the YARN service is running\n+as. Which means that the client application needs to know that information.\n+\n+For other resource managers, the renewer mostly does not matter, since there is no service that\n+is doing the renewal. Except that it sometimes leaks into library code, such as in SPARK-20328.\n+\n+\n+2. What tokens are renewed?\n+\n+This is probably the biggest caveat.\n+\n+As discussed in the previous section, DTs are service-specific, and require service-specific\n+libraries for creation *and* renewal. This means that for YARN to be able to renew application\n+tokens, YARN needs:\n+\n+* The client libraries for all the services the application is using\n+* Information about how to connect to the services the application is using\n+* Permissions to connect to those services\n+\n+In reality, though, most of the time YARN has access to a single HDFS cluster, and that will be\n+the extent of its DT renewal features. Any other tokens sent to YARN will be distributed to\n+containers, but will not be renewed.\n+\n+This means that those tokens will expire way before their max lifetime, unless some other code\n+takes care of renewing them.\n+\n+And, to make matters worse, not all client libraries even implement token renewal. To use the\n+example of a service supported by Spark, the `renew()` method of HBase tokens is a no-op. So\n+the only way to \"renew\" an HBase token is to create a new one.\n+\n+\n+3. What happens when tokens expire for good?\n+\n+The final caveat is that DTs have a maximum life, regardless of renewal. And after that deadline\n+is met, you need to create new tokens to be able to connect to the services. Which means you need\n+the ability to connect to the service without a delegation token, which means you need Kerberos\n+credentials.\n+\n+This is especially important for long-running applications that run unsupervised. They must be\n+able to keep on going without having someone logging into a terminal and typing a password every\n+few days.\n+\n+\n+## DT Renewal in Spark\n+\n+Because of the issues explained above, Spark implements a different way of doing renewal. Spark's\n+solution is a compromise: it targets the lowest common denominator, which is services like HBase\n+that do not support actual token renewal.\n+\n+In Spark, DT \"renewal\" is enabled by giving the application a Kerberos keytab. A keytab is\n+basically your Kerberos password written into a plain text file, which is why it's so sensitive.\n+By having the keytab, Spark can indefinitely maintain a valid Kerberos TGT.\n+\n+With Kerberos credentials available, Spark will create new DTs for the configured services as old\n+ones expire. So Spark doesn't renew tokens as explained in the previous section: it will create new\n+tokens at every renewal interval instead, and distribute those tokens to executors.\n+\n+This also has another advantage on top of supporting services like HBase: it removes the dependency\n+on an external renewal service (like YARN). That way, Spark's renewal feature can be used with\n+resource managers that are not DT-aware, such as Mesos or Kubernetes, as long as the application\n+has access to a keytab.\n+\n+\n+## DTs and Proxy Users\n+\n+\"Proxy users\" is Hadoop-speak for impersonation. It allows user A to impersonate user B when\n+connecting to a service, if that service allows it.\n+\n+Spark allows impersonation when submitting applications, so that the whole application runs as\n+user B in the above example.\n+\n+Spark does not allow token renewal when impersonation is on. Impersonation was added in Spark\n+as a means for services (like Hive or Oozie) to start Spark applications on behalf of users.\n+That means that those services would provide the Spark launcher code with privileged credentials,\n+to submit applications and, potentially, user code to run as a regular user.\n+\n+In that situation, the service credentials should never be made available to the Spark application,\n+since that would be tantamount to giving your service credentials to unprivileged users.\n+\n+The above also implies that running impersonated applications in client mode can be a security\n+concern, since arbitrary user code would have access to the same local content as the privileged\n+user. But unlike token renewal, Spark does not prevent that configuration from running.\n+\n+When impersonating, the Spark launcher will create DTs for the \"proxy\" user. In the example\n+used above, that means that when code authenticates to a service using the DTs, the authenticated\n+user will be \"B\", not \"A\".\n+\n+Note that \"proxy user\" is a very Hadoop-specific concept. It does not apply to OS users (which\n+is why the client-mode case is an issue) and to services that do not authenticate using Hadoop's\n+`UserGroupInformation` system. It is generally used in the context of YARN - since an application\n+submitted as a proxy user will run as that particular user in the YARN cluster, obeying any\n+Hadoop-to-local-OS-user mapping configured for the service. But the overall support should work\n+for connecting to other services even when YARN is not being used.\n+\n+\n+## Limitations of DT support in Spark\n+\n+There are certain limitations to bear in mind when talking about DTs in Spark.\n+\n+The first one is that not all DTs actually expose their renewal period. This is generally a\n+service configuration that is not generally exposed to clients. For this reason, certain DT\n+providers cannot provide a renewal period to the Spark code, thus requiring that the service's\n+configuration is in some way synchronized with another one that does provide that information.\n+\n+The HDFS service, which is generally available when DTs are needed in the first place, provides\n+that information, so in general it's a good idea for all services using DTs to use the same\n+configuration as HDFS for the renewal period.\n+\n+The second one is that Spark doesn't always know what delegation tokens will be needed. For\n+example, when submitting an application in cluster mode without a keytab, the launcher needs\n+to create DTs without knowing what the application code will actually be doing. This means that\n+Spark will try to get as many delegation tokens as is possible based on the configuration\n+available. That means that if an HBase configuration is available to the launcher but the app\n+doesn't actually use HBase, a DT will still be generated. The user would have to explicitly\n+opt out of generating HBase tokens in that case.",
    "line": 242
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "This is the wrong document for that. This is explaining delegation tokens in general, not the internal Spark API for dealing with them.",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-03T19:26:33Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demistify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application so they can authenticate\n+to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g.\n+Spark executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates\n+unnecessary extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used to replace Kerberos authentication in\n+distributed applications, although there is nothing (aside from maybe implementation details) that\n+ties them to Kerberos.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and\n+use them. While it would be possible to create a shared API or even a shared service to manage the\n+creation and use of DTs, that doesn't currently exist, and retrofitting such a system would be a\n+huge change in a bunch of different services.\n+\n+Spark works around this by having a (somewhat) pluggable DT creation API. Support for new\n+services can be added by implementing a \"DT provider\" that is then called by Spark when\n+generating delegation tokens for an application. Spark distributes tokens to executors using\n+the `UserGroupInformation` Hadoop API, and it's up to the DT provider and the respective\n+client library to agree on how to use those tokens.\n+\n+Once they are created, the semantics of how DTs operate are also service-specific. But, in general,\n+they try to follow the semantics of Kerberos tokens:\n+\n+* A \"lifetime\" which is for how long the DT is valid before it requires renewal.\n+* A \"renewable life\" which is for how long the DT can be renewed.\n+\n+Once the token reaches its \"renewable life\", a new one needs to be created by contacting the\n+appropriate service, restarting the above process.\n+\n+\n+## DT Renewal, Renewers, and YARN\n+\n+This is the most confusing part of DT handling, and part of it is because much of the system was\n+designed with MapReduce, and later YARN, in mind.\n+\n+As seen above, DTs need to be renewed periodically until they finally expire for good. An example\n+of this is the default configuration of HDFS services: delegation tokens are valid for up to 7\n+days, and need to be renewed every 24h. If 24h pass without the token being renewed, the token\n+cannot be used anymore. And the token cannot be renewed anymore after 7 days.\n+\n+This raises the question: who renews tokens? And for a long time the answer was YARN.\n+\n+When YARN applications are submitted, a set of DTs is also submitted with them. YARN takes care\n+of distributing these tokens to containers (using conventions set by the `UserGroupInformation`\n+API) and, also, keeping them renewed while the app is running. These tokens are used not just\n+by the application; they are also used by YARN itself to implement features like log collection\n+and aggregation.\n+\n+But this has a few caveats.\n+\n+\n+1. Who renews the tokens?\n+\n+This is handled mostly transparently by the Hadoop libraries in the case of YARN. Some services have\n+the concept of a token \"renewer\". This \"renewer\" is the name of the principal that is allowed to\n+renew the DT. When submitting to YARN, that will be the principal that the YARN service is running\n+as. Which means that the client application needs to know that information.\n+\n+For other resource managers, the renewer mostly does not matter, since there is no service that\n+is doing the renewal. Except that it sometimes leaks into library code, such as in SPARK-20328.\n+\n+\n+2. What tokens are renewed?\n+\n+This is probably the biggest caveat.\n+\n+As discussed in the previous section, DTs are service-specific, and require service-specific\n+libraries for creation *and* renewal. This means that for YARN to be able to renew application\n+tokens, YARN needs:\n+\n+* The client libraries for all the services the application is using\n+* Information about how to connect to the services the application is using\n+* Permissions to connect to those services\n+\n+In reality, though, most of the time YARN has access to a single HDFS cluster, and that will be\n+the extent of its DT renewal features. Any other tokens sent to YARN will be distributed to\n+containers, but will not be renewed.\n+\n+This means that those tokens will expire way before their max lifetime, unless some other code\n+takes care of renewing them.\n+\n+And, to make matters worse, not all client libraries even implement token renewal. To use the\n+example of a service supported by Spark, the `renew()` method of HBase tokens is a no-op. So\n+the only way to \"renew\" an HBase token is to create a new one.\n+\n+\n+3. What happens when tokens expire for good?\n+\n+The final caveat is that DTs have a maximum life, regardless of renewal. And after that deadline\n+is met, you need to create new tokens to be able to connect to the services. Which means you need\n+the ability to connect to the service without a delegation token, which means you need Kerberos\n+credentials.\n+\n+This is especially important for long-running applications that run unsupervised. They must be\n+able to keep on going without having someone logging into a terminal and typing a password every\n+few days.\n+\n+\n+## DT Renewal in Spark\n+\n+Because of the issues explained above, Spark implements a different way of doing renewal. Spark's\n+solution is a compromise: it targets the lowest common denominator, which is services like HBase\n+that do not support actual token renewal.\n+\n+In Spark, DT \"renewal\" is enabled by giving the application a Kerberos keytab. A keytab is\n+basically your Kerberos password written into a plain text file, which is why it's so sensitive.\n+By having the keytab, Spark can indefinitely maintain a valid Kerberos TGT.\n+\n+With Kerberos credentials available, Spark will create new DTs for the configured services as old\n+ones expire. So Spark doesn't renew tokens as explained in the previous section: it will create new\n+tokens at every renewal interval instead, and distribute those tokens to executors.\n+\n+This also has another advantage on top of supporting services like HBase: it removes the dependency\n+on an external renewal service (like YARN). That way, Spark's renewal feature can be used with\n+resource managers that are not DT-aware, such as Mesos or Kubernetes, as long as the application\n+has access to a keytab.\n+\n+\n+## DTs and Proxy Users\n+\n+\"Proxy users\" is Hadoop-speak for impersonation. It allows user A to impersonate user B when\n+connecting to a service, if that service allows it.\n+\n+Spark allows impersonation when submitting applications, so that the whole application runs as\n+user B in the above example.\n+\n+Spark does not allow token renewal when impersonation is on. Impersonation was added in Spark\n+as a means for services (like Hive or Oozie) to start Spark applications on behalf of users.\n+That means that those services would provide the Spark launcher code with privileged credentials,\n+to submit applications and, potentially, user code to run as a regular user.\n+\n+In that situation, the service credentials should never be made available to the Spark application,\n+since that would be tantamount to giving your service credentials to unprivileged users.\n+\n+The above also implies that running impersonated applications in client mode can be a security\n+concern, since arbitrary user code would have access to the same local content as the privileged\n+user. But unlike token renewal, Spark does not prevent that configuration from running.\n+\n+When impersonating, the Spark launcher will create DTs for the \"proxy\" user. In the example\n+used above, that means that when code authenticates to a service using the DTs, the authenticated\n+user will be \"B\", not \"A\".\n+\n+Note that \"proxy user\" is a very Hadoop-specific concept. It does not apply to OS users (which\n+is why the client-mode case is an issue) and to services that do not authenticate using Hadoop's\n+`UserGroupInformation` system. It is generally used in the context of YARN - since an application\n+submitted as a proxy user will run as that particular user in the YARN cluster, obeying any\n+Hadoop-to-local-OS-user mapping configured for the service. But the overall support should work\n+for connecting to other services even when YARN is not being used.\n+\n+\n+## Limitations of DT support in Spark\n+\n+There are certain limitations to bear in mind when talking about DTs in Spark.\n+\n+The first one is that not all DTs actually expose their renewal period. This is generally a\n+service configuration that is not generally exposed to clients. For this reason, certain DT\n+providers cannot provide a renewal period to the Spark code, thus requiring that the service's\n+configuration is in some way synchronized with another one that does provide that information.\n+\n+The HDFS service, which is generally available when DTs are needed in the first place, provides\n+that information, so in general it's a good idea for all services using DTs to use the same\n+configuration as HDFS for the renewal period.\n+\n+The second one is that Spark doesn't always know what delegation tokens will be needed. For\n+example, when submitting an application in cluster mode without a keytab, the launcher needs\n+to create DTs without knowing what the application code will actually be doing. This means that\n+Spark will try to get as many delegation tokens as is possible based on the configuration\n+available. That means that if an HBase configuration is available to the launcher but the app\n+doesn't actually use HBase, a DT will still be generated. The user would have to explicitly\n+opt out of generating HBase tokens in that case.",
    "line": 242
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "OK.",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-03T19:29:54Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demistify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application so they can authenticate\n+to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g.\n+Spark executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates\n+unnecessary extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used to replace Kerberos authentication in\n+distributed applications, although there is nothing (aside from maybe implementation details) that\n+ties them to Kerberos.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and\n+use them. While it would be possible to create a shared API or even a shared service to manage the\n+creation and use of DTs, that doesn't currently exist, and retrofitting such a system would be a\n+huge change in a bunch of different services.\n+\n+Spark works around this by having a (somewhat) pluggable DT creation API. Support for new\n+services can be added by implementing a \"DT provider\" that is then called by Spark when\n+generating delegation tokens for an application. Spark distributes tokens to executors using\n+the `UserGroupInformation` Hadoop API, and it's up to the DT provider and the respective\n+client library to agree on how to use those tokens.\n+\n+Once they are created, the semantics of how DTs operate are also service-specific. But, in general,\n+they try to follow the semantics of Kerberos tokens:\n+\n+* A \"lifetime\" which is for how long the DT is valid before it requires renewal.\n+* A \"renewable life\" which is for how long the DT can be renewed.\n+\n+Once the token reaches its \"renewable life\", a new one needs to be created by contacting the\n+appropriate service, restarting the above process.\n+\n+\n+## DT Renewal, Renewers, and YARN\n+\n+This is the most confusing part of DT handling, and part of it is because much of the system was\n+designed with MapReduce, and later YARN, in mind.\n+\n+As seen above, DTs need to be renewed periodically until they finally expire for good. An example\n+of this is the default configuration of HDFS services: delegation tokens are valid for up to 7\n+days, and need to be renewed every 24h. If 24h pass without the token being renewed, the token\n+cannot be used anymore. And the token cannot be renewed anymore after 7 days.\n+\n+This raises the question: who renews tokens? And for a long time the answer was YARN.\n+\n+When YARN applications are submitted, a set of DTs is also submitted with them. YARN takes care\n+of distributing these tokens to containers (using conventions set by the `UserGroupInformation`\n+API) and, also, keeping them renewed while the app is running. These tokens are used not just\n+by the application; they are also used by YARN itself to implement features like log collection\n+and aggregation.\n+\n+But this has a few caveats.\n+\n+\n+1. Who renews the tokens?\n+\n+This is handled mostly transparently by the Hadoop libraries in the case of YARN. Some services have\n+the concept of a token \"renewer\". This \"renewer\" is the name of the principal that is allowed to\n+renew the DT. When submitting to YARN, that will be the principal that the YARN service is running\n+as. Which means that the client application needs to know that information.\n+\n+For other resource managers, the renewer mostly does not matter, since there is no service that\n+is doing the renewal. Except that it sometimes leaks into library code, such as in SPARK-20328.\n+\n+\n+2. What tokens are renewed?\n+\n+This is probably the biggest caveat.\n+\n+As discussed in the previous section, DTs are service-specific, and require service-specific\n+libraries for creation *and* renewal. This means that for YARN to be able to renew application\n+tokens, YARN needs:\n+\n+* The client libraries for all the services the application is using\n+* Information about how to connect to the services the application is using\n+* Permissions to connect to those services\n+\n+In reality, though, most of the time YARN has access to a single HDFS cluster, and that will be\n+the extent of its DT renewal features. Any other tokens sent to YARN will be distributed to\n+containers, but will not be renewed.\n+\n+This means that those tokens will expire way before their max lifetime, unless some other code\n+takes care of renewing them.\n+\n+And, to make matters worse, not all client libraries even implement token renewal. To use the\n+example of a service supported by Spark, the `renew()` method of HBase tokens is a no-op. So\n+the only way to \"renew\" an HBase token is to create a new one.\n+\n+\n+3. What happens when tokens expire for good?\n+\n+The final caveat is that DTs have a maximum life, regardless of renewal. And after that deadline\n+is met, you need to create new tokens to be able to connect to the services. Which means you need\n+the ability to connect to the service without a delegation token, which means you need Kerberos\n+credentials.\n+\n+This is especially important for long-running applications that run unsupervised. They must be\n+able to keep on going without having someone logging into a terminal and typing a password every\n+few days.\n+\n+\n+## DT Renewal in Spark\n+\n+Because of the issues explained above, Spark implements a different way of doing renewal. Spark's\n+solution is a compromise: it targets the lowest common denominator, which is services like HBase\n+that do not support actual token renewal.\n+\n+In Spark, DT \"renewal\" is enabled by giving the application a Kerberos keytab. A keytab is\n+basically your Kerberos password written into a plain text file, which is why it's so sensitive.\n+By having the keytab, Spark can indefinitely maintain a valid Kerberos TGT.\n+\n+With Kerberos credentials available, Spark will create new DTs for the configured services as old\n+ones expire. So Spark doesn't renew tokens as explained in the previous section: it will create new\n+tokens at every renewal interval instead, and distribute those tokens to executors.\n+\n+This also has another advantage on top of supporting services like HBase: it removes the dependency\n+on an external renewal service (like YARN). That way, Spark's renewal feature can be used with\n+resource managers that are not DT-aware, such as Mesos or Kubernetes, as long as the application\n+has access to a keytab.\n+\n+\n+## DTs and Proxy Users\n+\n+\"Proxy users\" is Hadoop-speak for impersonation. It allows user A to impersonate user B when\n+connecting to a service, if that service allows it.\n+\n+Spark allows impersonation when submitting applications, so that the whole application runs as\n+user B in the above example.\n+\n+Spark does not allow token renewal when impersonation is on. Impersonation was added in Spark\n+as a means for services (like Hive or Oozie) to start Spark applications on behalf of users.\n+That means that those services would provide the Spark launcher code with privileged credentials,\n+to submit applications and, potentially, user code to run as a regular user.\n+\n+In that situation, the service credentials should never be made available to the Spark application,\n+since that would be tantamount to giving your service credentials to unprivileged users.\n+\n+The above also implies that running impersonated applications in client mode can be a security\n+concern, since arbitrary user code would have access to the same local content as the privileged\n+user. But unlike token renewal, Spark does not prevent that configuration from running.\n+\n+When impersonating, the Spark launcher will create DTs for the \"proxy\" user. In the example\n+used above, that means that when code authenticates to a service using the DTs, the authenticated\n+user will be \"B\", not \"A\".\n+\n+Note that \"proxy user\" is a very Hadoop-specific concept. It does not apply to OS users (which\n+is why the client-mode case is an issue) and to services that do not authenticate using Hadoop's\n+`UserGroupInformation` system. It is generally used in the context of YARN - since an application\n+submitted as a proxy user will run as that particular user in the YARN cluster, obeying any\n+Hadoop-to-local-OS-user mapping configured for the service. But the overall support should work\n+for connecting to other services even when YARN is not being used.\n+\n+\n+## Limitations of DT support in Spark\n+\n+There are certain limitations to bear in mind when talking about DTs in Spark.\n+\n+The first one is that not all DTs actually expose their renewal period. This is generally a\n+service configuration that is not generally exposed to clients. For this reason, certain DT\n+providers cannot provide a renewal period to the Spark code, thus requiring that the service's\n+configuration is in some way synchronized with another one that does provide that information.\n+\n+The HDFS service, which is generally available when DTs are needed in the first place, provides\n+that information, so in general it's a good idea for all services using DTs to use the same\n+configuration as HDFS for the renewal period.\n+\n+The second one is that Spark doesn't always know what delegation tokens will be needed. For\n+example, when submitting an application in cluster mode without a keytab, the launcher needs\n+to create DTs without knowing what the application code will actually be doing. This means that\n+Spark will try to get as many delegation tokens as is possible based on the configuration\n+available. That means that if an HBase configuration is available to the launcher but the app\n+doesn't actually use HBase, a DT will still be generated. The user would have to explicitly\n+opt out of generating HBase tokens in that case.",
    "line": 242
  }],
  "prId": 23348
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "`Without access to Kerberos credentials,\r\nSpark cannot create DTs`\r\nOne exception is Kafka, which can get token with SSL certificate authentication.",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-03T14:12:12Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demistify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application so they can authenticate\n+to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g.\n+Spark executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates\n+unnecessary extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used to replace Kerberos authentication in\n+distributed applications, although there is nothing (aside from maybe implementation details) that\n+ties them to Kerberos.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and\n+use them. While it would be possible to create a shared API or even a shared service to manage the\n+creation and use of DTs, that doesn't currently exist, and retrofitting such a system would be a\n+huge change in a bunch of different services.\n+\n+Spark works around this by having a (somewhat) pluggable DT creation API. Support for new\n+services can be added by implementing a \"DT provider\" that is then called by Spark when\n+generating delegation tokens for an application. Spark distributes tokens to executors using\n+the `UserGroupInformation` Hadoop API, and it's up to the DT provider and the respective\n+client library to agree on how to use those tokens.\n+\n+Once they are created, the semantics of how DTs operate are also service-specific. But, in general,\n+they try to follow the semantics of Kerberos tokens:\n+\n+* A \"lifetime\" which is for how long the DT is valid before it requires renewal.\n+* A \"renewable life\" which is for how long the DT can be renewed.\n+\n+Once the token reaches its \"renewable life\", a new one needs to be created by contacting the\n+appropriate service, restarting the above process.\n+\n+\n+## DT Renewal, Renewers, and YARN\n+\n+This is the most confusing part of DT handling, and part of it is because much of the system was\n+designed with MapReduce, and later YARN, in mind.\n+\n+As seen above, DTs need to be renewed periodically until they finally expire for good. An example\n+of this is the default configuration of HDFS services: delegation tokens are valid for up to 7\n+days, and need to be renewed every 24h. If 24h pass without the token being renewed, the token\n+cannot be used anymore. And the token cannot be renewed anymore after 7 days.\n+\n+This raises the question: who renews tokens? And for a long time the answer was YARN.\n+\n+When YARN applications are submitted, a set of DTs is also submitted with them. YARN takes care\n+of distributing these tokens to containers (using conventions set by the `UserGroupInformation`\n+API) and, also, keeping them renewed while the app is running. These tokens are used not just\n+by the application; they are also used by YARN itself to implement features like log collection\n+and aggregation.\n+\n+But this has a few caveats.\n+\n+\n+1. Who renews the tokens?\n+\n+This is handled mostly transparently by the Hadoop libraries in the case of YARN. Some services have\n+the concept of a token \"renewer\". This \"renewer\" is the name of the principal that is allowed to\n+renew the DT. When submitting to YARN, that will be the principal that the YARN service is running\n+as. Which means that the client application needs to know that information.\n+\n+For other resource managers, the renewer mostly does not matter, since there is no service that\n+is doing the renewal. Except that it sometimes leaks into library code, such as in SPARK-20328.\n+\n+\n+2. What tokens are renewed?\n+\n+This is probably the biggest caveat.\n+\n+As discussed in the previous section, DTs are service-specific, and require service-specific\n+libraries for creation *and* renewal. This means that for YARN to be able to renew application\n+tokens, YARN needs:\n+\n+* The client libraries for all the services the application is using\n+* Information about how to connect to the services the application is using\n+* Permissions to connect to those services\n+\n+In reality, though, most of the time YARN has access to a single HDFS cluster, and that will be\n+the extent of its DT renewal features. Any other tokens sent to YARN will be distributed to\n+containers, but will not be renewed.\n+\n+This means that those tokens will expire way before their max lifetime, unless some other code\n+takes care of renewing them.\n+\n+And, to make matters worse, not all client libraries even implement token renewal. To use the\n+example of a service supported by Spark, the `renew()` method of HBase tokens is a no-op. So\n+the only way to \"renew\" an HBase token is to create a new one.\n+\n+\n+3. What happens when tokens expire for good?\n+\n+The final caveat is that DTs have a maximum life, regardless of renewal. And after that deadline\n+is met, you need to create new tokens to be able to connect to the services. Which means you need\n+the ability to connect to the service without a delegation token, which means you need Kerberos\n+credentials.\n+\n+This is especially important for long-running applications that run unsupervised. They must be\n+able to keep on going without having someone logging into a terminal and typing a password every\n+few days.\n+\n+\n+## DT Renewal in Spark\n+\n+Because of the issues explained above, Spark implements a different way of doing renewal. Spark's\n+solution is a compromise: it targets the lowest common denominator, which is services like HBase\n+that do not support actual token renewal.\n+\n+In Spark, DT \"renewal\" is enabled by giving the application a Kerberos keytab. A keytab is\n+basically your Kerberos password written into a plain text file, which is why it's so sensitive.\n+By having the keytab, Spark can indefinitely maintain a valid Kerberos TGT.\n+\n+With Kerberos credentials available, Spark will create new DTs for the configured services as old\n+ones expire. So Spark doesn't renew tokens as explained in the previous section: it will create new\n+tokens at every renewal interval instead, and distribute those tokens to executors.\n+\n+This also has another advantage on top of supporting services like HBase: it removes the dependency\n+on an external renewal service (like YARN). That way, Spark's renewal feature can be used with\n+resource managers that are not DT-aware, such as Mesos or Kubernetes, as long as the application\n+has access to a keytab.\n+\n+\n+## DTs and Proxy Users\n+\n+\"Proxy users\" is Hadoop-speak for impersonation. It allows user A to impersonate user B when\n+connecting to a service, if that service allows it.\n+\n+Spark allows impersonation when submitting applications, so that the whole application runs as\n+user B in the above example.\n+\n+Spark does not allow token renewal when impersonation is on. Impersonation was added in Spark\n+as a means for services (like Hive or Oozie) to start Spark applications on behalf of users.\n+That means that those services would provide the Spark launcher code with privileged credentials,\n+to submit applications and, potentially, user code to run as a regular user.\n+\n+In that situation, the service credentials should never be made available to the Spark application,\n+since that would be tantamount to giving your service credentials to unprivileged users.\n+\n+The above also implies that running impersonated applications in client mode can be a security\n+concern, since arbitrary user code would have access to the same local content as the privileged\n+user. But unlike token renewal, Spark does not prevent that configuration from running.\n+\n+When impersonating, the Spark launcher will create DTs for the \"proxy\" user. In the example\n+used above, that means that when code authenticates to a service using the DTs, the authenticated\n+user will be \"B\", not \"A\".\n+\n+Note that \"proxy user\" is a very Hadoop-specific concept. It does not apply to OS users (which\n+is why the client-mode case is an issue) and to services that do not authenticate using Hadoop's\n+`UserGroupInformation` system. It is generally used in the context of YARN - since an application\n+submitted as a proxy user will run as that particular user in the YARN cluster, obeying any\n+Hadoop-to-local-OS-user mapping configured for the service. But the overall support should work\n+for connecting to other services even when YARN is not being used.\n+\n+\n+## Limitations of DT support in Spark\n+\n+There are certain limitations to bear in mind when talking about DTs in Spark.\n+\n+The first one is that not all DTs actually expose their renewal period. This is generally a\n+service configuration that is not generally exposed to clients. For this reason, certain DT\n+providers cannot provide a renewal period to the Spark code, thus requiring that the service's\n+configuration is in some way synchronized with another one that does provide that information.\n+\n+The HDFS service, which is generally available when DTs are needed in the first place, provides\n+that information, so in general it's a good idea for all services using DTs to use the same\n+configuration as HDFS for the renewal period.\n+\n+The second one is that Spark doesn't always know what delegation tokens will be needed. For\n+example, when submitting an application in cluster mode without a keytab, the launcher needs\n+to create DTs without knowing what the application code will actually be doing. This means that\n+Spark will try to get as many delegation tokens as is possible based on the configuration\n+available. That means that if an HBase configuration is available to the launcher but the app\n+doesn't actually use HBase, a DT will still be generated. The user would have to explicitly\n+opt out of generating HBase tokens in that case.\n+\n+The third one is that it's hard to create DTs \"as needed\". Without access to Kerberos credentials,"
  }],
  "prId": 23348
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "not a big deal but I think we better stick to one way. \" Key Distribution Center (KDC)\" vs \"TGT (ticket granting ticket)\".",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-04T02:04:22Z",
    "diffHunk": "@@ -0,0 +1,239 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demystify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application (e.g. Spark executors),\n+so they can authenticate to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the Key Distribution Center (KDC) and generation of a service ticket. In a distributed system,"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "I'll just revert to KDC. Readers of this document are expected to at least be a little familiar with Kerberos, so those terms should be familiar.",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-04T19:17:47Z",
    "diffHunk": "@@ -0,0 +1,239 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demystify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application (e.g. Spark executors),\n+so they can authenticate to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the Key Distribution Center (KDC) and generation of a service ticket. In a distributed system,"
  }],
  "prId": 23348
}, {
  "comments": [{
    "author": {
      "login": "tgravescs"
    },
    "body": "this isn't true, delegation tokens don't allow for single place.  you could just as well have a keytab in the single place that distributes the tgts. And really its not a single place since it distributes them to executors.  We might rephrase this as an implementation detail that they go to the driver and are distributed",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-07T13:52:49Z",
    "diffHunk": "@@ -0,0 +1,238 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demystify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity",
    "line": 19
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": ">  delegation tokens don't allow for single place\r\n\r\nHmm, yes they do. Yes, you could do those other things, but then you wouldn't need DTs. So DTs are one way to allow a single place to handle kerberos credentials.\r\n\r\n>  delegation tokens don't allow for single place\r\n\r\nNot sure I follow. Only a single place has *kerberos* credentials. DTs are distributed to executors, but they are not *kerberos* credentials.\r\n",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-07T17:53:52Z",
    "diffHunk": "@@ -0,0 +1,238 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demystify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity",
    "line": 19
  }, {
    "author": {
      "login": "tgravescs"
    },
    "body": "Oh I think I see what you are trying to say here.  I think you need to elaborate more on this.  I think you are trying to say that if you are sending kerberos credentials like a keytab that allows us to do all DT handling in a single place - the driver.",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-07T18:52:58Z",
    "diffHunk": "@@ -0,0 +1,238 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demystify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity",
    "line": 19
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "No, what I'm trying to say is that when you use DTs, there's a single place that needs to handle kerberos credentials.\r\n\r\nIt doesn't matter if you're using keytabs or kinit. It's still a single place. And that single place is responsible for distributing the DTs to the other entities that needs to authenticate against services.\r\n\r\nIt also doesn't need to be the Spark driver. As I mention in other places in the document, the DTs can be created by an external entity (like Oozie does).",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-07T19:34:02Z",
    "diffHunk": "@@ -0,0 +1,238 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demystify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity",
    "line": 19
  }, {
    "author": {
      "login": "tgravescs"
    },
    "body": "ok, sorry I understand now, I somehow really misread this. ",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-07T19:41:21Z",
    "diffHunk": "@@ -0,0 +1,238 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demystify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity",
    "line": 19
  }],
  "prId": 23348
}, {
  "comments": [{
    "author": {
      "login": "tgravescs"
    },
    "body": "remove extra newline",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-07T13:59:26Z",
    "diffHunk": "@@ -0,0 +1,238 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demystify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application (e.g. Spark executors),\n+so they can authenticate to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g. Spark\n+executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates unnecessary\n+extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+",
    "line": 35
  }],
  "prId": 23348
}, {
  "comments": [{
    "author": {
      "login": "tgravescs"
    },
    "body": "I think we should rephrase this to make sure users aren't confused by you only need 1 token for everything. I think the intent here is to say we use the token we acquired once rather then continuing to get new tokens.\r\n\r\n",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-07T14:24:51Z",
    "diffHunk": "@@ -0,0 +1,238 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demystify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application (e.g. Spark executors),\n+so they can authenticate to services.\n+\n+* A single token is used for authentication"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "The intent here is explained in the following paragraph. I'll add \"per service\" but I don't see how any other rephrasing can replace the following paragraph.",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-07T17:55:37Z",
    "diffHunk": "@@ -0,0 +1,238 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demystify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application (e.g. Spark executors),\n+so they can authenticate to services.\n+\n+* A single token is used for authentication"
  }],
  "prId": 23348
}, {
  "comments": [{
    "author": {
      "login": "tgravescs"
    },
    "body": "I'm not sure what benefit this paragraph adds?   If you want to state you need service specific logic to create I think that is fine but the rest of it seems extra to me, talking about what we could do doesn't add any benefit to the user/dev as that would be a architecture change across many different apache projects",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-07T14:29:12Z",
    "diffHunk": "@@ -0,0 +1,238 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demystify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application (e.g. Spark executors),\n+so they can authenticate to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g. Spark\n+executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates unnecessary\n+extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used by many services to replace Kerberos\n+authentication, or even other forms of authentication, although there is nothing (aside from\n+maybe implementation details) that ties them to Kerberos or any other authentication mechanism.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and",
    "line": 59
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "It explains why Spark needs so much code to deal with delegation tokens, including an extensible interface, that wouldn't be needed if they were more like kerberos (where you just talk to one place).",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-07T17:56:37Z",
    "diffHunk": "@@ -0,0 +1,238 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demystify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application (e.g. Spark executors),\n+so they can authenticate to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g. Spark\n+executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates unnecessary\n+extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used by many services to replace Kerberos\n+authentication, or even other forms of authentication, although there is nothing (aside from\n+maybe implementation details) that ties them to Kerberos or any other authentication mechanism.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and",
    "line": 59
  }, {
    "author": {
      "login": "tgravescs"
    },
    "body": "ok, I guess I don't see the benefit in talking about what we could do, I would prefer to see it just state the drawback and if you want to compare to kerberos, compare to it.  But its not that big of a deal either.",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-07T19:07:39Z",
    "diffHunk": "@@ -0,0 +1,238 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demystify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application (e.g. Spark executors),\n+so they can authenticate to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g. Spark\n+executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates unnecessary\n+extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used by many services to replace Kerberos\n+authentication, or even other forms of authentication, although there is nothing (aside from\n+maybe implementation details) that ties them to Kerberos or any other authentication mechanism.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and",
    "line": 59
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "ok, I'll remove.",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-07T19:35:14Z",
    "diffHunk": "@@ -0,0 +1,238 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demystify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application (e.g. Spark executors),\n+so they can authenticate to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g. Spark\n+executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates unnecessary\n+extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used by many services to replace Kerberos\n+authentication, or even other forms of authentication, although there is nothing (aside from\n+maybe implementation details) that ties them to Kerberos or any other authentication mechanism.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and",
    "line": 59
  }],
  "prId": 23348
}, {
  "comments": [{
    "author": {
      "login": "tgravescs"
    },
    "body": "I think this is a yarn only way of distributing, @vanzin k8s, mesos doesn't use UGI does it?",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-07T14:32:59Z",
    "diffHunk": "@@ -0,0 +1,238 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demystify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application (e.g. Spark executors),\n+so they can authenticate to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g. Spark\n+executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates unnecessary\n+extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used by many services to replace Kerberos\n+authentication, or even other forms of authentication, although there is nothing (aside from\n+maybe implementation details) that ties them to Kerberos or any other authentication mechanism.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and\n+use them. While it would be possible to create a shared API or even a shared service to manage the\n+creation and use of DTs, that doesn't currently exist, and retrofitting such a system would be a\n+huge change in a bunch of different services.\n+\n+Spark works around this by having a (somewhat) pluggable, internal DT creation API. Support for new\n+services can be added by implementing a `HadoopDelegationTokenProvider` that is then called by Spark\n+when generating delegation tokens for an application. Spark distributes tokens to executors using\n+the `UserGroupInformation` Hadoop API, and it's up to the DT provider and the respective client"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "You're thinking about writing the tokens to a file and automatically loading them, which is a YARN-only thing. Spark has its own mechanism to distribute tokens, which it uses even in YARN.\r\n\r\nThe YARN token API is mostly used so YARN can use the app's tokens for log aggregation and stuff.",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-07T17:57:39Z",
    "diffHunk": "@@ -0,0 +1,238 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demystify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application (e.g. Spark executors),\n+so they can authenticate to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g. Spark\n+executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates unnecessary\n+extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used by many services to replace Kerberos\n+authentication, or even other forms of authentication, although there is nothing (aside from\n+maybe implementation details) that ties them to Kerberos or any other authentication mechanism.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and\n+use them. While it would be possible to create a shared API or even a shared service to manage the\n+creation and use of DTs, that doesn't currently exist, and retrofitting such a system would be a\n+huge change in a bunch of different services.\n+\n+Spark works around this by having a (somewhat) pluggable, internal DT creation API. Support for new\n+services can be added by implementing a `HadoopDelegationTokenProvider` that is then called by Spark\n+when generating delegation tokens for an application. Spark distributes tokens to executors using\n+the `UserGroupInformation` Hadoop API, and it's up to the DT provider and the respective client"
  }, {
    "author": {
      "login": "tgravescs"
    },
    "body": "I'm not following, the sentence above says spark distributes tokens to the executors using the UGI hadoop api. this works on yarn because its passed when you launch an executor, but how does the UGI get from the driver to executors on mesos?",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-07T18:47:38Z",
    "diffHunk": "@@ -0,0 +1,238 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demystify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application (e.g. Spark executors),\n+so they can authenticate to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g. Spark\n+executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates unnecessary\n+extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used by many services to replace Kerberos\n+authentication, or even other forms of authentication, although there is nothing (aside from\n+maybe implementation details) that ties them to Kerberos or any other authentication mechanism.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and\n+use them. While it would be possible to create a shared API or even a shared service to manage the\n+creation and use of DTs, that doesn't currently exist, and retrofitting such a system would be a\n+huge change in a bunch of different services.\n+\n+Spark works around this by having a (somewhat) pluggable, internal DT creation API. Support for new\n+services can be added by implementing a `HadoopDelegationTokenProvider` that is then called by Spark\n+when generating delegation tokens for an application. Spark distributes tokens to executors using\n+the `UserGroupInformation` Hadoop API, and it's up to the DT provider and the respective client"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "Spark distributes the tokens using its RPC system, but they're stashed in the UGI, so if an executor needs to get the tokens, it needs to look at the UGI.\r\n\r\nAnyway, I'll replace \"distributes\" since it seems to be causing confusion.",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-07T19:36:40Z",
    "diffHunk": "@@ -0,0 +1,238 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demystify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application (e.g. Spark executors),\n+so they can authenticate to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g. Spark\n+executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates unnecessary\n+extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used by many services to replace Kerberos\n+authentication, or even other forms of authentication, although there is nothing (aside from\n+maybe implementation details) that ties them to Kerberos or any other authentication mechanism.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and\n+use them. While it would be possible to create a shared API or even a shared service to manage the\n+creation and use of DTs, that doesn't currently exist, and retrofitting such a system would be a\n+huge change in a bunch of different services.\n+\n+Spark works around this by having a (somewhat) pluggable, internal DT creation API. Support for new\n+services can be added by implementing a `HadoopDelegationTokenProvider` that is then called by Spark\n+when generating delegation tokens for an application. Spark distributes tokens to executors using\n+the `UserGroupInformation` Hadoop API, and it's up to the DT provider and the respective client"
  }],
  "prId": 23348
}, {
  "comments": [{
    "author": {
      "login": "tgravescs"
    },
    "body": "I believe hive ones are also renewed.  I don't remember if hbase ones need renewed or are just a single lifetime. I think my point is this is a pretty broad statement that could be incorrect so I think we should change the wording.",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-07T14:59:01Z",
    "diffHunk": "@@ -0,0 +1,238 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demystify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application (e.g. Spark executors),\n+so they can authenticate to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g. Spark\n+executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates unnecessary\n+extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used by many services to replace Kerberos\n+authentication, or even other forms of authentication, although there is nothing (aside from\n+maybe implementation details) that ties them to Kerberos or any other authentication mechanism.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and\n+use them. While it would be possible to create a shared API or even a shared service to manage the\n+creation and use of DTs, that doesn't currently exist, and retrofitting such a system would be a\n+huge change in a bunch of different services.\n+\n+Spark works around this by having a (somewhat) pluggable, internal DT creation API. Support for new\n+services can be added by implementing a `HadoopDelegationTokenProvider` that is then called by Spark\n+when generating delegation tokens for an application. Spark distributes tokens to executors using\n+the `UserGroupInformation` Hadoop API, and it's up to the DT provider and the respective client\n+library to agree on how to use those tokens.\n+\n+Once they are created, the semantics of how DTs operate are also service-specific. But, in general,\n+they try to follow the semantics of Kerberos tokens:\n+\n+* A \"lifetime\" which is for how long the DT is valid before it requires renewal.\n+* A \"renewable life\" which is for how long the DT can be renewed.\n+\n+Once the token reaches its \"renewable life\", a new one needs to be created by contacting the\n+appropriate service, restarting the above process.\n+\n+\n+## DT Renewal, Renewers, and YARN\n+\n+This is the most confusing part of DT handling, and part of it is because much of the system was\n+designed with MapReduce, and later YARN, in mind.\n+\n+As seen above, DTs need to be renewed periodically until they finally expire for good. An example of\n+this is the default configuration of HDFS services: delegation tokens are valid for up to 7 days,\n+and need to be renewed every 24 hours. If 24 hours pass without the token being renewed, the token\n+cannot be used anymore. And the token cannot be renewed anymore after 7 days.\n+\n+This raises the question: who renews tokens? And for a long time the answer was YARN.\n+\n+When YARN applications are submitted, a set of DTs is also submitted with them. YARN takes care\n+of distributing these tokens to containers (using conventions set by the `UserGroupInformation`\n+API) and, also, keeping them renewed while the app is running. These tokens are used not just\n+by the application; they are also used by YARN itself to implement features like log collection\n+and aggregation.\n+\n+But this has a few caveats.\n+\n+\n+1. Who renews the tokens?\n+\n+This is handled mostly transparently by the Hadoop libraries in the case of YARN. Some services have\n+the concept of a token \"renewer\". This \"renewer\" is the name of the principal that is allowed to\n+renew the DT. When submitting to YARN, that will be the principal that the YARN service is running\n+as, which means that the client application needs to know that information.\n+\n+For other resource managers, the renewer mostly does not matter, since there is no service that\n+is doing the renewal. Except that it sometimes leaks into library code, such as in SPARK-20328.\n+\n+\n+2. What tokens are renewed?\n+\n+This is probably the biggest caveat.\n+\n+As discussed in the previous section, DTs are service-specific, and require service-specific\n+libraries for creation *and* renewal. This means that for YARN to be able to renew application\n+tokens, YARN needs:\n+\n+* The client libraries for all the services the application is using\n+* Information about how to connect to the services the application is using\n+* Permissions to connect to those services\n+\n+In reality, though, most of the time YARN has access to a single HDFS cluster, and that will be\n+the extent of its DT renewal features. Any other tokens sent to YARN will be distributed to",
    "line": 125
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "YARN does not have Hive client libraries in its classpath, so how can it even talk to the HMS or HS2 at all?",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-07T17:59:02Z",
    "diffHunk": "@@ -0,0 +1,238 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demystify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application (e.g. Spark executors),\n+so they can authenticate to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g. Spark\n+executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates unnecessary\n+extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used by many services to replace Kerberos\n+authentication, or even other forms of authentication, although there is nothing (aside from\n+maybe implementation details) that ties them to Kerberos or any other authentication mechanism.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and\n+use them. While it would be possible to create a shared API or even a shared service to manage the\n+creation and use of DTs, that doesn't currently exist, and retrofitting such a system would be a\n+huge change in a bunch of different services.\n+\n+Spark works around this by having a (somewhat) pluggable, internal DT creation API. Support for new\n+services can be added by implementing a `HadoopDelegationTokenProvider` that is then called by Spark\n+when generating delegation tokens for an application. Spark distributes tokens to executors using\n+the `UserGroupInformation` Hadoop API, and it's up to the DT provider and the respective client\n+library to agree on how to use those tokens.\n+\n+Once they are created, the semantics of how DTs operate are also service-specific. But, in general,\n+they try to follow the semantics of Kerberos tokens:\n+\n+* A \"lifetime\" which is for how long the DT is valid before it requires renewal.\n+* A \"renewable life\" which is for how long the DT can be renewed.\n+\n+Once the token reaches its \"renewable life\", a new one needs to be created by contacting the\n+appropriate service, restarting the above process.\n+\n+\n+## DT Renewal, Renewers, and YARN\n+\n+This is the most confusing part of DT handling, and part of it is because much of the system was\n+designed with MapReduce, and later YARN, in mind.\n+\n+As seen above, DTs need to be renewed periodically until they finally expire for good. An example of\n+this is the default configuration of HDFS services: delegation tokens are valid for up to 7 days,\n+and need to be renewed every 24 hours. If 24 hours pass without the token being renewed, the token\n+cannot be used anymore. And the token cannot be renewed anymore after 7 days.\n+\n+This raises the question: who renews tokens? And for a long time the answer was YARN.\n+\n+When YARN applications are submitted, a set of DTs is also submitted with them. YARN takes care\n+of distributing these tokens to containers (using conventions set by the `UserGroupInformation`\n+API) and, also, keeping them renewed while the app is running. These tokens are used not just\n+by the application; they are also used by YARN itself to implement features like log collection\n+and aggregation.\n+\n+But this has a few caveats.\n+\n+\n+1. Who renews the tokens?\n+\n+This is handled mostly transparently by the Hadoop libraries in the case of YARN. Some services have\n+the concept of a token \"renewer\". This \"renewer\" is the name of the principal that is allowed to\n+renew the DT. When submitting to YARN, that will be the principal that the YARN service is running\n+as, which means that the client application needs to know that information.\n+\n+For other resource managers, the renewer mostly does not matter, since there is no service that\n+is doing the renewal. Except that it sometimes leaks into library code, such as in SPARK-20328.\n+\n+\n+2. What tokens are renewed?\n+\n+This is probably the biggest caveat.\n+\n+As discussed in the previous section, DTs are service-specific, and require service-specific\n+libraries for creation *and* renewal. This means that for YARN to be able to renew application\n+tokens, YARN needs:\n+\n+* The client libraries for all the services the application is using\n+* Information about how to connect to the services the application is using\n+* Permissions to connect to those services\n+\n+In reality, though, most of the time YARN has access to a single HDFS cluster, and that will be\n+the extent of its DT renewal features. Any other tokens sent to YARN will be distributed to",
    "line": 125
  }, {
    "author": {
      "login": "tgravescs"
    },
    "body": "ok checked with our hive team I was wrong.",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-07T18:45:54Z",
    "diffHunk": "@@ -0,0 +1,238 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demystify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application (e.g. Spark executors),\n+so they can authenticate to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g. Spark\n+executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates unnecessary\n+extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used by many services to replace Kerberos\n+authentication, or even other forms of authentication, although there is nothing (aside from\n+maybe implementation details) that ties them to Kerberos or any other authentication mechanism.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and\n+use them. While it would be possible to create a shared API or even a shared service to manage the\n+creation and use of DTs, that doesn't currently exist, and retrofitting such a system would be a\n+huge change in a bunch of different services.\n+\n+Spark works around this by having a (somewhat) pluggable, internal DT creation API. Support for new\n+services can be added by implementing a `HadoopDelegationTokenProvider` that is then called by Spark\n+when generating delegation tokens for an application. Spark distributes tokens to executors using\n+the `UserGroupInformation` Hadoop API, and it's up to the DT provider and the respective client\n+library to agree on how to use those tokens.\n+\n+Once they are created, the semantics of how DTs operate are also service-specific. But, in general,\n+they try to follow the semantics of Kerberos tokens:\n+\n+* A \"lifetime\" which is for how long the DT is valid before it requires renewal.\n+* A \"renewable life\" which is for how long the DT can be renewed.\n+\n+Once the token reaches its \"renewable life\", a new one needs to be created by contacting the\n+appropriate service, restarting the above process.\n+\n+\n+## DT Renewal, Renewers, and YARN\n+\n+This is the most confusing part of DT handling, and part of it is because much of the system was\n+designed with MapReduce, and later YARN, in mind.\n+\n+As seen above, DTs need to be renewed periodically until they finally expire for good. An example of\n+this is the default configuration of HDFS services: delegation tokens are valid for up to 7 days,\n+and need to be renewed every 24 hours. If 24 hours pass without the token being renewed, the token\n+cannot be used anymore. And the token cannot be renewed anymore after 7 days.\n+\n+This raises the question: who renews tokens? And for a long time the answer was YARN.\n+\n+When YARN applications are submitted, a set of DTs is also submitted with them. YARN takes care\n+of distributing these tokens to containers (using conventions set by the `UserGroupInformation`\n+API) and, also, keeping them renewed while the app is running. These tokens are used not just\n+by the application; they are also used by YARN itself to implement features like log collection\n+and aggregation.\n+\n+But this has a few caveats.\n+\n+\n+1. Who renews the tokens?\n+\n+This is handled mostly transparently by the Hadoop libraries in the case of YARN. Some services have\n+the concept of a token \"renewer\". This \"renewer\" is the name of the principal that is allowed to\n+renew the DT. When submitting to YARN, that will be the principal that the YARN service is running\n+as, which means that the client application needs to know that information.\n+\n+For other resource managers, the renewer mostly does not matter, since there is no service that\n+is doing the renewal. Except that it sometimes leaks into library code, such as in SPARK-20328.\n+\n+\n+2. What tokens are renewed?\n+\n+This is probably the biggest caveat.\n+\n+As discussed in the previous section, DTs are service-specific, and require service-specific\n+libraries for creation *and* renewal. This means that for YARN to be able to renew application\n+tokens, YARN needs:\n+\n+* The client libraries for all the services the application is using\n+* Information about how to connect to the services the application is using\n+* Permissions to connect to those services\n+\n+In reality, though, most of the time YARN has access to a single HDFS cluster, and that will be\n+the extent of its DT renewal features. Any other tokens sent to YARN will be distributed to",
    "line": 125
  }],
  "prId": 23348
}, {
  "comments": [{
    "author": {
      "login": "tgravescs"
    },
    "body": "while this is true I'm not sure this is an issue, perhaps we should just point out that different services implement them differently, not all of them need a renewal.    ie your first sentence \"to make matters worse\" I think should be removed.  In a way its nice that hbase tokens don't need renewal as you don't have to worry about that in your code.  But it does lead to inconsistency between services.",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-07T15:03:08Z",
    "diffHunk": "@@ -0,0 +1,238 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demystify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application (e.g. Spark executors),\n+so they can authenticate to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g. Spark\n+executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates unnecessary\n+extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used by many services to replace Kerberos\n+authentication, or even other forms of authentication, although there is nothing (aside from\n+maybe implementation details) that ties them to Kerberos or any other authentication mechanism.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and\n+use them. While it would be possible to create a shared API or even a shared service to manage the\n+creation and use of DTs, that doesn't currently exist, and retrofitting such a system would be a\n+huge change in a bunch of different services.\n+\n+Spark works around this by having a (somewhat) pluggable, internal DT creation API. Support for new\n+services can be added by implementing a `HadoopDelegationTokenProvider` that is then called by Spark\n+when generating delegation tokens for an application. Spark distributes tokens to executors using\n+the `UserGroupInformation` Hadoop API, and it's up to the DT provider and the respective client\n+library to agree on how to use those tokens.\n+\n+Once they are created, the semantics of how DTs operate are also service-specific. But, in general,\n+they try to follow the semantics of Kerberos tokens:\n+\n+* A \"lifetime\" which is for how long the DT is valid before it requires renewal.\n+* A \"renewable life\" which is for how long the DT can be renewed.\n+\n+Once the token reaches its \"renewable life\", a new one needs to be created by contacting the\n+appropriate service, restarting the above process.\n+\n+\n+## DT Renewal, Renewers, and YARN\n+\n+This is the most confusing part of DT handling, and part of it is because much of the system was\n+designed with MapReduce, and later YARN, in mind.\n+\n+As seen above, DTs need to be renewed periodically until they finally expire for good. An example of\n+this is the default configuration of HDFS services: delegation tokens are valid for up to 7 days,\n+and need to be renewed every 24 hours. If 24 hours pass without the token being renewed, the token\n+cannot be used anymore. And the token cannot be renewed anymore after 7 days.\n+\n+This raises the question: who renews tokens? And for a long time the answer was YARN.\n+\n+When YARN applications are submitted, a set of DTs is also submitted with them. YARN takes care\n+of distributing these tokens to containers (using conventions set by the `UserGroupInformation`\n+API) and, also, keeping them renewed while the app is running. These tokens are used not just\n+by the application; they are also used by YARN itself to implement features like log collection\n+and aggregation.\n+\n+But this has a few caveats.\n+\n+\n+1. Who renews the tokens?\n+\n+This is handled mostly transparently by the Hadoop libraries in the case of YARN. Some services have\n+the concept of a token \"renewer\". This \"renewer\" is the name of the principal that is allowed to\n+renew the DT. When submitting to YARN, that will be the principal that the YARN service is running\n+as, which means that the client application needs to know that information.\n+\n+For other resource managers, the renewer mostly does not matter, since there is no service that\n+is doing the renewal. Except that it sometimes leaks into library code, such as in SPARK-20328.\n+\n+\n+2. What tokens are renewed?\n+\n+This is probably the biggest caveat.\n+\n+As discussed in the previous section, DTs are service-specific, and require service-specific\n+libraries for creation *and* renewal. This means that for YARN to be able to renew application\n+tokens, YARN needs:\n+\n+* The client libraries for all the services the application is using\n+* Information about how to connect to the services the application is using\n+* Permissions to connect to those services\n+\n+In reality, though, most of the time YARN has access to a single HDFS cluster, and that will be\n+the extent of its DT renewal features. Any other tokens sent to YARN will be distributed to\n+containers, but will not be renewed.\n+\n+This means that those tokens will expire way before their max lifetime, unless some other code\n+takes care of renewing them.\n+\n+And, to make matters worse, not all client libraries even implement token renewal. To use the\n+example of a service supported by Spark, the `renew()` method of HBase tokens is a no-op. So\n+the only way to \"renew\" an HBase token is to create a new one."
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "This is build-up for explaining why Spark handles DTs the way it does.",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-07T17:59:40Z",
    "diffHunk": "@@ -0,0 +1,238 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demystify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application (e.g. Spark executors),\n+so they can authenticate to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g. Spark\n+executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates unnecessary\n+extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used by many services to replace Kerberos\n+authentication, or even other forms of authentication, although there is nothing (aside from\n+maybe implementation details) that ties them to Kerberos or any other authentication mechanism.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and\n+use them. While it would be possible to create a shared API or even a shared service to manage the\n+creation and use of DTs, that doesn't currently exist, and retrofitting such a system would be a\n+huge change in a bunch of different services.\n+\n+Spark works around this by having a (somewhat) pluggable, internal DT creation API. Support for new\n+services can be added by implementing a `HadoopDelegationTokenProvider` that is then called by Spark\n+when generating delegation tokens for an application. Spark distributes tokens to executors using\n+the `UserGroupInformation` Hadoop API, and it's up to the DT provider and the respective client\n+library to agree on how to use those tokens.\n+\n+Once they are created, the semantics of how DTs operate are also service-specific. But, in general,\n+they try to follow the semantics of Kerberos tokens:\n+\n+* A \"lifetime\" which is for how long the DT is valid before it requires renewal.\n+* A \"renewable life\" which is for how long the DT can be renewed.\n+\n+Once the token reaches its \"renewable life\", a new one needs to be created by contacting the\n+appropriate service, restarting the above process.\n+\n+\n+## DT Renewal, Renewers, and YARN\n+\n+This is the most confusing part of DT handling, and part of it is because much of the system was\n+designed with MapReduce, and later YARN, in mind.\n+\n+As seen above, DTs need to be renewed periodically until they finally expire for good. An example of\n+this is the default configuration of HDFS services: delegation tokens are valid for up to 7 days,\n+and need to be renewed every 24 hours. If 24 hours pass without the token being renewed, the token\n+cannot be used anymore. And the token cannot be renewed anymore after 7 days.\n+\n+This raises the question: who renews tokens? And for a long time the answer was YARN.\n+\n+When YARN applications are submitted, a set of DTs is also submitted with them. YARN takes care\n+of distributing these tokens to containers (using conventions set by the `UserGroupInformation`\n+API) and, also, keeping them renewed while the app is running. These tokens are used not just\n+by the application; they are also used by YARN itself to implement features like log collection\n+and aggregation.\n+\n+But this has a few caveats.\n+\n+\n+1. Who renews the tokens?\n+\n+This is handled mostly transparently by the Hadoop libraries in the case of YARN. Some services have\n+the concept of a token \"renewer\". This \"renewer\" is the name of the principal that is allowed to\n+renew the DT. When submitting to YARN, that will be the principal that the YARN service is running\n+as, which means that the client application needs to know that information.\n+\n+For other resource managers, the renewer mostly does not matter, since there is no service that\n+is doing the renewal. Except that it sometimes leaks into library code, such as in SPARK-20328.\n+\n+\n+2. What tokens are renewed?\n+\n+This is probably the biggest caveat.\n+\n+As discussed in the previous section, DTs are service-specific, and require service-specific\n+libraries for creation *and* renewal. This means that for YARN to be able to renew application\n+tokens, YARN needs:\n+\n+* The client libraries for all the services the application is using\n+* Information about how to connect to the services the application is using\n+* Permissions to connect to those services\n+\n+In reality, though, most of the time YARN has access to a single HDFS cluster, and that will be\n+the extent of its DT renewal features. Any other tokens sent to YARN will be distributed to\n+containers, but will not be renewed.\n+\n+This means that those tokens will expire way before their max lifetime, unless some other code\n+takes care of renewing them.\n+\n+And, to make matters worse, not all client libraries even implement token renewal. To use the\n+example of a service supported by Spark, the `renew()` method of HBase tokens is a no-op. So\n+the only way to \"renew\" an HBase token is to create a new one."
  }],
  "prId": 23348
}, {
  "comments": [{
    "author": {
      "login": "tgravescs"
    },
    "body": "I think we should explicitly state the security ramifications of this.  distributed a keytab means that if its compromised someone can get tokens for any service as you.   ",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-07T15:06:21Z",
    "diffHunk": "@@ -0,0 +1,238 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demystify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application (e.g. Spark executors),\n+so they can authenticate to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g. Spark\n+executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates unnecessary\n+extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used by many services to replace Kerberos\n+authentication, or even other forms of authentication, although there is nothing (aside from\n+maybe implementation details) that ties them to Kerberos or any other authentication mechanism.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and\n+use them. While it would be possible to create a shared API or even a shared service to manage the\n+creation and use of DTs, that doesn't currently exist, and retrofitting such a system would be a\n+huge change in a bunch of different services.\n+\n+Spark works around this by having a (somewhat) pluggable, internal DT creation API. Support for new\n+services can be added by implementing a `HadoopDelegationTokenProvider` that is then called by Spark\n+when generating delegation tokens for an application. Spark distributes tokens to executors using\n+the `UserGroupInformation` Hadoop API, and it's up to the DT provider and the respective client\n+library to agree on how to use those tokens.\n+\n+Once they are created, the semantics of how DTs operate are also service-specific. But, in general,\n+they try to follow the semantics of Kerberos tokens:\n+\n+* A \"lifetime\" which is for how long the DT is valid before it requires renewal.\n+* A \"renewable life\" which is for how long the DT can be renewed.\n+\n+Once the token reaches its \"renewable life\", a new one needs to be created by contacting the\n+appropriate service, restarting the above process.\n+\n+\n+## DT Renewal, Renewers, and YARN\n+\n+This is the most confusing part of DT handling, and part of it is because much of the system was\n+designed with MapReduce, and later YARN, in mind.\n+\n+As seen above, DTs need to be renewed periodically until they finally expire for good. An example of\n+this is the default configuration of HDFS services: delegation tokens are valid for up to 7 days,\n+and need to be renewed every 24 hours. If 24 hours pass without the token being renewed, the token\n+cannot be used anymore. And the token cannot be renewed anymore after 7 days.\n+\n+This raises the question: who renews tokens? And for a long time the answer was YARN.\n+\n+When YARN applications are submitted, a set of DTs is also submitted with them. YARN takes care\n+of distributing these tokens to containers (using conventions set by the `UserGroupInformation`\n+API) and, also, keeping them renewed while the app is running. These tokens are used not just\n+by the application; they are also used by YARN itself to implement features like log collection\n+and aggregation.\n+\n+But this has a few caveats.\n+\n+\n+1. Who renews the tokens?\n+\n+This is handled mostly transparently by the Hadoop libraries in the case of YARN. Some services have\n+the concept of a token \"renewer\". This \"renewer\" is the name of the principal that is allowed to\n+renew the DT. When submitting to YARN, that will be the principal that the YARN service is running\n+as, which means that the client application needs to know that information.\n+\n+For other resource managers, the renewer mostly does not matter, since there is no service that\n+is doing the renewal. Except that it sometimes leaks into library code, such as in SPARK-20328.\n+\n+\n+2. What tokens are renewed?\n+\n+This is probably the biggest caveat.\n+\n+As discussed in the previous section, DTs are service-specific, and require service-specific\n+libraries for creation *and* renewal. This means that for YARN to be able to renew application\n+tokens, YARN needs:\n+\n+* The client libraries for all the services the application is using\n+* Information about how to connect to the services the application is using\n+* Permissions to connect to those services\n+\n+In reality, though, most of the time YARN has access to a single HDFS cluster, and that will be\n+the extent of its DT renewal features. Any other tokens sent to YARN will be distributed to\n+containers, but will not be renewed.\n+\n+This means that those tokens will expire way before their max lifetime, unless some other code\n+takes care of renewing them.\n+\n+And, to make matters worse, not all client libraries even implement token renewal. To use the\n+example of a service supported by Spark, the `renew()` method of HBase tokens is a no-op. So\n+the only way to \"renew\" an HBase token is to create a new one.\n+\n+\n+3. What happens when tokens expire for good?\n+\n+The final caveat is that DTs have a maximum life, regardless of renewal. And after that deadline\n+is met, you need to create new tokens to be able to connect to the services. That means you need\n+the ability to connect to the service without a delegation token, which requires some form of\n+authentication aside from DTs.\n+\n+This is especially important for long-running applications that run unsupervised. They must be\n+able to keep on going without having someone logging into a terminal and typing a password every\n+few days.\n+\n+\n+## DT Renewal in Spark\n+\n+Because of the issues explained above, Spark implements a different way of doing renewal. Spark's\n+solution is a compromise: it targets the lowest common denominator, which is services like HBase\n+that do not support actual token renewal.\n+\n+In Spark, DT \"renewal\" is enabled by giving the application a Kerberos keytab. A keytab is\n+basically your Kerberos password written into a plain text file, which is why it's so sensitive."
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "Explain more than \"it's like writing your password to a file\"?",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-07T18:00:47Z",
    "diffHunk": "@@ -0,0 +1,238 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demystify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application (e.g. Spark executors),\n+so they can authenticate to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g. Spark\n+executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates unnecessary\n+extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used by many services to replace Kerberos\n+authentication, or even other forms of authentication, although there is nothing (aside from\n+maybe implementation details) that ties them to Kerberos or any other authentication mechanism.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and\n+use them. While it would be possible to create a shared API or even a shared service to manage the\n+creation and use of DTs, that doesn't currently exist, and retrofitting such a system would be a\n+huge change in a bunch of different services.\n+\n+Spark works around this by having a (somewhat) pluggable, internal DT creation API. Support for new\n+services can be added by implementing a `HadoopDelegationTokenProvider` that is then called by Spark\n+when generating delegation tokens for an application. Spark distributes tokens to executors using\n+the `UserGroupInformation` Hadoop API, and it's up to the DT provider and the respective client\n+library to agree on how to use those tokens.\n+\n+Once they are created, the semantics of how DTs operate are also service-specific. But, in general,\n+they try to follow the semantics of Kerberos tokens:\n+\n+* A \"lifetime\" which is for how long the DT is valid before it requires renewal.\n+* A \"renewable life\" which is for how long the DT can be renewed.\n+\n+Once the token reaches its \"renewable life\", a new one needs to be created by contacting the\n+appropriate service, restarting the above process.\n+\n+\n+## DT Renewal, Renewers, and YARN\n+\n+This is the most confusing part of DT handling, and part of it is because much of the system was\n+designed with MapReduce, and later YARN, in mind.\n+\n+As seen above, DTs need to be renewed periodically until they finally expire for good. An example of\n+this is the default configuration of HDFS services: delegation tokens are valid for up to 7 days,\n+and need to be renewed every 24 hours. If 24 hours pass without the token being renewed, the token\n+cannot be used anymore. And the token cannot be renewed anymore after 7 days.\n+\n+This raises the question: who renews tokens? And for a long time the answer was YARN.\n+\n+When YARN applications are submitted, a set of DTs is also submitted with them. YARN takes care\n+of distributing these tokens to containers (using conventions set by the `UserGroupInformation`\n+API) and, also, keeping them renewed while the app is running. These tokens are used not just\n+by the application; they are also used by YARN itself to implement features like log collection\n+and aggregation.\n+\n+But this has a few caveats.\n+\n+\n+1. Who renews the tokens?\n+\n+This is handled mostly transparently by the Hadoop libraries in the case of YARN. Some services have\n+the concept of a token \"renewer\". This \"renewer\" is the name of the principal that is allowed to\n+renew the DT. When submitting to YARN, that will be the principal that the YARN service is running\n+as, which means that the client application needs to know that information.\n+\n+For other resource managers, the renewer mostly does not matter, since there is no service that\n+is doing the renewal. Except that it sometimes leaks into library code, such as in SPARK-20328.\n+\n+\n+2. What tokens are renewed?\n+\n+This is probably the biggest caveat.\n+\n+As discussed in the previous section, DTs are service-specific, and require service-specific\n+libraries for creation *and* renewal. This means that for YARN to be able to renew application\n+tokens, YARN needs:\n+\n+* The client libraries for all the services the application is using\n+* Information about how to connect to the services the application is using\n+* Permissions to connect to those services\n+\n+In reality, though, most of the time YARN has access to a single HDFS cluster, and that will be\n+the extent of its DT renewal features. Any other tokens sent to YARN will be distributed to\n+containers, but will not be renewed.\n+\n+This means that those tokens will expire way before their max lifetime, unless some other code\n+takes care of renewing them.\n+\n+And, to make matters worse, not all client libraries even implement token renewal. To use the\n+example of a service supported by Spark, the `renew()` method of HBase tokens is a no-op. So\n+the only way to \"renew\" an HBase token is to create a new one.\n+\n+\n+3. What happens when tokens expire for good?\n+\n+The final caveat is that DTs have a maximum life, regardless of renewal. And after that deadline\n+is met, you need to create new tokens to be able to connect to the services. That means you need\n+the ability to connect to the service without a delegation token, which requires some form of\n+authentication aside from DTs.\n+\n+This is especially important for long-running applications that run unsupervised. They must be\n+able to keep on going without having someone logging into a terminal and typing a password every\n+few days.\n+\n+\n+## DT Renewal in Spark\n+\n+Because of the issues explained above, Spark implements a different way of doing renewal. Spark's\n+solution is a compromise: it targets the lowest common denominator, which is services like HBase\n+that do not support actual token renewal.\n+\n+In Spark, DT \"renewal\" is enabled by giving the application a Kerberos keytab. A keytab is\n+basically your Kerberos password written into a plain text file, which is why it's so sensitive."
  }, {
    "author": {
      "login": "tgravescs"
    },
    "body": "Yes, I can see people not understanding that is a security concern.   If the file is written to HDFS with permissions restrictive, that might seem ok, but many security people would disagree.  Perhaps just add another sentence talking about this is a security concern because if the keytab is compromised people can get tokens for any service for an indefinite amount of time.  So users should evaluate if using this approach is ok.",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-07T19:14:07Z",
    "diffHunk": "@@ -0,0 +1,238 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demystify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application (e.g. Spark executors),\n+so they can authenticate to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g. Spark\n+executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates unnecessary\n+extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used by many services to replace Kerberos\n+authentication, or even other forms of authentication, although there is nothing (aside from\n+maybe implementation details) that ties them to Kerberos or any other authentication mechanism.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and\n+use them. While it would be possible to create a shared API or even a shared service to manage the\n+creation and use of DTs, that doesn't currently exist, and retrofitting such a system would be a\n+huge change in a bunch of different services.\n+\n+Spark works around this by having a (somewhat) pluggable, internal DT creation API. Support for new\n+services can be added by implementing a `HadoopDelegationTokenProvider` that is then called by Spark\n+when generating delegation tokens for an application. Spark distributes tokens to executors using\n+the `UserGroupInformation` Hadoop API, and it's up to the DT provider and the respective client\n+library to agree on how to use those tokens.\n+\n+Once they are created, the semantics of how DTs operate are also service-specific. But, in general,\n+they try to follow the semantics of Kerberos tokens:\n+\n+* A \"lifetime\" which is for how long the DT is valid before it requires renewal.\n+* A \"renewable life\" which is for how long the DT can be renewed.\n+\n+Once the token reaches its \"renewable life\", a new one needs to be created by contacting the\n+appropriate service, restarting the above process.\n+\n+\n+## DT Renewal, Renewers, and YARN\n+\n+This is the most confusing part of DT handling, and part of it is because much of the system was\n+designed with MapReduce, and later YARN, in mind.\n+\n+As seen above, DTs need to be renewed periodically until they finally expire for good. An example of\n+this is the default configuration of HDFS services: delegation tokens are valid for up to 7 days,\n+and need to be renewed every 24 hours. If 24 hours pass without the token being renewed, the token\n+cannot be used anymore. And the token cannot be renewed anymore after 7 days.\n+\n+This raises the question: who renews tokens? And for a long time the answer was YARN.\n+\n+When YARN applications are submitted, a set of DTs is also submitted with them. YARN takes care\n+of distributing these tokens to containers (using conventions set by the `UserGroupInformation`\n+API) and, also, keeping them renewed while the app is running. These tokens are used not just\n+by the application; they are also used by YARN itself to implement features like log collection\n+and aggregation.\n+\n+But this has a few caveats.\n+\n+\n+1. Who renews the tokens?\n+\n+This is handled mostly transparently by the Hadoop libraries in the case of YARN. Some services have\n+the concept of a token \"renewer\". This \"renewer\" is the name of the principal that is allowed to\n+renew the DT. When submitting to YARN, that will be the principal that the YARN service is running\n+as, which means that the client application needs to know that information.\n+\n+For other resource managers, the renewer mostly does not matter, since there is no service that\n+is doing the renewal. Except that it sometimes leaks into library code, such as in SPARK-20328.\n+\n+\n+2. What tokens are renewed?\n+\n+This is probably the biggest caveat.\n+\n+As discussed in the previous section, DTs are service-specific, and require service-specific\n+libraries for creation *and* renewal. This means that for YARN to be able to renew application\n+tokens, YARN needs:\n+\n+* The client libraries for all the services the application is using\n+* Information about how to connect to the services the application is using\n+* Permissions to connect to those services\n+\n+In reality, though, most of the time YARN has access to a single HDFS cluster, and that will be\n+the extent of its DT renewal features. Any other tokens sent to YARN will be distributed to\n+containers, but will not be renewed.\n+\n+This means that those tokens will expire way before their max lifetime, unless some other code\n+takes care of renewing them.\n+\n+And, to make matters worse, not all client libraries even implement token renewal. To use the\n+example of a service supported by Spark, the `renew()` method of HBase tokens is a no-op. So\n+the only way to \"renew\" an HBase token is to create a new one.\n+\n+\n+3. What happens when tokens expire for good?\n+\n+The final caveat is that DTs have a maximum life, regardless of renewal. And after that deadline\n+is met, you need to create new tokens to be able to connect to the services. That means you need\n+the ability to connect to the service without a delegation token, which requires some form of\n+authentication aside from DTs.\n+\n+This is especially important for long-running applications that run unsupervised. They must be\n+able to keep on going without having someone logging into a terminal and typing a password every\n+few days.\n+\n+\n+## DT Renewal in Spark\n+\n+Because of the issues explained above, Spark implements a different way of doing renewal. Spark's\n+solution is a compromise: it targets the lowest common denominator, which is services like HBase\n+that do not support actual token renewal.\n+\n+In Spark, DT \"renewal\" is enabled by giving the application a Kerberos keytab. A keytab is\n+basically your Kerberos password written into a plain text file, which is why it's so sensitive."
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "It sounds like you want to warn users of how dangerous this can be. This is not end-user documentation, so even if I make that statement here, it will probably not have the effect you want.",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-07T19:39:10Z",
    "diffHunk": "@@ -0,0 +1,238 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demystify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.\n+\n+DTs allow for a single place (e.g. the Spark driver) to require Kerberos credentials. That entity\n+can then distribute the DTs to other parts of the distributed application (e.g. Spark executors),\n+so they can authenticate to services.\n+\n+* A single token is used for authentication\n+\n+If Kerberos authentication were used, each client connection to a server would require a trip\n+to the KDC and generation of a service ticket. In a distributed system, the number of service\n+tickets can balloon pretty quickly when you think about the number of client processes (e.g. Spark\n+executors) vs. the number of service processes (e.g. HDFS DataNodes). That generates unnecessary\n+extra load on the KDC, and may even run into usage limits set up by the KDC admin.\n+\n+\n+So in short, DTs are *not* Kerberos tokens. They are used by many services to replace Kerberos\n+authentication, or even other forms of authentication, although there is nothing (aside from\n+maybe implementation details) that ties them to Kerberos or any other authentication mechanism.\n+\n+\n+## Lifecycle of DTs\n+\n+DTs, unlike Kerberos tokens, are service-specific. There is no centralized location you contact\n+to create a DT for a service. So, the first step needed to get a DT is being able to authenticate\n+to the service in question. In the Hadoop ecosystem, that is generally done using Kerberos.\n+\n+This requires Kerberos credentials to be available somewhere for the application to use. The user\n+is generally responsible for providing those credentials, which is most commonly done by logging\n+in to the KDC (e.g. using \"kinit\"). That generates a (Kerberos) \"token cache\" containing a TGT\n+(ticket granting ticket), which can then be used to request service tickets.\n+\n+There are other ways of obtaining TGTs, but, ultimately, you need a TGT to bootstrap the process.\n+\n+Once a TGT is available, the target service's client library can then be used to authenticate\n+to the service using the Kerberos credentials, and request the creation of a delegation token.\n+This token can now be sent to other processes and used to authenticate to different daemons\n+belonging to that service.\n+\n+And thus the first drawback of DTs becomes apparent: you need service-specific logic to create and\n+use them. While it would be possible to create a shared API or even a shared service to manage the\n+creation and use of DTs, that doesn't currently exist, and retrofitting such a system would be a\n+huge change in a bunch of different services.\n+\n+Spark works around this by having a (somewhat) pluggable, internal DT creation API. Support for new\n+services can be added by implementing a `HadoopDelegationTokenProvider` that is then called by Spark\n+when generating delegation tokens for an application. Spark distributes tokens to executors using\n+the `UserGroupInformation` Hadoop API, and it's up to the DT provider and the respective client\n+library to agree on how to use those tokens.\n+\n+Once they are created, the semantics of how DTs operate are also service-specific. But, in general,\n+they try to follow the semantics of Kerberos tokens:\n+\n+* A \"lifetime\" which is for how long the DT is valid before it requires renewal.\n+* A \"renewable life\" which is for how long the DT can be renewed.\n+\n+Once the token reaches its \"renewable life\", a new one needs to be created by contacting the\n+appropriate service, restarting the above process.\n+\n+\n+## DT Renewal, Renewers, and YARN\n+\n+This is the most confusing part of DT handling, and part of it is because much of the system was\n+designed with MapReduce, and later YARN, in mind.\n+\n+As seen above, DTs need to be renewed periodically until they finally expire for good. An example of\n+this is the default configuration of HDFS services: delegation tokens are valid for up to 7 days,\n+and need to be renewed every 24 hours. If 24 hours pass without the token being renewed, the token\n+cannot be used anymore. And the token cannot be renewed anymore after 7 days.\n+\n+This raises the question: who renews tokens? And for a long time the answer was YARN.\n+\n+When YARN applications are submitted, a set of DTs is also submitted with them. YARN takes care\n+of distributing these tokens to containers (using conventions set by the `UserGroupInformation`\n+API) and, also, keeping them renewed while the app is running. These tokens are used not just\n+by the application; they are also used by YARN itself to implement features like log collection\n+and aggregation.\n+\n+But this has a few caveats.\n+\n+\n+1. Who renews the tokens?\n+\n+This is handled mostly transparently by the Hadoop libraries in the case of YARN. Some services have\n+the concept of a token \"renewer\". This \"renewer\" is the name of the principal that is allowed to\n+renew the DT. When submitting to YARN, that will be the principal that the YARN service is running\n+as, which means that the client application needs to know that information.\n+\n+For other resource managers, the renewer mostly does not matter, since there is no service that\n+is doing the renewal. Except that it sometimes leaks into library code, such as in SPARK-20328.\n+\n+\n+2. What tokens are renewed?\n+\n+This is probably the biggest caveat.\n+\n+As discussed in the previous section, DTs are service-specific, and require service-specific\n+libraries for creation *and* renewal. This means that for YARN to be able to renew application\n+tokens, YARN needs:\n+\n+* The client libraries for all the services the application is using\n+* Information about how to connect to the services the application is using\n+* Permissions to connect to those services\n+\n+In reality, though, most of the time YARN has access to a single HDFS cluster, and that will be\n+the extent of its DT renewal features. Any other tokens sent to YARN will be distributed to\n+containers, but will not be renewed.\n+\n+This means that those tokens will expire way before their max lifetime, unless some other code\n+takes care of renewing them.\n+\n+And, to make matters worse, not all client libraries even implement token renewal. To use the\n+example of a service supported by Spark, the `renew()` method of HBase tokens is a no-op. So\n+the only way to \"renew\" an HBase token is to create a new one.\n+\n+\n+3. What happens when tokens expire for good?\n+\n+The final caveat is that DTs have a maximum life, regardless of renewal. And after that deadline\n+is met, you need to create new tokens to be able to connect to the services. That means you need\n+the ability to connect to the service without a delegation token, which requires some form of\n+authentication aside from DTs.\n+\n+This is especially important for long-running applications that run unsupervised. They must be\n+able to keep on going without having someone logging into a terminal and typing a password every\n+few days.\n+\n+\n+## DT Renewal in Spark\n+\n+Because of the issues explained above, Spark implements a different way of doing renewal. Spark's\n+solution is a compromise: it targets the lowest common denominator, which is services like HBase\n+that do not support actual token renewal.\n+\n+In Spark, DT \"renewal\" is enabled by giving the application a Kerberos keytab. A keytab is\n+basically your Kerberos password written into a plain text file, which is why it's so sensitive."
  }],
  "prId": 23348
}, {
  "comments": [{
    "author": {
      "login": "tgravescs"
    },
    "body": "We seem to be mixing the section of what is a delegation token with what the benefits are, the definition of what also seems to be in  the next section with the lifecycle, perhaps we need to reorganize this a bit to have more of a section on what is DT, within there you can go into the benefits or why\r\n\r\nMight mention here that with tokens you can't acquire more tokens vs with tgts or keytab you can get more tokens so its more secure to use tokens.",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-07T15:10:16Z",
    "diffHunk": "@@ -0,0 +1,238 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demystify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.",
    "line": 17
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "I'll rename the section instead of creating a new one.",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-07T18:01:42Z",
    "diffHunk": "@@ -0,0 +1,238 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demystify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:\n+\n+* No need to distribute Kerberos credentials\n+\n+In a distributed application, distributing Kerberos credentials is tricky. Not all users have\n+keytabs, and when they do, it's generally frowned upon to distribute them over the network as\n+part of application data.",
    "line": 17
  }],
  "prId": 23348
}, {
  "comments": [{
    "author": {
      "login": "tgravescs"
    },
    "body": "I think we have 3 things now, could change to a few to not have to list certain number",
    "commit": "42f7f834f26ebc1e2af81848f5031629d717737a",
    "createdAt": "2019-01-08T14:28:12Z",
    "diffHunk": "@@ -0,0 +1,245 @@\n+# Delegation Token Handling In Spark\n+\n+This document aims to explain and demystify delegation tokens as they are used by Spark, since\n+this topic is generally a huge source of confusion.\n+\n+\n+## What are delegation tokens and why use them?\n+\n+Delegation tokens (DTs from now on) are authentication tokens used by some services to replace\n+Kerberos service tokens. Many services in the Hadoop ecosystem have support for DTs, since they\n+have two very desirable advantages over Kerberos tokens:"
  }],
  "prId": 23348
}]