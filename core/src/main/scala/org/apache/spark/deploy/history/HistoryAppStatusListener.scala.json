[{
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "nit: rename to `replaceLogUrls` (or something like that)",
    "commit": "2d0480239a4eea655b95a521ac71ed44482acde7",
    "createdAt": "2019-01-17T21:36:07Z",
    "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.config.History._\n+import org.apache.spark.scheduler.SparkListenerExecutorAdded\n+import org.apache.spark.scheduler.cluster.ExecutorInfo\n+import org.apache.spark.status.{AppStatusListener, AppStatusSource, ElementTrackingStore}\n+\n+private[spark] class HistoryAppStatusListener(\n+    kvstore: ElementTrackingStore,\n+    conf: SparkConf,\n+    live: Boolean,\n+    appStatusSource: Option[AppStatusSource] = None,\n+    lastUpdateTime: Option[Long] = None)\n+  extends AppStatusListener(kvstore, conf, live, appStatusSource, lastUpdateTime) {\n+\n+  override def onExecutorAdded(event: SparkListenerExecutorAdded): Unit = {\n+    val execInfo = event.executorInfo\n+    val newExecInfo = new ExecutorInfo(execInfo.executorHost, execInfo.totalCores,\n+      renewLogUrls(execInfo), execInfo.attributes)\n+\n+    super.onExecutorAdded(event.copy(executorInfo = newExecInfo))\n+  }\n+\n+  def renewLogUrls(execInfo: ExecutorInfo): Map[String, String] = {"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "`replaceLogUrls` sounds better. Will address.",
    "commit": "2d0480239a4eea655b95a521ac71ed44482acde7",
    "createdAt": "2019-01-17T21:57:07Z",
    "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.config.History._\n+import org.apache.spark.scheduler.SparkListenerExecutorAdded\n+import org.apache.spark.scheduler.cluster.ExecutorInfo\n+import org.apache.spark.status.{AppStatusListener, AppStatusSource, ElementTrackingStore}\n+\n+private[spark] class HistoryAppStatusListener(\n+    kvstore: ElementTrackingStore,\n+    conf: SparkConf,\n+    live: Boolean,\n+    appStatusSource: Option[AppStatusSource] = None,\n+    lastUpdateTime: Option[Long] = None)\n+  extends AppStatusListener(kvstore, conf, live, appStatusSource, lastUpdateTime) {\n+\n+  override def onExecutorAdded(event: SparkListenerExecutorAdded): Unit = {\n+    val execInfo = event.executorInfo\n+    val newExecInfo = new ExecutorInfo(execInfo.executorHost, execInfo.totalCores,\n+      renewLogUrls(execInfo), execInfo.attributes)\n+\n+    super.onExecutorAdded(event.copy(executorInfo = newExecInfo))\n+  }\n+\n+  def renewLogUrls(execInfo: ExecutorInfo): Map[String, String] = {"
  }],
  "prId": 23260
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "`keySet`",
    "commit": "2d0480239a4eea655b95a521ac71ed44482acde7",
    "createdAt": "2019-01-17T21:42:18Z",
    "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.config.History._\n+import org.apache.spark.scheduler.SparkListenerExecutorAdded\n+import org.apache.spark.scheduler.cluster.ExecutorInfo\n+import org.apache.spark.status.{AppStatusListener, AppStatusSource, ElementTrackingStore}\n+\n+private[spark] class HistoryAppStatusListener(\n+    kvstore: ElementTrackingStore,\n+    conf: SparkConf,\n+    live: Boolean,\n+    appStatusSource: Option[AppStatusSource] = None,\n+    lastUpdateTime: Option[Long] = None)\n+  extends AppStatusListener(kvstore, conf, live, appStatusSource, lastUpdateTime) {\n+\n+  override def onExecutorAdded(event: SparkListenerExecutorAdded): Unit = {\n+    val execInfo = event.executorInfo\n+    val newExecInfo = new ExecutorInfo(execInfo.executorHost, execInfo.totalCores,\n+      renewLogUrls(execInfo), execInfo.attributes)\n+\n+    super.onExecutorAdded(event.copy(executorInfo = newExecInfo))\n+  }\n+\n+  def renewLogUrls(execInfo: ExecutorInfo): Map[String, String] = {\n+    val oldLogUrlMap = execInfo.logUrlMap\n+    val attributes = execInfo.attributes\n+\n+    conf.get(CUSTOM_EXECUTOR_LOG_URL) match {\n+      case Some(logUrlPattern) =>\n+        val pattern = \"\\\\{\\\\{([A-Za-z0-9_\\\\-]+)\\\\}\\\\}\".r\n+\n+        val allPatterns = pattern.findAllMatchIn(logUrlPattern).map(_.group(1)).toSet\n+        val allPatternsExceptFileName = allPatterns.filter(_ != \"FILE_NAME\")\n+        val allAttributeKeys = attributes.keys.toSet"
  }],
  "prId": 23260
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "`replacingUrl` -> `updatedUrl`\r\n\r\nyou could also make this more functional and avoid the mutable state, eg.\r\n\r\n```scala\r\nval updatedUrl = allPatternsExceptionFileName.foldLeft(logUrlPattern) { case( orig, pattern) =>\r\n  orig.replace(...)\r\n}\r\n```",
    "commit": "2d0480239a4eea655b95a521ac71ed44482acde7",
    "createdAt": "2019-01-17T21:46:36Z",
    "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.config.History._\n+import org.apache.spark.scheduler.SparkListenerExecutorAdded\n+import org.apache.spark.scheduler.cluster.ExecutorInfo\n+import org.apache.spark.status.{AppStatusListener, AppStatusSource, ElementTrackingStore}\n+\n+private[spark] class HistoryAppStatusListener(\n+    kvstore: ElementTrackingStore,\n+    conf: SparkConf,\n+    live: Boolean,\n+    appStatusSource: Option[AppStatusSource] = None,\n+    lastUpdateTime: Option[Long] = None)\n+  extends AppStatusListener(kvstore, conf, live, appStatusSource, lastUpdateTime) {\n+\n+  override def onExecutorAdded(event: SparkListenerExecutorAdded): Unit = {\n+    val execInfo = event.executorInfo\n+    val newExecInfo = new ExecutorInfo(execInfo.executorHost, execInfo.totalCores,\n+      renewLogUrls(execInfo), execInfo.attributes)\n+\n+    super.onExecutorAdded(event.copy(executorInfo = newExecInfo))\n+  }\n+\n+  def renewLogUrls(execInfo: ExecutorInfo): Map[String, String] = {\n+    val oldLogUrlMap = execInfo.logUrlMap\n+    val attributes = execInfo.attributes\n+\n+    conf.get(CUSTOM_EXECUTOR_LOG_URL) match {\n+      case Some(logUrlPattern) =>\n+        val pattern = \"\\\\{\\\\{([A-Za-z0-9_\\\\-]+)\\\\}\\\\}\".r\n+\n+        val allPatterns = pattern.findAllMatchIn(logUrlPattern).map(_.group(1)).toSet\n+        val allPatternsExceptFileName = allPatterns.filter(_ != \"FILE_NAME\")\n+        val allAttributeKeys = attributes.keys.toSet\n+        val allAttributeKeysExceptLogFiles = allAttributeKeys.filter(_ != \"LOG_FILES\")\n+\n+        if (allPatternsExceptFileName.diff(allAttributeKeysExceptLogFiles).nonEmpty) {\n+          logFailToRenewLogUrls(\"some of required attributes are missing in app's event log.\",\n+            allPatternsExceptFileName, allAttributeKeys)\n+          return oldLogUrlMap\n+        } else if (allPatterns.contains(\"FILE_NAME\") && !allAttributeKeys.contains(\"LOG_FILES\")) {\n+          logFailToRenewLogUrls(\"'FILE_NAME' parameter is provided, but file information is \" +\n+            \"missing in app's event log.\", allPatternsExceptFileName, allAttributeKeys)\n+          return oldLogUrlMap\n+        }\n+\n+        var replacingUrl = logUrlPattern\n+\n+        allPatternsExceptFileName.foreach { pattern =>\n+          // we already checked the existence of attribute when comparing keys\n+          replacingUrl = replacingUrl.replace(s\"{{$pattern}}\", attributes(pattern))\n+        }"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Yeah nice suggestion. Still need to familiarize with functional programming. :) Will address.",
    "commit": "2d0480239a4eea655b95a521ac71ed44482acde7",
    "createdAt": "2019-01-17T22:00:54Z",
    "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.config.History._\n+import org.apache.spark.scheduler.SparkListenerExecutorAdded\n+import org.apache.spark.scheduler.cluster.ExecutorInfo\n+import org.apache.spark.status.{AppStatusListener, AppStatusSource, ElementTrackingStore}\n+\n+private[spark] class HistoryAppStatusListener(\n+    kvstore: ElementTrackingStore,\n+    conf: SparkConf,\n+    live: Boolean,\n+    appStatusSource: Option[AppStatusSource] = None,\n+    lastUpdateTime: Option[Long] = None)\n+  extends AppStatusListener(kvstore, conf, live, appStatusSource, lastUpdateTime) {\n+\n+  override def onExecutorAdded(event: SparkListenerExecutorAdded): Unit = {\n+    val execInfo = event.executorInfo\n+    val newExecInfo = new ExecutorInfo(execInfo.executorHost, execInfo.totalCores,\n+      renewLogUrls(execInfo), execInfo.attributes)\n+\n+    super.onExecutorAdded(event.copy(executorInfo = newExecInfo))\n+  }\n+\n+  def renewLogUrls(execInfo: ExecutorInfo): Map[String, String] = {\n+    val oldLogUrlMap = execInfo.logUrlMap\n+    val attributes = execInfo.attributes\n+\n+    conf.get(CUSTOM_EXECUTOR_LOG_URL) match {\n+      case Some(logUrlPattern) =>\n+        val pattern = \"\\\\{\\\\{([A-Za-z0-9_\\\\-]+)\\\\}\\\\}\".r\n+\n+        val allPatterns = pattern.findAllMatchIn(logUrlPattern).map(_.group(1)).toSet\n+        val allPatternsExceptFileName = allPatterns.filter(_ != \"FILE_NAME\")\n+        val allAttributeKeys = attributes.keys.toSet\n+        val allAttributeKeysExceptLogFiles = allAttributeKeys.filter(_ != \"LOG_FILES\")\n+\n+        if (allPatternsExceptFileName.diff(allAttributeKeysExceptLogFiles).nonEmpty) {\n+          logFailToRenewLogUrls(\"some of required attributes are missing in app's event log.\",\n+            allPatternsExceptFileName, allAttributeKeys)\n+          return oldLogUrlMap\n+        } else if (allPatterns.contains(\"FILE_NAME\") && !allAttributeKeys.contains(\"LOG_FILES\")) {\n+          logFailToRenewLogUrls(\"'FILE_NAME' parameter is provided, but file information is \" +\n+            \"missing in app's event log.\", allPatternsExceptFileName, allAttributeKeys)\n+          return oldLogUrlMap\n+        }\n+\n+        var replacingUrl = logUrlPattern\n+\n+        allPatternsExceptFileName.foreach { pattern =>\n+          // we already checked the existence of attribute when comparing keys\n+          replacingUrl = replacingUrl.replace(s\"{{$pattern}}\", attributes(pattern))\n+        }"
  }],
  "prId": 23260
}]