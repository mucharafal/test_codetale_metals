[{
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Left FIXME for addressing scaladoc.",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-06T21:47:54Z",
    "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  // FIXME: javadoc - caller should provide event log files (either compacted or original)"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Just curious why `either compacted`?",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-11T09:27:10Z",
    "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  // FIXME: javadoc - caller should provide event log files (either compacted or original)"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "No that's missed point. Let me address the doc entirely. I'm now addressing UTs so it will be addressed later than that.",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-13T01:07:15Z",
    "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  // FIXME: javadoc - caller should provide event log files (either compacted or original)"
  }],
  "prId": 26416
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "At first I implemented this as an event listener, and realized the serialized JSON is no longer be same. Bigger size, and some information could even change (e.g. Spark version). So I changed the implementation to write its own string if it passes the filters.",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-06T21:51:52Z",
    "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  // FIXME: javadoc - caller should provide event log files (either compacted or original)\n+  //  sequentially if the last event log file is already a compacted file, everything\n+  //  will be skipped\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.isEmpty) {\n+      return Seq.empty[FileStatus]\n+    }\n+\n+    // skip everything if the last file is already a compacted file\n+    if (EventLogFileWriter.isCompacted(eventLogFiles.last.getPath)) {\n+      return Seq(eventLogFiles.last)\n+    }\n+\n+    val (filesToCompact, filesToRetain) = findFilesToCompact(eventLogFiles)\n+    if (filesToCompact.isEmpty) {\n+      filesToRetain\n+    } else {\n+      // first pass\n+      val bus = new ReplayListenerBus()\n+\n+      val builders = ServiceLoader.load(classOf[EventFilterBuilder],\n+        Utils.getContextOrSparkClassLoader).asScala\n+      builders.foreach(bus.addListener)\n+\n+      filesToCompact.foreach { log =>\n+        Utils.tryWithResource(EventLogFileReader.openEventLog(log.getPath, fs)) { in =>\n+          bus.replay(in, log.getPath.getName)\n+        }\n+      }\n+\n+      // second pass\n+      val rewriter = new FilteredEventLogFileRewriter(sparkConf, hadoopConf,\n+        filesToCompact, fs, builders.map(_.createFilter()).toSeq)\n+      rewriter.start()\n+      rewriter.rewrite()\n+      rewriter.stop()\n+\n+      // cleanup files which are replaced with new compacted file.\n+      cleanupCompactedFiles(filesToCompact)\n+\n+      fs.getFileStatus(new Path(rewriter.logPath)) :: filesToRetain.toList\n+    }\n+  }\n+\n+  private def cleanupCompactedFiles(files: Seq[FileStatus]): Unit = {\n+    files.foreach { file =>\n+      try {\n+        fs.delete(file.getPath, true)\n+      } catch {\n+        case _: IOException => logWarning(s\"Failed to remove ${file.getPath} / skip removing.\")\n+      }\n+    }\n+  }\n+\n+  private def findFilesToCompact(\n+      eventLogFiles: Seq[FileStatus]): (Seq[FileStatus], Seq[FileStatus]) = {\n+    val lastCompactedFileIdx = eventLogFiles.lastIndexWhere { fs =>\n+      EventLogFileWriter.isCompacted(fs.getPath)\n+    }\n+\n+    val files = if (lastCompactedFileIdx > -1) {\n+      eventLogFiles.drop(lastCompactedFileIdx)\n+    } else {\n+      eventLogFiles\n+    }\n+\n+    if (files.length > maxFilesToRetain) {\n+      (files.dropRight(maxFilesToRetain), files.takeRight(maxFilesToRetain))\n+    } else {\n+      (Seq.empty, files)\n+    }\n+  }\n+}\n+\n+class FilteredEventLogFileRewriter(",
    "line": 113
  }],
  "prId": 26416
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "It will still write the line if the line is JSON but denotes to unknown listener event or some properties are unknown. Even though the original event file may not be readable from SHS, it may not strictly need to make it fail here.",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-06T21:56:41Z",
    "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  // FIXME: javadoc - caller should provide event log files (either compacted or original)\n+  //  sequentially if the last event log file is already a compacted file, everything\n+  //  will be skipped\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.isEmpty) {\n+      return Seq.empty[FileStatus]\n+    }\n+\n+    // skip everything if the last file is already a compacted file\n+    if (EventLogFileWriter.isCompacted(eventLogFiles.last.getPath)) {\n+      return Seq(eventLogFiles.last)\n+    }\n+\n+    val (filesToCompact, filesToRetain) = findFilesToCompact(eventLogFiles)\n+    if (filesToCompact.isEmpty) {\n+      filesToRetain\n+    } else {\n+      // first pass\n+      val bus = new ReplayListenerBus()\n+\n+      val builders = ServiceLoader.load(classOf[EventFilterBuilder],\n+        Utils.getContextOrSparkClassLoader).asScala\n+      builders.foreach(bus.addListener)\n+\n+      filesToCompact.foreach { log =>\n+        Utils.tryWithResource(EventLogFileReader.openEventLog(log.getPath, fs)) { in =>\n+          bus.replay(in, log.getPath.getName)\n+        }\n+      }\n+\n+      // second pass\n+      val rewriter = new FilteredEventLogFileRewriter(sparkConf, hadoopConf,\n+        filesToCompact, fs, builders.map(_.createFilter()).toSeq)\n+      rewriter.start()\n+      rewriter.rewrite()\n+      rewriter.stop()\n+\n+      // cleanup files which are replaced with new compacted file.\n+      cleanupCompactedFiles(filesToCompact)\n+\n+      fs.getFileStatus(new Path(rewriter.logPath)) :: filesToRetain.toList\n+    }\n+  }\n+\n+  private def cleanupCompactedFiles(files: Seq[FileStatus]): Unit = {\n+    files.foreach { file =>\n+      try {\n+        fs.delete(file.getPath, true)\n+      } catch {\n+        case _: IOException => logWarning(s\"Failed to remove ${file.getPath} / skip removing.\")\n+      }\n+    }\n+  }\n+\n+  private def findFilesToCompact(\n+      eventLogFiles: Seq[FileStatus]): (Seq[FileStatus], Seq[FileStatus]) = {\n+    val lastCompactedFileIdx = eventLogFiles.lastIndexWhere { fs =>\n+      EventLogFileWriter.isCompacted(fs.getPath)\n+    }\n+\n+    val files = if (lastCompactedFileIdx > -1) {\n+      eventLogFiles.drop(lastCompactedFileIdx)\n+    } else {\n+      eventLogFiles\n+    }\n+\n+    if (files.length > maxFilesToRetain) {\n+      (files.dropRight(maxFilesToRetain), files.takeRight(maxFilesToRetain))\n+    } else {\n+      (Seq.empty, files)\n+    }\n+  }\n+}\n+\n+class FilteredEventLogFileRewriter(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    eventLogFiles: Seq[FileStatus],\n+    fs: FileSystem,\n+    filters: Seq[EventFilter]) extends Logging {\n+\n+  require(eventLogFiles.nonEmpty)\n+\n+  private val targetEventLogFilePath = eventLogFiles.last.getPath\n+  private val logWriter: CompactedEventLogFileWriter = new CompactedEventLogFileWriter(\n+    targetEventLogFilePath, \"dummy\", None, targetEventLogFilePath.getParent.toUri,\n+    sparkConf, hadoopConf)\n+\n+  def logPath: String = logWriter.logPath\n+\n+  def start(): Unit = {\n+    logWriter.start()\n+  }\n+\n+  def stop(): Unit = {\n+    logWriter.stop()\n+  }\n+\n+  def rewrite(): Unit = {\n+    eventLogFiles.foreach(rewriteFile)\n+  }\n+\n+  private def rewriteFile(fileStatus: FileStatus): Unit = {\n+    Utils.tryWithResource(EventLogFileReader.openEventLog(fileStatus.getPath, fs)) { in =>\n+      val lines = Source.fromInputStream(in)(Codec.UTF8).getLines()\n+\n+      var currentLine: String = null\n+      var lineNumber: Int = 0\n+\n+      try {\n+        val lineEntries = lines.zipWithIndex\n+\n+        while (lineEntries.hasNext) {\n+          try {\n+            val entry = lineEntries.next()\n+\n+            currentLine = entry._1\n+            lineNumber = entry._2 + 1\n+\n+            val event = JsonProtocol.sparkEventFromJson(parse(currentLine))\n+            if (checkFilters(event)) {\n+              logWriter.writeLine(currentLine)\n+            }\n+          } catch {\n+            // ignore any exception occurred from unidentified json"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Why not reducing this catch scope to `sparkEventFromJson` and then any kind of `NonFatal` can be caught?",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-14T16:36:29Z",
    "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  // FIXME: javadoc - caller should provide event log files (either compacted or original)\n+  //  sequentially if the last event log file is already a compacted file, everything\n+  //  will be skipped\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.isEmpty) {\n+      return Seq.empty[FileStatus]\n+    }\n+\n+    // skip everything if the last file is already a compacted file\n+    if (EventLogFileWriter.isCompacted(eventLogFiles.last.getPath)) {\n+      return Seq(eventLogFiles.last)\n+    }\n+\n+    val (filesToCompact, filesToRetain) = findFilesToCompact(eventLogFiles)\n+    if (filesToCompact.isEmpty) {\n+      filesToRetain\n+    } else {\n+      // first pass\n+      val bus = new ReplayListenerBus()\n+\n+      val builders = ServiceLoader.load(classOf[EventFilterBuilder],\n+        Utils.getContextOrSparkClassLoader).asScala\n+      builders.foreach(bus.addListener)\n+\n+      filesToCompact.foreach { log =>\n+        Utils.tryWithResource(EventLogFileReader.openEventLog(log.getPath, fs)) { in =>\n+          bus.replay(in, log.getPath.getName)\n+        }\n+      }\n+\n+      // second pass\n+      val rewriter = new FilteredEventLogFileRewriter(sparkConf, hadoopConf,\n+        filesToCompact, fs, builders.map(_.createFilter()).toSeq)\n+      rewriter.start()\n+      rewriter.rewrite()\n+      rewriter.stop()\n+\n+      // cleanup files which are replaced with new compacted file.\n+      cleanupCompactedFiles(filesToCompact)\n+\n+      fs.getFileStatus(new Path(rewriter.logPath)) :: filesToRetain.toList\n+    }\n+  }\n+\n+  private def cleanupCompactedFiles(files: Seq[FileStatus]): Unit = {\n+    files.foreach { file =>\n+      try {\n+        fs.delete(file.getPath, true)\n+      } catch {\n+        case _: IOException => logWarning(s\"Failed to remove ${file.getPath} / skip removing.\")\n+      }\n+    }\n+  }\n+\n+  private def findFilesToCompact(\n+      eventLogFiles: Seq[FileStatus]): (Seq[FileStatus], Seq[FileStatus]) = {\n+    val lastCompactedFileIdx = eventLogFiles.lastIndexWhere { fs =>\n+      EventLogFileWriter.isCompacted(fs.getPath)\n+    }\n+\n+    val files = if (lastCompactedFileIdx > -1) {\n+      eventLogFiles.drop(lastCompactedFileIdx)\n+    } else {\n+      eventLogFiles\n+    }\n+\n+    if (files.length > maxFilesToRetain) {\n+      (files.dropRight(maxFilesToRetain), files.takeRight(maxFilesToRetain))\n+    } else {\n+      (Seq.empty, files)\n+    }\n+  }\n+}\n+\n+class FilteredEventLogFileRewriter(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    eventLogFiles: Seq[FileStatus],\n+    fs: FileSystem,\n+    filters: Seq[EventFilter]) extends Logging {\n+\n+  require(eventLogFiles.nonEmpty)\n+\n+  private val targetEventLogFilePath = eventLogFiles.last.getPath\n+  private val logWriter: CompactedEventLogFileWriter = new CompactedEventLogFileWriter(\n+    targetEventLogFilePath, \"dummy\", None, targetEventLogFilePath.getParent.toUri,\n+    sparkConf, hadoopConf)\n+\n+  def logPath: String = logWriter.logPath\n+\n+  def start(): Unit = {\n+    logWriter.start()\n+  }\n+\n+  def stop(): Unit = {\n+    logWriter.stop()\n+  }\n+\n+  def rewrite(): Unit = {\n+    eventLogFiles.foreach(rewriteFile)\n+  }\n+\n+  private def rewriteFile(fileStatus: FileStatus): Unit = {\n+    Utils.tryWithResource(EventLogFileReader.openEventLog(fileStatus.getPath, fs)) { in =>\n+      val lines = Source.fromInputStream(in)(Codec.UTF8).getLines()\n+\n+      var currentLine: String = null\n+      var lineNumber: Int = 0\n+\n+      try {\n+        val lineEntries = lines.zipWithIndex\n+\n+        while (lineEntries.hasNext) {\n+          try {\n+            val entry = lineEntries.next()\n+\n+            currentLine = entry._1\n+            lineNumber = entry._2 + 1\n+\n+            val event = JsonProtocol.sparkEventFromJson(parse(currentLine))\n+            if (checkFilters(event)) {\n+              logWriter.writeLine(currentLine)\n+            }\n+          } catch {\n+            // ignore any exception occurred from unidentified json"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Yeah that sounds clearer. Will address.",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-15T03:22:36Z",
    "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  // FIXME: javadoc - caller should provide event log files (either compacted or original)\n+  //  sequentially if the last event log file is already a compacted file, everything\n+  //  will be skipped\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.isEmpty) {\n+      return Seq.empty[FileStatus]\n+    }\n+\n+    // skip everything if the last file is already a compacted file\n+    if (EventLogFileWriter.isCompacted(eventLogFiles.last.getPath)) {\n+      return Seq(eventLogFiles.last)\n+    }\n+\n+    val (filesToCompact, filesToRetain) = findFilesToCompact(eventLogFiles)\n+    if (filesToCompact.isEmpty) {\n+      filesToRetain\n+    } else {\n+      // first pass\n+      val bus = new ReplayListenerBus()\n+\n+      val builders = ServiceLoader.load(classOf[EventFilterBuilder],\n+        Utils.getContextOrSparkClassLoader).asScala\n+      builders.foreach(bus.addListener)\n+\n+      filesToCompact.foreach { log =>\n+        Utils.tryWithResource(EventLogFileReader.openEventLog(log.getPath, fs)) { in =>\n+          bus.replay(in, log.getPath.getName)\n+        }\n+      }\n+\n+      // second pass\n+      val rewriter = new FilteredEventLogFileRewriter(sparkConf, hadoopConf,\n+        filesToCompact, fs, builders.map(_.createFilter()).toSeq)\n+      rewriter.start()\n+      rewriter.rewrite()\n+      rewriter.stop()\n+\n+      // cleanup files which are replaced with new compacted file.\n+      cleanupCompactedFiles(filesToCompact)\n+\n+      fs.getFileStatus(new Path(rewriter.logPath)) :: filesToRetain.toList\n+    }\n+  }\n+\n+  private def cleanupCompactedFiles(files: Seq[FileStatus]): Unit = {\n+    files.foreach { file =>\n+      try {\n+        fs.delete(file.getPath, true)\n+      } catch {\n+        case _: IOException => logWarning(s\"Failed to remove ${file.getPath} / skip removing.\")\n+      }\n+    }\n+  }\n+\n+  private def findFilesToCompact(\n+      eventLogFiles: Seq[FileStatus]): (Seq[FileStatus], Seq[FileStatus]) = {\n+    val lastCompactedFileIdx = eventLogFiles.lastIndexWhere { fs =>\n+      EventLogFileWriter.isCompacted(fs.getPath)\n+    }\n+\n+    val files = if (lastCompactedFileIdx > -1) {\n+      eventLogFiles.drop(lastCompactedFileIdx)\n+    } else {\n+      eventLogFiles\n+    }\n+\n+    if (files.length > maxFilesToRetain) {\n+      (files.dropRight(maxFilesToRetain), files.takeRight(maxFilesToRetain))\n+    } else {\n+      (Seq.empty, files)\n+    }\n+  }\n+}\n+\n+class FilteredEventLogFileRewriter(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    eventLogFiles: Seq[FileStatus],\n+    fs: FileSystem,\n+    filters: Seq[EventFilter]) extends Logging {\n+\n+  require(eventLogFiles.nonEmpty)\n+\n+  private val targetEventLogFilePath = eventLogFiles.last.getPath\n+  private val logWriter: CompactedEventLogFileWriter = new CompactedEventLogFileWriter(\n+    targetEventLogFilePath, \"dummy\", None, targetEventLogFilePath.getParent.toUri,\n+    sparkConf, hadoopConf)\n+\n+  def logPath: String = logWriter.logPath\n+\n+  def start(): Unit = {\n+    logWriter.start()\n+  }\n+\n+  def stop(): Unit = {\n+    logWriter.stop()\n+  }\n+\n+  def rewrite(): Unit = {\n+    eventLogFiles.foreach(rewriteFile)\n+  }\n+\n+  private def rewriteFile(fileStatus: FileStatus): Unit = {\n+    Utils.tryWithResource(EventLogFileReader.openEventLog(fileStatus.getPath, fs)) { in =>\n+      val lines = Source.fromInputStream(in)(Codec.UTF8).getLines()\n+\n+      var currentLine: String = null\n+      var lineNumber: Int = 0\n+\n+      try {\n+        val lineEntries = lines.zipWithIndex\n+\n+        while (lineEntries.hasNext) {\n+          try {\n+            val entry = lineEntries.next()\n+\n+            currentLine = entry._1\n+            lineNumber = entry._2 + 1\n+\n+            val event = JsonProtocol.sparkEventFromJson(parse(currentLine))\n+            if (checkFilters(event)) {\n+              logWriter.writeLine(currentLine)\n+            }\n+          } catch {\n+            // ignore any exception occurred from unidentified json"
  }],
  "prId": 26416
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Here we apply ServiceLoader since compactor is in `spark-core` and we have to have a filter implementation for `spark-sql` as well.",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-06T22:03:46Z",
    "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  // FIXME: javadoc - caller should provide event log files (either compacted or original)\n+  //  sequentially if the last event log file is already a compacted file, everything\n+  //  will be skipped\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.isEmpty) {\n+      return Seq.empty[FileStatus]\n+    }\n+\n+    // skip everything if the last file is already a compacted file\n+    if (EventLogFileWriter.isCompacted(eventLogFiles.last.getPath)) {\n+      return Seq(eventLogFiles.last)\n+    }\n+\n+    val (filesToCompact, filesToRetain) = findFilesToCompact(eventLogFiles)\n+    if (filesToCompact.isEmpty) {\n+      filesToRetain\n+    } else {\n+      // first pass\n+      val bus = new ReplayListenerBus()\n+\n+      val builders = ServiceLoader.load(classOf[EventFilterBuilder],"
  }],
  "prId": 26416
}, {
  "comments": [{
    "author": {
      "login": "Ngone51"
    },
    "body": "Just use `eventLogFiles.drop(lastCompactedFileIdx)` should be ok, as `drop` can handle negative number correctly.",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-08T12:16:19Z",
    "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  // FIXME: javadoc - caller should provide event log files (either compacted or original)\n+  //  sequentially if the last event log file is already a compacted file, everything\n+  //  will be skipped\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.isEmpty) {\n+      return Seq.empty[FileStatus]\n+    }\n+\n+    // skip everything if the last file is already a compacted file\n+    if (EventLogFileWriter.isCompacted(eventLogFiles.last.getPath)) {\n+      return Seq(eventLogFiles.last)\n+    }\n+\n+    val (filesToCompact, filesToRetain) = findFilesToCompact(eventLogFiles)\n+    if (filesToCompact.isEmpty) {\n+      filesToRetain\n+    } else {\n+      // first pass\n+      val bus = new ReplayListenerBus()\n+\n+      val builders = ServiceLoader.load(classOf[EventFilterBuilder],\n+        Utils.getContextOrSparkClassLoader).asScala\n+      builders.foreach(bus.addListener)\n+\n+      filesToCompact.foreach { log =>\n+        Utils.tryWithResource(EventLogFileReader.openEventLog(log.getPath, fs)) { in =>\n+          bus.replay(in, log.getPath.getName)\n+        }\n+      }\n+\n+      // second pass\n+      val rewriter = new FilteredEventLogFileRewriter(sparkConf, hadoopConf,\n+        filesToCompact, fs, builders.map(_.createFilter()).toSeq)\n+      rewriter.start()\n+      rewriter.rewrite()\n+      rewriter.stop()\n+\n+      // cleanup files which are replaced with new compacted file.\n+      cleanupCompactedFiles(filesToCompact)\n+\n+      fs.getFileStatus(new Path(rewriter.logPath)) :: filesToRetain.toList\n+    }\n+  }\n+\n+  private def cleanupCompactedFiles(files: Seq[FileStatus]): Unit = {\n+    files.foreach { file =>\n+      try {\n+        fs.delete(file.getPath, true)\n+      } catch {\n+        case _: IOException => logWarning(s\"Failed to remove ${file.getPath} / skip removing.\")\n+      }\n+    }\n+  }\n+\n+  private def findFilesToCompact(\n+      eventLogFiles: Seq[FileStatus]): (Seq[FileStatus], Seq[FileStatus]) = {\n+    val lastCompactedFileIdx = eventLogFiles.lastIndexWhere { fs =>\n+      EventLogFileWriter.isCompacted(fs.getPath)\n+    }\n+\n+    val files = if (lastCompactedFileIdx > -1) {\n+      eventLogFiles.drop(lastCompactedFileIdx)\n+    } else {\n+      eventLogFiles\n+    }"
  }],
  "prId": 26416
}, {
  "comments": [{
    "author": {
      "login": "Ngone51"
    },
    "body": "Why needs to override ?",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-08T12:42:44Z",
    "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  // FIXME: javadoc - caller should provide event log files (either compacted or original)\n+  //  sequentially if the last event log file is already a compacted file, everything\n+  //  will be skipped\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.isEmpty) {\n+      return Seq.empty[FileStatus]\n+    }\n+\n+    // skip everything if the last file is already a compacted file\n+    if (EventLogFileWriter.isCompacted(eventLogFiles.last.getPath)) {\n+      return Seq(eventLogFiles.last)\n+    }\n+\n+    val (filesToCompact, filesToRetain) = findFilesToCompact(eventLogFiles)\n+    if (filesToCompact.isEmpty) {\n+      filesToRetain\n+    } else {\n+      // first pass\n+      val bus = new ReplayListenerBus()\n+\n+      val builders = ServiceLoader.load(classOf[EventFilterBuilder],\n+        Utils.getContextOrSparkClassLoader).asScala\n+      builders.foreach(bus.addListener)\n+\n+      filesToCompact.foreach { log =>\n+        Utils.tryWithResource(EventLogFileReader.openEventLog(log.getPath, fs)) { in =>\n+          bus.replay(in, log.getPath.getName)\n+        }\n+      }\n+\n+      // second pass\n+      val rewriter = new FilteredEventLogFileRewriter(sparkConf, hadoopConf,\n+        filesToCompact, fs, builders.map(_.createFilter()).toSeq)\n+      rewriter.start()\n+      rewriter.rewrite()\n+      rewriter.stop()\n+\n+      // cleanup files which are replaced with new compacted file.\n+      cleanupCompactedFiles(filesToCompact)\n+\n+      fs.getFileStatus(new Path(rewriter.logPath)) :: filesToRetain.toList\n+    }\n+  }\n+\n+  private def cleanupCompactedFiles(files: Seq[FileStatus]): Unit = {\n+    files.foreach { file =>\n+      try {\n+        fs.delete(file.getPath, true)\n+      } catch {\n+        case _: IOException => logWarning(s\"Failed to remove ${file.getPath} / skip removing.\")\n+      }\n+    }\n+  }\n+\n+  private def findFilesToCompact(\n+      eventLogFiles: Seq[FileStatus]): (Seq[FileStatus], Seq[FileStatus]) = {\n+    val lastCompactedFileIdx = eventLogFiles.lastIndexWhere { fs =>\n+      EventLogFileWriter.isCompacted(fs.getPath)\n+    }\n+\n+    val files = if (lastCompactedFileIdx > -1) {\n+      eventLogFiles.drop(lastCompactedFileIdx)\n+    } else {\n+      eventLogFiles\n+    }\n+\n+    if (files.length > maxFilesToRetain) {\n+      (files.dropRight(maxFilesToRetain), files.takeRight(maxFilesToRetain))\n+    } else {\n+      (Seq.empty, files)\n+    }\n+  }\n+}\n+\n+class FilteredEventLogFileRewriter(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    eventLogFiles: Seq[FileStatus],\n+    fs: FileSystem,\n+    filters: Seq[EventFilter]) extends Logging {\n+\n+  require(eventLogFiles.nonEmpty)\n+\n+  private val targetEventLogFilePath = eventLogFiles.last.getPath\n+  private val logWriter: CompactedEventLogFileWriter = new CompactedEventLogFileWriter(\n+    targetEventLogFilePath, \"dummy\", None, targetEventLogFilePath.getParent.toUri,\n+    sparkConf, hadoopConf)\n+\n+  def logPath: String = logWriter.logPath\n+\n+  def start(): Unit = {\n+    logWriter.start()\n+  }\n+\n+  def stop(): Unit = {\n+    logWriter.stop()\n+  }\n+\n+  def rewrite(): Unit = {\n+    eventLogFiles.foreach(rewriteFile)\n+  }\n+\n+  private def rewriteFile(fileStatus: FileStatus): Unit = {\n+    Utils.tryWithResource(EventLogFileReader.openEventLog(fileStatus.getPath, fs)) { in =>\n+      val lines = Source.fromInputStream(in)(Codec.UTF8).getLines()\n+\n+      var currentLine: String = null\n+      var lineNumber: Int = 0\n+\n+      try {\n+        val lineEntries = lines.zipWithIndex\n+\n+        while (lineEntries.hasNext) {\n+          try {\n+            val entry = lineEntries.next()\n+\n+            currentLine = entry._1\n+            lineNumber = entry._2 + 1\n+\n+            val event = JsonProtocol.sparkEventFromJson(parse(currentLine))\n+            if (checkFilters(event)) {\n+              logWriter.writeLine(currentLine)\n+            }\n+          } catch {\n+            // ignore any exception occurred from unidentified json\n+            // just skip handling and write the line\n+            case _: ClassNotFoundException => logWriter.writeLine(currentLine)\n+            case _: UnrecognizedPropertyException => logWriter.writeLine(currentLine)\n+          }\n+        }\n+        true\n+      } catch {\n+        case e: Exception =>\n+          logError(s\"Exception parsing Spark event log: ${fileStatus.getPath.getName}\", e)\n+          logError(s\"Malformed line #$lineNumber: $currentLine\\n\")\n+          throw e\n+      }\n+    }\n+  }\n+\n+  private def checkFilters(event: SparkListenerEvent): Boolean = {\n+    val results = filters.flatMap(filter => applyFilter(filter, event))\n+    results.isEmpty || results.forall(_ == true)\n+  }\n+\n+  private def applyFilter(filter: EventFilter, event: SparkListenerEvent): Option[Boolean] = {\n+    event match {\n+      case event: SparkListenerStageSubmitted => filter.filterStageSubmitted(event)\n+      case event: SparkListenerStageCompleted => filter.filterStageCompleted(event)\n+      case event: SparkListenerJobStart => filter.filterJobStart(event)\n+      case event: SparkListenerJobEnd => filter.filterJobEnd(event)\n+      case event: SparkListenerTaskStart => filter.filterTaskStart(event)\n+      case event: SparkListenerTaskGettingResult => filter.filterTaskGettingResult(event)\n+      case event: SparkListenerTaskEnd => filter.filterTaskEnd(event)\n+      case event: SparkListenerEnvironmentUpdate => filter.filterEnvironmentUpdate(event)\n+      case event: SparkListenerBlockManagerAdded => filter.filterBlockManagerAdded(event)\n+      case event: SparkListenerBlockManagerRemoved => filter.filterBlockManagerRemoved(event)\n+      case event: SparkListenerUnpersistRDD => filter.filterUnpersistRDD(event)\n+      case event: SparkListenerApplicationStart => filter.filterApplicationStart(event)\n+      case event: SparkListenerApplicationEnd => filter.filterApplicationEnd(event)\n+      case event: SparkListenerExecutorMetricsUpdate => filter.filterExecutorMetricsUpdate(event)\n+      case event: SparkListenerStageExecutorMetrics => filter.filterStageExecutorMetrics(event)\n+      case event: SparkListenerExecutorAdded => filter.filterExecutorAdded(event)\n+      case event: SparkListenerExecutorRemoved => filter.filterExecutorRemoved(event)\n+      case event: SparkListenerExecutorBlacklistedForStage =>\n+        filter.filterExecutorBlacklistedForStage(event)\n+      case event: SparkListenerNodeBlacklistedForStage =>\n+        filter.filterNodeBlacklistedForStage(event)\n+      case event: SparkListenerExecutorBlacklisted => filter.filterExecutorBlacklisted(event)\n+      case event: SparkListenerExecutorUnblacklisted => filter.filterExecutorUnblacklisted(event)\n+      case event: SparkListenerNodeBlacklisted => filter.filterNodeBlacklisted(event)\n+      case event: SparkListenerNodeUnblacklisted => filter.filterNodeUnblacklisted(event)\n+      case event: SparkListenerBlockUpdated => filter.filterBlockUpdated(event)\n+      case event: SparkListenerSpeculativeTaskSubmitted =>\n+        filter.filterSpeculativeTaskSubmitted(event)\n+      case _ => filter.filterOtherEvent(event)\n+    }\n+  }\n+}\n+\n+class CompactedEventLogFileWriter(\n+    originalFilePath: Path,\n+    appId: String,\n+    appAttemptId: Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  override val logPath: String = originalFilePath.toUri.toString + EventLogFileWriter.COMPACTED\n+\n+  override def writeLine(line: String, flushLogger: Boolean): Unit = {\n+    super.writeLine(line, flushLogger)\n+  }"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Ah that seems to be missed after refactoring. Will remove.",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-10T04:46:45Z",
    "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  // FIXME: javadoc - caller should provide event log files (either compacted or original)\n+  //  sequentially if the last event log file is already a compacted file, everything\n+  //  will be skipped\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.isEmpty) {\n+      return Seq.empty[FileStatus]\n+    }\n+\n+    // skip everything if the last file is already a compacted file\n+    if (EventLogFileWriter.isCompacted(eventLogFiles.last.getPath)) {\n+      return Seq(eventLogFiles.last)\n+    }\n+\n+    val (filesToCompact, filesToRetain) = findFilesToCompact(eventLogFiles)\n+    if (filesToCompact.isEmpty) {\n+      filesToRetain\n+    } else {\n+      // first pass\n+      val bus = new ReplayListenerBus()\n+\n+      val builders = ServiceLoader.load(classOf[EventFilterBuilder],\n+        Utils.getContextOrSparkClassLoader).asScala\n+      builders.foreach(bus.addListener)\n+\n+      filesToCompact.foreach { log =>\n+        Utils.tryWithResource(EventLogFileReader.openEventLog(log.getPath, fs)) { in =>\n+          bus.replay(in, log.getPath.getName)\n+        }\n+      }\n+\n+      // second pass\n+      val rewriter = new FilteredEventLogFileRewriter(sparkConf, hadoopConf,\n+        filesToCompact, fs, builders.map(_.createFilter()).toSeq)\n+      rewriter.start()\n+      rewriter.rewrite()\n+      rewriter.stop()\n+\n+      // cleanup files which are replaced with new compacted file.\n+      cleanupCompactedFiles(filesToCompact)\n+\n+      fs.getFileStatus(new Path(rewriter.logPath)) :: filesToRetain.toList\n+    }\n+  }\n+\n+  private def cleanupCompactedFiles(files: Seq[FileStatus]): Unit = {\n+    files.foreach { file =>\n+      try {\n+        fs.delete(file.getPath, true)\n+      } catch {\n+        case _: IOException => logWarning(s\"Failed to remove ${file.getPath} / skip removing.\")\n+      }\n+    }\n+  }\n+\n+  private def findFilesToCompact(\n+      eventLogFiles: Seq[FileStatus]): (Seq[FileStatus], Seq[FileStatus]) = {\n+    val lastCompactedFileIdx = eventLogFiles.lastIndexWhere { fs =>\n+      EventLogFileWriter.isCompacted(fs.getPath)\n+    }\n+\n+    val files = if (lastCompactedFileIdx > -1) {\n+      eventLogFiles.drop(lastCompactedFileIdx)\n+    } else {\n+      eventLogFiles\n+    }\n+\n+    if (files.length > maxFilesToRetain) {\n+      (files.dropRight(maxFilesToRetain), files.takeRight(maxFilesToRetain))\n+    } else {\n+      (Seq.empty, files)\n+    }\n+  }\n+}\n+\n+class FilteredEventLogFileRewriter(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    eventLogFiles: Seq[FileStatus],\n+    fs: FileSystem,\n+    filters: Seq[EventFilter]) extends Logging {\n+\n+  require(eventLogFiles.nonEmpty)\n+\n+  private val targetEventLogFilePath = eventLogFiles.last.getPath\n+  private val logWriter: CompactedEventLogFileWriter = new CompactedEventLogFileWriter(\n+    targetEventLogFilePath, \"dummy\", None, targetEventLogFilePath.getParent.toUri,\n+    sparkConf, hadoopConf)\n+\n+  def logPath: String = logWriter.logPath\n+\n+  def start(): Unit = {\n+    logWriter.start()\n+  }\n+\n+  def stop(): Unit = {\n+    logWriter.stop()\n+  }\n+\n+  def rewrite(): Unit = {\n+    eventLogFiles.foreach(rewriteFile)\n+  }\n+\n+  private def rewriteFile(fileStatus: FileStatus): Unit = {\n+    Utils.tryWithResource(EventLogFileReader.openEventLog(fileStatus.getPath, fs)) { in =>\n+      val lines = Source.fromInputStream(in)(Codec.UTF8).getLines()\n+\n+      var currentLine: String = null\n+      var lineNumber: Int = 0\n+\n+      try {\n+        val lineEntries = lines.zipWithIndex\n+\n+        while (lineEntries.hasNext) {\n+          try {\n+            val entry = lineEntries.next()\n+\n+            currentLine = entry._1\n+            lineNumber = entry._2 + 1\n+\n+            val event = JsonProtocol.sparkEventFromJson(parse(currentLine))\n+            if (checkFilters(event)) {\n+              logWriter.writeLine(currentLine)\n+            }\n+          } catch {\n+            // ignore any exception occurred from unidentified json\n+            // just skip handling and write the line\n+            case _: ClassNotFoundException => logWriter.writeLine(currentLine)\n+            case _: UnrecognizedPropertyException => logWriter.writeLine(currentLine)\n+          }\n+        }\n+        true\n+      } catch {\n+        case e: Exception =>\n+          logError(s\"Exception parsing Spark event log: ${fileStatus.getPath.getName}\", e)\n+          logError(s\"Malformed line #$lineNumber: $currentLine\\n\")\n+          throw e\n+      }\n+    }\n+  }\n+\n+  private def checkFilters(event: SparkListenerEvent): Boolean = {\n+    val results = filters.flatMap(filter => applyFilter(filter, event))\n+    results.isEmpty || results.forall(_ == true)\n+  }\n+\n+  private def applyFilter(filter: EventFilter, event: SparkListenerEvent): Option[Boolean] = {\n+    event match {\n+      case event: SparkListenerStageSubmitted => filter.filterStageSubmitted(event)\n+      case event: SparkListenerStageCompleted => filter.filterStageCompleted(event)\n+      case event: SparkListenerJobStart => filter.filterJobStart(event)\n+      case event: SparkListenerJobEnd => filter.filterJobEnd(event)\n+      case event: SparkListenerTaskStart => filter.filterTaskStart(event)\n+      case event: SparkListenerTaskGettingResult => filter.filterTaskGettingResult(event)\n+      case event: SparkListenerTaskEnd => filter.filterTaskEnd(event)\n+      case event: SparkListenerEnvironmentUpdate => filter.filterEnvironmentUpdate(event)\n+      case event: SparkListenerBlockManagerAdded => filter.filterBlockManagerAdded(event)\n+      case event: SparkListenerBlockManagerRemoved => filter.filterBlockManagerRemoved(event)\n+      case event: SparkListenerUnpersistRDD => filter.filterUnpersistRDD(event)\n+      case event: SparkListenerApplicationStart => filter.filterApplicationStart(event)\n+      case event: SparkListenerApplicationEnd => filter.filterApplicationEnd(event)\n+      case event: SparkListenerExecutorMetricsUpdate => filter.filterExecutorMetricsUpdate(event)\n+      case event: SparkListenerStageExecutorMetrics => filter.filterStageExecutorMetrics(event)\n+      case event: SparkListenerExecutorAdded => filter.filterExecutorAdded(event)\n+      case event: SparkListenerExecutorRemoved => filter.filterExecutorRemoved(event)\n+      case event: SparkListenerExecutorBlacklistedForStage =>\n+        filter.filterExecutorBlacklistedForStage(event)\n+      case event: SparkListenerNodeBlacklistedForStage =>\n+        filter.filterNodeBlacklistedForStage(event)\n+      case event: SparkListenerExecutorBlacklisted => filter.filterExecutorBlacklisted(event)\n+      case event: SparkListenerExecutorUnblacklisted => filter.filterExecutorUnblacklisted(event)\n+      case event: SparkListenerNodeBlacklisted => filter.filterNodeBlacklisted(event)\n+      case event: SparkListenerNodeUnblacklisted => filter.filterNodeUnblacklisted(event)\n+      case event: SparkListenerBlockUpdated => filter.filterBlockUpdated(event)\n+      case event: SparkListenerSpeculativeTaskSubmitted =>\n+        filter.filterSpeculativeTaskSubmitted(event)\n+      case _ => filter.filterOtherEvent(event)\n+    }\n+  }\n+}\n+\n+class CompactedEventLogFileWriter(\n+    originalFilePath: Path,\n+    appId: String,\n+    appAttemptId: Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  override val logPath: String = originalFilePath.toUri.toString + EventLogFileWriter.COMPACTED\n+\n+  override def writeLine(line: String, flushLogger: Boolean): Unit = {\n+    super.writeLine(line, flushLogger)\n+  }"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "My bad, in base class it's defined as `protected` which is correct as we don't want to allow event writer to write a line instead of json (though we know it's just a line). This class is a variant of writer which should write a line \"as it is\", instead of rebuilding json and writing it - hence I decide to not touch a base class and only expose the method for this class.",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-10T05:31:46Z",
    "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  // FIXME: javadoc - caller should provide event log files (either compacted or original)\n+  //  sequentially if the last event log file is already a compacted file, everything\n+  //  will be skipped\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.isEmpty) {\n+      return Seq.empty[FileStatus]\n+    }\n+\n+    // skip everything if the last file is already a compacted file\n+    if (EventLogFileWriter.isCompacted(eventLogFiles.last.getPath)) {\n+      return Seq(eventLogFiles.last)\n+    }\n+\n+    val (filesToCompact, filesToRetain) = findFilesToCompact(eventLogFiles)\n+    if (filesToCompact.isEmpty) {\n+      filesToRetain\n+    } else {\n+      // first pass\n+      val bus = new ReplayListenerBus()\n+\n+      val builders = ServiceLoader.load(classOf[EventFilterBuilder],\n+        Utils.getContextOrSparkClassLoader).asScala\n+      builders.foreach(bus.addListener)\n+\n+      filesToCompact.foreach { log =>\n+        Utils.tryWithResource(EventLogFileReader.openEventLog(log.getPath, fs)) { in =>\n+          bus.replay(in, log.getPath.getName)\n+        }\n+      }\n+\n+      // second pass\n+      val rewriter = new FilteredEventLogFileRewriter(sparkConf, hadoopConf,\n+        filesToCompact, fs, builders.map(_.createFilter()).toSeq)\n+      rewriter.start()\n+      rewriter.rewrite()\n+      rewriter.stop()\n+\n+      // cleanup files which are replaced with new compacted file.\n+      cleanupCompactedFiles(filesToCompact)\n+\n+      fs.getFileStatus(new Path(rewriter.logPath)) :: filesToRetain.toList\n+    }\n+  }\n+\n+  private def cleanupCompactedFiles(files: Seq[FileStatus]): Unit = {\n+    files.foreach { file =>\n+      try {\n+        fs.delete(file.getPath, true)\n+      } catch {\n+        case _: IOException => logWarning(s\"Failed to remove ${file.getPath} / skip removing.\")\n+      }\n+    }\n+  }\n+\n+  private def findFilesToCompact(\n+      eventLogFiles: Seq[FileStatus]): (Seq[FileStatus], Seq[FileStatus]) = {\n+    val lastCompactedFileIdx = eventLogFiles.lastIndexWhere { fs =>\n+      EventLogFileWriter.isCompacted(fs.getPath)\n+    }\n+\n+    val files = if (lastCompactedFileIdx > -1) {\n+      eventLogFiles.drop(lastCompactedFileIdx)\n+    } else {\n+      eventLogFiles\n+    }\n+\n+    if (files.length > maxFilesToRetain) {\n+      (files.dropRight(maxFilesToRetain), files.takeRight(maxFilesToRetain))\n+    } else {\n+      (Seq.empty, files)\n+    }\n+  }\n+}\n+\n+class FilteredEventLogFileRewriter(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    eventLogFiles: Seq[FileStatus],\n+    fs: FileSystem,\n+    filters: Seq[EventFilter]) extends Logging {\n+\n+  require(eventLogFiles.nonEmpty)\n+\n+  private val targetEventLogFilePath = eventLogFiles.last.getPath\n+  private val logWriter: CompactedEventLogFileWriter = new CompactedEventLogFileWriter(\n+    targetEventLogFilePath, \"dummy\", None, targetEventLogFilePath.getParent.toUri,\n+    sparkConf, hadoopConf)\n+\n+  def logPath: String = logWriter.logPath\n+\n+  def start(): Unit = {\n+    logWriter.start()\n+  }\n+\n+  def stop(): Unit = {\n+    logWriter.stop()\n+  }\n+\n+  def rewrite(): Unit = {\n+    eventLogFiles.foreach(rewriteFile)\n+  }\n+\n+  private def rewriteFile(fileStatus: FileStatus): Unit = {\n+    Utils.tryWithResource(EventLogFileReader.openEventLog(fileStatus.getPath, fs)) { in =>\n+      val lines = Source.fromInputStream(in)(Codec.UTF8).getLines()\n+\n+      var currentLine: String = null\n+      var lineNumber: Int = 0\n+\n+      try {\n+        val lineEntries = lines.zipWithIndex\n+\n+        while (lineEntries.hasNext) {\n+          try {\n+            val entry = lineEntries.next()\n+\n+            currentLine = entry._1\n+            lineNumber = entry._2 + 1\n+\n+            val event = JsonProtocol.sparkEventFromJson(parse(currentLine))\n+            if (checkFilters(event)) {\n+              logWriter.writeLine(currentLine)\n+            }\n+          } catch {\n+            // ignore any exception occurred from unidentified json\n+            // just skip handling and write the line\n+            case _: ClassNotFoundException => logWriter.writeLine(currentLine)\n+            case _: UnrecognizedPropertyException => logWriter.writeLine(currentLine)\n+          }\n+        }\n+        true\n+      } catch {\n+        case e: Exception =>\n+          logError(s\"Exception parsing Spark event log: ${fileStatus.getPath.getName}\", e)\n+          logError(s\"Malformed line #$lineNumber: $currentLine\\n\")\n+          throw e\n+      }\n+    }\n+  }\n+\n+  private def checkFilters(event: SparkListenerEvent): Boolean = {\n+    val results = filters.flatMap(filter => applyFilter(filter, event))\n+    results.isEmpty || results.forall(_ == true)\n+  }\n+\n+  private def applyFilter(filter: EventFilter, event: SparkListenerEvent): Option[Boolean] = {\n+    event match {\n+      case event: SparkListenerStageSubmitted => filter.filterStageSubmitted(event)\n+      case event: SparkListenerStageCompleted => filter.filterStageCompleted(event)\n+      case event: SparkListenerJobStart => filter.filterJobStart(event)\n+      case event: SparkListenerJobEnd => filter.filterJobEnd(event)\n+      case event: SparkListenerTaskStart => filter.filterTaskStart(event)\n+      case event: SparkListenerTaskGettingResult => filter.filterTaskGettingResult(event)\n+      case event: SparkListenerTaskEnd => filter.filterTaskEnd(event)\n+      case event: SparkListenerEnvironmentUpdate => filter.filterEnvironmentUpdate(event)\n+      case event: SparkListenerBlockManagerAdded => filter.filterBlockManagerAdded(event)\n+      case event: SparkListenerBlockManagerRemoved => filter.filterBlockManagerRemoved(event)\n+      case event: SparkListenerUnpersistRDD => filter.filterUnpersistRDD(event)\n+      case event: SparkListenerApplicationStart => filter.filterApplicationStart(event)\n+      case event: SparkListenerApplicationEnd => filter.filterApplicationEnd(event)\n+      case event: SparkListenerExecutorMetricsUpdate => filter.filterExecutorMetricsUpdate(event)\n+      case event: SparkListenerStageExecutorMetrics => filter.filterStageExecutorMetrics(event)\n+      case event: SparkListenerExecutorAdded => filter.filterExecutorAdded(event)\n+      case event: SparkListenerExecutorRemoved => filter.filterExecutorRemoved(event)\n+      case event: SparkListenerExecutorBlacklistedForStage =>\n+        filter.filterExecutorBlacklistedForStage(event)\n+      case event: SparkListenerNodeBlacklistedForStage =>\n+        filter.filterNodeBlacklistedForStage(event)\n+      case event: SparkListenerExecutorBlacklisted => filter.filterExecutorBlacklisted(event)\n+      case event: SparkListenerExecutorUnblacklisted => filter.filterExecutorUnblacklisted(event)\n+      case event: SparkListenerNodeBlacklisted => filter.filterNodeBlacklisted(event)\n+      case event: SparkListenerNodeUnblacklisted => filter.filterNodeUnblacklisted(event)\n+      case event: SparkListenerBlockUpdated => filter.filterBlockUpdated(event)\n+      case event: SparkListenerSpeculativeTaskSubmitted =>\n+        filter.filterSpeculativeTaskSubmitted(event)\n+      case _ => filter.filterOtherEvent(event)\n+    }\n+  }\n+}\n+\n+class CompactedEventLogFileWriter(\n+    originalFilePath: Path,\n+    appId: String,\n+    appAttemptId: Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  override val logPath: String = originalFilePath.toUri.toString + EventLogFileWriter.COMPACTED\n+\n+  override def writeLine(line: String, flushLogger: Boolean): Unit = {\n+    super.writeLine(line, flushLogger)\n+  }"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "It's not breaking the Liskov rule so have no strong opinion but I think it would be better to make it public instead of overriding (less lines and less questions in other coders head).",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-14T16:50:16Z",
    "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  // FIXME: javadoc - caller should provide event log files (either compacted or original)\n+  //  sequentially if the last event log file is already a compacted file, everything\n+  //  will be skipped\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.isEmpty) {\n+      return Seq.empty[FileStatus]\n+    }\n+\n+    // skip everything if the last file is already a compacted file\n+    if (EventLogFileWriter.isCompacted(eventLogFiles.last.getPath)) {\n+      return Seq(eventLogFiles.last)\n+    }\n+\n+    val (filesToCompact, filesToRetain) = findFilesToCompact(eventLogFiles)\n+    if (filesToCompact.isEmpty) {\n+      filesToRetain\n+    } else {\n+      // first pass\n+      val bus = new ReplayListenerBus()\n+\n+      val builders = ServiceLoader.load(classOf[EventFilterBuilder],\n+        Utils.getContextOrSparkClassLoader).asScala\n+      builders.foreach(bus.addListener)\n+\n+      filesToCompact.foreach { log =>\n+        Utils.tryWithResource(EventLogFileReader.openEventLog(log.getPath, fs)) { in =>\n+          bus.replay(in, log.getPath.getName)\n+        }\n+      }\n+\n+      // second pass\n+      val rewriter = new FilteredEventLogFileRewriter(sparkConf, hadoopConf,\n+        filesToCompact, fs, builders.map(_.createFilter()).toSeq)\n+      rewriter.start()\n+      rewriter.rewrite()\n+      rewriter.stop()\n+\n+      // cleanup files which are replaced with new compacted file.\n+      cleanupCompactedFiles(filesToCompact)\n+\n+      fs.getFileStatus(new Path(rewriter.logPath)) :: filesToRetain.toList\n+    }\n+  }\n+\n+  private def cleanupCompactedFiles(files: Seq[FileStatus]): Unit = {\n+    files.foreach { file =>\n+      try {\n+        fs.delete(file.getPath, true)\n+      } catch {\n+        case _: IOException => logWarning(s\"Failed to remove ${file.getPath} / skip removing.\")\n+      }\n+    }\n+  }\n+\n+  private def findFilesToCompact(\n+      eventLogFiles: Seq[FileStatus]): (Seq[FileStatus], Seq[FileStatus]) = {\n+    val lastCompactedFileIdx = eventLogFiles.lastIndexWhere { fs =>\n+      EventLogFileWriter.isCompacted(fs.getPath)\n+    }\n+\n+    val files = if (lastCompactedFileIdx > -1) {\n+      eventLogFiles.drop(lastCompactedFileIdx)\n+    } else {\n+      eventLogFiles\n+    }\n+\n+    if (files.length > maxFilesToRetain) {\n+      (files.dropRight(maxFilesToRetain), files.takeRight(maxFilesToRetain))\n+    } else {\n+      (Seq.empty, files)\n+    }\n+  }\n+}\n+\n+class FilteredEventLogFileRewriter(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    eventLogFiles: Seq[FileStatus],\n+    fs: FileSystem,\n+    filters: Seq[EventFilter]) extends Logging {\n+\n+  require(eventLogFiles.nonEmpty)\n+\n+  private val targetEventLogFilePath = eventLogFiles.last.getPath\n+  private val logWriter: CompactedEventLogFileWriter = new CompactedEventLogFileWriter(\n+    targetEventLogFilePath, \"dummy\", None, targetEventLogFilePath.getParent.toUri,\n+    sparkConf, hadoopConf)\n+\n+  def logPath: String = logWriter.logPath\n+\n+  def start(): Unit = {\n+    logWriter.start()\n+  }\n+\n+  def stop(): Unit = {\n+    logWriter.stop()\n+  }\n+\n+  def rewrite(): Unit = {\n+    eventLogFiles.foreach(rewriteFile)\n+  }\n+\n+  private def rewriteFile(fileStatus: FileStatus): Unit = {\n+    Utils.tryWithResource(EventLogFileReader.openEventLog(fileStatus.getPath, fs)) { in =>\n+      val lines = Source.fromInputStream(in)(Codec.UTF8).getLines()\n+\n+      var currentLine: String = null\n+      var lineNumber: Int = 0\n+\n+      try {\n+        val lineEntries = lines.zipWithIndex\n+\n+        while (lineEntries.hasNext) {\n+          try {\n+            val entry = lineEntries.next()\n+\n+            currentLine = entry._1\n+            lineNumber = entry._2 + 1\n+\n+            val event = JsonProtocol.sparkEventFromJson(parse(currentLine))\n+            if (checkFilters(event)) {\n+              logWriter.writeLine(currentLine)\n+            }\n+          } catch {\n+            // ignore any exception occurred from unidentified json\n+            // just skip handling and write the line\n+            case _: ClassNotFoundException => logWriter.writeLine(currentLine)\n+            case _: UnrecognizedPropertyException => logWriter.writeLine(currentLine)\n+          }\n+        }\n+        true\n+      } catch {\n+        case e: Exception =>\n+          logError(s\"Exception parsing Spark event log: ${fileStatus.getPath.getName}\", e)\n+          logError(s\"Malformed line #$lineNumber: $currentLine\\n\")\n+          throw e\n+      }\n+    }\n+  }\n+\n+  private def checkFilters(event: SparkListenerEvent): Boolean = {\n+    val results = filters.flatMap(filter => applyFilter(filter, event))\n+    results.isEmpty || results.forall(_ == true)\n+  }\n+\n+  private def applyFilter(filter: EventFilter, event: SparkListenerEvent): Option[Boolean] = {\n+    event match {\n+      case event: SparkListenerStageSubmitted => filter.filterStageSubmitted(event)\n+      case event: SparkListenerStageCompleted => filter.filterStageCompleted(event)\n+      case event: SparkListenerJobStart => filter.filterJobStart(event)\n+      case event: SparkListenerJobEnd => filter.filterJobEnd(event)\n+      case event: SparkListenerTaskStart => filter.filterTaskStart(event)\n+      case event: SparkListenerTaskGettingResult => filter.filterTaskGettingResult(event)\n+      case event: SparkListenerTaskEnd => filter.filterTaskEnd(event)\n+      case event: SparkListenerEnvironmentUpdate => filter.filterEnvironmentUpdate(event)\n+      case event: SparkListenerBlockManagerAdded => filter.filterBlockManagerAdded(event)\n+      case event: SparkListenerBlockManagerRemoved => filter.filterBlockManagerRemoved(event)\n+      case event: SparkListenerUnpersistRDD => filter.filterUnpersistRDD(event)\n+      case event: SparkListenerApplicationStart => filter.filterApplicationStart(event)\n+      case event: SparkListenerApplicationEnd => filter.filterApplicationEnd(event)\n+      case event: SparkListenerExecutorMetricsUpdate => filter.filterExecutorMetricsUpdate(event)\n+      case event: SparkListenerStageExecutorMetrics => filter.filterStageExecutorMetrics(event)\n+      case event: SparkListenerExecutorAdded => filter.filterExecutorAdded(event)\n+      case event: SparkListenerExecutorRemoved => filter.filterExecutorRemoved(event)\n+      case event: SparkListenerExecutorBlacklistedForStage =>\n+        filter.filterExecutorBlacklistedForStage(event)\n+      case event: SparkListenerNodeBlacklistedForStage =>\n+        filter.filterNodeBlacklistedForStage(event)\n+      case event: SparkListenerExecutorBlacklisted => filter.filterExecutorBlacklisted(event)\n+      case event: SparkListenerExecutorUnblacklisted => filter.filterExecutorUnblacklisted(event)\n+      case event: SparkListenerNodeBlacklisted => filter.filterNodeBlacklisted(event)\n+      case event: SparkListenerNodeUnblacklisted => filter.filterNodeUnblacklisted(event)\n+      case event: SparkListenerBlockUpdated => filter.filterBlockUpdated(event)\n+      case event: SparkListenerSpeculativeTaskSubmitted =>\n+        filter.filterSpeculativeTaskSubmitted(event)\n+      case _ => filter.filterOtherEvent(event)\n+    }\n+  }\n+}\n+\n+class CompactedEventLogFileWriter(\n+    originalFilePath: Path,\n+    appId: String,\n+    appAttemptId: Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  override val logPath: String = originalFilePath.toUri.toString + EventLogFileWriter.COMPACTED\n+\n+  override def writeLine(line: String, flushLogger: Boolean): Unit = {\n+    super.writeLine(line, flushLogger)\n+  }"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "So CompactedEventLogFileWriter is just one of exceptional case to not repeat the code in EventLogFileWriter. I didn't want to let CompactedEventLogFileWriter extend EventLogFileWriter but then lots of code will be redundant so had to have one exception.\r\n\r\nBy intention EventLogFileWriter should still hide the method. If we are not feeling convenient with this, I'll change CompactedEventLogFileWriter to not extend EventLogFileWriter, but try to share the code.",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-15T03:33:33Z",
    "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  // FIXME: javadoc - caller should provide event log files (either compacted or original)\n+  //  sequentially if the last event log file is already a compacted file, everything\n+  //  will be skipped\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.isEmpty) {\n+      return Seq.empty[FileStatus]\n+    }\n+\n+    // skip everything if the last file is already a compacted file\n+    if (EventLogFileWriter.isCompacted(eventLogFiles.last.getPath)) {\n+      return Seq(eventLogFiles.last)\n+    }\n+\n+    val (filesToCompact, filesToRetain) = findFilesToCompact(eventLogFiles)\n+    if (filesToCompact.isEmpty) {\n+      filesToRetain\n+    } else {\n+      // first pass\n+      val bus = new ReplayListenerBus()\n+\n+      val builders = ServiceLoader.load(classOf[EventFilterBuilder],\n+        Utils.getContextOrSparkClassLoader).asScala\n+      builders.foreach(bus.addListener)\n+\n+      filesToCompact.foreach { log =>\n+        Utils.tryWithResource(EventLogFileReader.openEventLog(log.getPath, fs)) { in =>\n+          bus.replay(in, log.getPath.getName)\n+        }\n+      }\n+\n+      // second pass\n+      val rewriter = new FilteredEventLogFileRewriter(sparkConf, hadoopConf,\n+        filesToCompact, fs, builders.map(_.createFilter()).toSeq)\n+      rewriter.start()\n+      rewriter.rewrite()\n+      rewriter.stop()\n+\n+      // cleanup files which are replaced with new compacted file.\n+      cleanupCompactedFiles(filesToCompact)\n+\n+      fs.getFileStatus(new Path(rewriter.logPath)) :: filesToRetain.toList\n+    }\n+  }\n+\n+  private def cleanupCompactedFiles(files: Seq[FileStatus]): Unit = {\n+    files.foreach { file =>\n+      try {\n+        fs.delete(file.getPath, true)\n+      } catch {\n+        case _: IOException => logWarning(s\"Failed to remove ${file.getPath} / skip removing.\")\n+      }\n+    }\n+  }\n+\n+  private def findFilesToCompact(\n+      eventLogFiles: Seq[FileStatus]): (Seq[FileStatus], Seq[FileStatus]) = {\n+    val lastCompactedFileIdx = eventLogFiles.lastIndexWhere { fs =>\n+      EventLogFileWriter.isCompacted(fs.getPath)\n+    }\n+\n+    val files = if (lastCompactedFileIdx > -1) {\n+      eventLogFiles.drop(lastCompactedFileIdx)\n+    } else {\n+      eventLogFiles\n+    }\n+\n+    if (files.length > maxFilesToRetain) {\n+      (files.dropRight(maxFilesToRetain), files.takeRight(maxFilesToRetain))\n+    } else {\n+      (Seq.empty, files)\n+    }\n+  }\n+}\n+\n+class FilteredEventLogFileRewriter(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    eventLogFiles: Seq[FileStatus],\n+    fs: FileSystem,\n+    filters: Seq[EventFilter]) extends Logging {\n+\n+  require(eventLogFiles.nonEmpty)\n+\n+  private val targetEventLogFilePath = eventLogFiles.last.getPath\n+  private val logWriter: CompactedEventLogFileWriter = new CompactedEventLogFileWriter(\n+    targetEventLogFilePath, \"dummy\", None, targetEventLogFilePath.getParent.toUri,\n+    sparkConf, hadoopConf)\n+\n+  def logPath: String = logWriter.logPath\n+\n+  def start(): Unit = {\n+    logWriter.start()\n+  }\n+\n+  def stop(): Unit = {\n+    logWriter.stop()\n+  }\n+\n+  def rewrite(): Unit = {\n+    eventLogFiles.foreach(rewriteFile)\n+  }\n+\n+  private def rewriteFile(fileStatus: FileStatus): Unit = {\n+    Utils.tryWithResource(EventLogFileReader.openEventLog(fileStatus.getPath, fs)) { in =>\n+      val lines = Source.fromInputStream(in)(Codec.UTF8).getLines()\n+\n+      var currentLine: String = null\n+      var lineNumber: Int = 0\n+\n+      try {\n+        val lineEntries = lines.zipWithIndex\n+\n+        while (lineEntries.hasNext) {\n+          try {\n+            val entry = lineEntries.next()\n+\n+            currentLine = entry._1\n+            lineNumber = entry._2 + 1\n+\n+            val event = JsonProtocol.sparkEventFromJson(parse(currentLine))\n+            if (checkFilters(event)) {\n+              logWriter.writeLine(currentLine)\n+            }\n+          } catch {\n+            // ignore any exception occurred from unidentified json\n+            // just skip handling and write the line\n+            case _: ClassNotFoundException => logWriter.writeLine(currentLine)\n+            case _: UnrecognizedPropertyException => logWriter.writeLine(currentLine)\n+          }\n+        }\n+        true\n+      } catch {\n+        case e: Exception =>\n+          logError(s\"Exception parsing Spark event log: ${fileStatus.getPath.getName}\", e)\n+          logError(s\"Malformed line #$lineNumber: $currentLine\\n\")\n+          throw e\n+      }\n+    }\n+  }\n+\n+  private def checkFilters(event: SparkListenerEvent): Boolean = {\n+    val results = filters.flatMap(filter => applyFilter(filter, event))\n+    results.isEmpty || results.forall(_ == true)\n+  }\n+\n+  private def applyFilter(filter: EventFilter, event: SparkListenerEvent): Option[Boolean] = {\n+    event match {\n+      case event: SparkListenerStageSubmitted => filter.filterStageSubmitted(event)\n+      case event: SparkListenerStageCompleted => filter.filterStageCompleted(event)\n+      case event: SparkListenerJobStart => filter.filterJobStart(event)\n+      case event: SparkListenerJobEnd => filter.filterJobEnd(event)\n+      case event: SparkListenerTaskStart => filter.filterTaskStart(event)\n+      case event: SparkListenerTaskGettingResult => filter.filterTaskGettingResult(event)\n+      case event: SparkListenerTaskEnd => filter.filterTaskEnd(event)\n+      case event: SparkListenerEnvironmentUpdate => filter.filterEnvironmentUpdate(event)\n+      case event: SparkListenerBlockManagerAdded => filter.filterBlockManagerAdded(event)\n+      case event: SparkListenerBlockManagerRemoved => filter.filterBlockManagerRemoved(event)\n+      case event: SparkListenerUnpersistRDD => filter.filterUnpersistRDD(event)\n+      case event: SparkListenerApplicationStart => filter.filterApplicationStart(event)\n+      case event: SparkListenerApplicationEnd => filter.filterApplicationEnd(event)\n+      case event: SparkListenerExecutorMetricsUpdate => filter.filterExecutorMetricsUpdate(event)\n+      case event: SparkListenerStageExecutorMetrics => filter.filterStageExecutorMetrics(event)\n+      case event: SparkListenerExecutorAdded => filter.filterExecutorAdded(event)\n+      case event: SparkListenerExecutorRemoved => filter.filterExecutorRemoved(event)\n+      case event: SparkListenerExecutorBlacklistedForStage =>\n+        filter.filterExecutorBlacklistedForStage(event)\n+      case event: SparkListenerNodeBlacklistedForStage =>\n+        filter.filterNodeBlacklistedForStage(event)\n+      case event: SparkListenerExecutorBlacklisted => filter.filterExecutorBlacklisted(event)\n+      case event: SparkListenerExecutorUnblacklisted => filter.filterExecutorUnblacklisted(event)\n+      case event: SparkListenerNodeBlacklisted => filter.filterNodeBlacklisted(event)\n+      case event: SparkListenerNodeUnblacklisted => filter.filterNodeUnblacklisted(event)\n+      case event: SparkListenerBlockUpdated => filter.filterBlockUpdated(event)\n+      case event: SparkListenerSpeculativeTaskSubmitted =>\n+        filter.filterSpeculativeTaskSubmitted(event)\n+      case _ => filter.filterOtherEvent(event)\n+    }\n+  }\n+}\n+\n+class CompactedEventLogFileWriter(\n+    originalFilePath: Path,\n+    appId: String,\n+    appAttemptId: Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  override val logPath: String = originalFilePath.toUri.toString + EventLogFileWriter.COMPACTED\n+\n+  override def writeLine(line: String, flushLogger: Boolean): Unit = {\n+    super.writeLine(line, flushLogger)\n+  }"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "I'm changing my mind to just think every line is json which should be mostly correct - will call `writeEvent` instead.",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-15T04:41:41Z",
    "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  // FIXME: javadoc - caller should provide event log files (either compacted or original)\n+  //  sequentially if the last event log file is already a compacted file, everything\n+  //  will be skipped\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.isEmpty) {\n+      return Seq.empty[FileStatus]\n+    }\n+\n+    // skip everything if the last file is already a compacted file\n+    if (EventLogFileWriter.isCompacted(eventLogFiles.last.getPath)) {\n+      return Seq(eventLogFiles.last)\n+    }\n+\n+    val (filesToCompact, filesToRetain) = findFilesToCompact(eventLogFiles)\n+    if (filesToCompact.isEmpty) {\n+      filesToRetain\n+    } else {\n+      // first pass\n+      val bus = new ReplayListenerBus()\n+\n+      val builders = ServiceLoader.load(classOf[EventFilterBuilder],\n+        Utils.getContextOrSparkClassLoader).asScala\n+      builders.foreach(bus.addListener)\n+\n+      filesToCompact.foreach { log =>\n+        Utils.tryWithResource(EventLogFileReader.openEventLog(log.getPath, fs)) { in =>\n+          bus.replay(in, log.getPath.getName)\n+        }\n+      }\n+\n+      // second pass\n+      val rewriter = new FilteredEventLogFileRewriter(sparkConf, hadoopConf,\n+        filesToCompact, fs, builders.map(_.createFilter()).toSeq)\n+      rewriter.start()\n+      rewriter.rewrite()\n+      rewriter.stop()\n+\n+      // cleanup files which are replaced with new compacted file.\n+      cleanupCompactedFiles(filesToCompact)\n+\n+      fs.getFileStatus(new Path(rewriter.logPath)) :: filesToRetain.toList\n+    }\n+  }\n+\n+  private def cleanupCompactedFiles(files: Seq[FileStatus]): Unit = {\n+    files.foreach { file =>\n+      try {\n+        fs.delete(file.getPath, true)\n+      } catch {\n+        case _: IOException => logWarning(s\"Failed to remove ${file.getPath} / skip removing.\")\n+      }\n+    }\n+  }\n+\n+  private def findFilesToCompact(\n+      eventLogFiles: Seq[FileStatus]): (Seq[FileStatus], Seq[FileStatus]) = {\n+    val lastCompactedFileIdx = eventLogFiles.lastIndexWhere { fs =>\n+      EventLogFileWriter.isCompacted(fs.getPath)\n+    }\n+\n+    val files = if (lastCompactedFileIdx > -1) {\n+      eventLogFiles.drop(lastCompactedFileIdx)\n+    } else {\n+      eventLogFiles\n+    }\n+\n+    if (files.length > maxFilesToRetain) {\n+      (files.dropRight(maxFilesToRetain), files.takeRight(maxFilesToRetain))\n+    } else {\n+      (Seq.empty, files)\n+    }\n+  }\n+}\n+\n+class FilteredEventLogFileRewriter(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    eventLogFiles: Seq[FileStatus],\n+    fs: FileSystem,\n+    filters: Seq[EventFilter]) extends Logging {\n+\n+  require(eventLogFiles.nonEmpty)\n+\n+  private val targetEventLogFilePath = eventLogFiles.last.getPath\n+  private val logWriter: CompactedEventLogFileWriter = new CompactedEventLogFileWriter(\n+    targetEventLogFilePath, \"dummy\", None, targetEventLogFilePath.getParent.toUri,\n+    sparkConf, hadoopConf)\n+\n+  def logPath: String = logWriter.logPath\n+\n+  def start(): Unit = {\n+    logWriter.start()\n+  }\n+\n+  def stop(): Unit = {\n+    logWriter.stop()\n+  }\n+\n+  def rewrite(): Unit = {\n+    eventLogFiles.foreach(rewriteFile)\n+  }\n+\n+  private def rewriteFile(fileStatus: FileStatus): Unit = {\n+    Utils.tryWithResource(EventLogFileReader.openEventLog(fileStatus.getPath, fs)) { in =>\n+      val lines = Source.fromInputStream(in)(Codec.UTF8).getLines()\n+\n+      var currentLine: String = null\n+      var lineNumber: Int = 0\n+\n+      try {\n+        val lineEntries = lines.zipWithIndex\n+\n+        while (lineEntries.hasNext) {\n+          try {\n+            val entry = lineEntries.next()\n+\n+            currentLine = entry._1\n+            lineNumber = entry._2 + 1\n+\n+            val event = JsonProtocol.sparkEventFromJson(parse(currentLine))\n+            if (checkFilters(event)) {\n+              logWriter.writeLine(currentLine)\n+            }\n+          } catch {\n+            // ignore any exception occurred from unidentified json\n+            // just skip handling and write the line\n+            case _: ClassNotFoundException => logWriter.writeLine(currentLine)\n+            case _: UnrecognizedPropertyException => logWriter.writeLine(currentLine)\n+          }\n+        }\n+        true\n+      } catch {\n+        case e: Exception =>\n+          logError(s\"Exception parsing Spark event log: ${fileStatus.getPath.getName}\", e)\n+          logError(s\"Malformed line #$lineNumber: $currentLine\\n\")\n+          throw e\n+      }\n+    }\n+  }\n+\n+  private def checkFilters(event: SparkListenerEvent): Boolean = {\n+    val results = filters.flatMap(filter => applyFilter(filter, event))\n+    results.isEmpty || results.forall(_ == true)\n+  }\n+\n+  private def applyFilter(filter: EventFilter, event: SparkListenerEvent): Option[Boolean] = {\n+    event match {\n+      case event: SparkListenerStageSubmitted => filter.filterStageSubmitted(event)\n+      case event: SparkListenerStageCompleted => filter.filterStageCompleted(event)\n+      case event: SparkListenerJobStart => filter.filterJobStart(event)\n+      case event: SparkListenerJobEnd => filter.filterJobEnd(event)\n+      case event: SparkListenerTaskStart => filter.filterTaskStart(event)\n+      case event: SparkListenerTaskGettingResult => filter.filterTaskGettingResult(event)\n+      case event: SparkListenerTaskEnd => filter.filterTaskEnd(event)\n+      case event: SparkListenerEnvironmentUpdate => filter.filterEnvironmentUpdate(event)\n+      case event: SparkListenerBlockManagerAdded => filter.filterBlockManagerAdded(event)\n+      case event: SparkListenerBlockManagerRemoved => filter.filterBlockManagerRemoved(event)\n+      case event: SparkListenerUnpersistRDD => filter.filterUnpersistRDD(event)\n+      case event: SparkListenerApplicationStart => filter.filterApplicationStart(event)\n+      case event: SparkListenerApplicationEnd => filter.filterApplicationEnd(event)\n+      case event: SparkListenerExecutorMetricsUpdate => filter.filterExecutorMetricsUpdate(event)\n+      case event: SparkListenerStageExecutorMetrics => filter.filterStageExecutorMetrics(event)\n+      case event: SparkListenerExecutorAdded => filter.filterExecutorAdded(event)\n+      case event: SparkListenerExecutorRemoved => filter.filterExecutorRemoved(event)\n+      case event: SparkListenerExecutorBlacklistedForStage =>\n+        filter.filterExecutorBlacklistedForStage(event)\n+      case event: SparkListenerNodeBlacklistedForStage =>\n+        filter.filterNodeBlacklistedForStage(event)\n+      case event: SparkListenerExecutorBlacklisted => filter.filterExecutorBlacklisted(event)\n+      case event: SparkListenerExecutorUnblacklisted => filter.filterExecutorUnblacklisted(event)\n+      case event: SparkListenerNodeBlacklisted => filter.filterNodeBlacklisted(event)\n+      case event: SparkListenerNodeUnblacklisted => filter.filterNodeUnblacklisted(event)\n+      case event: SparkListenerBlockUpdated => filter.filterBlockUpdated(event)\n+      case event: SparkListenerSpeculativeTaskSubmitted =>\n+        filter.filterSpeculativeTaskSubmitted(event)\n+      case _ => filter.filterOtherEvent(event)\n+    }\n+  }\n+}\n+\n+class CompactedEventLogFileWriter(\n+    originalFilePath: Path,\n+    appId: String,\n+    appAttemptId: Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  override val logPath: String = originalFilePath.toUri.toString + EventLogFileWriter.COMPACTED\n+\n+  override def writeLine(line: String, flushLogger: Boolean): Unit = {\n+    super.writeLine(line, flushLogger)\n+  }"
  }],
  "prId": 26416
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Why not `eventLogFiles`?",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-11T09:28:27Z",
    "diffHunk": "@@ -0,0 +1,235 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  // FIXME: javadoc - caller should provide event log files (either compacted or original)\n+  //  sequentially if the last event log file is already a compacted file, everything\n+  //  will be skipped\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.isEmpty) {\n+      return Seq.empty[FileStatus]"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "No big deal. Will change.",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-13T01:05:56Z",
    "diffHunk": "@@ -0,0 +1,235 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  // FIXME: javadoc - caller should provide event log files (either compacted or original)\n+  //  sequentially if the last event log file is already a compacted file, everything\n+  //  will be skipped\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.isEmpty) {\n+      return Seq.empty[FileStatus]"
  }],
  "prId": 26416
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "When exactly could that happen? Compacted file created but not able to delete original event files?",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-11T09:46:25Z",
    "diffHunk": "@@ -0,0 +1,235 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  // FIXME: javadoc - caller should provide event log files (either compacted or original)\n+  //  sequentially if the last event log file is already a compacted file, everything\n+  //  will be skipped\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.isEmpty) {\n+      return Seq.empty[FileStatus]\n+    }\n+\n+    // skip everything if the last file is already a compacted file\n+    if (EventLogFileWriter.isCompacted(eventLogFiles.last.getPath)) {\n+      return Seq(eventLogFiles.last)",
    "line": 63
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "If the logic works properly it shouldn't happen. (max retained files should be 1 or higher) \r\nIt's added for safety (I prefer defensive programming) and early return (it should have same result if we don't return here anyway).",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-13T01:05:07Z",
    "diffHunk": "@@ -0,0 +1,235 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  // FIXME: javadoc - caller should provide event log files (either compacted or original)\n+  //  sequentially if the last event log file is already a compacted file, everything\n+  //  will be skipped\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.isEmpty) {\n+      return Seq.empty[FileStatus]\n+    }\n+\n+    // skip everything if the last file is already a compacted file\n+    if (EventLogFileWriter.isCompacted(eventLogFiles.last.getPath)) {\n+      return Seq(eventLogFiles.last)",
    "line": 63
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Is it possible that compacted event files were not removed in previous round what we can potentially try to clean again here?",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-14T14:53:53Z",
    "diffHunk": "@@ -0,0 +1,235 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  // FIXME: javadoc - caller should provide event log files (either compacted or original)\n+  //  sequentially if the last event log file is already a compacted file, everything\n+  //  will be skipped\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.isEmpty) {\n+      return Seq.empty[FileStatus]\n+    }\n+\n+    // skip everything if the last file is already a compacted file\n+    if (EventLogFileWriter.isCompacted(eventLogFiles.last.getPath)) {\n+      return Seq(eventLogFiles.last)",
    "line": 63
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "I guess it's possible. Btw, we have to do it in RollingEventLogFilesFileReader if we would like to do it, as RollingEventLogFilesFileReader will not give the list of event log file earlier than the latest compact file.",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-15T03:08:28Z",
    "diffHunk": "@@ -0,0 +1,235 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  // FIXME: javadoc - caller should provide event log files (either compacted or original)\n+  //  sequentially if the last event log file is already a compacted file, everything\n+  //  will be skipped\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.isEmpty) {\n+      return Seq.empty[FileStatus]\n+    }\n+\n+    // skip everything if the last file is already a compacted file\n+    if (EventLogFileWriter.isCompacted(eventLogFiles.last.getPath)) {\n+      return Seq(eventLogFiles.last)",
    "line": 63
  }],
  "prId": 26416
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "This will compact all the files at once under `maxFilesToRetain`. Do we need to be so aggressive? Is it not possible to split the compaction and let other functionalities to run.",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-11T10:04:02Z",
    "diffHunk": "@@ -0,0 +1,235 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  // FIXME: javadoc - caller should provide event log files (either compacted or original)\n+  //  sequentially if the last event log file is already a compacted file, everything\n+  //  will be skipped\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.isEmpty) {\n+      return Seq.empty[FileStatus]\n+    }\n+\n+    // skip everything if the last file is already a compacted file\n+    if (EventLogFileWriter.isCompacted(eventLogFiles.last.getPath)) {\n+      return Seq(eventLogFiles.last)\n+    }\n+\n+    val (filesToCompact, filesToRetain) = findFilesToCompact(eventLogFiles)\n+    if (filesToCompact.isEmpty) {\n+      filesToRetain\n+    } else {\n+      // first pass\n+      val bus = new ReplayListenerBus()\n+\n+      val builders = ServiceLoader.load(classOf[EventFilterBuilder],\n+        Utils.getContextOrSparkClassLoader).asScala\n+      builders.foreach(bus.addListener)\n+\n+      filesToCompact.foreach { log =>\n+        Utils.tryWithResource(EventLogFileReader.openEventLog(log.getPath, fs)) { in =>\n+          bus.replay(in, log.getPath.getName)\n+        }\n+      }\n+\n+      // second pass\n+      val rewriter = new FilteredEventLogFileRewriter(sparkConf, hadoopConf,\n+        filesToCompact, fs, builders.map(_.createFilter()).toSeq)\n+      rewriter.start()\n+      rewriter.rewrite()\n+      rewriter.stop()\n+\n+      // cleanup files which are replaced with new compacted file.\n+      cleanupCompactedFiles(filesToCompact)\n+\n+      fs.getFileStatus(new Path(rewriter.logPath)) :: filesToRetain.toList\n+    }\n+  }\n+\n+  private def cleanupCompactedFiles(files: Seq[FileStatus]): Unit = {\n+    files.foreach { file =>\n+      try {\n+        fs.delete(file.getPath, true)\n+      } catch {\n+        case _: IOException => logWarning(s\"Failed to remove ${file.getPath} / skip removing.\")\n+      }\n+    }\n+  }\n+\n+  private def findFilesToCompact(\n+      eventLogFiles: Seq[FileStatus]): (Seq[FileStatus], Seq[FileStatus]) = {\n+    val lastCompactedFileIdx = eventLogFiles.lastIndexWhere { fs =>\n+      EventLogFileWriter.isCompacted(fs.getPath)\n+    }\n+    val files = eventLogFiles.drop(lastCompactedFileIdx)\n+\n+    if (files.length > maxFilesToRetain) {\n+      (files.dropRight(maxFilesToRetain), files.takeRight(maxFilesToRetain))",
    "line": 100
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "You cannot cleanup log file unless compacted. So as long as `maxFilesToRetain` has the semantic of cleaning old files, that is the minimum, so in fact it's opposite.\r\n\r\nWe may be able to add one more config to let compactor be able to compact over retained files to speed up replaying, though I think the original approach would be better in point of replaying the logs. Once I finish this I would try to go back and see how we can do incremental replay with snapshot approach.",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-13T00:57:07Z",
    "diffHunk": "@@ -0,0 +1,235 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  // FIXME: javadoc - caller should provide event log files (either compacted or original)\n+  //  sequentially if the last event log file is already a compacted file, everything\n+  //  will be skipped\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.isEmpty) {\n+      return Seq.empty[FileStatus]\n+    }\n+\n+    // skip everything if the last file is already a compacted file\n+    if (EventLogFileWriter.isCompacted(eventLogFiles.last.getPath)) {\n+      return Seq(eventLogFiles.last)\n+    }\n+\n+    val (filesToCompact, filesToRetain) = findFilesToCompact(eventLogFiles)\n+    if (filesToCompact.isEmpty) {\n+      filesToRetain\n+    } else {\n+      // first pass\n+      val bus = new ReplayListenerBus()\n+\n+      val builders = ServiceLoader.load(classOf[EventFilterBuilder],\n+        Utils.getContextOrSparkClassLoader).asScala\n+      builders.foreach(bus.addListener)\n+\n+      filesToCompact.foreach { log =>\n+        Utils.tryWithResource(EventLogFileReader.openEventLog(log.getPath, fs)) { in =>\n+          bus.replay(in, log.getPath.getName)\n+        }\n+      }\n+\n+      // second pass\n+      val rewriter = new FilteredEventLogFileRewriter(sparkConf, hadoopConf,\n+        filesToCompact, fs, builders.map(_.createFilter()).toSeq)\n+      rewriter.start()\n+      rewriter.rewrite()\n+      rewriter.stop()\n+\n+      // cleanup files which are replaced with new compacted file.\n+      cleanupCompactedFiles(filesToCompact)\n+\n+      fs.getFileStatus(new Path(rewriter.logPath)) :: filesToRetain.toList\n+    }\n+  }\n+\n+  private def cleanupCompactedFiles(files: Seq[FileStatus]): Unit = {\n+    files.foreach { file =>\n+      try {\n+        fs.delete(file.getPath, true)\n+      } catch {\n+        case _: IOException => logWarning(s\"Failed to remove ${file.getPath} / skip removing.\")\n+      }\n+    }\n+  }\n+\n+  private def findFilesToCompact(\n+      eventLogFiles: Seq[FileStatus]): (Seq[FileStatus], Seq[FileStatus]) = {\n+    val lastCompactedFileIdx = eventLogFiles.lastIndexWhere { fs =>\n+      EventLogFileWriter.isCompacted(fs.getPath)\n+    }\n+    val files = eventLogFiles.drop(lastCompactedFileIdx)\n+\n+    if (files.length > maxFilesToRetain) {\n+      (files.dropRight(maxFilesToRetain), files.takeRight(maxFilesToRetain))",
    "line": 100
  }],
  "prId": 26416
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Not yet see the helpfulness in these comments (this + `second pass`). What is it?",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-11T10:10:56Z",
    "diffHunk": "@@ -0,0 +1,235 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  // FIXME: javadoc - caller should provide event log files (either compacted or original)\n+  //  sequentially if the last event log file is already a compacted file, everything\n+  //  will be skipped\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.isEmpty) {\n+      return Seq.empty[FileStatus]\n+    }\n+\n+    // skip everything if the last file is already a compacted file\n+    if (EventLogFileWriter.isCompacted(eventLogFiles.last.getPath)) {\n+      return Seq(eventLogFiles.last)\n+    }\n+\n+    val (filesToCompact, filesToRetain) = findFilesToCompact(eventLogFiles)\n+    if (filesToCompact.isEmpty) {\n+      filesToRetain\n+    } else {\n+      // first pass"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "I'll just remove it. I have to explain how it works in class's scaladoc anyway.",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-13T01:01:49Z",
    "diffHunk": "@@ -0,0 +1,235 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  // FIXME: javadoc - caller should provide event log files (either compacted or original)\n+  //  sequentially if the last event log file is already a compacted file, everything\n+  //  will be skipped\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.isEmpty) {\n+      return Seq.empty[FileStatus]\n+    }\n+\n+    // skip everything if the last file is already a compacted file\n+    if (EventLogFileWriter.isCompacted(eventLogFiles.last.getPath)) {\n+      return Seq(eventLogFiles.last)\n+    }\n+\n+    val (filesToCompact, filesToRetain) = findFilesToCompact(eventLogFiles)\n+    if (filesToCompact.isEmpty) {\n+      filesToRetain\n+    } else {\n+      // first pass"
  }],
  "prId": 26416
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "As I see it's possible that compacted file created successfully but the original event files was unable to delete. What's the plan there?",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-11T10:13:46Z",
    "diffHunk": "@@ -0,0 +1,235 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  // FIXME: javadoc - caller should provide event log files (either compacted or original)\n+  //  sequentially if the last event log file is already a compacted file, everything\n+  //  will be skipped\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.isEmpty) {\n+      return Seq.empty[FileStatus]\n+    }\n+\n+    // skip everything if the last file is already a compacted file\n+    if (EventLogFileWriter.isCompacted(eventLogFiles.last.getPath)) {\n+      return Seq(eventLogFiles.last)\n+    }\n+\n+    val (filesToCompact, filesToRetain) = findFilesToCompact(eventLogFiles)\n+    if (filesToCompact.isEmpty) {\n+      filesToRetain\n+    } else {\n+      // first pass\n+      val bus = new ReplayListenerBus()\n+\n+      val builders = ServiceLoader.load(classOf[EventFilterBuilder],\n+        Utils.getContextOrSparkClassLoader).asScala\n+      builders.foreach(bus.addListener)\n+\n+      filesToCompact.foreach { log =>\n+        Utils.tryWithResource(EventLogFileReader.openEventLog(log.getPath, fs)) { in =>\n+          bus.replay(in, log.getPath.getName)\n+        }\n+      }\n+\n+      // second pass\n+      val rewriter = new FilteredEventLogFileRewriter(sparkConf, hadoopConf,\n+        filesToCompact, fs, builders.map(_.createFilter()).toSeq)\n+      rewriter.start()\n+      rewriter.rewrite()\n+      rewriter.stop()\n+\n+      // cleanup files which are replaced with new compacted file."
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "As I commented, the logic doesn't break even original file and compact file co-exist for same index.",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-13T01:00:44Z",
    "diffHunk": "@@ -0,0 +1,235 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  // FIXME: javadoc - caller should provide event log files (either compacted or original)\n+  //  sequentially if the last event log file is already a compacted file, everything\n+  //  will be skipped\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.isEmpty) {\n+      return Seq.empty[FileStatus]\n+    }\n+\n+    // skip everything if the last file is already a compacted file\n+    if (EventLogFileWriter.isCompacted(eventLogFiles.last.getPath)) {\n+      return Seq(eventLogFiles.last)\n+    }\n+\n+    val (filesToCompact, filesToRetain) = findFilesToCompact(eventLogFiles)\n+    if (filesToCompact.isEmpty) {\n+      filesToRetain\n+    } else {\n+      // first pass\n+      val bus = new ReplayListenerBus()\n+\n+      val builders = ServiceLoader.load(classOf[EventFilterBuilder],\n+        Utils.getContextOrSparkClassLoader).asScala\n+      builders.foreach(bus.addListener)\n+\n+      filesToCompact.foreach { log =>\n+        Utils.tryWithResource(EventLogFileReader.openEventLog(log.getPath, fs)) { in =>\n+          bus.replay(in, log.getPath.getName)\n+        }\n+      }\n+\n+      // second pass\n+      val rewriter = new FilteredEventLogFileRewriter(sparkConf, hadoopConf,\n+        filesToCompact, fs, builders.map(_.createFilter()).toSeq)\n+      rewriter.start()\n+      rewriter.rewrite()\n+      rewriter.stop()\n+\n+      // cleanup files which are replaced with new compacted file."
  }],
  "prId": 26416
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Why not checking the returned boolean?",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-11T10:15:34Z",
    "diffHunk": "@@ -0,0 +1,235 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  // FIXME: javadoc - caller should provide event log files (either compacted or original)\n+  //  sequentially if the last event log file is already a compacted file, everything\n+  //  will be skipped\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.isEmpty) {\n+      return Seq.empty[FileStatus]\n+    }\n+\n+    // skip everything if the last file is already a compacted file\n+    if (EventLogFileWriter.isCompacted(eventLogFiles.last.getPath)) {\n+      return Seq(eventLogFiles.last)\n+    }\n+\n+    val (filesToCompact, filesToRetain) = findFilesToCompact(eventLogFiles)\n+    if (filesToCompact.isEmpty) {\n+      filesToRetain\n+    } else {\n+      // first pass\n+      val bus = new ReplayListenerBus()\n+\n+      val builders = ServiceLoader.load(classOf[EventFilterBuilder],\n+        Utils.getContextOrSparkClassLoader).asScala\n+      builders.foreach(bus.addListener)\n+\n+      filesToCompact.foreach { log =>\n+        Utils.tryWithResource(EventLogFileReader.openEventLog(log.getPath, fs)) { in =>\n+          bus.replay(in, log.getPath.getName)\n+        }\n+      }\n+\n+      // second pass\n+      val rewriter = new FilteredEventLogFileRewriter(sparkConf, hadoopConf,\n+        filesToCompact, fs, builders.map(_.createFilter()).toSeq)\n+      rewriter.start()\n+      rewriter.rewrite()\n+      rewriter.stop()\n+\n+      // cleanup files which are replaced with new compacted file.\n+      cleanupCompactedFiles(filesToCompact)\n+\n+      fs.getFileStatus(new Path(rewriter.logPath)) :: filesToRetain.toList\n+    }\n+  }\n+\n+  private def cleanupCompactedFiles(files: Seq[FileStatus]): Unit = {\n+    files.foreach { file =>\n+      try {\n+        fs.delete(file.getPath, true)"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Will do and leave log message.",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-13T00:59:51Z",
    "diffHunk": "@@ -0,0 +1,235 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  // FIXME: javadoc - caller should provide event log files (either compacted or original)\n+  //  sequentially if the last event log file is already a compacted file, everything\n+  //  will be skipped\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.isEmpty) {\n+      return Seq.empty[FileStatus]\n+    }\n+\n+    // skip everything if the last file is already a compacted file\n+    if (EventLogFileWriter.isCompacted(eventLogFiles.last.getPath)) {\n+      return Seq(eventLogFiles.last)\n+    }\n+\n+    val (filesToCompact, filesToRetain) = findFilesToCompact(eventLogFiles)\n+    if (filesToCompact.isEmpty) {\n+      filesToRetain\n+    } else {\n+      // first pass\n+      val bus = new ReplayListenerBus()\n+\n+      val builders = ServiceLoader.load(classOf[EventFilterBuilder],\n+        Utils.getContextOrSparkClassLoader).asScala\n+      builders.foreach(bus.addListener)\n+\n+      filesToCompact.foreach { log =>\n+        Utils.tryWithResource(EventLogFileReader.openEventLog(log.getPath, fs)) { in =>\n+          bus.replay(in, log.getPath.getName)\n+        }\n+      }\n+\n+      // second pass\n+      val rewriter = new FilteredEventLogFileRewriter(sparkConf, hadoopConf,\n+        filesToCompact, fs, builders.map(_.createFilter()).toSeq)\n+      rewriter.start()\n+      rewriter.rewrite()\n+      rewriter.stop()\n+\n+      // cleanup files which are replaced with new compacted file.\n+      cleanupCompactedFiles(filesToCompact)\n+\n+      fs.getFileStatus(new Path(rewriter.logPath)) :: filesToRetain.toList\n+    }\n+  }\n+\n+  private def cleanupCompactedFiles(files: Seq[FileStatus]): Unit = {\n+    files.foreach { file =>\n+      try {\n+        fs.delete(file.getPath, true)"
  }],
  "prId": 26416
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Maybe we can call such vals `COMPACTED_SUFFIX`? When I see `stripSuffix` I know what it is but such case when stays alone maybe better to name it more meaningful.",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-11T10:25:43Z",
    "diffHunk": "@@ -0,0 +1,235 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  // FIXME: javadoc - caller should provide event log files (either compacted or original)\n+  //  sequentially if the last event log file is already a compacted file, everything\n+  //  will be skipped\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.isEmpty) {\n+      return Seq.empty[FileStatus]\n+    }\n+\n+    // skip everything if the last file is already a compacted file\n+    if (EventLogFileWriter.isCompacted(eventLogFiles.last.getPath)) {\n+      return Seq(eventLogFiles.last)\n+    }\n+\n+    val (filesToCompact, filesToRetain) = findFilesToCompact(eventLogFiles)\n+    if (filesToCompact.isEmpty) {\n+      filesToRetain\n+    } else {\n+      // first pass\n+      val bus = new ReplayListenerBus()\n+\n+      val builders = ServiceLoader.load(classOf[EventFilterBuilder],\n+        Utils.getContextOrSparkClassLoader).asScala\n+      builders.foreach(bus.addListener)\n+\n+      filesToCompact.foreach { log =>\n+        Utils.tryWithResource(EventLogFileReader.openEventLog(log.getPath, fs)) { in =>\n+          bus.replay(in, log.getPath.getName)\n+        }\n+      }\n+\n+      // second pass\n+      val rewriter = new FilteredEventLogFileRewriter(sparkConf, hadoopConf,\n+        filesToCompact, fs, builders.map(_.createFilter()).toSeq)\n+      rewriter.start()\n+      rewriter.rewrite()\n+      rewriter.stop()\n+\n+      // cleanup files which are replaced with new compacted file.\n+      cleanupCompactedFiles(filesToCompact)\n+\n+      fs.getFileStatus(new Path(rewriter.logPath)) :: filesToRetain.toList\n+    }\n+  }\n+\n+  private def cleanupCompactedFiles(files: Seq[FileStatus]): Unit = {\n+    files.foreach { file =>\n+      try {\n+        fs.delete(file.getPath, true)\n+      } catch {\n+        case _: IOException => logWarning(s\"Failed to remove ${file.getPath} / skip removing.\")\n+      }\n+    }\n+  }\n+\n+  private def findFilesToCompact(\n+      eventLogFiles: Seq[FileStatus]): (Seq[FileStatus], Seq[FileStatus]) = {\n+    val lastCompactedFileIdx = eventLogFiles.lastIndexWhere { fs =>\n+      EventLogFileWriter.isCompacted(fs.getPath)\n+    }\n+    val files = eventLogFiles.drop(lastCompactedFileIdx)\n+\n+    if (files.length > maxFilesToRetain) {\n+      (files.dropRight(maxFilesToRetain), files.takeRight(maxFilesToRetain))\n+    } else {\n+      (Seq.empty, files)\n+    }\n+  }\n+}\n+\n+class FilteredEventLogFileRewriter(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    eventLogFiles: Seq[FileStatus],\n+    fs: FileSystem,\n+    filters: Seq[EventFilter]) extends Logging {\n+\n+  require(eventLogFiles.nonEmpty)\n+\n+  private val targetEventLogFilePath = eventLogFiles.last.getPath\n+  private val logWriter: CompactedEventLogFileWriter = new CompactedEventLogFileWriter(\n+    targetEventLogFilePath, \"dummy\", None, targetEventLogFilePath.getParent.toUri,\n+    sparkConf, hadoopConf)\n+\n+  def logPath: String = logWriter.logPath\n+\n+  def start(): Unit = {\n+    logWriter.start()\n+  }\n+\n+  def stop(): Unit = {\n+    logWriter.stop()\n+  }\n+\n+  def rewrite(): Unit = {\n+    eventLogFiles.foreach(rewriteFile)\n+  }\n+\n+  private def rewriteFile(fileStatus: FileStatus): Unit = {\n+    Utils.tryWithResource(EventLogFileReader.openEventLog(fileStatus.getPath, fs)) { in =>\n+      val lines = Source.fromInputStream(in)(Codec.UTF8).getLines()\n+\n+      var currentLine: String = null\n+      var lineNumber: Int = 0\n+\n+      try {\n+        val lineEntries = lines.zipWithIndex\n+\n+        while (lineEntries.hasNext) {\n+          try {\n+            val entry = lineEntries.next()\n+\n+            currentLine = entry._1\n+            lineNumber = entry._2 + 1\n+\n+            val event = JsonProtocol.sparkEventFromJson(parse(currentLine))\n+            if (checkFilters(event)) {\n+              logWriter.writeLine(currentLine)\n+            }\n+          } catch {\n+            // ignore any exception occurred from unidentified json\n+            // just skip handling and write the line\n+            case _: ClassNotFoundException => logWriter.writeLine(currentLine)\n+            case _: UnrecognizedPropertyException => logWriter.writeLine(currentLine)\n+          }\n+        }\n+        true\n+      } catch {\n+        case e: Exception =>\n+          logError(s\"Exception parsing Spark event log: ${fileStatus.getPath.getName}\", e)\n+          logError(s\"Malformed line #$lineNumber: $currentLine\\n\")\n+          throw e\n+      }\n+    }\n+  }\n+\n+  private def checkFilters(event: SparkListenerEvent): Boolean = {\n+    val results = filters.flatMap(filter => applyFilter(filter, event))\n+    results.isEmpty || results.forall(_ == true)\n+  }\n+\n+  private def applyFilter(filter: EventFilter, event: SparkListenerEvent): Option[Boolean] = {\n+    event match {\n+      case event: SparkListenerStageSubmitted => filter.filterStageSubmitted(event)\n+      case event: SparkListenerStageCompleted => filter.filterStageCompleted(event)\n+      case event: SparkListenerJobStart => filter.filterJobStart(event)\n+      case event: SparkListenerJobEnd => filter.filterJobEnd(event)\n+      case event: SparkListenerTaskStart => filter.filterTaskStart(event)\n+      case event: SparkListenerTaskGettingResult => filter.filterTaskGettingResult(event)\n+      case event: SparkListenerTaskEnd => filter.filterTaskEnd(event)\n+      case event: SparkListenerEnvironmentUpdate => filter.filterEnvironmentUpdate(event)\n+      case event: SparkListenerBlockManagerAdded => filter.filterBlockManagerAdded(event)\n+      case event: SparkListenerBlockManagerRemoved => filter.filterBlockManagerRemoved(event)\n+      case event: SparkListenerUnpersistRDD => filter.filterUnpersistRDD(event)\n+      case event: SparkListenerApplicationStart => filter.filterApplicationStart(event)\n+      case event: SparkListenerApplicationEnd => filter.filterApplicationEnd(event)\n+      case event: SparkListenerExecutorMetricsUpdate => filter.filterExecutorMetricsUpdate(event)\n+      case event: SparkListenerStageExecutorMetrics => filter.filterStageExecutorMetrics(event)\n+      case event: SparkListenerExecutorAdded => filter.filterExecutorAdded(event)\n+      case event: SparkListenerExecutorRemoved => filter.filterExecutorRemoved(event)\n+      case event: SparkListenerExecutorBlacklistedForStage =>\n+        filter.filterExecutorBlacklistedForStage(event)\n+      case event: SparkListenerNodeBlacklistedForStage =>\n+        filter.filterNodeBlacklistedForStage(event)\n+      case event: SparkListenerExecutorBlacklisted => filter.filterExecutorBlacklisted(event)\n+      case event: SparkListenerExecutorUnblacklisted => filter.filterExecutorUnblacklisted(event)\n+      case event: SparkListenerNodeBlacklisted => filter.filterNodeBlacklisted(event)\n+      case event: SparkListenerNodeUnblacklisted => filter.filterNodeUnblacklisted(event)\n+      case event: SparkListenerBlockUpdated => filter.filterBlockUpdated(event)\n+      case event: SparkListenerSpeculativeTaskSubmitted =>\n+        filter.filterSpeculativeTaskSubmitted(event)\n+      case _ => filter.filterOtherEvent(event)\n+    }\n+  }\n+}\n+\n+class CompactedEventLogFileWriter(\n+    originalFilePath: Path,\n+    appId: String,\n+    appAttemptId: Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  override val logPath: String = originalFilePath.toUri.toString + EventLogFileWriter.COMPACTED",
    "line": 161
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "I tend to make longer name for clarity but I feel the community prefers concise name more. Same applied to `IN_PROGRESS`. So let's hear more voices on this.",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-13T00:48:53Z",
    "diffHunk": "@@ -0,0 +1,235 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  // FIXME: javadoc - caller should provide event log files (either compacted or original)\n+  //  sequentially if the last event log file is already a compacted file, everything\n+  //  will be skipped\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.isEmpty) {\n+      return Seq.empty[FileStatus]\n+    }\n+\n+    // skip everything if the last file is already a compacted file\n+    if (EventLogFileWriter.isCompacted(eventLogFiles.last.getPath)) {\n+      return Seq(eventLogFiles.last)\n+    }\n+\n+    val (filesToCompact, filesToRetain) = findFilesToCompact(eventLogFiles)\n+    if (filesToCompact.isEmpty) {\n+      filesToRetain\n+    } else {\n+      // first pass\n+      val bus = new ReplayListenerBus()\n+\n+      val builders = ServiceLoader.load(classOf[EventFilterBuilder],\n+        Utils.getContextOrSparkClassLoader).asScala\n+      builders.foreach(bus.addListener)\n+\n+      filesToCompact.foreach { log =>\n+        Utils.tryWithResource(EventLogFileReader.openEventLog(log.getPath, fs)) { in =>\n+          bus.replay(in, log.getPath.getName)\n+        }\n+      }\n+\n+      // second pass\n+      val rewriter = new FilteredEventLogFileRewriter(sparkConf, hadoopConf,\n+        filesToCompact, fs, builders.map(_.createFilter()).toSeq)\n+      rewriter.start()\n+      rewriter.rewrite()\n+      rewriter.stop()\n+\n+      // cleanup files which are replaced with new compacted file.\n+      cleanupCompactedFiles(filesToCompact)\n+\n+      fs.getFileStatus(new Path(rewriter.logPath)) :: filesToRetain.toList\n+    }\n+  }\n+\n+  private def cleanupCompactedFiles(files: Seq[FileStatus]): Unit = {\n+    files.foreach { file =>\n+      try {\n+        fs.delete(file.getPath, true)\n+      } catch {\n+        case _: IOException => logWarning(s\"Failed to remove ${file.getPath} / skip removing.\")\n+      }\n+    }\n+  }\n+\n+  private def findFilesToCompact(\n+      eventLogFiles: Seq[FileStatus]): (Seq[FileStatus], Seq[FileStatus]) = {\n+    val lastCompactedFileIdx = eventLogFiles.lastIndexWhere { fs =>\n+      EventLogFileWriter.isCompacted(fs.getPath)\n+    }\n+    val files = eventLogFiles.drop(lastCompactedFileIdx)\n+\n+    if (files.length > maxFilesToRetain) {\n+      (files.dropRight(maxFilesToRetain), files.takeRight(maxFilesToRetain))\n+    } else {\n+      (Seq.empty, files)\n+    }\n+  }\n+}\n+\n+class FilteredEventLogFileRewriter(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    eventLogFiles: Seq[FileStatus],\n+    fs: FileSystem,\n+    filters: Seq[EventFilter]) extends Logging {\n+\n+  require(eventLogFiles.nonEmpty)\n+\n+  private val targetEventLogFilePath = eventLogFiles.last.getPath\n+  private val logWriter: CompactedEventLogFileWriter = new CompactedEventLogFileWriter(\n+    targetEventLogFilePath, \"dummy\", None, targetEventLogFilePath.getParent.toUri,\n+    sparkConf, hadoopConf)\n+\n+  def logPath: String = logWriter.logPath\n+\n+  def start(): Unit = {\n+    logWriter.start()\n+  }\n+\n+  def stop(): Unit = {\n+    logWriter.stop()\n+  }\n+\n+  def rewrite(): Unit = {\n+    eventLogFiles.foreach(rewriteFile)\n+  }\n+\n+  private def rewriteFile(fileStatus: FileStatus): Unit = {\n+    Utils.tryWithResource(EventLogFileReader.openEventLog(fileStatus.getPath, fs)) { in =>\n+      val lines = Source.fromInputStream(in)(Codec.UTF8).getLines()\n+\n+      var currentLine: String = null\n+      var lineNumber: Int = 0\n+\n+      try {\n+        val lineEntries = lines.zipWithIndex\n+\n+        while (lineEntries.hasNext) {\n+          try {\n+            val entry = lineEntries.next()\n+\n+            currentLine = entry._1\n+            lineNumber = entry._2 + 1\n+\n+            val event = JsonProtocol.sparkEventFromJson(parse(currentLine))\n+            if (checkFilters(event)) {\n+              logWriter.writeLine(currentLine)\n+            }\n+          } catch {\n+            // ignore any exception occurred from unidentified json\n+            // just skip handling and write the line\n+            case _: ClassNotFoundException => logWriter.writeLine(currentLine)\n+            case _: UnrecognizedPropertyException => logWriter.writeLine(currentLine)\n+          }\n+        }\n+        true\n+      } catch {\n+        case e: Exception =>\n+          logError(s\"Exception parsing Spark event log: ${fileStatus.getPath.getName}\", e)\n+          logError(s\"Malformed line #$lineNumber: $currentLine\\n\")\n+          throw e\n+      }\n+    }\n+  }\n+\n+  private def checkFilters(event: SparkListenerEvent): Boolean = {\n+    val results = filters.flatMap(filter => applyFilter(filter, event))\n+    results.isEmpty || results.forall(_ == true)\n+  }\n+\n+  private def applyFilter(filter: EventFilter, event: SparkListenerEvent): Option[Boolean] = {\n+    event match {\n+      case event: SparkListenerStageSubmitted => filter.filterStageSubmitted(event)\n+      case event: SparkListenerStageCompleted => filter.filterStageCompleted(event)\n+      case event: SparkListenerJobStart => filter.filterJobStart(event)\n+      case event: SparkListenerJobEnd => filter.filterJobEnd(event)\n+      case event: SparkListenerTaskStart => filter.filterTaskStart(event)\n+      case event: SparkListenerTaskGettingResult => filter.filterTaskGettingResult(event)\n+      case event: SparkListenerTaskEnd => filter.filterTaskEnd(event)\n+      case event: SparkListenerEnvironmentUpdate => filter.filterEnvironmentUpdate(event)\n+      case event: SparkListenerBlockManagerAdded => filter.filterBlockManagerAdded(event)\n+      case event: SparkListenerBlockManagerRemoved => filter.filterBlockManagerRemoved(event)\n+      case event: SparkListenerUnpersistRDD => filter.filterUnpersistRDD(event)\n+      case event: SparkListenerApplicationStart => filter.filterApplicationStart(event)\n+      case event: SparkListenerApplicationEnd => filter.filterApplicationEnd(event)\n+      case event: SparkListenerExecutorMetricsUpdate => filter.filterExecutorMetricsUpdate(event)\n+      case event: SparkListenerStageExecutorMetrics => filter.filterStageExecutorMetrics(event)\n+      case event: SparkListenerExecutorAdded => filter.filterExecutorAdded(event)\n+      case event: SparkListenerExecutorRemoved => filter.filterExecutorRemoved(event)\n+      case event: SparkListenerExecutorBlacklistedForStage =>\n+        filter.filterExecutorBlacklistedForStage(event)\n+      case event: SparkListenerNodeBlacklistedForStage =>\n+        filter.filterNodeBlacklistedForStage(event)\n+      case event: SparkListenerExecutorBlacklisted => filter.filterExecutorBlacklisted(event)\n+      case event: SparkListenerExecutorUnblacklisted => filter.filterExecutorUnblacklisted(event)\n+      case event: SparkListenerNodeBlacklisted => filter.filterNodeBlacklisted(event)\n+      case event: SparkListenerNodeUnblacklisted => filter.filterNodeUnblacklisted(event)\n+      case event: SparkListenerBlockUpdated => filter.filterBlockUpdated(event)\n+      case event: SparkListenerSpeculativeTaskSubmitted =>\n+        filter.filterSpeculativeTaskSubmitted(event)\n+      case _ => filter.filterOtherEvent(event)\n+    }\n+  }\n+}\n+\n+class CompactedEventLogFileWriter(\n+    originalFilePath: Path,\n+    appId: String,\n+    appAttemptId: Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  override val logPath: String = originalFilePath.toUri.toString + EventLogFileWriter.COMPACTED",
    "line": 161
  }],
  "prId": 26416
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Only slightly touched the filtering part (and no formed view on this) but I'm always afraid a bit when I see switch case with event types. It's extremely easy to add +-1 to the list and it's always questionable when new event type comes where other programmer has no idea that this part must be modified to reach proper implementation. Will comes back to this when fully processed...",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-11T10:42:21Z",
    "diffHunk": "@@ -0,0 +1,235 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  // FIXME: javadoc - caller should provide event log files (either compacted or original)\n+  //  sequentially if the last event log file is already a compacted file, everything\n+  //  will be skipped\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.isEmpty) {\n+      return Seq.empty[FileStatus]\n+    }\n+\n+    // skip everything if the last file is already a compacted file\n+    if (EventLogFileWriter.isCompacted(eventLogFiles.last.getPath)) {\n+      return Seq(eventLogFiles.last)\n+    }\n+\n+    val (filesToCompact, filesToRetain) = findFilesToCompact(eventLogFiles)\n+    if (filesToCompact.isEmpty) {\n+      filesToRetain\n+    } else {\n+      // first pass\n+      val bus = new ReplayListenerBus()\n+\n+      val builders = ServiceLoader.load(classOf[EventFilterBuilder],\n+        Utils.getContextOrSparkClassLoader).asScala\n+      builders.foreach(bus.addListener)\n+\n+      filesToCompact.foreach { log =>\n+        Utils.tryWithResource(EventLogFileReader.openEventLog(log.getPath, fs)) { in =>\n+          bus.replay(in, log.getPath.getName)\n+        }\n+      }\n+\n+      // second pass\n+      val rewriter = new FilteredEventLogFileRewriter(sparkConf, hadoopConf,\n+        filesToCompact, fs, builders.map(_.createFilter()).toSeq)\n+      rewriter.start()\n+      rewriter.rewrite()\n+      rewriter.stop()\n+\n+      // cleanup files which are replaced with new compacted file.\n+      cleanupCompactedFiles(filesToCompact)\n+\n+      fs.getFileStatus(new Path(rewriter.logPath)) :: filesToRetain.toList\n+    }\n+  }\n+\n+  private def cleanupCompactedFiles(files: Seq[FileStatus]): Unit = {\n+    files.foreach { file =>\n+      try {\n+        fs.delete(file.getPath, true)\n+      } catch {\n+        case _: IOException => logWarning(s\"Failed to remove ${file.getPath} / skip removing.\")\n+      }\n+    }\n+  }\n+\n+  private def findFilesToCompact(\n+      eventLogFiles: Seq[FileStatus]): (Seq[FileStatus], Seq[FileStatus]) = {\n+    val lastCompactedFileIdx = eventLogFiles.lastIndexWhere { fs =>\n+      EventLogFileWriter.isCompacted(fs.getPath)\n+    }\n+    val files = eventLogFiles.drop(lastCompactedFileIdx)\n+\n+    if (files.length > maxFilesToRetain) {\n+      (files.dropRight(maxFilesToRetain), files.takeRight(maxFilesToRetain))\n+    } else {\n+      (Seq.empty, files)\n+    }\n+  }\n+}\n+\n+class FilteredEventLogFileRewriter(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    eventLogFiles: Seq[FileStatus],\n+    fs: FileSystem,\n+    filters: Seq[EventFilter]) extends Logging {\n+\n+  require(eventLogFiles.nonEmpty)\n+\n+  private val targetEventLogFilePath = eventLogFiles.last.getPath\n+  private val logWriter: CompactedEventLogFileWriter = new CompactedEventLogFileWriter(\n+    targetEventLogFilePath, \"dummy\", None, targetEventLogFilePath.getParent.toUri,\n+    sparkConf, hadoopConf)\n+\n+  def logPath: String = logWriter.logPath\n+\n+  def start(): Unit = {\n+    logWriter.start()\n+  }\n+\n+  def stop(): Unit = {\n+    logWriter.stop()\n+  }\n+\n+  def rewrite(): Unit = {\n+    eventLogFiles.foreach(rewriteFile)\n+  }\n+\n+  private def rewriteFile(fileStatus: FileStatus): Unit = {\n+    Utils.tryWithResource(EventLogFileReader.openEventLog(fileStatus.getPath, fs)) { in =>\n+      val lines = Source.fromInputStream(in)(Codec.UTF8).getLines()\n+\n+      var currentLine: String = null\n+      var lineNumber: Int = 0\n+\n+      try {\n+        val lineEntries = lines.zipWithIndex\n+\n+        while (lineEntries.hasNext) {\n+          try {\n+            val entry = lineEntries.next()\n+\n+            currentLine = entry._1\n+            lineNumber = entry._2 + 1\n+\n+            val event = JsonProtocol.sparkEventFromJson(parse(currentLine))\n+            if (checkFilters(event)) {\n+              logWriter.writeLine(currentLine)\n+            }\n+          } catch {\n+            // ignore any exception occurred from unidentified json\n+            // just skip handling and write the line\n+            case _: ClassNotFoundException => logWriter.writeLine(currentLine)\n+            case _: UnrecognizedPropertyException => logWriter.writeLine(currentLine)\n+          }\n+        }\n+        true\n+      } catch {\n+        case e: Exception =>\n+          logError(s\"Exception parsing Spark event log: ${fileStatus.getPath.getName}\", e)\n+          logError(s\"Malformed line #$lineNumber: $currentLine\\n\")\n+          throw e\n+      }\n+    }\n+  }\n+\n+  private def checkFilters(event: SparkListenerEvent): Boolean = {\n+    val results = filters.flatMap(filter => applyFilter(filter, event))\n+    results.isEmpty || results.forall(_ == true)\n+  }\n+\n+  private def applyFilter(filter: EventFilter, event: SparkListenerEvent): Option[Boolean] = {\n+    event match {\n+      case event: SparkListenerStageSubmitted => filter.filterStageSubmitted(event)"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Yeah you're right. This class actually extended SparkListener to guarantee same coverage but I realized I shouldn't rewrite the json but keep the origin json so this is the result. \r\n\r\nSo the listener should be aware of origin line along with event instance to write origin line after passing filter. Maybe we can have one, not sure it's somewhat beauty, but may be able to address inconsistency between this pattern matching and SparkListener. Let me try to address this.",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-13T01:16:07Z",
    "diffHunk": "@@ -0,0 +1,235 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  // FIXME: javadoc - caller should provide event log files (either compacted or original)\n+  //  sequentially if the last event log file is already a compacted file, everything\n+  //  will be skipped\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.isEmpty) {\n+      return Seq.empty[FileStatus]\n+    }\n+\n+    // skip everything if the last file is already a compacted file\n+    if (EventLogFileWriter.isCompacted(eventLogFiles.last.getPath)) {\n+      return Seq(eventLogFiles.last)\n+    }\n+\n+    val (filesToCompact, filesToRetain) = findFilesToCompact(eventLogFiles)\n+    if (filesToCompact.isEmpty) {\n+      filesToRetain\n+    } else {\n+      // first pass\n+      val bus = new ReplayListenerBus()\n+\n+      val builders = ServiceLoader.load(classOf[EventFilterBuilder],\n+        Utils.getContextOrSparkClassLoader).asScala\n+      builders.foreach(bus.addListener)\n+\n+      filesToCompact.foreach { log =>\n+        Utils.tryWithResource(EventLogFileReader.openEventLog(log.getPath, fs)) { in =>\n+          bus.replay(in, log.getPath.getName)\n+        }\n+      }\n+\n+      // second pass\n+      val rewriter = new FilteredEventLogFileRewriter(sparkConf, hadoopConf,\n+        filesToCompact, fs, builders.map(_.createFilter()).toSeq)\n+      rewriter.start()\n+      rewriter.rewrite()\n+      rewriter.stop()\n+\n+      // cleanup files which are replaced with new compacted file.\n+      cleanupCompactedFiles(filesToCompact)\n+\n+      fs.getFileStatus(new Path(rewriter.logPath)) :: filesToRetain.toList\n+    }\n+  }\n+\n+  private def cleanupCompactedFiles(files: Seq[FileStatus]): Unit = {\n+    files.foreach { file =>\n+      try {\n+        fs.delete(file.getPath, true)\n+      } catch {\n+        case _: IOException => logWarning(s\"Failed to remove ${file.getPath} / skip removing.\")\n+      }\n+    }\n+  }\n+\n+  private def findFilesToCompact(\n+      eventLogFiles: Seq[FileStatus]): (Seq[FileStatus], Seq[FileStatus]) = {\n+    val lastCompactedFileIdx = eventLogFiles.lastIndexWhere { fs =>\n+      EventLogFileWriter.isCompacted(fs.getPath)\n+    }\n+    val files = eventLogFiles.drop(lastCompactedFileIdx)\n+\n+    if (files.length > maxFilesToRetain) {\n+      (files.dropRight(maxFilesToRetain), files.takeRight(maxFilesToRetain))\n+    } else {\n+      (Seq.empty, files)\n+    }\n+  }\n+}\n+\n+class FilteredEventLogFileRewriter(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    eventLogFiles: Seq[FileStatus],\n+    fs: FileSystem,\n+    filters: Seq[EventFilter]) extends Logging {\n+\n+  require(eventLogFiles.nonEmpty)\n+\n+  private val targetEventLogFilePath = eventLogFiles.last.getPath\n+  private val logWriter: CompactedEventLogFileWriter = new CompactedEventLogFileWriter(\n+    targetEventLogFilePath, \"dummy\", None, targetEventLogFilePath.getParent.toUri,\n+    sparkConf, hadoopConf)\n+\n+  def logPath: String = logWriter.logPath\n+\n+  def start(): Unit = {\n+    logWriter.start()\n+  }\n+\n+  def stop(): Unit = {\n+    logWriter.stop()\n+  }\n+\n+  def rewrite(): Unit = {\n+    eventLogFiles.foreach(rewriteFile)\n+  }\n+\n+  private def rewriteFile(fileStatus: FileStatus): Unit = {\n+    Utils.tryWithResource(EventLogFileReader.openEventLog(fileStatus.getPath, fs)) { in =>\n+      val lines = Source.fromInputStream(in)(Codec.UTF8).getLines()\n+\n+      var currentLine: String = null\n+      var lineNumber: Int = 0\n+\n+      try {\n+        val lineEntries = lines.zipWithIndex\n+\n+        while (lineEntries.hasNext) {\n+          try {\n+            val entry = lineEntries.next()\n+\n+            currentLine = entry._1\n+            lineNumber = entry._2 + 1\n+\n+            val event = JsonProtocol.sparkEventFromJson(parse(currentLine))\n+            if (checkFilters(event)) {\n+              logWriter.writeLine(currentLine)\n+            }\n+          } catch {\n+            // ignore any exception occurred from unidentified json\n+            // just skip handling and write the line\n+            case _: ClassNotFoundException => logWriter.writeLine(currentLine)\n+            case _: UnrecognizedPropertyException => logWriter.writeLine(currentLine)\n+          }\n+        }\n+        true\n+      } catch {\n+        case e: Exception =>\n+          logError(s\"Exception parsing Spark event log: ${fileStatus.getPath.getName}\", e)\n+          logError(s\"Malformed line #$lineNumber: $currentLine\\n\")\n+          throw e\n+      }\n+    }\n+  }\n+\n+  private def checkFilters(event: SparkListenerEvent): Boolean = {\n+    val results = filters.flatMap(filter => applyFilter(filter, event))\n+    results.isEmpty || results.forall(_ == true)\n+  }\n+\n+  private def applyFilter(filter: EventFilter, event: SparkListenerEvent): Option[Boolean] = {\n+    event match {\n+      case event: SparkListenerStageSubmitted => filter.filterStageSubmitted(event)"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Btw even the case we can miss adding filter for newer event types, but we can make sure these lines are not dropped, then that should be OK.",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-13T01:17:51Z",
    "diffHunk": "@@ -0,0 +1,235 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  // FIXME: javadoc - caller should provide event log files (either compacted or original)\n+  //  sequentially if the last event log file is already a compacted file, everything\n+  //  will be skipped\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.isEmpty) {\n+      return Seq.empty[FileStatus]\n+    }\n+\n+    // skip everything if the last file is already a compacted file\n+    if (EventLogFileWriter.isCompacted(eventLogFiles.last.getPath)) {\n+      return Seq(eventLogFiles.last)\n+    }\n+\n+    val (filesToCompact, filesToRetain) = findFilesToCompact(eventLogFiles)\n+    if (filesToCompact.isEmpty) {\n+      filesToRetain\n+    } else {\n+      // first pass\n+      val bus = new ReplayListenerBus()\n+\n+      val builders = ServiceLoader.load(classOf[EventFilterBuilder],\n+        Utils.getContextOrSparkClassLoader).asScala\n+      builders.foreach(bus.addListener)\n+\n+      filesToCompact.foreach { log =>\n+        Utils.tryWithResource(EventLogFileReader.openEventLog(log.getPath, fs)) { in =>\n+          bus.replay(in, log.getPath.getName)\n+        }\n+      }\n+\n+      // second pass\n+      val rewriter = new FilteredEventLogFileRewriter(sparkConf, hadoopConf,\n+        filesToCompact, fs, builders.map(_.createFilter()).toSeq)\n+      rewriter.start()\n+      rewriter.rewrite()\n+      rewriter.stop()\n+\n+      // cleanup files which are replaced with new compacted file.\n+      cleanupCompactedFiles(filesToCompact)\n+\n+      fs.getFileStatus(new Path(rewriter.logPath)) :: filesToRetain.toList\n+    }\n+  }\n+\n+  private def cleanupCompactedFiles(files: Seq[FileStatus]): Unit = {\n+    files.foreach { file =>\n+      try {\n+        fs.delete(file.getPath, true)\n+      } catch {\n+        case _: IOException => logWarning(s\"Failed to remove ${file.getPath} / skip removing.\")\n+      }\n+    }\n+  }\n+\n+  private def findFilesToCompact(\n+      eventLogFiles: Seq[FileStatus]): (Seq[FileStatus], Seq[FileStatus]) = {\n+    val lastCompactedFileIdx = eventLogFiles.lastIndexWhere { fs =>\n+      EventLogFileWriter.isCompacted(fs.getPath)\n+    }\n+    val files = eventLogFiles.drop(lastCompactedFileIdx)\n+\n+    if (files.length > maxFilesToRetain) {\n+      (files.dropRight(maxFilesToRetain), files.takeRight(maxFilesToRetain))\n+    } else {\n+      (Seq.empty, files)\n+    }\n+  }\n+}\n+\n+class FilteredEventLogFileRewriter(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    eventLogFiles: Seq[FileStatus],\n+    fs: FileSystem,\n+    filters: Seq[EventFilter]) extends Logging {\n+\n+  require(eventLogFiles.nonEmpty)\n+\n+  private val targetEventLogFilePath = eventLogFiles.last.getPath\n+  private val logWriter: CompactedEventLogFileWriter = new CompactedEventLogFileWriter(\n+    targetEventLogFilePath, \"dummy\", None, targetEventLogFilePath.getParent.toUri,\n+    sparkConf, hadoopConf)\n+\n+  def logPath: String = logWriter.logPath\n+\n+  def start(): Unit = {\n+    logWriter.start()\n+  }\n+\n+  def stop(): Unit = {\n+    logWriter.stop()\n+  }\n+\n+  def rewrite(): Unit = {\n+    eventLogFiles.foreach(rewriteFile)\n+  }\n+\n+  private def rewriteFile(fileStatus: FileStatus): Unit = {\n+    Utils.tryWithResource(EventLogFileReader.openEventLog(fileStatus.getPath, fs)) { in =>\n+      val lines = Source.fromInputStream(in)(Codec.UTF8).getLines()\n+\n+      var currentLine: String = null\n+      var lineNumber: Int = 0\n+\n+      try {\n+        val lineEntries = lines.zipWithIndex\n+\n+        while (lineEntries.hasNext) {\n+          try {\n+            val entry = lineEntries.next()\n+\n+            currentLine = entry._1\n+            lineNumber = entry._2 + 1\n+\n+            val event = JsonProtocol.sparkEventFromJson(parse(currentLine))\n+            if (checkFilters(event)) {\n+              logWriter.writeLine(currentLine)\n+            }\n+          } catch {\n+            // ignore any exception occurred from unidentified json\n+            // just skip handling and write the line\n+            case _: ClassNotFoundException => logWriter.writeLine(currentLine)\n+            case _: UnrecognizedPropertyException => logWriter.writeLine(currentLine)\n+          }\n+        }\n+        true\n+      } catch {\n+        case e: Exception =>\n+          logError(s\"Exception parsing Spark event log: ${fileStatus.getPath.getName}\", e)\n+          logError(s\"Malformed line #$lineNumber: $currentLine\\n\")\n+          throw e\n+      }\n+    }\n+  }\n+\n+  private def checkFilters(event: SparkListenerEvent): Boolean = {\n+    val results = filters.flatMap(filter => applyFilter(filter, event))\n+    results.isEmpty || results.forall(_ == true)\n+  }\n+\n+  private def applyFilter(filter: EventFilter, event: SparkListenerEvent): Option[Boolean] = {\n+    event match {\n+      case event: SparkListenerStageSubmitted => filter.filterStageSubmitted(event)"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Please consider my last comment here; once this is providing safety I'd just keep it as it is. Injecting SparkListener into this logic makes thing being complicated.",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-13T22:49:54Z",
    "diffHunk": "@@ -0,0 +1,235 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  // FIXME: javadoc - caller should provide event log files (either compacted or original)\n+  //  sequentially if the last event log file is already a compacted file, everything\n+  //  will be skipped\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.isEmpty) {\n+      return Seq.empty[FileStatus]\n+    }\n+\n+    // skip everything if the last file is already a compacted file\n+    if (EventLogFileWriter.isCompacted(eventLogFiles.last.getPath)) {\n+      return Seq(eventLogFiles.last)\n+    }\n+\n+    val (filesToCompact, filesToRetain) = findFilesToCompact(eventLogFiles)\n+    if (filesToCompact.isEmpty) {\n+      filesToRetain\n+    } else {\n+      // first pass\n+      val bus = new ReplayListenerBus()\n+\n+      val builders = ServiceLoader.load(classOf[EventFilterBuilder],\n+        Utils.getContextOrSparkClassLoader).asScala\n+      builders.foreach(bus.addListener)\n+\n+      filesToCompact.foreach { log =>\n+        Utils.tryWithResource(EventLogFileReader.openEventLog(log.getPath, fs)) { in =>\n+          bus.replay(in, log.getPath.getName)\n+        }\n+      }\n+\n+      // second pass\n+      val rewriter = new FilteredEventLogFileRewriter(sparkConf, hadoopConf,\n+        filesToCompact, fs, builders.map(_.createFilter()).toSeq)\n+      rewriter.start()\n+      rewriter.rewrite()\n+      rewriter.stop()\n+\n+      // cleanup files which are replaced with new compacted file.\n+      cleanupCompactedFiles(filesToCompact)\n+\n+      fs.getFileStatus(new Path(rewriter.logPath)) :: filesToRetain.toList\n+    }\n+  }\n+\n+  private def cleanupCompactedFiles(files: Seq[FileStatus]): Unit = {\n+    files.foreach { file =>\n+      try {\n+        fs.delete(file.getPath, true)\n+      } catch {\n+        case _: IOException => logWarning(s\"Failed to remove ${file.getPath} / skip removing.\")\n+      }\n+    }\n+  }\n+\n+  private def findFilesToCompact(\n+      eventLogFiles: Seq[FileStatus]): (Seq[FileStatus], Seq[FileStatus]) = {\n+    val lastCompactedFileIdx = eventLogFiles.lastIndexWhere { fs =>\n+      EventLogFileWriter.isCompacted(fs.getPath)\n+    }\n+    val files = eventLogFiles.drop(lastCompactedFileIdx)\n+\n+    if (files.length > maxFilesToRetain) {\n+      (files.dropRight(maxFilesToRetain), files.takeRight(maxFilesToRetain))\n+    } else {\n+      (Seq.empty, files)\n+    }\n+  }\n+}\n+\n+class FilteredEventLogFileRewriter(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    eventLogFiles: Seq[FileStatus],\n+    fs: FileSystem,\n+    filters: Seq[EventFilter]) extends Logging {\n+\n+  require(eventLogFiles.nonEmpty)\n+\n+  private val targetEventLogFilePath = eventLogFiles.last.getPath\n+  private val logWriter: CompactedEventLogFileWriter = new CompactedEventLogFileWriter(\n+    targetEventLogFilePath, \"dummy\", None, targetEventLogFilePath.getParent.toUri,\n+    sparkConf, hadoopConf)\n+\n+  def logPath: String = logWriter.logPath\n+\n+  def start(): Unit = {\n+    logWriter.start()\n+  }\n+\n+  def stop(): Unit = {\n+    logWriter.stop()\n+  }\n+\n+  def rewrite(): Unit = {\n+    eventLogFiles.foreach(rewriteFile)\n+  }\n+\n+  private def rewriteFile(fileStatus: FileStatus): Unit = {\n+    Utils.tryWithResource(EventLogFileReader.openEventLog(fileStatus.getPath, fs)) { in =>\n+      val lines = Source.fromInputStream(in)(Codec.UTF8).getLines()\n+\n+      var currentLine: String = null\n+      var lineNumber: Int = 0\n+\n+      try {\n+        val lineEntries = lines.zipWithIndex\n+\n+        while (lineEntries.hasNext) {\n+          try {\n+            val entry = lineEntries.next()\n+\n+            currentLine = entry._1\n+            lineNumber = entry._2 + 1\n+\n+            val event = JsonProtocol.sparkEventFromJson(parse(currentLine))\n+            if (checkFilters(event)) {\n+              logWriter.writeLine(currentLine)\n+            }\n+          } catch {\n+            // ignore any exception occurred from unidentified json\n+            // just skip handling and write the line\n+            case _: ClassNotFoundException => logWriter.writeLine(currentLine)\n+            case _: UnrecognizedPropertyException => logWriter.writeLine(currentLine)\n+          }\n+        }\n+        true\n+      } catch {\n+        case e: Exception =>\n+          logError(s\"Exception parsing Spark event log: ${fileStatus.getPath.getName}\", e)\n+          logError(s\"Malformed line #$lineNumber: $currentLine\\n\")\n+          throw e\n+      }\n+    }\n+  }\n+\n+  private def checkFilters(event: SparkListenerEvent): Boolean = {\n+    val results = filters.flatMap(filter => applyFilter(filter, event))\n+    results.isEmpty || results.forall(_ == true)\n+  }\n+\n+  private def applyFilter(filter: EventFilter, event: SparkListenerEvent): Option[Boolean] = {\n+    event match {\n+      case event: SparkListenerStageSubmitted => filter.filterStageSubmitted(event)"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "> we can make sure these lines are not dropped, then that should be OK.\r\n\r\n+1 on this. I've just reached the hearth of the filtering and double checked if `None` returned (which is the default) then it will be kept in the file. Until now not found a test which enforces this (just point it out if I've missed it). In case it doesn't exist maybe we can create `YouShouldNeverUseThisEvent extends SparkListenerEvent` in test and the filter should keep it.",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-14T16:12:57Z",
    "diffHunk": "@@ -0,0 +1,235 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  // FIXME: javadoc - caller should provide event log files (either compacted or original)\n+  //  sequentially if the last event log file is already a compacted file, everything\n+  //  will be skipped\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.isEmpty) {\n+      return Seq.empty[FileStatus]\n+    }\n+\n+    // skip everything if the last file is already a compacted file\n+    if (EventLogFileWriter.isCompacted(eventLogFiles.last.getPath)) {\n+      return Seq(eventLogFiles.last)\n+    }\n+\n+    val (filesToCompact, filesToRetain) = findFilesToCompact(eventLogFiles)\n+    if (filesToCompact.isEmpty) {\n+      filesToRetain\n+    } else {\n+      // first pass\n+      val bus = new ReplayListenerBus()\n+\n+      val builders = ServiceLoader.load(classOf[EventFilterBuilder],\n+        Utils.getContextOrSparkClassLoader).asScala\n+      builders.foreach(bus.addListener)\n+\n+      filesToCompact.foreach { log =>\n+        Utils.tryWithResource(EventLogFileReader.openEventLog(log.getPath, fs)) { in =>\n+          bus.replay(in, log.getPath.getName)\n+        }\n+      }\n+\n+      // second pass\n+      val rewriter = new FilteredEventLogFileRewriter(sparkConf, hadoopConf,\n+        filesToCompact, fs, builders.map(_.createFilter()).toSeq)\n+      rewriter.start()\n+      rewriter.rewrite()\n+      rewriter.stop()\n+\n+      // cleanup files which are replaced with new compacted file.\n+      cleanupCompactedFiles(filesToCompact)\n+\n+      fs.getFileStatus(new Path(rewriter.logPath)) :: filesToRetain.toList\n+    }\n+  }\n+\n+  private def cleanupCompactedFiles(files: Seq[FileStatus]): Unit = {\n+    files.foreach { file =>\n+      try {\n+        fs.delete(file.getPath, true)\n+      } catch {\n+        case _: IOException => logWarning(s\"Failed to remove ${file.getPath} / skip removing.\")\n+      }\n+    }\n+  }\n+\n+  private def findFilesToCompact(\n+      eventLogFiles: Seq[FileStatus]): (Seq[FileStatus], Seq[FileStatus]) = {\n+    val lastCompactedFileIdx = eventLogFiles.lastIndexWhere { fs =>\n+      EventLogFileWriter.isCompacted(fs.getPath)\n+    }\n+    val files = eventLogFiles.drop(lastCompactedFileIdx)\n+\n+    if (files.length > maxFilesToRetain) {\n+      (files.dropRight(maxFilesToRetain), files.takeRight(maxFilesToRetain))\n+    } else {\n+      (Seq.empty, files)\n+    }\n+  }\n+}\n+\n+class FilteredEventLogFileRewriter(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    eventLogFiles: Seq[FileStatus],\n+    fs: FileSystem,\n+    filters: Seq[EventFilter]) extends Logging {\n+\n+  require(eventLogFiles.nonEmpty)\n+\n+  private val targetEventLogFilePath = eventLogFiles.last.getPath\n+  private val logWriter: CompactedEventLogFileWriter = new CompactedEventLogFileWriter(\n+    targetEventLogFilePath, \"dummy\", None, targetEventLogFilePath.getParent.toUri,\n+    sparkConf, hadoopConf)\n+\n+  def logPath: String = logWriter.logPath\n+\n+  def start(): Unit = {\n+    logWriter.start()\n+  }\n+\n+  def stop(): Unit = {\n+    logWriter.stop()\n+  }\n+\n+  def rewrite(): Unit = {\n+    eventLogFiles.foreach(rewriteFile)\n+  }\n+\n+  private def rewriteFile(fileStatus: FileStatus): Unit = {\n+    Utils.tryWithResource(EventLogFileReader.openEventLog(fileStatus.getPath, fs)) { in =>\n+      val lines = Source.fromInputStream(in)(Codec.UTF8).getLines()\n+\n+      var currentLine: String = null\n+      var lineNumber: Int = 0\n+\n+      try {\n+        val lineEntries = lines.zipWithIndex\n+\n+        while (lineEntries.hasNext) {\n+          try {\n+            val entry = lineEntries.next()\n+\n+            currentLine = entry._1\n+            lineNumber = entry._2 + 1\n+\n+            val event = JsonProtocol.sparkEventFromJson(parse(currentLine))\n+            if (checkFilters(event)) {\n+              logWriter.writeLine(currentLine)\n+            }\n+          } catch {\n+            // ignore any exception occurred from unidentified json\n+            // just skip handling and write the line\n+            case _: ClassNotFoundException => logWriter.writeLine(currentLine)\n+            case _: UnrecognizedPropertyException => logWriter.writeLine(currentLine)\n+          }\n+        }\n+        true\n+      } catch {\n+        case e: Exception =>\n+          logError(s\"Exception parsing Spark event log: ${fileStatus.getPath.getName}\", e)\n+          logError(s\"Malformed line #$lineNumber: $currentLine\\n\")\n+          throw e\n+      }\n+    }\n+  }\n+\n+  private def checkFilters(event: SparkListenerEvent): Boolean = {\n+    val results = filters.flatMap(filter => applyFilter(filter, event))\n+    results.isEmpty || results.forall(_ == true)\n+  }\n+\n+  private def applyFilter(filter: EventFilter, event: SparkListenerEvent): Option[Boolean] = {\n+    event match {\n+      case event: SparkListenerStageSubmitted => filter.filterStageSubmitted(event)"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Please check the UT of FilteredEventLogFileRewriterSuite out, it covers None & None.",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-15T03:28:08Z",
    "diffHunk": "@@ -0,0 +1,235 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  // FIXME: javadoc - caller should provide event log files (either compacted or original)\n+  //  sequentially if the last event log file is already a compacted file, everything\n+  //  will be skipped\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.isEmpty) {\n+      return Seq.empty[FileStatus]\n+    }\n+\n+    // skip everything if the last file is already a compacted file\n+    if (EventLogFileWriter.isCompacted(eventLogFiles.last.getPath)) {\n+      return Seq(eventLogFiles.last)\n+    }\n+\n+    val (filesToCompact, filesToRetain) = findFilesToCompact(eventLogFiles)\n+    if (filesToCompact.isEmpty) {\n+      filesToRetain\n+    } else {\n+      // first pass\n+      val bus = new ReplayListenerBus()\n+\n+      val builders = ServiceLoader.load(classOf[EventFilterBuilder],\n+        Utils.getContextOrSparkClassLoader).asScala\n+      builders.foreach(bus.addListener)\n+\n+      filesToCompact.foreach { log =>\n+        Utils.tryWithResource(EventLogFileReader.openEventLog(log.getPath, fs)) { in =>\n+          bus.replay(in, log.getPath.getName)\n+        }\n+      }\n+\n+      // second pass\n+      val rewriter = new FilteredEventLogFileRewriter(sparkConf, hadoopConf,\n+        filesToCompact, fs, builders.map(_.createFilter()).toSeq)\n+      rewriter.start()\n+      rewriter.rewrite()\n+      rewriter.stop()\n+\n+      // cleanup files which are replaced with new compacted file.\n+      cleanupCompactedFiles(filesToCompact)\n+\n+      fs.getFileStatus(new Path(rewriter.logPath)) :: filesToRetain.toList\n+    }\n+  }\n+\n+  private def cleanupCompactedFiles(files: Seq[FileStatus]): Unit = {\n+    files.foreach { file =>\n+      try {\n+        fs.delete(file.getPath, true)\n+      } catch {\n+        case _: IOException => logWarning(s\"Failed to remove ${file.getPath} / skip removing.\")\n+      }\n+    }\n+  }\n+\n+  private def findFilesToCompact(\n+      eventLogFiles: Seq[FileStatus]): (Seq[FileStatus], Seq[FileStatus]) = {\n+    val lastCompactedFileIdx = eventLogFiles.lastIndexWhere { fs =>\n+      EventLogFileWriter.isCompacted(fs.getPath)\n+    }\n+    val files = eventLogFiles.drop(lastCompactedFileIdx)\n+\n+    if (files.length > maxFilesToRetain) {\n+      (files.dropRight(maxFilesToRetain), files.takeRight(maxFilesToRetain))\n+    } else {\n+      (Seq.empty, files)\n+    }\n+  }\n+}\n+\n+class FilteredEventLogFileRewriter(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    eventLogFiles: Seq[FileStatus],\n+    fs: FileSystem,\n+    filters: Seq[EventFilter]) extends Logging {\n+\n+  require(eventLogFiles.nonEmpty)\n+\n+  private val targetEventLogFilePath = eventLogFiles.last.getPath\n+  private val logWriter: CompactedEventLogFileWriter = new CompactedEventLogFileWriter(\n+    targetEventLogFilePath, \"dummy\", None, targetEventLogFilePath.getParent.toUri,\n+    sparkConf, hadoopConf)\n+\n+  def logPath: String = logWriter.logPath\n+\n+  def start(): Unit = {\n+    logWriter.start()\n+  }\n+\n+  def stop(): Unit = {\n+    logWriter.stop()\n+  }\n+\n+  def rewrite(): Unit = {\n+    eventLogFiles.foreach(rewriteFile)\n+  }\n+\n+  private def rewriteFile(fileStatus: FileStatus): Unit = {\n+    Utils.tryWithResource(EventLogFileReader.openEventLog(fileStatus.getPath, fs)) { in =>\n+      val lines = Source.fromInputStream(in)(Codec.UTF8).getLines()\n+\n+      var currentLine: String = null\n+      var lineNumber: Int = 0\n+\n+      try {\n+        val lineEntries = lines.zipWithIndex\n+\n+        while (lineEntries.hasNext) {\n+          try {\n+            val entry = lineEntries.next()\n+\n+            currentLine = entry._1\n+            lineNumber = entry._2 + 1\n+\n+            val event = JsonProtocol.sparkEventFromJson(parse(currentLine))\n+            if (checkFilters(event)) {\n+              logWriter.writeLine(currentLine)\n+            }\n+          } catch {\n+            // ignore any exception occurred from unidentified json\n+            // just skip handling and write the line\n+            case _: ClassNotFoundException => logWriter.writeLine(currentLine)\n+            case _: UnrecognizedPropertyException => logWriter.writeLine(currentLine)\n+          }\n+        }\n+        true\n+      } catch {\n+        case e: Exception =>\n+          logError(s\"Exception parsing Spark event log: ${fileStatus.getPath.getName}\", e)\n+          logError(s\"Malformed line #$lineNumber: $currentLine\\n\")\n+          throw e\n+      }\n+    }\n+  }\n+\n+  private def checkFilters(event: SparkListenerEvent): Boolean = {\n+    val results = filters.flatMap(filter => applyFilter(filter, event))\n+    results.isEmpty || results.forall(_ == true)\n+  }\n+\n+  private def applyFilter(filter: EventFilter, event: SparkListenerEvent): Option[Boolean] = {\n+    event match {\n+      case event: SparkListenerStageSubmitted => filter.filterStageSubmitted(event)"
  }],
  "prId": 26416
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "As I see `toUri` is used several places so only adding my comment here not to bomb the PR.\r\nThere was an issue with `toUri` several times which still stands:\r\n```\r\nscala> new org.apache.hadoop.fs.Path(\"/.../chk %@#chk\").toUri\r\nres0: java.net.URI = /.../chk%20%25@%23chk\r\n```\r\n",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-14T14:23:27Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+/**\n+ * This class compacts the old event log files into one compact file, via two phases reading:\n+ *\n+ * 1) Initialize available [[EventFilterBuilder]] instances, and replay the old event log files with\n+ * builders, so that these builders can gather the information to create [[EventFilter]] instances.\n+ * 2) Initialize [[EventFilter]] instances from [[EventFilterBuilder]] instances, and replay the\n+ * old event log files with filters. Rewrite the content to the compact file if the filters decide\n+ * to filter in.\n+ *\n+ * This class assumes caller will provide the sorted list of files which are sorted by the index of\n+ * event log file - caller should keep in mind that this class doesn't care about the semantic of\n+ * ordering.\n+ *\n+ * When compacting the files, the range of compaction for given file list is determined as:\n+ * (rightmost compact file ~ the file where there're `maxFilesToRetain` files on the right side)\n+ *\n+ * If there's no compact file in the list, it starts from the first file. If there're not enough\n+ * files after rightmost compact file, compaction will be skipped.\n+ */\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.length <= maxFilesToRetain) {\n+      return eventLogFiles\n+    }\n+\n+    if (EventLogFileWriter.isCompacted(eventLogFiles.last.getPath)) {\n+      return Seq(eventLogFiles.last)\n+    }\n+\n+    val (filesToCompact, filesToRetain) = findFilesToCompact(eventLogFiles)\n+    if (filesToCompact.isEmpty) {\n+      filesToRetain\n+    } else {\n+      val bus = new ReplayListenerBus()\n+\n+      val builders = ServiceLoader.load(classOf[EventFilterBuilder],\n+        Utils.getContextOrSparkClassLoader).asScala.toSeq\n+      builders.foreach(bus.addListener)\n+\n+      filesToCompact.foreach { log =>\n+        Utils.tryWithResource(EventLogFileReader.openEventLog(log.getPath, fs)) { in =>\n+          bus.replay(in, log.getPath.getName)\n+        }\n+      }\n+\n+      val rewriter = new FilteredEventLogFileRewriter(sparkConf, hadoopConf, fs,\n+        builders.map(_.createFilter()))\n+      val compactedPath = rewriter.rewrite(filesToCompact)\n+\n+      cleanupCompactedFiles(filesToCompact)\n+\n+      fs.getFileStatus(new Path(compactedPath)) :: filesToRetain.toList\n+    }\n+  }\n+\n+  private def cleanupCompactedFiles(files: Seq[FileStatus]): Unit = {\n+    files.foreach { file =>\n+      var deleted = false\n+      try {\n+        deleted = fs.delete(file.getPath, true)\n+      } catch {\n+        case _: IOException =>\n+      }\n+      if (!deleted) {\n+        logWarning(s\"Failed to remove ${file.getPath} / skip removing.\")\n+      }\n+    }\n+  }\n+\n+  private def findFilesToCompact(\n+      eventLogFiles: Seq[FileStatus]): (Seq[FileStatus], Seq[FileStatus]) = {\n+    val files = RollingEventLogFilesFileReader.dropBeforeLastCompactFile(eventLogFiles)\n+    if (files.length > maxFilesToRetain) {\n+      (files.dropRight(maxFilesToRetain), files.takeRight(maxFilesToRetain))\n+    } else {\n+      (Seq.empty, files)\n+    }\n+  }\n+}\n+\n+/**\n+ * This class rewrites the event log files into one compact file: the compact file will only\n+ * contain the events which pass the filters. Events will be filtered out only when all filters\n+ * decide to filter out the event or don't mind about the event. Otherwise, the original line for\n+ * the event is written to the compact file as it is.\n+ */\n+class FilteredEventLogFileRewriter(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem,\n+    filters: Seq[EventFilter]) extends Logging {\n+\n+  def rewrite(eventLogFiles: Seq[FileStatus]): String = {\n+    require(eventLogFiles.nonEmpty)\n+\n+    val targetEventLogFilePath = eventLogFiles.last.getPath\n+    val logWriter: CompactedEventLogFileWriter = new CompactedEventLogFileWriter(\n+      targetEventLogFilePath, \"dummy\", None, targetEventLogFilePath.getParent.toUri,"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "So that should be existing issue if the URI is the thing we don't feel safe, and I'm not totally sure how it would hurt.\r\n\r\n> branch-2.4\r\n\r\nhttps://github.com/apache/spark/blob/7bdc76f9c1f062aecdaf361527adbd9f50518bc9/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala#L52-L58\r\n\r\nI feel it worths to file a new issue and address from there, not only for event logging but also for every places.",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-15T03:16:55Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+/**\n+ * This class compacts the old event log files into one compact file, via two phases reading:\n+ *\n+ * 1) Initialize available [[EventFilterBuilder]] instances, and replay the old event log files with\n+ * builders, so that these builders can gather the information to create [[EventFilter]] instances.\n+ * 2) Initialize [[EventFilter]] instances from [[EventFilterBuilder]] instances, and replay the\n+ * old event log files with filters. Rewrite the content to the compact file if the filters decide\n+ * to filter in.\n+ *\n+ * This class assumes caller will provide the sorted list of files which are sorted by the index of\n+ * event log file - caller should keep in mind that this class doesn't care about the semantic of\n+ * ordering.\n+ *\n+ * When compacting the files, the range of compaction for given file list is determined as:\n+ * (rightmost compact file ~ the file where there're `maxFilesToRetain` files on the right side)\n+ *\n+ * If there's no compact file in the list, it starts from the first file. If there're not enough\n+ * files after rightmost compact file, compaction will be skipped.\n+ */\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.length <= maxFilesToRetain) {\n+      return eventLogFiles\n+    }\n+\n+    if (EventLogFileWriter.isCompacted(eventLogFiles.last.getPath)) {\n+      return Seq(eventLogFiles.last)\n+    }\n+\n+    val (filesToCompact, filesToRetain) = findFilesToCompact(eventLogFiles)\n+    if (filesToCompact.isEmpty) {\n+      filesToRetain\n+    } else {\n+      val bus = new ReplayListenerBus()\n+\n+      val builders = ServiceLoader.load(classOf[EventFilterBuilder],\n+        Utils.getContextOrSparkClassLoader).asScala.toSeq\n+      builders.foreach(bus.addListener)\n+\n+      filesToCompact.foreach { log =>\n+        Utils.tryWithResource(EventLogFileReader.openEventLog(log.getPath, fs)) { in =>\n+          bus.replay(in, log.getPath.getName)\n+        }\n+      }\n+\n+      val rewriter = new FilteredEventLogFileRewriter(sparkConf, hadoopConf, fs,\n+        builders.map(_.createFilter()))\n+      val compactedPath = rewriter.rewrite(filesToCompact)\n+\n+      cleanupCompactedFiles(filesToCompact)\n+\n+      fs.getFileStatus(new Path(compactedPath)) :: filesToRetain.toList\n+    }\n+  }\n+\n+  private def cleanupCompactedFiles(files: Seq[FileStatus]): Unit = {\n+    files.foreach { file =>\n+      var deleted = false\n+      try {\n+        deleted = fs.delete(file.getPath, true)\n+      } catch {\n+        case _: IOException =>\n+      }\n+      if (!deleted) {\n+        logWarning(s\"Failed to remove ${file.getPath} / skip removing.\")\n+      }\n+    }\n+  }\n+\n+  private def findFilesToCompact(\n+      eventLogFiles: Seq[FileStatus]): (Seq[FileStatus], Seq[FileStatus]) = {\n+    val files = RollingEventLogFilesFileReader.dropBeforeLastCompactFile(eventLogFiles)\n+    if (files.length > maxFilesToRetain) {\n+      (files.dropRight(maxFilesToRetain), files.takeRight(maxFilesToRetain))\n+    } else {\n+      (Seq.empty, files)\n+    }\n+  }\n+}\n+\n+/**\n+ * This class rewrites the event log files into one compact file: the compact file will only\n+ * contain the events which pass the filters. Events will be filtered out only when all filters\n+ * decide to filter out the event or don't mind about the event. Otherwise, the original line for\n+ * the event is written to the compact file as it is.\n+ */\n+class FilteredEventLogFileRewriter(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem,\n+    filters: Seq[EventFilter]) extends Logging {\n+\n+  def rewrite(eventLogFiles: Seq[FileStatus]): String = {\n+    require(eventLogFiles.nonEmpty)\n+\n+    val targetEventLogFilePath = eventLogFiles.last.getPath\n+    val logWriter: CompactedEventLogFileWriter = new CompactedEventLogFileWriter(\n+      targetEventLogFilePath, \"dummy\", None, targetEventLogFilePath.getParent.toUri,"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "> I'm not totally sure how it would hurt\r\n\r\nIt can create directories which are not meant by the user. In the example the user expects directory/file `/.../chk %@#chk` but `/.../chk%20%25@%23chk` created. If the `toUri` is consistently used this is the \"only\" problem (for ex. https://github.com/apache/spark/pull/23733).\r\n\r\nIf in some places it's used but not on other places it can that maybe SHS can't read things back which was written out (not checked just giving an example).\r\n",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-15T12:08:36Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+/**\n+ * This class compacts the old event log files into one compact file, via two phases reading:\n+ *\n+ * 1) Initialize available [[EventFilterBuilder]] instances, and replay the old event log files with\n+ * builders, so that these builders can gather the information to create [[EventFilter]] instances.\n+ * 2) Initialize [[EventFilter]] instances from [[EventFilterBuilder]] instances, and replay the\n+ * old event log files with filters. Rewrite the content to the compact file if the filters decide\n+ * to filter in.\n+ *\n+ * This class assumes caller will provide the sorted list of files which are sorted by the index of\n+ * event log file - caller should keep in mind that this class doesn't care about the semantic of\n+ * ordering.\n+ *\n+ * When compacting the files, the range of compaction for given file list is determined as:\n+ * (rightmost compact file ~ the file where there're `maxFilesToRetain` files on the right side)\n+ *\n+ * If there's no compact file in the list, it starts from the first file. If there're not enough\n+ * files after rightmost compact file, compaction will be skipped.\n+ */\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.length <= maxFilesToRetain) {\n+      return eventLogFiles\n+    }\n+\n+    if (EventLogFileWriter.isCompacted(eventLogFiles.last.getPath)) {\n+      return Seq(eventLogFiles.last)\n+    }\n+\n+    val (filesToCompact, filesToRetain) = findFilesToCompact(eventLogFiles)\n+    if (filesToCompact.isEmpty) {\n+      filesToRetain\n+    } else {\n+      val bus = new ReplayListenerBus()\n+\n+      val builders = ServiceLoader.load(classOf[EventFilterBuilder],\n+        Utils.getContextOrSparkClassLoader).asScala.toSeq\n+      builders.foreach(bus.addListener)\n+\n+      filesToCompact.foreach { log =>\n+        Utils.tryWithResource(EventLogFileReader.openEventLog(log.getPath, fs)) { in =>\n+          bus.replay(in, log.getPath.getName)\n+        }\n+      }\n+\n+      val rewriter = new FilteredEventLogFileRewriter(sparkConf, hadoopConf, fs,\n+        builders.map(_.createFilter()))\n+      val compactedPath = rewriter.rewrite(filesToCompact)\n+\n+      cleanupCompactedFiles(filesToCompact)\n+\n+      fs.getFileStatus(new Path(compactedPath)) :: filesToRetain.toList\n+    }\n+  }\n+\n+  private def cleanupCompactedFiles(files: Seq[FileStatus]): Unit = {\n+    files.foreach { file =>\n+      var deleted = false\n+      try {\n+        deleted = fs.delete(file.getPath, true)\n+      } catch {\n+        case _: IOException =>\n+      }\n+      if (!deleted) {\n+        logWarning(s\"Failed to remove ${file.getPath} / skip removing.\")\n+      }\n+    }\n+  }\n+\n+  private def findFilesToCompact(\n+      eventLogFiles: Seq[FileStatus]): (Seq[FileStatus], Seq[FileStatus]) = {\n+    val files = RollingEventLogFilesFileReader.dropBeforeLastCompactFile(eventLogFiles)\n+    if (files.length > maxFilesToRetain) {\n+      (files.dropRight(maxFilesToRetain), files.takeRight(maxFilesToRetain))\n+    } else {\n+      (Seq.empty, files)\n+    }\n+  }\n+}\n+\n+/**\n+ * This class rewrites the event log files into one compact file: the compact file will only\n+ * contain the events which pass the filters. Events will be filtered out only when all filters\n+ * decide to filter out the event or don't mind about the event. Otherwise, the original line for\n+ * the event is written to the compact file as it is.\n+ */\n+class FilteredEventLogFileRewriter(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem,\n+    filters: Seq[EventFilter]) extends Logging {\n+\n+  def rewrite(eventLogFiles: Seq[FileStatus]): String = {\n+    require(eventLogFiles.nonEmpty)\n+\n+    val targetEventLogFilePath = eventLogFiles.last.getPath\n+    val logWriter: CompactedEventLogFileWriter = new CompactedEventLogFileWriter(\n+      targetEventLogFilePath, \"dummy\", None, targetEventLogFilePath.getParent.toUri,"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Since this exists in previous versions it makes sense to handle it in a different jira to keep the focus.",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-15T12:25:28Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+/**\n+ * This class compacts the old event log files into one compact file, via two phases reading:\n+ *\n+ * 1) Initialize available [[EventFilterBuilder]] instances, and replay the old event log files with\n+ * builders, so that these builders can gather the information to create [[EventFilter]] instances.\n+ * 2) Initialize [[EventFilter]] instances from [[EventFilterBuilder]] instances, and replay the\n+ * old event log files with filters. Rewrite the content to the compact file if the filters decide\n+ * to filter in.\n+ *\n+ * This class assumes caller will provide the sorted list of files which are sorted by the index of\n+ * event log file - caller should keep in mind that this class doesn't care about the semantic of\n+ * ordering.\n+ *\n+ * When compacting the files, the range of compaction for given file list is determined as:\n+ * (rightmost compact file ~ the file where there're `maxFilesToRetain` files on the right side)\n+ *\n+ * If there's no compact file in the list, it starts from the first file. If there're not enough\n+ * files after rightmost compact file, compaction will be skipped.\n+ */\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.length <= maxFilesToRetain) {\n+      return eventLogFiles\n+    }\n+\n+    if (EventLogFileWriter.isCompacted(eventLogFiles.last.getPath)) {\n+      return Seq(eventLogFiles.last)\n+    }\n+\n+    val (filesToCompact, filesToRetain) = findFilesToCompact(eventLogFiles)\n+    if (filesToCompact.isEmpty) {\n+      filesToRetain\n+    } else {\n+      val bus = new ReplayListenerBus()\n+\n+      val builders = ServiceLoader.load(classOf[EventFilterBuilder],\n+        Utils.getContextOrSparkClassLoader).asScala.toSeq\n+      builders.foreach(bus.addListener)\n+\n+      filesToCompact.foreach { log =>\n+        Utils.tryWithResource(EventLogFileReader.openEventLog(log.getPath, fs)) { in =>\n+          bus.replay(in, log.getPath.getName)\n+        }\n+      }\n+\n+      val rewriter = new FilteredEventLogFileRewriter(sparkConf, hadoopConf, fs,\n+        builders.map(_.createFilter()))\n+      val compactedPath = rewriter.rewrite(filesToCompact)\n+\n+      cleanupCompactedFiles(filesToCompact)\n+\n+      fs.getFileStatus(new Path(compactedPath)) :: filesToRetain.toList\n+    }\n+  }\n+\n+  private def cleanupCompactedFiles(files: Seq[FileStatus]): Unit = {\n+    files.foreach { file =>\n+      var deleted = false\n+      try {\n+        deleted = fs.delete(file.getPath, true)\n+      } catch {\n+        case _: IOException =>\n+      }\n+      if (!deleted) {\n+        logWarning(s\"Failed to remove ${file.getPath} / skip removing.\")\n+      }\n+    }\n+  }\n+\n+  private def findFilesToCompact(\n+      eventLogFiles: Seq[FileStatus]): (Seq[FileStatus], Seq[FileStatus]) = {\n+    val files = RollingEventLogFilesFileReader.dropBeforeLastCompactFile(eventLogFiles)\n+    if (files.length > maxFilesToRetain) {\n+      (files.dropRight(maxFilesToRetain), files.takeRight(maxFilesToRetain))\n+    } else {\n+      (Seq.empty, files)\n+    }\n+  }\n+}\n+\n+/**\n+ * This class rewrites the event log files into one compact file: the compact file will only\n+ * contain the events which pass the filters. Events will be filtered out only when all filters\n+ * decide to filter out the event or don't mind about the event. Otherwise, the original line for\n+ * the event is written to the compact file as it is.\n+ */\n+class FilteredEventLogFileRewriter(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem,\n+    filters: Seq[EventFilter]) extends Logging {\n+\n+  def rewrite(eventLogFiles: Seq[FileStatus]): String = {\n+    require(eventLogFiles.nonEmpty)\n+\n+    val targetEventLogFilePath = eventLogFiles.last.getPath\n+    val logWriter: CompactedEventLogFileWriter = new CompactedEventLogFileWriter(\n+      targetEventLogFilePath, \"dummy\", None, targetEventLogFilePath.getParent.toUri,"
  }],
  "prId": 26416
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Nit: `eventLogFiles.foreach(file => rewriteFile(logWriter, file))`",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-14T14:26:55Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+/**\n+ * This class compacts the old event log files into one compact file, via two phases reading:\n+ *\n+ * 1) Initialize available [[EventFilterBuilder]] instances, and replay the old event log files with\n+ * builders, so that these builders can gather the information to create [[EventFilter]] instances.\n+ * 2) Initialize [[EventFilter]] instances from [[EventFilterBuilder]] instances, and replay the\n+ * old event log files with filters. Rewrite the content to the compact file if the filters decide\n+ * to filter in.\n+ *\n+ * This class assumes caller will provide the sorted list of files which are sorted by the index of\n+ * event log file - caller should keep in mind that this class doesn't care about the semantic of\n+ * ordering.\n+ *\n+ * When compacting the files, the range of compaction for given file list is determined as:\n+ * (rightmost compact file ~ the file where there're `maxFilesToRetain` files on the right side)\n+ *\n+ * If there's no compact file in the list, it starts from the first file. If there're not enough\n+ * files after rightmost compact file, compaction will be skipped.\n+ */\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.length <= maxFilesToRetain) {\n+      return eventLogFiles\n+    }\n+\n+    if (EventLogFileWriter.isCompacted(eventLogFiles.last.getPath)) {\n+      return Seq(eventLogFiles.last)\n+    }\n+\n+    val (filesToCompact, filesToRetain) = findFilesToCompact(eventLogFiles)\n+    if (filesToCompact.isEmpty) {\n+      filesToRetain\n+    } else {\n+      val bus = new ReplayListenerBus()\n+\n+      val builders = ServiceLoader.load(classOf[EventFilterBuilder],\n+        Utils.getContextOrSparkClassLoader).asScala.toSeq\n+      builders.foreach(bus.addListener)\n+\n+      filesToCompact.foreach { log =>\n+        Utils.tryWithResource(EventLogFileReader.openEventLog(log.getPath, fs)) { in =>\n+          bus.replay(in, log.getPath.getName)\n+        }\n+      }\n+\n+      val rewriter = new FilteredEventLogFileRewriter(sparkConf, hadoopConf, fs,\n+        builders.map(_.createFilter()))\n+      val compactedPath = rewriter.rewrite(filesToCompact)\n+\n+      cleanupCompactedFiles(filesToCompact)\n+\n+      fs.getFileStatus(new Path(compactedPath)) :: filesToRetain.toList\n+    }\n+  }\n+\n+  private def cleanupCompactedFiles(files: Seq[FileStatus]): Unit = {\n+    files.foreach { file =>\n+      var deleted = false\n+      try {\n+        deleted = fs.delete(file.getPath, true)\n+      } catch {\n+        case _: IOException =>\n+      }\n+      if (!deleted) {\n+        logWarning(s\"Failed to remove ${file.getPath} / skip removing.\")\n+      }\n+    }\n+  }\n+\n+  private def findFilesToCompact(\n+      eventLogFiles: Seq[FileStatus]): (Seq[FileStatus], Seq[FileStatus]) = {\n+    val files = RollingEventLogFilesFileReader.dropBeforeLastCompactFile(eventLogFiles)\n+    if (files.length > maxFilesToRetain) {\n+      (files.dropRight(maxFilesToRetain), files.takeRight(maxFilesToRetain))\n+    } else {\n+      (Seq.empty, files)\n+    }\n+  }\n+}\n+\n+/**\n+ * This class rewrites the event log files into one compact file: the compact file will only\n+ * contain the events which pass the filters. Events will be filtered out only when all filters\n+ * decide to filter out the event or don't mind about the event. Otherwise, the original line for\n+ * the event is written to the compact file as it is.\n+ */\n+class FilteredEventLogFileRewriter(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem,\n+    filters: Seq[EventFilter]) extends Logging {\n+\n+  def rewrite(eventLogFiles: Seq[FileStatus]): String = {\n+    require(eventLogFiles.nonEmpty)\n+\n+    val targetEventLogFilePath = eventLogFiles.last.getPath\n+    val logWriter: CompactedEventLogFileWriter = new CompactedEventLogFileWriter(\n+      targetEventLogFilePath, \"dummy\", None, targetEventLogFilePath.getParent.toUri,\n+      sparkConf, hadoopConf)\n+\n+    logWriter.start()\n+    eventLogFiles.foreach { file => rewriteFile(logWriter, file) }"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "That (use `{ }`) has been the suggestion @vanzin provided over the bunch of previous PRs.",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-15T03:18:14Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+/**\n+ * This class compacts the old event log files into one compact file, via two phases reading:\n+ *\n+ * 1) Initialize available [[EventFilterBuilder]] instances, and replay the old event log files with\n+ * builders, so that these builders can gather the information to create [[EventFilter]] instances.\n+ * 2) Initialize [[EventFilter]] instances from [[EventFilterBuilder]] instances, and replay the\n+ * old event log files with filters. Rewrite the content to the compact file if the filters decide\n+ * to filter in.\n+ *\n+ * This class assumes caller will provide the sorted list of files which are sorted by the index of\n+ * event log file - caller should keep in mind that this class doesn't care about the semantic of\n+ * ordering.\n+ *\n+ * When compacting the files, the range of compaction for given file list is determined as:\n+ * (rightmost compact file ~ the file where there're `maxFilesToRetain` files on the right side)\n+ *\n+ * If there's no compact file in the list, it starts from the first file. If there're not enough\n+ * files after rightmost compact file, compaction will be skipped.\n+ */\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.length <= maxFilesToRetain) {\n+      return eventLogFiles\n+    }\n+\n+    if (EventLogFileWriter.isCompacted(eventLogFiles.last.getPath)) {\n+      return Seq(eventLogFiles.last)\n+    }\n+\n+    val (filesToCompact, filesToRetain) = findFilesToCompact(eventLogFiles)\n+    if (filesToCompact.isEmpty) {\n+      filesToRetain\n+    } else {\n+      val bus = new ReplayListenerBus()\n+\n+      val builders = ServiceLoader.load(classOf[EventFilterBuilder],\n+        Utils.getContextOrSparkClassLoader).asScala.toSeq\n+      builders.foreach(bus.addListener)\n+\n+      filesToCompact.foreach { log =>\n+        Utils.tryWithResource(EventLogFileReader.openEventLog(log.getPath, fs)) { in =>\n+          bus.replay(in, log.getPath.getName)\n+        }\n+      }\n+\n+      val rewriter = new FilteredEventLogFileRewriter(sparkConf, hadoopConf, fs,\n+        builders.map(_.createFilter()))\n+      val compactedPath = rewriter.rewrite(filesToCompact)\n+\n+      cleanupCompactedFiles(filesToCompact)\n+\n+      fs.getFileStatus(new Path(compactedPath)) :: filesToRetain.toList\n+    }\n+  }\n+\n+  private def cleanupCompactedFiles(files: Seq[FileStatus]): Unit = {\n+    files.foreach { file =>\n+      var deleted = false\n+      try {\n+        deleted = fs.delete(file.getPath, true)\n+      } catch {\n+        case _: IOException =>\n+      }\n+      if (!deleted) {\n+        logWarning(s\"Failed to remove ${file.getPath} / skip removing.\")\n+      }\n+    }\n+  }\n+\n+  private def findFilesToCompact(\n+      eventLogFiles: Seq[FileStatus]): (Seq[FileStatus], Seq[FileStatus]) = {\n+    val files = RollingEventLogFilesFileReader.dropBeforeLastCompactFile(eventLogFiles)\n+    if (files.length > maxFilesToRetain) {\n+      (files.dropRight(maxFilesToRetain), files.takeRight(maxFilesToRetain))\n+    } else {\n+      (Seq.empty, files)\n+    }\n+  }\n+}\n+\n+/**\n+ * This class rewrites the event log files into one compact file: the compact file will only\n+ * contain the events which pass the filters. Events will be filtered out only when all filters\n+ * decide to filter out the event or don't mind about the event. Otherwise, the original line for\n+ * the event is written to the compact file as it is.\n+ */\n+class FilteredEventLogFileRewriter(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem,\n+    filters: Seq[EventFilter]) extends Logging {\n+\n+  def rewrite(eventLogFiles: Seq[FileStatus]): String = {\n+    require(eventLogFiles.nonEmpty)\n+\n+    val targetEventLogFilePath = eventLogFiles.last.getPath\n+    val logWriter: CompactedEventLogFileWriter = new CompactedEventLogFileWriter(\n+      targetEventLogFilePath, \"dummy\", None, targetEventLogFilePath.getParent.toUri,\n+      sparkConf, hadoopConf)\n+\n+    logWriter.start()\n+    eventLogFiles.foreach { file => rewriteFile(logWriter, file) }"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Aha, so that's not considered small logic then.",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-15T11:24:09Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+/**\n+ * This class compacts the old event log files into one compact file, via two phases reading:\n+ *\n+ * 1) Initialize available [[EventFilterBuilder]] instances, and replay the old event log files with\n+ * builders, so that these builders can gather the information to create [[EventFilter]] instances.\n+ * 2) Initialize [[EventFilter]] instances from [[EventFilterBuilder]] instances, and replay the\n+ * old event log files with filters. Rewrite the content to the compact file if the filters decide\n+ * to filter in.\n+ *\n+ * This class assumes caller will provide the sorted list of files which are sorted by the index of\n+ * event log file - caller should keep in mind that this class doesn't care about the semantic of\n+ * ordering.\n+ *\n+ * When compacting the files, the range of compaction for given file list is determined as:\n+ * (rightmost compact file ~ the file where there're `maxFilesToRetain` files on the right side)\n+ *\n+ * If there's no compact file in the list, it starts from the first file. If there're not enough\n+ * files after rightmost compact file, compaction will be skipped.\n+ */\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.length <= maxFilesToRetain) {\n+      return eventLogFiles\n+    }\n+\n+    if (EventLogFileWriter.isCompacted(eventLogFiles.last.getPath)) {\n+      return Seq(eventLogFiles.last)\n+    }\n+\n+    val (filesToCompact, filesToRetain) = findFilesToCompact(eventLogFiles)\n+    if (filesToCompact.isEmpty) {\n+      filesToRetain\n+    } else {\n+      val bus = new ReplayListenerBus()\n+\n+      val builders = ServiceLoader.load(classOf[EventFilterBuilder],\n+        Utils.getContextOrSparkClassLoader).asScala.toSeq\n+      builders.foreach(bus.addListener)\n+\n+      filesToCompact.foreach { log =>\n+        Utils.tryWithResource(EventLogFileReader.openEventLog(log.getPath, fs)) { in =>\n+          bus.replay(in, log.getPath.getName)\n+        }\n+      }\n+\n+      val rewriter = new FilteredEventLogFileRewriter(sparkConf, hadoopConf, fs,\n+        builders.map(_.createFilter()))\n+      val compactedPath = rewriter.rewrite(filesToCompact)\n+\n+      cleanupCompactedFiles(filesToCompact)\n+\n+      fs.getFileStatus(new Path(compactedPath)) :: filesToRetain.toList\n+    }\n+  }\n+\n+  private def cleanupCompactedFiles(files: Seq[FileStatus]): Unit = {\n+    files.foreach { file =>\n+      var deleted = false\n+      try {\n+        deleted = fs.delete(file.getPath, true)\n+      } catch {\n+        case _: IOException =>\n+      }\n+      if (!deleted) {\n+        logWarning(s\"Failed to remove ${file.getPath} / skip removing.\")\n+      }\n+    }\n+  }\n+\n+  private def findFilesToCompact(\n+      eventLogFiles: Seq[FileStatus]): (Seq[FileStatus], Seq[FileStatus]) = {\n+    val files = RollingEventLogFilesFileReader.dropBeforeLastCompactFile(eventLogFiles)\n+    if (files.length > maxFilesToRetain) {\n+      (files.dropRight(maxFilesToRetain), files.takeRight(maxFilesToRetain))\n+    } else {\n+      (Seq.empty, files)\n+    }\n+  }\n+}\n+\n+/**\n+ * This class rewrites the event log files into one compact file: the compact file will only\n+ * contain the events which pass the filters. Events will be filtered out only when all filters\n+ * decide to filter out the event or don't mind about the event. Otherwise, the original line for\n+ * the event is written to the compact file as it is.\n+ */\n+class FilteredEventLogFileRewriter(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem,\n+    filters: Seq[EventFilter]) extends Logging {\n+\n+  def rewrite(eventLogFiles: Seq[FileStatus]): String = {\n+    require(eventLogFiles.nonEmpty)\n+\n+    val targetEventLogFilePath = eventLogFiles.last.getPath\n+    val logWriter: CompactedEventLogFileWriter = new CompactedEventLogFileWriter(\n+      targetEventLogFilePath, \"dummy\", None, targetEventLogFilePath.getParent.toUri,\n+      sparkConf, hadoopConf)\n+\n+    logWriter.start()\n+    eventLogFiles.foreach { file => rewriteFile(logWriter, file) }"
  }],
  "prId": 26416
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Why do we need the type?",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-14T16:25:10Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+/**\n+ * This class compacts the old event log files into one compact file, via two phases reading:\n+ *\n+ * 1) Initialize available [[EventFilterBuilder]] instances, and replay the old event log files with\n+ * builders, so that these builders can gather the information to create [[EventFilter]] instances.\n+ * 2) Initialize [[EventFilter]] instances from [[EventFilterBuilder]] instances, and replay the\n+ * old event log files with filters. Rewrite the content to the compact file if the filters decide\n+ * to filter in.\n+ *\n+ * This class assumes caller will provide the sorted list of files which are sorted by the index of\n+ * event log file - caller should keep in mind that this class doesn't care about the semantic of\n+ * ordering.\n+ *\n+ * When compacting the files, the range of compaction for given file list is determined as:\n+ * (rightmost compact file ~ the file where there're `maxFilesToRetain` files on the right side)\n+ *\n+ * If there's no compact file in the list, it starts from the first file. If there're not enough\n+ * files after rightmost compact file, compaction will be skipped.\n+ */\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.length <= maxFilesToRetain) {\n+      return eventLogFiles\n+    }\n+\n+    if (EventLogFileWriter.isCompacted(eventLogFiles.last.getPath)) {\n+      return Seq(eventLogFiles.last)\n+    }\n+\n+    val (filesToCompact, filesToRetain) = findFilesToCompact(eventLogFiles)\n+    if (filesToCompact.isEmpty) {\n+      filesToRetain\n+    } else {\n+      val bus = new ReplayListenerBus()\n+\n+      val builders = ServiceLoader.load(classOf[EventFilterBuilder],\n+        Utils.getContextOrSparkClassLoader).asScala.toSeq\n+      builders.foreach(bus.addListener)\n+\n+      filesToCompact.foreach { log =>\n+        Utils.tryWithResource(EventLogFileReader.openEventLog(log.getPath, fs)) { in =>\n+          bus.replay(in, log.getPath.getName)\n+        }\n+      }\n+\n+      val rewriter = new FilteredEventLogFileRewriter(sparkConf, hadoopConf, fs,\n+        builders.map(_.createFilter()))\n+      val compactedPath = rewriter.rewrite(filesToCompact)\n+\n+      cleanupCompactedFiles(filesToCompact)\n+\n+      fs.getFileStatus(new Path(compactedPath)) :: filesToRetain.toList\n+    }\n+  }\n+\n+  private def cleanupCompactedFiles(files: Seq[FileStatus]): Unit = {\n+    files.foreach { file =>\n+      var deleted = false\n+      try {\n+        deleted = fs.delete(file.getPath, true)\n+      } catch {\n+        case _: IOException =>\n+      }\n+      if (!deleted) {\n+        logWarning(s\"Failed to remove ${file.getPath} / skip removing.\")\n+      }\n+    }\n+  }\n+\n+  private def findFilesToCompact(\n+      eventLogFiles: Seq[FileStatus]): (Seq[FileStatus], Seq[FileStatus]) = {\n+    val files = RollingEventLogFilesFileReader.dropBeforeLastCompactFile(eventLogFiles)\n+    if (files.length > maxFilesToRetain) {\n+      (files.dropRight(maxFilesToRetain), files.takeRight(maxFilesToRetain))\n+    } else {\n+      (Seq.empty, files)\n+    }\n+  }\n+}\n+\n+/**\n+ * This class rewrites the event log files into one compact file: the compact file will only\n+ * contain the events which pass the filters. Events will be filtered out only when all filters\n+ * decide to filter out the event or don't mind about the event. Otherwise, the original line for\n+ * the event is written to the compact file as it is.\n+ */\n+class FilteredEventLogFileRewriter(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem,\n+    filters: Seq[EventFilter]) extends Logging {\n+\n+  def rewrite(eventLogFiles: Seq[FileStatus]): String = {\n+    require(eventLogFiles.nonEmpty)\n+\n+    val targetEventLogFilePath = eventLogFiles.last.getPath\n+    val logWriter: CompactedEventLogFileWriter = new CompactedEventLogFileWriter("
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "~~Because we override the target file path, so it's not same as SingleEventLogFileWriter.~~\r\n\r\nEDIT: I see what you meant. Will address.",
    "commit": "e5d925025a606cbb5c365303149272900f255e33",
    "createdAt": "2019-11-15T03:09:46Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util.ServiceLoader\n+\n+import scala.collection.JavaConverters._\n+import scala.io.{Codec, Source}\n+\n+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+/**\n+ * This class compacts the old event log files into one compact file, via two phases reading:\n+ *\n+ * 1) Initialize available [[EventFilterBuilder]] instances, and replay the old event log files with\n+ * builders, so that these builders can gather the information to create [[EventFilter]] instances.\n+ * 2) Initialize [[EventFilter]] instances from [[EventFilterBuilder]] instances, and replay the\n+ * old event log files with filters. Rewrite the content to the compact file if the filters decide\n+ * to filter in.\n+ *\n+ * This class assumes caller will provide the sorted list of files which are sorted by the index of\n+ * event log file - caller should keep in mind that this class doesn't care about the semantic of\n+ * ordering.\n+ *\n+ * When compacting the files, the range of compaction for given file list is determined as:\n+ * (rightmost compact file ~ the file where there're `maxFilesToRetain` files on the right side)\n+ *\n+ * If there's no compact file in the list, it starts from the first file. If there're not enough\n+ * files after rightmost compact file, compaction will be skipped.\n+ */\n+class EventLogFileCompactor(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem) extends Logging {\n+\n+  private val maxFilesToRetain: Int = sparkConf.get(EVENT_LOG_ROLLING_MAX_FILES_TO_RETAIN)\n+\n+  def compact(eventLogFiles: Seq[FileStatus]): Seq[FileStatus] = {\n+    if (eventLogFiles.length <= maxFilesToRetain) {\n+      return eventLogFiles\n+    }\n+\n+    if (EventLogFileWriter.isCompacted(eventLogFiles.last.getPath)) {\n+      return Seq(eventLogFiles.last)\n+    }\n+\n+    val (filesToCompact, filesToRetain) = findFilesToCompact(eventLogFiles)\n+    if (filesToCompact.isEmpty) {\n+      filesToRetain\n+    } else {\n+      val bus = new ReplayListenerBus()\n+\n+      val builders = ServiceLoader.load(classOf[EventFilterBuilder],\n+        Utils.getContextOrSparkClassLoader).asScala.toSeq\n+      builders.foreach(bus.addListener)\n+\n+      filesToCompact.foreach { log =>\n+        Utils.tryWithResource(EventLogFileReader.openEventLog(log.getPath, fs)) { in =>\n+          bus.replay(in, log.getPath.getName)\n+        }\n+      }\n+\n+      val rewriter = new FilteredEventLogFileRewriter(sparkConf, hadoopConf, fs,\n+        builders.map(_.createFilter()))\n+      val compactedPath = rewriter.rewrite(filesToCompact)\n+\n+      cleanupCompactedFiles(filesToCompact)\n+\n+      fs.getFileStatus(new Path(compactedPath)) :: filesToRetain.toList\n+    }\n+  }\n+\n+  private def cleanupCompactedFiles(files: Seq[FileStatus]): Unit = {\n+    files.foreach { file =>\n+      var deleted = false\n+      try {\n+        deleted = fs.delete(file.getPath, true)\n+      } catch {\n+        case _: IOException =>\n+      }\n+      if (!deleted) {\n+        logWarning(s\"Failed to remove ${file.getPath} / skip removing.\")\n+      }\n+    }\n+  }\n+\n+  private def findFilesToCompact(\n+      eventLogFiles: Seq[FileStatus]): (Seq[FileStatus], Seq[FileStatus]) = {\n+    val files = RollingEventLogFilesFileReader.dropBeforeLastCompactFile(eventLogFiles)\n+    if (files.length > maxFilesToRetain) {\n+      (files.dropRight(maxFilesToRetain), files.takeRight(maxFilesToRetain))\n+    } else {\n+      (Seq.empty, files)\n+    }\n+  }\n+}\n+\n+/**\n+ * This class rewrites the event log files into one compact file: the compact file will only\n+ * contain the events which pass the filters. Events will be filtered out only when all filters\n+ * decide to filter out the event or don't mind about the event. Otherwise, the original line for\n+ * the event is written to the compact file as it is.\n+ */\n+class FilteredEventLogFileRewriter(\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration,\n+    fs: FileSystem,\n+    filters: Seq[EventFilter]) extends Logging {\n+\n+  def rewrite(eventLogFiles: Seq[FileStatus]): String = {\n+    require(eventLogFiles.nonEmpty)\n+\n+    val targetEventLogFilePath = eventLogFiles.last.getPath\n+    val logWriter: CompactedEventLogFileWriter = new CompactedEventLogFileWriter("
  }],
  "prId": 26416
}]