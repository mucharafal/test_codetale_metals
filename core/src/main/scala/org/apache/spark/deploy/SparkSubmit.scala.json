[{
  "comments": [{
    "author": {
      "login": "felixcheung"
    },
    "body": "is't `spark.jars` used somewhere?",
    "commit": "c229cdcc83c802e52b28100f5a4eb1f41a5840cd",
    "createdAt": "2019-03-19T05:01:49Z",
    "diffHunk": "@@ -542,10 +542,10 @@ private[spark] class SparkSubmit extends Logging {\n       OptionAssigner(args.totalExecutorCores, STANDALONE | MESOS | KUBERNETES, ALL_DEPLOY_MODES,\n         confKey = CORES_MAX.key),\n       OptionAssigner(args.files, LOCAL | STANDALONE | MESOS | KUBERNETES, ALL_DEPLOY_MODES,\n-        confKey = \"spark.files\"),\n-      OptionAssigner(args.jars, LOCAL, CLIENT, confKey = \"spark.jars\"),\n+        confKey = FILES.key),\n+      OptionAssigner(args.jars, LOCAL, CLIENT, confKey = JARS.key),\n       OptionAssigner(args.jars, STANDALONE | MESOS | KUBERNETES, ALL_DEPLOY_MODES,\n-        confKey = \"spark.jars\"),",
    "line": 9
  }, {
    "author": {
      "login": "hehuiyuan"
    },
    "body": "In the org.apache.spark.internal.config，many variables  have been defined and used them  elsewhere.\r\nBut 'spark.jars' and 'spark.files' are not used  in the  SparkContext and Submit.\r\n\r\nI mean, we can use the variables of FILES and JARS instead of \"spark.jars\" and \"spark.files\".\r\n",
    "commit": "c229cdcc83c802e52b28100f5a4eb1f41a5840cd",
    "createdAt": "2019-03-19T07:53:10Z",
    "diffHunk": "@@ -542,10 +542,10 @@ private[spark] class SparkSubmit extends Logging {\n       OptionAssigner(args.totalExecutorCores, STANDALONE | MESOS | KUBERNETES, ALL_DEPLOY_MODES,\n         confKey = CORES_MAX.key),\n       OptionAssigner(args.files, LOCAL | STANDALONE | MESOS | KUBERNETES, ALL_DEPLOY_MODES,\n-        confKey = \"spark.files\"),\n-      OptionAssigner(args.jars, LOCAL, CLIENT, confKey = \"spark.jars\"),\n+        confKey = FILES.key),\n+      OptionAssigner(args.jars, LOCAL, CLIENT, confKey = JARS.key),\n       OptionAssigner(args.jars, STANDALONE | MESOS | KUBERNETES, ALL_DEPLOY_MODES,\n-        confKey = \"spark.jars\"),",
    "line": 9
  }, {
    "author": {
      "login": "attilapiros"
    },
    "body": "I think there are some other places where you can make this change, like:\r\nhttps://github.com/apache/spark/blob/37c16db99aa0055bdedb7b5f9b17f7949f79c7c3/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala#L187\r\n\r\nI think this is what @felixcheung was asking about. ",
    "commit": "c229cdcc83c802e52b28100f5a4eb1f41a5840cd",
    "createdAt": "2019-03-19T12:43:30Z",
    "diffHunk": "@@ -542,10 +542,10 @@ private[spark] class SparkSubmit extends Logging {\n       OptionAssigner(args.totalExecutorCores, STANDALONE | MESOS | KUBERNETES, ALL_DEPLOY_MODES,\n         confKey = CORES_MAX.key),\n       OptionAssigner(args.files, LOCAL | STANDALONE | MESOS | KUBERNETES, ALL_DEPLOY_MODES,\n-        confKey = \"spark.files\"),\n-      OptionAssigner(args.jars, LOCAL, CLIENT, confKey = \"spark.jars\"),\n+        confKey = FILES.key),\n+      OptionAssigner(args.jars, LOCAL, CLIENT, confKey = JARS.key),\n       OptionAssigner(args.jars, STANDALONE | MESOS | KUBERNETES, ALL_DEPLOY_MODES,\n-        confKey = \"spark.jars\"),",
    "line": 9
  }, {
    "author": {
      "login": "attilapiros"
    },
    "body": "This find & grep combo can help you to find all usage of \"spark.files\":\r\n```\r\n$ find . \\( -name \"*.java\" -o -name \"*.scala\" \\) -exec grep \\\"spark.files\\\" \\{} \\; -print\r\n```",
    "commit": "c229cdcc83c802e52b28100f5a4eb1f41a5840cd",
    "createdAt": "2019-03-19T12:55:34Z",
    "diffHunk": "@@ -542,10 +542,10 @@ private[spark] class SparkSubmit extends Logging {\n       OptionAssigner(args.totalExecutorCores, STANDALONE | MESOS | KUBERNETES, ALL_DEPLOY_MODES,\n         confKey = CORES_MAX.key),\n       OptionAssigner(args.files, LOCAL | STANDALONE | MESOS | KUBERNETES, ALL_DEPLOY_MODES,\n-        confKey = \"spark.files\"),\n-      OptionAssigner(args.jars, LOCAL, CLIENT, confKey = \"spark.jars\"),\n+        confKey = FILES.key),\n+      OptionAssigner(args.jars, LOCAL, CLIENT, confKey = JARS.key),\n       OptionAssigner(args.jars, STANDALONE | MESOS | KUBERNETES, ALL_DEPLOY_MODES,\n-        confKey = \"spark.jars\"),",
    "line": 9
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Yup, let's fix it together ",
    "commit": "c229cdcc83c802e52b28100f5a4eb1f41a5840cd",
    "createdAt": "2019-03-19T13:19:53Z",
    "diffHunk": "@@ -542,10 +542,10 @@ private[spark] class SparkSubmit extends Logging {\n       OptionAssigner(args.totalExecutorCores, STANDALONE | MESOS | KUBERNETES, ALL_DEPLOY_MODES,\n         confKey = CORES_MAX.key),\n       OptionAssigner(args.files, LOCAL | STANDALONE | MESOS | KUBERNETES, ALL_DEPLOY_MODES,\n-        confKey = \"spark.files\"),\n-      OptionAssigner(args.jars, LOCAL, CLIENT, confKey = \"spark.jars\"),\n+        confKey = FILES.key),\n+      OptionAssigner(args.jars, LOCAL, CLIENT, confKey = JARS.key),\n       OptionAssigner(args.jars, STANDALONE | MESOS | KUBERNETES, ALL_DEPLOY_MODES,\n-        confKey = \"spark.jars\"),",
    "line": 9
  }, {
    "author": {
      "login": "hehuiyuan"
    },
    "body": "for example:\r\n397th line in SparkContext ；\r\n684th in SparkSubmitSuite",
    "commit": "c229cdcc83c802e52b28100f5a4eb1f41a5840cd",
    "createdAt": "2019-03-19T14:36:39Z",
    "diffHunk": "@@ -542,10 +542,10 @@ private[spark] class SparkSubmit extends Logging {\n       OptionAssigner(args.totalExecutorCores, STANDALONE | MESOS | KUBERNETES, ALL_DEPLOY_MODES,\n         confKey = CORES_MAX.key),\n       OptionAssigner(args.files, LOCAL | STANDALONE | MESOS | KUBERNETES, ALL_DEPLOY_MODES,\n-        confKey = \"spark.files\"),\n-      OptionAssigner(args.jars, LOCAL, CLIENT, confKey = \"spark.jars\"),\n+        confKey = FILES.key),\n+      OptionAssigner(args.jars, LOCAL, CLIENT, confKey = JARS.key),\n       OptionAssigner(args.jars, STANDALONE | MESOS | KUBERNETES, ALL_DEPLOY_MODES,\n-        confKey = \"spark.jars\"),",
    "line": 9
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "@hehuiyuan  can you update the PR?",
    "commit": "c229cdcc83c802e52b28100f5a4eb1f41a5840cd",
    "createdAt": "2019-03-20T12:12:58Z",
    "diffHunk": "@@ -542,10 +542,10 @@ private[spark] class SparkSubmit extends Logging {\n       OptionAssigner(args.totalExecutorCores, STANDALONE | MESOS | KUBERNETES, ALL_DEPLOY_MODES,\n         confKey = CORES_MAX.key),\n       OptionAssigner(args.files, LOCAL | STANDALONE | MESOS | KUBERNETES, ALL_DEPLOY_MODES,\n-        confKey = \"spark.files\"),\n-      OptionAssigner(args.jars, LOCAL, CLIENT, confKey = \"spark.jars\"),\n+        confKey = FILES.key),\n+      OptionAssigner(args.jars, LOCAL, CLIENT, confKey = JARS.key),\n       OptionAssigner(args.jars, STANDALONE | MESOS | KUBERNETES, ALL_DEPLOY_MODES,\n-        confKey = \"spark.jars\"),",
    "line": 9
  }, {
    "author": {
      "login": "hehuiyuan"
    },
    "body": "ok",
    "commit": "c229cdcc83c802e52b28100f5a4eb1f41a5840cd",
    "createdAt": "2019-03-20T14:49:55Z",
    "diffHunk": "@@ -542,10 +542,10 @@ private[spark] class SparkSubmit extends Logging {\n       OptionAssigner(args.totalExecutorCores, STANDALONE | MESOS | KUBERNETES, ALL_DEPLOY_MODES,\n         confKey = CORES_MAX.key),\n       OptionAssigner(args.files, LOCAL | STANDALONE | MESOS | KUBERNETES, ALL_DEPLOY_MODES,\n-        confKey = \"spark.files\"),\n-      OptionAssigner(args.jars, LOCAL, CLIENT, confKey = \"spark.jars\"),\n+        confKey = FILES.key),\n+      OptionAssigner(args.jars, LOCAL, CLIENT, confKey = JARS.key),\n       OptionAssigner(args.jars, STANDALONE | MESOS | KUBERNETES, ALL_DEPLOY_MODES,\n-        confKey = \"spark.jars\"),",
    "line": 9
  }],
  "prId": 24123
}]