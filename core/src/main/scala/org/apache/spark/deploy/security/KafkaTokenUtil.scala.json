[{
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`private[spark]` is redundant when the class is already tagged that way.",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-11-14T21:33:13Z",
    "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import java.text.SimpleDateFormat\n+import java.util.Properties\n+\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier\n+import org.apache.kafka.clients.CommonClientConfigs\n+import org.apache.kafka.clients.admin.{AdminClient, CreateDelegationTokenOptions}\n+import org.apache.kafka.common.config.SaslConfigs\n+import org.apache.kafka.common.security.token.delegation.DelegationToken\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+\n+private[spark] object KafkaTokenUtil extends Logging {\n+  private[spark] val TOKEN_KIND = new Text(\"KAFKA_DELEGATION_TOKEN\")"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Fixed.",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-11-15T14:20:26Z",
    "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import java.text.SimpleDateFormat\n+import java.util.Properties\n+\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier\n+import org.apache.kafka.clients.CommonClientConfigs\n+import org.apache.kafka.clients.admin.{AdminClient, CreateDelegationTokenOptions}\n+import org.apache.kafka.common.config.SaslConfigs\n+import org.apache.kafka.common.security.token.delegation.DelegationToken\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+\n+private[spark] object KafkaTokenUtil extends Logging {\n+  private[spark] val TOKEN_KIND = new Text(\"KAFKA_DELEGATION_TOKEN\")"
  }],
  "prId": 22598
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "no semi-colons",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-11-14T21:33:39Z",
    "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import java.text.SimpleDateFormat\n+import java.util.Properties\n+\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier\n+import org.apache.kafka.clients.CommonClientConfigs\n+import org.apache.kafka.clients.admin.{AdminClient, CreateDelegationTokenOptions}\n+import org.apache.kafka.common.config.SaslConfigs\n+import org.apache.kafka.common.security.token.delegation.DelegationToken\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+\n+private[spark] object KafkaTokenUtil extends Logging {\n+  private[spark] val TOKEN_KIND = new Text(\"KAFKA_DELEGATION_TOKEN\")\n+  private[spark] val TOKEN_SERVICE = new Text(\"kafka.server.delegation.token\")\n+\n+  private[spark] class KafkaDelegationTokenIdentifier extends AbstractDelegationTokenIdentifier {\n+    override def getKind: Text = TOKEN_KIND;"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Fixed.",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-11-15T14:20:32Z",
    "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import java.text.SimpleDateFormat\n+import java.util.Properties\n+\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier\n+import org.apache.kafka.clients.CommonClientConfigs\n+import org.apache.kafka.clients.admin.{AdminClient, CreateDelegationTokenOptions}\n+import org.apache.kafka.common.config.SaslConfigs\n+import org.apache.kafka.common.security.token.delegation.DelegationToken\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+\n+private[spark] object KafkaTokenUtil extends Logging {\n+  private[spark] val TOKEN_KIND = new Text(\"KAFKA_DELEGATION_TOKEN\")\n+  private[spark] val TOKEN_SERVICE = new Text(\"kafka.server.delegation.token\")\n+\n+  private[spark] class KafkaDelegationTokenIdentifier extends AbstractDelegationTokenIdentifier {\n+    override def getKind: Text = TOKEN_KIND;"
  }],
  "prId": 22598
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "debug?",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-11-14T21:34:52Z",
    "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import java.text.SimpleDateFormat\n+import java.util.Properties\n+\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier\n+import org.apache.kafka.clients.CommonClientConfigs\n+import org.apache.kafka.clients.admin.{AdminClient, CreateDelegationTokenOptions}\n+import org.apache.kafka.common.config.SaslConfigs\n+import org.apache.kafka.common.security.token.delegation.DelegationToken\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+\n+private[spark] object KafkaTokenUtil extends Logging {\n+  private[spark] val TOKEN_KIND = new Text(\"KAFKA_DELEGATION_TOKEN\")\n+  private[spark] val TOKEN_SERVICE = new Text(\"kafka.server.delegation.token\")\n+\n+  private[spark] class KafkaDelegationTokenIdentifier extends AbstractDelegationTokenIdentifier {\n+    override def getKind: Text = TOKEN_KIND;\n+  }\n+\n+  private[security] def obtainToken(sparkConf: SparkConf): (Token[_ <: TokenIdentifier], Long) = {\n+    val adminClient = AdminClient.create(createAdminClientProperties(sparkConf))\n+    val createDelegationTokenOptions = new CreateDelegationTokenOptions()\n+    val createResult = adminClient.createDelegationToken(createDelegationTokenOptions)\n+    val token = createResult.delegationToken().get()\n+    printToken(token)\n+\n+    (new Token[KafkaDelegationTokenIdentifier](\n+      token.tokenInfo.tokenId.getBytes,\n+      token.hmacAsBase64String.getBytes,\n+      TOKEN_KIND,\n+      TOKEN_SERVICE\n+    ), token.tokenInfo.expiryTimestamp)\n+  }\n+\n+  private[security] def createAdminClientProperties(sparkConf: SparkConf): Properties = {\n+    val adminClientProperties = new Properties\n+\n+    val bootstrapServers = sparkConf.get(KAFKA_BOOTSTRAP_SERVERS)\n+    require(bootstrapServers.nonEmpty, s\"Tried to obtain kafka delegation token but bootstrap \" +\n+      \"servers not configured.\")\n+    adminClientProperties.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers.get)\n+\n+    val protocol = sparkConf.get(KAFKA_SECURITY_PROTOCOL)\n+    adminClientProperties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, protocol)\n+    if (protocol.endsWith(\"SSL\")) {\n+      logInfo(\"SSL protocol detected.\")"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Fixed.",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-11-15T14:20:39Z",
    "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import java.text.SimpleDateFormat\n+import java.util.Properties\n+\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier\n+import org.apache.kafka.clients.CommonClientConfigs\n+import org.apache.kafka.clients.admin.{AdminClient, CreateDelegationTokenOptions}\n+import org.apache.kafka.common.config.SaslConfigs\n+import org.apache.kafka.common.security.token.delegation.DelegationToken\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+\n+private[spark] object KafkaTokenUtil extends Logging {\n+  private[spark] val TOKEN_KIND = new Text(\"KAFKA_DELEGATION_TOKEN\")\n+  private[spark] val TOKEN_SERVICE = new Text(\"kafka.server.delegation.token\")\n+\n+  private[spark] class KafkaDelegationTokenIdentifier extends AbstractDelegationTokenIdentifier {\n+    override def getKind: Text = TOKEN_KIND;\n+  }\n+\n+  private[security] def obtainToken(sparkConf: SparkConf): (Token[_ <: TokenIdentifier], Long) = {\n+    val adminClient = AdminClient.create(createAdminClientProperties(sparkConf))\n+    val createDelegationTokenOptions = new CreateDelegationTokenOptions()\n+    val createResult = adminClient.createDelegationToken(createDelegationTokenOptions)\n+    val token = createResult.delegationToken().get()\n+    printToken(token)\n+\n+    (new Token[KafkaDelegationTokenIdentifier](\n+      token.tokenInfo.tokenId.getBytes,\n+      token.hmacAsBase64String.getBytes,\n+      TOKEN_KIND,\n+      TOKEN_SERVICE\n+    ), token.tokenInfo.expiryTimestamp)\n+  }\n+\n+  private[security] def createAdminClientProperties(sparkConf: SparkConf): Properties = {\n+    val adminClientProperties = new Properties\n+\n+    val bootstrapServers = sparkConf.get(KAFKA_BOOTSTRAP_SERVERS)\n+    require(bootstrapServers.nonEmpty, s\"Tried to obtain kafka delegation token but bootstrap \" +\n+      \"servers not configured.\")\n+    adminClientProperties.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers.get)\n+\n+    val protocol = sparkConf.get(KAFKA_SECURITY_PROTOCOL)\n+    adminClientProperties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, protocol)\n+    if (protocol.endsWith(\"SSL\")) {\n+      logInfo(\"SSL protocol detected.\")"
  }],
  "prId": 22598
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "If I understand this correctly, this code is trying to log in again, right? Two questions:\r\n\r\n- why is that necessary?\r\n- won't this fail if the user only has a TGT (i.e. login via kinit)?\r\n\r\nI was expecting that you could reuse the already available login information to talk to the Kafka broker here and get delegation tokens. For example, you can do this to change the JVM's security context to the current user, if it's not already set:\r\n\r\n```\r\norg.apache.hadoop.security.UserGroupInformation.getCurrentUser().doAs(\r\n  new java.security.PrivilegedExceptionAction[Unit] { \r\n    override def run(): Unit = {\r\n      println(javax.security.auth.Subject.getSubject(\r\n        java.security.AccessController.getContext()))\r\n    }\r\n  }\r\n)\r\n\r\n// Exiting paste mode, now interpreting.\r\n\r\nSubject:\r\n        Principal: UnixPrincipal: blah\r\n        Principal: UnixNumericUserPrincipal: 2000\r\n        Principal: UnixNumericGroupPrincipal [Primary Group]: 2000\r\n ```\r\n\r\nWouldn't that be enough here? That `Subject` has a TGT, so that should be enough to talk to Kafka. Having to do yet another login just for Kafka feels a bit weird.",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-11-14T22:02:29Z",
    "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import java.text.SimpleDateFormat\n+import java.util.Properties\n+\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier\n+import org.apache.kafka.clients.CommonClientConfigs\n+import org.apache.kafka.clients.admin.{AdminClient, CreateDelegationTokenOptions}\n+import org.apache.kafka.common.config.SaslConfigs\n+import org.apache.kafka.common.security.token.delegation.DelegationToken\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+\n+private[spark] object KafkaTokenUtil extends Logging {\n+  private[spark] val TOKEN_KIND = new Text(\"KAFKA_DELEGATION_TOKEN\")\n+  private[spark] val TOKEN_SERVICE = new Text(\"kafka.server.delegation.token\")\n+\n+  private[spark] class KafkaDelegationTokenIdentifier extends AbstractDelegationTokenIdentifier {\n+    override def getKind: Text = TOKEN_KIND;\n+  }\n+\n+  private[security] def obtainToken(sparkConf: SparkConf): (Token[_ <: TokenIdentifier], Long) = {\n+    val adminClient = AdminClient.create(createAdminClientProperties(sparkConf))\n+    val createDelegationTokenOptions = new CreateDelegationTokenOptions()\n+    val createResult = adminClient.createDelegationToken(createDelegationTokenOptions)\n+    val token = createResult.delegationToken().get()\n+    printToken(token)\n+\n+    (new Token[KafkaDelegationTokenIdentifier](\n+      token.tokenInfo.tokenId.getBytes,\n+      token.hmacAsBase64String.getBytes,\n+      TOKEN_KIND,\n+      TOKEN_SERVICE\n+    ), token.tokenInfo.expiryTimestamp)\n+  }\n+\n+  private[security] def createAdminClientProperties(sparkConf: SparkConf): Properties = {\n+    val adminClientProperties = new Properties\n+\n+    val bootstrapServers = sparkConf.get(KAFKA_BOOTSTRAP_SERVERS)\n+    require(bootstrapServers.nonEmpty, s\"Tried to obtain kafka delegation token but bootstrap \" +\n+      \"servers not configured.\")\n+    adminClientProperties.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers.get)\n+\n+    val protocol = sparkConf.get(KAFKA_SECURITY_PROTOCOL)\n+    adminClientProperties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, protocol)\n+    if (protocol.endsWith(\"SSL\")) {\n+      logInfo(\"SSL protocol detected.\")\n+      sparkConf.get(KAFKA_TRUSTSTORE_LOCATION).foreach { truststoreLocation =>\n+        adminClientProperties.put(\"ssl.truststore.location\", truststoreLocation)\n+      }\n+      sparkConf.get(KAFKA_TRUSTSTORE_PASSWORD).foreach { truststorePassword =>\n+        adminClientProperties.put(\"ssl.truststore.password\", truststorePassword)\n+      }\n+    } else {\n+      logWarning(\"Obtaining kafka delegation token through plain communication channel. Please \" +\n+        \"consider the security impact.\")\n+    }\n+\n+    // There are multiple possibilities to log in:"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "you are right: the user shouldn't need to be logging in again, as normal end-user use is to be kinited in, rather than giving a keytab",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-11-15T14:49:26Z",
    "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import java.text.SimpleDateFormat\n+import java.util.Properties\n+\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier\n+import org.apache.kafka.clients.CommonClientConfigs\n+import org.apache.kafka.clients.admin.{AdminClient, CreateDelegationTokenOptions}\n+import org.apache.kafka.common.config.SaslConfigs\n+import org.apache.kafka.common.security.token.delegation.DelegationToken\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+\n+private[spark] object KafkaTokenUtil extends Logging {\n+  private[spark] val TOKEN_KIND = new Text(\"KAFKA_DELEGATION_TOKEN\")\n+  private[spark] val TOKEN_SERVICE = new Text(\"kafka.server.delegation.token\")\n+\n+  private[spark] class KafkaDelegationTokenIdentifier extends AbstractDelegationTokenIdentifier {\n+    override def getKind: Text = TOKEN_KIND;\n+  }\n+\n+  private[security] def obtainToken(sparkConf: SparkConf): (Token[_ <: TokenIdentifier], Long) = {\n+    val adminClient = AdminClient.create(createAdminClientProperties(sparkConf))\n+    val createDelegationTokenOptions = new CreateDelegationTokenOptions()\n+    val createResult = adminClient.createDelegationToken(createDelegationTokenOptions)\n+    val token = createResult.delegationToken().get()\n+    printToken(token)\n+\n+    (new Token[KafkaDelegationTokenIdentifier](\n+      token.tokenInfo.tokenId.getBytes,\n+      token.hmacAsBase64String.getBytes,\n+      TOKEN_KIND,\n+      TOKEN_SERVICE\n+    ), token.tokenInfo.expiryTimestamp)\n+  }\n+\n+  private[security] def createAdminClientProperties(sparkConf: SparkConf): Properties = {\n+    val adminClientProperties = new Properties\n+\n+    val bootstrapServers = sparkConf.get(KAFKA_BOOTSTRAP_SERVERS)\n+    require(bootstrapServers.nonEmpty, s\"Tried to obtain kafka delegation token but bootstrap \" +\n+      \"servers not configured.\")\n+    adminClientProperties.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers.get)\n+\n+    val protocol = sparkConf.get(KAFKA_SECURITY_PROTOCOL)\n+    adminClientProperties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, protocol)\n+    if (protocol.endsWith(\"SSL\")) {\n+      logInfo(\"SSL protocol detected.\")\n+      sparkConf.get(KAFKA_TRUSTSTORE_LOCATION).foreach { truststoreLocation =>\n+        adminClientProperties.put(\"ssl.truststore.location\", truststoreLocation)\n+      }\n+      sparkConf.get(KAFKA_TRUSTSTORE_PASSWORD).foreach { truststorePassword =>\n+        adminClientProperties.put(\"ssl.truststore.password\", truststorePassword)\n+      }\n+    } else {\n+      logWarning(\"Obtaining kafka delegation token through plain communication channel. Please \" +\n+        \"consider the security impact.\")\n+    }\n+\n+    // There are multiple possibilities to log in:"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "This code doesn't try to log in but provides dynamic jaas configuration to kafka. In the original implementation only jaas was possible now I've extended it to make things easier for the user.\r\n\r\n> why is that necessary?\r\n\r\nThis eliminates the need to provide file based jaas configuration.\r\nOld way: https://github.com/gaborgsomogyi/spark-structured-secure-kafka-app\r\nNew way: https://github.com/gaborgsomogyi/spark-structured-secure-dt-kafka-app\r\n\r\n> won't this fail if the user only has a TGT (i.e. login via kinit)?\r\n\r\nIt works. TGT can be provided to kafka by providing a file based jaas file with content like:\r\n```\r\nuseTicketCache=true\r\nticketCache=<path>\r\n```\r\n\r\nPlease see how kafka client authentication works: https://cwiki.apache.org/confluence/display/KAFKA/KIP-85%3A+Dynamic+JAAS+configuration+for+Kafka+clients",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-11-15T15:49:11Z",
    "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import java.text.SimpleDateFormat\n+import java.util.Properties\n+\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier\n+import org.apache.kafka.clients.CommonClientConfigs\n+import org.apache.kafka.clients.admin.{AdminClient, CreateDelegationTokenOptions}\n+import org.apache.kafka.common.config.SaslConfigs\n+import org.apache.kafka.common.security.token.delegation.DelegationToken\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+\n+private[spark] object KafkaTokenUtil extends Logging {\n+  private[spark] val TOKEN_KIND = new Text(\"KAFKA_DELEGATION_TOKEN\")\n+  private[spark] val TOKEN_SERVICE = new Text(\"kafka.server.delegation.token\")\n+\n+  private[spark] class KafkaDelegationTokenIdentifier extends AbstractDelegationTokenIdentifier {\n+    override def getKind: Text = TOKEN_KIND;\n+  }\n+\n+  private[security] def obtainToken(sparkConf: SparkConf): (Token[_ <: TokenIdentifier], Long) = {\n+    val adminClient = AdminClient.create(createAdminClientProperties(sparkConf))\n+    val createDelegationTokenOptions = new CreateDelegationTokenOptions()\n+    val createResult = adminClient.createDelegationToken(createDelegationTokenOptions)\n+    val token = createResult.delegationToken().get()\n+    printToken(token)\n+\n+    (new Token[KafkaDelegationTokenIdentifier](\n+      token.tokenInfo.tokenId.getBytes,\n+      token.hmacAsBase64String.getBytes,\n+      TOKEN_KIND,\n+      TOKEN_SERVICE\n+    ), token.tokenInfo.expiryTimestamp)\n+  }\n+\n+  private[security] def createAdminClientProperties(sparkConf: SparkConf): Properties = {\n+    val adminClientProperties = new Properties\n+\n+    val bootstrapServers = sparkConf.get(KAFKA_BOOTSTRAP_SERVERS)\n+    require(bootstrapServers.nonEmpty, s\"Tried to obtain kafka delegation token but bootstrap \" +\n+      \"servers not configured.\")\n+    adminClientProperties.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers.get)\n+\n+    val protocol = sparkConf.get(KAFKA_SECURITY_PROTOCOL)\n+    adminClientProperties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, protocol)\n+    if (protocol.endsWith(\"SSL\")) {\n+      logInfo(\"SSL protocol detected.\")\n+      sparkConf.get(KAFKA_TRUSTSTORE_LOCATION).foreach { truststoreLocation =>\n+        adminClientProperties.put(\"ssl.truststore.location\", truststoreLocation)\n+      }\n+      sparkConf.get(KAFKA_TRUSTSTORE_PASSWORD).foreach { truststorePassword =>\n+        adminClientProperties.put(\"ssl.truststore.password\", truststorePassword)\n+      }\n+    } else {\n+      logWarning(\"Obtaining kafka delegation token through plain communication channel. Please \" +\n+        \"consider the security impact.\")\n+    }\n+\n+    // There are multiple possibilities to log in:"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "JAAS configuration files control how to *log in*. That's what the JAAS API does; it's an abstraction to log into different auth services (much like PAM). So if you're using JAAS here, you're asking the Kafka API to log in. (And btw, even though it's not explicit, that's basically what the Kafka doc you linked to is saying.)\r\n\r\nIf you give it a tgt-based config file, you're telling JAAS to load the credentials from the ticket cache.\r\n\r\nIf you give it a keytab-based config file, you're telling JAAS to use the keytab to authenticate against the KDC.\r\n\r\netc etc\r\n\r\nAnd that's not needed at this point, because that has already been done. Once JAAS logs in, what happens is that you have a `Subject` in the current security context with the appropriate credentials. As my example above shows, that's already done by the Hadoop APIs that Spark uses.\r\n\r\nSo my question is why can't the Kafka libraries use the already logged in `Subject`.",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-11-15T17:16:40Z",
    "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import java.text.SimpleDateFormat\n+import java.util.Properties\n+\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier\n+import org.apache.kafka.clients.CommonClientConfigs\n+import org.apache.kafka.clients.admin.{AdminClient, CreateDelegationTokenOptions}\n+import org.apache.kafka.common.config.SaslConfigs\n+import org.apache.kafka.common.security.token.delegation.DelegationToken\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+\n+private[spark] object KafkaTokenUtil extends Logging {\n+  private[spark] val TOKEN_KIND = new Text(\"KAFKA_DELEGATION_TOKEN\")\n+  private[spark] val TOKEN_SERVICE = new Text(\"kafka.server.delegation.token\")\n+\n+  private[spark] class KafkaDelegationTokenIdentifier extends AbstractDelegationTokenIdentifier {\n+    override def getKind: Text = TOKEN_KIND;\n+  }\n+\n+  private[security] def obtainToken(sparkConf: SparkConf): (Token[_ <: TokenIdentifier], Long) = {\n+    val adminClient = AdminClient.create(createAdminClientProperties(sparkConf))\n+    val createDelegationTokenOptions = new CreateDelegationTokenOptions()\n+    val createResult = adminClient.createDelegationToken(createDelegationTokenOptions)\n+    val token = createResult.delegationToken().get()\n+    printToken(token)\n+\n+    (new Token[KafkaDelegationTokenIdentifier](\n+      token.tokenInfo.tokenId.getBytes,\n+      token.hmacAsBase64String.getBytes,\n+      TOKEN_KIND,\n+      TOKEN_SERVICE\n+    ), token.tokenInfo.expiryTimestamp)\n+  }\n+\n+  private[security] def createAdminClientProperties(sparkConf: SparkConf): Properties = {\n+    val adminClientProperties = new Properties\n+\n+    val bootstrapServers = sparkConf.get(KAFKA_BOOTSTRAP_SERVERS)\n+    require(bootstrapServers.nonEmpty, s\"Tried to obtain kafka delegation token but bootstrap \" +\n+      \"servers not configured.\")\n+    adminClientProperties.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers.get)\n+\n+    val protocol = sparkConf.get(KAFKA_SECURITY_PROTOCOL)\n+    adminClientProperties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, protocol)\n+    if (protocol.endsWith(\"SSL\")) {\n+      logInfo(\"SSL protocol detected.\")\n+      sparkConf.get(KAFKA_TRUSTSTORE_LOCATION).foreach { truststoreLocation =>\n+        adminClientProperties.put(\"ssl.truststore.location\", truststoreLocation)\n+      }\n+      sparkConf.get(KAFKA_TRUSTSTORE_PASSWORD).foreach { truststorePassword =>\n+        adminClientProperties.put(\"ssl.truststore.password\", truststorePassword)\n+      }\n+    } else {\n+      logWarning(\"Obtaining kafka delegation token through plain communication channel. Please \" +\n+        \"consider the security impact.\")\n+    }\n+\n+    // There are multiple possibilities to log in:"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "I've just double checked my knowledge and this case kafka looks for JAAS entry but will not find it so it throws this exception:\r\n```\r\nCaused by: java.lang.IllegalArgumentException: Could not find a 'KafkaClient' entry in the JAAS configuration. System property 'java.security.auth.login.config' is not set\r\n```\r\n",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-11-15T22:47:40Z",
    "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import java.text.SimpleDateFormat\n+import java.util.Properties\n+\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier\n+import org.apache.kafka.clients.CommonClientConfigs\n+import org.apache.kafka.clients.admin.{AdminClient, CreateDelegationTokenOptions}\n+import org.apache.kafka.common.config.SaslConfigs\n+import org.apache.kafka.common.security.token.delegation.DelegationToken\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+\n+private[spark] object KafkaTokenUtil extends Logging {\n+  private[spark] val TOKEN_KIND = new Text(\"KAFKA_DELEGATION_TOKEN\")\n+  private[spark] val TOKEN_SERVICE = new Text(\"kafka.server.delegation.token\")\n+\n+  private[spark] class KafkaDelegationTokenIdentifier extends AbstractDelegationTokenIdentifier {\n+    override def getKind: Text = TOKEN_KIND;\n+  }\n+\n+  private[security] def obtainToken(sparkConf: SparkConf): (Token[_ <: TokenIdentifier], Long) = {\n+    val adminClient = AdminClient.create(createAdminClientProperties(sparkConf))\n+    val createDelegationTokenOptions = new CreateDelegationTokenOptions()\n+    val createResult = adminClient.createDelegationToken(createDelegationTokenOptions)\n+    val token = createResult.delegationToken().get()\n+    printToken(token)\n+\n+    (new Token[KafkaDelegationTokenIdentifier](\n+      token.tokenInfo.tokenId.getBytes,\n+      token.hmacAsBase64String.getBytes,\n+      TOKEN_KIND,\n+      TOKEN_SERVICE\n+    ), token.tokenInfo.expiryTimestamp)\n+  }\n+\n+  private[security] def createAdminClientProperties(sparkConf: SparkConf): Properties = {\n+    val adminClientProperties = new Properties\n+\n+    val bootstrapServers = sparkConf.get(KAFKA_BOOTSTRAP_SERVERS)\n+    require(bootstrapServers.nonEmpty, s\"Tried to obtain kafka delegation token but bootstrap \" +\n+      \"servers not configured.\")\n+    adminClientProperties.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers.get)\n+\n+    val protocol = sparkConf.get(KAFKA_SECURITY_PROTOCOL)\n+    adminClientProperties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, protocol)\n+    if (protocol.endsWith(\"SSL\")) {\n+      logInfo(\"SSL protocol detected.\")\n+      sparkConf.get(KAFKA_TRUSTSTORE_LOCATION).foreach { truststoreLocation =>\n+        adminClientProperties.put(\"ssl.truststore.location\", truststoreLocation)\n+      }\n+      sparkConf.get(KAFKA_TRUSTSTORE_PASSWORD).foreach { truststorePassword =>\n+        adminClientProperties.put(\"ssl.truststore.password\", truststorePassword)\n+      }\n+    } else {\n+      logWarning(\"Obtaining kafka delegation token through plain communication channel. Please \" +\n+        \"consider the security impact.\")\n+    }\n+\n+    // There are multiple possibilities to log in:"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "Where does that exception come from? The message by itself doesn't help.\r\n\r\nAnd that still does not answer my question. The fact it's even talking about JAAS configs tells me that it's still trying to log in. My question is whether there's an API in Kafka that means \"just connect to the server with my existing credentials\".\r\n\r\nAt this point in the code, you are already logged in. You already have credentials. Kafka should not be forcing you to go through all this again.\r\n\r\nIt may be that the answer is \"Kafka doesn't have this API\" in which case a bug in Kafka should be filed, and we may have to live with this until that is fixed. But before going forward here, I'd really like to be sure that is really the case.",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-11-15T22:59:29Z",
    "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import java.text.SimpleDateFormat\n+import java.util.Properties\n+\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier\n+import org.apache.kafka.clients.CommonClientConfigs\n+import org.apache.kafka.clients.admin.{AdminClient, CreateDelegationTokenOptions}\n+import org.apache.kafka.common.config.SaslConfigs\n+import org.apache.kafka.common.security.token.delegation.DelegationToken\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+\n+private[spark] object KafkaTokenUtil extends Logging {\n+  private[spark] val TOKEN_KIND = new Text(\"KAFKA_DELEGATION_TOKEN\")\n+  private[spark] val TOKEN_SERVICE = new Text(\"kafka.server.delegation.token\")\n+\n+  private[spark] class KafkaDelegationTokenIdentifier extends AbstractDelegationTokenIdentifier {\n+    override def getKind: Text = TOKEN_KIND;\n+  }\n+\n+  private[security] def obtainToken(sparkConf: SparkConf): (Token[_ <: TokenIdentifier], Long) = {\n+    val adminClient = AdminClient.create(createAdminClientProperties(sparkConf))\n+    val createDelegationTokenOptions = new CreateDelegationTokenOptions()\n+    val createResult = adminClient.createDelegationToken(createDelegationTokenOptions)\n+    val token = createResult.delegationToken().get()\n+    printToken(token)\n+\n+    (new Token[KafkaDelegationTokenIdentifier](\n+      token.tokenInfo.tokenId.getBytes,\n+      token.hmacAsBase64String.getBytes,\n+      TOKEN_KIND,\n+      TOKEN_SERVICE\n+    ), token.tokenInfo.expiryTimestamp)\n+  }\n+\n+  private[security] def createAdminClientProperties(sparkConf: SparkConf): Properties = {\n+    val adminClientProperties = new Properties\n+\n+    val bootstrapServers = sparkConf.get(KAFKA_BOOTSTRAP_SERVERS)\n+    require(bootstrapServers.nonEmpty, s\"Tried to obtain kafka delegation token but bootstrap \" +\n+      \"servers not configured.\")\n+    adminClientProperties.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers.get)\n+\n+    val protocol = sparkConf.get(KAFKA_SECURITY_PROTOCOL)\n+    adminClientProperties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, protocol)\n+    if (protocol.endsWith(\"SSL\")) {\n+      logInfo(\"SSL protocol detected.\")\n+      sparkConf.get(KAFKA_TRUSTSTORE_LOCATION).foreach { truststoreLocation =>\n+        adminClientProperties.put(\"ssl.truststore.location\", truststoreLocation)\n+      }\n+      sparkConf.get(KAFKA_TRUSTSTORE_PASSWORD).foreach { truststorePassword =>\n+        adminClientProperties.put(\"ssl.truststore.password\", truststorePassword)\n+      }\n+    } else {\n+      logWarning(\"Obtaining kafka delegation token through plain communication channel. Please \" +\n+        \"consider the security impact.\")\n+    }\n+\n+    // There are multiple possibilities to log in:"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "OK, speaking with them...",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-11-15T23:13:56Z",
    "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import java.text.SimpleDateFormat\n+import java.util.Properties\n+\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier\n+import org.apache.kafka.clients.CommonClientConfigs\n+import org.apache.kafka.clients.admin.{AdminClient, CreateDelegationTokenOptions}\n+import org.apache.kafka.common.config.SaslConfigs\n+import org.apache.kafka.common.security.token.delegation.DelegationToken\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+\n+private[spark] object KafkaTokenUtil extends Logging {\n+  private[spark] val TOKEN_KIND = new Text(\"KAFKA_DELEGATION_TOKEN\")\n+  private[spark] val TOKEN_SERVICE = new Text(\"kafka.server.delegation.token\")\n+\n+  private[spark] class KafkaDelegationTokenIdentifier extends AbstractDelegationTokenIdentifier {\n+    override def getKind: Text = TOKEN_KIND;\n+  }\n+\n+  private[security] def obtainToken(sparkConf: SparkConf): (Token[_ <: TokenIdentifier], Long) = {\n+    val adminClient = AdminClient.create(createAdminClientProperties(sparkConf))\n+    val createDelegationTokenOptions = new CreateDelegationTokenOptions()\n+    val createResult = adminClient.createDelegationToken(createDelegationTokenOptions)\n+    val token = createResult.delegationToken().get()\n+    printToken(token)\n+\n+    (new Token[KafkaDelegationTokenIdentifier](\n+      token.tokenInfo.tokenId.getBytes,\n+      token.hmacAsBase64String.getBytes,\n+      TOKEN_KIND,\n+      TOKEN_SERVICE\n+    ), token.tokenInfo.expiryTimestamp)\n+  }\n+\n+  private[security] def createAdminClientProperties(sparkConf: SparkConf): Properties = {\n+    val adminClientProperties = new Properties\n+\n+    val bootstrapServers = sparkConf.get(KAFKA_BOOTSTRAP_SERVERS)\n+    require(bootstrapServers.nonEmpty, s\"Tried to obtain kafka delegation token but bootstrap \" +\n+      \"servers not configured.\")\n+    adminClientProperties.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers.get)\n+\n+    val protocol = sparkConf.get(KAFKA_SECURITY_PROTOCOL)\n+    adminClientProperties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, protocol)\n+    if (protocol.endsWith(\"SSL\")) {\n+      logInfo(\"SSL protocol detected.\")\n+      sparkConf.get(KAFKA_TRUSTSTORE_LOCATION).foreach { truststoreLocation =>\n+        adminClientProperties.put(\"ssl.truststore.location\", truststoreLocation)\n+      }\n+      sparkConf.get(KAFKA_TRUSTSTORE_PASSWORD).foreach { truststorePassword =>\n+        adminClientProperties.put(\"ssl.truststore.password\", truststorePassword)\n+      }\n+    } else {\n+      logWarning(\"Obtaining kafka delegation token through plain communication channel. Please \" +\n+        \"consider the security impact.\")\n+    }\n+\n+    // There are multiple possibilities to log in:"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "I've spoken with them and confirmed the same thing which is stated in the official API documentation: https://kafka.apache.org/documentation/#security_sasl_kerberos\r\nThere is no such possibility.",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-11-19T09:24:18Z",
    "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import java.text.SimpleDateFormat\n+import java.util.Properties\n+\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier\n+import org.apache.kafka.clients.CommonClientConfigs\n+import org.apache.kafka.clients.admin.{AdminClient, CreateDelegationTokenOptions}\n+import org.apache.kafka.common.config.SaslConfigs\n+import org.apache.kafka.common.security.token.delegation.DelegationToken\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+\n+private[spark] object KafkaTokenUtil extends Logging {\n+  private[spark] val TOKEN_KIND = new Text(\"KAFKA_DELEGATION_TOKEN\")\n+  private[spark] val TOKEN_SERVICE = new Text(\"kafka.server.delegation.token\")\n+\n+  private[spark] class KafkaDelegationTokenIdentifier extends AbstractDelegationTokenIdentifier {\n+    override def getKind: Text = TOKEN_KIND;\n+  }\n+\n+  private[security] def obtainToken(sparkConf: SparkConf): (Token[_ <: TokenIdentifier], Long) = {\n+    val adminClient = AdminClient.create(createAdminClientProperties(sparkConf))\n+    val createDelegationTokenOptions = new CreateDelegationTokenOptions()\n+    val createResult = adminClient.createDelegationToken(createDelegationTokenOptions)\n+    val token = createResult.delegationToken().get()\n+    printToken(token)\n+\n+    (new Token[KafkaDelegationTokenIdentifier](\n+      token.tokenInfo.tokenId.getBytes,\n+      token.hmacAsBase64String.getBytes,\n+      TOKEN_KIND,\n+      TOKEN_SERVICE\n+    ), token.tokenInfo.expiryTimestamp)\n+  }\n+\n+  private[security] def createAdminClientProperties(sparkConf: SparkConf): Properties = {\n+    val adminClientProperties = new Properties\n+\n+    val bootstrapServers = sparkConf.get(KAFKA_BOOTSTRAP_SERVERS)\n+    require(bootstrapServers.nonEmpty, s\"Tried to obtain kafka delegation token but bootstrap \" +\n+      \"servers not configured.\")\n+    adminClientProperties.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers.get)\n+\n+    val protocol = sparkConf.get(KAFKA_SECURITY_PROTOCOL)\n+    adminClientProperties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, protocol)\n+    if (protocol.endsWith(\"SSL\")) {\n+      logInfo(\"SSL protocol detected.\")\n+      sparkConf.get(KAFKA_TRUSTSTORE_LOCATION).foreach { truststoreLocation =>\n+        adminClientProperties.put(\"ssl.truststore.location\", truststoreLocation)\n+      }\n+      sparkConf.get(KAFKA_TRUSTSTORE_PASSWORD).foreach { truststorePassword =>\n+        adminClientProperties.put(\"ssl.truststore.password\", truststorePassword)\n+      }\n+    } else {\n+      logWarning(\"Obtaining kafka delegation token through plain communication channel. Please \" +\n+        \"consider the security impact.\")\n+    }\n+\n+    // There are multiple possibilities to log in:"
  }],
  "prId": 22598
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "So, currently there's no way to use the existing login. Did you file a bug for Kafka to implement that? That bug should be referenced here.\r\n\r\nAlso, what about the existing TGT case? You mentioned you need a different JAAS config for that, but I don't see that handled here? Users shouldn't need to manually provide that. They don't for any other service supported by this framework.\r\n\r\nSo this should be `if (keytab.isDefined) useKeytabConfig else useTgtConfig`.",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-11-26T21:44:06Z",
    "diffHunk": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import java.text.SimpleDateFormat\n+import java.util.Properties\n+\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier\n+import org.apache.kafka.clients.CommonClientConfigs\n+import org.apache.kafka.clients.admin.{AdminClient, CreateDelegationTokenOptions}\n+import org.apache.kafka.common.config.SaslConfigs\n+import org.apache.kafka.common.security.auth.SecurityProtocol.{SASL_PLAINTEXT, SASL_SSL, SSL}\n+import org.apache.kafka.common.security.token.delegation.DelegationToken\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+\n+private[spark] object KafkaTokenUtil extends Logging {\n+  val TOKEN_KIND = new Text(\"KAFKA_DELEGATION_TOKEN\")\n+  val TOKEN_SERVICE = new Text(\"kafka.server.delegation.token\")\n+\n+  private[spark] class KafkaDelegationTokenIdentifier extends AbstractDelegationTokenIdentifier {\n+    override def getKind: Text = TOKEN_KIND\n+  }\n+\n+  private[security] def obtainToken(sparkConf: SparkConf): (Token[_ <: TokenIdentifier], Long) = {\n+    val adminClient = AdminClient.create(createAdminClientProperties(sparkConf))\n+    val createDelegationTokenOptions = new CreateDelegationTokenOptions()\n+    val createResult = adminClient.createDelegationToken(createDelegationTokenOptions)\n+    val token = createResult.delegationToken().get()\n+    printToken(token)\n+\n+    (new Token[KafkaDelegationTokenIdentifier](\n+      token.tokenInfo.tokenId.getBytes,\n+      token.hmacAsBase64String.getBytes,\n+      TOKEN_KIND,\n+      TOKEN_SERVICE\n+    ), token.tokenInfo.expiryTimestamp)\n+  }\n+\n+  private[security] def createAdminClientProperties(sparkConf: SparkConf): Properties = {\n+    val adminClientProperties = new Properties\n+\n+    val bootstrapServers = sparkConf.get(KAFKA_BOOTSTRAP_SERVERS)\n+    require(bootstrapServers.nonEmpty, s\"Tried to obtain kafka delegation token but bootstrap \" +\n+      \"servers not configured.\")\n+    adminClientProperties.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers.get)\n+\n+    val protocol = sparkConf.get(KAFKA_SECURITY_PROTOCOL)\n+    adminClientProperties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, protocol)\n+    protocol match {\n+      case SASL_SSL.name =>\n+        setTrustStoreProperties(sparkConf, adminClientProperties)\n+\n+      case SSL.name =>\n+        setTrustStoreProperties(sparkConf, adminClientProperties)\n+        setKeyStoreProperties(sparkConf, adminClientProperties)\n+        logWarning(\"Obtaining kafka delegation token with SSL protocol. Please \" +\n+          \"configure 2-way authentication on the broker side.\")\n+\n+      case SASL_PLAINTEXT.name =>\n+        logWarning(\"Obtaining kafka delegation token through plain communication channel. Please \" +\n+          \"consider the security impact.\")\n+    }\n+\n+    // There are multiple possibilities to log in:"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "The jira is: https://issues.apache.org/jira/browse/KAFKA-7677\r\nThe guys will take a look at it...\r\n\r\nRelated `useTgtConfig` I've considered this and would make the user experience way much better. On the other hand this would close the possibility to use custom JAAS configuration (don't know how many people use this). I'm fine with that but one has to count with this consequence. Should we do this then?",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-11-27T10:59:50Z",
    "diffHunk": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import java.text.SimpleDateFormat\n+import java.util.Properties\n+\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier\n+import org.apache.kafka.clients.CommonClientConfigs\n+import org.apache.kafka.clients.admin.{AdminClient, CreateDelegationTokenOptions}\n+import org.apache.kafka.common.config.SaslConfigs\n+import org.apache.kafka.common.security.auth.SecurityProtocol.{SASL_PLAINTEXT, SASL_SSL, SSL}\n+import org.apache.kafka.common.security.token.delegation.DelegationToken\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+\n+private[spark] object KafkaTokenUtil extends Logging {\n+  val TOKEN_KIND = new Text(\"KAFKA_DELEGATION_TOKEN\")\n+  val TOKEN_SERVICE = new Text(\"kafka.server.delegation.token\")\n+\n+  private[spark] class KafkaDelegationTokenIdentifier extends AbstractDelegationTokenIdentifier {\n+    override def getKind: Text = TOKEN_KIND\n+  }\n+\n+  private[security] def obtainToken(sparkConf: SparkConf): (Token[_ <: TokenIdentifier], Long) = {\n+    val adminClient = AdminClient.create(createAdminClientProperties(sparkConf))\n+    val createDelegationTokenOptions = new CreateDelegationTokenOptions()\n+    val createResult = adminClient.createDelegationToken(createDelegationTokenOptions)\n+    val token = createResult.delegationToken().get()\n+    printToken(token)\n+\n+    (new Token[KafkaDelegationTokenIdentifier](\n+      token.tokenInfo.tokenId.getBytes,\n+      token.hmacAsBase64String.getBytes,\n+      TOKEN_KIND,\n+      TOKEN_SERVICE\n+    ), token.tokenInfo.expiryTimestamp)\n+  }\n+\n+  private[security] def createAdminClientProperties(sparkConf: SparkConf): Properties = {\n+    val adminClientProperties = new Properties\n+\n+    val bootstrapServers = sparkConf.get(KAFKA_BOOTSTRAP_SERVERS)\n+    require(bootstrapServers.nonEmpty, s\"Tried to obtain kafka delegation token but bootstrap \" +\n+      \"servers not configured.\")\n+    adminClientProperties.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers.get)\n+\n+    val protocol = sparkConf.get(KAFKA_SECURITY_PROTOCOL)\n+    adminClientProperties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, protocol)\n+    protocol match {\n+      case SASL_SSL.name =>\n+        setTrustStoreProperties(sparkConf, adminClientProperties)\n+\n+      case SSL.name =>\n+        setTrustStoreProperties(sparkConf, adminClientProperties)\n+        setKeyStoreProperties(sparkConf, adminClientProperties)\n+        logWarning(\"Obtaining kafka delegation token with SSL protocol. Please \" +\n+          \"configure 2-way authentication on the broker side.\")\n+\n+      case SASL_PLAINTEXT.name =>\n+        logWarning(\"Obtaining kafka delegation token through plain communication channel. Please \" +\n+          \"consider the security impact.\")\n+    }\n+\n+    // There are multiple possibilities to log in:"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "If you want to keep the ability to provide a custom JAAS config, then just check for that. Isn't there a system property you need to set?",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-11-27T16:18:32Z",
    "diffHunk": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import java.text.SimpleDateFormat\n+import java.util.Properties\n+\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier\n+import org.apache.kafka.clients.CommonClientConfigs\n+import org.apache.kafka.clients.admin.{AdminClient, CreateDelegationTokenOptions}\n+import org.apache.kafka.common.config.SaslConfigs\n+import org.apache.kafka.common.security.auth.SecurityProtocol.{SASL_PLAINTEXT, SASL_SSL, SSL}\n+import org.apache.kafka.common.security.token.delegation.DelegationToken\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+\n+private[spark] object KafkaTokenUtil extends Logging {\n+  val TOKEN_KIND = new Text(\"KAFKA_DELEGATION_TOKEN\")\n+  val TOKEN_SERVICE = new Text(\"kafka.server.delegation.token\")\n+\n+  private[spark] class KafkaDelegationTokenIdentifier extends AbstractDelegationTokenIdentifier {\n+    override def getKind: Text = TOKEN_KIND\n+  }\n+\n+  private[security] def obtainToken(sparkConf: SparkConf): (Token[_ <: TokenIdentifier], Long) = {\n+    val adminClient = AdminClient.create(createAdminClientProperties(sparkConf))\n+    val createDelegationTokenOptions = new CreateDelegationTokenOptions()\n+    val createResult = adminClient.createDelegationToken(createDelegationTokenOptions)\n+    val token = createResult.delegationToken().get()\n+    printToken(token)\n+\n+    (new Token[KafkaDelegationTokenIdentifier](\n+      token.tokenInfo.tokenId.getBytes,\n+      token.hmacAsBase64String.getBytes,\n+      TOKEN_KIND,\n+      TOKEN_SERVICE\n+    ), token.tokenInfo.expiryTimestamp)\n+  }\n+\n+  private[security] def createAdminClientProperties(sparkConf: SparkConf): Properties = {\n+    val adminClientProperties = new Properties\n+\n+    val bootstrapServers = sparkConf.get(KAFKA_BOOTSTRAP_SERVERS)\n+    require(bootstrapServers.nonEmpty, s\"Tried to obtain kafka delegation token but bootstrap \" +\n+      \"servers not configured.\")\n+    adminClientProperties.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers.get)\n+\n+    val protocol = sparkConf.get(KAFKA_SECURITY_PROTOCOL)\n+    adminClientProperties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, protocol)\n+    protocol match {\n+      case SASL_SSL.name =>\n+        setTrustStoreProperties(sparkConf, adminClientProperties)\n+\n+      case SSL.name =>\n+        setTrustStoreProperties(sparkConf, adminClientProperties)\n+        setKeyStoreProperties(sparkConf, adminClientProperties)\n+        logWarning(\"Obtaining kafka delegation token with SSL protocol. Please \" +\n+          \"configure 2-way authentication on the broker side.\")\n+\n+      case SASL_PLAINTEXT.name =>\n+        logWarning(\"Obtaining kafka delegation token through plain communication channel. Please \" +\n+          \"consider the security impact.\")\n+    }\n+\n+    // There are multiple possibilities to log in:"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "\"java.security.auth.login.config\"",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-11-28T18:19:54Z",
    "diffHunk": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import java.text.SimpleDateFormat\n+import java.util.Properties\n+\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier\n+import org.apache.kafka.clients.CommonClientConfigs\n+import org.apache.kafka.clients.admin.{AdminClient, CreateDelegationTokenOptions}\n+import org.apache.kafka.common.config.SaslConfigs\n+import org.apache.kafka.common.security.auth.SecurityProtocol.{SASL_PLAINTEXT, SASL_SSL, SSL}\n+import org.apache.kafka.common.security.token.delegation.DelegationToken\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+\n+private[spark] object KafkaTokenUtil extends Logging {\n+  val TOKEN_KIND = new Text(\"KAFKA_DELEGATION_TOKEN\")\n+  val TOKEN_SERVICE = new Text(\"kafka.server.delegation.token\")\n+\n+  private[spark] class KafkaDelegationTokenIdentifier extends AbstractDelegationTokenIdentifier {\n+    override def getKind: Text = TOKEN_KIND\n+  }\n+\n+  private[security] def obtainToken(sparkConf: SparkConf): (Token[_ <: TokenIdentifier], Long) = {\n+    val adminClient = AdminClient.create(createAdminClientProperties(sparkConf))\n+    val createDelegationTokenOptions = new CreateDelegationTokenOptions()\n+    val createResult = adminClient.createDelegationToken(createDelegationTokenOptions)\n+    val token = createResult.delegationToken().get()\n+    printToken(token)\n+\n+    (new Token[KafkaDelegationTokenIdentifier](\n+      token.tokenInfo.tokenId.getBytes,\n+      token.hmacAsBase64String.getBytes,\n+      TOKEN_KIND,\n+      TOKEN_SERVICE\n+    ), token.tokenInfo.expiryTimestamp)\n+  }\n+\n+  private[security] def createAdminClientProperties(sparkConf: SparkConf): Properties = {\n+    val adminClientProperties = new Properties\n+\n+    val bootstrapServers = sparkConf.get(KAFKA_BOOTSTRAP_SERVERS)\n+    require(bootstrapServers.nonEmpty, s\"Tried to obtain kafka delegation token but bootstrap \" +\n+      \"servers not configured.\")\n+    adminClientProperties.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers.get)\n+\n+    val protocol = sparkConf.get(KAFKA_SECURITY_PROTOCOL)\n+    adminClientProperties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, protocol)\n+    protocol match {\n+      case SASL_SSL.name =>\n+        setTrustStoreProperties(sparkConf, adminClientProperties)\n+\n+      case SSL.name =>\n+        setTrustStoreProperties(sparkConf, adminClientProperties)\n+        setKeyStoreProperties(sparkConf, adminClientProperties)\n+        logWarning(\"Obtaining kafka delegation token with SSL protocol. Please \" +\n+          \"configure 2-way authentication on the broker side.\")\n+\n+      case SASL_PLAINTEXT.name =>\n+        logWarning(\"Obtaining kafka delegation token through plain communication channel. Please \" +\n+          \"consider the security impact.\")\n+    }\n+\n+    // There are multiple possibilities to log in:"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "+ non-normative list of [what I think are the relevant JVM Settings](https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/KDiag.java#L88)",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-11-28T18:26:54Z",
    "diffHunk": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import java.text.SimpleDateFormat\n+import java.util.Properties\n+\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier\n+import org.apache.kafka.clients.CommonClientConfigs\n+import org.apache.kafka.clients.admin.{AdminClient, CreateDelegationTokenOptions}\n+import org.apache.kafka.common.config.SaslConfigs\n+import org.apache.kafka.common.security.auth.SecurityProtocol.{SASL_PLAINTEXT, SASL_SSL, SSL}\n+import org.apache.kafka.common.security.token.delegation.DelegationToken\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+\n+private[spark] object KafkaTokenUtil extends Logging {\n+  val TOKEN_KIND = new Text(\"KAFKA_DELEGATION_TOKEN\")\n+  val TOKEN_SERVICE = new Text(\"kafka.server.delegation.token\")\n+\n+  private[spark] class KafkaDelegationTokenIdentifier extends AbstractDelegationTokenIdentifier {\n+    override def getKind: Text = TOKEN_KIND\n+  }\n+\n+  private[security] def obtainToken(sparkConf: SparkConf): (Token[_ <: TokenIdentifier], Long) = {\n+    val adminClient = AdminClient.create(createAdminClientProperties(sparkConf))\n+    val createDelegationTokenOptions = new CreateDelegationTokenOptions()\n+    val createResult = adminClient.createDelegationToken(createDelegationTokenOptions)\n+    val token = createResult.delegationToken().get()\n+    printToken(token)\n+\n+    (new Token[KafkaDelegationTokenIdentifier](\n+      token.tokenInfo.tokenId.getBytes,\n+      token.hmacAsBase64String.getBytes,\n+      TOKEN_KIND,\n+      TOKEN_SERVICE\n+    ), token.tokenInfo.expiryTimestamp)\n+  }\n+\n+  private[security] def createAdminClientProperties(sparkConf: SparkConf): Properties = {\n+    val adminClientProperties = new Properties\n+\n+    val bootstrapServers = sparkConf.get(KAFKA_BOOTSTRAP_SERVERS)\n+    require(bootstrapServers.nonEmpty, s\"Tried to obtain kafka delegation token but bootstrap \" +\n+      \"servers not configured.\")\n+    adminClientProperties.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers.get)\n+\n+    val protocol = sparkConf.get(KAFKA_SECURITY_PROTOCOL)\n+    adminClientProperties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, protocol)\n+    protocol match {\n+      case SASL_SSL.name =>\n+        setTrustStoreProperties(sparkConf, adminClientProperties)\n+\n+      case SSL.name =>\n+        setTrustStoreProperties(sparkConf, adminClientProperties)\n+        setKeyStoreProperties(sparkConf, adminClientProperties)\n+        logWarning(\"Obtaining kafka delegation token with SSL protocol. Please \" +\n+          \"configure 2-way authentication on the broker side.\")\n+\n+      case SASL_PLAINTEXT.name =>\n+        logWarning(\"Obtaining kafka delegation token through plain communication channel. Please \" +\n+          \"consider the security impact.\")\n+    }\n+\n+    // There are multiple possibilities to log in:"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Kafka has it's public stuff to check for client config: `JaasContext.loadClientContext`. It throws exception when no config provided. This part works, testing the others...",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-11-28T19:45:54Z",
    "diffHunk": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import java.text.SimpleDateFormat\n+import java.util.Properties\n+\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier\n+import org.apache.kafka.clients.CommonClientConfigs\n+import org.apache.kafka.clients.admin.{AdminClient, CreateDelegationTokenOptions}\n+import org.apache.kafka.common.config.SaslConfigs\n+import org.apache.kafka.common.security.auth.SecurityProtocol.{SASL_PLAINTEXT, SASL_SSL, SSL}\n+import org.apache.kafka.common.security.token.delegation.DelegationToken\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+\n+private[spark] object KafkaTokenUtil extends Logging {\n+  val TOKEN_KIND = new Text(\"KAFKA_DELEGATION_TOKEN\")\n+  val TOKEN_SERVICE = new Text(\"kafka.server.delegation.token\")\n+\n+  private[spark] class KafkaDelegationTokenIdentifier extends AbstractDelegationTokenIdentifier {\n+    override def getKind: Text = TOKEN_KIND\n+  }\n+\n+  private[security] def obtainToken(sparkConf: SparkConf): (Token[_ <: TokenIdentifier], Long) = {\n+    val adminClient = AdminClient.create(createAdminClientProperties(sparkConf))\n+    val createDelegationTokenOptions = new CreateDelegationTokenOptions()\n+    val createResult = adminClient.createDelegationToken(createDelegationTokenOptions)\n+    val token = createResult.delegationToken().get()\n+    printToken(token)\n+\n+    (new Token[KafkaDelegationTokenIdentifier](\n+      token.tokenInfo.tokenId.getBytes,\n+      token.hmacAsBase64String.getBytes,\n+      TOKEN_KIND,\n+      TOKEN_SERVICE\n+    ), token.tokenInfo.expiryTimestamp)\n+  }\n+\n+  private[security] def createAdminClientProperties(sparkConf: SparkConf): Properties = {\n+    val adminClientProperties = new Properties\n+\n+    val bootstrapServers = sparkConf.get(KAFKA_BOOTSTRAP_SERVERS)\n+    require(bootstrapServers.nonEmpty, s\"Tried to obtain kafka delegation token but bootstrap \" +\n+      \"servers not configured.\")\n+    adminClientProperties.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers.get)\n+\n+    val protocol = sparkConf.get(KAFKA_SECURITY_PROTOCOL)\n+    adminClientProperties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, protocol)\n+    protocol match {\n+      case SASL_SSL.name =>\n+        setTrustStoreProperties(sparkConf, adminClientProperties)\n+\n+      case SSL.name =>\n+        setTrustStoreProperties(sparkConf, adminClientProperties)\n+        setKeyStoreProperties(sparkConf, adminClientProperties)\n+        logWarning(\"Obtaining kafka delegation token with SSL protocol. Please \" +\n+          \"configure 2-way authentication on the broker side.\")\n+\n+      case SASL_PLAINTEXT.name =>\n+        logWarning(\"Obtaining kafka delegation token through plain communication channel. Please \" +\n+          \"consider the security impact.\")\n+    }\n+\n+    // There are multiple possibilities to log in:"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Added + tested the fallback mechanism.",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-11-29T13:49:39Z",
    "diffHunk": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import java.text.SimpleDateFormat\n+import java.util.Properties\n+\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier\n+import org.apache.kafka.clients.CommonClientConfigs\n+import org.apache.kafka.clients.admin.{AdminClient, CreateDelegationTokenOptions}\n+import org.apache.kafka.common.config.SaslConfigs\n+import org.apache.kafka.common.security.auth.SecurityProtocol.{SASL_PLAINTEXT, SASL_SSL, SSL}\n+import org.apache.kafka.common.security.token.delegation.DelegationToken\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+\n+private[spark] object KafkaTokenUtil extends Logging {\n+  val TOKEN_KIND = new Text(\"KAFKA_DELEGATION_TOKEN\")\n+  val TOKEN_SERVICE = new Text(\"kafka.server.delegation.token\")\n+\n+  private[spark] class KafkaDelegationTokenIdentifier extends AbstractDelegationTokenIdentifier {\n+    override def getKind: Text = TOKEN_KIND\n+  }\n+\n+  private[security] def obtainToken(sparkConf: SparkConf): (Token[_ <: TokenIdentifier], Long) = {\n+    val adminClient = AdminClient.create(createAdminClientProperties(sparkConf))\n+    val createDelegationTokenOptions = new CreateDelegationTokenOptions()\n+    val createResult = adminClient.createDelegationToken(createDelegationTokenOptions)\n+    val token = createResult.delegationToken().get()\n+    printToken(token)\n+\n+    (new Token[KafkaDelegationTokenIdentifier](\n+      token.tokenInfo.tokenId.getBytes,\n+      token.hmacAsBase64String.getBytes,\n+      TOKEN_KIND,\n+      TOKEN_SERVICE\n+    ), token.tokenInfo.expiryTimestamp)\n+  }\n+\n+  private[security] def createAdminClientProperties(sparkConf: SparkConf): Properties = {\n+    val adminClientProperties = new Properties\n+\n+    val bootstrapServers = sparkConf.get(KAFKA_BOOTSTRAP_SERVERS)\n+    require(bootstrapServers.nonEmpty, s\"Tried to obtain kafka delegation token but bootstrap \" +\n+      \"servers not configured.\")\n+    adminClientProperties.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers.get)\n+\n+    val protocol = sparkConf.get(KAFKA_SECURITY_PROTOCOL)\n+    adminClientProperties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, protocol)\n+    protocol match {\n+      case SASL_SSL.name =>\n+        setTrustStoreProperties(sparkConf, adminClientProperties)\n+\n+      case SSL.name =>\n+        setTrustStoreProperties(sparkConf, adminClientProperties)\n+        setKeyStoreProperties(sparkConf, adminClientProperties)\n+        logWarning(\"Obtaining kafka delegation token with SSL protocol. Please \" +\n+          \"configure 2-way authentication on the broker side.\")\n+\n+      case SASL_PLAINTEXT.name =>\n+        logWarning(\"Obtaining kafka delegation token through plain communication channel. Please \" +\n+          \"consider the security impact.\")\n+    }\n+\n+    // There are multiple possibilities to log in:"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Now the following order applies:\r\n* Global JVM configuration which is kafka specific (kafka looks for KafkaClient entry)\r\n  This can be configured many ways not just `java.security.auth.login.config` but the mentioned `JaasContext.loadClientContext` handles them.\r\n* Keytab\r\n* Ticket cache\r\nI've described this in the code as well to make the intention clear.\r\n",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-11-29T13:57:24Z",
    "diffHunk": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import java.text.SimpleDateFormat\n+import java.util.Properties\n+\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier\n+import org.apache.kafka.clients.CommonClientConfigs\n+import org.apache.kafka.clients.admin.{AdminClient, CreateDelegationTokenOptions}\n+import org.apache.kafka.common.config.SaslConfigs\n+import org.apache.kafka.common.security.auth.SecurityProtocol.{SASL_PLAINTEXT, SASL_SSL, SSL}\n+import org.apache.kafka.common.security.token.delegation.DelegationToken\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+\n+private[spark] object KafkaTokenUtil extends Logging {\n+  val TOKEN_KIND = new Text(\"KAFKA_DELEGATION_TOKEN\")\n+  val TOKEN_SERVICE = new Text(\"kafka.server.delegation.token\")\n+\n+  private[spark] class KafkaDelegationTokenIdentifier extends AbstractDelegationTokenIdentifier {\n+    override def getKind: Text = TOKEN_KIND\n+  }\n+\n+  private[security] def obtainToken(sparkConf: SparkConf): (Token[_ <: TokenIdentifier], Long) = {\n+    val adminClient = AdminClient.create(createAdminClientProperties(sparkConf))\n+    val createDelegationTokenOptions = new CreateDelegationTokenOptions()\n+    val createResult = adminClient.createDelegationToken(createDelegationTokenOptions)\n+    val token = createResult.delegationToken().get()\n+    printToken(token)\n+\n+    (new Token[KafkaDelegationTokenIdentifier](\n+      token.tokenInfo.tokenId.getBytes,\n+      token.hmacAsBase64String.getBytes,\n+      TOKEN_KIND,\n+      TOKEN_SERVICE\n+    ), token.tokenInfo.expiryTimestamp)\n+  }\n+\n+  private[security] def createAdminClientProperties(sparkConf: SparkConf): Properties = {\n+    val adminClientProperties = new Properties\n+\n+    val bootstrapServers = sparkConf.get(KAFKA_BOOTSTRAP_SERVERS)\n+    require(bootstrapServers.nonEmpty, s\"Tried to obtain kafka delegation token but bootstrap \" +\n+      \"servers not configured.\")\n+    adminClientProperties.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers.get)\n+\n+    val protocol = sparkConf.get(KAFKA_SECURITY_PROTOCOL)\n+    adminClientProperties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, protocol)\n+    protocol match {\n+      case SASL_SSL.name =>\n+        setTrustStoreProperties(sparkConf, adminClientProperties)\n+\n+      case SSL.name =>\n+        setTrustStoreProperties(sparkConf, adminClientProperties)\n+        setKeyStoreProperties(sparkConf, adminClientProperties)\n+        logWarning(\"Obtaining kafka delegation token with SSL protocol. Please \" +\n+          \"configure 2-way authentication on the broker side.\")\n+\n+      case SASL_PLAINTEXT.name =>\n+        logWarning(\"Obtaining kafka delegation token through plain communication channel. Please \" +\n+          \"consider the security impact.\")\n+    }\n+\n+    // There are multiple possibilities to log in:"
  }],
  "prId": 22598
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Mention the Kafka bug here?",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-11-29T19:10:07Z",
    "diffHunk": "@@ -0,0 +1,200 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import java.{ util => ju }\n+import java.text.SimpleDateFormat\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier\n+import org.apache.kafka.clients.CommonClientConfigs\n+import org.apache.kafka.clients.admin.{AdminClient, CreateDelegationTokenOptions}\n+import org.apache.kafka.common.config.SaslConfigs\n+import org.apache.kafka.common.security.JaasContext\n+import org.apache.kafka.common.security.auth.SecurityProtocol.{SASL_PLAINTEXT, SASL_SSL, SSL}\n+import org.apache.kafka.common.security.token.delegation.DelegationToken\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+\n+private[spark] object KafkaTokenUtil extends Logging {\n+  val TOKEN_KIND = new Text(\"KAFKA_DELEGATION_TOKEN\")\n+  val TOKEN_SERVICE = new Text(\"kafka.server.delegation.token\")\n+\n+  private[spark] class KafkaDelegationTokenIdentifier extends AbstractDelegationTokenIdentifier {\n+    override def getKind: Text = TOKEN_KIND\n+  }\n+\n+  private[security] def obtainToken(sparkConf: SparkConf): (Token[_ <: TokenIdentifier], Long) = {\n+    val adminClient = AdminClient.create(createAdminClientProperties(sparkConf))\n+    val createDelegationTokenOptions = new CreateDelegationTokenOptions()\n+    val createResult = adminClient.createDelegationToken(createDelegationTokenOptions)\n+    val token = createResult.delegationToken().get()\n+    printToken(token)\n+\n+    (new Token[KafkaDelegationTokenIdentifier](\n+      token.tokenInfo.tokenId.getBytes,\n+      token.hmacAsBase64String.getBytes,\n+      TOKEN_KIND,\n+      TOKEN_SERVICE\n+    ), token.tokenInfo.expiryTimestamp)\n+  }\n+\n+  private[security] def createAdminClientProperties(sparkConf: SparkConf): ju.Properties = {\n+    val adminClientProperties = new ju.Properties\n+\n+    val bootstrapServers = sparkConf.get(KAFKA_BOOTSTRAP_SERVERS)\n+    require(bootstrapServers.nonEmpty, s\"Tried to obtain kafka delegation token but bootstrap \" +\n+      \"servers not configured.\")\n+    adminClientProperties.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers.get)\n+\n+    val protocol = sparkConf.get(KAFKA_SECURITY_PROTOCOL)\n+    adminClientProperties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, protocol)\n+    protocol match {\n+      case SASL_SSL.name =>\n+        setTrustStoreProperties(sparkConf, adminClientProperties)\n+\n+      case SSL.name =>\n+        setTrustStoreProperties(sparkConf, adminClientProperties)\n+        setKeyStoreProperties(sparkConf, adminClientProperties)\n+        logWarning(\"Obtaining kafka delegation token with SSL protocol. Please \" +\n+          \"configure 2-way authentication on the broker side.\")\n+\n+      case SASL_PLAINTEXT.name =>\n+        logWarning(\"Obtaining kafka delegation token through plain communication channel. Please \" +\n+          \"consider the security impact.\")\n+    }\n+\n+    // There are multiple possibilities to log in and applied in the following order:",
    "line": 87
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Added.",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-11-29T20:59:36Z",
    "diffHunk": "@@ -0,0 +1,200 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.security\n+\n+import java.{ util => ju }\n+import java.text.SimpleDateFormat\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.security.token.{Token, TokenIdentifier}\n+import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier\n+import org.apache.kafka.clients.CommonClientConfigs\n+import org.apache.kafka.clients.admin.{AdminClient, CreateDelegationTokenOptions}\n+import org.apache.kafka.common.config.SaslConfigs\n+import org.apache.kafka.common.security.JaasContext\n+import org.apache.kafka.common.security.auth.SecurityProtocol.{SASL_PLAINTEXT, SASL_SSL, SSL}\n+import org.apache.kafka.common.security.token.delegation.DelegationToken\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+\n+private[spark] object KafkaTokenUtil extends Logging {\n+  val TOKEN_KIND = new Text(\"KAFKA_DELEGATION_TOKEN\")\n+  val TOKEN_SERVICE = new Text(\"kafka.server.delegation.token\")\n+\n+  private[spark] class KafkaDelegationTokenIdentifier extends AbstractDelegationTokenIdentifier {\n+    override def getKind: Text = TOKEN_KIND\n+  }\n+\n+  private[security] def obtainToken(sparkConf: SparkConf): (Token[_ <: TokenIdentifier], Long) = {\n+    val adminClient = AdminClient.create(createAdminClientProperties(sparkConf))\n+    val createDelegationTokenOptions = new CreateDelegationTokenOptions()\n+    val createResult = adminClient.createDelegationToken(createDelegationTokenOptions)\n+    val token = createResult.delegationToken().get()\n+    printToken(token)\n+\n+    (new Token[KafkaDelegationTokenIdentifier](\n+      token.tokenInfo.tokenId.getBytes,\n+      token.hmacAsBase64String.getBytes,\n+      TOKEN_KIND,\n+      TOKEN_SERVICE\n+    ), token.tokenInfo.expiryTimestamp)\n+  }\n+\n+  private[security] def createAdminClientProperties(sparkConf: SparkConf): ju.Properties = {\n+    val adminClientProperties = new ju.Properties\n+\n+    val bootstrapServers = sparkConf.get(KAFKA_BOOTSTRAP_SERVERS)\n+    require(bootstrapServers.nonEmpty, s\"Tried to obtain kafka delegation token but bootstrap \" +\n+      \"servers not configured.\")\n+    adminClientProperties.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers.get)\n+\n+    val protocol = sparkConf.get(KAFKA_SECURITY_PROTOCOL)\n+    adminClientProperties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, protocol)\n+    protocol match {\n+      case SASL_SSL.name =>\n+        setTrustStoreProperties(sparkConf, adminClientProperties)\n+\n+      case SSL.name =>\n+        setTrustStoreProperties(sparkConf, adminClientProperties)\n+        setKeyStoreProperties(sparkConf, adminClientProperties)\n+        logWarning(\"Obtaining kafka delegation token with SSL protocol. Please \" +\n+          \"configure 2-way authentication on the broker side.\")\n+\n+      case SASL_PLAINTEXT.name =>\n+        logWarning(\"Obtaining kafka delegation token through plain communication channel. Please \" +\n+          \"consider the security impact.\")\n+    }\n+\n+    // There are multiple possibilities to log in and applied in the following order:",
    "line": 87
  }],
  "prId": 22598
}]