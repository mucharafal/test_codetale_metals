[{
  "comments": [{
    "author": {
      "login": "tgravescs"
    },
    "body": "I'm really not following what you are saying here with the manually run it first.  Doesn't that defeat the purpose.  Note that I login from keytab for the spark history server to read hdfs file.  Perhaps go take a look at that.  HistoryServer.initSecurity().\n\nAlso there is a SparkHadoopUtil.loginUserFromKeytab utility function \n",
    "commit": "626318d8bd89bc2f5121b8cf11d5bea07f426b1c",
    "createdAt": "2015-01-21T16:40:23Z",
    "diffHunk": "@@ -0,0 +1,76 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.security.PrivilegedAction\n+\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.spark.{SparkEnv, Logging}\n+import sun.security.jgss.krb5.Krb5InitCredential\n+\n+import scala.sys.process.Process\n+\n+private[spark] class StandaloneSparkHadoopUtil extends SparkHadoopUtil {\n+\n+  val hadoopSecurityAuthenticationKey = \"hadoop.security.authentication\"\n+  val principalKey = \"dfs.namenode.kerberos.principal\"\n+  val keytabKey = \"dfs.namenode.keytab.file\"\n+  val securityEnabledButKeyNotDefined = \"Hadoop security was enabled, but %s\" +\n+    \"was not set in the hadoop configuration.\"\n+\n+  lazy val configuredPrivilegedUGI: UserGroupInformation = {\n+    if (\"kerberos\".equals(conf.get(hadoopSecurityAuthenticationKey, \"simple\"))) {\n+      logInfo(\"Setting up kerberos to authenticate\")\n+      getConfValueOrNone(principalKey) match {\n+        case Some(principal) =>\n+          getConfValueOrNone(keytabKey) match {\n+            case Some(keytab) =>\n+              // Log in with the HDFS principal. However, this does NOT get the TGT\n+              // kinit must be executed manually in spark-env.sh or somewhere similar\n+              UserGroupInformation.loginUserFromKeytabAndReturnUGI(principal, keytab)"
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "I really don't understand this fully either, most of the comments are from the long time I spent empirically testing this. But when I explicitly ran kdestroy -A before launching Spark and reading from HDFS, I kept receiving stack traces complaining about no tgt (ticket-granting-ticket) being there, despite the fact that I had invoked loginUserFromKeytab(). What's worth remarking is that once kinit is run once, the ticket can be renewed, or the kerberos ticket configured so that it never needs to be renewed at all. I'm not sure - are users of the history server merely running kinit once and then letting the ticket be renewed often enough or setting the tickets' lifetime to indefinite?\n",
    "commit": "626318d8bd89bc2f5121b8cf11d5bea07f426b1c",
    "createdAt": "2015-01-21T17:33:06Z",
    "diffHunk": "@@ -0,0 +1,76 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.security.PrivilegedAction\n+\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.spark.{SparkEnv, Logging}\n+import sun.security.jgss.krb5.Krb5InitCredential\n+\n+import scala.sys.process.Process\n+\n+private[spark] class StandaloneSparkHadoopUtil extends SparkHadoopUtil {\n+\n+  val hadoopSecurityAuthenticationKey = \"hadoop.security.authentication\"\n+  val principalKey = \"dfs.namenode.kerberos.principal\"\n+  val keytabKey = \"dfs.namenode.keytab.file\"\n+  val securityEnabledButKeyNotDefined = \"Hadoop security was enabled, but %s\" +\n+    \"was not set in the hadoop configuration.\"\n+\n+  lazy val configuredPrivilegedUGI: UserGroupInformation = {\n+    if (\"kerberos\".equals(conf.get(hadoopSecurityAuthenticationKey, \"simple\"))) {\n+      logInfo(\"Setting up kerberos to authenticate\")\n+      getConfValueOrNone(principalKey) match {\n+        case Some(principal) =>\n+          getConfValueOrNone(keytabKey) match {\n+            case Some(keytab) =>\n+              // Log in with the HDFS principal. However, this does NOT get the TGT\n+              // kinit must be executed manually in spark-env.sh or somewhere similar\n+              UserGroupInformation.loginUserFromKeytabAndReturnUGI(principal, keytab)"
  }, {
    "author": {
      "login": "tgravescs"
    },
    "body": "You do not need to run kinit before launching the history server.  It logs in using the keytab specified.  The Hadoop rpc mechanism will handle relogging in from the keytab when necessary. so as long as it continues to try to access HDFS its fine. \n",
    "commit": "626318d8bd89bc2f5121b8cf11d5bea07f426b1c",
    "createdAt": "2015-01-21T19:30:19Z",
    "diffHunk": "@@ -0,0 +1,76 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.security.PrivilegedAction\n+\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.spark.{SparkEnv, Logging}\n+import sun.security.jgss.krb5.Krb5InitCredential\n+\n+import scala.sys.process.Process\n+\n+private[spark] class StandaloneSparkHadoopUtil extends SparkHadoopUtil {\n+\n+  val hadoopSecurityAuthenticationKey = \"hadoop.security.authentication\"\n+  val principalKey = \"dfs.namenode.kerberos.principal\"\n+  val keytabKey = \"dfs.namenode.keytab.file\"\n+  val securityEnabledButKeyNotDefined = \"Hadoop security was enabled, but %s\" +\n+    \"was not set in the hadoop configuration.\"\n+\n+  lazy val configuredPrivilegedUGI: UserGroupInformation = {\n+    if (\"kerberos\".equals(conf.get(hadoopSecurityAuthenticationKey, \"simple\"))) {\n+      logInfo(\"Setting up kerberos to authenticate\")\n+      getConfValueOrNone(principalKey) match {\n+        case Some(principal) =>\n+          getConfValueOrNone(keytabKey) match {\n+            case Some(keytab) =>\n+              // Log in with the HDFS principal. However, this does NOT get the TGT\n+              // kinit must be executed manually in spark-env.sh or somewhere similar\n+              UserGroupInformation.loginUserFromKeytabAndReturnUGI(principal, keytab)"
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "I'm confused as to why this doesn't work on this code as well then. Clearly the login call is made with the right principal and keytab. I tried to figure this out for several hours and got nowhere, do you have any idea why the analogous isn't working here?\n",
    "commit": "626318d8bd89bc2f5121b8cf11d5bea07f426b1c",
    "createdAt": "2015-01-21T19:33:08Z",
    "diffHunk": "@@ -0,0 +1,76 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.security.PrivilegedAction\n+\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.spark.{SparkEnv, Logging}\n+import sun.security.jgss.krb5.Krb5InitCredential\n+\n+import scala.sys.process.Process\n+\n+private[spark] class StandaloneSparkHadoopUtil extends SparkHadoopUtil {\n+\n+  val hadoopSecurityAuthenticationKey = \"hadoop.security.authentication\"\n+  val principalKey = \"dfs.namenode.kerberos.principal\"\n+  val keytabKey = \"dfs.namenode.keytab.file\"\n+  val securityEnabledButKeyNotDefined = \"Hadoop security was enabled, but %s\" +\n+    \"was not set in the hadoop configuration.\"\n+\n+  lazy val configuredPrivilegedUGI: UserGroupInformation = {\n+    if (\"kerberos\".equals(conf.get(hadoopSecurityAuthenticationKey, \"simple\"))) {\n+      logInfo(\"Setting up kerberos to authenticate\")\n+      getConfValueOrNone(principalKey) match {\n+        case Some(principal) =>\n+          getConfValueOrNone(keytabKey) match {\n+            case Some(keytab) =>\n+              // Log in with the HDFS principal. However, this does NOT get the TGT\n+              // kinit must be executed manually in spark-env.sh or somewhere similar\n+              UserGroupInformation.loginUserFromKeytabAndReturnUGI(principal, keytab)"
  }, {
    "author": {
      "login": "pwendell"
    },
    "body": "@tgravescs if you have any idea it would be helpful (personally not really keyed in on how this works). @mccheah my understanding was that in general if you have a keytab you shouldn't need to run kinit ever in order for Hadoop to be able to log in.\n\nYou might take a look at how this was done in previous patches that also tried to do something similar:\n\nhttps://github.com/apache/spark/pull/2320/files#diff-7f5edfa5bc1efcd65bab2f7e0e3320a5R222\n",
    "commit": "626318d8bd89bc2f5121b8cf11d5bea07f426b1c",
    "createdAt": "2015-01-30T00:44:35Z",
    "diffHunk": "@@ -0,0 +1,76 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.security.PrivilegedAction\n+\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.spark.{SparkEnv, Logging}\n+import sun.security.jgss.krb5.Krb5InitCredential\n+\n+import scala.sys.process.Process\n+\n+private[spark] class StandaloneSparkHadoopUtil extends SparkHadoopUtil {\n+\n+  val hadoopSecurityAuthenticationKey = \"hadoop.security.authentication\"\n+  val principalKey = \"dfs.namenode.kerberos.principal\"\n+  val keytabKey = \"dfs.namenode.keytab.file\"\n+  val securityEnabledButKeyNotDefined = \"Hadoop security was enabled, but %s\" +\n+    \"was not set in the hadoop configuration.\"\n+\n+  lazy val configuredPrivilegedUGI: UserGroupInformation = {\n+    if (\"kerberos\".equals(conf.get(hadoopSecurityAuthenticationKey, \"simple\"))) {\n+      logInfo(\"Setting up kerberos to authenticate\")\n+      getConfValueOrNone(principalKey) match {\n+        case Some(principal) =>\n+          getConfValueOrNone(keytabKey) match {\n+            case Some(keytab) =>\n+              // Log in with the HDFS principal. However, this does NOT get the TGT\n+              // kinit must be executed manually in spark-env.sh or somewhere similar\n+              UserGroupInformation.loginUserFromKeytabAndReturnUGI(principal, keytab)"
  }],
  "prId": 4106
}, {
  "comments": [{
    "author": {
      "login": "tgravescs"
    },
    "body": "I think it would be better for these to all be spark. configs\n",
    "commit": "626318d8bd89bc2f5121b8cf11d5bea07f426b1c",
    "createdAt": "2015-01-21T16:42:47Z",
    "diffHunk": "@@ -0,0 +1,76 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.security.PrivilegedAction\n+\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.spark.{SparkEnv, Logging}\n+import sun.security.jgss.krb5.Krb5InitCredential\n+\n+import scala.sys.process.Process\n+\n+private[spark] class StandaloneSparkHadoopUtil extends SparkHadoopUtil {\n+\n+  val hadoopSecurityAuthenticationKey = \"hadoop.security.authentication\""
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "I was debating that - would like @pwendell to weigh in here. I figured it would be better to reduce information redundancy; presumably hdfs-site.xml configurations already defined these principals and keytabs. But I'm probably just splitting hairs at that point.\n",
    "commit": "626318d8bd89bc2f5121b8cf11d5bea07f426b1c",
    "createdAt": "2015-01-21T18:18:43Z",
    "diffHunk": "@@ -0,0 +1,76 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.security.PrivilegedAction\n+\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.spark.{SparkEnv, Logging}\n+import sun.security.jgss.krb5.Krb5InitCredential\n+\n+import scala.sys.process.Process\n+\n+private[spark] class StandaloneSparkHadoopUtil extends SparkHadoopUtil {\n+\n+  val hadoopSecurityAuthenticationKey = \"hadoop.security.authentication\""
  }, {
    "author": {
      "login": "pwendell"
    },
    "body": "Hey @mccheah I'd actually prefer to have our own configurations for this just to make it more explicit (under the spark namespace), similar to what @tgravescs suggested. It's just easier for people to debug, and more explicit. I think later on we could have a mode where we support reading them from hadoop confs, but IMO it's best to keep this spark local.\n",
    "commit": "626318d8bd89bc2f5121b8cf11d5bea07f426b1c",
    "createdAt": "2015-01-30T00:41:07Z",
    "diffHunk": "@@ -0,0 +1,76 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.security.PrivilegedAction\n+\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.spark.{SparkEnv, Logging}\n+import sun.security.jgss.krb5.Krb5InitCredential\n+\n+import scala.sys.process.Process\n+\n+private[spark] class StandaloneSparkHadoopUtil extends SparkHadoopUtil {\n+\n+  val hadoopSecurityAuthenticationKey = \"hadoop.security.authentication\""
  }],
  "prId": 4106
}]