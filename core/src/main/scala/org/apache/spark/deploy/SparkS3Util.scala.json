[{
  "comments": [{
    "author": {
      "login": "davies"
    },
    "body": "Is the AmazonS3Client thread safe? Or should we use a client for each thread (mapper)?\n",
    "commit": "5bdd92443c33fc2ec72fabb141c878cf3488d1f4",
    "createdAt": "2015-09-08T03:43:48Z",
    "diffHunk": "@@ -0,0 +1,293 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.parallel.ForkJoinTaskSupport\n+import scala.collection.parallel.mutable.ParArray\n+import scala.concurrent.forkjoin.ForkJoinPool\n+\n+import com.amazonaws.{AmazonClientException, ClientConfiguration, Protocol}\n+import com.amazonaws.auth.{AWSCredentialsProvider, BasicAWSCredentials, InstanceProfileCredentialsProvider, STSAssumeRoleSessionCredentialsProvider}\n+import com.amazonaws.internal.StaticCredentialsProvider\n+import com.amazonaws.services.s3.AmazonS3Client\n+import com.amazonaws.services.s3.model.{ListObjectsRequest, ObjectListing, S3ObjectSummary}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{BlockLocation, FileStatus, LocatedFileStatus, Path, RemoteIterator}\n+import org.apache.hadoop.fs.s3.S3Credentials\n+import org.apache.hadoop.io.compress.{CompressionCodecFactory, SplittableCompressionCodec}\n+import org.apache.hadoop.mapred.{FileSplit, InputSplit, JobConf}\n+\n+import org.apache.spark.{SparkConf, Logging}\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.util.Utils\n+\n+import com.google.common.base.{Preconditions, Strings}\n+import com.google.common.cache.{Cache, CacheBuilder}\n+import com.google.common.collect.AbstractSequentialIterator\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Contains util methods to interact with S3 from Spark.\n+ */\n+@DeveloperApi\n+object SparkS3Util extends Logging {\n+  private val sparkConf = new SparkConf()\n+  val conf: Configuration = SparkHadoopUtil.get.newConfiguration(sparkConf)\n+\n+  private val s3ClientCache: Cache[String, AmazonS3Client] = CacheBuilder\n+    .newBuilder\n+    .concurrencyLevel(Runtime.getRuntime.availableProcessors)\n+    .build[String, AmazonS3Client]\n+\n+  private val S3_CONNECT_TIMEOUT: String = \"spark.s3.connect.timeout\"\n+  private val S3_MAX_CONNECTIONS: String = \"spark.s3.max.connections\"\n+  private val S3_MAX_ERROR_RETRIES: String = \"spark.s3.max.error.retries\"\n+  private val S3_SOCKET_TIMEOUT: String = \"spark.s3.socket.timeout\"\n+  private val S3_SSL_ENABLED: String = \"spark.s3.ssl.enabled\"\n+  private val S3_USE_INSTANCE_CREDENTIALS: String = \"spark.s3.use.instance.credentials\"\n+\n+  private val FILEINPUTFORMAT_INPUTDIR: String =\n+    \"mapreduce.input.fileinputformat.inputdir\"\n+  private val FILEINPUTFORMAT_LIST_STATUS_NUM_THREADS: String =\n+    \"mapreduce.input.fileinputformat.list-status.num-threads\"\n+  private val FILEINPUTFORMAT_NUMINPUTFILES: String =\n+    \"mapreduce.input.fileinputformat.numinputfiles\"\n+  private val FILEINPUTFORMAT_SPLIT_MINSIZE: String =\n+    \"mapreduce.input.fileinputformat.split.minsize\"\n+\n+  private def getS3Client(bucket: String): AmazonS3Client = {\n+    val sslEnabled: Boolean = sparkConf.getBoolean(S3_SSL_ENABLED, true)\n+    val maxErrorRetries: Int = sparkConf.getInt(S3_MAX_ERROR_RETRIES, 10)\n+    val connectTimeout: Int = sparkConf.getInt(S3_CONNECT_TIMEOUT, 5000)\n+    val socketTimeout: Int = sparkConf.getInt(S3_SOCKET_TIMEOUT, 5000)\n+    val maxConnections: Int = sparkConf.getInt(S3_MAX_CONNECTIONS, 500)\n+    val useInstanceCredentials: Boolean = sparkConf.getBoolean(S3_USE_INSTANCE_CREDENTIALS, false)\n+\n+    val clientConf: ClientConfiguration = new ClientConfiguration\n+    clientConf.setMaxErrorRetry(maxErrorRetries)\n+    clientConf.setProtocol(if (sslEnabled) Protocol.HTTPS else Protocol.HTTP)\n+    clientConf.setConnectionTimeout(connectTimeout)\n+    clientConf.setSocketTimeout(socketTimeout)\n+    clientConf.setMaxConnections(maxConnections)\n+\n+    val s3RoleArn = Option(conf.get(\"aws.iam.role.arn\"))\n+    val s3RoleArnDefault = Option(conf.get(\"aws.iam.role.arn.default\"))\n+    val credentialsProvider: AWSCredentialsProvider =\n+      s3RoleArn match {\n+        case Some(role) =>\n+          logDebug(\"Use user-specified IAM role: \" + role)\n+          new STSAssumeRoleSessionCredentialsProvider(\n+            role, \"RoleSessionName-\" + Utils.random.nextInt)\n+        case None if useInstanceCredentials =>\n+          logDebug(\"Use IAM role associated with the instance\")\n+          new InstanceProfileCredentialsProvider\n+        case _ =>\n+          s3RoleArnDefault match {\n+            case Some(role) =>\n+              logDebug(\"Use default IAM role configured in Hadoop config: \" + role)\n+              new STSAssumeRoleSessionCredentialsProvider(\n+                role, \"RoleSessionName-\" + Utils.random.nextInt)\n+            case _ =>\n+              try {\n+                logDebug(\"Use AWS key pairs\")\n+                val credentials: S3Credentials = new S3Credentials\n+                credentials.initialize(URI.create(bucket), conf)\n+                new StaticCredentialsProvider(\n+                  new BasicAWSCredentials(credentials.getAccessKey, credentials.getSecretAccessKey))\n+              } catch {\n+                case e: Exception => throw new RuntimeException(\"S3 credentials not configured\", e)\n+              }\n+        }\n+      }\n+\n+    val providerName: String = credentialsProvider.getClass.getSimpleName\n+    val s3ClientKey: String =\n+      if (s3RoleArn == null) providerName\n+      else providerName + \"_\" + s3RoleArn\n+\n+    Option(s3ClientCache.getIfPresent(s3ClientKey)).getOrElse {\n+      val newClient = new AmazonS3Client(credentialsProvider, clientConf)\n+      s3ClientCache.put(s3ClientKey, newClient)\n+      newClient\n+    }\n+  }\n+\n+  private def keyFromPath(path: Path): String = {\n+    Preconditions.checkArgument(path.isAbsolute, \"Path is not absolute: %s\", path)\n+    var key: String = Strings.nullToEmpty(path.toUri.getPath)\n+    if (key.startsWith(\"/\")) {\n+      key = key.substring(1)\n+    }\n+    if (key.endsWith(\"/\")) {\n+      key = key.substring(0, key.length - 1)\n+    }\n+    key\n+  }\n+\n+  private def listPrefix(s3: AmazonS3Client, path: Path): Iterator[LocatedFileStatus] = {\n+    val uri: URI = path.toUri\n+    val key: String = keyFromPath(path)\n+    val request: ListObjectsRequest = new ListObjectsRequest()\n+      .withBucketName(uri.getAuthority)\n+      .withPrefix(key)\n+\n+    val listings = new AbstractSequentialIterator[ObjectListing](s3.listObjects(request)) {\n+      protected def computeNext(previous: ObjectListing): ObjectListing = {\n+        if (!previous.isTruncated) {\n+          return null\n+        }\n+        s3.listNextBatchOfObjects(previous)\n+      }\n+    }.asScala\n+\n+    val bucket: String = uri.getScheme + \"://\" + uri.getAuthority\n+    listings\n+      .map(listing => statusFromObjects(bucket, listing.getObjectSummaries))\n+      .reduceLeft(_ ++ _)\n+  }\n+\n+  private def statusFromObjects(\n+      bucket: String,\n+      objects: util.List[S3ObjectSummary]): Iterator[LocatedFileStatus] = {\n+    val blockSize: Int = conf.getInt(\"fs.s3n.block.size\", 67108864)\n+    val list: ArrayBuffer[LocatedFileStatus] = ArrayBuffer()\n+    for (obj: S3ObjectSummary <- objects.asScala) {\n+      if (!obj.getKey.endsWith(\"/\")) {\n+        val status: FileStatus = new FileStatus(\n+          obj.getSize, false, 1, blockSize,\n+          obj.getLastModified.getTime,\n+          new Path(bucket + \"/\" + obj.getKey))\n+        list += createLocatedFileStatus(status)\n+      }\n+    }\n+    list.iterator\n+  }\n+\n+  private def createLocatedFileStatus(status: FileStatus): LocatedFileStatus = {\n+    val fakeLocation: Array[BlockLocation] = Array[BlockLocation]()\n+    new LocatedFileStatus(status, fakeLocation)\n+  }\n+\n+  private def listLocatedStatus(\n+      s3: AmazonS3Client,\n+      path: Path): RemoteIterator[LocatedFileStatus] = {\n+    new RemoteIterator[LocatedFileStatus]() {\n+      private final val iterator: Iterator[LocatedFileStatus] = listPrefix(s3, path)\n+\n+      def hasNext: Boolean = {\n+        try {\n+          iterator.hasNext\n+        } catch {\n+          case e: AmazonClientException => throw new IOException(e)\n+        }\n+      }\n+\n+      def next(): LocatedFileStatus = {\n+        try {\n+          iterator.next()\n+        } catch {\n+          case e: AmazonClientException => throw new IOException(e)\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * List paths in parallel and merge returned `FileStatus` objects into a single list.\n+   */\n+  private def listStatus(\n+      s3: AmazonS3Client,\n+      paths: List[String],\n+      parallelism: Int): Array[FileStatus] = {\n+    val list: ArrayBuffer[FileStatus] = ArrayBuffer()\n+    val parArray: ParArray[String] = paths.toParArray\n+    parArray.tasksupport = new ForkJoinTaskSupport(new ForkJoinPool(parallelism))\n+    parArray\n+      .map { path => listLocatedStatus(s3, new Path(path)) }"
  }, {
    "author": {
      "login": "piaozhexiu"
    },
    "body": "Yes, I am positive. We use AmazonS3Client in multi threads in various tools.\n\nAlso, [AmazonS3Client Java Doc](http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/AmazonS3Client.html) explicitly says if a method is thread unsafe. But `listObjects` and `listNextBatchOfObjects` are not among those thread unsafe methods.\n",
    "commit": "5bdd92443c33fc2ec72fabb141c878cf3488d1f4",
    "createdAt": "2015-09-08T04:36:33Z",
    "diffHunk": "@@ -0,0 +1,293 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+import scala.collection.parallel.ForkJoinTaskSupport\n+import scala.collection.parallel.mutable.ParArray\n+import scala.concurrent.forkjoin.ForkJoinPool\n+\n+import com.amazonaws.{AmazonClientException, ClientConfiguration, Protocol}\n+import com.amazonaws.auth.{AWSCredentialsProvider, BasicAWSCredentials, InstanceProfileCredentialsProvider, STSAssumeRoleSessionCredentialsProvider}\n+import com.amazonaws.internal.StaticCredentialsProvider\n+import com.amazonaws.services.s3.AmazonS3Client\n+import com.amazonaws.services.s3.model.{ListObjectsRequest, ObjectListing, S3ObjectSummary}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{BlockLocation, FileStatus, LocatedFileStatus, Path, RemoteIterator}\n+import org.apache.hadoop.fs.s3.S3Credentials\n+import org.apache.hadoop.io.compress.{CompressionCodecFactory, SplittableCompressionCodec}\n+import org.apache.hadoop.mapred.{FileSplit, InputSplit, JobConf}\n+\n+import org.apache.spark.{SparkConf, Logging}\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.util.Utils\n+\n+import com.google.common.base.{Preconditions, Strings}\n+import com.google.common.cache.{Cache, CacheBuilder}\n+import com.google.common.collect.AbstractSequentialIterator\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Contains util methods to interact with S3 from Spark.\n+ */\n+@DeveloperApi\n+object SparkS3Util extends Logging {\n+  private val sparkConf = new SparkConf()\n+  val conf: Configuration = SparkHadoopUtil.get.newConfiguration(sparkConf)\n+\n+  private val s3ClientCache: Cache[String, AmazonS3Client] = CacheBuilder\n+    .newBuilder\n+    .concurrencyLevel(Runtime.getRuntime.availableProcessors)\n+    .build[String, AmazonS3Client]\n+\n+  private val S3_CONNECT_TIMEOUT: String = \"spark.s3.connect.timeout\"\n+  private val S3_MAX_CONNECTIONS: String = \"spark.s3.max.connections\"\n+  private val S3_MAX_ERROR_RETRIES: String = \"spark.s3.max.error.retries\"\n+  private val S3_SOCKET_TIMEOUT: String = \"spark.s3.socket.timeout\"\n+  private val S3_SSL_ENABLED: String = \"spark.s3.ssl.enabled\"\n+  private val S3_USE_INSTANCE_CREDENTIALS: String = \"spark.s3.use.instance.credentials\"\n+\n+  private val FILEINPUTFORMAT_INPUTDIR: String =\n+    \"mapreduce.input.fileinputformat.inputdir\"\n+  private val FILEINPUTFORMAT_LIST_STATUS_NUM_THREADS: String =\n+    \"mapreduce.input.fileinputformat.list-status.num-threads\"\n+  private val FILEINPUTFORMAT_NUMINPUTFILES: String =\n+    \"mapreduce.input.fileinputformat.numinputfiles\"\n+  private val FILEINPUTFORMAT_SPLIT_MINSIZE: String =\n+    \"mapreduce.input.fileinputformat.split.minsize\"\n+\n+  private def getS3Client(bucket: String): AmazonS3Client = {\n+    val sslEnabled: Boolean = sparkConf.getBoolean(S3_SSL_ENABLED, true)\n+    val maxErrorRetries: Int = sparkConf.getInt(S3_MAX_ERROR_RETRIES, 10)\n+    val connectTimeout: Int = sparkConf.getInt(S3_CONNECT_TIMEOUT, 5000)\n+    val socketTimeout: Int = sparkConf.getInt(S3_SOCKET_TIMEOUT, 5000)\n+    val maxConnections: Int = sparkConf.getInt(S3_MAX_CONNECTIONS, 500)\n+    val useInstanceCredentials: Boolean = sparkConf.getBoolean(S3_USE_INSTANCE_CREDENTIALS, false)\n+\n+    val clientConf: ClientConfiguration = new ClientConfiguration\n+    clientConf.setMaxErrorRetry(maxErrorRetries)\n+    clientConf.setProtocol(if (sslEnabled) Protocol.HTTPS else Protocol.HTTP)\n+    clientConf.setConnectionTimeout(connectTimeout)\n+    clientConf.setSocketTimeout(socketTimeout)\n+    clientConf.setMaxConnections(maxConnections)\n+\n+    val s3RoleArn = Option(conf.get(\"aws.iam.role.arn\"))\n+    val s3RoleArnDefault = Option(conf.get(\"aws.iam.role.arn.default\"))\n+    val credentialsProvider: AWSCredentialsProvider =\n+      s3RoleArn match {\n+        case Some(role) =>\n+          logDebug(\"Use user-specified IAM role: \" + role)\n+          new STSAssumeRoleSessionCredentialsProvider(\n+            role, \"RoleSessionName-\" + Utils.random.nextInt)\n+        case None if useInstanceCredentials =>\n+          logDebug(\"Use IAM role associated with the instance\")\n+          new InstanceProfileCredentialsProvider\n+        case _ =>\n+          s3RoleArnDefault match {\n+            case Some(role) =>\n+              logDebug(\"Use default IAM role configured in Hadoop config: \" + role)\n+              new STSAssumeRoleSessionCredentialsProvider(\n+                role, \"RoleSessionName-\" + Utils.random.nextInt)\n+            case _ =>\n+              try {\n+                logDebug(\"Use AWS key pairs\")\n+                val credentials: S3Credentials = new S3Credentials\n+                credentials.initialize(URI.create(bucket), conf)\n+                new StaticCredentialsProvider(\n+                  new BasicAWSCredentials(credentials.getAccessKey, credentials.getSecretAccessKey))\n+              } catch {\n+                case e: Exception => throw new RuntimeException(\"S3 credentials not configured\", e)\n+              }\n+        }\n+      }\n+\n+    val providerName: String = credentialsProvider.getClass.getSimpleName\n+    val s3ClientKey: String =\n+      if (s3RoleArn == null) providerName\n+      else providerName + \"_\" + s3RoleArn\n+\n+    Option(s3ClientCache.getIfPresent(s3ClientKey)).getOrElse {\n+      val newClient = new AmazonS3Client(credentialsProvider, clientConf)\n+      s3ClientCache.put(s3ClientKey, newClient)\n+      newClient\n+    }\n+  }\n+\n+  private def keyFromPath(path: Path): String = {\n+    Preconditions.checkArgument(path.isAbsolute, \"Path is not absolute: %s\", path)\n+    var key: String = Strings.nullToEmpty(path.toUri.getPath)\n+    if (key.startsWith(\"/\")) {\n+      key = key.substring(1)\n+    }\n+    if (key.endsWith(\"/\")) {\n+      key = key.substring(0, key.length - 1)\n+    }\n+    key\n+  }\n+\n+  private def listPrefix(s3: AmazonS3Client, path: Path): Iterator[LocatedFileStatus] = {\n+    val uri: URI = path.toUri\n+    val key: String = keyFromPath(path)\n+    val request: ListObjectsRequest = new ListObjectsRequest()\n+      .withBucketName(uri.getAuthority)\n+      .withPrefix(key)\n+\n+    val listings = new AbstractSequentialIterator[ObjectListing](s3.listObjects(request)) {\n+      protected def computeNext(previous: ObjectListing): ObjectListing = {\n+        if (!previous.isTruncated) {\n+          return null\n+        }\n+        s3.listNextBatchOfObjects(previous)\n+      }\n+    }.asScala\n+\n+    val bucket: String = uri.getScheme + \"://\" + uri.getAuthority\n+    listings\n+      .map(listing => statusFromObjects(bucket, listing.getObjectSummaries))\n+      .reduceLeft(_ ++ _)\n+  }\n+\n+  private def statusFromObjects(\n+      bucket: String,\n+      objects: util.List[S3ObjectSummary]): Iterator[LocatedFileStatus] = {\n+    val blockSize: Int = conf.getInt(\"fs.s3n.block.size\", 67108864)\n+    val list: ArrayBuffer[LocatedFileStatus] = ArrayBuffer()\n+    for (obj: S3ObjectSummary <- objects.asScala) {\n+      if (!obj.getKey.endsWith(\"/\")) {\n+        val status: FileStatus = new FileStatus(\n+          obj.getSize, false, 1, blockSize,\n+          obj.getLastModified.getTime,\n+          new Path(bucket + \"/\" + obj.getKey))\n+        list += createLocatedFileStatus(status)\n+      }\n+    }\n+    list.iterator\n+  }\n+\n+  private def createLocatedFileStatus(status: FileStatus): LocatedFileStatus = {\n+    val fakeLocation: Array[BlockLocation] = Array[BlockLocation]()\n+    new LocatedFileStatus(status, fakeLocation)\n+  }\n+\n+  private def listLocatedStatus(\n+      s3: AmazonS3Client,\n+      path: Path): RemoteIterator[LocatedFileStatus] = {\n+    new RemoteIterator[LocatedFileStatus]() {\n+      private final val iterator: Iterator[LocatedFileStatus] = listPrefix(s3, path)\n+\n+      def hasNext: Boolean = {\n+        try {\n+          iterator.hasNext\n+        } catch {\n+          case e: AmazonClientException => throw new IOException(e)\n+        }\n+      }\n+\n+      def next(): LocatedFileStatus = {\n+        try {\n+          iterator.next()\n+        } catch {\n+          case e: AmazonClientException => throw new IOException(e)\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * List paths in parallel and merge returned `FileStatus` objects into a single list.\n+   */\n+  private def listStatus(\n+      s3: AmazonS3Client,\n+      paths: List[String],\n+      parallelism: Int): Array[FileStatus] = {\n+    val list: ArrayBuffer[FileStatus] = ArrayBuffer()\n+    val parArray: ParArray[String] = paths.toParArray\n+    parArray.tasksupport = new ForkJoinTaskSupport(new ForkJoinPool(parallelism))\n+    parArray\n+      .map { path => listLocatedStatus(s3, new Path(path)) }"
  }],
  "prId": 8512
}, {
  "comments": [{
    "author": {
      "login": "liancheng"
    },
    "body": "Unfortunately Hadoop 1 doesn't have `LocatedFileStatus` or `RemoteIterator`. Our PR builder uses Hadoop 2.3.0.\n",
    "commit": "5bdd92443c33fc2ec72fabb141c878cf3488d1f4",
    "createdAt": "2015-09-11T15:47:04Z",
    "diffHunk": "@@ -0,0 +1,299 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.amazonaws.{AmazonClientException, ClientConfiguration, Protocol}\n+import com.amazonaws.auth.{AWSCredentialsProvider, BasicAWSCredentials, InstanceProfileCredentialsProvider, STSAssumeRoleSessionCredentialsProvider}\n+import com.amazonaws.internal.StaticCredentialsProvider\n+import com.amazonaws.services.s3.AmazonS3Client\n+import com.amazonaws.services.s3.model.{ListObjectsRequest, ObjectListing, S3ObjectSummary}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{BlockLocation, FileStatus, LocatedFileStatus, Path, PathFilter, RemoteIterator}"
  }, {
    "author": {
      "login": "davies"
    },
    "body": "What's the workaround?\n",
    "commit": "5bdd92443c33fc2ec72fabb141c878cf3488d1f4",
    "createdAt": "2015-09-11T15:51:09Z",
    "diffHunk": "@@ -0,0 +1,299 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.amazonaws.{AmazonClientException, ClientConfiguration, Protocol}\n+import com.amazonaws.auth.{AWSCredentialsProvider, BasicAWSCredentials, InstanceProfileCredentialsProvider, STSAssumeRoleSessionCredentialsProvider}\n+import com.amazonaws.internal.StaticCredentialsProvider\n+import com.amazonaws.services.s3.AmazonS3Client\n+import com.amazonaws.services.s3.model.{ListObjectsRequest, ObjectListing, S3ObjectSummary}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{BlockLocation, FileStatus, LocatedFileStatus, Path, PathFilter, RemoteIterator}"
  }, {
    "author": {
      "login": "liancheng"
    },
    "body": "@piaozhexiu I guess you were referring to `FileInputFormat` in Hadoop 2 when implementing `SparkS3Util`? I think neither `LocatedFileStatus` nor `RemoteIterator` is required here, since S3 don't have locality issue. We can fix this issue by following [Hadoop 1 `FileInputFormat`](https://github.com/apache/hadoop-common/blob/branch-1.2/src/mapred/org/apache/hadoop/mapred/FileInputFormat.java).\n",
    "commit": "5bdd92443c33fc2ec72fabb141c878cf3488d1f4",
    "createdAt": "2015-09-11T16:55:37Z",
    "diffHunk": "@@ -0,0 +1,299 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.amazonaws.{AmazonClientException, ClientConfiguration, Protocol}\n+import com.amazonaws.auth.{AWSCredentialsProvider, BasicAWSCredentials, InstanceProfileCredentialsProvider, STSAssumeRoleSessionCredentialsProvider}\n+import com.amazonaws.internal.StaticCredentialsProvider\n+import com.amazonaws.services.s3.AmazonS3Client\n+import com.amazonaws.services.s3.model.{ListObjectsRequest, ObjectListing, S3ObjectSummary}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{BlockLocation, FileStatus, LocatedFileStatus, Path, PathFilter, RemoteIterator}"
  }, {
    "author": {
      "login": "piaozhexiu"
    },
    "body": "Fixed.\n",
    "commit": "5bdd92443c33fc2ec72fabb141c878cf3488d1f4",
    "createdAt": "2015-10-09T18:14:17Z",
    "diffHunk": "@@ -0,0 +1,299 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.amazonaws.{AmazonClientException, ClientConfiguration, Protocol}\n+import com.amazonaws.auth.{AWSCredentialsProvider, BasicAWSCredentials, InstanceProfileCredentialsProvider, STSAssumeRoleSessionCredentialsProvider}\n+import com.amazonaws.internal.StaticCredentialsProvider\n+import com.amazonaws.services.s3.AmazonS3Client\n+import com.amazonaws.services.s3.model.{ListObjectsRequest, ObjectListing, S3ObjectSummary}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{BlockLocation, FileStatus, LocatedFileStatus, Path, PathFilter, RemoteIterator}"
  }],
  "prId": 8512
}, {
  "comments": [{
    "author": {
      "login": "liancheng"
    },
    "body": "Nit: This can be simplified to:\n\n``` scala\nOption(path.toUri.getScheme).exists(_.toUpperCase.startsWith(\"S3\"))\n```\n",
    "commit": "5bdd92443c33fc2ec72fabb141c878cf3488d1f4",
    "createdAt": "2015-09-11T17:10:51Z",
    "diffHunk": "@@ -0,0 +1,299 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.amazonaws.{AmazonClientException, ClientConfiguration, Protocol}\n+import com.amazonaws.auth.{AWSCredentialsProvider, BasicAWSCredentials, InstanceProfileCredentialsProvider, STSAssumeRoleSessionCredentialsProvider}\n+import com.amazonaws.internal.StaticCredentialsProvider\n+import com.amazonaws.services.s3.AmazonS3Client\n+import com.amazonaws.services.s3.model.{ListObjectsRequest, ObjectListing, S3ObjectSummary}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{BlockLocation, FileStatus, LocatedFileStatus, Path, PathFilter, RemoteIterator}\n+import org.apache.hadoop.fs.s3.S3Credentials\n+import org.apache.hadoop.io.compress.{CompressionCodecFactory, SplittableCompressionCodec}\n+import org.apache.hadoop.mapred.{FileSplit, InputSplit, JobConf}\n+\n+import org.apache.spark.{SparkConf, Logging}\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.util.Utils\n+\n+import com.google.common.base.{Preconditions, Strings}\n+import com.google.common.cache.{Cache, CacheBuilder}\n+import com.google.common.collect.AbstractSequentialIterator\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Contains util methods to interact with S3 from Spark.\n+ */\n+@DeveloperApi\n+object SparkS3Util extends Logging {\n+  private val sparkConf = new SparkConf()\n+  val conf: Configuration = SparkHadoopUtil.get.newConfiguration(sparkConf)\n+\n+  private val s3ClientCache: Cache[String, AmazonS3Client] = CacheBuilder\n+    .newBuilder\n+    .concurrencyLevel(Runtime.getRuntime.availableProcessors)\n+    .build[String, AmazonS3Client]\n+\n+  private val S3_CONNECT_TIMEOUT: String = \"spark.s3.connect.timeout\"\n+  private val S3_MAX_CONNECTIONS: String = \"spark.s3.max.connections\"\n+  private val S3_MAX_ERROR_RETRIES: String = \"spark.s3.max.error.retries\"\n+  private val S3_SOCKET_TIMEOUT: String = \"spark.s3.socket.timeout\"\n+  private val S3_SSL_ENABLED: String = \"spark.s3.ssl.enabled\"\n+  private val S3_USE_INSTANCE_CREDENTIALS: String = \"spark.s3.use.instance.credentials\"\n+\n+  private val FILEINPUTFORMAT_INPUTDIR: String =\n+    \"mapreduce.input.fileinputformat.inputdir\"\n+  private val FILEINPUTFORMAT_NUMINPUTFILES: String =\n+    \"mapreduce.input.fileinputformat.numinputfiles\"\n+  private val FILEINPUTFORMAT_SPLIT_MINSIZE: String =\n+    \"mapreduce.input.fileinputformat.split.minsize\"\n+\n+  private val hiddenFileFilter = new PathFilter() {\n+    override def accept(p: Path): Boolean = {\n+      val name: String = p.getName()\n+      !name.startsWith(\"_\") && !name.startsWith(\".\")\n+    }\n+  }\n+\n+  private def getS3Client(bucket: String): AmazonS3Client = {\n+    val sslEnabled: Boolean = sparkConf.getBoolean(S3_SSL_ENABLED, true)\n+    val maxErrorRetries: Int = sparkConf.getInt(S3_MAX_ERROR_RETRIES, 10)\n+    val connectTimeout: Int = sparkConf.getInt(S3_CONNECT_TIMEOUT, 5000)\n+    val socketTimeout: Int = sparkConf.getInt(S3_SOCKET_TIMEOUT, 5000)\n+    val maxConnections: Int = sparkConf.getInt(S3_MAX_CONNECTIONS, 500)\n+    val useInstanceCredentials: Boolean = sparkConf.getBoolean(S3_USE_INSTANCE_CREDENTIALS, false)\n+\n+    val clientConf: ClientConfiguration = new ClientConfiguration\n+    clientConf.setMaxErrorRetry(maxErrorRetries)\n+    clientConf.setProtocol(if (sslEnabled) Protocol.HTTPS else Protocol.HTTP)\n+    clientConf.setConnectionTimeout(connectTimeout)\n+    clientConf.setSocketTimeout(socketTimeout)\n+    clientConf.setMaxConnections(maxConnections)\n+\n+    val s3RoleArn = Option(conf.get(\"aws.iam.role.arn\"))\n+    val s3RoleArnDefault = Option(conf.get(\"aws.iam.role.arn.default\"))\n+    val credentialsProvider: AWSCredentialsProvider =\n+      s3RoleArn match {\n+        case Some(role) =>\n+          logDebug(\"Use user-specified IAM role: \" + role)\n+          new STSAssumeRoleSessionCredentialsProvider(\n+            role, \"RoleSessionName-\" + Utils.random.nextInt)\n+        case None if useInstanceCredentials =>\n+          logDebug(\"Use IAM role associated with the instance\")\n+          new InstanceProfileCredentialsProvider\n+        case _ =>\n+          s3RoleArnDefault match {\n+            case Some(role) =>\n+              logDebug(\"Use default IAM role configured in Hadoop config: \" + role)\n+              new STSAssumeRoleSessionCredentialsProvider(\n+                role, \"RoleSessionName-\" + Utils.random.nextInt)\n+            case _ =>\n+              try {\n+                logDebug(\"Use AWS key pairs\")\n+                val credentials: S3Credentials = new S3Credentials\n+                credentials.initialize(URI.create(bucket), conf)\n+                new StaticCredentialsProvider(\n+                  new BasicAWSCredentials(credentials.getAccessKey, credentials.getSecretAccessKey))\n+              } catch {\n+                case e: Exception => throw new RuntimeException(\"S3 credentials not configured\", e)\n+              }\n+        }\n+      }\n+\n+    val providerName: String = credentialsProvider.getClass.getSimpleName\n+    val s3ClientKey: String =\n+      if (s3RoleArn == null) providerName\n+      else providerName + \"_\" + s3RoleArn\n+\n+    Option(s3ClientCache.getIfPresent(s3ClientKey)).getOrElse {\n+      val newClient = new AmazonS3Client(credentialsProvider, clientConf)\n+      s3ClientCache.put(s3ClientKey, newClient)\n+      newClient\n+    }\n+  }\n+\n+  private def keyFromPath(path: Path): String = {\n+    Preconditions.checkArgument(path.isAbsolute, \"Path is not absolute: %s\", path)\n+    var key: String = Strings.nullToEmpty(path.toUri.getPath)\n+    if (key.startsWith(\"/\")) {\n+      key = key.substring(1)\n+    }\n+    if (key.endsWith(\"/\")) {\n+      key = key.substring(0, key.length - 1)\n+    }\n+    key\n+  }\n+\n+  private def listPrefix(s3: AmazonS3Client, path: Path): Iterator[LocatedFileStatus] = {\n+    val uri: URI = path.toUri\n+    val key: String = keyFromPath(path)\n+    val request: ListObjectsRequest = new ListObjectsRequest()\n+      .withBucketName(uri.getAuthority)\n+      .withPrefix(key)\n+\n+    val listings = new AbstractSequentialIterator[ObjectListing](s3.listObjects(request)) {\n+      protected def computeNext(previous: ObjectListing): ObjectListing = {\n+        if (!previous.isTruncated) {\n+          return null\n+        }\n+        s3.listNextBatchOfObjects(previous)\n+      }\n+    }.asScala\n+\n+    val bucket: String = uri.getScheme + \"://\" + uri.getAuthority\n+    listings\n+      .map(listing => statusFromObjects(bucket, listing.getObjectSummaries))\n+      .reduceLeft(_ ++ _)\n+  }\n+\n+  private def statusFromObjects(\n+      bucket: String,\n+      objects: util.List[S3ObjectSummary]): Iterator[LocatedFileStatus] = {\n+    val blockSize: Int = conf.getInt(\"fs.s3n.block.size\", 67108864)\n+    val list: ArrayBuffer[LocatedFileStatus] = ArrayBuffer()\n+    for (obj: S3ObjectSummary <- objects.asScala) {\n+      if (!obj.getKey.endsWith(\"/\")) {\n+        val path = new Path(bucket + \"/\" + obj.getKey)\n+        if (hiddenFileFilter.accept(path)) {\n+          val status: FileStatus = new FileStatus(\n+            obj.getSize,\n+            false,\n+            1,\n+            blockSize,\n+            obj.getLastModified.getTime,\n+            path)\n+          list += createLocatedFileStatus(status)\n+        }\n+      }\n+    }\n+    list.iterator\n+  }\n+\n+  private def createLocatedFileStatus(status: FileStatus): LocatedFileStatus = {\n+    val fakeLocation: Array[BlockLocation] = Array[BlockLocation]()\n+    new LocatedFileStatus(status, fakeLocation)\n+  }\n+\n+  private def listLocatedStatus(\n+      s3: AmazonS3Client,\n+      path: Path): RemoteIterator[LocatedFileStatus] = {\n+    new RemoteIterator[LocatedFileStatus]() {\n+      private final val iterator: Iterator[LocatedFileStatus] = listPrefix(s3, path)\n+\n+      def hasNext: Boolean = {\n+        try {\n+          iterator.hasNext\n+        } catch {\n+          case e: AmazonClientException => throw new IOException(e)\n+        }\n+      }\n+\n+      def next(): LocatedFileStatus = {\n+        try {\n+          iterator.next()\n+        } catch {\n+          case e: AmazonClientException => throw new IOException(e)\n+        }\n+      }\n+    }\n+  }\n+\n+  private def listStatus(s3: AmazonS3Client, paths: List[String]): Array[FileStatus] = {\n+    val list: ArrayBuffer[FileStatus] = ArrayBuffer()\n+    paths.foreach { path =>\n+      val iterator = listLocatedStatus(s3, new Path(path))\n+      while (iterator.hasNext) {\n+        list += iterator.next\n+      }\n+    }\n+    list.toArray\n+  }\n+\n+  private def isSplitable(conf: JobConf, file: Path): Boolean = {\n+    val compressionCodecs = new CompressionCodecFactory(conf)\n+    val codec = compressionCodecs.getCodec(file)\n+    if (codec == null) {\n+      true\n+    } else {\n+      codec.isInstanceOf[SplittableCompressionCodec]\n+    }\n+  }\n+\n+  /**\n+   * Return whether the given path is an S3 path or not.\n+   */\n+  def isS3Path(path: Path): Boolean = {\n+    val scheme = Option(path.toUri.getScheme)\n+    scheme.map(_.toUpperCase.startsWith(\"S3\")).getOrElse(false)"
  }, {
    "author": {
      "login": "piaozhexiu"
    },
    "body": "Fixed.\n",
    "commit": "5bdd92443c33fc2ec72fabb141c878cf3488d1f4",
    "createdAt": "2015-10-09T18:14:22Z",
    "diffHunk": "@@ -0,0 +1,299 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.amazonaws.{AmazonClientException, ClientConfiguration, Protocol}\n+import com.amazonaws.auth.{AWSCredentialsProvider, BasicAWSCredentials, InstanceProfileCredentialsProvider, STSAssumeRoleSessionCredentialsProvider}\n+import com.amazonaws.internal.StaticCredentialsProvider\n+import com.amazonaws.services.s3.AmazonS3Client\n+import com.amazonaws.services.s3.model.{ListObjectsRequest, ObjectListing, S3ObjectSummary}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{BlockLocation, FileStatus, LocatedFileStatus, Path, PathFilter, RemoteIterator}\n+import org.apache.hadoop.fs.s3.S3Credentials\n+import org.apache.hadoop.io.compress.{CompressionCodecFactory, SplittableCompressionCodec}\n+import org.apache.hadoop.mapred.{FileSplit, InputSplit, JobConf}\n+\n+import org.apache.spark.{SparkConf, Logging}\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.util.Utils\n+\n+import com.google.common.base.{Preconditions, Strings}\n+import com.google.common.cache.{Cache, CacheBuilder}\n+import com.google.common.collect.AbstractSequentialIterator\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Contains util methods to interact with S3 from Spark.\n+ */\n+@DeveloperApi\n+object SparkS3Util extends Logging {\n+  private val sparkConf = new SparkConf()\n+  val conf: Configuration = SparkHadoopUtil.get.newConfiguration(sparkConf)\n+\n+  private val s3ClientCache: Cache[String, AmazonS3Client] = CacheBuilder\n+    .newBuilder\n+    .concurrencyLevel(Runtime.getRuntime.availableProcessors)\n+    .build[String, AmazonS3Client]\n+\n+  private val S3_CONNECT_TIMEOUT: String = \"spark.s3.connect.timeout\"\n+  private val S3_MAX_CONNECTIONS: String = \"spark.s3.max.connections\"\n+  private val S3_MAX_ERROR_RETRIES: String = \"spark.s3.max.error.retries\"\n+  private val S3_SOCKET_TIMEOUT: String = \"spark.s3.socket.timeout\"\n+  private val S3_SSL_ENABLED: String = \"spark.s3.ssl.enabled\"\n+  private val S3_USE_INSTANCE_CREDENTIALS: String = \"spark.s3.use.instance.credentials\"\n+\n+  private val FILEINPUTFORMAT_INPUTDIR: String =\n+    \"mapreduce.input.fileinputformat.inputdir\"\n+  private val FILEINPUTFORMAT_NUMINPUTFILES: String =\n+    \"mapreduce.input.fileinputformat.numinputfiles\"\n+  private val FILEINPUTFORMAT_SPLIT_MINSIZE: String =\n+    \"mapreduce.input.fileinputformat.split.minsize\"\n+\n+  private val hiddenFileFilter = new PathFilter() {\n+    override def accept(p: Path): Boolean = {\n+      val name: String = p.getName()\n+      !name.startsWith(\"_\") && !name.startsWith(\".\")\n+    }\n+  }\n+\n+  private def getS3Client(bucket: String): AmazonS3Client = {\n+    val sslEnabled: Boolean = sparkConf.getBoolean(S3_SSL_ENABLED, true)\n+    val maxErrorRetries: Int = sparkConf.getInt(S3_MAX_ERROR_RETRIES, 10)\n+    val connectTimeout: Int = sparkConf.getInt(S3_CONNECT_TIMEOUT, 5000)\n+    val socketTimeout: Int = sparkConf.getInt(S3_SOCKET_TIMEOUT, 5000)\n+    val maxConnections: Int = sparkConf.getInt(S3_MAX_CONNECTIONS, 500)\n+    val useInstanceCredentials: Boolean = sparkConf.getBoolean(S3_USE_INSTANCE_CREDENTIALS, false)\n+\n+    val clientConf: ClientConfiguration = new ClientConfiguration\n+    clientConf.setMaxErrorRetry(maxErrorRetries)\n+    clientConf.setProtocol(if (sslEnabled) Protocol.HTTPS else Protocol.HTTP)\n+    clientConf.setConnectionTimeout(connectTimeout)\n+    clientConf.setSocketTimeout(socketTimeout)\n+    clientConf.setMaxConnections(maxConnections)\n+\n+    val s3RoleArn = Option(conf.get(\"aws.iam.role.arn\"))\n+    val s3RoleArnDefault = Option(conf.get(\"aws.iam.role.arn.default\"))\n+    val credentialsProvider: AWSCredentialsProvider =\n+      s3RoleArn match {\n+        case Some(role) =>\n+          logDebug(\"Use user-specified IAM role: \" + role)\n+          new STSAssumeRoleSessionCredentialsProvider(\n+            role, \"RoleSessionName-\" + Utils.random.nextInt)\n+        case None if useInstanceCredentials =>\n+          logDebug(\"Use IAM role associated with the instance\")\n+          new InstanceProfileCredentialsProvider\n+        case _ =>\n+          s3RoleArnDefault match {\n+            case Some(role) =>\n+              logDebug(\"Use default IAM role configured in Hadoop config: \" + role)\n+              new STSAssumeRoleSessionCredentialsProvider(\n+                role, \"RoleSessionName-\" + Utils.random.nextInt)\n+            case _ =>\n+              try {\n+                logDebug(\"Use AWS key pairs\")\n+                val credentials: S3Credentials = new S3Credentials\n+                credentials.initialize(URI.create(bucket), conf)\n+                new StaticCredentialsProvider(\n+                  new BasicAWSCredentials(credentials.getAccessKey, credentials.getSecretAccessKey))\n+              } catch {\n+                case e: Exception => throw new RuntimeException(\"S3 credentials not configured\", e)\n+              }\n+        }\n+      }\n+\n+    val providerName: String = credentialsProvider.getClass.getSimpleName\n+    val s3ClientKey: String =\n+      if (s3RoleArn == null) providerName\n+      else providerName + \"_\" + s3RoleArn\n+\n+    Option(s3ClientCache.getIfPresent(s3ClientKey)).getOrElse {\n+      val newClient = new AmazonS3Client(credentialsProvider, clientConf)\n+      s3ClientCache.put(s3ClientKey, newClient)\n+      newClient\n+    }\n+  }\n+\n+  private def keyFromPath(path: Path): String = {\n+    Preconditions.checkArgument(path.isAbsolute, \"Path is not absolute: %s\", path)\n+    var key: String = Strings.nullToEmpty(path.toUri.getPath)\n+    if (key.startsWith(\"/\")) {\n+      key = key.substring(1)\n+    }\n+    if (key.endsWith(\"/\")) {\n+      key = key.substring(0, key.length - 1)\n+    }\n+    key\n+  }\n+\n+  private def listPrefix(s3: AmazonS3Client, path: Path): Iterator[LocatedFileStatus] = {\n+    val uri: URI = path.toUri\n+    val key: String = keyFromPath(path)\n+    val request: ListObjectsRequest = new ListObjectsRequest()\n+      .withBucketName(uri.getAuthority)\n+      .withPrefix(key)\n+\n+    val listings = new AbstractSequentialIterator[ObjectListing](s3.listObjects(request)) {\n+      protected def computeNext(previous: ObjectListing): ObjectListing = {\n+        if (!previous.isTruncated) {\n+          return null\n+        }\n+        s3.listNextBatchOfObjects(previous)\n+      }\n+    }.asScala\n+\n+    val bucket: String = uri.getScheme + \"://\" + uri.getAuthority\n+    listings\n+      .map(listing => statusFromObjects(bucket, listing.getObjectSummaries))\n+      .reduceLeft(_ ++ _)\n+  }\n+\n+  private def statusFromObjects(\n+      bucket: String,\n+      objects: util.List[S3ObjectSummary]): Iterator[LocatedFileStatus] = {\n+    val blockSize: Int = conf.getInt(\"fs.s3n.block.size\", 67108864)\n+    val list: ArrayBuffer[LocatedFileStatus] = ArrayBuffer()\n+    for (obj: S3ObjectSummary <- objects.asScala) {\n+      if (!obj.getKey.endsWith(\"/\")) {\n+        val path = new Path(bucket + \"/\" + obj.getKey)\n+        if (hiddenFileFilter.accept(path)) {\n+          val status: FileStatus = new FileStatus(\n+            obj.getSize,\n+            false,\n+            1,\n+            blockSize,\n+            obj.getLastModified.getTime,\n+            path)\n+          list += createLocatedFileStatus(status)\n+        }\n+      }\n+    }\n+    list.iterator\n+  }\n+\n+  private def createLocatedFileStatus(status: FileStatus): LocatedFileStatus = {\n+    val fakeLocation: Array[BlockLocation] = Array[BlockLocation]()\n+    new LocatedFileStatus(status, fakeLocation)\n+  }\n+\n+  private def listLocatedStatus(\n+      s3: AmazonS3Client,\n+      path: Path): RemoteIterator[LocatedFileStatus] = {\n+    new RemoteIterator[LocatedFileStatus]() {\n+      private final val iterator: Iterator[LocatedFileStatus] = listPrefix(s3, path)\n+\n+      def hasNext: Boolean = {\n+        try {\n+          iterator.hasNext\n+        } catch {\n+          case e: AmazonClientException => throw new IOException(e)\n+        }\n+      }\n+\n+      def next(): LocatedFileStatus = {\n+        try {\n+          iterator.next()\n+        } catch {\n+          case e: AmazonClientException => throw new IOException(e)\n+        }\n+      }\n+    }\n+  }\n+\n+  private def listStatus(s3: AmazonS3Client, paths: List[String]): Array[FileStatus] = {\n+    val list: ArrayBuffer[FileStatus] = ArrayBuffer()\n+    paths.foreach { path =>\n+      val iterator = listLocatedStatus(s3, new Path(path))\n+      while (iterator.hasNext) {\n+        list += iterator.next\n+      }\n+    }\n+    list.toArray\n+  }\n+\n+  private def isSplitable(conf: JobConf, file: Path): Boolean = {\n+    val compressionCodecs = new CompressionCodecFactory(conf)\n+    val codec = compressionCodecs.getCodec(file)\n+    if (codec == null) {\n+      true\n+    } else {\n+      codec.isInstanceOf[SplittableCompressionCodec]\n+    }\n+  }\n+\n+  /**\n+   * Return whether the given path is an S3 path or not.\n+   */\n+  def isS3Path(path: Path): Boolean = {\n+    val scheme = Option(path.toUri.getScheme)\n+    scheme.map(_.toUpperCase.startsWith(\"S3\")).getOrElse(false)"
  }],
  "prId": 8512
}, {
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "Instead of `split` at here, can we use `getInputPaths` at here? It will properly unescape paths encoded in the string.\n",
    "commit": "5bdd92443c33fc2ec72fabb141c878cf3488d1f4",
    "createdAt": "2015-09-15T21:40:04Z",
    "diffHunk": "@@ -0,0 +1,299 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.amazonaws.{AmazonClientException, ClientConfiguration, Protocol}\n+import com.amazonaws.auth.{AWSCredentialsProvider, BasicAWSCredentials, InstanceProfileCredentialsProvider, STSAssumeRoleSessionCredentialsProvider}\n+import com.amazonaws.internal.StaticCredentialsProvider\n+import com.amazonaws.services.s3.AmazonS3Client\n+import com.amazonaws.services.s3.model.{ListObjectsRequest, ObjectListing, S3ObjectSummary}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{BlockLocation, FileStatus, LocatedFileStatus, Path, PathFilter, RemoteIterator}\n+import org.apache.hadoop.fs.s3.S3Credentials\n+import org.apache.hadoop.io.compress.{CompressionCodecFactory, SplittableCompressionCodec}\n+import org.apache.hadoop.mapred.{FileSplit, InputSplit, JobConf}\n+\n+import org.apache.spark.{SparkConf, Logging}\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.util.Utils\n+\n+import com.google.common.base.{Preconditions, Strings}\n+import com.google.common.cache.{Cache, CacheBuilder}\n+import com.google.common.collect.AbstractSequentialIterator\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Contains util methods to interact with S3 from Spark.\n+ */\n+@DeveloperApi\n+object SparkS3Util extends Logging {\n+  private val sparkConf = new SparkConf()\n+  val conf: Configuration = SparkHadoopUtil.get.newConfiguration(sparkConf)\n+\n+  private val s3ClientCache: Cache[String, AmazonS3Client] = CacheBuilder\n+    .newBuilder\n+    .concurrencyLevel(Runtime.getRuntime.availableProcessors)\n+    .build[String, AmazonS3Client]\n+\n+  private val S3_CONNECT_TIMEOUT: String = \"spark.s3.connect.timeout\"\n+  private val S3_MAX_CONNECTIONS: String = \"spark.s3.max.connections\"\n+  private val S3_MAX_ERROR_RETRIES: String = \"spark.s3.max.error.retries\"\n+  private val S3_SOCKET_TIMEOUT: String = \"spark.s3.socket.timeout\"\n+  private val S3_SSL_ENABLED: String = \"spark.s3.ssl.enabled\"\n+  private val S3_USE_INSTANCE_CREDENTIALS: String = \"spark.s3.use.instance.credentials\"\n+\n+  private val FILEINPUTFORMAT_INPUTDIR: String =\n+    \"mapreduce.input.fileinputformat.inputdir\"\n+  private val FILEINPUTFORMAT_NUMINPUTFILES: String =\n+    \"mapreduce.input.fileinputformat.numinputfiles\"\n+  private val FILEINPUTFORMAT_SPLIT_MINSIZE: String =\n+    \"mapreduce.input.fileinputformat.split.minsize\"\n+\n+  private val hiddenFileFilter = new PathFilter() {\n+    override def accept(p: Path): Boolean = {\n+      val name: String = p.getName()\n+      !name.startsWith(\"_\") && !name.startsWith(\".\")\n+    }\n+  }\n+\n+  private def getS3Client(bucket: String): AmazonS3Client = {\n+    val sslEnabled: Boolean = sparkConf.getBoolean(S3_SSL_ENABLED, true)\n+    val maxErrorRetries: Int = sparkConf.getInt(S3_MAX_ERROR_RETRIES, 10)\n+    val connectTimeout: Int = sparkConf.getInt(S3_CONNECT_TIMEOUT, 5000)\n+    val socketTimeout: Int = sparkConf.getInt(S3_SOCKET_TIMEOUT, 5000)\n+    val maxConnections: Int = sparkConf.getInt(S3_MAX_CONNECTIONS, 500)\n+    val useInstanceCredentials: Boolean = sparkConf.getBoolean(S3_USE_INSTANCE_CREDENTIALS, false)\n+\n+    val clientConf: ClientConfiguration = new ClientConfiguration\n+    clientConf.setMaxErrorRetry(maxErrorRetries)\n+    clientConf.setProtocol(if (sslEnabled) Protocol.HTTPS else Protocol.HTTP)\n+    clientConf.setConnectionTimeout(connectTimeout)\n+    clientConf.setSocketTimeout(socketTimeout)\n+    clientConf.setMaxConnections(maxConnections)\n+\n+    val s3RoleArn = Option(conf.get(\"aws.iam.role.arn\"))\n+    val s3RoleArnDefault = Option(conf.get(\"aws.iam.role.arn.default\"))\n+    val credentialsProvider: AWSCredentialsProvider =\n+      s3RoleArn match {\n+        case Some(role) =>\n+          logDebug(\"Use user-specified IAM role: \" + role)\n+          new STSAssumeRoleSessionCredentialsProvider(\n+            role, \"RoleSessionName-\" + Utils.random.nextInt)\n+        case None if useInstanceCredentials =>\n+          logDebug(\"Use IAM role associated with the instance\")\n+          new InstanceProfileCredentialsProvider\n+        case _ =>\n+          s3RoleArnDefault match {\n+            case Some(role) =>\n+              logDebug(\"Use default IAM role configured in Hadoop config: \" + role)\n+              new STSAssumeRoleSessionCredentialsProvider(\n+                role, \"RoleSessionName-\" + Utils.random.nextInt)\n+            case _ =>\n+              try {\n+                logDebug(\"Use AWS key pairs\")\n+                val credentials: S3Credentials = new S3Credentials\n+                credentials.initialize(URI.create(bucket), conf)\n+                new StaticCredentialsProvider(\n+                  new BasicAWSCredentials(credentials.getAccessKey, credentials.getSecretAccessKey))\n+              } catch {\n+                case e: Exception => throw new RuntimeException(\"S3 credentials not configured\", e)\n+              }\n+        }\n+      }\n+\n+    val providerName: String = credentialsProvider.getClass.getSimpleName\n+    val s3ClientKey: String =\n+      if (s3RoleArn == null) providerName\n+      else providerName + \"_\" + s3RoleArn\n+\n+    Option(s3ClientCache.getIfPresent(s3ClientKey)).getOrElse {\n+      val newClient = new AmazonS3Client(credentialsProvider, clientConf)\n+      s3ClientCache.put(s3ClientKey, newClient)\n+      newClient\n+    }\n+  }\n+\n+  private def keyFromPath(path: Path): String = {\n+    Preconditions.checkArgument(path.isAbsolute, \"Path is not absolute: %s\", path)\n+    var key: String = Strings.nullToEmpty(path.toUri.getPath)\n+    if (key.startsWith(\"/\")) {\n+      key = key.substring(1)\n+    }\n+    if (key.endsWith(\"/\")) {\n+      key = key.substring(0, key.length - 1)\n+    }\n+    key\n+  }\n+\n+  private def listPrefix(s3: AmazonS3Client, path: Path): Iterator[LocatedFileStatus] = {\n+    val uri: URI = path.toUri\n+    val key: String = keyFromPath(path)\n+    val request: ListObjectsRequest = new ListObjectsRequest()\n+      .withBucketName(uri.getAuthority)\n+      .withPrefix(key)\n+\n+    val listings = new AbstractSequentialIterator[ObjectListing](s3.listObjects(request)) {\n+      protected def computeNext(previous: ObjectListing): ObjectListing = {\n+        if (!previous.isTruncated) {\n+          return null\n+        }\n+        s3.listNextBatchOfObjects(previous)\n+      }\n+    }.asScala\n+\n+    val bucket: String = uri.getScheme + \"://\" + uri.getAuthority\n+    listings\n+      .map(listing => statusFromObjects(bucket, listing.getObjectSummaries))\n+      .reduceLeft(_ ++ _)\n+  }\n+\n+  private def statusFromObjects(\n+      bucket: String,\n+      objects: util.List[S3ObjectSummary]): Iterator[LocatedFileStatus] = {\n+    val blockSize: Int = conf.getInt(\"fs.s3n.block.size\", 67108864)\n+    val list: ArrayBuffer[LocatedFileStatus] = ArrayBuffer()\n+    for (obj: S3ObjectSummary <- objects.asScala) {\n+      if (!obj.getKey.endsWith(\"/\")) {\n+        val path = new Path(bucket + \"/\" + obj.getKey)\n+        if (hiddenFileFilter.accept(path)) {\n+          val status: FileStatus = new FileStatus(\n+            obj.getSize,\n+            false,\n+            1,\n+            blockSize,\n+            obj.getLastModified.getTime,\n+            path)\n+          list += createLocatedFileStatus(status)\n+        }\n+      }\n+    }\n+    list.iterator\n+  }\n+\n+  private def createLocatedFileStatus(status: FileStatus): LocatedFileStatus = {\n+    val fakeLocation: Array[BlockLocation] = Array[BlockLocation]()\n+    new LocatedFileStatus(status, fakeLocation)\n+  }\n+\n+  private def listLocatedStatus(\n+      s3: AmazonS3Client,\n+      path: Path): RemoteIterator[LocatedFileStatus] = {\n+    new RemoteIterator[LocatedFileStatus]() {\n+      private final val iterator: Iterator[LocatedFileStatus] = listPrefix(s3, path)\n+\n+      def hasNext: Boolean = {\n+        try {\n+          iterator.hasNext\n+        } catch {\n+          case e: AmazonClientException => throw new IOException(e)\n+        }\n+      }\n+\n+      def next(): LocatedFileStatus = {\n+        try {\n+          iterator.next()\n+        } catch {\n+          case e: AmazonClientException => throw new IOException(e)\n+        }\n+      }\n+    }\n+  }\n+\n+  private def listStatus(s3: AmazonS3Client, paths: List[String]): Array[FileStatus] = {\n+    val list: ArrayBuffer[FileStatus] = ArrayBuffer()\n+    paths.foreach { path =>\n+      val iterator = listLocatedStatus(s3, new Path(path))\n+      while (iterator.hasNext) {\n+        list += iterator.next\n+      }\n+    }\n+    list.toArray\n+  }\n+\n+  private def isSplitable(conf: JobConf, file: Path): Boolean = {\n+    val compressionCodecs = new CompressionCodecFactory(conf)\n+    val codec = compressionCodecs.getCodec(file)\n+    if (codec == null) {\n+      true\n+    } else {\n+      codec.isInstanceOf[SplittableCompressionCodec]\n+    }\n+  }\n+\n+  /**\n+   * Return whether the given path is an S3 path or not.\n+   */\n+  def isS3Path(path: Path): Boolean = {\n+    val scheme = Option(path.toUri.getScheme)\n+    scheme.map(_.toUpperCase.startsWith(\"S3\")).getOrElse(false)\n+  }\n+\n+  /**\n+   * This is based on `FileInputFormat.getSplits` method. Two key differences are:\n+   *   1) Use `AmazonS3Client.listObjects` instead of `FileSystem.listStatus`.\n+   *   2) Bypass data locality hints since they're irrelevant to S3 objects.\n+   */\n+  def getSplits(jobConf: JobConf, minSplits: Int): Array[InputSplit] = {\n+    val inputDirs: Array[String] = jobConf.get(FILEINPUTFORMAT_INPUTDIR).split(\",\")"
  }, {
    "author": {
      "login": "piaozhexiu"
    },
    "body": "Fixed.\n",
    "commit": "5bdd92443c33fc2ec72fabb141c878cf3488d1f4",
    "createdAt": "2015-10-09T18:14:29Z",
    "diffHunk": "@@ -0,0 +1,299 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.amazonaws.{AmazonClientException, ClientConfiguration, Protocol}\n+import com.amazonaws.auth.{AWSCredentialsProvider, BasicAWSCredentials, InstanceProfileCredentialsProvider, STSAssumeRoleSessionCredentialsProvider}\n+import com.amazonaws.internal.StaticCredentialsProvider\n+import com.amazonaws.services.s3.AmazonS3Client\n+import com.amazonaws.services.s3.model.{ListObjectsRequest, ObjectListing, S3ObjectSummary}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{BlockLocation, FileStatus, LocatedFileStatus, Path, PathFilter, RemoteIterator}\n+import org.apache.hadoop.fs.s3.S3Credentials\n+import org.apache.hadoop.io.compress.{CompressionCodecFactory, SplittableCompressionCodec}\n+import org.apache.hadoop.mapred.{FileSplit, InputSplit, JobConf}\n+\n+import org.apache.spark.{SparkConf, Logging}\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.util.Utils\n+\n+import com.google.common.base.{Preconditions, Strings}\n+import com.google.common.cache.{Cache, CacheBuilder}\n+import com.google.common.collect.AbstractSequentialIterator\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Contains util methods to interact with S3 from Spark.\n+ */\n+@DeveloperApi\n+object SparkS3Util extends Logging {\n+  private val sparkConf = new SparkConf()\n+  val conf: Configuration = SparkHadoopUtil.get.newConfiguration(sparkConf)\n+\n+  private val s3ClientCache: Cache[String, AmazonS3Client] = CacheBuilder\n+    .newBuilder\n+    .concurrencyLevel(Runtime.getRuntime.availableProcessors)\n+    .build[String, AmazonS3Client]\n+\n+  private val S3_CONNECT_TIMEOUT: String = \"spark.s3.connect.timeout\"\n+  private val S3_MAX_CONNECTIONS: String = \"spark.s3.max.connections\"\n+  private val S3_MAX_ERROR_RETRIES: String = \"spark.s3.max.error.retries\"\n+  private val S3_SOCKET_TIMEOUT: String = \"spark.s3.socket.timeout\"\n+  private val S3_SSL_ENABLED: String = \"spark.s3.ssl.enabled\"\n+  private val S3_USE_INSTANCE_CREDENTIALS: String = \"spark.s3.use.instance.credentials\"\n+\n+  private val FILEINPUTFORMAT_INPUTDIR: String =\n+    \"mapreduce.input.fileinputformat.inputdir\"\n+  private val FILEINPUTFORMAT_NUMINPUTFILES: String =\n+    \"mapreduce.input.fileinputformat.numinputfiles\"\n+  private val FILEINPUTFORMAT_SPLIT_MINSIZE: String =\n+    \"mapreduce.input.fileinputformat.split.minsize\"\n+\n+  private val hiddenFileFilter = new PathFilter() {\n+    override def accept(p: Path): Boolean = {\n+      val name: String = p.getName()\n+      !name.startsWith(\"_\") && !name.startsWith(\".\")\n+    }\n+  }\n+\n+  private def getS3Client(bucket: String): AmazonS3Client = {\n+    val sslEnabled: Boolean = sparkConf.getBoolean(S3_SSL_ENABLED, true)\n+    val maxErrorRetries: Int = sparkConf.getInt(S3_MAX_ERROR_RETRIES, 10)\n+    val connectTimeout: Int = sparkConf.getInt(S3_CONNECT_TIMEOUT, 5000)\n+    val socketTimeout: Int = sparkConf.getInt(S3_SOCKET_TIMEOUT, 5000)\n+    val maxConnections: Int = sparkConf.getInt(S3_MAX_CONNECTIONS, 500)\n+    val useInstanceCredentials: Boolean = sparkConf.getBoolean(S3_USE_INSTANCE_CREDENTIALS, false)\n+\n+    val clientConf: ClientConfiguration = new ClientConfiguration\n+    clientConf.setMaxErrorRetry(maxErrorRetries)\n+    clientConf.setProtocol(if (sslEnabled) Protocol.HTTPS else Protocol.HTTP)\n+    clientConf.setConnectionTimeout(connectTimeout)\n+    clientConf.setSocketTimeout(socketTimeout)\n+    clientConf.setMaxConnections(maxConnections)\n+\n+    val s3RoleArn = Option(conf.get(\"aws.iam.role.arn\"))\n+    val s3RoleArnDefault = Option(conf.get(\"aws.iam.role.arn.default\"))\n+    val credentialsProvider: AWSCredentialsProvider =\n+      s3RoleArn match {\n+        case Some(role) =>\n+          logDebug(\"Use user-specified IAM role: \" + role)\n+          new STSAssumeRoleSessionCredentialsProvider(\n+            role, \"RoleSessionName-\" + Utils.random.nextInt)\n+        case None if useInstanceCredentials =>\n+          logDebug(\"Use IAM role associated with the instance\")\n+          new InstanceProfileCredentialsProvider\n+        case _ =>\n+          s3RoleArnDefault match {\n+            case Some(role) =>\n+              logDebug(\"Use default IAM role configured in Hadoop config: \" + role)\n+              new STSAssumeRoleSessionCredentialsProvider(\n+                role, \"RoleSessionName-\" + Utils.random.nextInt)\n+            case _ =>\n+              try {\n+                logDebug(\"Use AWS key pairs\")\n+                val credentials: S3Credentials = new S3Credentials\n+                credentials.initialize(URI.create(bucket), conf)\n+                new StaticCredentialsProvider(\n+                  new BasicAWSCredentials(credentials.getAccessKey, credentials.getSecretAccessKey))\n+              } catch {\n+                case e: Exception => throw new RuntimeException(\"S3 credentials not configured\", e)\n+              }\n+        }\n+      }\n+\n+    val providerName: String = credentialsProvider.getClass.getSimpleName\n+    val s3ClientKey: String =\n+      if (s3RoleArn == null) providerName\n+      else providerName + \"_\" + s3RoleArn\n+\n+    Option(s3ClientCache.getIfPresent(s3ClientKey)).getOrElse {\n+      val newClient = new AmazonS3Client(credentialsProvider, clientConf)\n+      s3ClientCache.put(s3ClientKey, newClient)\n+      newClient\n+    }\n+  }\n+\n+  private def keyFromPath(path: Path): String = {\n+    Preconditions.checkArgument(path.isAbsolute, \"Path is not absolute: %s\", path)\n+    var key: String = Strings.nullToEmpty(path.toUri.getPath)\n+    if (key.startsWith(\"/\")) {\n+      key = key.substring(1)\n+    }\n+    if (key.endsWith(\"/\")) {\n+      key = key.substring(0, key.length - 1)\n+    }\n+    key\n+  }\n+\n+  private def listPrefix(s3: AmazonS3Client, path: Path): Iterator[LocatedFileStatus] = {\n+    val uri: URI = path.toUri\n+    val key: String = keyFromPath(path)\n+    val request: ListObjectsRequest = new ListObjectsRequest()\n+      .withBucketName(uri.getAuthority)\n+      .withPrefix(key)\n+\n+    val listings = new AbstractSequentialIterator[ObjectListing](s3.listObjects(request)) {\n+      protected def computeNext(previous: ObjectListing): ObjectListing = {\n+        if (!previous.isTruncated) {\n+          return null\n+        }\n+        s3.listNextBatchOfObjects(previous)\n+      }\n+    }.asScala\n+\n+    val bucket: String = uri.getScheme + \"://\" + uri.getAuthority\n+    listings\n+      .map(listing => statusFromObjects(bucket, listing.getObjectSummaries))\n+      .reduceLeft(_ ++ _)\n+  }\n+\n+  private def statusFromObjects(\n+      bucket: String,\n+      objects: util.List[S3ObjectSummary]): Iterator[LocatedFileStatus] = {\n+    val blockSize: Int = conf.getInt(\"fs.s3n.block.size\", 67108864)\n+    val list: ArrayBuffer[LocatedFileStatus] = ArrayBuffer()\n+    for (obj: S3ObjectSummary <- objects.asScala) {\n+      if (!obj.getKey.endsWith(\"/\")) {\n+        val path = new Path(bucket + \"/\" + obj.getKey)\n+        if (hiddenFileFilter.accept(path)) {\n+          val status: FileStatus = new FileStatus(\n+            obj.getSize,\n+            false,\n+            1,\n+            blockSize,\n+            obj.getLastModified.getTime,\n+            path)\n+          list += createLocatedFileStatus(status)\n+        }\n+      }\n+    }\n+    list.iterator\n+  }\n+\n+  private def createLocatedFileStatus(status: FileStatus): LocatedFileStatus = {\n+    val fakeLocation: Array[BlockLocation] = Array[BlockLocation]()\n+    new LocatedFileStatus(status, fakeLocation)\n+  }\n+\n+  private def listLocatedStatus(\n+      s3: AmazonS3Client,\n+      path: Path): RemoteIterator[LocatedFileStatus] = {\n+    new RemoteIterator[LocatedFileStatus]() {\n+      private final val iterator: Iterator[LocatedFileStatus] = listPrefix(s3, path)\n+\n+      def hasNext: Boolean = {\n+        try {\n+          iterator.hasNext\n+        } catch {\n+          case e: AmazonClientException => throw new IOException(e)\n+        }\n+      }\n+\n+      def next(): LocatedFileStatus = {\n+        try {\n+          iterator.next()\n+        } catch {\n+          case e: AmazonClientException => throw new IOException(e)\n+        }\n+      }\n+    }\n+  }\n+\n+  private def listStatus(s3: AmazonS3Client, paths: List[String]): Array[FileStatus] = {\n+    val list: ArrayBuffer[FileStatus] = ArrayBuffer()\n+    paths.foreach { path =>\n+      val iterator = listLocatedStatus(s3, new Path(path))\n+      while (iterator.hasNext) {\n+        list += iterator.next\n+      }\n+    }\n+    list.toArray\n+  }\n+\n+  private def isSplitable(conf: JobConf, file: Path): Boolean = {\n+    val compressionCodecs = new CompressionCodecFactory(conf)\n+    val codec = compressionCodecs.getCodec(file)\n+    if (codec == null) {\n+      true\n+    } else {\n+      codec.isInstanceOf[SplittableCompressionCodec]\n+    }\n+  }\n+\n+  /**\n+   * Return whether the given path is an S3 path or not.\n+   */\n+  def isS3Path(path: Path): Boolean = {\n+    val scheme = Option(path.toUri.getScheme)\n+    scheme.map(_.toUpperCase.startsWith(\"S3\")).getOrElse(false)\n+  }\n+\n+  /**\n+   * This is based on `FileInputFormat.getSplits` method. Two key differences are:\n+   *   1) Use `AmazonS3Client.listObjects` instead of `FileSystem.listStatus`.\n+   *   2) Bypass data locality hints since they're irrelevant to S3 objects.\n+   */\n+  def getSplits(jobConf: JobConf, minSplits: Int): Array[InputSplit] = {\n+    val inputDirs: Array[String] = jobConf.get(FILEINPUTFORMAT_INPUTDIR).split(\",\")"
  }],
  "prId": 8512
}, {
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "Do we need to create those `val`s at here? Are they available as public static fields in `FileInputFormat`? Also, to make it work with both hadoop 1 and hadoop 2, do we need to set both versions of those flags?\n",
    "commit": "5bdd92443c33fc2ec72fabb141c878cf3488d1f4",
    "createdAt": "2015-09-15T21:46:38Z",
    "diffHunk": "@@ -0,0 +1,299 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.amazonaws.{AmazonClientException, ClientConfiguration, Protocol}\n+import com.amazonaws.auth.{AWSCredentialsProvider, BasicAWSCredentials, InstanceProfileCredentialsProvider, STSAssumeRoleSessionCredentialsProvider}\n+import com.amazonaws.internal.StaticCredentialsProvider\n+import com.amazonaws.services.s3.AmazonS3Client\n+import com.amazonaws.services.s3.model.{ListObjectsRequest, ObjectListing, S3ObjectSummary}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{BlockLocation, FileStatus, LocatedFileStatus, Path, PathFilter, RemoteIterator}\n+import org.apache.hadoop.fs.s3.S3Credentials\n+import org.apache.hadoop.io.compress.{CompressionCodecFactory, SplittableCompressionCodec}\n+import org.apache.hadoop.mapred.{FileSplit, InputSplit, JobConf}\n+\n+import org.apache.spark.{SparkConf, Logging}\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.util.Utils\n+\n+import com.google.common.base.{Preconditions, Strings}\n+import com.google.common.cache.{Cache, CacheBuilder}\n+import com.google.common.collect.AbstractSequentialIterator\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Contains util methods to interact with S3 from Spark.\n+ */\n+@DeveloperApi\n+object SparkS3Util extends Logging {\n+  private val sparkConf = new SparkConf()\n+  val conf: Configuration = SparkHadoopUtil.get.newConfiguration(sparkConf)\n+\n+  private val s3ClientCache: Cache[String, AmazonS3Client] = CacheBuilder\n+    .newBuilder\n+    .concurrencyLevel(Runtime.getRuntime.availableProcessors)\n+    .build[String, AmazonS3Client]\n+\n+  private val S3_CONNECT_TIMEOUT: String = \"spark.s3.connect.timeout\"\n+  private val S3_MAX_CONNECTIONS: String = \"spark.s3.max.connections\"\n+  private val S3_MAX_ERROR_RETRIES: String = \"spark.s3.max.error.retries\"\n+  private val S3_SOCKET_TIMEOUT: String = \"spark.s3.socket.timeout\"\n+  private val S3_SSL_ENABLED: String = \"spark.s3.ssl.enabled\"\n+  private val S3_USE_INSTANCE_CREDENTIALS: String = \"spark.s3.use.instance.credentials\"\n+\n+  private val FILEINPUTFORMAT_INPUTDIR: String =\n+    \"mapreduce.input.fileinputformat.inputdir\"\n+  private val FILEINPUTFORMAT_NUMINPUTFILES: String =\n+    \"mapreduce.input.fileinputformat.numinputfiles\"\n+  private val FILEINPUTFORMAT_SPLIT_MINSIZE: String =\n+    \"mapreduce.input.fileinputformat.split.minsize\""
  }, {
    "author": {
      "login": "piaozhexiu"
    },
    "body": "Unfortunately, Hadoop 1 FileInputFormat doesn't have public static fields for those strings, so I need them.\n\nRegarding compatibility, I used old property names that are compatible with both Hadoop 1 and 2.\n",
    "commit": "5bdd92443c33fc2ec72fabb141c878cf3488d1f4",
    "createdAt": "2015-10-09T18:16:54Z",
    "diffHunk": "@@ -0,0 +1,299 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.amazonaws.{AmazonClientException, ClientConfiguration, Protocol}\n+import com.amazonaws.auth.{AWSCredentialsProvider, BasicAWSCredentials, InstanceProfileCredentialsProvider, STSAssumeRoleSessionCredentialsProvider}\n+import com.amazonaws.internal.StaticCredentialsProvider\n+import com.amazonaws.services.s3.AmazonS3Client\n+import com.amazonaws.services.s3.model.{ListObjectsRequest, ObjectListing, S3ObjectSummary}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{BlockLocation, FileStatus, LocatedFileStatus, Path, PathFilter, RemoteIterator}\n+import org.apache.hadoop.fs.s3.S3Credentials\n+import org.apache.hadoop.io.compress.{CompressionCodecFactory, SplittableCompressionCodec}\n+import org.apache.hadoop.mapred.{FileSplit, InputSplit, JobConf}\n+\n+import org.apache.spark.{SparkConf, Logging}\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.util.Utils\n+\n+import com.google.common.base.{Preconditions, Strings}\n+import com.google.common.cache.{Cache, CacheBuilder}\n+import com.google.common.collect.AbstractSequentialIterator\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Contains util methods to interact with S3 from Spark.\n+ */\n+@DeveloperApi\n+object SparkS3Util extends Logging {\n+  private val sparkConf = new SparkConf()\n+  val conf: Configuration = SparkHadoopUtil.get.newConfiguration(sparkConf)\n+\n+  private val s3ClientCache: Cache[String, AmazonS3Client] = CacheBuilder\n+    .newBuilder\n+    .concurrencyLevel(Runtime.getRuntime.availableProcessors)\n+    .build[String, AmazonS3Client]\n+\n+  private val S3_CONNECT_TIMEOUT: String = \"spark.s3.connect.timeout\"\n+  private val S3_MAX_CONNECTIONS: String = \"spark.s3.max.connections\"\n+  private val S3_MAX_ERROR_RETRIES: String = \"spark.s3.max.error.retries\"\n+  private val S3_SOCKET_TIMEOUT: String = \"spark.s3.socket.timeout\"\n+  private val S3_SSL_ENABLED: String = \"spark.s3.ssl.enabled\"\n+  private val S3_USE_INSTANCE_CREDENTIALS: String = \"spark.s3.use.instance.credentials\"\n+\n+  private val FILEINPUTFORMAT_INPUTDIR: String =\n+    \"mapreduce.input.fileinputformat.inputdir\"\n+  private val FILEINPUTFORMAT_NUMINPUTFILES: String =\n+    \"mapreduce.input.fileinputformat.numinputfiles\"\n+  private val FILEINPUTFORMAT_SPLIT_MINSIZE: String =\n+    \"mapreduce.input.fileinputformat.split.minsize\""
  }],
  "prId": 8512
}, {
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "Add a comment to explain why we need this?\n",
    "commit": "5bdd92443c33fc2ec72fabb141c878cf3488d1f4",
    "createdAt": "2015-09-15T21:56:09Z",
    "diffHunk": "@@ -0,0 +1,299 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.amazonaws.{AmazonClientException, ClientConfiguration, Protocol}\n+import com.amazonaws.auth.{AWSCredentialsProvider, BasicAWSCredentials, InstanceProfileCredentialsProvider, STSAssumeRoleSessionCredentialsProvider}\n+import com.amazonaws.internal.StaticCredentialsProvider\n+import com.amazonaws.services.s3.AmazonS3Client\n+import com.amazonaws.services.s3.model.{ListObjectsRequest, ObjectListing, S3ObjectSummary}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{BlockLocation, FileStatus, LocatedFileStatus, Path, PathFilter, RemoteIterator}\n+import org.apache.hadoop.fs.s3.S3Credentials\n+import org.apache.hadoop.io.compress.{CompressionCodecFactory, SplittableCompressionCodec}\n+import org.apache.hadoop.mapred.{FileSplit, InputSplit, JobConf}\n+\n+import org.apache.spark.{SparkConf, Logging}\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.util.Utils\n+\n+import com.google.common.base.{Preconditions, Strings}\n+import com.google.common.cache.{Cache, CacheBuilder}\n+import com.google.common.collect.AbstractSequentialIterator\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Contains util methods to interact with S3 from Spark.\n+ */\n+@DeveloperApi\n+object SparkS3Util extends Logging {\n+  private val sparkConf = new SparkConf()\n+  val conf: Configuration = SparkHadoopUtil.get.newConfiguration(sparkConf)\n+\n+  private val s3ClientCache: Cache[String, AmazonS3Client] = CacheBuilder\n+    .newBuilder\n+    .concurrencyLevel(Runtime.getRuntime.availableProcessors)\n+    .build[String, AmazonS3Client]\n+\n+  private val S3_CONNECT_TIMEOUT: String = \"spark.s3.connect.timeout\"\n+  private val S3_MAX_CONNECTIONS: String = \"spark.s3.max.connections\"\n+  private val S3_MAX_ERROR_RETRIES: String = \"spark.s3.max.error.retries\"\n+  private val S3_SOCKET_TIMEOUT: String = \"spark.s3.socket.timeout\"\n+  private val S3_SSL_ENABLED: String = \"spark.s3.ssl.enabled\"\n+  private val S3_USE_INSTANCE_CREDENTIALS: String = \"spark.s3.use.instance.credentials\"\n+\n+  private val FILEINPUTFORMAT_INPUTDIR: String =\n+    \"mapreduce.input.fileinputformat.inputdir\"\n+  private val FILEINPUTFORMAT_NUMINPUTFILES: String =\n+    \"mapreduce.input.fileinputformat.numinputfiles\"\n+  private val FILEINPUTFORMAT_SPLIT_MINSIZE: String =\n+    \"mapreduce.input.fileinputformat.split.minsize\"\n+\n+  private val hiddenFileFilter = new PathFilter() {\n+    override def accept(p: Path): Boolean = {\n+      val name: String = p.getName()\n+      !name.startsWith(\"_\") && !name.startsWith(\".\")\n+    }\n+  }\n+\n+  private def getS3Client(bucket: String): AmazonS3Client = {\n+    val sslEnabled: Boolean = sparkConf.getBoolean(S3_SSL_ENABLED, true)\n+    val maxErrorRetries: Int = sparkConf.getInt(S3_MAX_ERROR_RETRIES, 10)\n+    val connectTimeout: Int = sparkConf.getInt(S3_CONNECT_TIMEOUT, 5000)\n+    val socketTimeout: Int = sparkConf.getInt(S3_SOCKET_TIMEOUT, 5000)\n+    val maxConnections: Int = sparkConf.getInt(S3_MAX_CONNECTIONS, 500)\n+    val useInstanceCredentials: Boolean = sparkConf.getBoolean(S3_USE_INSTANCE_CREDENTIALS, false)\n+\n+    val clientConf: ClientConfiguration = new ClientConfiguration\n+    clientConf.setMaxErrorRetry(maxErrorRetries)\n+    clientConf.setProtocol(if (sslEnabled) Protocol.HTTPS else Protocol.HTTP)\n+    clientConf.setConnectionTimeout(connectTimeout)\n+    clientConf.setSocketTimeout(socketTimeout)\n+    clientConf.setMaxConnections(maxConnections)\n+\n+    val s3RoleArn = Option(conf.get(\"aws.iam.role.arn\"))\n+    val s3RoleArnDefault = Option(conf.get(\"aws.iam.role.arn.default\"))\n+    val credentialsProvider: AWSCredentialsProvider =\n+      s3RoleArn match {\n+        case Some(role) =>\n+          logDebug(\"Use user-specified IAM role: \" + role)\n+          new STSAssumeRoleSessionCredentialsProvider(\n+            role, \"RoleSessionName-\" + Utils.random.nextInt)\n+        case None if useInstanceCredentials =>\n+          logDebug(\"Use IAM role associated with the instance\")\n+          new InstanceProfileCredentialsProvider\n+        case _ =>\n+          s3RoleArnDefault match {\n+            case Some(role) =>\n+              logDebug(\"Use default IAM role configured in Hadoop config: \" + role)\n+              new STSAssumeRoleSessionCredentialsProvider(\n+                role, \"RoleSessionName-\" + Utils.random.nextInt)\n+            case _ =>\n+              try {\n+                logDebug(\"Use AWS key pairs\")\n+                val credentials: S3Credentials = new S3Credentials\n+                credentials.initialize(URI.create(bucket), conf)\n+                new StaticCredentialsProvider(\n+                  new BasicAWSCredentials(credentials.getAccessKey, credentials.getSecretAccessKey))\n+              } catch {\n+                case e: Exception => throw new RuntimeException(\"S3 credentials not configured\", e)\n+              }\n+        }\n+      }\n+\n+    val providerName: String = credentialsProvider.getClass.getSimpleName\n+    val s3ClientKey: String =\n+      if (s3RoleArn == null) providerName\n+      else providerName + \"_\" + s3RoleArn\n+\n+    Option(s3ClientCache.getIfPresent(s3ClientKey)).getOrElse {\n+      val newClient = new AmazonS3Client(credentialsProvider, clientConf)\n+      s3ClientCache.put(s3ClientKey, newClient)\n+      newClient\n+    }\n+  }\n+\n+  private def keyFromPath(path: Path): String = {\n+    Preconditions.checkArgument(path.isAbsolute, \"Path is not absolute: %s\", path)\n+    var key: String = Strings.nullToEmpty(path.toUri.getPath)\n+    if (key.startsWith(\"/\")) {\n+      key = key.substring(1)\n+    }\n+    if (key.endsWith(\"/\")) {\n+      key = key.substring(0, key.length - 1)\n+    }\n+    key\n+  }\n+\n+  private def listPrefix(s3: AmazonS3Client, path: Path): Iterator[LocatedFileStatus] = {\n+    val uri: URI = path.toUri\n+    val key: String = keyFromPath(path)\n+    val request: ListObjectsRequest = new ListObjectsRequest()\n+      .withBucketName(uri.getAuthority)\n+      .withPrefix(key)\n+\n+    val listings = new AbstractSequentialIterator[ObjectListing](s3.listObjects(request)) {\n+      protected def computeNext(previous: ObjectListing): ObjectListing = {\n+        if (!previous.isTruncated) {\n+          return null\n+        }\n+        s3.listNextBatchOfObjects(previous)\n+      }\n+    }.asScala\n+\n+    val bucket: String = uri.getScheme + \"://\" + uri.getAuthority\n+    listings\n+      .map(listing => statusFromObjects(bucket, listing.getObjectSummaries))\n+      .reduceLeft(_ ++ _)\n+  }\n+\n+  private def statusFromObjects(\n+      bucket: String,\n+      objects: util.List[S3ObjectSummary]): Iterator[LocatedFileStatus] = {\n+    val blockSize: Int = conf.getInt(\"fs.s3n.block.size\", 67108864)\n+    val list: ArrayBuffer[LocatedFileStatus] = ArrayBuffer()\n+    for (obj: S3ObjectSummary <- objects.asScala) {\n+      if (!obj.getKey.endsWith(\"/\")) {\n+        val path = new Path(bucket + \"/\" + obj.getKey)\n+        if (hiddenFileFilter.accept(path)) {\n+          val status: FileStatus = new FileStatus(\n+            obj.getSize,\n+            false,\n+            1,\n+            blockSize,\n+            obj.getLastModified.getTime,\n+            path)\n+          list += createLocatedFileStatus(status)\n+        }\n+      }\n+    }\n+    list.iterator\n+  }\n+\n+  private def createLocatedFileStatus(status: FileStatus): LocatedFileStatus = {\n+    val fakeLocation: Array[BlockLocation] = Array[BlockLocation]()\n+    new LocatedFileStatus(status, fakeLocation)\n+  }\n+\n+  private def listLocatedStatus(\n+      s3: AmazonS3Client,\n+      path: Path): RemoteIterator[LocatedFileStatus] = {\n+    new RemoteIterator[LocatedFileStatus]() {\n+      private final val iterator: Iterator[LocatedFileStatus] = listPrefix(s3, path)\n+\n+      def hasNext: Boolean = {\n+        try {\n+          iterator.hasNext\n+        } catch {\n+          case e: AmazonClientException => throw new IOException(e)\n+        }\n+      }\n+\n+      def next(): LocatedFileStatus = {\n+        try {\n+          iterator.next()\n+        } catch {\n+          case e: AmazonClientException => throw new IOException(e)\n+        }\n+      }\n+    }\n+  }\n+\n+  private def listStatus(s3: AmazonS3Client, paths: List[String]): Array[FileStatus] = {\n+    val list: ArrayBuffer[FileStatus] = ArrayBuffer()\n+    paths.foreach { path =>\n+      val iterator = listLocatedStatus(s3, new Path(path))\n+      while (iterator.hasNext) {\n+        list += iterator.next\n+      }\n+    }\n+    list.toArray\n+  }\n+\n+  private def isSplitable(conf: JobConf, file: Path): Boolean = {\n+    val compressionCodecs = new CompressionCodecFactory(conf)\n+    val codec = compressionCodecs.getCodec(file)\n+    if (codec == null) {\n+      true\n+    } else {\n+      codec.isInstanceOf[SplittableCompressionCodec]\n+    }\n+  }\n+\n+  /**\n+   * Return whether the given path is an S3 path or not.\n+   */\n+  def isS3Path(path: Path): Boolean = {\n+    val scheme = Option(path.toUri.getScheme)\n+    scheme.map(_.toUpperCase.startsWith(\"S3\")).getOrElse(false)\n+  }\n+\n+  /**\n+   * This is based on `FileInputFormat.getSplits` method. Two key differences are:\n+   *   1) Use `AmazonS3Client.listObjects` instead of `FileSystem.listStatus`.\n+   *   2) Bypass data locality hints since they're irrelevant to S3 objects.\n+   */\n+  def getSplits(jobConf: JobConf, minSplits: Int): Array[InputSplit] = {\n+    val inputDirs: Array[String] = jobConf.get(FILEINPUTFORMAT_INPUTDIR).split(\",\")\n+    val files: Array[FileStatus] = inputDirs.toList\n+      .groupBy[String] { path =>\n+        val uri = URI.create(path)\n+        uri.getScheme + \"://\" + uri.getAuthority\n+      }\n+      .map { case (bucket, paths) => (bucket, listStatus(getS3Client(bucket), paths)) }\n+      .values.reduceLeft(_ ++ _)\n+    jobConf.setLong(FILEINPUTFORMAT_NUMINPUTFILES, files.length)\n+\n+    val totalSize: Long = files.foldLeft(0L) { (sum, file) => sum + file.getLen }\n+    val goalSize: Long = totalSize / (if (minSplits == 0) 1 else minSplits)\n+    val minSize: Long = jobConf.getLong(FILEINPUTFORMAT_SPLIT_MINSIZE, 1)\n+    val splits: ArrayBuffer[InputSplit] = ArrayBuffer[InputSplit]()\n+    val fakeHosts: Array[String] = Array()"
  }, {
    "author": {
      "login": "piaozhexiu"
    },
    "body": "Fixed.\n",
    "commit": "5bdd92443c33fc2ec72fabb141c878cf3488d1f4",
    "createdAt": "2015-10-09T19:08:06Z",
    "diffHunk": "@@ -0,0 +1,299 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io.IOException\n+import java.net.URI\n+import java.util\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.amazonaws.{AmazonClientException, ClientConfiguration, Protocol}\n+import com.amazonaws.auth.{AWSCredentialsProvider, BasicAWSCredentials, InstanceProfileCredentialsProvider, STSAssumeRoleSessionCredentialsProvider}\n+import com.amazonaws.internal.StaticCredentialsProvider\n+import com.amazonaws.services.s3.AmazonS3Client\n+import com.amazonaws.services.s3.model.{ListObjectsRequest, ObjectListing, S3ObjectSummary}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{BlockLocation, FileStatus, LocatedFileStatus, Path, PathFilter, RemoteIterator}\n+import org.apache.hadoop.fs.s3.S3Credentials\n+import org.apache.hadoop.io.compress.{CompressionCodecFactory, SplittableCompressionCodec}\n+import org.apache.hadoop.mapred.{FileSplit, InputSplit, JobConf}\n+\n+import org.apache.spark.{SparkConf, Logging}\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.util.Utils\n+\n+import com.google.common.base.{Preconditions, Strings}\n+import com.google.common.cache.{Cache, CacheBuilder}\n+import com.google.common.collect.AbstractSequentialIterator\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Contains util methods to interact with S3 from Spark.\n+ */\n+@DeveloperApi\n+object SparkS3Util extends Logging {\n+  private val sparkConf = new SparkConf()\n+  val conf: Configuration = SparkHadoopUtil.get.newConfiguration(sparkConf)\n+\n+  private val s3ClientCache: Cache[String, AmazonS3Client] = CacheBuilder\n+    .newBuilder\n+    .concurrencyLevel(Runtime.getRuntime.availableProcessors)\n+    .build[String, AmazonS3Client]\n+\n+  private val S3_CONNECT_TIMEOUT: String = \"spark.s3.connect.timeout\"\n+  private val S3_MAX_CONNECTIONS: String = \"spark.s3.max.connections\"\n+  private val S3_MAX_ERROR_RETRIES: String = \"spark.s3.max.error.retries\"\n+  private val S3_SOCKET_TIMEOUT: String = \"spark.s3.socket.timeout\"\n+  private val S3_SSL_ENABLED: String = \"spark.s3.ssl.enabled\"\n+  private val S3_USE_INSTANCE_CREDENTIALS: String = \"spark.s3.use.instance.credentials\"\n+\n+  private val FILEINPUTFORMAT_INPUTDIR: String =\n+    \"mapreduce.input.fileinputformat.inputdir\"\n+  private val FILEINPUTFORMAT_NUMINPUTFILES: String =\n+    \"mapreduce.input.fileinputformat.numinputfiles\"\n+  private val FILEINPUTFORMAT_SPLIT_MINSIZE: String =\n+    \"mapreduce.input.fileinputformat.split.minsize\"\n+\n+  private val hiddenFileFilter = new PathFilter() {\n+    override def accept(p: Path): Boolean = {\n+      val name: String = p.getName()\n+      !name.startsWith(\"_\") && !name.startsWith(\".\")\n+    }\n+  }\n+\n+  private def getS3Client(bucket: String): AmazonS3Client = {\n+    val sslEnabled: Boolean = sparkConf.getBoolean(S3_SSL_ENABLED, true)\n+    val maxErrorRetries: Int = sparkConf.getInt(S3_MAX_ERROR_RETRIES, 10)\n+    val connectTimeout: Int = sparkConf.getInt(S3_CONNECT_TIMEOUT, 5000)\n+    val socketTimeout: Int = sparkConf.getInt(S3_SOCKET_TIMEOUT, 5000)\n+    val maxConnections: Int = sparkConf.getInt(S3_MAX_CONNECTIONS, 500)\n+    val useInstanceCredentials: Boolean = sparkConf.getBoolean(S3_USE_INSTANCE_CREDENTIALS, false)\n+\n+    val clientConf: ClientConfiguration = new ClientConfiguration\n+    clientConf.setMaxErrorRetry(maxErrorRetries)\n+    clientConf.setProtocol(if (sslEnabled) Protocol.HTTPS else Protocol.HTTP)\n+    clientConf.setConnectionTimeout(connectTimeout)\n+    clientConf.setSocketTimeout(socketTimeout)\n+    clientConf.setMaxConnections(maxConnections)\n+\n+    val s3RoleArn = Option(conf.get(\"aws.iam.role.arn\"))\n+    val s3RoleArnDefault = Option(conf.get(\"aws.iam.role.arn.default\"))\n+    val credentialsProvider: AWSCredentialsProvider =\n+      s3RoleArn match {\n+        case Some(role) =>\n+          logDebug(\"Use user-specified IAM role: \" + role)\n+          new STSAssumeRoleSessionCredentialsProvider(\n+            role, \"RoleSessionName-\" + Utils.random.nextInt)\n+        case None if useInstanceCredentials =>\n+          logDebug(\"Use IAM role associated with the instance\")\n+          new InstanceProfileCredentialsProvider\n+        case _ =>\n+          s3RoleArnDefault match {\n+            case Some(role) =>\n+              logDebug(\"Use default IAM role configured in Hadoop config: \" + role)\n+              new STSAssumeRoleSessionCredentialsProvider(\n+                role, \"RoleSessionName-\" + Utils.random.nextInt)\n+            case _ =>\n+              try {\n+                logDebug(\"Use AWS key pairs\")\n+                val credentials: S3Credentials = new S3Credentials\n+                credentials.initialize(URI.create(bucket), conf)\n+                new StaticCredentialsProvider(\n+                  new BasicAWSCredentials(credentials.getAccessKey, credentials.getSecretAccessKey))\n+              } catch {\n+                case e: Exception => throw new RuntimeException(\"S3 credentials not configured\", e)\n+              }\n+        }\n+      }\n+\n+    val providerName: String = credentialsProvider.getClass.getSimpleName\n+    val s3ClientKey: String =\n+      if (s3RoleArn == null) providerName\n+      else providerName + \"_\" + s3RoleArn\n+\n+    Option(s3ClientCache.getIfPresent(s3ClientKey)).getOrElse {\n+      val newClient = new AmazonS3Client(credentialsProvider, clientConf)\n+      s3ClientCache.put(s3ClientKey, newClient)\n+      newClient\n+    }\n+  }\n+\n+  private def keyFromPath(path: Path): String = {\n+    Preconditions.checkArgument(path.isAbsolute, \"Path is not absolute: %s\", path)\n+    var key: String = Strings.nullToEmpty(path.toUri.getPath)\n+    if (key.startsWith(\"/\")) {\n+      key = key.substring(1)\n+    }\n+    if (key.endsWith(\"/\")) {\n+      key = key.substring(0, key.length - 1)\n+    }\n+    key\n+  }\n+\n+  private def listPrefix(s3: AmazonS3Client, path: Path): Iterator[LocatedFileStatus] = {\n+    val uri: URI = path.toUri\n+    val key: String = keyFromPath(path)\n+    val request: ListObjectsRequest = new ListObjectsRequest()\n+      .withBucketName(uri.getAuthority)\n+      .withPrefix(key)\n+\n+    val listings = new AbstractSequentialIterator[ObjectListing](s3.listObjects(request)) {\n+      protected def computeNext(previous: ObjectListing): ObjectListing = {\n+        if (!previous.isTruncated) {\n+          return null\n+        }\n+        s3.listNextBatchOfObjects(previous)\n+      }\n+    }.asScala\n+\n+    val bucket: String = uri.getScheme + \"://\" + uri.getAuthority\n+    listings\n+      .map(listing => statusFromObjects(bucket, listing.getObjectSummaries))\n+      .reduceLeft(_ ++ _)\n+  }\n+\n+  private def statusFromObjects(\n+      bucket: String,\n+      objects: util.List[S3ObjectSummary]): Iterator[LocatedFileStatus] = {\n+    val blockSize: Int = conf.getInt(\"fs.s3n.block.size\", 67108864)\n+    val list: ArrayBuffer[LocatedFileStatus] = ArrayBuffer()\n+    for (obj: S3ObjectSummary <- objects.asScala) {\n+      if (!obj.getKey.endsWith(\"/\")) {\n+        val path = new Path(bucket + \"/\" + obj.getKey)\n+        if (hiddenFileFilter.accept(path)) {\n+          val status: FileStatus = new FileStatus(\n+            obj.getSize,\n+            false,\n+            1,\n+            blockSize,\n+            obj.getLastModified.getTime,\n+            path)\n+          list += createLocatedFileStatus(status)\n+        }\n+      }\n+    }\n+    list.iterator\n+  }\n+\n+  private def createLocatedFileStatus(status: FileStatus): LocatedFileStatus = {\n+    val fakeLocation: Array[BlockLocation] = Array[BlockLocation]()\n+    new LocatedFileStatus(status, fakeLocation)\n+  }\n+\n+  private def listLocatedStatus(\n+      s3: AmazonS3Client,\n+      path: Path): RemoteIterator[LocatedFileStatus] = {\n+    new RemoteIterator[LocatedFileStatus]() {\n+      private final val iterator: Iterator[LocatedFileStatus] = listPrefix(s3, path)\n+\n+      def hasNext: Boolean = {\n+        try {\n+          iterator.hasNext\n+        } catch {\n+          case e: AmazonClientException => throw new IOException(e)\n+        }\n+      }\n+\n+      def next(): LocatedFileStatus = {\n+        try {\n+          iterator.next()\n+        } catch {\n+          case e: AmazonClientException => throw new IOException(e)\n+        }\n+      }\n+    }\n+  }\n+\n+  private def listStatus(s3: AmazonS3Client, paths: List[String]): Array[FileStatus] = {\n+    val list: ArrayBuffer[FileStatus] = ArrayBuffer()\n+    paths.foreach { path =>\n+      val iterator = listLocatedStatus(s3, new Path(path))\n+      while (iterator.hasNext) {\n+        list += iterator.next\n+      }\n+    }\n+    list.toArray\n+  }\n+\n+  private def isSplitable(conf: JobConf, file: Path): Boolean = {\n+    val compressionCodecs = new CompressionCodecFactory(conf)\n+    val codec = compressionCodecs.getCodec(file)\n+    if (codec == null) {\n+      true\n+    } else {\n+      codec.isInstanceOf[SplittableCompressionCodec]\n+    }\n+  }\n+\n+  /**\n+   * Return whether the given path is an S3 path or not.\n+   */\n+  def isS3Path(path: Path): Boolean = {\n+    val scheme = Option(path.toUri.getScheme)\n+    scheme.map(_.toUpperCase.startsWith(\"S3\")).getOrElse(false)\n+  }\n+\n+  /**\n+   * This is based on `FileInputFormat.getSplits` method. Two key differences are:\n+   *   1) Use `AmazonS3Client.listObjects` instead of `FileSystem.listStatus`.\n+   *   2) Bypass data locality hints since they're irrelevant to S3 objects.\n+   */\n+  def getSplits(jobConf: JobConf, minSplits: Int): Array[InputSplit] = {\n+    val inputDirs: Array[String] = jobConf.get(FILEINPUTFORMAT_INPUTDIR).split(\",\")\n+    val files: Array[FileStatus] = inputDirs.toList\n+      .groupBy[String] { path =>\n+        val uri = URI.create(path)\n+        uri.getScheme + \"://\" + uri.getAuthority\n+      }\n+      .map { case (bucket, paths) => (bucket, listStatus(getS3Client(bucket), paths)) }\n+      .values.reduceLeft(_ ++ _)\n+    jobConf.setLong(FILEINPUTFORMAT_NUMINPUTFILES, files.length)\n+\n+    val totalSize: Long = files.foldLeft(0L) { (sum, file) => sum + file.getLen }\n+    val goalSize: Long = totalSize / (if (minSplits == 0) 1 else minSplits)\n+    val minSize: Long = jobConf.getLong(FILEINPUTFORMAT_SPLIT_MINSIZE, 1)\n+    val splits: ArrayBuffer[InputSplit] = ArrayBuffer[InputSplit]()\n+    val fakeHosts: Array[String] = Array()"
  }],
  "prId": 8512
}, {
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "How about we set the default value to `false`?\n",
    "commit": "5bdd92443c33fc2ec72fabb141c878cf3488d1f4",
    "createdAt": "2015-10-29T20:12:47Z",
    "diffHunk": "@@ -0,0 +1,336 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.net.URI\n+import java.util\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.amazonaws.{AmazonClientException, AmazonServiceException, ClientConfiguration, Protocol}\n+import com.amazonaws.auth.{AWSCredentialsProvider, BasicAWSCredentials, InstanceProfileCredentialsProvider, STSAssumeRoleSessionCredentialsProvider}\n+import com.amazonaws.internal.StaticCredentialsProvider\n+import com.amazonaws.services.s3.AmazonS3Client\n+import com.amazonaws.services.s3.model.{ListObjectsRequest, ObjectListing, S3ObjectSummary}\n+\n+import com.google.common.annotations.VisibleForTesting\n+import com.google.common.base.{Preconditions, Strings}\n+import com.google.common.cache.{Cache, CacheBuilder}\n+import com.google.common.collect.AbstractSequentialIterator\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, GlobPattern, Path, PathFilter}\n+import org.apache.hadoop.fs.s3.S3Credentials\n+import org.apache.hadoop.io.compress.{CompressionCodecFactory, SplittableCompressionCodec}\n+import org.apache.hadoop.mapred.{FileInputFormat, FileSplit, InputSplit, JobConf}\n+\n+import org.apache.spark.{SparkConf, Logging}\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Contains util methods to interact with S3 from Spark.\n+ */\n+@DeveloperApi\n+object SparkS3Util extends Logging {\n+  val sparkConf = new SparkConf()\n+  val conf: Configuration = SparkHadoopUtil.get.newConfiguration(sparkConf)\n+\n+  private val s3ClientCache: Cache[String, AmazonS3Client] = CacheBuilder\n+    .newBuilder\n+    .concurrencyLevel(Runtime.getRuntime.availableProcessors)\n+    .build[String, AmazonS3Client]\n+\n+  // Flag to enable S3 bulk listing. It is true by default.\n+  private val S3_BULK_LISTING_ENABLED: String = \"spark.s3.bulk.listing.enabled\"\n+\n+  // Properties for AmazonS3Client. Default values should just work most of time.\n+  private val S3_CONNECT_TIMEOUT: String = \"spark.s3.connect.timeout\"\n+  private val S3_MAX_CONNECTIONS: String = \"spark.s3.max.connections\"\n+  private val S3_MAX_ERROR_RETRIES: String = \"spark.s3.max.error.retries\"\n+  private val S3_SOCKET_TIMEOUT: String = \"spark.s3.socket.timeout\"\n+  private val S3_SSL_ENABLED: String = \"spark.s3.ssl.enabled\"\n+  private val S3_USE_INSTANCE_CREDENTIALS: String = \"spark.s3.use.instance.credentials\"\n+\n+  // Ignore hidden files whose name starts with \"_\" and \".\", or ends with \"$folder$\".\n+  private val hiddenFileFilter = new PathFilter() {\n+    override def accept(p: Path): Boolean = {\n+      val name: String = p.getName()\n+      !name.startsWith(\"_\") && !name.startsWith(\".\") && !name.endsWith(\"$folder$\")\n+    }\n+  }\n+\n+  /**\n+   * Initialize AmazonS3Client per bucket. Since access permissions might be different from bucket\n+   * to bucket, it is necessary to initialize AmazonS3Client on a per bucket basis.\n+   */\n+  private def getS3Client(bucket: String): AmazonS3Client = {\n+    val sslEnabled: Boolean = sparkConf.getBoolean(S3_SSL_ENABLED, true)\n+    val maxErrorRetries: Int = sparkConf.getInt(S3_MAX_ERROR_RETRIES, 10)\n+    val connectTimeout: Int = sparkConf.getInt(S3_CONNECT_TIMEOUT, 5000)\n+    val socketTimeout: Int = sparkConf.getInt(S3_SOCKET_TIMEOUT, 5000)\n+    val maxConnections: Int = sparkConf.getInt(S3_MAX_CONNECTIONS, 5)\n+    val useInstanceCredentials: Boolean = sparkConf.getBoolean(S3_USE_INSTANCE_CREDENTIALS, false)\n+\n+    val clientConf: ClientConfiguration = new ClientConfiguration\n+    clientConf.setMaxErrorRetry(maxErrorRetries)\n+    clientConf.setProtocol(if (sslEnabled) Protocol.HTTPS else Protocol.HTTP)\n+    clientConf.setConnectionTimeout(connectTimeout)\n+    clientConf.setSocketTimeout(socketTimeout)\n+    clientConf.setMaxConnections(maxConnections)\n+\n+    // There are different ways of obtaining S3 bucket access. Try them in the following order:\n+    //   1) Check if user specified an IAM role. If so, use it.\n+    //   2) Check if instance is associated with an IAM role. If so, use it.\n+    //   3) Check if default IAM role is set in Hadoop conf. If so, use it.\n+    //   4) If no IAM role is found, search for an AWS key pair.\n+    // If no credentials are found, throw an exception.\n+    val s3RoleArn = Option(conf.get(\"aws.iam.role.arn\"))\n+    val s3RoleArnDefault = Option(conf.get(\"aws.iam.role.arn.default\"))\n+    val credentialsProvider: AWSCredentialsProvider =\n+      s3RoleArn match {\n+        case Some(role) =>\n+          logDebug(\"Use user-specified IAM role: \" + role)\n+          new STSAssumeRoleSessionCredentialsProvider(\n+            role, \"RoleSessionName-\" + Utils.random.nextInt)\n+        case None if useInstanceCredentials =>\n+          logDebug(\"Use IAM role associated with the instance\")\n+          new InstanceProfileCredentialsProvider\n+        case _ =>\n+          s3RoleArnDefault match {\n+            case Some(role) =>\n+              logDebug(\"Use default IAM role configured in Hadoop config: \" + role)\n+              new STSAssumeRoleSessionCredentialsProvider(\n+                role, \"RoleSessionName-\" + Utils.random.nextInt)\n+            case _ =>\n+              try {\n+                logDebug(\"Use AWS key pair\")\n+                val credentials: S3Credentials = new S3Credentials\n+                credentials.initialize(URI.create(bucket), conf)\n+                new StaticCredentialsProvider(\n+                  new BasicAWSCredentials(credentials.getAccessKey, credentials.getSecretAccessKey))\n+              } catch {\n+                case e: Exception => throw new RuntimeException(\"S3 credentials not configured\", e)\n+              }\n+        }\n+      }\n+\n+    val providerName: String = credentialsProvider.getClass.getSimpleName\n+    val s3ClientKey: String =\n+      if (s3RoleArn == null) providerName\n+      else providerName + \"_\" + s3RoleArn\n+\n+    Option(s3ClientCache.getIfPresent(s3ClientKey)).getOrElse {\n+      val newClient = new AmazonS3Client(credentialsProvider, clientConf)\n+      s3ClientCache.put(s3ClientKey, newClient)\n+      newClient\n+    }\n+  }\n+\n+  /**\n+   * Helper function to extract S3 object key name from the given path.\n+   */\n+  private def keyFromPath(path: Path): String = {\n+    Preconditions.checkArgument(path.isAbsolute, \"Path is not absolute: %s\", path)\n+    var key: String = Strings.nullToEmpty(path.toUri.getPath)\n+    if (key.startsWith(\"/\")) {\n+      key = key.substring(1)\n+    }\n+    if (key.endsWith(\"/\")) {\n+      key = key.substring(0, key.length - 1)\n+    }\n+    key\n+  }\n+\n+  /**\n+   * Helper function to convert S3ObjectSummary into FileStatus.\n+   */\n+  private def statusFromObjects(\n+      bucket: String,\n+      objects: util.List[S3ObjectSummary]): Iterator[FileStatus] = {\n+    val blockSize: Long = getS3BlockSize()\n+    val list: ArrayBuffer[FileStatus] = ArrayBuffer()\n+    for (obj: S3ObjectSummary <- objects.asScala) {\n+      if (!obj.getKey.endsWith(\"/\")) {\n+        val path = new Path(bucket + \"/\" + obj.getKey)\n+        if (hiddenFileFilter.accept(path)) {\n+          val status: FileStatus = new FileStatus(\n+            obj.getSize,\n+            false,\n+            1,\n+            blockSize,\n+            obj.getLastModified.getTime,\n+            path)\n+          list += status\n+        }\n+      }\n+    }\n+    list.iterator\n+  }\n+\n+  /**\n+   * For the given path, list S3 objects and return an iterator of returned FileStatuses.\n+   */\n+  @throws(classOf[AmazonClientException])\n+  @throws(classOf[AmazonServiceException])\n+  private def listPrefix(s3: AmazonS3Client, path: Path): Iterator[FileStatus] = {\n+    val uri: URI = path.toUri\n+    val key: String = keyFromPath(path)\n+    val request: ListObjectsRequest = new ListObjectsRequest()\n+      .withBucketName(uri.getAuthority)\n+      .withPrefix(key)\n+\n+    val listings = new AbstractSequentialIterator[ObjectListing](s3.listObjects(request)) {\n+      protected def computeNext(previous: ObjectListing): ObjectListing = {\n+        if (!previous.isTruncated) {\n+          return null\n+        }\n+        s3.listNextBatchOfObjects(previous)\n+      }\n+    }.asScala\n+\n+    val bucket: String = uri.getScheme + \"://\" + uri.getAuthority\n+    listings\n+      .map(listing => statusFromObjects(bucket, listing.getObjectSummaries))\n+      .reduceLeft(_ ++ _)\n+  }\n+\n+  /**\n+   * For the given list of paths, sequentially list S3 objects per path and combine returned\n+   * FileStatuses into a single array.\n+   */\n+  private def listStatus(s3: AmazonS3Client, paths: List[Path]): Array[FileStatus] = {\n+    val list: ArrayBuffer[FileStatus] = ArrayBuffer()\n+    paths.foreach { path =>\n+      val iterator = listPrefix(s3, path)\n+      while (iterator.hasNext) {\n+        list += iterator.next\n+      }\n+    }\n+    list.toArray\n+  }\n+\n+  /**\n+   * Find S3 block size from Hadoop conf. Try both s3 and s3n names.\n+   */\n+  private def getS3BlockSize(): Long = {\n+    val value = Option(conf.get(\"fs.s3.block.size\"))\n+      .getOrElse(conf.get(\"fs.s3n.block.size\", \"67108864\"))\n+    value.toLong\n+  }\n+\n+  /**\n+   * Find min split size from Hadoop conf. Try both Hadoop 1 and 2 names.\n+   */\n+  private def getMinSplitSize(): Long = {\n+    val value = Option(conf.get(\"mapred.min.split.size\"))\n+      .getOrElse(conf.get(\"mapreduce.input.fileinputformat.split.minsize\", \"134217728\"))\n+    value.toLong\n+  }\n+\n+  /**\n+   * Return whether the given path is an S3 path or not.\n+   */\n+  private def isS3Path(path: Path): Boolean = {\n+    Option(path.toUri.getScheme).exists(_.toUpperCase.startsWith(\"S3\"))\n+  }\n+\n+  /**\n+   * Return whether S3 bulk listing can be enabled or not.\n+   */\n+  def s3BulkListingEnabled(jobConf: JobConf): Boolean = {\n+    val enabledByUser = sparkConf.getBoolean(S3_BULK_LISTING_ENABLED, true)"
  }, {
    "author": {
      "login": "piaozhexiu"
    },
    "body": "Fixed.\n",
    "commit": "5bdd92443c33fc2ec72fabb141c878cf3488d1f4",
    "createdAt": "2015-10-30T17:40:42Z",
    "diffHunk": "@@ -0,0 +1,336 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.net.URI\n+import java.util\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.amazonaws.{AmazonClientException, AmazonServiceException, ClientConfiguration, Protocol}\n+import com.amazonaws.auth.{AWSCredentialsProvider, BasicAWSCredentials, InstanceProfileCredentialsProvider, STSAssumeRoleSessionCredentialsProvider}\n+import com.amazonaws.internal.StaticCredentialsProvider\n+import com.amazonaws.services.s3.AmazonS3Client\n+import com.amazonaws.services.s3.model.{ListObjectsRequest, ObjectListing, S3ObjectSummary}\n+\n+import com.google.common.annotations.VisibleForTesting\n+import com.google.common.base.{Preconditions, Strings}\n+import com.google.common.cache.{Cache, CacheBuilder}\n+import com.google.common.collect.AbstractSequentialIterator\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, GlobPattern, Path, PathFilter}\n+import org.apache.hadoop.fs.s3.S3Credentials\n+import org.apache.hadoop.io.compress.{CompressionCodecFactory, SplittableCompressionCodec}\n+import org.apache.hadoop.mapred.{FileInputFormat, FileSplit, InputSplit, JobConf}\n+\n+import org.apache.spark.{SparkConf, Logging}\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Contains util methods to interact with S3 from Spark.\n+ */\n+@DeveloperApi\n+object SparkS3Util extends Logging {\n+  val sparkConf = new SparkConf()\n+  val conf: Configuration = SparkHadoopUtil.get.newConfiguration(sparkConf)\n+\n+  private val s3ClientCache: Cache[String, AmazonS3Client] = CacheBuilder\n+    .newBuilder\n+    .concurrencyLevel(Runtime.getRuntime.availableProcessors)\n+    .build[String, AmazonS3Client]\n+\n+  // Flag to enable S3 bulk listing. It is true by default.\n+  private val S3_BULK_LISTING_ENABLED: String = \"spark.s3.bulk.listing.enabled\"\n+\n+  // Properties for AmazonS3Client. Default values should just work most of time.\n+  private val S3_CONNECT_TIMEOUT: String = \"spark.s3.connect.timeout\"\n+  private val S3_MAX_CONNECTIONS: String = \"spark.s3.max.connections\"\n+  private val S3_MAX_ERROR_RETRIES: String = \"spark.s3.max.error.retries\"\n+  private val S3_SOCKET_TIMEOUT: String = \"spark.s3.socket.timeout\"\n+  private val S3_SSL_ENABLED: String = \"spark.s3.ssl.enabled\"\n+  private val S3_USE_INSTANCE_CREDENTIALS: String = \"spark.s3.use.instance.credentials\"\n+\n+  // Ignore hidden files whose name starts with \"_\" and \".\", or ends with \"$folder$\".\n+  private val hiddenFileFilter = new PathFilter() {\n+    override def accept(p: Path): Boolean = {\n+      val name: String = p.getName()\n+      !name.startsWith(\"_\") && !name.startsWith(\".\") && !name.endsWith(\"$folder$\")\n+    }\n+  }\n+\n+  /**\n+   * Initialize AmazonS3Client per bucket. Since access permissions might be different from bucket\n+   * to bucket, it is necessary to initialize AmazonS3Client on a per bucket basis.\n+   */\n+  private def getS3Client(bucket: String): AmazonS3Client = {\n+    val sslEnabled: Boolean = sparkConf.getBoolean(S3_SSL_ENABLED, true)\n+    val maxErrorRetries: Int = sparkConf.getInt(S3_MAX_ERROR_RETRIES, 10)\n+    val connectTimeout: Int = sparkConf.getInt(S3_CONNECT_TIMEOUT, 5000)\n+    val socketTimeout: Int = sparkConf.getInt(S3_SOCKET_TIMEOUT, 5000)\n+    val maxConnections: Int = sparkConf.getInt(S3_MAX_CONNECTIONS, 5)\n+    val useInstanceCredentials: Boolean = sparkConf.getBoolean(S3_USE_INSTANCE_CREDENTIALS, false)\n+\n+    val clientConf: ClientConfiguration = new ClientConfiguration\n+    clientConf.setMaxErrorRetry(maxErrorRetries)\n+    clientConf.setProtocol(if (sslEnabled) Protocol.HTTPS else Protocol.HTTP)\n+    clientConf.setConnectionTimeout(connectTimeout)\n+    clientConf.setSocketTimeout(socketTimeout)\n+    clientConf.setMaxConnections(maxConnections)\n+\n+    // There are different ways of obtaining S3 bucket access. Try them in the following order:\n+    //   1) Check if user specified an IAM role. If so, use it.\n+    //   2) Check if instance is associated with an IAM role. If so, use it.\n+    //   3) Check if default IAM role is set in Hadoop conf. If so, use it.\n+    //   4) If no IAM role is found, search for an AWS key pair.\n+    // If no credentials are found, throw an exception.\n+    val s3RoleArn = Option(conf.get(\"aws.iam.role.arn\"))\n+    val s3RoleArnDefault = Option(conf.get(\"aws.iam.role.arn.default\"))\n+    val credentialsProvider: AWSCredentialsProvider =\n+      s3RoleArn match {\n+        case Some(role) =>\n+          logDebug(\"Use user-specified IAM role: \" + role)\n+          new STSAssumeRoleSessionCredentialsProvider(\n+            role, \"RoleSessionName-\" + Utils.random.nextInt)\n+        case None if useInstanceCredentials =>\n+          logDebug(\"Use IAM role associated with the instance\")\n+          new InstanceProfileCredentialsProvider\n+        case _ =>\n+          s3RoleArnDefault match {\n+            case Some(role) =>\n+              logDebug(\"Use default IAM role configured in Hadoop config: \" + role)\n+              new STSAssumeRoleSessionCredentialsProvider(\n+                role, \"RoleSessionName-\" + Utils.random.nextInt)\n+            case _ =>\n+              try {\n+                logDebug(\"Use AWS key pair\")\n+                val credentials: S3Credentials = new S3Credentials\n+                credentials.initialize(URI.create(bucket), conf)\n+                new StaticCredentialsProvider(\n+                  new BasicAWSCredentials(credentials.getAccessKey, credentials.getSecretAccessKey))\n+              } catch {\n+                case e: Exception => throw new RuntimeException(\"S3 credentials not configured\", e)\n+              }\n+        }\n+      }\n+\n+    val providerName: String = credentialsProvider.getClass.getSimpleName\n+    val s3ClientKey: String =\n+      if (s3RoleArn == null) providerName\n+      else providerName + \"_\" + s3RoleArn\n+\n+    Option(s3ClientCache.getIfPresent(s3ClientKey)).getOrElse {\n+      val newClient = new AmazonS3Client(credentialsProvider, clientConf)\n+      s3ClientCache.put(s3ClientKey, newClient)\n+      newClient\n+    }\n+  }\n+\n+  /**\n+   * Helper function to extract S3 object key name from the given path.\n+   */\n+  private def keyFromPath(path: Path): String = {\n+    Preconditions.checkArgument(path.isAbsolute, \"Path is not absolute: %s\", path)\n+    var key: String = Strings.nullToEmpty(path.toUri.getPath)\n+    if (key.startsWith(\"/\")) {\n+      key = key.substring(1)\n+    }\n+    if (key.endsWith(\"/\")) {\n+      key = key.substring(0, key.length - 1)\n+    }\n+    key\n+  }\n+\n+  /**\n+   * Helper function to convert S3ObjectSummary into FileStatus.\n+   */\n+  private def statusFromObjects(\n+      bucket: String,\n+      objects: util.List[S3ObjectSummary]): Iterator[FileStatus] = {\n+    val blockSize: Long = getS3BlockSize()\n+    val list: ArrayBuffer[FileStatus] = ArrayBuffer()\n+    for (obj: S3ObjectSummary <- objects.asScala) {\n+      if (!obj.getKey.endsWith(\"/\")) {\n+        val path = new Path(bucket + \"/\" + obj.getKey)\n+        if (hiddenFileFilter.accept(path)) {\n+          val status: FileStatus = new FileStatus(\n+            obj.getSize,\n+            false,\n+            1,\n+            blockSize,\n+            obj.getLastModified.getTime,\n+            path)\n+          list += status\n+        }\n+      }\n+    }\n+    list.iterator\n+  }\n+\n+  /**\n+   * For the given path, list S3 objects and return an iterator of returned FileStatuses.\n+   */\n+  @throws(classOf[AmazonClientException])\n+  @throws(classOf[AmazonServiceException])\n+  private def listPrefix(s3: AmazonS3Client, path: Path): Iterator[FileStatus] = {\n+    val uri: URI = path.toUri\n+    val key: String = keyFromPath(path)\n+    val request: ListObjectsRequest = new ListObjectsRequest()\n+      .withBucketName(uri.getAuthority)\n+      .withPrefix(key)\n+\n+    val listings = new AbstractSequentialIterator[ObjectListing](s3.listObjects(request)) {\n+      protected def computeNext(previous: ObjectListing): ObjectListing = {\n+        if (!previous.isTruncated) {\n+          return null\n+        }\n+        s3.listNextBatchOfObjects(previous)\n+      }\n+    }.asScala\n+\n+    val bucket: String = uri.getScheme + \"://\" + uri.getAuthority\n+    listings\n+      .map(listing => statusFromObjects(bucket, listing.getObjectSummaries))\n+      .reduceLeft(_ ++ _)\n+  }\n+\n+  /**\n+   * For the given list of paths, sequentially list S3 objects per path and combine returned\n+   * FileStatuses into a single array.\n+   */\n+  private def listStatus(s3: AmazonS3Client, paths: List[Path]): Array[FileStatus] = {\n+    val list: ArrayBuffer[FileStatus] = ArrayBuffer()\n+    paths.foreach { path =>\n+      val iterator = listPrefix(s3, path)\n+      while (iterator.hasNext) {\n+        list += iterator.next\n+      }\n+    }\n+    list.toArray\n+  }\n+\n+  /**\n+   * Find S3 block size from Hadoop conf. Try both s3 and s3n names.\n+   */\n+  private def getS3BlockSize(): Long = {\n+    val value = Option(conf.get(\"fs.s3.block.size\"))\n+      .getOrElse(conf.get(\"fs.s3n.block.size\", \"67108864\"))\n+    value.toLong\n+  }\n+\n+  /**\n+   * Find min split size from Hadoop conf. Try both Hadoop 1 and 2 names.\n+   */\n+  private def getMinSplitSize(): Long = {\n+    val value = Option(conf.get(\"mapred.min.split.size\"))\n+      .getOrElse(conf.get(\"mapreduce.input.fileinputformat.split.minsize\", \"134217728\"))\n+    value.toLong\n+  }\n+\n+  /**\n+   * Return whether the given path is an S3 path or not.\n+   */\n+  private def isS3Path(path: Path): Boolean = {\n+    Option(path.toUri.getScheme).exists(_.toUpperCase.startsWith(\"S3\"))\n+  }\n+\n+  /**\n+   * Return whether S3 bulk listing can be enabled or not.\n+   */\n+  def s3BulkListingEnabled(jobConf: JobConf): Boolean = {\n+    val enabledByUser = sparkConf.getBoolean(S3_BULK_LISTING_ENABLED, true)"
  }],
  "prId": 8512
}, {
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "Instead of creating a `SparkConf` at here, can we use `SparkEnv.get.conf``to get the conf associated with the`SparkContext`? If we create a new one at here, if users set confs we used for this tool in their application (not in the default conf file), we will not be able to pick up the settings, right?\n",
    "commit": "5bdd92443c33fc2ec72fabb141c878cf3488d1f4",
    "createdAt": "2015-10-29T20:24:04Z",
    "diffHunk": "@@ -0,0 +1,336 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.net.URI\n+import java.util\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.amazonaws.{AmazonClientException, AmazonServiceException, ClientConfiguration, Protocol}\n+import com.amazonaws.auth.{AWSCredentialsProvider, BasicAWSCredentials, InstanceProfileCredentialsProvider, STSAssumeRoleSessionCredentialsProvider}\n+import com.amazonaws.internal.StaticCredentialsProvider\n+import com.amazonaws.services.s3.AmazonS3Client\n+import com.amazonaws.services.s3.model.{ListObjectsRequest, ObjectListing, S3ObjectSummary}\n+\n+import com.google.common.annotations.VisibleForTesting\n+import com.google.common.base.{Preconditions, Strings}\n+import com.google.common.cache.{Cache, CacheBuilder}\n+import com.google.common.collect.AbstractSequentialIterator\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, GlobPattern, Path, PathFilter}\n+import org.apache.hadoop.fs.s3.S3Credentials\n+import org.apache.hadoop.io.compress.{CompressionCodecFactory, SplittableCompressionCodec}\n+import org.apache.hadoop.mapred.{FileInputFormat, FileSplit, InputSplit, JobConf}\n+\n+import org.apache.spark.{SparkConf, Logging}\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Contains util methods to interact with S3 from Spark.\n+ */\n+@DeveloperApi\n+object SparkS3Util extends Logging {\n+  val sparkConf = new SparkConf()"
  }, {
    "author": {
      "login": "piaozhexiu"
    },
    "body": "Fixed.\n",
    "commit": "5bdd92443c33fc2ec72fabb141c878cf3488d1f4",
    "createdAt": "2015-10-30T17:40:52Z",
    "diffHunk": "@@ -0,0 +1,336 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.net.URI\n+import java.util\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.amazonaws.{AmazonClientException, AmazonServiceException, ClientConfiguration, Protocol}\n+import com.amazonaws.auth.{AWSCredentialsProvider, BasicAWSCredentials, InstanceProfileCredentialsProvider, STSAssumeRoleSessionCredentialsProvider}\n+import com.amazonaws.internal.StaticCredentialsProvider\n+import com.amazonaws.services.s3.AmazonS3Client\n+import com.amazonaws.services.s3.model.{ListObjectsRequest, ObjectListing, S3ObjectSummary}\n+\n+import com.google.common.annotations.VisibleForTesting\n+import com.google.common.base.{Preconditions, Strings}\n+import com.google.common.cache.{Cache, CacheBuilder}\n+import com.google.common.collect.AbstractSequentialIterator\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, GlobPattern, Path, PathFilter}\n+import org.apache.hadoop.fs.s3.S3Credentials\n+import org.apache.hadoop.io.compress.{CompressionCodecFactory, SplittableCompressionCodec}\n+import org.apache.hadoop.mapred.{FileInputFormat, FileSplit, InputSplit, JobConf}\n+\n+import org.apache.spark.{SparkConf, Logging}\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Contains util methods to interact with S3 from Spark.\n+ */\n+@DeveloperApi\n+object SparkS3Util extends Logging {\n+  val sparkConf = new SparkConf()"
  }],
  "prId": 8512
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "Shouldn't this just be private[spark]? \n",
    "commit": "5bdd92443c33fc2ec72fabb141c878cf3488d1f4",
    "createdAt": "2015-11-03T05:38:19Z",
    "diffHunk": "@@ -0,0 +1,336 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.net.URI\n+import java.util\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.amazonaws.{AmazonClientException, AmazonServiceException, ClientConfiguration, Protocol}\n+import com.amazonaws.auth.{AWSCredentialsProvider, BasicAWSCredentials, InstanceProfileCredentialsProvider, STSAssumeRoleSessionCredentialsProvider}\n+import com.amazonaws.internal.StaticCredentialsProvider\n+import com.amazonaws.services.s3.AmazonS3Client\n+import com.amazonaws.services.s3.model.{ListObjectsRequest, ObjectListing, S3ObjectSummary}\n+\n+import com.google.common.annotations.VisibleForTesting\n+import com.google.common.base.{Preconditions, Strings}\n+import com.google.common.cache.{Cache, CacheBuilder}\n+import com.google.common.collect.AbstractSequentialIterator\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, GlobPattern, Path, PathFilter}\n+import org.apache.hadoop.fs.s3.S3Credentials\n+import org.apache.hadoop.io.compress.{CompressionCodecFactory, SplittableCompressionCodec}\n+import org.apache.hadoop.mapred.{FileInputFormat, FileSplit, InputSplit, JobConf}\n+\n+import org.apache.spark.{Logging, SparkEnv}\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Contains util methods to interact with S3 from Spark.\n+ */\n+@DeveloperApi\n+object SparkS3Util extends Logging {",
    "line": 51
  }, {
    "author": {
      "login": "piaozhexiu"
    },
    "body": "We could, but there are cases when users need to override aws confidentials at runtime by accessing to this object. For eg, at Netflix, there was an S3 bucket called `vault` which had different access permissions from default buckets. To access to this bucket, users had to explicitly set a special IAM role in user code.\n",
    "commit": "5bdd92443c33fc2ec72fabb141c878cf3488d1f4",
    "createdAt": "2015-11-09T04:22:19Z",
    "diffHunk": "@@ -0,0 +1,336 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.net.URI\n+import java.util\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.amazonaws.{AmazonClientException, AmazonServiceException, ClientConfiguration, Protocol}\n+import com.amazonaws.auth.{AWSCredentialsProvider, BasicAWSCredentials, InstanceProfileCredentialsProvider, STSAssumeRoleSessionCredentialsProvider}\n+import com.amazonaws.internal.StaticCredentialsProvider\n+import com.amazonaws.services.s3.AmazonS3Client\n+import com.amazonaws.services.s3.model.{ListObjectsRequest, ObjectListing, S3ObjectSummary}\n+\n+import com.google.common.annotations.VisibleForTesting\n+import com.google.common.base.{Preconditions, Strings}\n+import com.google.common.cache.{Cache, CacheBuilder}\n+import com.google.common.collect.AbstractSequentialIterator\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, GlobPattern, Path, PathFilter}\n+import org.apache.hadoop.fs.s3.S3Credentials\n+import org.apache.hadoop.io.compress.{CompressionCodecFactory, SplittableCompressionCodec}\n+import org.apache.hadoop.mapred.{FileInputFormat, FileSplit, InputSplit, JobConf}\n+\n+import org.apache.spark.{Logging, SparkEnv}\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Contains util methods to interact with S3 from Spark.\n+ */\n+@DeveloperApi\n+object SparkS3Util extends Logging {",
    "line": 51
  }],
  "prId": 8512
}, {
  "comments": [{
    "author": {
      "login": "steveloughran"
    },
    "body": "consider a sanity check here of a minimum blocksize. Setting one of these properties to \"1\" is an entertaining way to overload things\n",
    "commit": "5bdd92443c33fc2ec72fabb141c878cf3488d1f4",
    "createdAt": "2015-12-08T18:20:09Z",
    "diffHunk": "@@ -0,0 +1,336 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.net.URI\n+import java.util\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.amazonaws.{AmazonClientException, AmazonServiceException, ClientConfiguration, Protocol}\n+import com.amazonaws.auth.{AWSCredentialsProvider, BasicAWSCredentials, InstanceProfileCredentialsProvider, STSAssumeRoleSessionCredentialsProvider}\n+import com.amazonaws.internal.StaticCredentialsProvider\n+import com.amazonaws.services.s3.AmazonS3Client\n+import com.amazonaws.services.s3.model.{ListObjectsRequest, ObjectListing, S3ObjectSummary}\n+\n+import com.google.common.annotations.VisibleForTesting\n+import com.google.common.base.{Preconditions, Strings}\n+import com.google.common.cache.{Cache, CacheBuilder}\n+import com.google.common.collect.AbstractSequentialIterator\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, GlobPattern, Path, PathFilter}\n+import org.apache.hadoop.fs.s3.S3Credentials\n+import org.apache.hadoop.io.compress.{CompressionCodecFactory, SplittableCompressionCodec}\n+import org.apache.hadoop.mapred.{FileInputFormat, FileSplit, InputSplit, JobConf}\n+\n+import org.apache.spark.{Logging, SparkEnv}\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Contains util methods to interact with S3 from Spark.\n+ */\n+@DeveloperApi\n+object SparkS3Util extends Logging {\n+  val sparkConf = SparkEnv.get.conf\n+  val conf: Configuration = SparkHadoopUtil.get.newConfiguration(sparkConf)\n+\n+  private val s3ClientCache: Cache[String, AmazonS3Client] = CacheBuilder\n+    .newBuilder\n+    .concurrencyLevel(Runtime.getRuntime.availableProcessors)\n+    .build[String, AmazonS3Client]\n+\n+  // Flag to enable S3 bulk listing. It is true by default.\n+  private val S3_BULK_LISTING_ENABLED: String = \"spark.s3.bulk.listing.enabled\"\n+\n+  // Properties for AmazonS3Client. Default values should just work most of time.\n+  private val S3_CONNECT_TIMEOUT: String = \"spark.s3.connect.timeout\"\n+  private val S3_MAX_CONNECTIONS: String = \"spark.s3.max.connections\"\n+  private val S3_MAX_ERROR_RETRIES: String = \"spark.s3.max.error.retries\"\n+  private val S3_SOCKET_TIMEOUT: String = \"spark.s3.socket.timeout\"\n+  private val S3_SSL_ENABLED: String = \"spark.s3.ssl.enabled\"\n+  private val S3_USE_INSTANCE_CREDENTIALS: String = \"spark.s3.use.instance.credentials\"\n+\n+  // Ignore hidden files whose name starts with \"_\" and \".\", or ends with \"$folder$\".\n+  private val hiddenFileFilter = new PathFilter() {\n+    override def accept(p: Path): Boolean = {\n+      val name: String = p.getName()\n+      !name.startsWith(\"_\") && !name.startsWith(\".\") && !name.endsWith(\"$folder$\")\n+    }\n+  }\n+\n+  /**\n+   * Initialize AmazonS3Client per bucket. Since access permissions might be different from bucket\n+   * to bucket, it is necessary to initialize AmazonS3Client on a per bucket basis.\n+   */\n+  private def getS3Client(bucket: String): AmazonS3Client = {\n+    val sslEnabled: Boolean = sparkConf.getBoolean(S3_SSL_ENABLED, true)\n+    val maxErrorRetries: Int = sparkConf.getInt(S3_MAX_ERROR_RETRIES, 10)\n+    val connectTimeout: Int = sparkConf.getInt(S3_CONNECT_TIMEOUT, 5000)\n+    val socketTimeout: Int = sparkConf.getInt(S3_SOCKET_TIMEOUT, 5000)\n+    val maxConnections: Int = sparkConf.getInt(S3_MAX_CONNECTIONS, 5)\n+    val useInstanceCredentials: Boolean = sparkConf.getBoolean(S3_USE_INSTANCE_CREDENTIALS, false)\n+\n+    val clientConf: ClientConfiguration = new ClientConfiguration\n+    clientConf.setMaxErrorRetry(maxErrorRetries)\n+    clientConf.setProtocol(if (sslEnabled) Protocol.HTTPS else Protocol.HTTP)\n+    clientConf.setConnectionTimeout(connectTimeout)\n+    clientConf.setSocketTimeout(socketTimeout)\n+    clientConf.setMaxConnections(maxConnections)\n+\n+    // There are different ways of obtaining S3 bucket access. Try them in the following order:\n+    //   1) Check if user specified an IAM role. If so, use it.\n+    //   2) Check if instance is associated with an IAM role. If so, use it.\n+    //   3) Check if default IAM role is set in Hadoop conf. If so, use it.\n+    //   4) If no IAM role is found, search for an AWS key pair.\n+    // If no credentials are found, throw an exception.\n+    val s3RoleArn = Option(conf.get(\"aws.iam.role.arn\"))\n+    val s3RoleArnDefault = Option(conf.get(\"aws.iam.role.arn.default\"))\n+    val credentialsProvider: AWSCredentialsProvider =\n+      s3RoleArn match {\n+        case Some(role) =>\n+          logDebug(\"Use user-specified IAM role: \" + role)\n+          new STSAssumeRoleSessionCredentialsProvider(\n+            role, \"RoleSessionName-\" + Utils.random.nextInt)\n+        case None if useInstanceCredentials =>\n+          logDebug(\"Use IAM role associated with the instance\")\n+          new InstanceProfileCredentialsProvider\n+        case _ =>\n+          s3RoleArnDefault match {\n+            case Some(role) =>\n+              logDebug(\"Use default IAM role configured in Hadoop config: \" + role)\n+              new STSAssumeRoleSessionCredentialsProvider(\n+                role, \"RoleSessionName-\" + Utils.random.nextInt)\n+            case _ =>\n+              try {\n+                logDebug(\"Use AWS key pair\")\n+                val credentials: S3Credentials = new S3Credentials\n+                credentials.initialize(URI.create(bucket), conf)\n+                new StaticCredentialsProvider(\n+                  new BasicAWSCredentials(credentials.getAccessKey, credentials.getSecretAccessKey))\n+              } catch {\n+                case e: Exception => throw new RuntimeException(\"S3 credentials not configured\", e)\n+              }\n+        }\n+      }\n+\n+    val providerName: String = credentialsProvider.getClass.getSimpleName\n+    val s3ClientKey: String =\n+      if (s3RoleArn == null) providerName\n+      else providerName + \"_\" + s3RoleArn\n+\n+    Option(s3ClientCache.getIfPresent(s3ClientKey)).getOrElse {\n+      val newClient = new AmazonS3Client(credentialsProvider, clientConf)\n+      s3ClientCache.put(s3ClientKey, newClient)\n+      newClient\n+    }\n+  }\n+\n+  /**\n+   * Helper function to extract S3 object key name from the given path.\n+   */\n+  private def keyFromPath(path: Path): String = {\n+    Preconditions.checkArgument(path.isAbsolute, \"Path is not absolute: %s\", path)\n+    var key: String = Strings.nullToEmpty(path.toUri.getPath)\n+    if (key.startsWith(\"/\")) {\n+      key = key.substring(1)\n+    }\n+    if (key.endsWith(\"/\")) {\n+      key = key.substring(0, key.length - 1)\n+    }\n+    key\n+  }\n+\n+  /**\n+   * Helper function to convert S3ObjectSummary into FileStatus.\n+   */\n+  private def statusFromObjects(\n+      bucket: String,\n+      objects: util.List[S3ObjectSummary]): Iterator[FileStatus] = {\n+    val blockSize: Long = getS3BlockSize()\n+    val list: ArrayBuffer[FileStatus] = ArrayBuffer()\n+    for (obj: S3ObjectSummary <- objects.asScala) {\n+      if (!obj.getKey.endsWith(\"/\")) {\n+        val path = new Path(bucket + \"/\" + obj.getKey)\n+        if (hiddenFileFilter.accept(path)) {\n+          val status: FileStatus = new FileStatus(\n+            obj.getSize,\n+            false,\n+            1,\n+            blockSize,\n+            obj.getLastModified.getTime,\n+            path)\n+          list += status\n+        }\n+      }\n+    }\n+    list.iterator\n+  }\n+\n+  /**\n+   * For the given path, list S3 objects and return an iterator of returned FileStatuses.\n+   */\n+  @throws(classOf[AmazonClientException])\n+  @throws(classOf[AmazonServiceException])\n+  private def listPrefix(s3: AmazonS3Client, path: Path): Iterator[FileStatus] = {\n+    val uri: URI = path.toUri\n+    val key: String = keyFromPath(path)\n+    val request: ListObjectsRequest = new ListObjectsRequest()\n+      .withBucketName(uri.getAuthority)\n+      .withPrefix(key)\n+\n+    val listings = new AbstractSequentialIterator[ObjectListing](s3.listObjects(request)) {\n+      protected def computeNext(previous: ObjectListing): ObjectListing = {\n+        if (!previous.isTruncated) {\n+          return null\n+        }\n+        s3.listNextBatchOfObjects(previous)\n+      }\n+    }.asScala\n+\n+    val bucket: String = uri.getScheme + \"://\" + uri.getAuthority\n+    listings\n+      .map(listing => statusFromObjects(bucket, listing.getObjectSummaries))\n+      .reduceLeft(_ ++ _)\n+  }\n+\n+  /**\n+   * For the given list of paths, sequentially list S3 objects per path and combine returned\n+   * FileStatuses into a single array.\n+   */\n+  private def listStatus(s3: AmazonS3Client, paths: List[Path]): Array[FileStatus] = {\n+    val list: ArrayBuffer[FileStatus] = ArrayBuffer()\n+    paths.foreach { path =>\n+      val iterator = listPrefix(s3, path)\n+      while (iterator.hasNext) {\n+        list += iterator.next\n+      }\n+    }\n+    list.toArray\n+  }\n+\n+  /**\n+   * Find S3 block size from Hadoop conf. Try both s3 and s3n names.\n+   */\n+  private def getS3BlockSize(): Long = {\n+    val value = Option(conf.get(\"fs.s3.block.size\"))\n+      .getOrElse(conf.get(\"fs.s3n.block.size\", \"67108864\"))\n+    value.toLong"
  }, {
    "author": {
      "login": "piaozhexiu"
    },
    "body": "@steveloughran Thanks for the comment. I agree. What size of a min blocksize do you think is reasonable for such a check? 10mb?\n",
    "commit": "5bdd92443c33fc2ec72fabb141c878cf3488d1f4",
    "createdAt": "2015-12-08T18:26:33Z",
    "diffHunk": "@@ -0,0 +1,336 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.net.URI\n+import java.util\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.amazonaws.{AmazonClientException, AmazonServiceException, ClientConfiguration, Protocol}\n+import com.amazonaws.auth.{AWSCredentialsProvider, BasicAWSCredentials, InstanceProfileCredentialsProvider, STSAssumeRoleSessionCredentialsProvider}\n+import com.amazonaws.internal.StaticCredentialsProvider\n+import com.amazonaws.services.s3.AmazonS3Client\n+import com.amazonaws.services.s3.model.{ListObjectsRequest, ObjectListing, S3ObjectSummary}\n+\n+import com.google.common.annotations.VisibleForTesting\n+import com.google.common.base.{Preconditions, Strings}\n+import com.google.common.cache.{Cache, CacheBuilder}\n+import com.google.common.collect.AbstractSequentialIterator\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, GlobPattern, Path, PathFilter}\n+import org.apache.hadoop.fs.s3.S3Credentials\n+import org.apache.hadoop.io.compress.{CompressionCodecFactory, SplittableCompressionCodec}\n+import org.apache.hadoop.mapred.{FileInputFormat, FileSplit, InputSplit, JobConf}\n+\n+import org.apache.spark.{Logging, SparkEnv}\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Contains util methods to interact with S3 from Spark.\n+ */\n+@DeveloperApi\n+object SparkS3Util extends Logging {\n+  val sparkConf = SparkEnv.get.conf\n+  val conf: Configuration = SparkHadoopUtil.get.newConfiguration(sparkConf)\n+\n+  private val s3ClientCache: Cache[String, AmazonS3Client] = CacheBuilder\n+    .newBuilder\n+    .concurrencyLevel(Runtime.getRuntime.availableProcessors)\n+    .build[String, AmazonS3Client]\n+\n+  // Flag to enable S3 bulk listing. It is true by default.\n+  private val S3_BULK_LISTING_ENABLED: String = \"spark.s3.bulk.listing.enabled\"\n+\n+  // Properties for AmazonS3Client. Default values should just work most of time.\n+  private val S3_CONNECT_TIMEOUT: String = \"spark.s3.connect.timeout\"\n+  private val S3_MAX_CONNECTIONS: String = \"spark.s3.max.connections\"\n+  private val S3_MAX_ERROR_RETRIES: String = \"spark.s3.max.error.retries\"\n+  private val S3_SOCKET_TIMEOUT: String = \"spark.s3.socket.timeout\"\n+  private val S3_SSL_ENABLED: String = \"spark.s3.ssl.enabled\"\n+  private val S3_USE_INSTANCE_CREDENTIALS: String = \"spark.s3.use.instance.credentials\"\n+\n+  // Ignore hidden files whose name starts with \"_\" and \".\", or ends with \"$folder$\".\n+  private val hiddenFileFilter = new PathFilter() {\n+    override def accept(p: Path): Boolean = {\n+      val name: String = p.getName()\n+      !name.startsWith(\"_\") && !name.startsWith(\".\") && !name.endsWith(\"$folder$\")\n+    }\n+  }\n+\n+  /**\n+   * Initialize AmazonS3Client per bucket. Since access permissions might be different from bucket\n+   * to bucket, it is necessary to initialize AmazonS3Client on a per bucket basis.\n+   */\n+  private def getS3Client(bucket: String): AmazonS3Client = {\n+    val sslEnabled: Boolean = sparkConf.getBoolean(S3_SSL_ENABLED, true)\n+    val maxErrorRetries: Int = sparkConf.getInt(S3_MAX_ERROR_RETRIES, 10)\n+    val connectTimeout: Int = sparkConf.getInt(S3_CONNECT_TIMEOUT, 5000)\n+    val socketTimeout: Int = sparkConf.getInt(S3_SOCKET_TIMEOUT, 5000)\n+    val maxConnections: Int = sparkConf.getInt(S3_MAX_CONNECTIONS, 5)\n+    val useInstanceCredentials: Boolean = sparkConf.getBoolean(S3_USE_INSTANCE_CREDENTIALS, false)\n+\n+    val clientConf: ClientConfiguration = new ClientConfiguration\n+    clientConf.setMaxErrorRetry(maxErrorRetries)\n+    clientConf.setProtocol(if (sslEnabled) Protocol.HTTPS else Protocol.HTTP)\n+    clientConf.setConnectionTimeout(connectTimeout)\n+    clientConf.setSocketTimeout(socketTimeout)\n+    clientConf.setMaxConnections(maxConnections)\n+\n+    // There are different ways of obtaining S3 bucket access. Try them in the following order:\n+    //   1) Check if user specified an IAM role. If so, use it.\n+    //   2) Check if instance is associated with an IAM role. If so, use it.\n+    //   3) Check if default IAM role is set in Hadoop conf. If so, use it.\n+    //   4) If no IAM role is found, search for an AWS key pair.\n+    // If no credentials are found, throw an exception.\n+    val s3RoleArn = Option(conf.get(\"aws.iam.role.arn\"))\n+    val s3RoleArnDefault = Option(conf.get(\"aws.iam.role.arn.default\"))\n+    val credentialsProvider: AWSCredentialsProvider =\n+      s3RoleArn match {\n+        case Some(role) =>\n+          logDebug(\"Use user-specified IAM role: \" + role)\n+          new STSAssumeRoleSessionCredentialsProvider(\n+            role, \"RoleSessionName-\" + Utils.random.nextInt)\n+        case None if useInstanceCredentials =>\n+          logDebug(\"Use IAM role associated with the instance\")\n+          new InstanceProfileCredentialsProvider\n+        case _ =>\n+          s3RoleArnDefault match {\n+            case Some(role) =>\n+              logDebug(\"Use default IAM role configured in Hadoop config: \" + role)\n+              new STSAssumeRoleSessionCredentialsProvider(\n+                role, \"RoleSessionName-\" + Utils.random.nextInt)\n+            case _ =>\n+              try {\n+                logDebug(\"Use AWS key pair\")\n+                val credentials: S3Credentials = new S3Credentials\n+                credentials.initialize(URI.create(bucket), conf)\n+                new StaticCredentialsProvider(\n+                  new BasicAWSCredentials(credentials.getAccessKey, credentials.getSecretAccessKey))\n+              } catch {\n+                case e: Exception => throw new RuntimeException(\"S3 credentials not configured\", e)\n+              }\n+        }\n+      }\n+\n+    val providerName: String = credentialsProvider.getClass.getSimpleName\n+    val s3ClientKey: String =\n+      if (s3RoleArn == null) providerName\n+      else providerName + \"_\" + s3RoleArn\n+\n+    Option(s3ClientCache.getIfPresent(s3ClientKey)).getOrElse {\n+      val newClient = new AmazonS3Client(credentialsProvider, clientConf)\n+      s3ClientCache.put(s3ClientKey, newClient)\n+      newClient\n+    }\n+  }\n+\n+  /**\n+   * Helper function to extract S3 object key name from the given path.\n+   */\n+  private def keyFromPath(path: Path): String = {\n+    Preconditions.checkArgument(path.isAbsolute, \"Path is not absolute: %s\", path)\n+    var key: String = Strings.nullToEmpty(path.toUri.getPath)\n+    if (key.startsWith(\"/\")) {\n+      key = key.substring(1)\n+    }\n+    if (key.endsWith(\"/\")) {\n+      key = key.substring(0, key.length - 1)\n+    }\n+    key\n+  }\n+\n+  /**\n+   * Helper function to convert S3ObjectSummary into FileStatus.\n+   */\n+  private def statusFromObjects(\n+      bucket: String,\n+      objects: util.List[S3ObjectSummary]): Iterator[FileStatus] = {\n+    val blockSize: Long = getS3BlockSize()\n+    val list: ArrayBuffer[FileStatus] = ArrayBuffer()\n+    for (obj: S3ObjectSummary <- objects.asScala) {\n+      if (!obj.getKey.endsWith(\"/\")) {\n+        val path = new Path(bucket + \"/\" + obj.getKey)\n+        if (hiddenFileFilter.accept(path)) {\n+          val status: FileStatus = new FileStatus(\n+            obj.getSize,\n+            false,\n+            1,\n+            blockSize,\n+            obj.getLastModified.getTime,\n+            path)\n+          list += status\n+        }\n+      }\n+    }\n+    list.iterator\n+  }\n+\n+  /**\n+   * For the given path, list S3 objects and return an iterator of returned FileStatuses.\n+   */\n+  @throws(classOf[AmazonClientException])\n+  @throws(classOf[AmazonServiceException])\n+  private def listPrefix(s3: AmazonS3Client, path: Path): Iterator[FileStatus] = {\n+    val uri: URI = path.toUri\n+    val key: String = keyFromPath(path)\n+    val request: ListObjectsRequest = new ListObjectsRequest()\n+      .withBucketName(uri.getAuthority)\n+      .withPrefix(key)\n+\n+    val listings = new AbstractSequentialIterator[ObjectListing](s3.listObjects(request)) {\n+      protected def computeNext(previous: ObjectListing): ObjectListing = {\n+        if (!previous.isTruncated) {\n+          return null\n+        }\n+        s3.listNextBatchOfObjects(previous)\n+      }\n+    }.asScala\n+\n+    val bucket: String = uri.getScheme + \"://\" + uri.getAuthority\n+    listings\n+      .map(listing => statusFromObjects(bucket, listing.getObjectSummaries))\n+      .reduceLeft(_ ++ _)\n+  }\n+\n+  /**\n+   * For the given list of paths, sequentially list S3 objects per path and combine returned\n+   * FileStatuses into a single array.\n+   */\n+  private def listStatus(s3: AmazonS3Client, paths: List[Path]): Array[FileStatus] = {\n+    val list: ArrayBuffer[FileStatus] = ArrayBuffer()\n+    paths.foreach { path =>\n+      val iterator = listPrefix(s3, path)\n+      while (iterator.hasNext) {\n+        list += iterator.next\n+      }\n+    }\n+    list.toArray\n+  }\n+\n+  /**\n+   * Find S3 block size from Hadoop conf. Try both s3 and s3n names.\n+   */\n+  private def getS3BlockSize(): Long = {\n+    val value = Option(conf.get(\"fs.s3.block.size\"))\n+      .getOrElse(conf.get(\"fs.s3n.block.size\", \"67108864\"))\n+    value.toLong"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "probably. The only time someone would intentionally use a small blocksize is in testing things.\n",
    "commit": "5bdd92443c33fc2ec72fabb141c878cf3488d1f4",
    "createdAt": "2015-12-08T18:27:40Z",
    "diffHunk": "@@ -0,0 +1,336 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.net.URI\n+import java.util\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.amazonaws.{AmazonClientException, AmazonServiceException, ClientConfiguration, Protocol}\n+import com.amazonaws.auth.{AWSCredentialsProvider, BasicAWSCredentials, InstanceProfileCredentialsProvider, STSAssumeRoleSessionCredentialsProvider}\n+import com.amazonaws.internal.StaticCredentialsProvider\n+import com.amazonaws.services.s3.AmazonS3Client\n+import com.amazonaws.services.s3.model.{ListObjectsRequest, ObjectListing, S3ObjectSummary}\n+\n+import com.google.common.annotations.VisibleForTesting\n+import com.google.common.base.{Preconditions, Strings}\n+import com.google.common.cache.{Cache, CacheBuilder}\n+import com.google.common.collect.AbstractSequentialIterator\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, GlobPattern, Path, PathFilter}\n+import org.apache.hadoop.fs.s3.S3Credentials\n+import org.apache.hadoop.io.compress.{CompressionCodecFactory, SplittableCompressionCodec}\n+import org.apache.hadoop.mapred.{FileInputFormat, FileSplit, InputSplit, JobConf}\n+\n+import org.apache.spark.{Logging, SparkEnv}\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Contains util methods to interact with S3 from Spark.\n+ */\n+@DeveloperApi\n+object SparkS3Util extends Logging {\n+  val sparkConf = SparkEnv.get.conf\n+  val conf: Configuration = SparkHadoopUtil.get.newConfiguration(sparkConf)\n+\n+  private val s3ClientCache: Cache[String, AmazonS3Client] = CacheBuilder\n+    .newBuilder\n+    .concurrencyLevel(Runtime.getRuntime.availableProcessors)\n+    .build[String, AmazonS3Client]\n+\n+  // Flag to enable S3 bulk listing. It is true by default.\n+  private val S3_BULK_LISTING_ENABLED: String = \"spark.s3.bulk.listing.enabled\"\n+\n+  // Properties for AmazonS3Client. Default values should just work most of time.\n+  private val S3_CONNECT_TIMEOUT: String = \"spark.s3.connect.timeout\"\n+  private val S3_MAX_CONNECTIONS: String = \"spark.s3.max.connections\"\n+  private val S3_MAX_ERROR_RETRIES: String = \"spark.s3.max.error.retries\"\n+  private val S3_SOCKET_TIMEOUT: String = \"spark.s3.socket.timeout\"\n+  private val S3_SSL_ENABLED: String = \"spark.s3.ssl.enabled\"\n+  private val S3_USE_INSTANCE_CREDENTIALS: String = \"spark.s3.use.instance.credentials\"\n+\n+  // Ignore hidden files whose name starts with \"_\" and \".\", or ends with \"$folder$\".\n+  private val hiddenFileFilter = new PathFilter() {\n+    override def accept(p: Path): Boolean = {\n+      val name: String = p.getName()\n+      !name.startsWith(\"_\") && !name.startsWith(\".\") && !name.endsWith(\"$folder$\")\n+    }\n+  }\n+\n+  /**\n+   * Initialize AmazonS3Client per bucket. Since access permissions might be different from bucket\n+   * to bucket, it is necessary to initialize AmazonS3Client on a per bucket basis.\n+   */\n+  private def getS3Client(bucket: String): AmazonS3Client = {\n+    val sslEnabled: Boolean = sparkConf.getBoolean(S3_SSL_ENABLED, true)\n+    val maxErrorRetries: Int = sparkConf.getInt(S3_MAX_ERROR_RETRIES, 10)\n+    val connectTimeout: Int = sparkConf.getInt(S3_CONNECT_TIMEOUT, 5000)\n+    val socketTimeout: Int = sparkConf.getInt(S3_SOCKET_TIMEOUT, 5000)\n+    val maxConnections: Int = sparkConf.getInt(S3_MAX_CONNECTIONS, 5)\n+    val useInstanceCredentials: Boolean = sparkConf.getBoolean(S3_USE_INSTANCE_CREDENTIALS, false)\n+\n+    val clientConf: ClientConfiguration = new ClientConfiguration\n+    clientConf.setMaxErrorRetry(maxErrorRetries)\n+    clientConf.setProtocol(if (sslEnabled) Protocol.HTTPS else Protocol.HTTP)\n+    clientConf.setConnectionTimeout(connectTimeout)\n+    clientConf.setSocketTimeout(socketTimeout)\n+    clientConf.setMaxConnections(maxConnections)\n+\n+    // There are different ways of obtaining S3 bucket access. Try them in the following order:\n+    //   1) Check if user specified an IAM role. If so, use it.\n+    //   2) Check if instance is associated with an IAM role. If so, use it.\n+    //   3) Check if default IAM role is set in Hadoop conf. If so, use it.\n+    //   4) If no IAM role is found, search for an AWS key pair.\n+    // If no credentials are found, throw an exception.\n+    val s3RoleArn = Option(conf.get(\"aws.iam.role.arn\"))\n+    val s3RoleArnDefault = Option(conf.get(\"aws.iam.role.arn.default\"))\n+    val credentialsProvider: AWSCredentialsProvider =\n+      s3RoleArn match {\n+        case Some(role) =>\n+          logDebug(\"Use user-specified IAM role: \" + role)\n+          new STSAssumeRoleSessionCredentialsProvider(\n+            role, \"RoleSessionName-\" + Utils.random.nextInt)\n+        case None if useInstanceCredentials =>\n+          logDebug(\"Use IAM role associated with the instance\")\n+          new InstanceProfileCredentialsProvider\n+        case _ =>\n+          s3RoleArnDefault match {\n+            case Some(role) =>\n+              logDebug(\"Use default IAM role configured in Hadoop config: \" + role)\n+              new STSAssumeRoleSessionCredentialsProvider(\n+                role, \"RoleSessionName-\" + Utils.random.nextInt)\n+            case _ =>\n+              try {\n+                logDebug(\"Use AWS key pair\")\n+                val credentials: S3Credentials = new S3Credentials\n+                credentials.initialize(URI.create(bucket), conf)\n+                new StaticCredentialsProvider(\n+                  new BasicAWSCredentials(credentials.getAccessKey, credentials.getSecretAccessKey))\n+              } catch {\n+                case e: Exception => throw new RuntimeException(\"S3 credentials not configured\", e)\n+              }\n+        }\n+      }\n+\n+    val providerName: String = credentialsProvider.getClass.getSimpleName\n+    val s3ClientKey: String =\n+      if (s3RoleArn == null) providerName\n+      else providerName + \"_\" + s3RoleArn\n+\n+    Option(s3ClientCache.getIfPresent(s3ClientKey)).getOrElse {\n+      val newClient = new AmazonS3Client(credentialsProvider, clientConf)\n+      s3ClientCache.put(s3ClientKey, newClient)\n+      newClient\n+    }\n+  }\n+\n+  /**\n+   * Helper function to extract S3 object key name from the given path.\n+   */\n+  private def keyFromPath(path: Path): String = {\n+    Preconditions.checkArgument(path.isAbsolute, \"Path is not absolute: %s\", path)\n+    var key: String = Strings.nullToEmpty(path.toUri.getPath)\n+    if (key.startsWith(\"/\")) {\n+      key = key.substring(1)\n+    }\n+    if (key.endsWith(\"/\")) {\n+      key = key.substring(0, key.length - 1)\n+    }\n+    key\n+  }\n+\n+  /**\n+   * Helper function to convert S3ObjectSummary into FileStatus.\n+   */\n+  private def statusFromObjects(\n+      bucket: String,\n+      objects: util.List[S3ObjectSummary]): Iterator[FileStatus] = {\n+    val blockSize: Long = getS3BlockSize()\n+    val list: ArrayBuffer[FileStatus] = ArrayBuffer()\n+    for (obj: S3ObjectSummary <- objects.asScala) {\n+      if (!obj.getKey.endsWith(\"/\")) {\n+        val path = new Path(bucket + \"/\" + obj.getKey)\n+        if (hiddenFileFilter.accept(path)) {\n+          val status: FileStatus = new FileStatus(\n+            obj.getSize,\n+            false,\n+            1,\n+            blockSize,\n+            obj.getLastModified.getTime,\n+            path)\n+          list += status\n+        }\n+      }\n+    }\n+    list.iterator\n+  }\n+\n+  /**\n+   * For the given path, list S3 objects and return an iterator of returned FileStatuses.\n+   */\n+  @throws(classOf[AmazonClientException])\n+  @throws(classOf[AmazonServiceException])\n+  private def listPrefix(s3: AmazonS3Client, path: Path): Iterator[FileStatus] = {\n+    val uri: URI = path.toUri\n+    val key: String = keyFromPath(path)\n+    val request: ListObjectsRequest = new ListObjectsRequest()\n+      .withBucketName(uri.getAuthority)\n+      .withPrefix(key)\n+\n+    val listings = new AbstractSequentialIterator[ObjectListing](s3.listObjects(request)) {\n+      protected def computeNext(previous: ObjectListing): ObjectListing = {\n+        if (!previous.isTruncated) {\n+          return null\n+        }\n+        s3.listNextBatchOfObjects(previous)\n+      }\n+    }.asScala\n+\n+    val bucket: String = uri.getScheme + \"://\" + uri.getAuthority\n+    listings\n+      .map(listing => statusFromObjects(bucket, listing.getObjectSummaries))\n+      .reduceLeft(_ ++ _)\n+  }\n+\n+  /**\n+   * For the given list of paths, sequentially list S3 objects per path and combine returned\n+   * FileStatuses into a single array.\n+   */\n+  private def listStatus(s3: AmazonS3Client, paths: List[Path]): Array[FileStatus] = {\n+    val list: ArrayBuffer[FileStatus] = ArrayBuffer()\n+    paths.foreach { path =>\n+      val iterator = listPrefix(s3, path)\n+      while (iterator.hasNext) {\n+        list += iterator.next\n+      }\n+    }\n+    list.toArray\n+  }\n+\n+  /**\n+   * Find S3 block size from Hadoop conf. Try both s3 and s3n names.\n+   */\n+  private def getS3BlockSize(): Long = {\n+    val value = Option(conf.get(\"fs.s3.block.size\"))\n+      .getOrElse(conf.get(\"fs.s3n.block.size\", \"67108864\"))\n+    value.toLong"
  }],
  "prId": 8512
}, {
  "comments": [{
    "author": {
      "login": "steveloughran"
    },
    "body": "I think you should avoid getting involved with `s3a://`. \n\nIf this code gets involved with s3a, then you may confuse some of the stuff there, especially HADOOP-11918 (listing empty dir -> FNFE) and in HADOOP-12537, its sts support (though that's not shipped yet). S3a uses the same bulk listing API and should therefore be something you can just leave alone.\n",
    "commit": "5bdd92443c33fc2ec72fabb141c878cf3488d1f4",
    "createdAt": "2015-12-09T12:00:22Z",
    "diffHunk": "@@ -0,0 +1,342 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.net.URI\n+import java.util\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.amazonaws.{AmazonClientException, AmazonServiceException, ClientConfiguration, Protocol}\n+import com.amazonaws.auth.{AWSCredentialsProvider, BasicAWSCredentials, InstanceProfileCredentialsProvider, STSAssumeRoleSessionCredentialsProvider}\n+import com.amazonaws.internal.StaticCredentialsProvider\n+import com.amazonaws.services.s3.AmazonS3Client\n+import com.amazonaws.services.s3.model.{ListObjectsRequest, ObjectListing, S3ObjectSummary}\n+\n+import com.google.common.base.{Preconditions, Strings}\n+import com.google.common.cache.{Cache, CacheBuilder}\n+import com.google.common.collect.AbstractSequentialIterator\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, GlobPattern, Path, PathFilter}\n+import org.apache.hadoop.fs.s3.S3Credentials\n+import org.apache.hadoop.io.compress.{CompressionCodecFactory, SplittableCompressionCodec}\n+import org.apache.hadoop.mapred.{FileInputFormat, FileSplit, InputSplit, JobConf}\n+\n+import org.apache.spark.{Logging, SparkEnv}\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Contains util methods to interact with S3 from Spark.\n+ */\n+@DeveloperApi\n+object SparkS3Util extends Logging {\n+  val sparkConf = SparkEnv.get.conf\n+  val conf: Configuration = SparkHadoopUtil.get.newConfiguration(sparkConf)\n+\n+  private val s3ClientCache: Cache[String, AmazonS3Client] = CacheBuilder\n+    .newBuilder\n+    .concurrencyLevel(Runtime.getRuntime.availableProcessors)\n+    .build[String, AmazonS3Client]\n+\n+  // Flag to enable S3 bulk listing. It is true by default.\n+  private val S3_BULK_LISTING_ENABLED: String = \"spark.s3.bulk.listing.enabled\"\n+\n+  // Properties for AmazonS3Client. Default values should just work most of time.\n+  private val S3_CONNECT_TIMEOUT: String = \"spark.s3.connect.timeout\"\n+  private val S3_MAX_CONNECTIONS: String = \"spark.s3.max.connections\"\n+  private val S3_MAX_ERROR_RETRIES: String = \"spark.s3.max.error.retries\"\n+  private val S3_SOCKET_TIMEOUT: String = \"spark.s3.socket.timeout\"\n+  private val S3_SSL_ENABLED: String = \"spark.s3.ssl.enabled\"\n+  private val S3_USE_INSTANCE_CREDENTIALS: String = \"spark.s3.use.instance.credentials\"\n+\n+  // Ignore hidden files whose name starts with \"_\" and \".\", or ends with \"$folder$\".\n+  private val hiddenFileFilter = new PathFilter() {\n+    override def accept(p: Path): Boolean = {\n+      val name: String = p.getName()\n+      !name.startsWith(\"_\") && !name.startsWith(\".\") && !name.endsWith(\"$folder$\")\n+    }\n+  }\n+\n+  /**\n+   * Initialize AmazonS3Client per bucket. Since access permissions might be different from bucket\n+   * to bucket, it is necessary to initialize AmazonS3Client on a per bucket basis.\n+   */\n+  private def getS3Client(bucket: String): AmazonS3Client = {\n+    val sslEnabled: Boolean = sparkConf.getBoolean(S3_SSL_ENABLED, true)\n+    val maxErrorRetries: Int = sparkConf.getInt(S3_MAX_ERROR_RETRIES, 10)\n+    val connectTimeout: Int = sparkConf.getInt(S3_CONNECT_TIMEOUT, 5000)\n+    val socketTimeout: Int = sparkConf.getInt(S3_SOCKET_TIMEOUT, 5000)\n+    val maxConnections: Int = sparkConf.getInt(S3_MAX_CONNECTIONS, 5)\n+    val useInstanceCredentials: Boolean = sparkConf.getBoolean(S3_USE_INSTANCE_CREDENTIALS, false)\n+\n+    val clientConf: ClientConfiguration = new ClientConfiguration\n+    clientConf.setMaxErrorRetry(maxErrorRetries)\n+    clientConf.setProtocol(if (sslEnabled) Protocol.HTTPS else Protocol.HTTP)\n+    clientConf.setConnectionTimeout(connectTimeout)\n+    clientConf.setSocketTimeout(socketTimeout)\n+    clientConf.setMaxConnections(maxConnections)\n+\n+    // There are different ways of obtaining S3 bucket access. Try them in the following order:\n+    //   1) Check if user specified an IAM role. If so, use it.\n+    //   2) Check if instance is associated with an IAM role. If so, use it.\n+    //   3) Check if default IAM role is set in Hadoop conf. If so, use it.\n+    //   4) If no IAM role is found, search for an AWS key pair.\n+    // If no credentials are found, throw an exception.\n+    val s3RoleArn = Option(conf.get(\"aws.iam.role.arn\"))\n+    val s3RoleArnDefault = Option(conf.get(\"aws.iam.role.arn.default\"))\n+    val credentialsProvider: AWSCredentialsProvider =\n+      s3RoleArn match {\n+        case Some(role) =>\n+          logDebug(\"Use user-specified IAM role: \" + role)\n+          new STSAssumeRoleSessionCredentialsProvider(\n+            role, \"RoleSessionName-\" + Utils.random.nextInt)\n+        case None if useInstanceCredentials =>\n+          logDebug(\"Use IAM role associated with the instance\")\n+          new InstanceProfileCredentialsProvider\n+        case _ =>\n+          s3RoleArnDefault match {\n+            case Some(role) =>\n+              logDebug(\"Use default IAM role configured in Hadoop config: \" + role)\n+              new STSAssumeRoleSessionCredentialsProvider(\n+                role, \"RoleSessionName-\" + Utils.random.nextInt)\n+            case _ =>\n+              try {\n+                logDebug(\"Use AWS key pair\")\n+                val credentials: S3Credentials = new S3Credentials\n+                credentials.initialize(URI.create(bucket), conf)\n+                new StaticCredentialsProvider(\n+                  new BasicAWSCredentials(credentials.getAccessKey, credentials.getSecretAccessKey))\n+              } catch {\n+                case e: Exception => throw new RuntimeException(\"S3 credentials not configured\", e)\n+              }\n+        }\n+      }\n+\n+    val providerName: String = credentialsProvider.getClass.getSimpleName\n+    val s3ClientKey: String =\n+      if (s3RoleArn == null) providerName\n+      else providerName + \"_\" + s3RoleArn\n+\n+    Option(s3ClientCache.getIfPresent(s3ClientKey)).getOrElse {\n+      val newClient = new AmazonS3Client(credentialsProvider, clientConf)\n+      s3ClientCache.put(s3ClientKey, newClient)\n+      newClient\n+    }\n+  }\n+\n+  /**\n+   * Helper function to extract S3 object key name from the given path.\n+   */\n+  private def keyFromPath(path: Path): String = {\n+    Preconditions.checkArgument(path.isAbsolute, \"Path is not absolute: %s\", path)\n+    var key: String = Strings.nullToEmpty(path.toUri.getPath)\n+    if (key.startsWith(\"/\")) {\n+      key = key.substring(1)\n+    }\n+    if (key.endsWith(\"/\")) {\n+      key = key.substring(0, key.length - 1)\n+    }\n+    key\n+  }\n+\n+  /**\n+   * Helper function to convert S3ObjectSummary into FileStatus.\n+   */\n+  private def statusFromObjects(\n+      bucket: String,\n+      objects: util.List[S3ObjectSummary]): Iterator[FileStatus] = {\n+    val blockSize: Long = getS3BlockSize()\n+    val list: ArrayBuffer[FileStatus] = ArrayBuffer()\n+    for (obj: S3ObjectSummary <- objects.asScala) {\n+      if (!obj.getKey.endsWith(\"/\")) {\n+        val path = new Path(bucket + \"/\" + obj.getKey)\n+        if (hiddenFileFilter.accept(path)) {\n+          val status: FileStatus = new FileStatus(\n+            obj.getSize,\n+            false,\n+            1,\n+            blockSize,\n+            obj.getLastModified.getTime,\n+            path)\n+          list += status\n+        }\n+      }\n+    }\n+    list.iterator\n+  }\n+\n+  /**\n+   * For the given path, list S3 objects and return an iterator of returned FileStatuses.\n+   */\n+  @throws(classOf[AmazonClientException])\n+  @throws(classOf[AmazonServiceException])\n+  private def listPrefix(s3: AmazonS3Client, path: Path): Iterator[FileStatus] = {\n+    val uri: URI = path.toUri\n+    val key: String = keyFromPath(path)\n+    val request: ListObjectsRequest = new ListObjectsRequest()\n+      .withBucketName(uri.getAuthority)\n+      .withPrefix(key)\n+\n+    val listings = new AbstractSequentialIterator[ObjectListing](s3.listObjects(request)) {\n+      protected def computeNext(previous: ObjectListing): ObjectListing = {\n+        if (!previous.isTruncated) {\n+          return null\n+        }\n+        s3.listNextBatchOfObjects(previous)\n+      }\n+    }.asScala\n+\n+    val bucket: String = uri.getScheme + \"://\" + uri.getAuthority\n+    listings\n+      .map(listing => statusFromObjects(bucket, listing.getObjectSummaries))\n+      .reduceLeft(_ ++ _)\n+  }\n+\n+  /**\n+   * For the given list of paths, sequentially list S3 objects per path and combine returned\n+   * FileStatuses into a single array.\n+   */\n+  private def listStatus(s3: AmazonS3Client, paths: List[Path]): Array[FileStatus] = {\n+    val list: ArrayBuffer[FileStatus] = ArrayBuffer()\n+    paths.foreach { path =>\n+      val iterator = listPrefix(s3, path)\n+      while (iterator.hasNext) {\n+        list += iterator.next\n+      }\n+    }\n+    list.toArray\n+  }\n+\n+  /**\n+   * Find S3 block size from Hadoop conf. Try both s3 and s3n names.\n+   */\n+  private def getS3BlockSize(): Long = {\n+    val minS3BlockSize = 10485760; // 10mb\n+    val defaultS3BlockSize = 67108864; // 64mb\n+    val value = Option(conf.get(\"fs.s3.block.size\"))\n+      .getOrElse(conf.get(\"fs.s3n.block.size\", defaultS3BlockSize.toString)).toLong\n+    if (value < minS3BlockSize) {\n+      logWarning(\"S3 block size is set too small: \" + value + \". Overriding it to 10mb.\");\n+      minS3BlockSize\n+    } else {\n+      value\n+    }\n+  }\n+\n+  /**\n+   * Find min split size from Hadoop conf. Try both Hadoop 1 and 2 names.\n+   */\n+  private def getMinSplitSize(): Long = {\n+    val value = Option(conf.get(\"mapred.min.split.size\"))\n+      .getOrElse(conf.get(\"mapreduce.input.fileinputformat.split.minsize\", \"134217728\"))\n+    value.toLong\n+  }\n+\n+  /**\n+   * Return whether the given path is an S3 path or not.\n+   */\n+  private def isS3Path(path: Path): Boolean = {\n+    Option(path.toUri.getScheme).exists(_.toUpperCase.startsWith(\"S3\"))",
    "line": 258
  }],
  "prId": 8512
}]