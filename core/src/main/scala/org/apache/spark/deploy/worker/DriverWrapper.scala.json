[{
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "I think it is reasonable to use `Utils.localHostName()`. But since `spark.driver.port` is used below, what is `spark.driver.host` for? Should we just pick `spark.driver.host` to use?",
    "commit": "b188cc9a9e290683210d3c4a6841d37ca00b112f",
    "createdAt": "2017-08-06T08:26:48Z",
    "diffHunk": "@@ -38,8 +39,10 @@ object DriverWrapper {\n        */\n       case workerUrl :: userJar :: mainClass :: extraArgs =>\n         val conf = new SparkConf()\n-        val rpcEnv = RpcEnv.create(\"Driver\",\n-          Utils.localHostName(), 0, conf, new SecurityManager(conf))\n+        val host: String = Utils.localHostName()",
    "line": 23
  }, {
    "author": {
      "login": "ScrapCodes"
    },
    "body": "Please correct me, AFAIU `spark.driver.host` may not be specified on each node of the cluster and its value might be global(i.e. cluster wide). Since driver is launched on a random node during a failover, it's value can not be pre-assigned and has to be picked up from that node's local environment.\r\n\r\nAbout `spark.driver.port`, I am not sure, but the fact is - it might work, even if it is global cluster wide constant.",
    "commit": "b188cc9a9e290683210d3c4a6841d37ca00b112f",
    "createdAt": "2017-08-24T11:02:13Z",
    "diffHunk": "@@ -38,8 +39,10 @@ object DriverWrapper {\n        */\n       case workerUrl :: userJar :: mainClass :: extraArgs =>\n         val conf = new SparkConf()\n-        val rpcEnv = RpcEnv.create(\"Driver\",\n-          Utils.localHostName(), 0, conf, new SecurityManager(conf))\n+        val host: String = Utils.localHostName()",
    "line": 23
  }],
  "prId": 17357
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "This `DriverWrapper` actually runs within the same JVM of driver, and initialize log4j instance earlier. Will this be a problem?",
    "commit": "b188cc9a9e290683210d3c4a6841d37ca00b112f",
    "createdAt": "2017-10-06T01:54:44Z",
    "diffHunk": "@@ -23,14 +23,15 @@ import org.apache.commons.lang3.StringUtils\n \n import org.apache.spark.{SecurityManager, SparkConf}\n import org.apache.spark.deploy.{DependencyUtils, SparkHadoopUtil, SparkSubmit}\n+import org.apache.spark.internal.Logging\n import org.apache.spark.rpc.RpcEnv\n import org.apache.spark.util.{ChildFirstURLClassLoader, MutableURLClassLoader, Utils}\n \n /**\n  * Utility object for launching driver programs such that they share fate with the Worker process.\n  * This is used in standalone cluster mode only.\n  */\n-object DriverWrapper {\n+object DriverWrapper extends Logging {",
    "line": 13
  }, {
    "author": {
      "login": "ScrapCodes"
    },
    "body": "I can not conceive, why that can be a problem. Can you also describe, why do you think, it can be a problem?",
    "commit": "b188cc9a9e290683210d3c4a6841d37ca00b112f",
    "createdAt": "2017-10-09T08:30:28Z",
    "diffHunk": "@@ -23,14 +23,15 @@ import org.apache.commons.lang3.StringUtils\n \n import org.apache.spark.{SecurityManager, SparkConf}\n import org.apache.spark.deploy.{DependencyUtils, SparkHadoopUtil, SparkSubmit}\n+import org.apache.spark.internal.Logging\n import org.apache.spark.rpc.RpcEnv\n import org.apache.spark.util.{ChildFirstURLClassLoader, MutableURLClassLoader, Utils}\n \n /**\n  * Utility object for launching driver programs such that they share fate with the Worker process.\n  * This is used in standalone cluster mode only.\n  */\n-object DriverWrapper {\n+object DriverWrapper extends Logging {",
    "line": 13
  }],
  "prId": 17357
}]