[{
  "comments": [{
    "author": {
      "login": "ajbozarth"
    },
    "body": "Why this change from cacheSize to maxMemory, are they synonymous?",
    "commit": "81b3f3dd223785e02ad2df2a6e40046a76539681",
    "createdAt": "2017-11-15T23:19:59Z",
    "diffHunk": "@@ -89,39 +83,33 @@ class SparkStatusTracker private[spark] (sc: SparkContext) {\n    * garbage collected.\n    */\n   def getStageInfo(stageId: Int): Option[SparkStageInfo] = {\n-    jobProgressListener.synchronized {\n-      for (\n-        info <- jobProgressListener.stageIdToInfo.get(stageId);\n-        data <- jobProgressListener.stageIdToData.get((stageId, info.attemptId))\n-      ) yield {\n-        new SparkStageInfoImpl(\n-          stageId,\n-          info.attemptId,\n-          info.submissionTime.getOrElse(0),\n-          info.name,\n-          info.numTasks,\n-          data.numActiveTasks,\n-          data.numCompleteTasks,\n-          data.numFailedTasks)\n-      }\n+    store.asOption(store.lastStageAttempt(stageId)).map { stage =>\n+      new SparkStageInfoImpl(\n+        stageId,\n+        stage.attemptId,\n+        stage.submissionTime.map(_.getTime()).getOrElse(0L),\n+        stage.name,\n+        stage.numTasks,\n+        stage.numActiveTasks,\n+        stage.numCompleteTasks,\n+        stage.numFailedTasks)\n     }\n   }\n \n   /**\n    * Returns information of all known executors, including host, port, cacheSize, numRunningTasks.\n    */\n   def getExecutorInfos: Array[SparkExecutorInfo] = {\n-    val executorIdToRunningTasks: Map[String, Int] =\n-      sc.taskScheduler.asInstanceOf[TaskSchedulerImpl].runningTasksByExecutors\n-\n-    sc.getExecutorStorageStatus.map { status =>\n-      val bmId = status.blockManagerId\n+    store.executorList(true).map { exec =>\n+      val (host, port) = exec.hostPort.split(\":\", 2) match {\n+        case Array(h, p) => (h, p.toInt)\n+        case Array(h) => (h, -1)\n+      }\n       new SparkExecutorInfoImpl(\n-        bmId.host,\n-        bmId.port,\n-        status.cacheSize,\n-        executorIdToRunningTasks.getOrElse(bmId.executorId, 0)\n-      )\n-    }\n+        host,\n+        port,\n+        exec.maxMemory,"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "```\r\n  /** Return the memory used by caching RDDs */\r\n  def cacheSize: Long = onHeapCacheSize.getOrElse(0L) + offHeapCacheSize.getOrElse(0L)\r\n```\r\n\r\nSo you're right, this isn't correct.",
    "commit": "81b3f3dd223785e02ad2df2a6e40046a76539681",
    "createdAt": "2017-11-15T23:59:45Z",
    "diffHunk": "@@ -89,39 +83,33 @@ class SparkStatusTracker private[spark] (sc: SparkContext) {\n    * garbage collected.\n    */\n   def getStageInfo(stageId: Int): Option[SparkStageInfo] = {\n-    jobProgressListener.synchronized {\n-      for (\n-        info <- jobProgressListener.stageIdToInfo.get(stageId);\n-        data <- jobProgressListener.stageIdToData.get((stageId, info.attemptId))\n-      ) yield {\n-        new SparkStageInfoImpl(\n-          stageId,\n-          info.attemptId,\n-          info.submissionTime.getOrElse(0),\n-          info.name,\n-          info.numTasks,\n-          data.numActiveTasks,\n-          data.numCompleteTasks,\n-          data.numFailedTasks)\n-      }\n+    store.asOption(store.lastStageAttempt(stageId)).map { stage =>\n+      new SparkStageInfoImpl(\n+        stageId,\n+        stage.attemptId,\n+        stage.submissionTime.map(_.getTime()).getOrElse(0L),\n+        stage.name,\n+        stage.numTasks,\n+        stage.numActiveTasks,\n+        stage.numCompleteTasks,\n+        stage.numFailedTasks)\n     }\n   }\n \n   /**\n    * Returns information of all known executors, including host, port, cacheSize, numRunningTasks.\n    */\n   def getExecutorInfos: Array[SparkExecutorInfo] = {\n-    val executorIdToRunningTasks: Map[String, Int] =\n-      sc.taskScheduler.asInstanceOf[TaskSchedulerImpl].runningTasksByExecutors\n-\n-    sc.getExecutorStorageStatus.map { status =>\n-      val bmId = status.blockManagerId\n+    store.executorList(true).map { exec =>\n+      val (host, port) = exec.hostPort.split(\":\", 2) match {\n+        case Array(h, p) => (h, p.toInt)\n+        case Array(h) => (h, -1)\n+      }\n       new SparkExecutorInfoImpl(\n-        bmId.host,\n-        bmId.port,\n-        status.cacheSize,\n-        executorIdToRunningTasks.getOrElse(bmId.executorId, 0)\n-      )\n-    }\n+        host,\n+        port,\n+        exec.maxMemory,"
  }],
  "prId": 19750
}]