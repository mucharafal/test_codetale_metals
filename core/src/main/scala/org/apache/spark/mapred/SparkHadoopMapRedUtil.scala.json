[{
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "This is the key change in this patch; these two lines are technically the minimum diff required to fix this bug. The rest of the changes are renaming / cleanup to make the units a bit clearer.\n",
    "commit": "edbbf6fae97f67c5d9a309019514745cf35a2cbe",
    "createdAt": "2015-09-01T01:45:38Z",
    "diffHunk": "@@ -122,7 +121,8 @@ object SparkHadoopMapRedUtil extends Logging {\n \n       if (shouldCoordinateWithDriver) {\n         val outputCommitCoordinator = SparkEnv.get.outputCommitCoordinator\n-        val canCommit = outputCommitCoordinator.canCommit(jobId, splitId, attemptId)\n+        val taskAttemptNumber = TaskContext.get().attemptNumber()",
    "line": 15
  }],
  "prId": 8544
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "Here, I removed this parameter because we should always get the attempt number from the TaskContext in order to ensure that it's correct. This had a ripple-effect on a few other callsites, which I've updated.\n",
    "commit": "edbbf6fae97f67c5d9a309019514745cf35a2cbe",
    "createdAt": "2015-09-01T01:48:21Z",
    "diffHunk": "@@ -91,8 +91,7 @@ object SparkHadoopMapRedUtil extends Logging {\n       committer: MapReduceOutputCommitter,\n       mrTaskContext: MapReduceTaskAttemptContext,\n       jobId: Int,\n-      splitId: Int,\n-      attemptId: Int): Unit = {",
    "line": 5
  }],
  "prId": 8544
}]