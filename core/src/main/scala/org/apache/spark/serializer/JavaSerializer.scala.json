[{
  "comments": [{
    "author": {
      "login": "mateiz"
    },
    "body": "Should say 10000th\n",
    "commit": "9ef7cb84a060e6618766f47695ae8e41cfb15246",
    "createdAt": "2014-03-03T05:36:59Z",
    "diffHunk": "@@ -23,9 +23,27 @@ import java.nio.ByteBuffer\n import org.apache.spark.SparkConf\n import org.apache.spark.util.ByteBufferInputStream\n \n-private[spark] class JavaSerializationStream(out: OutputStream) extends SerializationStream {\n+private[spark] class JavaSerializationStream(out: OutputStream,\n+                                             conf: SparkConf) extends SerializationStream {\n   val objOut = new ObjectOutputStream(out)\n-  def writeObject[T](t: T): SerializationStream = { objOut.writeObject(t); this }\n+  var counter = 0\n+  val counterReset = conf.getInt(\"spark.serializer.objectStreamReset\", 10000)\n+\n+  /* Calling reset to avoid memory leak:\n+   * http://stackoverflow.com/questions/1281549/memory-leak-traps-in-the-java-standard-api\n+   * But only call it every 1000th time to avoid bloated serialization streams (when"
  }],
  "prId": 50
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "nit: break line on \"extends\" instead\n",
    "commit": "9ef7cb84a060e6618766f47695ae8e41cfb15246",
    "createdAt": "2014-03-04T01:20:30Z",
    "diffHunk": "@@ -23,9 +23,27 @@ import java.nio.ByteBuffer\n import org.apache.spark.SparkConf\n import org.apache.spark.util.ByteBufferInputStream\n \n-private[spark] class JavaSerializationStream(out: OutputStream) extends SerializationStream {\n+private[spark] class JavaSerializationStream(out: OutputStream,\n+                                             conf: SparkConf) extends SerializationStream {"
  }],
  "prId": 50
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "nit: use java doc format `/** comment */` rather than scala doc\n",
    "commit": "9ef7cb84a060e6618766f47695ae8e41cfb15246",
    "createdAt": "2014-03-04T01:21:50Z",
    "diffHunk": "@@ -23,9 +23,27 @@ import java.nio.ByteBuffer\n import org.apache.spark.SparkConf\n import org.apache.spark.util.ByteBufferInputStream\n \n-private[spark] class JavaSerializationStream(out: OutputStream) extends SerializationStream {\n+private[spark] class JavaSerializationStream(out: OutputStream,\n+                                             conf: SparkConf) extends SerializationStream {\n   val objOut = new ObjectOutputStream(out)\n-  def writeObject[T](t: T): SerializationStream = { objOut.writeObject(t); this }\n+  var counter = 0\n+  val counterReset = conf.getInt(\"spark.serializer.objectStreamReset\", 10000)\n+\n+  /* Calling reset to avoid memory leak:"
  }],
  "prId": 50
}]