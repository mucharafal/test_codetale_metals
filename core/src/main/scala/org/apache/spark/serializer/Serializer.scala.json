[{
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "I'm not sure that we want to commit to this as a stable public API, which is why I've chosen to mark this as private and leave comments warning users that this API is private and subject to change.  If someone can think of a better way to restrict use / implementation of this method, I'd be happy to incorporate that change.\n",
    "commit": "50a68ca42058da46caff93ed1af925810555d994",
    "createdAt": "2015-05-05T22:02:48Z",
    "diffHunk": "@@ -63,6 +63,39 @@ abstract class Serializer {\n \n   /** Creates a new [[SerializerInstance]]. */\n   def newInstance(): SerializerInstance\n+\n+  /**\n+   * :: Private ::\n+   * Returns true if this serializer supports relocation of its serialized objects and false\n+   * otherwise. This should return true if and only if reordering the bytes of serialized objects\n+   * in serialization stream output is equivalent to having re-ordered those elements prior to\n+   * serializing them. More specifically, the following should hold if a serializer supports\n+   * relocation:\n+   *\n+   * {{{\n+   * serOut.open()\n+   * position = 0\n+   * serOut.write(obj1)\n+   * serOut.flush()\n+   * position = # of bytes writen to stream so far\n+   * obj1Bytes = output[0:position-1]\n+   * serOut.write(obj2)\n+   * serOut.flush()\n+   * position2 = # of bytes written to stream so far\n+   * obj2Bytes = output[position:position2-1]\n+   * serIn.open([obj2bytes] concatenate [obj1bytes]) should return (obj2, obj1)\n+   * }}}\n+   *\n+   * In general, this property should hold for serializers that are stateless and that do not\n+   * write special metadata at the beginning or end of the serialization stream.\n+   *\n+   * This API is private to Spark; this method should not be overridden in third-party subclasses\n+   * or called in user code and is subject to removal in future Spark releases.\n+   *\n+   * See SPARK-7311 for more details.\n+   */\n+  @Private",
    "line": 44
  }],
  "prId": 5924
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "I'm not set on this name, by the way; happy to change if someone thinks of a less verbose name that's not misleading.\n",
    "commit": "50a68ca42058da46caff93ed1af925810555d994",
    "createdAt": "2015-05-05T22:13:12Z",
    "diffHunk": "@@ -63,6 +63,39 @@ abstract class Serializer {\n \n   /** Creates a new [[SerializerInstance]]. */\n   def newInstance(): SerializerInstance\n+\n+  /**\n+   * :: Private ::\n+   * Returns true if this serializer supports relocation of its serialized objects and false\n+   * otherwise. This should return true if and only if reordering the bytes of serialized objects\n+   * in serialization stream output is equivalent to having re-ordered those elements prior to\n+   * serializing them. More specifically, the following should hold if a serializer supports\n+   * relocation:\n+   *\n+   * {{{\n+   * serOut.open()\n+   * position = 0\n+   * serOut.write(obj1)\n+   * serOut.flush()\n+   * position = # of bytes writen to stream so far\n+   * obj1Bytes = output[0:position-1]\n+   * serOut.write(obj2)\n+   * serOut.flush()\n+   * position2 = # of bytes written to stream so far\n+   * obj2Bytes = output[position:position2-1]\n+   * serIn.open([obj2bytes] concatenate [obj1bytes]) should return (obj2, obj1)\n+   * }}}\n+   *\n+   * In general, this property should hold for serializers that are stateless and that do not\n+   * write special metadata at the beginning or end of the serialization stream.\n+   *\n+   * This API is private to Spark; this method should not be overridden in third-party subclasses\n+   * or called in user code and is subject to removal in future Spark releases.\n+   *\n+   * See SPARK-7311 for more details.\n+   */\n+  @Private\n+  private[spark] def supportsRelocationOfSerializedObjects: Boolean = false",
    "line": 45
  }],
  "prId": 5924
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "nit: alphabetize\n",
    "commit": "50a68ca42058da46caff93ed1af925810555d994",
    "createdAt": "2015-05-05T22:47:46Z",
    "diffHunk": "@@ -23,7 +23,7 @@ import java.nio.ByteBuffer\n import scala.reflect.ClassTag\n \n import org.apache.spark.{SparkConf, SparkEnv}\n-import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.annotation.{Private, Experimental, DeveloperApi}"
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "Argh, I really need to fix my IntelliJ settings.  I switched versions and didn't port all of my import sorting settings over, so stuff like this keeps happening :(\n",
    "commit": "50a68ca42058da46caff93ed1af925810555d994",
    "createdAt": "2015-05-05T22:48:52Z",
    "diffHunk": "@@ -23,7 +23,7 @@ import java.nio.ByteBuffer\n import scala.reflect.ClassTag\n \n import org.apache.spark.{SparkConf, SparkEnv}\n-import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.annotation.{Private, Experimental, DeveloperApi}"
  }],
  "prId": 5924
}]