[{
  "comments": [{
    "author": {
      "login": "jose-torres"
    },
    "body": "If we need a bit to mark whether it's continuous or not, ContinuousShuffleDependency shouldn't be inheriting ShuffleDependency in the first place.",
    "commit": "56442dc1c7450518d9bc84b4bfeddb017daa967b",
    "createdAt": "2018-05-10T15:28:29Z",
    "diffHunk": "@@ -65,15 +65,17 @@ abstract class NarrowDependency[T](_rdd: RDD[T]) extends Dependency[T] {\n  * @param keyOrdering key ordering for RDD's shuffles\n  * @param aggregator map/reduce-side aggregator for RDD's shuffle\n  * @param mapSideCombine whether to perform partial aggregation (also known as map-side combine)\n+ * @param isContinuous mark the dependency is base for continuous processing or not",
    "line": 4
  }, {
    "author": {
      "login": "xuanyuanking"
    },
    "body": "Actually we can implement ContinuousShuffleDependency by inheriting Dependency. In current way we consider ContinuousShuffleDependency as a special kind of ShuffleDependency, also few interface can be changed.",
    "commit": "56442dc1c7450518d9bc84b4bfeddb017daa967b",
    "createdAt": "2018-05-11T12:14:24Z",
    "diffHunk": "@@ -65,15 +65,17 @@ abstract class NarrowDependency[T](_rdd: RDD[T]) extends Dependency[T] {\n  * @param keyOrdering key ordering for RDD's shuffles\n  * @param aggregator map/reduce-side aggregator for RDD's shuffle\n  * @param mapSideCombine whether to perform partial aggregation (also known as map-side combine)\n+ * @param isContinuous mark the dependency is base for continuous processing or not",
    "line": 4
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "Considering ContinuousShuffleDependency to be a special kind of ShuffleDependency doesn't make sense if we need to add this much special casing to do it.",
    "commit": "56442dc1c7450518d9bc84b4bfeddb017daa967b",
    "createdAt": "2018-05-11T14:53:32Z",
    "diffHunk": "@@ -65,15 +65,17 @@ abstract class NarrowDependency[T](_rdd: RDD[T]) extends Dependency[T] {\n  * @param keyOrdering key ordering for RDD's shuffles\n  * @param aggregator map/reduce-side aggregator for RDD's shuffle\n  * @param mapSideCombine whether to perform partial aggregation (also known as map-side combine)\n+ * @param isContinuous mark the dependency is base for continuous processing or not",
    "line": 4
  }, {
    "author": {
      "login": "xuanyuanking"
    },
    "body": "Got it, if we think currently implementation is tricky here, we can change the implementation by getting rid of the ContinuousShuffleDependency, just as you said in jira comments :\"We might not need this to be an actual org.apache.spark.Dependency.\"",
    "commit": "56442dc1c7450518d9bc84b4bfeddb017daa967b",
    "createdAt": "2018-05-15T12:36:39Z",
    "diffHunk": "@@ -65,15 +65,17 @@ abstract class NarrowDependency[T](_rdd: RDD[T]) extends Dependency[T] {\n  * @param keyOrdering key ordering for RDD's shuffles\n  * @param aggregator map/reduce-side aggregator for RDD's shuffle\n  * @param mapSideCombine whether to perform partial aggregation (also known as map-side combine)\n+ * @param isContinuous mark the dependency is base for continuous processing or not",
    "line": 4
  }],
  "prId": 21293
}, {
  "comments": [{
    "author": {
      "login": "jose-torres"
    },
    "body": "We surely can't artificially construct shuffle IDs like this. IIUC only shuffle IDs returned from SparkContext.newShuffleId() are valid shuffle IDs.",
    "commit": "56442dc1c7450518d9bc84b4bfeddb017daa967b",
    "createdAt": "2018-05-10T15:31:03Z",
    "diffHunk": "@@ -88,14 +90,53 @@ class ShuffleDependency[K: ClassTag, V: ClassTag, C: ClassTag](\n   private[spark] val combinerClassName: Option[String] =\n     Option(reflect.classTag[C]).map(_.runtimeClass.getName)\n \n-  val shuffleId: Int = _rdd.context.newShuffleId()\n+  val shuffleId: Int = if (isContinuous) {\n+    // This will not be reset in continuous processing, set an invalid value for now.\n+    Int.MinValue\n+  } else {\n+    _rdd.context.newShuffleId()\n+  }\n \n-  val shuffleHandle: ShuffleHandle = _rdd.context.env.shuffleManager.registerShuffle(\n-    shuffleId, _rdd.partitions.length, this)\n+  val shuffleHandle: ShuffleHandle = if (isContinuous) {\n+    null\n+  } else {\n+    _rdd.context.env.shuffleManager.registerShuffle(\n+      shuffleId, _rdd.partitions.length, this)\n+  }\n \n-  _rdd.sparkContext.cleaner.foreach(_.registerShuffleForCleanup(this))\n+  if (!isContinuous) {\n+    _rdd.sparkContext.cleaner.foreach(_.registerShuffleForCleanup(this))\n+  }\n }\n \n+/**\n+ * :: DeveloperApi ::\n+ * Represents a dependency on the output of a shuffle stage of continuous type.\n+ * Different with ShuffleDependency, the continuous dependency only create on Executor side,\n+ * so the rdd in param is deserialized from taskBinary.\n+ */\n+@DeveloperApi\n+class ContinuousShuffleDependency[K: ClassTag, V: ClassTag, C: ClassTag](\n+    rdd: RDD[_ <: Product2[K, V]],\n+    dep: ShuffleDependency[K, V, C],\n+    continuousEpoch: Int,\n+    totalShuffleNum: Int,\n+    shuffleNumMaps: Int)\n+  extends ShuffleDependency[K, V, C](\n+    rdd,\n+    dep.partitioner,\n+    dep.serializer,\n+    dep.keyOrdering,\n+    dep.aggregator,\n+    dep.mapSideCombine, true) {\n+\n+  val baseShuffleId: Int = dep.shuffleId\n+\n+  override val shuffleId: Int = continuousEpoch * totalShuffleNum + baseShuffleId",
    "line": 78
  }, {
    "author": {
      "login": "xuanyuanking"
    },
    "body": "Here we use the generated shuffleId only for shuffle registering and identifying, can we consider the DAG never change and each epoch will has same dependency just with diff shuffleId?",
    "commit": "56442dc1c7450518d9bc84b4bfeddb017daa967b",
    "createdAt": "2018-05-11T12:20:41Z",
    "diffHunk": "@@ -88,14 +90,53 @@ class ShuffleDependency[K: ClassTag, V: ClassTag, C: ClassTag](\n   private[spark] val combinerClassName: Option[String] =\n     Option(reflect.classTag[C]).map(_.runtimeClass.getName)\n \n-  val shuffleId: Int = _rdd.context.newShuffleId()\n+  val shuffleId: Int = if (isContinuous) {\n+    // This will not be reset in continuous processing, set an invalid value for now.\n+    Int.MinValue\n+  } else {\n+    _rdd.context.newShuffleId()\n+  }\n \n-  val shuffleHandle: ShuffleHandle = _rdd.context.env.shuffleManager.registerShuffle(\n-    shuffleId, _rdd.partitions.length, this)\n+  val shuffleHandle: ShuffleHandle = if (isContinuous) {\n+    null\n+  } else {\n+    _rdd.context.env.shuffleManager.registerShuffle(\n+      shuffleId, _rdd.partitions.length, this)\n+  }\n \n-  _rdd.sparkContext.cleaner.foreach(_.registerShuffleForCleanup(this))\n+  if (!isContinuous) {\n+    _rdd.sparkContext.cleaner.foreach(_.registerShuffleForCleanup(this))\n+  }\n }\n \n+/**\n+ * :: DeveloperApi ::\n+ * Represents a dependency on the output of a shuffle stage of continuous type.\n+ * Different with ShuffleDependency, the continuous dependency only create on Executor side,\n+ * so the rdd in param is deserialized from taskBinary.\n+ */\n+@DeveloperApi\n+class ContinuousShuffleDependency[K: ClassTag, V: ClassTag, C: ClassTag](\n+    rdd: RDD[_ <: Product2[K, V]],\n+    dep: ShuffleDependency[K, V, C],\n+    continuousEpoch: Int,\n+    totalShuffleNum: Int,\n+    shuffleNumMaps: Int)\n+  extends ShuffleDependency[K, V, C](\n+    rdd,\n+    dep.partitioner,\n+    dep.serializer,\n+    dep.keyOrdering,\n+    dep.aggregator,\n+    dep.mapSideCombine, true) {\n+\n+  val baseShuffleId: Int = dep.shuffleId\n+\n+  override val shuffleId: Int = continuousEpoch * totalShuffleNum + baseShuffleId",
    "line": 78
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "Sorry, I'm not sure I understand. The problem isn't on the continuous processing side here. The SparkContext is responsible for assigning shuffle IDs, so it's not valid to register a ContinuousShuffleDependency with a shuffle ID which SparkContext did not assign.",
    "commit": "56442dc1c7450518d9bc84b4bfeddb017daa967b",
    "createdAt": "2018-05-11T14:55:12Z",
    "diffHunk": "@@ -88,14 +90,53 @@ class ShuffleDependency[K: ClassTag, V: ClassTag, C: ClassTag](\n   private[spark] val combinerClassName: Option[String] =\n     Option(reflect.classTag[C]).map(_.runtimeClass.getName)\n \n-  val shuffleId: Int = _rdd.context.newShuffleId()\n+  val shuffleId: Int = if (isContinuous) {\n+    // This will not be reset in continuous processing, set an invalid value for now.\n+    Int.MinValue\n+  } else {\n+    _rdd.context.newShuffleId()\n+  }\n \n-  val shuffleHandle: ShuffleHandle = _rdd.context.env.shuffleManager.registerShuffle(\n-    shuffleId, _rdd.partitions.length, this)\n+  val shuffleHandle: ShuffleHandle = if (isContinuous) {\n+    null\n+  } else {\n+    _rdd.context.env.shuffleManager.registerShuffle(\n+      shuffleId, _rdd.partitions.length, this)\n+  }\n \n-  _rdd.sparkContext.cleaner.foreach(_.registerShuffleForCleanup(this))\n+  if (!isContinuous) {\n+    _rdd.sparkContext.cleaner.foreach(_.registerShuffleForCleanup(this))\n+  }\n }\n \n+/**\n+ * :: DeveloperApi ::\n+ * Represents a dependency on the output of a shuffle stage of continuous type.\n+ * Different with ShuffleDependency, the continuous dependency only create on Executor side,\n+ * so the rdd in param is deserialized from taskBinary.\n+ */\n+@DeveloperApi\n+class ContinuousShuffleDependency[K: ClassTag, V: ClassTag, C: ClassTag](\n+    rdd: RDD[_ <: Product2[K, V]],\n+    dep: ShuffleDependency[K, V, C],\n+    continuousEpoch: Int,\n+    totalShuffleNum: Int,\n+    shuffleNumMaps: Int)\n+  extends ShuffleDependency[K, V, C](\n+    rdd,\n+    dep.partitioner,\n+    dep.serializer,\n+    dep.keyOrdering,\n+    dep.aggregator,\n+    dep.mapSideCombine, true) {\n+\n+  val baseShuffleId: Int = dep.shuffleId\n+\n+  override val shuffleId: Int = continuousEpoch * totalShuffleNum + baseShuffleId",
    "line": 78
  }, {
    "author": {
      "login": "xuanyuanking"
    },
    "body": "Got it, if we move EpochCoordinator to SparkEnv, I think we can re-implement the shuffle register on driver side, totally controlled by EpochCoordinator. Even if the EpochCoordinator doesn't manage the shuffle register work, I think EpochCoordinator should move to SparkEnv and take more work in shuffle support in your design, am I right?",
    "commit": "56442dc1c7450518d9bc84b4bfeddb017daa967b",
    "createdAt": "2018-05-15T12:40:39Z",
    "diffHunk": "@@ -88,14 +90,53 @@ class ShuffleDependency[K: ClassTag, V: ClassTag, C: ClassTag](\n   private[spark] val combinerClassName: Option[String] =\n     Option(reflect.classTag[C]).map(_.runtimeClass.getName)\n \n-  val shuffleId: Int = _rdd.context.newShuffleId()\n+  val shuffleId: Int = if (isContinuous) {\n+    // This will not be reset in continuous processing, set an invalid value for now.\n+    Int.MinValue\n+  } else {\n+    _rdd.context.newShuffleId()\n+  }\n \n-  val shuffleHandle: ShuffleHandle = _rdd.context.env.shuffleManager.registerShuffle(\n-    shuffleId, _rdd.partitions.length, this)\n+  val shuffleHandle: ShuffleHandle = if (isContinuous) {\n+    null\n+  } else {\n+    _rdd.context.env.shuffleManager.registerShuffle(\n+      shuffleId, _rdd.partitions.length, this)\n+  }\n \n-  _rdd.sparkContext.cleaner.foreach(_.registerShuffleForCleanup(this))\n+  if (!isContinuous) {\n+    _rdd.sparkContext.cleaner.foreach(_.registerShuffleForCleanup(this))\n+  }\n }\n \n+/**\n+ * :: DeveloperApi ::\n+ * Represents a dependency on the output of a shuffle stage of continuous type.\n+ * Different with ShuffleDependency, the continuous dependency only create on Executor side,\n+ * so the rdd in param is deserialized from taskBinary.\n+ */\n+@DeveloperApi\n+class ContinuousShuffleDependency[K: ClassTag, V: ClassTag, C: ClassTag](\n+    rdd: RDD[_ <: Product2[K, V]],\n+    dep: ShuffleDependency[K, V, C],\n+    continuousEpoch: Int,\n+    totalShuffleNum: Int,\n+    shuffleNumMaps: Int)\n+  extends ShuffleDependency[K, V, C](\n+    rdd,\n+    dep.partitioner,\n+    dep.serializer,\n+    dep.keyOrdering,\n+    dep.aggregator,\n+    dep.mapSideCombine, true) {\n+\n+  val baseShuffleId: Int = dep.shuffleId\n+\n+  override val shuffleId: Int = continuousEpoch * totalShuffleNum + baseShuffleId",
    "line": 78
  }],
  "prId": 21293
}]