[{
  "comments": [{
    "author": {
      "login": "smurakozi"
    },
    "body": "Nit: extra line",
    "commit": "344c4647fc84da8f265d9b8d66f0e9424b41359f",
    "createdAt": "2018-02-08T10:36:33Z",
    "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.metrics.sink\n+\n+import java.net.URI\n+import java.util\n+import java.util.Properties\n+import java.util.concurrent.TimeUnit\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+import com.codahale.metrics._\n+import io.prometheus.client.CollectorRegistry\n+import io.prometheus.client.dropwizard.DropwizardExports\n+\n+import org.apache.spark.{SecurityManager, SparkConf, SparkContext, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.METRICS_NAMESPACE\n+import org.apache.spark.metrics.MetricsSystem\n+import org.apache.spark.metrics.prometheus.client.exporter.PushGatewayWithTimestamp\n+\n+\n+private[spark] class PrometheusSink(\n+                                     val property: Properties,\n+                                     val registry: MetricRegistry,\n+                                     securityMgr: SecurityManager)\n+  extends Sink with Logging {\n+\n+  protected class Reporter(registry: MetricRegistry)\n+    extends ScheduledReporter(\n+      registry,\n+      \"prometheus-reporter\",\n+      MetricFilter.ALL,\n+      TimeUnit.SECONDS,\n+      TimeUnit.MILLISECONDS) {\n+\n+    val defaultSparkConf: SparkConf = new SparkConf(true)\n+\n+    override def report(\n+                         gauges: util.SortedMap[String, Gauge[_]],\n+                         counters: util.SortedMap[String, Counter],\n+                         histograms: util.SortedMap[String, Histogram],\n+                         meters: util.SortedMap[String, Meter],\n+                         timers: util.SortedMap[String, Timer]): Unit = {\n+\n+      // SparkEnv may become available only after metrics sink creation thus retrieving\n+      // SparkConf from spark env here and not during the creation/initialisation of PrometheusSink.\n+      val sparkConf: SparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(defaultSparkConf)\n+\n+      val metricsNamespace: Option[String] = sparkConf.get(METRICS_NAMESPACE)\n+      val sparkAppId: Option[String] = sparkConf.getOption(\"spark.app.id\")\n+      val executorId: Option[String] = sparkConf.getOption(\"spark.executor.id\")\n+\n+      logInfo(s\"metricsNamespace=$metricsNamespace, sparkAppId=$sparkAppId, \" +\n+        s\"executorId=$executorId\")\n+\n+      val role: String = (sparkAppId, executorId) match {\n+        case (Some(_), Some(SparkContext.DRIVER_IDENTIFIER)) => \"driver\"\n+        case (Some(_), Some(_)) => \"executor\"\n+        case _ => \"shuffle\"\n+      }\n+\n+      val job: String = role match {\n+        case \"driver\" => metricsNamespace.getOrElse(sparkAppId.get)\n+        case \"executor\" => metricsNamespace.getOrElse(sparkAppId.get)\n+        case _ => metricsNamespace.getOrElse(\"shuffle\")\n+      }\n+      logInfo(s\"role=$role, job=$job\")\n+\n+      val groupingKey: Map[String, String] = (role, executorId) match {\n+        case (\"driver\", _) => Map(\"role\" -> role)\n+        case (\"executor\", Some(id)) => Map (\"role\" -> role, \"number\" -> id)\n+        case _ => Map(\"role\" -> role)\n+      }\n+\n+"
  }, {
    "author": {
      "login": "stoader"
    },
    "body": "Empty line removed.",
    "commit": "344c4647fc84da8f265d9b8d66f0e9424b41359f",
    "createdAt": "2018-02-09T22:09:25Z",
    "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.metrics.sink\n+\n+import java.net.URI\n+import java.util\n+import java.util.Properties\n+import java.util.concurrent.TimeUnit\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+import com.codahale.metrics._\n+import io.prometheus.client.CollectorRegistry\n+import io.prometheus.client.dropwizard.DropwizardExports\n+\n+import org.apache.spark.{SecurityManager, SparkConf, SparkContext, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.METRICS_NAMESPACE\n+import org.apache.spark.metrics.MetricsSystem\n+import org.apache.spark.metrics.prometheus.client.exporter.PushGatewayWithTimestamp\n+\n+\n+private[spark] class PrometheusSink(\n+                                     val property: Properties,\n+                                     val registry: MetricRegistry,\n+                                     securityMgr: SecurityManager)\n+  extends Sink with Logging {\n+\n+  protected class Reporter(registry: MetricRegistry)\n+    extends ScheduledReporter(\n+      registry,\n+      \"prometheus-reporter\",\n+      MetricFilter.ALL,\n+      TimeUnit.SECONDS,\n+      TimeUnit.MILLISECONDS) {\n+\n+    val defaultSparkConf: SparkConf = new SparkConf(true)\n+\n+    override def report(\n+                         gauges: util.SortedMap[String, Gauge[_]],\n+                         counters: util.SortedMap[String, Counter],\n+                         histograms: util.SortedMap[String, Histogram],\n+                         meters: util.SortedMap[String, Meter],\n+                         timers: util.SortedMap[String, Timer]): Unit = {\n+\n+      // SparkEnv may become available only after metrics sink creation thus retrieving\n+      // SparkConf from spark env here and not during the creation/initialisation of PrometheusSink.\n+      val sparkConf: SparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(defaultSparkConf)\n+\n+      val metricsNamespace: Option[String] = sparkConf.get(METRICS_NAMESPACE)\n+      val sparkAppId: Option[String] = sparkConf.getOption(\"spark.app.id\")\n+      val executorId: Option[String] = sparkConf.getOption(\"spark.executor.id\")\n+\n+      logInfo(s\"metricsNamespace=$metricsNamespace, sparkAppId=$sparkAppId, \" +\n+        s\"executorId=$executorId\")\n+\n+      val role: String = (sparkAppId, executorId) match {\n+        case (Some(_), Some(SparkContext.DRIVER_IDENTIFIER)) => \"driver\"\n+        case (Some(_), Some(_)) => \"executor\"\n+        case _ => \"shuffle\"\n+      }\n+\n+      val job: String = role match {\n+        case \"driver\" => metricsNamespace.getOrElse(sparkAppId.get)\n+        case \"executor\" => metricsNamespace.getOrElse(sparkAppId.get)\n+        case _ => metricsNamespace.getOrElse(\"shuffle\")\n+      }\n+      logInfo(s\"role=$role, job=$job\")\n+\n+      val groupingKey: Map[String, String] = (role, executorId) match {\n+        case (\"driver\", _) => Map(\"role\" -> role)\n+        case (\"executor\", Some(id)) => Map (\"role\" -> role, \"number\" -> id)\n+        case _ => Map(\"role\" -> role)\n+      }\n+\n+"
  }],
  "prId": 19775
}, {
  "comments": [{
    "author": {
      "login": "smurakozi"
    },
    "body": "Nit: extra line",
    "commit": "344c4647fc84da8f265d9b8d66f0e9424b41359f",
    "createdAt": "2018-02-08T10:40:23Z",
    "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.metrics.sink\n+\n+import java.net.URI\n+import java.util\n+import java.util.Properties\n+import java.util.concurrent.TimeUnit\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+import com.codahale.metrics._\n+import io.prometheus.client.CollectorRegistry\n+import io.prometheus.client.dropwizard.DropwizardExports\n+\n+import org.apache.spark.{SecurityManager, SparkConf, SparkContext, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.METRICS_NAMESPACE\n+import org.apache.spark.metrics.MetricsSystem\n+import org.apache.spark.metrics.prometheus.client.exporter.PushGatewayWithTimestamp\n+\n+\n+private[spark] class PrometheusSink(\n+                                     val property: Properties,\n+                                     val registry: MetricRegistry,\n+                                     securityMgr: SecurityManager)\n+  extends Sink with Logging {\n+\n+  protected class Reporter(registry: MetricRegistry)\n+    extends ScheduledReporter(\n+      registry,\n+      \"prometheus-reporter\",\n+      MetricFilter.ALL,\n+      TimeUnit.SECONDS,\n+      TimeUnit.MILLISECONDS) {\n+\n+    val defaultSparkConf: SparkConf = new SparkConf(true)\n+\n+    override def report(\n+                         gauges: util.SortedMap[String, Gauge[_]],\n+                         counters: util.SortedMap[String, Counter],\n+                         histograms: util.SortedMap[String, Histogram],\n+                         meters: util.SortedMap[String, Meter],\n+                         timers: util.SortedMap[String, Timer]): Unit = {\n+\n+      // SparkEnv may become available only after metrics sink creation thus retrieving\n+      // SparkConf from spark env here and not during the creation/initialisation of PrometheusSink.\n+      val sparkConf: SparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(defaultSparkConf)\n+\n+      val metricsNamespace: Option[String] = sparkConf.get(METRICS_NAMESPACE)\n+      val sparkAppId: Option[String] = sparkConf.getOption(\"spark.app.id\")\n+      val executorId: Option[String] = sparkConf.getOption(\"spark.executor.id\")\n+\n+      logInfo(s\"metricsNamespace=$metricsNamespace, sparkAppId=$sparkAppId, \" +\n+        s\"executorId=$executorId\")\n+\n+      val role: String = (sparkAppId, executorId) match {\n+        case (Some(_), Some(SparkContext.DRIVER_IDENTIFIER)) => \"driver\"\n+        case (Some(_), Some(_)) => \"executor\"\n+        case _ => \"shuffle\"\n+      }\n+\n+      val job: String = role match {\n+        case \"driver\" => metricsNamespace.getOrElse(sparkAppId.get)\n+        case \"executor\" => metricsNamespace.getOrElse(sparkAppId.get)\n+        case _ => metricsNamespace.getOrElse(\"shuffle\")\n+      }\n+      logInfo(s\"role=$role, job=$job\")\n+\n+      val groupingKey: Map[String, String] = (role, executorId) match {\n+        case (\"driver\", _) => Map(\"role\" -> role)\n+        case (\"executor\", Some(id)) => Map (\"role\" -> role, \"number\" -> id)\n+        case _ => Map(\"role\" -> role)\n+      }\n+\n+\n+      pushGateway.pushAdd(pushRegistry, job, groupingKey.asJava,\n+        s\"${System.currentTimeMillis}\")\n+\n+    }\n+\n+  }\n+\n+  val DEFAULT_PUSH_PERIOD: Int = 10\n+  val DEFAULT_PUSH_PERIOD_UNIT: TimeUnit = TimeUnit.SECONDS\n+  val DEFAULT_PUSHGATEWAY_ADDRESS: String = \"127.0.0.1:9091\"\n+  val DEFAULT_PUSHGATEWAY_ADDRESS_PROTOCOL: String = \"http\"\n+\n+  val KEY_PUSH_PERIOD = \"period\"\n+  val KEY_PUSH_PERIOD_UNIT = \"unit\"\n+  val KEY_PUSHGATEWAY_ADDRESS = \"pushgateway-address\"\n+  val KEY_PUSHGATEWAY_ADDRESS_PROTOCOL = \"pushgateway-address-protocol\"\n+\n+\n+  val pollPeriod: Int =\n+    Option(property.getProperty(KEY_PUSH_PERIOD))\n+      .map(_.toInt)\n+      .getOrElse(DEFAULT_PUSH_PERIOD)\n+\n+  val pollUnit: TimeUnit =\n+    Option(property.getProperty(KEY_PUSH_PERIOD_UNIT))\n+      .map { s => TimeUnit.valueOf(s.toUpperCase) }\n+      .getOrElse(DEFAULT_PUSH_PERIOD_UNIT)\n+\n+  val pushGatewayAddress =\n+    Option(property.getProperty(KEY_PUSHGATEWAY_ADDRESS))\n+      .getOrElse(DEFAULT_PUSHGATEWAY_ADDRESS)\n+\n+  val pushGatewayAddressProtocol =\n+    Option(property.getProperty(KEY_PUSHGATEWAY_ADDRESS_PROTOCOL))\n+      .getOrElse(DEFAULT_PUSHGATEWAY_ADDRESS_PROTOCOL)\n+\n+  // validate pushgateway host:port\n+  Try(new URI(s\"$pushGatewayAddressProtocol://$pushGatewayAddress\")).get\n+\n+  MetricsSystem.checkMinimalPollingPeriod(pollUnit, pollPeriod)\n+\n+  logInfo(\"Initializing Prometheus Sink...\")\n+  logInfo(s\"Metrics polling period -> $pollPeriod $pollUnit\")\n+  logInfo(s\"$KEY_PUSHGATEWAY_ADDRESS -> $pushGatewayAddress\")\n+  logInfo(s\"$KEY_PUSHGATEWAY_ADDRESS_PROTOCOL -> $pushGatewayAddressProtocol\")\n+\n+  val pushRegistry: CollectorRegistry = new CollectorRegistry()\n+  val sparkMetricExports: DropwizardExports = new DropwizardExports(registry)\n+  val pushGateway: PushGatewayWithTimestamp =\n+    new PushGatewayWithTimestamp(s\"$pushGatewayAddressProtocol://$pushGatewayAddress\")\n+\n+  val reporter = new Reporter(registry)\n+\n+  override def start(): Unit = {\n+    sparkMetricExports.register(pushRegistry)\n+"
  }, {
    "author": {
      "login": "stoader"
    },
    "body": "Empty line removed.",
    "commit": "344c4647fc84da8f265d9b8d66f0e9424b41359f",
    "createdAt": "2018-02-09T22:09:24Z",
    "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.metrics.sink\n+\n+import java.net.URI\n+import java.util\n+import java.util.Properties\n+import java.util.concurrent.TimeUnit\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+import com.codahale.metrics._\n+import io.prometheus.client.CollectorRegistry\n+import io.prometheus.client.dropwizard.DropwizardExports\n+\n+import org.apache.spark.{SecurityManager, SparkConf, SparkContext, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.METRICS_NAMESPACE\n+import org.apache.spark.metrics.MetricsSystem\n+import org.apache.spark.metrics.prometheus.client.exporter.PushGatewayWithTimestamp\n+\n+\n+private[spark] class PrometheusSink(\n+                                     val property: Properties,\n+                                     val registry: MetricRegistry,\n+                                     securityMgr: SecurityManager)\n+  extends Sink with Logging {\n+\n+  protected class Reporter(registry: MetricRegistry)\n+    extends ScheduledReporter(\n+      registry,\n+      \"prometheus-reporter\",\n+      MetricFilter.ALL,\n+      TimeUnit.SECONDS,\n+      TimeUnit.MILLISECONDS) {\n+\n+    val defaultSparkConf: SparkConf = new SparkConf(true)\n+\n+    override def report(\n+                         gauges: util.SortedMap[String, Gauge[_]],\n+                         counters: util.SortedMap[String, Counter],\n+                         histograms: util.SortedMap[String, Histogram],\n+                         meters: util.SortedMap[String, Meter],\n+                         timers: util.SortedMap[String, Timer]): Unit = {\n+\n+      // SparkEnv may become available only after metrics sink creation thus retrieving\n+      // SparkConf from spark env here and not during the creation/initialisation of PrometheusSink.\n+      val sparkConf: SparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(defaultSparkConf)\n+\n+      val metricsNamespace: Option[String] = sparkConf.get(METRICS_NAMESPACE)\n+      val sparkAppId: Option[String] = sparkConf.getOption(\"spark.app.id\")\n+      val executorId: Option[String] = sparkConf.getOption(\"spark.executor.id\")\n+\n+      logInfo(s\"metricsNamespace=$metricsNamespace, sparkAppId=$sparkAppId, \" +\n+        s\"executorId=$executorId\")\n+\n+      val role: String = (sparkAppId, executorId) match {\n+        case (Some(_), Some(SparkContext.DRIVER_IDENTIFIER)) => \"driver\"\n+        case (Some(_), Some(_)) => \"executor\"\n+        case _ => \"shuffle\"\n+      }\n+\n+      val job: String = role match {\n+        case \"driver\" => metricsNamespace.getOrElse(sparkAppId.get)\n+        case \"executor\" => metricsNamespace.getOrElse(sparkAppId.get)\n+        case _ => metricsNamespace.getOrElse(\"shuffle\")\n+      }\n+      logInfo(s\"role=$role, job=$job\")\n+\n+      val groupingKey: Map[String, String] = (role, executorId) match {\n+        case (\"driver\", _) => Map(\"role\" -> role)\n+        case (\"executor\", Some(id)) => Map (\"role\" -> role, \"number\" -> id)\n+        case _ => Map(\"role\" -> role)\n+      }\n+\n+\n+      pushGateway.pushAdd(pushRegistry, job, groupingKey.asJava,\n+        s\"${System.currentTimeMillis}\")\n+\n+    }\n+\n+  }\n+\n+  val DEFAULT_PUSH_PERIOD: Int = 10\n+  val DEFAULT_PUSH_PERIOD_UNIT: TimeUnit = TimeUnit.SECONDS\n+  val DEFAULT_PUSHGATEWAY_ADDRESS: String = \"127.0.0.1:9091\"\n+  val DEFAULT_PUSHGATEWAY_ADDRESS_PROTOCOL: String = \"http\"\n+\n+  val KEY_PUSH_PERIOD = \"period\"\n+  val KEY_PUSH_PERIOD_UNIT = \"unit\"\n+  val KEY_PUSHGATEWAY_ADDRESS = \"pushgateway-address\"\n+  val KEY_PUSHGATEWAY_ADDRESS_PROTOCOL = \"pushgateway-address-protocol\"\n+\n+\n+  val pollPeriod: Int =\n+    Option(property.getProperty(KEY_PUSH_PERIOD))\n+      .map(_.toInt)\n+      .getOrElse(DEFAULT_PUSH_PERIOD)\n+\n+  val pollUnit: TimeUnit =\n+    Option(property.getProperty(KEY_PUSH_PERIOD_UNIT))\n+      .map { s => TimeUnit.valueOf(s.toUpperCase) }\n+      .getOrElse(DEFAULT_PUSH_PERIOD_UNIT)\n+\n+  val pushGatewayAddress =\n+    Option(property.getProperty(KEY_PUSHGATEWAY_ADDRESS))\n+      .getOrElse(DEFAULT_PUSHGATEWAY_ADDRESS)\n+\n+  val pushGatewayAddressProtocol =\n+    Option(property.getProperty(KEY_PUSHGATEWAY_ADDRESS_PROTOCOL))\n+      .getOrElse(DEFAULT_PUSHGATEWAY_ADDRESS_PROTOCOL)\n+\n+  // validate pushgateway host:port\n+  Try(new URI(s\"$pushGatewayAddressProtocol://$pushGatewayAddress\")).get\n+\n+  MetricsSystem.checkMinimalPollingPeriod(pollUnit, pollPeriod)\n+\n+  logInfo(\"Initializing Prometheus Sink...\")\n+  logInfo(s\"Metrics polling period -> $pollPeriod $pollUnit\")\n+  logInfo(s\"$KEY_PUSHGATEWAY_ADDRESS -> $pushGatewayAddress\")\n+  logInfo(s\"$KEY_PUSHGATEWAY_ADDRESS_PROTOCOL -> $pushGatewayAddressProtocol\")\n+\n+  val pushRegistry: CollectorRegistry = new CollectorRegistry()\n+  val sparkMetricExports: DropwizardExports = new DropwizardExports(registry)\n+  val pushGateway: PushGatewayWithTimestamp =\n+    new PushGatewayWithTimestamp(s\"$pushGatewayAddressProtocol://$pushGatewayAddress\")\n+\n+  val reporter = new Reporter(registry)\n+\n+  override def start(): Unit = {\n+    sparkMetricExports.register(pushRegistry)\n+"
  }],
  "prId": 19775
}, {
  "comments": [{
    "author": {
      "login": "smurakozi"
    },
    "body": "`securityMgr` is never used",
    "commit": "344c4647fc84da8f265d9b8d66f0e9424b41359f",
    "createdAt": "2018-02-08T10:42:19Z",
    "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.metrics.sink\n+\n+import java.net.URI\n+import java.util\n+import java.util.Properties\n+import java.util.concurrent.TimeUnit\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+import com.codahale.metrics._\n+import io.prometheus.client.CollectorRegistry\n+import io.prometheus.client.dropwizard.DropwizardExports\n+\n+import org.apache.spark.{SecurityManager, SparkConf, SparkContext, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.METRICS_NAMESPACE\n+import org.apache.spark.metrics.MetricsSystem\n+import org.apache.spark.metrics.prometheus.client.exporter.PushGatewayWithTimestamp\n+\n+\n+private[spark] class PrometheusSink(\n+                                     val property: Properties,\n+                                     val registry: MetricRegistry,\n+                                     securityMgr: SecurityManager)",
    "line": 42
  }, {
    "author": {
      "login": "stoader"
    },
    "body": "The parameter list for a `Sink` is imposed by`MetricsSystem` which instantiates the configured sinks (see https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala#L199). `PrometheusSink` doesn't need `SecurityManager` this is why `securityMgr` is not used (similar to CsvSink, ConsoleSink).",
    "commit": "344c4647fc84da8f265d9b8d66f0e9424b41359f",
    "createdAt": "2018-02-09T22:00:06Z",
    "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.metrics.sink\n+\n+import java.net.URI\n+import java.util\n+import java.util.Properties\n+import java.util.concurrent.TimeUnit\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+import com.codahale.metrics._\n+import io.prometheus.client.CollectorRegistry\n+import io.prometheus.client.dropwizard.DropwizardExports\n+\n+import org.apache.spark.{SecurityManager, SparkConf, SparkContext, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.METRICS_NAMESPACE\n+import org.apache.spark.metrics.MetricsSystem\n+import org.apache.spark.metrics.prometheus.client.exporter.PushGatewayWithTimestamp\n+\n+\n+private[spark] class PrometheusSink(\n+                                     val property: Properties,\n+                                     val registry: MetricRegistry,\n+                                     securityMgr: SecurityManager)",
    "line": 42
  }],
  "prId": 19775
}]