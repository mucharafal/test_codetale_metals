[{
  "comments": [{
    "author": {
      "login": "jerryshao"
    },
    "body": "Hi @rahulsinghaliitd , do we really need to use `SparkConf` to get this unique app name? I think metrics system is driven by its own configuration system, here you involved `SparkConf` as another configuration system just only for `CsvSink`, this will give people the chance to bypass the original way and lose its controllability. I think it would be nice to modify the way to get `appUniqueName` (eg. get it from SparkEnv).\n",
    "commit": "10a0d8e54adfd2687a9747c168f43c9384058c9d",
    "createdAt": "2014-06-12T13:25:48Z",
    "diffHunk": "@@ -53,11 +53,14 @@ private[spark] class CsvSink(val property: Properties, val registry: MetricRegis\n     case None => CSV_DEFAULT_DIR\n   }\n \n+  val file= new File(pollDir + conf.get(\"spark.app.uniqueName\"))"
  }, {
    "author": {
      "login": "rahulsinghaliitd"
    },
    "body": "@jerryshao Thanks for the feedback! I was not aware of the fact the SparkEnv provides access to SparkConf. Will follow up with the suggested modification.\n",
    "commit": "10a0d8e54adfd2687a9747c168f43c9384058c9d",
    "createdAt": "2014-06-12T13:49:56Z",
    "diffHunk": "@@ -53,11 +53,14 @@ private[spark] class CsvSink(val property: Properties, val registry: MetricRegis\n     case None => CSV_DEFAULT_DIR\n   }\n \n+  val file= new File(pollDir + conf.get(\"spark.app.uniqueName\"))"
  }, {
    "author": {
      "login": "jerryshao"
    },
    "body": "Forgot to mention that several sinks like CsvSink will be started in master and worker for standalone mode if configured, in this situation you may not get unique app name or SparkEnv (something created only when app started). So maybe you should consider this situation when there's no app name available.\n",
    "commit": "10a0d8e54adfd2687a9747c168f43c9384058c9d",
    "createdAt": "2014-06-12T14:21:33Z",
    "diffHunk": "@@ -53,11 +53,14 @@ private[spark] class CsvSink(val property: Properties, val registry: MetricRegis\n     case None => CSV_DEFAULT_DIR\n   }\n \n+  val file= new File(pollDir + conf.get(\"spark.app.uniqueName\"))"
  }, {
    "author": {
      "login": "rahulsinghaliitd"
    },
    "body": "Hi @jerryshao ,\n1. At the moment the Sinks are being created, SparkEnv has not been created. I may be able to modify the Properties being passed to this Sink or even get the SparkConf from SecurityManager. But neither of those approaches seems generic to me. For e.g. we will need hadoopConf if we wanted the csv directory to be on HDFS.\n2. Thanks for pointing out the problem with Master and Worker. I have for now added app names to these classes. Please let me know if you think adding null checks in CsvSink would also be useful.\n",
    "commit": "10a0d8e54adfd2687a9747c168f43c9384058c9d",
    "createdAt": "2014-06-13T07:52:45Z",
    "diffHunk": "@@ -53,11 +53,14 @@ private[spark] class CsvSink(val property: Properties, val registry: MetricRegis\n     case None => CSV_DEFAULT_DIR\n   }\n \n+  val file= new File(pollDir + conf.get(\"spark.app.uniqueName\"))"
  }],
  "prId": 1067
}]