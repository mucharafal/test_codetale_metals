[{
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "override\n",
    "commit": "bf3f3f6bab69cae01056fd21e2f6d238e0ab707d",
    "createdAt": "2015-05-26T04:12:09Z",
    "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import java.io.{File, FileInputStream, FileOutputStream}\n+\n+import org.apache.spark._\n+import org.apache.spark.executor.ShuffleWriteMetrics\n+import org.apache.spark.serializer.Serializer\n+import org.apache.spark.storage.{BlockId, BlockManager, BlockObjectWriter}\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.collection._\n+\n+/**\n+ * This class handles sort-based shuffle's `bypassMergeSort` write path, which is used for shuffles\n+ * for which no Ordering and no Aggregator is given and the number of partitions is\n+ * less than `spark.shuffle.sort.bypassMergeThreshold`.\n+ *\n+ * This path used to be part of [[ExternalSorter]] but was refactored into its own class in order to\n+ * reduce code complexity; see SPARK-7855 for more details.\n+ *\n+ * There have been proposals to completely remove this code path; see SPARK-6026 for details.\n+ */\n+private[spark] class BypassMergeSortShuffleWriter[K, V](\n+    conf: SparkConf,\n+    blockManager: BlockManager,\n+    partitioner: Partitioner,\n+    writeMetrics: ShuffleWriteMetrics,\n+    serializer: Serializer)\n+  extends Logging with SortShuffleFileWriter[K, V] {\n+\n+  private[this] val numPartitions = partitioner.numPartitions\n+\n+  /** Array of file writers for each partition */\n+  private[this] var partitionWriters: Array[BlockObjectWriter] = _\n+\n+  def insertAll(records: Iterator[_ <: Product2[K, V]]): Unit = {"
  }],
  "prId": 6397
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "override\n",
    "commit": "bf3f3f6bab69cae01056fd21e2f6d238e0ab707d",
    "createdAt": "2015-05-26T04:12:11Z",
    "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import java.io.{File, FileInputStream, FileOutputStream}\n+\n+import org.apache.spark._\n+import org.apache.spark.executor.ShuffleWriteMetrics\n+import org.apache.spark.serializer.Serializer\n+import org.apache.spark.storage.{BlockId, BlockManager, BlockObjectWriter}\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.collection._\n+\n+/**\n+ * This class handles sort-based shuffle's `bypassMergeSort` write path, which is used for shuffles\n+ * for which no Ordering and no Aggregator is given and the number of partitions is\n+ * less than `spark.shuffle.sort.bypassMergeThreshold`.\n+ *\n+ * This path used to be part of [[ExternalSorter]] but was refactored into its own class in order to\n+ * reduce code complexity; see SPARK-7855 for more details.\n+ *\n+ * There have been proposals to completely remove this code path; see SPARK-6026 for details.\n+ */\n+private[spark] class BypassMergeSortShuffleWriter[K, V](\n+    conf: SparkConf,\n+    blockManager: BlockManager,\n+    partitioner: Partitioner,\n+    writeMetrics: ShuffleWriteMetrics,\n+    serializer: Serializer)\n+  extends Logging with SortShuffleFileWriter[K, V] {\n+\n+  private[this] val numPartitions = partitioner.numPartitions\n+\n+  /** Array of file writers for each partition */\n+  private[this] var partitionWriters: Array[BlockObjectWriter] = _\n+\n+  def insertAll(records: Iterator[_ <: Product2[K, V]]): Unit = {\n+    assert (partitionWriters == null)\n+    if (records.hasNext) {\n+      val serInstance = serializer.newInstance()\n+      // Use getSizeAsKb (not bytes) to maintain backwards compatibility if no units are provided\n+      val fileBufferSize = conf.getSizeAsKb(\"spark.shuffle.file.buffer\", \"32k\").toInt * 1024\n+      val openStartTime = System.nanoTime\n+      partitionWriters = Array.fill(numPartitions) {\n+        val (blockId, file) = blockManager.diskBlockManager.createTempShuffleBlock()\n+        val writer =\n+          blockManager.getDiskWriter(blockId, file, serInstance, fileBufferSize, writeMetrics)\n+        writer.open()\n+      }\n+      // Creating the file to write to and creating a disk writer both involve interacting with\n+      // the disk, and can take a long time in aggregate when we open many files, so should be\n+      // included in the shuffle write time.\n+      writeMetrics.incShuffleWriteTime(System.nanoTime - openStartTime)\n+\n+      while (records.hasNext) {\n+        val record = records.next()\n+        val key: K = record._1\n+        partitionWriters(partitioner.getPartition(key)).write(key, record._2)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Write all the data added into this writer into a single file in the disk store. This is\n+   * called by the SortShuffleWriter and can go through an efficient path of just concatenating\n+   * the per-partition binary files.\n+   *\n+   * @param blockId block ID to write to. The index file will be blockId.name + \".index\".\n+   * @param context a TaskContext for a running Spark task, for us to update shuffle metrics.\n+   * @return array of lengths, in bytes, of each partition of the file (used by map output tracker)\n+   */\n+  def writePartitionedFile(blockId: BlockId, context: TaskContext, file: File): Array[Long] = {"
  }],
  "prId": 6397
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "i think i'd prefer an explicit return here. i was lookin at the code and thought : \"oh what's going on. isn't this a no-op\".\n",
    "commit": "bf3f3f6bab69cae01056fd21e2f6d238e0ab707d",
    "createdAt": "2015-05-26T04:44:23Z",
    "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import java.io.{File, FileInputStream, FileOutputStream}\n+\n+import org.apache.spark._\n+import org.apache.spark.executor.ShuffleWriteMetrics\n+import org.apache.spark.serializer.Serializer\n+import org.apache.spark.storage.{BlockId, BlockManager, BlockObjectWriter}\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.collection._\n+\n+/**\n+ * This class handles sort-based shuffle's `bypassMergeSort` write path, which is used for shuffles\n+ * for which no Ordering and no Aggregator is given and the number of partitions is\n+ * less than `spark.shuffle.sort.bypassMergeThreshold`.\n+ *\n+ * This path used to be part of [[ExternalSorter]] but was refactored into its own class in order to\n+ * reduce code complexity; see SPARK-7855 for more details.\n+ *\n+ * There have been proposals to completely remove this code path; see SPARK-6026 for details.\n+ */\n+private[spark] class BypassMergeSortShuffleWriter[K, V](\n+    conf: SparkConf,\n+    blockManager: BlockManager,\n+    partitioner: Partitioner,\n+    writeMetrics: ShuffleWriteMetrics,\n+    serializer: Serializer)\n+  extends Logging with SortShuffleFileWriter[K, V] {\n+\n+  private[this] val numPartitions = partitioner.numPartitions\n+\n+  /** Array of file writers for each partition */\n+  private[this] var partitionWriters: Array[BlockObjectWriter] = _\n+\n+  def insertAll(records: Iterator[_ <: Product2[K, V]]): Unit = {\n+    assert (partitionWriters == null)\n+    if (records.hasNext) {\n+      val serInstance = serializer.newInstance()\n+      // Use getSizeAsKb (not bytes) to maintain backwards compatibility if no units are provided\n+      val fileBufferSize = conf.getSizeAsKb(\"spark.shuffle.file.buffer\", \"32k\").toInt * 1024\n+      val openStartTime = System.nanoTime\n+      partitionWriters = Array.fill(numPartitions) {\n+        val (blockId, file) = blockManager.diskBlockManager.createTempShuffleBlock()\n+        val writer =\n+          blockManager.getDiskWriter(blockId, file, serInstance, fileBufferSize, writeMetrics)\n+        writer.open()\n+      }\n+      // Creating the file to write to and creating a disk writer both involve interacting with\n+      // the disk, and can take a long time in aggregate when we open many files, so should be\n+      // included in the shuffle write time.\n+      writeMetrics.incShuffleWriteTime(System.nanoTime - openStartTime)\n+\n+      while (records.hasNext) {\n+        val record = records.next()\n+        val key: K = record._1\n+        partitionWriters(partitioner.getPartition(key)).write(key, record._2)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Write all the data added into this writer into a single file in the disk store. This is\n+   * called by the SortShuffleWriter and can go through an efficient path of just concatenating\n+   * the per-partition binary files.\n+   *\n+   * @param blockId block ID to write to. The index file will be blockId.name + \".index\".\n+   * @param context a TaskContext for a running Spark task, for us to update shuffle metrics.\n+   * @return array of lengths, in bytes, of each partition of the file (used by map output tracker)\n+   */\n+  def writePartitionedFile(blockId: BlockId, context: TaskContext, file: File): Array[Long] = {\n+    if (partitionWriters == null) {\n+      // We were passed an empty iterator\n+      Array.fill(numPartitions)(0L)"
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "I was purposely trying to avoid ever using `return` in Scala code.  I can add it back if you think it will improve clarity.\n",
    "commit": "bf3f3f6bab69cae01056fd21e2f6d238e0ab707d",
    "createdAt": "2015-05-26T04:50:17Z",
    "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import java.io.{File, FileInputStream, FileOutputStream}\n+\n+import org.apache.spark._\n+import org.apache.spark.executor.ShuffleWriteMetrics\n+import org.apache.spark.serializer.Serializer\n+import org.apache.spark.storage.{BlockId, BlockManager, BlockObjectWriter}\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.collection._\n+\n+/**\n+ * This class handles sort-based shuffle's `bypassMergeSort` write path, which is used for shuffles\n+ * for which no Ordering and no Aggregator is given and the number of partitions is\n+ * less than `spark.shuffle.sort.bypassMergeThreshold`.\n+ *\n+ * This path used to be part of [[ExternalSorter]] but was refactored into its own class in order to\n+ * reduce code complexity; see SPARK-7855 for more details.\n+ *\n+ * There have been proposals to completely remove this code path; see SPARK-6026 for details.\n+ */\n+private[spark] class BypassMergeSortShuffleWriter[K, V](\n+    conf: SparkConf,\n+    blockManager: BlockManager,\n+    partitioner: Partitioner,\n+    writeMetrics: ShuffleWriteMetrics,\n+    serializer: Serializer)\n+  extends Logging with SortShuffleFileWriter[K, V] {\n+\n+  private[this] val numPartitions = partitioner.numPartitions\n+\n+  /** Array of file writers for each partition */\n+  private[this] var partitionWriters: Array[BlockObjectWriter] = _\n+\n+  def insertAll(records: Iterator[_ <: Product2[K, V]]): Unit = {\n+    assert (partitionWriters == null)\n+    if (records.hasNext) {\n+      val serInstance = serializer.newInstance()\n+      // Use getSizeAsKb (not bytes) to maintain backwards compatibility if no units are provided\n+      val fileBufferSize = conf.getSizeAsKb(\"spark.shuffle.file.buffer\", \"32k\").toInt * 1024\n+      val openStartTime = System.nanoTime\n+      partitionWriters = Array.fill(numPartitions) {\n+        val (blockId, file) = blockManager.diskBlockManager.createTempShuffleBlock()\n+        val writer =\n+          blockManager.getDiskWriter(blockId, file, serInstance, fileBufferSize, writeMetrics)\n+        writer.open()\n+      }\n+      // Creating the file to write to and creating a disk writer both involve interacting with\n+      // the disk, and can take a long time in aggregate when we open many files, so should be\n+      // included in the shuffle write time.\n+      writeMetrics.incShuffleWriteTime(System.nanoTime - openStartTime)\n+\n+      while (records.hasNext) {\n+        val record = records.next()\n+        val key: K = record._1\n+        partitionWriters(partitioner.getPartition(key)).write(key, record._2)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Write all the data added into this writer into a single file in the disk store. This is\n+   * called by the SortShuffleWriter and can go through an efficient path of just concatenating\n+   * the per-partition binary files.\n+   *\n+   * @param blockId block ID to write to. The index file will be blockId.name + \".index\".\n+   * @param context a TaskContext for a running Spark task, for us to update shuffle metrics.\n+   * @return array of lengths, in bytes, of each partition of the file (used by map output tracker)\n+   */\n+  def writePartitionedFile(blockId: BlockId, context: TaskContext, file: File): Array[Long] = {\n+    if (partitionWriters == null) {\n+      // We were passed an empty iterator\n+      Array.fill(numPartitions)(0L)"
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "btw this is also just\n\n``` scala\nnew Array[Long](numPartitions)\n```\n\nand much faster this way too\n",
    "commit": "bf3f3f6bab69cae01056fd21e2f6d238e0ab707d",
    "createdAt": "2015-05-26T04:53:15Z",
    "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import java.io.{File, FileInputStream, FileOutputStream}\n+\n+import org.apache.spark._\n+import org.apache.spark.executor.ShuffleWriteMetrics\n+import org.apache.spark.serializer.Serializer\n+import org.apache.spark.storage.{BlockId, BlockManager, BlockObjectWriter}\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.collection._\n+\n+/**\n+ * This class handles sort-based shuffle's `bypassMergeSort` write path, which is used for shuffles\n+ * for which no Ordering and no Aggregator is given and the number of partitions is\n+ * less than `spark.shuffle.sort.bypassMergeThreshold`.\n+ *\n+ * This path used to be part of [[ExternalSorter]] but was refactored into its own class in order to\n+ * reduce code complexity; see SPARK-7855 for more details.\n+ *\n+ * There have been proposals to completely remove this code path; see SPARK-6026 for details.\n+ */\n+private[spark] class BypassMergeSortShuffleWriter[K, V](\n+    conf: SparkConf,\n+    blockManager: BlockManager,\n+    partitioner: Partitioner,\n+    writeMetrics: ShuffleWriteMetrics,\n+    serializer: Serializer)\n+  extends Logging with SortShuffleFileWriter[K, V] {\n+\n+  private[this] val numPartitions = partitioner.numPartitions\n+\n+  /** Array of file writers for each partition */\n+  private[this] var partitionWriters: Array[BlockObjectWriter] = _\n+\n+  def insertAll(records: Iterator[_ <: Product2[K, V]]): Unit = {\n+    assert (partitionWriters == null)\n+    if (records.hasNext) {\n+      val serInstance = serializer.newInstance()\n+      // Use getSizeAsKb (not bytes) to maintain backwards compatibility if no units are provided\n+      val fileBufferSize = conf.getSizeAsKb(\"spark.shuffle.file.buffer\", \"32k\").toInt * 1024\n+      val openStartTime = System.nanoTime\n+      partitionWriters = Array.fill(numPartitions) {\n+        val (blockId, file) = blockManager.diskBlockManager.createTempShuffleBlock()\n+        val writer =\n+          blockManager.getDiskWriter(blockId, file, serInstance, fileBufferSize, writeMetrics)\n+        writer.open()\n+      }\n+      // Creating the file to write to and creating a disk writer both involve interacting with\n+      // the disk, and can take a long time in aggregate when we open many files, so should be\n+      // included in the shuffle write time.\n+      writeMetrics.incShuffleWriteTime(System.nanoTime - openStartTime)\n+\n+      while (records.hasNext) {\n+        val record = records.next()\n+        val key: K = record._1\n+        partitionWriters(partitioner.getPartition(key)).write(key, record._2)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Write all the data added into this writer into a single file in the disk store. This is\n+   * called by the SortShuffleWriter and can go through an efficient path of just concatenating\n+   * the per-partition binary files.\n+   *\n+   * @param blockId block ID to write to. The index file will be blockId.name + \".index\".\n+   * @param context a TaskContext for a running Spark task, for us to update shuffle metrics.\n+   * @return array of lengths, in bytes, of each partition of the file (used by map output tracker)\n+   */\n+  def writePartitionedFile(blockId: BlockId, context: TaskContext, file: File): Array[Long] = {\n+    if (partitionWriters == null) {\n+      // We were passed an empty iterator\n+      Array.fill(numPartitions)(0L)"
  }],
  "prId": 6397
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "a while loop maybe? this is fairly expensive for a large number of partitions\n",
    "commit": "bf3f3f6bab69cae01056fd21e2f6d238e0ab707d",
    "createdAt": "2015-05-26T04:54:36Z",
    "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import java.io.{File, FileInputStream, FileOutputStream}\n+\n+import org.apache.spark._\n+import org.apache.spark.executor.ShuffleWriteMetrics\n+import org.apache.spark.serializer.Serializer\n+import org.apache.spark.storage.{BlockId, BlockManager, BlockObjectWriter}\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.collection._\n+\n+/**\n+ * This class handles sort-based shuffle's `bypassMergeSort` write path, which is used for shuffles\n+ * for which no Ordering and no Aggregator is given and the number of partitions is\n+ * less than `spark.shuffle.sort.bypassMergeThreshold`.\n+ *\n+ * This path used to be part of [[ExternalSorter]] but was refactored into its own class in order to\n+ * reduce code complexity; see SPARK-7855 for more details.\n+ *\n+ * There have been proposals to completely remove this code path; see SPARK-6026 for details.\n+ */\n+private[spark] class BypassMergeSortShuffleWriter[K, V](\n+    conf: SparkConf,\n+    blockManager: BlockManager,\n+    partitioner: Partitioner,\n+    writeMetrics: ShuffleWriteMetrics,\n+    serializer: Serializer)\n+  extends Logging with SortShuffleFileWriter[K, V] {\n+\n+  private[this] val numPartitions = partitioner.numPartitions\n+\n+  /** Array of file writers for each partition */\n+  private[this] var partitionWriters: Array[BlockObjectWriter] = _\n+\n+  def insertAll(records: Iterator[_ <: Product2[K, V]]): Unit = {\n+    assert (partitionWriters == null)\n+    if (records.hasNext) {\n+      val serInstance = serializer.newInstance()\n+      // Use getSizeAsKb (not bytes) to maintain backwards compatibility if no units are provided\n+      val fileBufferSize = conf.getSizeAsKb(\"spark.shuffle.file.buffer\", \"32k\").toInt * 1024\n+      val openStartTime = System.nanoTime\n+      partitionWriters = Array.fill(numPartitions) {\n+        val (blockId, file) = blockManager.diskBlockManager.createTempShuffleBlock()\n+        val writer =\n+          blockManager.getDiskWriter(blockId, file, serInstance, fileBufferSize, writeMetrics)\n+        writer.open()\n+      }\n+      // Creating the file to write to and creating a disk writer both involve interacting with\n+      // the disk, and can take a long time in aggregate when we open many files, so should be\n+      // included in the shuffle write time.\n+      writeMetrics.incShuffleWriteTime(System.nanoTime - openStartTime)\n+\n+      while (records.hasNext) {\n+        val record = records.next()\n+        val key: K = record._1\n+        partitionWriters(partitioner.getPartition(key)).write(key, record._2)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Write all the data added into this writer into a single file in the disk store. This is\n+   * called by the SortShuffleWriter and can go through an efficient path of just concatenating\n+   * the per-partition binary files.\n+   *\n+   * @param blockId block ID to write to. The index file will be blockId.name + \".index\".\n+   * @param context a TaskContext for a running Spark task, for us to update shuffle metrics.\n+   * @return array of lengths, in bytes, of each partition of the file (used by map output tracker)\n+   */\n+  def writePartitionedFile(blockId: BlockId, context: TaskContext, file: File): Array[Long] = {\n+    if (partitionWriters == null) {\n+      // We were passed an empty iterator\n+      Array.fill(numPartitions)(0L)\n+    } else {\n+      partitionWriters.foreach(_.commitAndClose())\n+\n+      // Track location of each range in the output file\n+      val lengths = new Array[Long](numPartitions)\n+\n+      val transferToEnabled = conf.getBoolean(\"spark.file.transferTo\", true)\n+      val out = new FileOutputStream(file, true)\n+      val writeStartTime = System.nanoTime\n+      Utils.tryWithSafeFinally {\n+        for (i <- 0 until numPartitions) {"
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "I think that this was a carryover from the old code, but I don't mind changing it here.\n",
    "commit": "bf3f3f6bab69cae01056fd21e2f6d238e0ab707d",
    "createdAt": "2015-05-26T06:10:26Z",
    "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import java.io.{File, FileInputStream, FileOutputStream}\n+\n+import org.apache.spark._\n+import org.apache.spark.executor.ShuffleWriteMetrics\n+import org.apache.spark.serializer.Serializer\n+import org.apache.spark.storage.{BlockId, BlockManager, BlockObjectWriter}\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.collection._\n+\n+/**\n+ * This class handles sort-based shuffle's `bypassMergeSort` write path, which is used for shuffles\n+ * for which no Ordering and no Aggregator is given and the number of partitions is\n+ * less than `spark.shuffle.sort.bypassMergeThreshold`.\n+ *\n+ * This path used to be part of [[ExternalSorter]] but was refactored into its own class in order to\n+ * reduce code complexity; see SPARK-7855 for more details.\n+ *\n+ * There have been proposals to completely remove this code path; see SPARK-6026 for details.\n+ */\n+private[spark] class BypassMergeSortShuffleWriter[K, V](\n+    conf: SparkConf,\n+    blockManager: BlockManager,\n+    partitioner: Partitioner,\n+    writeMetrics: ShuffleWriteMetrics,\n+    serializer: Serializer)\n+  extends Logging with SortShuffleFileWriter[K, V] {\n+\n+  private[this] val numPartitions = partitioner.numPartitions\n+\n+  /** Array of file writers for each partition */\n+  private[this] var partitionWriters: Array[BlockObjectWriter] = _\n+\n+  def insertAll(records: Iterator[_ <: Product2[K, V]]): Unit = {\n+    assert (partitionWriters == null)\n+    if (records.hasNext) {\n+      val serInstance = serializer.newInstance()\n+      // Use getSizeAsKb (not bytes) to maintain backwards compatibility if no units are provided\n+      val fileBufferSize = conf.getSizeAsKb(\"spark.shuffle.file.buffer\", \"32k\").toInt * 1024\n+      val openStartTime = System.nanoTime\n+      partitionWriters = Array.fill(numPartitions) {\n+        val (blockId, file) = blockManager.diskBlockManager.createTempShuffleBlock()\n+        val writer =\n+          blockManager.getDiskWriter(blockId, file, serInstance, fileBufferSize, writeMetrics)\n+        writer.open()\n+      }\n+      // Creating the file to write to and creating a disk writer both involve interacting with\n+      // the disk, and can take a long time in aggregate when we open many files, so should be\n+      // included in the shuffle write time.\n+      writeMetrics.incShuffleWriteTime(System.nanoTime - openStartTime)\n+\n+      while (records.hasNext) {\n+        val record = records.next()\n+        val key: K = record._1\n+        partitionWriters(partitioner.getPartition(key)).write(key, record._2)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Write all the data added into this writer into a single file in the disk store. This is\n+   * called by the SortShuffleWriter and can go through an efficient path of just concatenating\n+   * the per-partition binary files.\n+   *\n+   * @param blockId block ID to write to. The index file will be blockId.name + \".index\".\n+   * @param context a TaskContext for a running Spark task, for us to update shuffle metrics.\n+   * @return array of lengths, in bytes, of each partition of the file (used by map output tracker)\n+   */\n+  def writePartitionedFile(blockId: BlockId, context: TaskContext, file: File): Array[Long] = {\n+    if (partitionWriters == null) {\n+      // We were passed an empty iterator\n+      Array.fill(numPartitions)(0L)\n+    } else {\n+      partitionWriters.foreach(_.commitAndClose())\n+\n+      // Track location of each range in the output file\n+      val lengths = new Array[Long](numPartitions)\n+\n+      val transferToEnabled = conf.getBoolean(\"spark.file.transferTo\", true)\n+      val out = new FileOutputStream(file, true)\n+      val writeStartTime = System.nanoTime\n+      Utils.tryWithSafeFinally {\n+        for (i <- 0 until numPartitions) {"
  }],
  "prId": 6397
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "can this ever throw an exception? if yes we should silence it.\n",
    "commit": "bf3f3f6bab69cae01056fd21e2f6d238e0ab707d",
    "createdAt": "2015-05-26T04:56:16Z",
    "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import java.io.{File, FileInputStream, FileOutputStream}\n+\n+import org.apache.spark._\n+import org.apache.spark.executor.ShuffleWriteMetrics\n+import org.apache.spark.serializer.Serializer\n+import org.apache.spark.storage.{BlockId, BlockManager, BlockObjectWriter}\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.collection._\n+\n+/**\n+ * This class handles sort-based shuffle's `bypassMergeSort` write path, which is used for shuffles\n+ * for which no Ordering and no Aggregator is given and the number of partitions is\n+ * less than `spark.shuffle.sort.bypassMergeThreshold`.\n+ *\n+ * This path used to be part of [[ExternalSorter]] but was refactored into its own class in order to\n+ * reduce code complexity; see SPARK-7855 for more details.\n+ *\n+ * There have been proposals to completely remove this code path; see SPARK-6026 for details.\n+ */\n+private[spark] class BypassMergeSortShuffleWriter[K, V](\n+    conf: SparkConf,\n+    blockManager: BlockManager,\n+    partitioner: Partitioner,\n+    writeMetrics: ShuffleWriteMetrics,\n+    serializer: Serializer)\n+  extends Logging with SortShuffleFileWriter[K, V] {\n+\n+  private[this] val numPartitions = partitioner.numPartitions\n+\n+  /** Array of file writers for each partition */\n+  private[this] var partitionWriters: Array[BlockObjectWriter] = _\n+\n+  def insertAll(records: Iterator[_ <: Product2[K, V]]): Unit = {\n+    assert (partitionWriters == null)\n+    if (records.hasNext) {\n+      val serInstance = serializer.newInstance()\n+      // Use getSizeAsKb (not bytes) to maintain backwards compatibility if no units are provided\n+      val fileBufferSize = conf.getSizeAsKb(\"spark.shuffle.file.buffer\", \"32k\").toInt * 1024\n+      val openStartTime = System.nanoTime\n+      partitionWriters = Array.fill(numPartitions) {\n+        val (blockId, file) = blockManager.diskBlockManager.createTempShuffleBlock()\n+        val writer =\n+          blockManager.getDiskWriter(blockId, file, serInstance, fileBufferSize, writeMetrics)\n+        writer.open()\n+      }\n+      // Creating the file to write to and creating a disk writer both involve interacting with\n+      // the disk, and can take a long time in aggregate when we open many files, so should be\n+      // included in the shuffle write time.\n+      writeMetrics.incShuffleWriteTime(System.nanoTime - openStartTime)\n+\n+      while (records.hasNext) {\n+        val record = records.next()\n+        val key: K = record._1\n+        partitionWriters(partitioner.getPartition(key)).write(key, record._2)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Write all the data added into this writer into a single file in the disk store. This is\n+   * called by the SortShuffleWriter and can go through an efficient path of just concatenating\n+   * the per-partition binary files.\n+   *\n+   * @param blockId block ID to write to. The index file will be blockId.name + \".index\".\n+   * @param context a TaskContext for a running Spark task, for us to update shuffle metrics.\n+   * @return array of lengths, in bytes, of each partition of the file (used by map output tracker)\n+   */\n+  def writePartitionedFile(blockId: BlockId, context: TaskContext, file: File): Array[Long] = {\n+    if (partitionWriters == null) {\n+      // We were passed an empty iterator\n+      Array.fill(numPartitions)(0L)\n+    } else {\n+      partitionWriters.foreach(_.commitAndClose())\n+\n+      // Track location of each range in the output file\n+      val lengths = new Array[Long](numPartitions)\n+\n+      val transferToEnabled = conf.getBoolean(\"spark.file.transferTo\", true)\n+      val out = new FileOutputStream(file, true)\n+      val writeStartTime = System.nanoTime\n+      Utils.tryWithSafeFinally {\n+        for (i <- 0 until numPartitions) {\n+          val in = new FileInputStream(partitionWriters(i).fileSegment().file)\n+          Utils.tryWithSafeFinally {\n+            lengths(i) = Utils.copyStream(in, out, closeStreams = false, transferToEnabled)\n+          } {\n+            in.close()\n+          }\n+          if (!blockManager.diskBlockManager.getFile(partitionWriters(i).blockId).delete()) {\n+            logError(\"Unable to delete file for partition i. \")\n+          }\n+        }\n+      } {\n+        out.close()\n+        context.taskMetrics().shuffleWriteMetrics.foreach { m =>\n+          m.incShuffleWriteTime(System.nanoTime - writeStartTime)\n+        }\n+      }\n+\n+      lengths\n+    }\n+  }\n+\n+  def stop(): Unit = {\n+    if (partitionWriters != null) {\n+      partitionWriters.foreach { w =>\n+        w.revertPartialWritesAndClose()"
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "Meh, I'll just rewrite this file in Java so that we have checked exceptions...\n",
    "commit": "bf3f3f6bab69cae01056fd21e2f6d238e0ab707d",
    "createdAt": "2015-05-26T04:57:13Z",
    "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import java.io.{File, FileInputStream, FileOutputStream}\n+\n+import org.apache.spark._\n+import org.apache.spark.executor.ShuffleWriteMetrics\n+import org.apache.spark.serializer.Serializer\n+import org.apache.spark.storage.{BlockId, BlockManager, BlockObjectWriter}\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.collection._\n+\n+/**\n+ * This class handles sort-based shuffle's `bypassMergeSort` write path, which is used for shuffles\n+ * for which no Ordering and no Aggregator is given and the number of partitions is\n+ * less than `spark.shuffle.sort.bypassMergeThreshold`.\n+ *\n+ * This path used to be part of [[ExternalSorter]] but was refactored into its own class in order to\n+ * reduce code complexity; see SPARK-7855 for more details.\n+ *\n+ * There have been proposals to completely remove this code path; see SPARK-6026 for details.\n+ */\n+private[spark] class BypassMergeSortShuffleWriter[K, V](\n+    conf: SparkConf,\n+    blockManager: BlockManager,\n+    partitioner: Partitioner,\n+    writeMetrics: ShuffleWriteMetrics,\n+    serializer: Serializer)\n+  extends Logging with SortShuffleFileWriter[K, V] {\n+\n+  private[this] val numPartitions = partitioner.numPartitions\n+\n+  /** Array of file writers for each partition */\n+  private[this] var partitionWriters: Array[BlockObjectWriter] = _\n+\n+  def insertAll(records: Iterator[_ <: Product2[K, V]]): Unit = {\n+    assert (partitionWriters == null)\n+    if (records.hasNext) {\n+      val serInstance = serializer.newInstance()\n+      // Use getSizeAsKb (not bytes) to maintain backwards compatibility if no units are provided\n+      val fileBufferSize = conf.getSizeAsKb(\"spark.shuffle.file.buffer\", \"32k\").toInt * 1024\n+      val openStartTime = System.nanoTime\n+      partitionWriters = Array.fill(numPartitions) {\n+        val (blockId, file) = blockManager.diskBlockManager.createTempShuffleBlock()\n+        val writer =\n+          blockManager.getDiskWriter(blockId, file, serInstance, fileBufferSize, writeMetrics)\n+        writer.open()\n+      }\n+      // Creating the file to write to and creating a disk writer both involve interacting with\n+      // the disk, and can take a long time in aggregate when we open many files, so should be\n+      // included in the shuffle write time.\n+      writeMetrics.incShuffleWriteTime(System.nanoTime - openStartTime)\n+\n+      while (records.hasNext) {\n+        val record = records.next()\n+        val key: K = record._1\n+        partitionWriters(partitioner.getPartition(key)).write(key, record._2)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Write all the data added into this writer into a single file in the disk store. This is\n+   * called by the SortShuffleWriter and can go through an efficient path of just concatenating\n+   * the per-partition binary files.\n+   *\n+   * @param blockId block ID to write to. The index file will be blockId.name + \".index\".\n+   * @param context a TaskContext for a running Spark task, for us to update shuffle metrics.\n+   * @return array of lengths, in bytes, of each partition of the file (used by map output tracker)\n+   */\n+  def writePartitionedFile(blockId: BlockId, context: TaskContext, file: File): Array[Long] = {\n+    if (partitionWriters == null) {\n+      // We were passed an empty iterator\n+      Array.fill(numPartitions)(0L)\n+    } else {\n+      partitionWriters.foreach(_.commitAndClose())\n+\n+      // Track location of each range in the output file\n+      val lengths = new Array[Long](numPartitions)\n+\n+      val transferToEnabled = conf.getBoolean(\"spark.file.transferTo\", true)\n+      val out = new FileOutputStream(file, true)\n+      val writeStartTime = System.nanoTime\n+      Utils.tryWithSafeFinally {\n+        for (i <- 0 until numPartitions) {\n+          val in = new FileInputStream(partitionWriters(i).fileSegment().file)\n+          Utils.tryWithSafeFinally {\n+            lengths(i) = Utils.copyStream(in, out, closeStreams = false, transferToEnabled)\n+          } {\n+            in.close()\n+          }\n+          if (!blockManager.diskBlockManager.getFile(partitionWriters(i).blockId).delete()) {\n+            logError(\"Unable to delete file for partition i. \")\n+          }\n+        }\n+      } {\n+        out.close()\n+        context.taskMetrics().shuffleWriteMetrics.foreach { m =>\n+          m.incShuffleWriteTime(System.nanoTime - writeStartTime)\n+        }\n+      }\n+\n+      lengths\n+    }\n+  }\n+\n+  def stop(): Unit = {\n+    if (partitionWriters != null) {\n+      partitionWriters.foreach { w =>\n+        w.revertPartialWritesAndClose()"
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "Actually, this method explicitly doesn't throw exceptions.  From its Scaladoc:\n\n```\n  /**\n   * Reverts writes that haven't been flushed yet. Callers should invoke this function\n   * when there are runtime exceptions. This method will not throw, though it may be\n   * unsuccessful in truncating written data.\n   */\n  def revertPartialWritesAndClose()\n```\n",
    "commit": "bf3f3f6bab69cae01056fd21e2f6d238e0ab707d",
    "createdAt": "2015-05-26T06:09:47Z",
    "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import java.io.{File, FileInputStream, FileOutputStream}\n+\n+import org.apache.spark._\n+import org.apache.spark.executor.ShuffleWriteMetrics\n+import org.apache.spark.serializer.Serializer\n+import org.apache.spark.storage.{BlockId, BlockManager, BlockObjectWriter}\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.collection._\n+\n+/**\n+ * This class handles sort-based shuffle's `bypassMergeSort` write path, which is used for shuffles\n+ * for which no Ordering and no Aggregator is given and the number of partitions is\n+ * less than `spark.shuffle.sort.bypassMergeThreshold`.\n+ *\n+ * This path used to be part of [[ExternalSorter]] but was refactored into its own class in order to\n+ * reduce code complexity; see SPARK-7855 for more details.\n+ *\n+ * There have been proposals to completely remove this code path; see SPARK-6026 for details.\n+ */\n+private[spark] class BypassMergeSortShuffleWriter[K, V](\n+    conf: SparkConf,\n+    blockManager: BlockManager,\n+    partitioner: Partitioner,\n+    writeMetrics: ShuffleWriteMetrics,\n+    serializer: Serializer)\n+  extends Logging with SortShuffleFileWriter[K, V] {\n+\n+  private[this] val numPartitions = partitioner.numPartitions\n+\n+  /** Array of file writers for each partition */\n+  private[this] var partitionWriters: Array[BlockObjectWriter] = _\n+\n+  def insertAll(records: Iterator[_ <: Product2[K, V]]): Unit = {\n+    assert (partitionWriters == null)\n+    if (records.hasNext) {\n+      val serInstance = serializer.newInstance()\n+      // Use getSizeAsKb (not bytes) to maintain backwards compatibility if no units are provided\n+      val fileBufferSize = conf.getSizeAsKb(\"spark.shuffle.file.buffer\", \"32k\").toInt * 1024\n+      val openStartTime = System.nanoTime\n+      partitionWriters = Array.fill(numPartitions) {\n+        val (blockId, file) = blockManager.diskBlockManager.createTempShuffleBlock()\n+        val writer =\n+          blockManager.getDiskWriter(blockId, file, serInstance, fileBufferSize, writeMetrics)\n+        writer.open()\n+      }\n+      // Creating the file to write to and creating a disk writer both involve interacting with\n+      // the disk, and can take a long time in aggregate when we open many files, so should be\n+      // included in the shuffle write time.\n+      writeMetrics.incShuffleWriteTime(System.nanoTime - openStartTime)\n+\n+      while (records.hasNext) {\n+        val record = records.next()\n+        val key: K = record._1\n+        partitionWriters(partitioner.getPartition(key)).write(key, record._2)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Write all the data added into this writer into a single file in the disk store. This is\n+   * called by the SortShuffleWriter and can go through an efficient path of just concatenating\n+   * the per-partition binary files.\n+   *\n+   * @param blockId block ID to write to. The index file will be blockId.name + \".index\".\n+   * @param context a TaskContext for a running Spark task, for us to update shuffle metrics.\n+   * @return array of lengths, in bytes, of each partition of the file (used by map output tracker)\n+   */\n+  def writePartitionedFile(blockId: BlockId, context: TaskContext, file: File): Array[Long] = {\n+    if (partitionWriters == null) {\n+      // We were passed an empty iterator\n+      Array.fill(numPartitions)(0L)\n+    } else {\n+      partitionWriters.foreach(_.commitAndClose())\n+\n+      // Track location of each range in the output file\n+      val lengths = new Array[Long](numPartitions)\n+\n+      val transferToEnabled = conf.getBoolean(\"spark.file.transferTo\", true)\n+      val out = new FileOutputStream(file, true)\n+      val writeStartTime = System.nanoTime\n+      Utils.tryWithSafeFinally {\n+        for (i <- 0 until numPartitions) {\n+          val in = new FileInputStream(partitionWriters(i).fileSegment().file)\n+          Utils.tryWithSafeFinally {\n+            lengths(i) = Utils.copyStream(in, out, closeStreams = false, transferToEnabled)\n+          } {\n+            in.close()\n+          }\n+          if (!blockManager.diskBlockManager.getFile(partitionWriters(i).blockId).delete()) {\n+            logError(\"Unable to delete file for partition i. \")\n+          }\n+        }\n+      } {\n+        out.close()\n+        context.taskMetrics().shuffleWriteMetrics.foreach { m =>\n+          m.incShuffleWriteTime(System.nanoTime - writeStartTime)\n+        }\n+      }\n+\n+      lengths\n+    }\n+  }\n+\n+  def stop(): Unit = {\n+    if (partitionWriters != null) {\n+      partitionWriters.foreach { w =>\n+        w.revertPartialWritesAndClose()"
  }],
  "prId": 6397
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "one thing missing from this comment is what does bypassmergesort actually means.\n",
    "commit": "bf3f3f6bab69cae01056fd21e2f6d238e0ab707d",
    "createdAt": "2015-05-26T06:07:55Z",
    "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import java.io.{File, FileInputStream, FileOutputStream}\n+\n+import org.apache.spark._\n+import org.apache.spark.executor.ShuffleWriteMetrics\n+import org.apache.spark.serializer.Serializer\n+import org.apache.spark.storage.{BlockId, BlockManager, BlockObjectWriter}\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.collection._\n+\n+/**\n+ * This class handles sort-based shuffle's `bypassMergeSort` write path, which is used for shuffles"
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "I'll rewrite this Scaladoc to provide a better explanation and to also address how this is different than HashShuffleWriter (the key difference is the fact that this output in a format that can be served with IndexShuffleBlockResolver).\n",
    "commit": "bf3f3f6bab69cae01056fd21e2f6d238e0ab707d",
    "createdAt": "2015-05-26T06:12:18Z",
    "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import java.io.{File, FileInputStream, FileOutputStream}\n+\n+import org.apache.spark._\n+import org.apache.spark.executor.ShuffleWriteMetrics\n+import org.apache.spark.serializer.Serializer\n+import org.apache.spark.storage.{BlockId, BlockManager, BlockObjectWriter}\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.collection._\n+\n+/**\n+ * This class handles sort-based shuffle's `bypassMergeSort` write path, which is used for shuffles"
  }],
  "prId": 6397
}]