[{
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "this is fine for wip - but for the final version let's make sure we have sufficient high level documentation in the form of javadoc\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-07T00:09:32Z",
    "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe\n+\n+import org.apache.spark._\n+import org.apache.spark.serializer.Serializer\n+import org.apache.spark.shuffle._\n+import org.apache.spark.shuffle.sort.SortShuffleManager\n+\n+private class UnsafeShuffleHandle[K, V](\n+    shuffleId: Int,\n+    override val numMaps: Int,\n+    override val dependency: ShuffleDependency[K, V, V])\n+  extends BaseShuffleHandle(shuffleId, numMaps, dependency) {\n+}\n+\n+private[spark] object UnsafeShuffleManager extends Logging {\n+  def canUseUnsafeShuffle[K, V, C](dependency: ShuffleDependency[K, V, C]): Boolean = {\n+    val shufId = dependency.shuffleId\n+    val serializer = Serializer.getSerializer(dependency.serializer)\n+    if (!serializer.supportsRelocationOfSerializedObjects) {\n+      log.debug(s\"Can't use UnsafeShuffle for shuffle $shufId because the serializer, \" +\n+        s\"${serializer.getClass.getName}, does not support object relocation\")\n+      false\n+    } else if (dependency.aggregator.isDefined) {\n+      log.debug(s\"Can't use UnsafeShuffle for shuffle $shufId because an aggregator is defined\")\n+      false\n+    } else if (dependency.keyOrdering.isDefined) {\n+      log.debug(s\"Can't use UnsafeShuffle for shuffle $shufId because a key ordering is defined\")\n+      false\n+    } else {\n+      log.debug(s\"Can use UnsafeShuffle for shuffle $shufId\")\n+      true\n+    }\n+  }\n+}\n+\n+private[spark] class UnsafeShuffleManager(conf: SparkConf) extends ShuffleManager {"
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "Yep, this is the next piece that I'm writing docs for :)\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-07T00:10:24Z",
    "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe\n+\n+import org.apache.spark._\n+import org.apache.spark.serializer.Serializer\n+import org.apache.spark.shuffle._\n+import org.apache.spark.shuffle.sort.SortShuffleManager\n+\n+private class UnsafeShuffleHandle[K, V](\n+    shuffleId: Int,\n+    override val numMaps: Int,\n+    override val dependency: ShuffleDependency[K, V, V])\n+  extends BaseShuffleHandle(shuffleId, numMaps, dependency) {\n+}\n+\n+private[spark] object UnsafeShuffleManager extends Logging {\n+  def canUseUnsafeShuffle[K, V, C](dependency: ShuffleDependency[K, V, C]): Boolean = {\n+    val shufId = dependency.shuffleId\n+    val serializer = Serializer.getSerializer(dependency.serializer)\n+    if (!serializer.supportsRelocationOfSerializedObjects) {\n+      log.debug(s\"Can't use UnsafeShuffle for shuffle $shufId because the serializer, \" +\n+        s\"${serializer.getClass.getName}, does not support object relocation\")\n+      false\n+    } else if (dependency.aggregator.isDefined) {\n+      log.debug(s\"Can't use UnsafeShuffle for shuffle $shufId because an aggregator is defined\")\n+      false\n+    } else if (dependency.keyOrdering.isDefined) {\n+      log.debug(s\"Can't use UnsafeShuffle for shuffle $shufId because a key ordering is defined\")\n+      false\n+    } else {\n+      log.debug(s\"Can use UnsafeShuffle for shuffle $shufId\")\n+      true\n+    }\n+  }\n+}\n+\n+private[spark] class UnsafeShuffleManager(conf: SparkConf) extends ShuffleManager {"
  }],
  "prId": 5868
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "I propose to use `log.warn` in `canUseUnsafeShuffle`. It would be much easier for people to compare the performances of `UnsafeShuffleManager` and `SortShuffleManager`. They usually need to know the new `UnsafeShuffleHandle` does take effect.\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-11T23:36:53Z",
    "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe\n+\n+import org.apache.spark._\n+import org.apache.spark.serializer.Serializer\n+import org.apache.spark.shuffle._\n+import org.apache.spark.shuffle.sort.SortShuffleManager\n+\n+/**\n+ * Subclass of [[BaseShuffleHandle]], used to identify when we've chosen to use the new shuffle.\n+ */\n+private class UnsafeShuffleHandle[K, V](\n+    shuffleId: Int,\n+    override val numMaps: Int,\n+    override val dependency: ShuffleDependency[K, V, V])\n+  extends BaseShuffleHandle(shuffleId, numMaps, dependency) {\n+}\n+\n+private[spark] object UnsafeShuffleManager extends Logging {\n+  /**\n+   * Helper method for determining whether a shuffle should use the optimized unsafe shuffle\n+   * path or whether it should fall back to the original sort-based shuffle.\n+   */\n+  def canUseUnsafeShuffle[K, V, C](dependency: ShuffleDependency[K, V, C]): Boolean = {\n+    val shufId = dependency.shuffleId\n+    val serializer = Serializer.getSerializer(dependency.serializer)\n+    if (!serializer.supportsRelocationOfSerializedObjects) {\n+      log.debug(s\"Can't use UnsafeShuffle for shuffle $shufId because the serializer, \" +",
    "line": 53
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "I considered this, but I worry that this will result in extremely chatty logs because many operations won't be able to use this new shuffle yet.  For example, this would trigger a warning whenever `reduceByKey` is used.\n\nThis is a tricky issue, especially as the number of special-case shuffle optimizations grows.  It will be very easy for users to slightly change their programs in ways that trigger slower code paths (e.g. by switching from LZF to LZ4 compression).  Conversely, this also creates the potential for small changes to result in huge secondary performance benefits in non-obvious ways: if a user were to switch from LZ4 to LZF, then the current code would hit a more efficient shuffle merge path and might exhibit huge speed-ups, but a user might misattribute this to LZF being faster / offering better compression in general, whereas it's really the optimized merge path that's activated by LZF's concatenatibility that is responsible for the speed up.  This is a general issue that's probably worth exploring as part of a broader discussion of how to expose internal knowledge of performance optimizations back to end users.\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-11T23:49:53Z",
    "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe\n+\n+import org.apache.spark._\n+import org.apache.spark.serializer.Serializer\n+import org.apache.spark.shuffle._\n+import org.apache.spark.shuffle.sort.SortShuffleManager\n+\n+/**\n+ * Subclass of [[BaseShuffleHandle]], used to identify when we've chosen to use the new shuffle.\n+ */\n+private class UnsafeShuffleHandle[K, V](\n+    shuffleId: Int,\n+    override val numMaps: Int,\n+    override val dependency: ShuffleDependency[K, V, V])\n+  extends BaseShuffleHandle(shuffleId, numMaps, dependency) {\n+}\n+\n+private[spark] object UnsafeShuffleManager extends Logging {\n+  /**\n+   * Helper method for determining whether a shuffle should use the optimized unsafe shuffle\n+   * path or whether it should fall back to the original sort-based shuffle.\n+   */\n+  def canUseUnsafeShuffle[K, V, C](dependency: ShuffleDependency[K, V, C]): Boolean = {\n+    val shufId = dependency.shuffleId\n+    val serializer = Serializer.getSerializer(dependency.serializer)\n+    if (!serializer.supportsRelocationOfSerializedObjects) {\n+      log.debug(s\"Can't use UnsafeShuffle for shuffle $shufId because the serializer, \" +",
    "line": 53
  }],
  "prId": 5868
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "`override val` is redundant\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-11T23:38:58Z",
    "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe\n+\n+import org.apache.spark._\n+import org.apache.spark.serializer.Serializer\n+import org.apache.spark.shuffle._\n+import org.apache.spark.shuffle.sort.SortShuffleManager\n+\n+/**\n+ * Subclass of [[BaseShuffleHandle]], used to identify when we've chosen to use the new shuffle.\n+ */\n+private class UnsafeShuffleHandle[K, V](\n+    shuffleId: Int,\n+    override val numMaps: Int,"
  }],
  "prId": 5868
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "`override val` is redundant\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-11T23:39:02Z",
    "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe\n+\n+import org.apache.spark._\n+import org.apache.spark.serializer.Serializer\n+import org.apache.spark.shuffle._\n+import org.apache.spark.shuffle.sort.SortShuffleManager\n+\n+/**\n+ * Subclass of [[BaseShuffleHandle]], used to identify when we've chosen to use the new shuffle.\n+ */\n+private class UnsafeShuffleHandle[K, V](\n+    shuffleId: Int,\n+    override val numMaps: Int,\n+    override val dependency: ShuffleDependency[K, V, V])"
  }],
  "prId": 5868
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "Here should be updated to MAXIMUM_PARTITION_ID + 1\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-12T05:52:33Z",
    "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe\n+\n+import org.apache.spark._\n+import org.apache.spark.serializer.Serializer\n+import org.apache.spark.shuffle._\n+import org.apache.spark.shuffle.sort.SortShuffleManager\n+\n+/**\n+ * Subclass of [[BaseShuffleHandle]], used to identify when we've chosen to use the new shuffle.\n+ */\n+private class UnsafeShuffleHandle[K, V](\n+    shuffleId: Int,\n+    numMaps: Int,\n+    dependency: ShuffleDependency[K, V, V])\n+  extends BaseShuffleHandle(shuffleId, numMaps, dependency) {\n+}\n+\n+private[spark] object UnsafeShuffleManager extends Logging {\n+  /**\n+   * Helper method for determining whether a shuffle should use the optimized unsafe shuffle\n+   * path or whether it should fall back to the original sort-based shuffle.\n+   */\n+  def canUseUnsafeShuffle[K, V, C](dependency: ShuffleDependency[K, V, C]): Boolean = {\n+    val shufId = dependency.shuffleId\n+    val serializer = Serializer.getSerializer(dependency.serializer)\n+    if (!serializer.supportsRelocationOfSerializedObjects) {\n+      log.debug(s\"Can't use UnsafeShuffle for shuffle $shufId because the serializer, \" +\n+        s\"${serializer.getClass.getName}, does not support object relocation\")\n+      false\n+    } else if (dependency.aggregator.isDefined) {\n+      log.debug(s\"Can't use UnsafeShuffle for shuffle $shufId because an aggregator is defined\")\n+      false\n+    } else if (dependency.keyOrdering.isDefined) {\n+      log.debug(s\"Can't use UnsafeShuffle for shuffle $shufId because a key ordering is defined\")\n+      false\n+    } else if (dependency.partitioner.numPartitions > PackedRecordPointer.MAXIMUM_PARTITION_ID) {"
  }],
  "prId": 5868
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "Here should be updated to MAXIMUM_PARTITION_ID + 1\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-12T05:52:37Z",
    "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe\n+\n+import org.apache.spark._\n+import org.apache.spark.serializer.Serializer\n+import org.apache.spark.shuffle._\n+import org.apache.spark.shuffle.sort.SortShuffleManager\n+\n+/**\n+ * Subclass of [[BaseShuffleHandle]], used to identify when we've chosen to use the new shuffle.\n+ */\n+private class UnsafeShuffleHandle[K, V](\n+    shuffleId: Int,\n+    numMaps: Int,\n+    dependency: ShuffleDependency[K, V, V])\n+  extends BaseShuffleHandle(shuffleId, numMaps, dependency) {\n+}\n+\n+private[spark] object UnsafeShuffleManager extends Logging {\n+  /**\n+   * Helper method for determining whether a shuffle should use the optimized unsafe shuffle\n+   * path or whether it should fall back to the original sort-based shuffle.\n+   */\n+  def canUseUnsafeShuffle[K, V, C](dependency: ShuffleDependency[K, V, C]): Boolean = {\n+    val shufId = dependency.shuffleId\n+    val serializer = Serializer.getSerializer(dependency.serializer)\n+    if (!serializer.supportsRelocationOfSerializedObjects) {\n+      log.debug(s\"Can't use UnsafeShuffle for shuffle $shufId because the serializer, \" +\n+        s\"${serializer.getClass.getName}, does not support object relocation\")\n+      false\n+    } else if (dependency.aggregator.isDefined) {\n+      log.debug(s\"Can't use UnsafeShuffle for shuffle $shufId because an aggregator is defined\")\n+      false\n+    } else if (dependency.keyOrdering.isDefined) {\n+      log.debug(s\"Can't use UnsafeShuffle for shuffle $shufId because a key ordering is defined\")\n+      false\n+    } else if (dependency.partitioner.numPartitions > PackedRecordPointer.MAXIMUM_PARTITION_ID) {\n+      log.debug(s\"Can't use UnsafeShuffle for shuffle $shufId because it has more than \" +\n+        s\"${PackedRecordPointer.MAXIMUM_PARTITION_ID} partitions\")"
  }],
  "prId": 5868
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "I think that this is the last major TODO to finish before this patch is good to go.  As written here, the code will clean up shuffles that fall back to the old code path, but not shuffles that use the new path.  I'll have to implement logic similar to what we have in SortShuffleManager to perform proper cleanup.  One gotcha is that I'll probably need to keep a map and a set so that we can determine whether a shuffle fell back to the old code path (in which case we'll just call sortShuffleManager.unregisterShuffle) or used the new path (in which case we'll instruct our shuffle block resolver to clean up for that shuffle).\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-13T06:17:35Z",
    "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe\n+\n+import org.apache.spark._\n+import org.apache.spark.serializer.Serializer\n+import org.apache.spark.shuffle._\n+import org.apache.spark.shuffle.sort.SortShuffleManager\n+\n+/**\n+ * Subclass of [[BaseShuffleHandle]], used to identify when we've chosen to use the new shuffle.\n+ */\n+private class UnsafeShuffleHandle[K, V](\n+    shuffleId: Int,\n+    numMaps: Int,\n+    dependency: ShuffleDependency[K, V, V])\n+  extends BaseShuffleHandle(shuffleId, numMaps, dependency) {\n+}\n+\n+private[spark] object UnsafeShuffleManager extends Logging {\n+\n+  /**\n+   * The maximum number of shuffle output partitions that UnsafeShuffleManager supports.\n+   */\n+  val MAX_SHUFFLE_OUTPUT_PARTITIONS = PackedRecordPointer.MAXIMUM_PARTITION_ID + 1\n+\n+  /**\n+   * Helper method for determining whether a shuffle should use the optimized unsafe shuffle\n+   * path or whether it should fall back to the original sort-based shuffle.\n+   */\n+  def canUseUnsafeShuffle[K, V, C](dependency: ShuffleDependency[K, V, C]): Boolean = {\n+    val shufId = dependency.shuffleId\n+    val serializer = Serializer.getSerializer(dependency.serializer)\n+    if (!serializer.supportsRelocationOfSerializedObjects) {\n+      log.debug(s\"Can't use UnsafeShuffle for shuffle $shufId because the serializer, \" +\n+        s\"${serializer.getClass.getName}, does not support object relocation\")\n+      false\n+    } else if (dependency.aggregator.isDefined) {\n+      log.debug(s\"Can't use UnsafeShuffle for shuffle $shufId because an aggregator is defined\")\n+      false\n+    } else if (dependency.keyOrdering.isDefined) {\n+      log.debug(s\"Can't use UnsafeShuffle for shuffle $shufId because a key ordering is defined\")\n+      false\n+    } else if (dependency.partitioner.numPartitions > MAX_SHUFFLE_OUTPUT_PARTITIONS) {\n+      log.debug(s\"Can't use UnsafeShuffle for shuffle $shufId because it has more than \" +\n+        s\"$MAX_SHUFFLE_OUTPUT_PARTITIONS partitions\")\n+      false\n+    } else {\n+      log.debug(s\"Can use UnsafeShuffle for shuffle $shufId\")\n+      true\n+    }\n+  }\n+}\n+\n+/**\n+ * A shuffle implementation that uses directly-managed memory to implement several performance\n+ * optimizations for certain types of shuffles. In cases where the new performance optimizations\n+ * cannot be applied, this shuffle manager delegates to [[SortShuffleManager]] to handle those\n+ * shuffles.\n+ *\n+ * UnsafeShuffleManager's optimizations will apply when _all_ of the following conditions hold:\n+ *\n+ *  - The shuffle dependency specifies no aggregation or output ordering.\n+ *  - The shuffle serializer supports relocation of serialized values (this is currently supported\n+ *    by KryoSerializer and Spark SQL's custom serializers).\n+ *  - The shuffle produces fewer than 16777216 output partitions.\n+ *  - No individual record is larger than 128 MB when serialized.\n+ *\n+ * In addition, extra spill-merging optimizations are automatically applied when the shuffle\n+ * compression codec supports concatenation of serialized streams. This is currently supported by\n+ * Spark's LZF serializer.\n+ *\n+ * At a high-level, UnsafeShuffleManager's design is similar to Spark's existing SortShuffleManager.\n+ * In sort-based shuffle, incoming records are sorted according to their target partition ids, then\n+ * written to a single map output file. Reducers fetch contiguous regions of this file in order to\n+ * read their portion of the map output. In cases where the map output data is too large to fit in\n+ * memory, sorted subsets of the output can are spilled to disk and those on-disk files are merged\n+ * to produce the final output file.\n+ *\n+ * UnsafeShuffleManager optimizes this process in several ways:\n+ *\n+ *  - Its sort operates on serialized binary data rather than Java objects, which reduces memory\n+ *    consumption and GC overheads. This optimization requires the record serializer to have certain\n+ *    properties to allow serialized records to be re-ordered without requiring deserialization.\n+ *    See SPARK-4550, where this optimization was first proposed and implemented, for more details.\n+ *\n+ *  - It uses a specialized cache-efficient sorter ([[UnsafeShuffleExternalSorter]]) that sorts\n+ *    arrays of compressed record pointers and partition ids. By using only 8 bytes of space per\n+ *    record in the sorting array, this fits more of the array into cache.\n+ *\n+ *  - The spill merging procedure operates on blocks of serialized records that belong to the same\n+ *    partition and does not need to deserialize records during the merge.\n+ *\n+ *  - When the spill compression codec supports concatenation of compressed data, the spill merge\n+ *    simply concatenates the serialized and compressed spill partitions to produce the final output\n+ *    partition.  This allows efficient data copying methods, like NIO's `transferTo`, to be used\n+ *    and avoids the need to allocate decompression or copying buffers during the merge.\n+ *\n+ * For more details on UnsafeShuffleManager's design, see SPARK-7081.\n+ */\n+private[spark] class UnsafeShuffleManager(conf: SparkConf) extends ShuffleManager with Logging {\n+\n+  if (!conf.getBoolean(\"spark.shuffle.spill\", true)) {\n+    logWarning(\n+      \"spark.shuffle.spill was set to false, but this is ignored by the tungsten-sort shuffle \" +\n+      \"manager; its optimized shuffles will continue to spill to disk when necessary.\")\n+  }\n+\n+\n+  private[this] val sortShuffleManager: SortShuffleManager = new SortShuffleManager(conf)\n+\n+  /**\n+   * Register a shuffle with the manager and obtain a handle for it to pass to tasks.\n+   */\n+  override def registerShuffle[K, V, C](\n+      shuffleId: Int,\n+      numMaps: Int,\n+      dependency: ShuffleDependency[K, V, C]): ShuffleHandle = {\n+    if (UnsafeShuffleManager.canUseUnsafeShuffle(dependency)) {\n+      new UnsafeShuffleHandle[K, V](\n+        shuffleId, numMaps, dependency.asInstanceOf[ShuffleDependency[K, V, V]])\n+    } else {\n+      new BaseShuffleHandle(shuffleId, numMaps, dependency)\n+    }\n+  }\n+\n+  /**\n+   * Get a reader for a range of reduce partitions (startPartition to endPartition-1, inclusive).\n+   * Called on executors by reduce tasks.\n+   */\n+  override def getReader[K, C](\n+      handle: ShuffleHandle,\n+      startPartition: Int,\n+      endPartition: Int,\n+      context: TaskContext): ShuffleReader[K, C] = {\n+    sortShuffleManager.getReader(handle, startPartition, endPartition, context)\n+  }\n+\n+  /** Get a writer for a given partition. Called on executors by map tasks. */\n+  override def getWriter[K, V](\n+      handle: ShuffleHandle,\n+      mapId: Int,\n+      context: TaskContext): ShuffleWriter[K, V] = {\n+    handle match {\n+      case unsafeShuffleHandle: UnsafeShuffleHandle[K, V] =>\n+        val env = SparkEnv.get\n+        // TODO: do we need to do anything to register the shuffle here?\n+        new UnsafeShuffleWriter(\n+          env.blockManager,\n+          shuffleBlockResolver.asInstanceOf[IndexShuffleBlockResolver],\n+          context.taskMemoryManager(),\n+          env.shuffleMemoryManager,\n+          unsafeShuffleHandle,\n+          mapId,\n+          context,\n+          env.conf)\n+      case other =>\n+        sortShuffleManager.getWriter(handle, mapId, context)\n+    }\n+  }\n+\n+  /** Remove a shuffle's metadata from the ShuffleManager. */\n+  override def unregisterShuffle(shuffleId: Int): Boolean = {\n+    // TODO: need to do something here for our unsafe path"
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "This is also a little tricky to test properly.  One approach that I'll explore is an end-to-end test that creates a fresh SparkContext, runs a single shuffle, keeps a handle to the shuffled RDD so that ContextCleaner doesn't reclaim the shuffle dependency, then builds a list of on-disk files that were created as a result of the shuffle (by scanning through `spark.local.dir`).  After gathering the list of files that should be cleaned up, I can manually call the BlockManagerMaster method to remove the shuffle, then assert that the shuffle files were actually cleaned up.\n",
    "commit": "ef0a86e41e9b390e6c0d60a6ed2105dbc54431f7",
    "createdAt": "2015-05-13T06:22:55Z",
    "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.unsafe\n+\n+import org.apache.spark._\n+import org.apache.spark.serializer.Serializer\n+import org.apache.spark.shuffle._\n+import org.apache.spark.shuffle.sort.SortShuffleManager\n+\n+/**\n+ * Subclass of [[BaseShuffleHandle]], used to identify when we've chosen to use the new shuffle.\n+ */\n+private class UnsafeShuffleHandle[K, V](\n+    shuffleId: Int,\n+    numMaps: Int,\n+    dependency: ShuffleDependency[K, V, V])\n+  extends BaseShuffleHandle(shuffleId, numMaps, dependency) {\n+}\n+\n+private[spark] object UnsafeShuffleManager extends Logging {\n+\n+  /**\n+   * The maximum number of shuffle output partitions that UnsafeShuffleManager supports.\n+   */\n+  val MAX_SHUFFLE_OUTPUT_PARTITIONS = PackedRecordPointer.MAXIMUM_PARTITION_ID + 1\n+\n+  /**\n+   * Helper method for determining whether a shuffle should use the optimized unsafe shuffle\n+   * path or whether it should fall back to the original sort-based shuffle.\n+   */\n+  def canUseUnsafeShuffle[K, V, C](dependency: ShuffleDependency[K, V, C]): Boolean = {\n+    val shufId = dependency.shuffleId\n+    val serializer = Serializer.getSerializer(dependency.serializer)\n+    if (!serializer.supportsRelocationOfSerializedObjects) {\n+      log.debug(s\"Can't use UnsafeShuffle for shuffle $shufId because the serializer, \" +\n+        s\"${serializer.getClass.getName}, does not support object relocation\")\n+      false\n+    } else if (dependency.aggregator.isDefined) {\n+      log.debug(s\"Can't use UnsafeShuffle for shuffle $shufId because an aggregator is defined\")\n+      false\n+    } else if (dependency.keyOrdering.isDefined) {\n+      log.debug(s\"Can't use UnsafeShuffle for shuffle $shufId because a key ordering is defined\")\n+      false\n+    } else if (dependency.partitioner.numPartitions > MAX_SHUFFLE_OUTPUT_PARTITIONS) {\n+      log.debug(s\"Can't use UnsafeShuffle for shuffle $shufId because it has more than \" +\n+        s\"$MAX_SHUFFLE_OUTPUT_PARTITIONS partitions\")\n+      false\n+    } else {\n+      log.debug(s\"Can use UnsafeShuffle for shuffle $shufId\")\n+      true\n+    }\n+  }\n+}\n+\n+/**\n+ * A shuffle implementation that uses directly-managed memory to implement several performance\n+ * optimizations for certain types of shuffles. In cases where the new performance optimizations\n+ * cannot be applied, this shuffle manager delegates to [[SortShuffleManager]] to handle those\n+ * shuffles.\n+ *\n+ * UnsafeShuffleManager's optimizations will apply when _all_ of the following conditions hold:\n+ *\n+ *  - The shuffle dependency specifies no aggregation or output ordering.\n+ *  - The shuffle serializer supports relocation of serialized values (this is currently supported\n+ *    by KryoSerializer and Spark SQL's custom serializers).\n+ *  - The shuffle produces fewer than 16777216 output partitions.\n+ *  - No individual record is larger than 128 MB when serialized.\n+ *\n+ * In addition, extra spill-merging optimizations are automatically applied when the shuffle\n+ * compression codec supports concatenation of serialized streams. This is currently supported by\n+ * Spark's LZF serializer.\n+ *\n+ * At a high-level, UnsafeShuffleManager's design is similar to Spark's existing SortShuffleManager.\n+ * In sort-based shuffle, incoming records are sorted according to their target partition ids, then\n+ * written to a single map output file. Reducers fetch contiguous regions of this file in order to\n+ * read their portion of the map output. In cases where the map output data is too large to fit in\n+ * memory, sorted subsets of the output can are spilled to disk and those on-disk files are merged\n+ * to produce the final output file.\n+ *\n+ * UnsafeShuffleManager optimizes this process in several ways:\n+ *\n+ *  - Its sort operates on serialized binary data rather than Java objects, which reduces memory\n+ *    consumption and GC overheads. This optimization requires the record serializer to have certain\n+ *    properties to allow serialized records to be re-ordered without requiring deserialization.\n+ *    See SPARK-4550, where this optimization was first proposed and implemented, for more details.\n+ *\n+ *  - It uses a specialized cache-efficient sorter ([[UnsafeShuffleExternalSorter]]) that sorts\n+ *    arrays of compressed record pointers and partition ids. By using only 8 bytes of space per\n+ *    record in the sorting array, this fits more of the array into cache.\n+ *\n+ *  - The spill merging procedure operates on blocks of serialized records that belong to the same\n+ *    partition and does not need to deserialize records during the merge.\n+ *\n+ *  - When the spill compression codec supports concatenation of compressed data, the spill merge\n+ *    simply concatenates the serialized and compressed spill partitions to produce the final output\n+ *    partition.  This allows efficient data copying methods, like NIO's `transferTo`, to be used\n+ *    and avoids the need to allocate decompression or copying buffers during the merge.\n+ *\n+ * For more details on UnsafeShuffleManager's design, see SPARK-7081.\n+ */\n+private[spark] class UnsafeShuffleManager(conf: SparkConf) extends ShuffleManager with Logging {\n+\n+  if (!conf.getBoolean(\"spark.shuffle.spill\", true)) {\n+    logWarning(\n+      \"spark.shuffle.spill was set to false, but this is ignored by the tungsten-sort shuffle \" +\n+      \"manager; its optimized shuffles will continue to spill to disk when necessary.\")\n+  }\n+\n+\n+  private[this] val sortShuffleManager: SortShuffleManager = new SortShuffleManager(conf)\n+\n+  /**\n+   * Register a shuffle with the manager and obtain a handle for it to pass to tasks.\n+   */\n+  override def registerShuffle[K, V, C](\n+      shuffleId: Int,\n+      numMaps: Int,\n+      dependency: ShuffleDependency[K, V, C]): ShuffleHandle = {\n+    if (UnsafeShuffleManager.canUseUnsafeShuffle(dependency)) {\n+      new UnsafeShuffleHandle[K, V](\n+        shuffleId, numMaps, dependency.asInstanceOf[ShuffleDependency[K, V, V]])\n+    } else {\n+      new BaseShuffleHandle(shuffleId, numMaps, dependency)\n+    }\n+  }\n+\n+  /**\n+   * Get a reader for a range of reduce partitions (startPartition to endPartition-1, inclusive).\n+   * Called on executors by reduce tasks.\n+   */\n+  override def getReader[K, C](\n+      handle: ShuffleHandle,\n+      startPartition: Int,\n+      endPartition: Int,\n+      context: TaskContext): ShuffleReader[K, C] = {\n+    sortShuffleManager.getReader(handle, startPartition, endPartition, context)\n+  }\n+\n+  /** Get a writer for a given partition. Called on executors by map tasks. */\n+  override def getWriter[K, V](\n+      handle: ShuffleHandle,\n+      mapId: Int,\n+      context: TaskContext): ShuffleWriter[K, V] = {\n+    handle match {\n+      case unsafeShuffleHandle: UnsafeShuffleHandle[K, V] =>\n+        val env = SparkEnv.get\n+        // TODO: do we need to do anything to register the shuffle here?\n+        new UnsafeShuffleWriter(\n+          env.blockManager,\n+          shuffleBlockResolver.asInstanceOf[IndexShuffleBlockResolver],\n+          context.taskMemoryManager(),\n+          env.shuffleMemoryManager,\n+          unsafeShuffleHandle,\n+          mapId,\n+          context,\n+          env.conf)\n+      case other =>\n+        sortShuffleManager.getWriter(handle, mapId, context)\n+    }\n+  }\n+\n+  /** Remove a shuffle's metadata from the ShuffleManager. */\n+  override def unregisterShuffle(shuffleId: Int): Boolean = {\n+    // TODO: need to do something here for our unsafe path"
  }],
  "prId": 5868
}]