[{
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: stray empty line.\n",
    "commit": "80e037dcb731a918d516eb02c12440d8ba3e71a7",
    "createdAt": "2015-11-02T18:58:14Z",
    "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.shuffle\n+\n+import java.io.{FileOutputStream, FileInputStream, File}\n+\n+import com.google.common.annotations.VisibleForTesting\n+\n+import org.apache.spark.storage.ShuffleMapStatusBlockId\n+import org.apache.spark.{SparkEnv, Logging}\n+import org.apache.spark.scheduler.MapStatus\n+import org.apache.spark.serializer.SerializerInstance\n+\n+/**\n+ * Ensures that on each executor, there are no conflicting writes to the same shuffle files.  It\n+ * implements \"first write wins\", by atomically moving all shuffle files into their final location,\n+ * only if the files did not already exist. See SPARK-8029\n+ */\n+private[spark] object ShuffleOutputCoordinator extends Logging {\n+\n+  /**\n+   * If any of the destination files do not exist, then move all of the temporary files to their\n+   * destinations, and return (true, the given MapStatus).  If all destination files exist, then\n+   * delete all temporary files, and return (false, the MapStatus from previously committed shuffle\n+   * output).\n+   *\n+   * Note that this will write to all destination files.  If the tmp file is missing, then a\n+   * zero-length destination file will be created.  This is so the ShuffleOutputCoordinator can work\n+   * even when there is a non-determinstic data, where the output exists in one attempt, but is\n+   * empty in another attempt.\n+   *\n+   * @param tmpToDest  Seq of (temporary, destination) file pairs\n+   * @param mapStatus the [[MapStatus]] for the output already written to the the temporary files\n+   * @return pair of: (1) true iff the set of temporary files was moved to the destination and (2)\n+   *         the MapStatus of the committed attempt.\n+   *"
  }],
  "prId": 9214
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: it's\n",
    "commit": "80e037dcb731a918d516eb02c12440d8ba3e71a7",
    "createdAt": "2015-11-02T19:00:14Z",
    "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.shuffle\n+\n+import java.io.{FileOutputStream, FileInputStream, File}\n+\n+import com.google.common.annotations.VisibleForTesting\n+\n+import org.apache.spark.storage.ShuffleMapStatusBlockId\n+import org.apache.spark.{SparkEnv, Logging}\n+import org.apache.spark.scheduler.MapStatus\n+import org.apache.spark.serializer.SerializerInstance\n+\n+/**\n+ * Ensures that on each executor, there are no conflicting writes to the same shuffle files.  It\n+ * implements \"first write wins\", by atomically moving all shuffle files into their final location,\n+ * only if the files did not already exist. See SPARK-8029\n+ */\n+private[spark] object ShuffleOutputCoordinator extends Logging {\n+\n+  /**\n+   * If any of the destination files do not exist, then move all of the temporary files to their\n+   * destinations, and return (true, the given MapStatus).  If all destination files exist, then\n+   * delete all temporary files, and return (false, the MapStatus from previously committed shuffle\n+   * output).\n+   *\n+   * Note that this will write to all destination files.  If the tmp file is missing, then a\n+   * zero-length destination file will be created.  This is so the ShuffleOutputCoordinator can work\n+   * even when there is a non-determinstic data, where the output exists in one attempt, but is\n+   * empty in another attempt.\n+   *\n+   * @param tmpToDest  Seq of (temporary, destination) file pairs\n+   * @param mapStatus the [[MapStatus]] for the output already written to the the temporary files\n+   * @return pair of: (1) true iff the set of temporary files was moved to the destination and (2)\n+   *         the MapStatus of the committed attempt.\n+   *\n+   */\n+  def commitOutputs(\n+      shuffleId: Int,\n+      partitionId: Int,\n+      tmpToDest: Seq[(File, File)],\n+      mapStatus: MapStatus,\n+      sparkEnv: SparkEnv): (Boolean, MapStatus) = synchronized {\n+    val mapStatusFile = sparkEnv.blockManager.diskBlockManager.getFile(\n+      ShuffleMapStatusBlockId(shuffleId, partitionId))\n+    val ser = sparkEnv.serializer.newInstance()\n+    commitOutputs(shuffleId, partitionId, tmpToDest, mapStatus, mapStatusFile, ser)\n+  }\n+\n+  @VisibleForTesting\n+  def commitOutputs(\n+      shuffleId: Int,\n+      partitionId: Int,\n+      tmpToDest: Seq[(File, File)],\n+      mapStatus: MapStatus,\n+      mapStatusFile: File,\n+      serializer: SerializerInstance): (Boolean, MapStatus) = synchronized {\n+    val destAlreadyExists = tmpToDest.forall{_._2.exists()} && mapStatusFile.exists()\n+    if (!destAlreadyExists) {\n+      tmpToDest.foreach { case (tmp, dest) =>\n+        // If *some* of the destination files exist, but not all of them, then its not clear"
  }],
  "prId": 9214
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "you need a `try..finally` here.\n",
    "commit": "80e037dcb731a918d516eb02c12440d8ba3e71a7",
    "createdAt": "2015-11-02T19:02:24Z",
    "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.shuffle\n+\n+import java.io.{FileOutputStream, FileInputStream, File}\n+\n+import com.google.common.annotations.VisibleForTesting\n+\n+import org.apache.spark.storage.ShuffleMapStatusBlockId\n+import org.apache.spark.{SparkEnv, Logging}\n+import org.apache.spark.scheduler.MapStatus\n+import org.apache.spark.serializer.SerializerInstance\n+\n+/**\n+ * Ensures that on each executor, there are no conflicting writes to the same shuffle files.  It\n+ * implements \"first write wins\", by atomically moving all shuffle files into their final location,\n+ * only if the files did not already exist. See SPARK-8029\n+ */\n+private[spark] object ShuffleOutputCoordinator extends Logging {\n+\n+  /**\n+   * If any of the destination files do not exist, then move all of the temporary files to their\n+   * destinations, and return (true, the given MapStatus).  If all destination files exist, then\n+   * delete all temporary files, and return (false, the MapStatus from previously committed shuffle\n+   * output).\n+   *\n+   * Note that this will write to all destination files.  If the tmp file is missing, then a\n+   * zero-length destination file will be created.  This is so the ShuffleOutputCoordinator can work\n+   * even when there is a non-determinstic data, where the output exists in one attempt, but is\n+   * empty in another attempt.\n+   *\n+   * @param tmpToDest  Seq of (temporary, destination) file pairs\n+   * @param mapStatus the [[MapStatus]] for the output already written to the the temporary files\n+   * @return pair of: (1) true iff the set of temporary files was moved to the destination and (2)\n+   *         the MapStatus of the committed attempt.\n+   *\n+   */\n+  def commitOutputs(\n+      shuffleId: Int,\n+      partitionId: Int,\n+      tmpToDest: Seq[(File, File)],\n+      mapStatus: MapStatus,\n+      sparkEnv: SparkEnv): (Boolean, MapStatus) = synchronized {\n+    val mapStatusFile = sparkEnv.blockManager.diskBlockManager.getFile(\n+      ShuffleMapStatusBlockId(shuffleId, partitionId))\n+    val ser = sparkEnv.serializer.newInstance()\n+    commitOutputs(shuffleId, partitionId, tmpToDest, mapStatus, mapStatusFile, ser)\n+  }\n+\n+  @VisibleForTesting\n+  def commitOutputs(\n+      shuffleId: Int,\n+      partitionId: Int,\n+      tmpToDest: Seq[(File, File)],\n+      mapStatus: MapStatus,\n+      mapStatusFile: File,\n+      serializer: SerializerInstance): (Boolean, MapStatus) = synchronized {\n+    val destAlreadyExists = tmpToDest.forall{_._2.exists()} && mapStatusFile.exists()\n+    if (!destAlreadyExists) {\n+      tmpToDest.foreach { case (tmp, dest) =>\n+        // If *some* of the destination files exist, but not all of them, then its not clear\n+        // what to do.  There could be a task already reading from this dest file when we delete\n+        // it -- but then again, something in that taskset would be doomed to fail in any case when\n+        // it got to the missing files.  Better to just put consistent output into place\n+        if (dest.exists()) {\n+          dest.delete()\n+        }\n+        if (tmp.exists()) {\n+          tmp.renameTo(dest)\n+        } else {\n+          // we always create the destination files, so this works correctly even when the\n+          // input data is non-deterministic (potentially empty in one iteration, and non-empty\n+          // in another)\n+          dest.createNewFile()\n+        }\n+      }\n+      val out = serializer.serializeStream(new FileOutputStream(mapStatusFile))\n+      out.writeObject(mapStatus)\n+      out.close()\n+      (true, mapStatus)\n+    } else {\n+      logInfo(s\"shuffle output for shuffle $shuffleId, partition $partitionId already exists, \" +\n+        s\"not overwriting.  Another task must have created this shuffle output.\")\n+      tmpToDest.foreach{ case (tmp, _) => tmp.delete()}\n+      val in = serializer.deserializeStream(new FileInputStream(mapStatusFile))\n+      val readStatus = in.readObject[MapStatus]"
  }],
  "prId": 9214
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "So, this justification feels a little weird to me. Let's say you have t1 and t2.\n- if t1 finishes and writes \"n\" files, and t2 finishes and writes \"n + 1\" files, then you'll get the output of t2\n- if instead t2 also creates \"n\" files, you'll get the output of t1 instead.\n\nOr am I misunderstanding something?\n\nAlso, the \"if a task is already reading from the file\" case will probably make this whole block of code fail on Windows; I believe `File.delete()` will fail if the file is open, unlike on POSIX fses.\n",
    "commit": "80e037dcb731a918d516eb02c12440d8ba3e71a7",
    "createdAt": "2015-11-02T19:06:30Z",
    "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.shuffle\n+\n+import java.io.{FileOutputStream, FileInputStream, File}\n+\n+import com.google.common.annotations.VisibleForTesting\n+\n+import org.apache.spark.storage.ShuffleMapStatusBlockId\n+import org.apache.spark.{SparkEnv, Logging}\n+import org.apache.spark.scheduler.MapStatus\n+import org.apache.spark.serializer.SerializerInstance\n+\n+/**\n+ * Ensures that on each executor, there are no conflicting writes to the same shuffle files.  It\n+ * implements \"first write wins\", by atomically moving all shuffle files into their final location,\n+ * only if the files did not already exist. See SPARK-8029\n+ */\n+private[spark] object ShuffleOutputCoordinator extends Logging {\n+\n+  /**\n+   * If any of the destination files do not exist, then move all of the temporary files to their\n+   * destinations, and return (true, the given MapStatus).  If all destination files exist, then\n+   * delete all temporary files, and return (false, the MapStatus from previously committed shuffle\n+   * output).\n+   *\n+   * Note that this will write to all destination files.  If the tmp file is missing, then a\n+   * zero-length destination file will be created.  This is so the ShuffleOutputCoordinator can work\n+   * even when there is a non-determinstic data, where the output exists in one attempt, but is\n+   * empty in another attempt.\n+   *\n+   * @param tmpToDest  Seq of (temporary, destination) file pairs\n+   * @param mapStatus the [[MapStatus]] for the output already written to the the temporary files\n+   * @return pair of: (1) true iff the set of temporary files was moved to the destination and (2)\n+   *         the MapStatus of the committed attempt.\n+   *\n+   */\n+  def commitOutputs(\n+      shuffleId: Int,\n+      partitionId: Int,\n+      tmpToDest: Seq[(File, File)],\n+      mapStatus: MapStatus,\n+      sparkEnv: SparkEnv): (Boolean, MapStatus) = synchronized {\n+    val mapStatusFile = sparkEnv.blockManager.diskBlockManager.getFile(\n+      ShuffleMapStatusBlockId(shuffleId, partitionId))\n+    val ser = sparkEnv.serializer.newInstance()\n+    commitOutputs(shuffleId, partitionId, tmpToDest, mapStatus, mapStatusFile, ser)\n+  }\n+\n+  @VisibleForTesting\n+  def commitOutputs(\n+      shuffleId: Int,\n+      partitionId: Int,\n+      tmpToDest: Seq[(File, File)],\n+      mapStatus: MapStatus,\n+      mapStatusFile: File,\n+      serializer: SerializerInstance): (Boolean, MapStatus) = synchronized {\n+    val destAlreadyExists = tmpToDest.forall{_._2.exists()} && mapStatusFile.exists()\n+    if (!destAlreadyExists) {\n+      tmpToDest.foreach { case (tmp, dest) =>\n+        // If *some* of the destination files exist, but not all of them, then its not clear\n+        // what to do.  There could be a task already reading from this dest file when we delete"
  }],
  "prId": 9214
}]