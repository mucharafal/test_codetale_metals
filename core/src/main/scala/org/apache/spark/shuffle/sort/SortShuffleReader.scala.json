[{
  "comments": [{
    "author": {
      "login": "chenghao-intel"
    },
    "body": "We never run into here right? As `dep.keyOrdering` should be always defined for the `SortShuffleReader`, simply throws an exception?\n",
    "commit": "d6c94da3e67b01855d4dc5d42c9068c77cba7453",
    "createdAt": "2015-04-10T18:08:08Z",
    "diffHunk": "@@ -0,0 +1,337 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import java.io.File\n+import java.io.FileOutputStream\n+import java.nio.ByteBuffer\n+import java.util.Comparator\n+\n+import scala.collection.mutable.{ArrayBuffer, HashMap, Queue}\n+import scala.util.{Failure, Success, Try}\n+\n+import org.apache.spark._\n+import org.apache.spark.executor.ShuffleWriteMetrics\n+import org.apache.spark.network.buffer.{ManagedBuffer, NioManagedBuffer}\n+import org.apache.spark.serializer.Serializer\n+import org.apache.spark.shuffle.{BaseShuffleHandle, FetchFailedException, ShuffleReader}\n+import org.apache.spark.storage._\n+import org.apache.spark.util.{CompletionIterator, Utils}\n+import org.apache.spark.util.collection.{MergeUtil, TieredDiskMerger}\n+\n+/**\n+ * SortShuffleReader merges and aggregates shuffle data that has already been sorted within each\n+ * map output block.\n+ *\n+ * As blocks are fetched, we store them in memory until we fail to acquire space from the\n+ * ShuffleMemoryManager. When this occurs, we merge some in-memory blocks to disk and go back to\n+ * fetching.\n+ *\n+ * TieredDiskMerger is responsible for managing the merged on-disk blocks and for supplying an\n+ * iterator with their merged contents. The final iterator that is passed to user code merges this\n+ * on-disk iterator with the in-memory blocks that have not yet been spilled.\n+ */\n+private[spark] class SortShuffleReader[K, C](\n+    handle: BaseShuffleHandle[K, _, C],\n+    startPartition: Int,\n+    endPartition: Int,\n+    context: TaskContext)\n+  extends ShuffleReader[K, C] with Logging {\n+\n+  /** Manage the fetched in-memory shuffle block and related buffer */\n+  case class MemoryShuffleBlock(blockId: BlockId, blockData: ManagedBuffer)\n+\n+  require(endPartition == startPartition + 1,\n+    \"Sort shuffle currently only supports fetching one partition\")\n+\n+  private val dep = handle.dependency\n+  private val conf = SparkEnv.get.conf\n+  private val blockManager = SparkEnv.get.blockManager\n+  private val ser = Serializer.getSerializer(dep.serializer)\n+  private val shuffleMemoryManager = SparkEnv.get.shuffleMemoryManager\n+\n+  private val fileBufferSize = conf.getInt(\"spark.shuffle.file.buffer.kb\", 32) * 1024\n+\n+  /** Queue to store in-memory shuffle blocks */\n+  private val inMemoryBlocks = new Queue[MemoryShuffleBlock]()\n+\n+  /**\n+   * Maintain block manager and reported size of each shuffle block. The block manager is used for\n+   * error reporting. The reported size, which, because of size compression, may be slightly\n+   * different than the size of the actual fetched block, is used for calculating how many blocks\n+   * to spill.\n+   */\n+  private val shuffleBlockMap = new HashMap[ShuffleBlockId, (BlockManagerId, Long)]()\n+\n+  /** keyComparator for mergeSort, id keyOrdering is not available,\n+    * using hashcode of key to compare */\n+  private val keyComparator: Comparator[K] = dep.keyOrdering.getOrElse(new Comparator[K] {\n+    override def compare(a: K, b: K) = {",
    "line": 84
  }],
  "prId": 3438
}, {
  "comments": [{
    "author": {
      "login": "jeanlyn"
    },
    "body": "`getDiskWriter` had changed on #5606\n",
    "commit": "d6c94da3e67b01855d4dc5d42c9068c77cba7453",
    "createdAt": "2015-05-14T03:33:49Z",
    "diffHunk": "@@ -0,0 +1,337 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import java.io.File\n+import java.io.FileOutputStream\n+import java.nio.ByteBuffer\n+import java.util.Comparator\n+\n+import scala.collection.mutable.{ArrayBuffer, HashMap, Queue}\n+import scala.util.{Failure, Success, Try}\n+\n+import org.apache.spark._\n+import org.apache.spark.executor.ShuffleWriteMetrics\n+import org.apache.spark.network.buffer.{ManagedBuffer, NioManagedBuffer}\n+import org.apache.spark.serializer.Serializer\n+import org.apache.spark.shuffle.{BaseShuffleHandle, FetchFailedException, ShuffleReader}\n+import org.apache.spark.storage._\n+import org.apache.spark.util.{CompletionIterator, Utils}\n+import org.apache.spark.util.collection.{MergeUtil, TieredDiskMerger}\n+\n+/**\n+ * SortShuffleReader merges and aggregates shuffle data that has already been sorted within each\n+ * map output block.\n+ *\n+ * As blocks are fetched, we store them in memory until we fail to acquire space from the\n+ * ShuffleMemoryManager. When this occurs, we merge some in-memory blocks to disk and go back to\n+ * fetching.\n+ *\n+ * TieredDiskMerger is responsible for managing the merged on-disk blocks and for supplying an\n+ * iterator with their merged contents. The final iterator that is passed to user code merges this\n+ * on-disk iterator with the in-memory blocks that have not yet been spilled.\n+ */\n+private[spark] class SortShuffleReader[K, C](\n+    handle: BaseShuffleHandle[K, _, C],\n+    startPartition: Int,\n+    endPartition: Int,\n+    context: TaskContext)\n+  extends ShuffleReader[K, C] with Logging {\n+\n+  /** Manage the fetched in-memory shuffle block and related buffer */\n+  case class MemoryShuffleBlock(blockId: BlockId, blockData: ManagedBuffer)\n+\n+  require(endPartition == startPartition + 1,\n+    \"Sort shuffle currently only supports fetching one partition\")\n+\n+  private val dep = handle.dependency\n+  private val conf = SparkEnv.get.conf\n+  private val blockManager = SparkEnv.get.blockManager\n+  private val ser = Serializer.getSerializer(dep.serializer)\n+  private val shuffleMemoryManager = SparkEnv.get.shuffleMemoryManager\n+\n+  private val fileBufferSize = conf.getInt(\"spark.shuffle.file.buffer.kb\", 32) * 1024\n+\n+  /** Queue to store in-memory shuffle blocks */\n+  private val inMemoryBlocks = new Queue[MemoryShuffleBlock]()\n+\n+  /**\n+   * Maintain block manager and reported size of each shuffle block. The block manager is used for\n+   * error reporting. The reported size, which, because of size compression, may be slightly\n+   * different than the size of the actual fetched block, is used for calculating how many blocks\n+   * to spill.\n+   */\n+  private val shuffleBlockMap = new HashMap[ShuffleBlockId, (BlockManagerId, Long)]()\n+\n+  /** keyComparator for mergeSort, id keyOrdering is not available,\n+    * using hashcode of key to compare */\n+  private val keyComparator: Comparator[K] = dep.keyOrdering.getOrElse(new Comparator[K] {\n+    override def compare(a: K, b: K) = {\n+      val h1 = if (a == null) 0 else a.hashCode()\n+      val h2 = if (b == null) 0 else b.hashCode()\n+      if (h1 < h2) -1 else if (h1 == h2) 0 else 1\n+    }\n+  })\n+\n+  /** A merge thread to merge on-disk blocks */\n+  private val tieredMerger = new TieredDiskMerger(conf, dep, keyComparator, context)\n+\n+  /** Shuffle block fetcher iterator */\n+  private var shuffleRawBlockFetcherItr: ShuffleRawBlockFetcherIterator = _\n+\n+  /** Number of bytes spilled in memory and on disk */\n+  private var _memoryBytesSpilled: Long = 0L\n+  private var _diskBytesSpilled: Long = 0L\n+\n+  /** Number of bytes left to fetch */\n+  private var unfetchedBytes: Long = 0L\n+\n+  def memoryBytesSpilled: Long = _memoryBytesSpilled\n+\n+  def diskBytesSpilled: Long = _diskBytesSpilled + tieredMerger.diskBytesSpilled\n+\n+  override def read(): Iterator[Product2[K, C]] = {\n+    tieredMerger.start()\n+\n+    computeShuffleBlocks()\n+\n+    for ((blockId, blockOption) <- fetchShuffleBlocks()) {\n+      val blockData = blockOption match {\n+        case Success(b) => b\n+        case Failure(e) =>\n+          blockId match {\n+            case b @ ShuffleBlockId(shuffleId, mapId, _) =>\n+              val address = shuffleBlockMap(b)._1\n+              throw new FetchFailedException (address, shuffleId.toInt, mapId.toInt, startPartition,\n+                Utils.exceptionString (e))\n+            case _ =>\n+              throw new SparkException (\n+                s\"Failed to get block $blockId, which is not a shuffle block\", e)\n+          }\n+      }\n+\n+      shuffleRawBlockFetcherItr.currentResult = null\n+\n+      // Try to fit block in memory. If this fails, merge in-memory blocks to disk.\n+      val blockSize = blockData.size\n+      val granted = shuffleMemoryManager.tryToAcquire(blockSize)\n+      if (granted >= blockSize) {\n+        if (blockData.isDirect) {\n+          // If the shuffle block is allocated on a direct buffer, copy it to an on-heap buffer,\n+          // otherwise off heap memory will be increased to the shuffle memory size.\n+          val onHeapBuffer = ByteBuffer.allocate(blockSize.toInt)\n+          onHeapBuffer.put(blockData.nioByteBuffer)\n+\n+          inMemoryBlocks += MemoryShuffleBlock(blockId, new NioManagedBuffer(onHeapBuffer))\n+          blockData.release()\n+        } else {\n+          inMemoryBlocks += MemoryShuffleBlock(blockId, blockData)\n+        }\n+      } else {\n+        logDebug(s\"Granted $granted memory is not enough to store shuffle block (id: $blockId, \" +\n+          s\"size: $blockSize), spilling in-memory blocks to release the memory\")\n+\n+        shuffleMemoryManager.release(granted)\n+        spillInMemoryBlocks(MemoryShuffleBlock(blockId, blockData))\n+      }\n+\n+      unfetchedBytes -= shuffleBlockMap(blockId.asInstanceOf[ShuffleBlockId])._2\n+    }\n+\n+    // Make sure all the blocks have been fetched.\n+    assert(unfetchedBytes == 0L)\n+\n+    tieredMerger.doneRegisteringOnDiskBlocks()\n+\n+    // Merge on-disk blocks with in-memory blocks to directly feed to the reducer.\n+    val finalItrGroup = inMemoryBlocksToIterators(inMemoryBlocks) ++ Seq(tieredMerger.readMerged())\n+    val mergedItr =\n+      MergeUtil.mergeSort(finalItrGroup, keyComparator, dep.keyOrdering, dep.aggregator)\n+\n+    // Update the spill metrics and do cleanup work when task is finished.\n+    context.taskMetrics().incMemoryBytesSpilled(memoryBytesSpilled)\n+    context.taskMetrics().incDiskBytesSpilled(diskBytesSpilled)\n+\n+    def releaseFinalShuffleMemory(): Unit = {\n+      inMemoryBlocks.foreach { block =>\n+        block.blockData.release()\n+        shuffleMemoryManager.release(block.blockData.size)\n+      }\n+      inMemoryBlocks.clear()\n+    }\n+    context.addTaskCompletionListener(_ => releaseFinalShuffleMemory())\n+\n+    // Release the in-memory block when iteration is completed.\n+    val completionItr = CompletionIterator[Product2[K, C], Iterator[Product2[K, C]]](\n+      mergedItr, releaseFinalShuffleMemory())\n+\n+    new InterruptibleIterator(context, completionItr.map(p => (p._1, p._2)))\n+  }\n+\n+  /**\n+   * Called when we've failed to acquire memory for a block we've just fetched. Figure out how many\n+   * blocks to spill and then spill them.\n+   */\n+  private def spillInMemoryBlocks(tippingBlock: MemoryShuffleBlock): Unit = {\n+    val (tmpBlockId, file) = blockManager.diskBlockManager.createTempShuffleBlock()\n+\n+    // If the remaining unfetched data would fit inside our current allocation, we don't want to\n+    // waste time spilling blocks beyond the space needed for it.\n+    // Note that the number of unfetchedBytes is not exact, because of the compression used on the\n+    // sizes of map output blocks.\n+    var bytesToSpill = unfetchedBytes\n+    val blocksToSpill = new ArrayBuffer[MemoryShuffleBlock]()\n+    blocksToSpill += tippingBlock\n+    bytesToSpill -= tippingBlock.blockData.size\n+    while (bytesToSpill > 0 && !inMemoryBlocks.isEmpty) {\n+      val block = inMemoryBlocks.dequeue()\n+      blocksToSpill += block\n+      bytesToSpill -= block.blockData.size\n+    }\n+\n+    _memoryBytesSpilled += blocksToSpill.map(_.blockData.size()).sum\n+\n+    if (blocksToSpill.size > 1) {\n+      spillMultipleBlocks(file, tmpBlockId, blocksToSpill, tippingBlock)\n+    } else {\n+      spillSingleBlock(file, blocksToSpill.head)\n+    }\n+\n+    tieredMerger.registerOnDiskBlock(tmpBlockId, file)\n+\n+    logInfo(s\"Merged ${blocksToSpill.size} in-memory blocks into file ${file.getName}\")\n+  }\n+\n+  private def spillSingleBlock(file: File, block: MemoryShuffleBlock): Unit = {\n+    val fos = new FileOutputStream(file)\n+    val buffer = block.blockData.nioByteBuffer()\n+    var channel = fos.getChannel\n+    var success = false\n+\n+    try {\n+      while (buffer.hasRemaining) {\n+        channel.write(buffer)\n+      }\n+      success = true\n+    } finally {\n+      if (channel != null) {\n+        channel.close()\n+        channel = null\n+      }\n+      if (!success) {\n+        if (file.exists()) {\n+          file.delete()\n+        }\n+      } else {\n+        _diskBytesSpilled += file.length()\n+      }\n+      // When we spill a single block, it's the single tipping block that we never acquired memory\n+      // from the shuffle memory manager for, so we don't need to release any memory from there.\n+      block.blockData.release()\n+    }\n+  }\n+\n+  /**\n+   * Merge multiple in-memory blocks to a single on-disk file.\n+   */\n+  private def spillMultipleBlocks(file: File, tmpBlockId: BlockId,\n+      blocksToSpill: Seq[MemoryShuffleBlock], tippingBlock: MemoryShuffleBlock): Unit = {\n+    val itrGroup = inMemoryBlocksToIterators(blocksToSpill)\n+    val partialMergedItr =\n+      MergeUtil.mergeSort(itrGroup, keyComparator, dep.keyOrdering, dep.aggregator)\n+    val curWriteMetrics = new ShuffleWriteMetrics()\n+    var writer = blockManager.getDiskWriter(tmpBlockId, file, ser, fileBufferSize, curWriteMetrics)",
    "line": 258
  }],
  "prId": 3438
}]