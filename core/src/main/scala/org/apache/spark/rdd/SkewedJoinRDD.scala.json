[{
  "comments": [{
    "author": {
      "login": "lianhuiwang"
    },
    "body": "i think K should be not a Ordering in Common Join. why K should be a Ordering in skew join.@zsxwing\nwhen i implement skewedGroupby with ExternalOrderingAppendOnlyMap, i find key in ExternalOrderingAppendOnlyMap should be a Ordering.\n",
    "commit": "af7eb714ab9916a628859682b8cbf9c4c2396029",
    "createdAt": "2015-01-05T13:39:45Z",
    "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd\n+\n+import java.io.{ObjectOutputStream, IOException}\n+\n+import scala.reflect.ClassTag\n+\n+import org.apache.spark._\n+import org.apache.spark.serializer.Serializer\n+import org.apache.spark.shuffle.ShuffleHandle\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.collection._\n+\n+private[spark] sealed trait JoinType[K, L, R, PAIR <: Product2[_, _]] extends Serializable {\n+\n+  def flatten(i: Iterator[(K, (Iterable[Chunk[L]], Iterable[Chunk[R]]))]): Iterator[(K, PAIR)]\n+\n+}\n+\n+private[spark] object JoinType {\n+\n+  def inner[K, L, R] = new JoinType[K, L, R, (L, R)] {\n+\n+    override def flatten(i: Iterator[(K, (Iterable[Chunk[L]], Iterable[Chunk[R]]))]) =\n+      i flatMap {\n+        case (key, pair) => {\n+          if (pair._1.size < pair._2.size) {\n+            yieldPair(pair._1, pair._2, key, (p1: L, p2: R) => (p1, p2))\n+          } else {\n+            yieldPair(pair._2, pair._1, key, (p2: R, p1: L) => (p1, p2))\n+          }\n+        }\n+      }\n+  }\n+\n+  def leftOuter[K, L, R] = new JoinType[K, L, R, (L, Option[R])] {\n+\n+    override def flatten(i: Iterator[(K, (Iterable[Chunk[L]], Iterable[Chunk[R]]))]) =\n+      i flatMap {\n+        case (key, pair) => {\n+          if (pair._2.size == 0) {\n+            for (chunk <- pair._1.iterator;\n+                 v <- chunk\n+            ) yield (key, (v, None)): (K, (L, Option[R]))\n+          }\n+          else if (pair._1.size < pair._2.size) {\n+            yieldPair(pair._1, pair._2, key, (p1: L, p2: R) => (p1, Some(p2)))\n+          } else {\n+            yieldPair(pair._2, pair._1, key, (p2: R, p1: L) => (p1, Some(p2)))\n+          }\n+        }\n+      }\n+  }\n+\n+  def rightOuter[K, L, R] = new JoinType[K, L, R, (Option[L], R)] {\n+\n+    override def flatten(i: Iterator[(K, (Iterable[Chunk[L]], Iterable[Chunk[R]]))]) =\n+      i flatMap {\n+        case (key, pair) => {\n+          if (pair._1.size == 0) {\n+            for (chunk <- pair._2.iterator;\n+                 v <- chunk\n+            ) yield (key, (None, v)): (K, (Option[L], R))\n+          }\n+          else if (pair._1.size < pair._2.size) {\n+            yieldPair(pair._1, pair._2, key, (p1: L, p2: R) => (Some(p1), p2))\n+          } else {\n+            yieldPair(pair._2, pair._1, key, (p2: R, p1: L) => (Some(p1), p2))\n+          }\n+        }\n+      }\n+  }\n+\n+  def fullOuter[K, L, R] = new JoinType[K, L, R, (Option[L], Option[R])] {\n+\n+    override def flatten(i: Iterator[(K, (Iterable[Chunk[L]], Iterable[Chunk[R]]))]) =\n+      i flatMap {\n+        case (key, pair) => {\n+          if (pair._1.size == 0) {\n+            for (chunk <- pair._2.iterator;\n+                 v <- chunk\n+            ) yield (key, (None, Some(v))): (K, (Option[L], Option[R]))\n+          }\n+          else if (pair._2.size == 0) {\n+            for (chunk <- pair._1.iterator;\n+                 v <- chunk\n+            ) yield (key, (Some(v), None)): (K, (Option[L], Option[R]))\n+          }\n+          else if (pair._1.size < pair._2.size) {\n+            yieldPair(pair._1, pair._2, key, (p1: L, p2: R) => (Some(p1), Some(p2)))\n+          } else {\n+            yieldPair(pair._2, pair._1, key, (p2: R, p1: L) => (Some(p1), Some(p2)))\n+          }\n+        }\n+      }\n+  }\n+\n+  private def yieldPair[K, OUT, IN, PAIR <: Product2[_, _]](\n+      outer: Iterable[Chunk[OUT]], inner: Iterable[Chunk[IN]], key: K, toPair: (OUT, IN) => PAIR) =\n+    for (\n+      outerChunk <- outer.iterator;\n+      innerChunk <- inner.iterator;\n+      outerValue <- outerChunk;\n+      innerValue <- innerChunk\n+    ) yield (key, toPair(outerValue, innerValue))\n+}\n+\n+private[spark] sealed trait JoinSplitDep extends Serializable\n+\n+private[spark] case class NarrowJoinSplitDep(\n+    rdd: RDD[_],\n+    splitIndex: Int,\n+    var split: Partition) extends JoinSplitDep {\n+\n+  @throws(classOf[IOException])\n+  private def writeObject(oos: ObjectOutputStream): Unit = Utils.tryOrIOException {\n+    // Update the reference to parent split at the time of task serialization\n+    split = rdd.partitions(splitIndex)\n+    oos.defaultWriteObject()\n+  }\n+}\n+\n+private[spark] case class ShuffleJoinSplitDep(handle: ShuffleHandle) extends JoinSplitDep\n+\n+private[spark] class JoinPartition(idx: Int, val left: JoinSplitDep, val right: JoinSplitDep)\n+  extends Partition with Serializable {\n+  override val index: Int = idx\n+\n+  override def hashCode(): Int = idx\n+}\n+\n+private[spark] class BufferWrapper private(val isChunkBuffer: Boolean, buffer: Iterable[Any])\n+  extends Serializable {\n+\n+  def this(buffer: CompactBuffer[_]) {\n+    this(false, buffer)\n+  }\n+\n+  def this(buffer: ChunkBuffer[_]) {\n+    this(true, buffer)\n+  }\n+\n+  def getChunkBuffer[T: ClassTag]: ChunkBuffer[T] = buffer.asInstanceOf[ChunkBuffer[T]]\n+\n+  def getCompactBuffer[T: ClassTag]: CompactBuffer[T] = buffer.asInstanceOf[CompactBuffer[T]]\n+\n+  def asChunkIterable[T: ClassTag]: Iterable[Chunk[T]] = {\n+    if (isChunkBuffer) {\n+      getChunkBuffer[T]\n+    }\n+    else {\n+      val buffer = getCompactBuffer[T]\n+      if (buffer.isEmpty) {\n+        Iterable[Chunk[T]]()\n+      }\n+      else {\n+        Iterable(new Chunk[T](buffer))\n+      }\n+    }\n+  }\n+}\n+\n+private[spark] class SkewedJoinRDD[K, L, R, PAIR <: Product2[_, _]](\n+    left: RDD[(K, L)], right: RDD[(K, R)], part: Partitioner, joinType: JoinType[K, L, R, PAIR])\n+    (implicit kt: ClassTag[K], lt: ClassTag[L], rt: ClassTag[R], ord: Ordering[K])",
    "line": 181
  }, {
    "author": {
      "login": "zsxwing"
    },
    "body": "> when i implement skewedGroupby with ExternalOrderingAppendOnlyMap, i find key in ExternalOrderingAppendOnlyMap should be a Ordering.\n\nRight. To avoid to prefetch all values of a key into memory. \n\nIâ€˜m curious How do you implement `skewedGroupby`. Have you solved the issue that someone calls `skewedGroupby(...).repartition(n)`?\n",
    "commit": "af7eb714ab9916a628859682b8cbf9c4c2396029",
    "createdAt": "2015-01-12T03:23:48Z",
    "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd\n+\n+import java.io.{ObjectOutputStream, IOException}\n+\n+import scala.reflect.ClassTag\n+\n+import org.apache.spark._\n+import org.apache.spark.serializer.Serializer\n+import org.apache.spark.shuffle.ShuffleHandle\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.collection._\n+\n+private[spark] sealed trait JoinType[K, L, R, PAIR <: Product2[_, _]] extends Serializable {\n+\n+  def flatten(i: Iterator[(K, (Iterable[Chunk[L]], Iterable[Chunk[R]]))]): Iterator[(K, PAIR)]\n+\n+}\n+\n+private[spark] object JoinType {\n+\n+  def inner[K, L, R] = new JoinType[K, L, R, (L, R)] {\n+\n+    override def flatten(i: Iterator[(K, (Iterable[Chunk[L]], Iterable[Chunk[R]]))]) =\n+      i flatMap {\n+        case (key, pair) => {\n+          if (pair._1.size < pair._2.size) {\n+            yieldPair(pair._1, pair._2, key, (p1: L, p2: R) => (p1, p2))\n+          } else {\n+            yieldPair(pair._2, pair._1, key, (p2: R, p1: L) => (p1, p2))\n+          }\n+        }\n+      }\n+  }\n+\n+  def leftOuter[K, L, R] = new JoinType[K, L, R, (L, Option[R])] {\n+\n+    override def flatten(i: Iterator[(K, (Iterable[Chunk[L]], Iterable[Chunk[R]]))]) =\n+      i flatMap {\n+        case (key, pair) => {\n+          if (pair._2.size == 0) {\n+            for (chunk <- pair._1.iterator;\n+                 v <- chunk\n+            ) yield (key, (v, None)): (K, (L, Option[R]))\n+          }\n+          else if (pair._1.size < pair._2.size) {\n+            yieldPair(pair._1, pair._2, key, (p1: L, p2: R) => (p1, Some(p2)))\n+          } else {\n+            yieldPair(pair._2, pair._1, key, (p2: R, p1: L) => (p1, Some(p2)))\n+          }\n+        }\n+      }\n+  }\n+\n+  def rightOuter[K, L, R] = new JoinType[K, L, R, (Option[L], R)] {\n+\n+    override def flatten(i: Iterator[(K, (Iterable[Chunk[L]], Iterable[Chunk[R]]))]) =\n+      i flatMap {\n+        case (key, pair) => {\n+          if (pair._1.size == 0) {\n+            for (chunk <- pair._2.iterator;\n+                 v <- chunk\n+            ) yield (key, (None, v)): (K, (Option[L], R))\n+          }\n+          else if (pair._1.size < pair._2.size) {\n+            yieldPair(pair._1, pair._2, key, (p1: L, p2: R) => (Some(p1), p2))\n+          } else {\n+            yieldPair(pair._2, pair._1, key, (p2: R, p1: L) => (Some(p1), p2))\n+          }\n+        }\n+      }\n+  }\n+\n+  def fullOuter[K, L, R] = new JoinType[K, L, R, (Option[L], Option[R])] {\n+\n+    override def flatten(i: Iterator[(K, (Iterable[Chunk[L]], Iterable[Chunk[R]]))]) =\n+      i flatMap {\n+        case (key, pair) => {\n+          if (pair._1.size == 0) {\n+            for (chunk <- pair._2.iterator;\n+                 v <- chunk\n+            ) yield (key, (None, Some(v))): (K, (Option[L], Option[R]))\n+          }\n+          else if (pair._2.size == 0) {\n+            for (chunk <- pair._1.iterator;\n+                 v <- chunk\n+            ) yield (key, (Some(v), None)): (K, (Option[L], Option[R]))\n+          }\n+          else if (pair._1.size < pair._2.size) {\n+            yieldPair(pair._1, pair._2, key, (p1: L, p2: R) => (Some(p1), Some(p2)))\n+          } else {\n+            yieldPair(pair._2, pair._1, key, (p2: R, p1: L) => (Some(p1), Some(p2)))\n+          }\n+        }\n+      }\n+  }\n+\n+  private def yieldPair[K, OUT, IN, PAIR <: Product2[_, _]](\n+      outer: Iterable[Chunk[OUT]], inner: Iterable[Chunk[IN]], key: K, toPair: (OUT, IN) => PAIR) =\n+    for (\n+      outerChunk <- outer.iterator;\n+      innerChunk <- inner.iterator;\n+      outerValue <- outerChunk;\n+      innerValue <- innerChunk\n+    ) yield (key, toPair(outerValue, innerValue))\n+}\n+\n+private[spark] sealed trait JoinSplitDep extends Serializable\n+\n+private[spark] case class NarrowJoinSplitDep(\n+    rdd: RDD[_],\n+    splitIndex: Int,\n+    var split: Partition) extends JoinSplitDep {\n+\n+  @throws(classOf[IOException])\n+  private def writeObject(oos: ObjectOutputStream): Unit = Utils.tryOrIOException {\n+    // Update the reference to parent split at the time of task serialization\n+    split = rdd.partitions(splitIndex)\n+    oos.defaultWriteObject()\n+  }\n+}\n+\n+private[spark] case class ShuffleJoinSplitDep(handle: ShuffleHandle) extends JoinSplitDep\n+\n+private[spark] class JoinPartition(idx: Int, val left: JoinSplitDep, val right: JoinSplitDep)\n+  extends Partition with Serializable {\n+  override val index: Int = idx\n+\n+  override def hashCode(): Int = idx\n+}\n+\n+private[spark] class BufferWrapper private(val isChunkBuffer: Boolean, buffer: Iterable[Any])\n+  extends Serializable {\n+\n+  def this(buffer: CompactBuffer[_]) {\n+    this(false, buffer)\n+  }\n+\n+  def this(buffer: ChunkBuffer[_]) {\n+    this(true, buffer)\n+  }\n+\n+  def getChunkBuffer[T: ClassTag]: ChunkBuffer[T] = buffer.asInstanceOf[ChunkBuffer[T]]\n+\n+  def getCompactBuffer[T: ClassTag]: CompactBuffer[T] = buffer.asInstanceOf[CompactBuffer[T]]\n+\n+  def asChunkIterable[T: ClassTag]: Iterable[Chunk[T]] = {\n+    if (isChunkBuffer) {\n+      getChunkBuffer[T]\n+    }\n+    else {\n+      val buffer = getCompactBuffer[T]\n+      if (buffer.isEmpty) {\n+        Iterable[Chunk[T]]()\n+      }\n+      else {\n+        Iterable(new Chunk[T](buffer))\n+      }\n+    }\n+  }\n+}\n+\n+private[spark] class SkewedJoinRDD[K, L, R, PAIR <: Product2[_, _]](\n+    left: RDD[(K, L)], right: RDD[(K, R)], part: Partitioner, joinType: JoinType[K, L, R, PAIR])\n+    (implicit kt: ClassTag[K], lt: ClassTag[L], rt: ClassTag[R], ord: Ordering[K])",
    "line": 181
  }, {
    "author": {
      "login": "lianhuiwang"
    },
    "body": "no, i just replace ExternalAppendOnlyMap with ExternalOrderingAppendOnlyMap in Aggregator. but i find ExternalOrderingAppendOnlyMap is not common for skewedGroupby and skewedJoin. in skewedJoin key in ExternalOrderingAppendOnlyMap  is a ordering. how about we can make ExternalOrderingAppendOnlyMap be general for all of skewedJoin and  skewedGroupby, they can resolve skew problem using it? @zsxwing\n",
    "commit": "af7eb714ab9916a628859682b8cbf9c4c2396029",
    "createdAt": "2015-01-12T03:39:30Z",
    "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd\n+\n+import java.io.{ObjectOutputStream, IOException}\n+\n+import scala.reflect.ClassTag\n+\n+import org.apache.spark._\n+import org.apache.spark.serializer.Serializer\n+import org.apache.spark.shuffle.ShuffleHandle\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.collection._\n+\n+private[spark] sealed trait JoinType[K, L, R, PAIR <: Product2[_, _]] extends Serializable {\n+\n+  def flatten(i: Iterator[(K, (Iterable[Chunk[L]], Iterable[Chunk[R]]))]): Iterator[(K, PAIR)]\n+\n+}\n+\n+private[spark] object JoinType {\n+\n+  def inner[K, L, R] = new JoinType[K, L, R, (L, R)] {\n+\n+    override def flatten(i: Iterator[(K, (Iterable[Chunk[L]], Iterable[Chunk[R]]))]) =\n+      i flatMap {\n+        case (key, pair) => {\n+          if (pair._1.size < pair._2.size) {\n+            yieldPair(pair._1, pair._2, key, (p1: L, p2: R) => (p1, p2))\n+          } else {\n+            yieldPair(pair._2, pair._1, key, (p2: R, p1: L) => (p1, p2))\n+          }\n+        }\n+      }\n+  }\n+\n+  def leftOuter[K, L, R] = new JoinType[K, L, R, (L, Option[R])] {\n+\n+    override def flatten(i: Iterator[(K, (Iterable[Chunk[L]], Iterable[Chunk[R]]))]) =\n+      i flatMap {\n+        case (key, pair) => {\n+          if (pair._2.size == 0) {\n+            for (chunk <- pair._1.iterator;\n+                 v <- chunk\n+            ) yield (key, (v, None)): (K, (L, Option[R]))\n+          }\n+          else if (pair._1.size < pair._2.size) {\n+            yieldPair(pair._1, pair._2, key, (p1: L, p2: R) => (p1, Some(p2)))\n+          } else {\n+            yieldPair(pair._2, pair._1, key, (p2: R, p1: L) => (p1, Some(p2)))\n+          }\n+        }\n+      }\n+  }\n+\n+  def rightOuter[K, L, R] = new JoinType[K, L, R, (Option[L], R)] {\n+\n+    override def flatten(i: Iterator[(K, (Iterable[Chunk[L]], Iterable[Chunk[R]]))]) =\n+      i flatMap {\n+        case (key, pair) => {\n+          if (pair._1.size == 0) {\n+            for (chunk <- pair._2.iterator;\n+                 v <- chunk\n+            ) yield (key, (None, v)): (K, (Option[L], R))\n+          }\n+          else if (pair._1.size < pair._2.size) {\n+            yieldPair(pair._1, pair._2, key, (p1: L, p2: R) => (Some(p1), p2))\n+          } else {\n+            yieldPair(pair._2, pair._1, key, (p2: R, p1: L) => (Some(p1), p2))\n+          }\n+        }\n+      }\n+  }\n+\n+  def fullOuter[K, L, R] = new JoinType[K, L, R, (Option[L], Option[R])] {\n+\n+    override def flatten(i: Iterator[(K, (Iterable[Chunk[L]], Iterable[Chunk[R]]))]) =\n+      i flatMap {\n+        case (key, pair) => {\n+          if (pair._1.size == 0) {\n+            for (chunk <- pair._2.iterator;\n+                 v <- chunk\n+            ) yield (key, (None, Some(v))): (K, (Option[L], Option[R]))\n+          }\n+          else if (pair._2.size == 0) {\n+            for (chunk <- pair._1.iterator;\n+                 v <- chunk\n+            ) yield (key, (Some(v), None)): (K, (Option[L], Option[R]))\n+          }\n+          else if (pair._1.size < pair._2.size) {\n+            yieldPair(pair._1, pair._2, key, (p1: L, p2: R) => (Some(p1), Some(p2)))\n+          } else {\n+            yieldPair(pair._2, pair._1, key, (p2: R, p1: L) => (Some(p1), Some(p2)))\n+          }\n+        }\n+      }\n+  }\n+\n+  private def yieldPair[K, OUT, IN, PAIR <: Product2[_, _]](\n+      outer: Iterable[Chunk[OUT]], inner: Iterable[Chunk[IN]], key: K, toPair: (OUT, IN) => PAIR) =\n+    for (\n+      outerChunk <- outer.iterator;\n+      innerChunk <- inner.iterator;\n+      outerValue <- outerChunk;\n+      innerValue <- innerChunk\n+    ) yield (key, toPair(outerValue, innerValue))\n+}\n+\n+private[spark] sealed trait JoinSplitDep extends Serializable\n+\n+private[spark] case class NarrowJoinSplitDep(\n+    rdd: RDD[_],\n+    splitIndex: Int,\n+    var split: Partition) extends JoinSplitDep {\n+\n+  @throws(classOf[IOException])\n+  private def writeObject(oos: ObjectOutputStream): Unit = Utils.tryOrIOException {\n+    // Update the reference to parent split at the time of task serialization\n+    split = rdd.partitions(splitIndex)\n+    oos.defaultWriteObject()\n+  }\n+}\n+\n+private[spark] case class ShuffleJoinSplitDep(handle: ShuffleHandle) extends JoinSplitDep\n+\n+private[spark] class JoinPartition(idx: Int, val left: JoinSplitDep, val right: JoinSplitDep)\n+  extends Partition with Serializable {\n+  override val index: Int = idx\n+\n+  override def hashCode(): Int = idx\n+}\n+\n+private[spark] class BufferWrapper private(val isChunkBuffer: Boolean, buffer: Iterable[Any])\n+  extends Serializable {\n+\n+  def this(buffer: CompactBuffer[_]) {\n+    this(false, buffer)\n+  }\n+\n+  def this(buffer: ChunkBuffer[_]) {\n+    this(true, buffer)\n+  }\n+\n+  def getChunkBuffer[T: ClassTag]: ChunkBuffer[T] = buffer.asInstanceOf[ChunkBuffer[T]]\n+\n+  def getCompactBuffer[T: ClassTag]: CompactBuffer[T] = buffer.asInstanceOf[CompactBuffer[T]]\n+\n+  def asChunkIterable[T: ClassTag]: Iterable[Chunk[T]] = {\n+    if (isChunkBuffer) {\n+      getChunkBuffer[T]\n+    }\n+    else {\n+      val buffer = getCompactBuffer[T]\n+      if (buffer.isEmpty) {\n+        Iterable[Chunk[T]]()\n+      }\n+      else {\n+        Iterable(new Chunk[T](buffer))\n+      }\n+    }\n+  }\n+}\n+\n+private[spark] class SkewedJoinRDD[K, L, R, PAIR <: Product2[_, _]](\n+    left: RDD[(K, L)], right: RDD[(K, R)], part: Partitioner, joinType: JoinType[K, L, R, PAIR])\n+    (implicit kt: ClassTag[K], lt: ClassTag[L], rt: ClassTag[R], ord: Ordering[K])",
    "line": 181
  }, {
    "author": {
      "login": "zsxwing"
    },
    "body": "I think `skewedGroupby` also requires the key is ordering. Otherwise, using a hash comparator, because the hash conflicts, the may require loading all values of a key into memory in some special situation. \n\nE.g., assume k1 and k2 have the same hashcode,  if using a hash comparator, the data in two external files may be (assume value is 1):\n\nfile1: (k1, 1), (k2, 1), (otherKey, 1), ...\nfile2: (k2, 1),  (k2, 1), (k2, 1), ... ,.... (k1, 1), (otherKey, 1), ...\n\nIn file2, k1 is behind all `k2` (We use a hash comparator, so it's possible). Becuase we scan file1 at first, we will process `k1` before `k2`. But when processing k1 in `file2`, we need to load all `k2`'s values in `file2`, it may cause OOM.\n\nIf the key is ordering, we can avoid this extreme case.\n",
    "commit": "af7eb714ab9916a628859682b8cbf9c4c2396029",
    "createdAt": "2015-01-12T04:05:08Z",
    "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd\n+\n+import java.io.{ObjectOutputStream, IOException}\n+\n+import scala.reflect.ClassTag\n+\n+import org.apache.spark._\n+import org.apache.spark.serializer.Serializer\n+import org.apache.spark.shuffle.ShuffleHandle\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.collection._\n+\n+private[spark] sealed trait JoinType[K, L, R, PAIR <: Product2[_, _]] extends Serializable {\n+\n+  def flatten(i: Iterator[(K, (Iterable[Chunk[L]], Iterable[Chunk[R]]))]): Iterator[(K, PAIR)]\n+\n+}\n+\n+private[spark] object JoinType {\n+\n+  def inner[K, L, R] = new JoinType[K, L, R, (L, R)] {\n+\n+    override def flatten(i: Iterator[(K, (Iterable[Chunk[L]], Iterable[Chunk[R]]))]) =\n+      i flatMap {\n+        case (key, pair) => {\n+          if (pair._1.size < pair._2.size) {\n+            yieldPair(pair._1, pair._2, key, (p1: L, p2: R) => (p1, p2))\n+          } else {\n+            yieldPair(pair._2, pair._1, key, (p2: R, p1: L) => (p1, p2))\n+          }\n+        }\n+      }\n+  }\n+\n+  def leftOuter[K, L, R] = new JoinType[K, L, R, (L, Option[R])] {\n+\n+    override def flatten(i: Iterator[(K, (Iterable[Chunk[L]], Iterable[Chunk[R]]))]) =\n+      i flatMap {\n+        case (key, pair) => {\n+          if (pair._2.size == 0) {\n+            for (chunk <- pair._1.iterator;\n+                 v <- chunk\n+            ) yield (key, (v, None)): (K, (L, Option[R]))\n+          }\n+          else if (pair._1.size < pair._2.size) {\n+            yieldPair(pair._1, pair._2, key, (p1: L, p2: R) => (p1, Some(p2)))\n+          } else {\n+            yieldPair(pair._2, pair._1, key, (p2: R, p1: L) => (p1, Some(p2)))\n+          }\n+        }\n+      }\n+  }\n+\n+  def rightOuter[K, L, R] = new JoinType[K, L, R, (Option[L], R)] {\n+\n+    override def flatten(i: Iterator[(K, (Iterable[Chunk[L]], Iterable[Chunk[R]]))]) =\n+      i flatMap {\n+        case (key, pair) => {\n+          if (pair._1.size == 0) {\n+            for (chunk <- pair._2.iterator;\n+                 v <- chunk\n+            ) yield (key, (None, v)): (K, (Option[L], R))\n+          }\n+          else if (pair._1.size < pair._2.size) {\n+            yieldPair(pair._1, pair._2, key, (p1: L, p2: R) => (Some(p1), p2))\n+          } else {\n+            yieldPair(pair._2, pair._1, key, (p2: R, p1: L) => (Some(p1), p2))\n+          }\n+        }\n+      }\n+  }\n+\n+  def fullOuter[K, L, R] = new JoinType[K, L, R, (Option[L], Option[R])] {\n+\n+    override def flatten(i: Iterator[(K, (Iterable[Chunk[L]], Iterable[Chunk[R]]))]) =\n+      i flatMap {\n+        case (key, pair) => {\n+          if (pair._1.size == 0) {\n+            for (chunk <- pair._2.iterator;\n+                 v <- chunk\n+            ) yield (key, (None, Some(v))): (K, (Option[L], Option[R]))\n+          }\n+          else if (pair._2.size == 0) {\n+            for (chunk <- pair._1.iterator;\n+                 v <- chunk\n+            ) yield (key, (Some(v), None)): (K, (Option[L], Option[R]))\n+          }\n+          else if (pair._1.size < pair._2.size) {\n+            yieldPair(pair._1, pair._2, key, (p1: L, p2: R) => (Some(p1), Some(p2)))\n+          } else {\n+            yieldPair(pair._2, pair._1, key, (p2: R, p1: L) => (Some(p1), Some(p2)))\n+          }\n+        }\n+      }\n+  }\n+\n+  private def yieldPair[K, OUT, IN, PAIR <: Product2[_, _]](\n+      outer: Iterable[Chunk[OUT]], inner: Iterable[Chunk[IN]], key: K, toPair: (OUT, IN) => PAIR) =\n+    for (\n+      outerChunk <- outer.iterator;\n+      innerChunk <- inner.iterator;\n+      outerValue <- outerChunk;\n+      innerValue <- innerChunk\n+    ) yield (key, toPair(outerValue, innerValue))\n+}\n+\n+private[spark] sealed trait JoinSplitDep extends Serializable\n+\n+private[spark] case class NarrowJoinSplitDep(\n+    rdd: RDD[_],\n+    splitIndex: Int,\n+    var split: Partition) extends JoinSplitDep {\n+\n+  @throws(classOf[IOException])\n+  private def writeObject(oos: ObjectOutputStream): Unit = Utils.tryOrIOException {\n+    // Update the reference to parent split at the time of task serialization\n+    split = rdd.partitions(splitIndex)\n+    oos.defaultWriteObject()\n+  }\n+}\n+\n+private[spark] case class ShuffleJoinSplitDep(handle: ShuffleHandle) extends JoinSplitDep\n+\n+private[spark] class JoinPartition(idx: Int, val left: JoinSplitDep, val right: JoinSplitDep)\n+  extends Partition with Serializable {\n+  override val index: Int = idx\n+\n+  override def hashCode(): Int = idx\n+}\n+\n+private[spark] class BufferWrapper private(val isChunkBuffer: Boolean, buffer: Iterable[Any])\n+  extends Serializable {\n+\n+  def this(buffer: CompactBuffer[_]) {\n+    this(false, buffer)\n+  }\n+\n+  def this(buffer: ChunkBuffer[_]) {\n+    this(true, buffer)\n+  }\n+\n+  def getChunkBuffer[T: ClassTag]: ChunkBuffer[T] = buffer.asInstanceOf[ChunkBuffer[T]]\n+\n+  def getCompactBuffer[T: ClassTag]: CompactBuffer[T] = buffer.asInstanceOf[CompactBuffer[T]]\n+\n+  def asChunkIterable[T: ClassTag]: Iterable[Chunk[T]] = {\n+    if (isChunkBuffer) {\n+      getChunkBuffer[T]\n+    }\n+    else {\n+      val buffer = getCompactBuffer[T]\n+      if (buffer.isEmpty) {\n+        Iterable[Chunk[T]]()\n+      }\n+      else {\n+        Iterable(new Chunk[T](buffer))\n+      }\n+    }\n+  }\n+}\n+\n+private[spark] class SkewedJoinRDD[K, L, R, PAIR <: Product2[_, _]](\n+    left: RDD[(K, L)], right: RDD[(K, R)], part: Partitioner, joinType: JoinType[K, L, R, PAIR])\n+    (implicit kt: ClassTag[K], lt: ClassTag[L], rt: ClassTag[R], ord: Ordering[K])",
    "line": 181
  }, {
    "author": {
      "login": "lianhuiwang"
    },
    "body": "so that is different between skewedGroupby and common Groupby. now in skewed operator,key must be a ordering. but in common operator key should not a ordering.this will affect how we implement skewed operator using previous no-skewed operator api. \n",
    "commit": "af7eb714ab9916a628859682b8cbf9c4c2396029",
    "createdAt": "2015-01-12T04:51:24Z",
    "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd\n+\n+import java.io.{ObjectOutputStream, IOException}\n+\n+import scala.reflect.ClassTag\n+\n+import org.apache.spark._\n+import org.apache.spark.serializer.Serializer\n+import org.apache.spark.shuffle.ShuffleHandle\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.collection._\n+\n+private[spark] sealed trait JoinType[K, L, R, PAIR <: Product2[_, _]] extends Serializable {\n+\n+  def flatten(i: Iterator[(K, (Iterable[Chunk[L]], Iterable[Chunk[R]]))]): Iterator[(K, PAIR)]\n+\n+}\n+\n+private[spark] object JoinType {\n+\n+  def inner[K, L, R] = new JoinType[K, L, R, (L, R)] {\n+\n+    override def flatten(i: Iterator[(K, (Iterable[Chunk[L]], Iterable[Chunk[R]]))]) =\n+      i flatMap {\n+        case (key, pair) => {\n+          if (pair._1.size < pair._2.size) {\n+            yieldPair(pair._1, pair._2, key, (p1: L, p2: R) => (p1, p2))\n+          } else {\n+            yieldPair(pair._2, pair._1, key, (p2: R, p1: L) => (p1, p2))\n+          }\n+        }\n+      }\n+  }\n+\n+  def leftOuter[K, L, R] = new JoinType[K, L, R, (L, Option[R])] {\n+\n+    override def flatten(i: Iterator[(K, (Iterable[Chunk[L]], Iterable[Chunk[R]]))]) =\n+      i flatMap {\n+        case (key, pair) => {\n+          if (pair._2.size == 0) {\n+            for (chunk <- pair._1.iterator;\n+                 v <- chunk\n+            ) yield (key, (v, None)): (K, (L, Option[R]))\n+          }\n+          else if (pair._1.size < pair._2.size) {\n+            yieldPair(pair._1, pair._2, key, (p1: L, p2: R) => (p1, Some(p2)))\n+          } else {\n+            yieldPair(pair._2, pair._1, key, (p2: R, p1: L) => (p1, Some(p2)))\n+          }\n+        }\n+      }\n+  }\n+\n+  def rightOuter[K, L, R] = new JoinType[K, L, R, (Option[L], R)] {\n+\n+    override def flatten(i: Iterator[(K, (Iterable[Chunk[L]], Iterable[Chunk[R]]))]) =\n+      i flatMap {\n+        case (key, pair) => {\n+          if (pair._1.size == 0) {\n+            for (chunk <- pair._2.iterator;\n+                 v <- chunk\n+            ) yield (key, (None, v)): (K, (Option[L], R))\n+          }\n+          else if (pair._1.size < pair._2.size) {\n+            yieldPair(pair._1, pair._2, key, (p1: L, p2: R) => (Some(p1), p2))\n+          } else {\n+            yieldPair(pair._2, pair._1, key, (p2: R, p1: L) => (Some(p1), p2))\n+          }\n+        }\n+      }\n+  }\n+\n+  def fullOuter[K, L, R] = new JoinType[K, L, R, (Option[L], Option[R])] {\n+\n+    override def flatten(i: Iterator[(K, (Iterable[Chunk[L]], Iterable[Chunk[R]]))]) =\n+      i flatMap {\n+        case (key, pair) => {\n+          if (pair._1.size == 0) {\n+            for (chunk <- pair._2.iterator;\n+                 v <- chunk\n+            ) yield (key, (None, Some(v))): (K, (Option[L], Option[R]))\n+          }\n+          else if (pair._2.size == 0) {\n+            for (chunk <- pair._1.iterator;\n+                 v <- chunk\n+            ) yield (key, (Some(v), None)): (K, (Option[L], Option[R]))\n+          }\n+          else if (pair._1.size < pair._2.size) {\n+            yieldPair(pair._1, pair._2, key, (p1: L, p2: R) => (Some(p1), Some(p2)))\n+          } else {\n+            yieldPair(pair._2, pair._1, key, (p2: R, p1: L) => (Some(p1), Some(p2)))\n+          }\n+        }\n+      }\n+  }\n+\n+  private def yieldPair[K, OUT, IN, PAIR <: Product2[_, _]](\n+      outer: Iterable[Chunk[OUT]], inner: Iterable[Chunk[IN]], key: K, toPair: (OUT, IN) => PAIR) =\n+    for (\n+      outerChunk <- outer.iterator;\n+      innerChunk <- inner.iterator;\n+      outerValue <- outerChunk;\n+      innerValue <- innerChunk\n+    ) yield (key, toPair(outerValue, innerValue))\n+}\n+\n+private[spark] sealed trait JoinSplitDep extends Serializable\n+\n+private[spark] case class NarrowJoinSplitDep(\n+    rdd: RDD[_],\n+    splitIndex: Int,\n+    var split: Partition) extends JoinSplitDep {\n+\n+  @throws(classOf[IOException])\n+  private def writeObject(oos: ObjectOutputStream): Unit = Utils.tryOrIOException {\n+    // Update the reference to parent split at the time of task serialization\n+    split = rdd.partitions(splitIndex)\n+    oos.defaultWriteObject()\n+  }\n+}\n+\n+private[spark] case class ShuffleJoinSplitDep(handle: ShuffleHandle) extends JoinSplitDep\n+\n+private[spark] class JoinPartition(idx: Int, val left: JoinSplitDep, val right: JoinSplitDep)\n+  extends Partition with Serializable {\n+  override val index: Int = idx\n+\n+  override def hashCode(): Int = idx\n+}\n+\n+private[spark] class BufferWrapper private(val isChunkBuffer: Boolean, buffer: Iterable[Any])\n+  extends Serializable {\n+\n+  def this(buffer: CompactBuffer[_]) {\n+    this(false, buffer)\n+  }\n+\n+  def this(buffer: ChunkBuffer[_]) {\n+    this(true, buffer)\n+  }\n+\n+  def getChunkBuffer[T: ClassTag]: ChunkBuffer[T] = buffer.asInstanceOf[ChunkBuffer[T]]\n+\n+  def getCompactBuffer[T: ClassTag]: CompactBuffer[T] = buffer.asInstanceOf[CompactBuffer[T]]\n+\n+  def asChunkIterable[T: ClassTag]: Iterable[Chunk[T]] = {\n+    if (isChunkBuffer) {\n+      getChunkBuffer[T]\n+    }\n+    else {\n+      val buffer = getCompactBuffer[T]\n+      if (buffer.isEmpty) {\n+        Iterable[Chunk[T]]()\n+      }\n+      else {\n+        Iterable(new Chunk[T](buffer))\n+      }\n+    }\n+  }\n+}\n+\n+private[spark] class SkewedJoinRDD[K, L, R, PAIR <: Product2[_, _]](\n+    left: RDD[(K, L)], right: RDD[(K, R)], part: Partitioner, joinType: JoinType[K, L, R, PAIR])\n+    (implicit kt: ClassTag[K], lt: ClassTag[L], rt: ClassTag[R], ord: Ordering[K])",
    "line": 181
  }],
  "prId": 3505
}]