[{
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "I'm not sure if we need a new RDD for this. We can just do\n\n```\nsc.parallelize(1 to numSlices, numSlices).mapPartitions { i =>\n  // generate the numbers for i-th slice\n}\n```\n",
    "commit": "cbf52002c0fb2e21c9e86d087e69041281cc411d",
    "createdAt": "2015-05-12T17:34:36Z",
    "diffHunk": "@@ -0,0 +1,69 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd\n+\n+import scala.collection.Map\n+\n+import org.apache.spark._\n+\n+private[spark] class RangeCollectionRDD("
  }],
  "prId": 6081
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "`slices(i).toArray.toSeq` will generate the whole data in the driver side, which will crash the driver easily. I'm not sure if we can reuse `ParallelCollectionPartition` here. Maybe we need to create a new partition class for `range`.\n",
    "commit": "cbf52002c0fb2e21c9e86d087e69041281cc411d",
    "createdAt": "2015-05-12T18:52:27Z",
    "diffHunk": "@@ -0,0 +1,69 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd\n+\n+import scala.collection.Map\n+\n+import org.apache.spark._\n+\n+private[spark] class RangeCollectionRDD(\n+    @transient sc: SparkContext,\n+    limit: Long,\n+    numSlices: Int,\n+    locationPrefs: Map[Int, Seq[String]])\n+  extends RDD[Long](sc, Nil) {\n+\n+  override def getPartitions: Array[Partition] = {\n+    val slices = RangeCollectionRDD.sliceIntoIterator(limit, numSlices).toArray\n+    slices.indices.map(i => new ParallelCollectionPartition(id, i, slices(i).toArray.toSeq)).toArray"
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "don't need to create a new partition class. just do what i suggested. basically all we need is the total range, total number of partitions, and partition id. then we can generate all the data on each partition.\n",
    "commit": "cbf52002c0fb2e21c9e86d087e69041281cc411d",
    "createdAt": "2015-05-12T18:58:29Z",
    "diffHunk": "@@ -0,0 +1,69 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd\n+\n+import scala.collection.Map\n+\n+import org.apache.spark._\n+\n+private[spark] class RangeCollectionRDD(\n+    @transient sc: SparkContext,\n+    limit: Long,\n+    numSlices: Int,\n+    locationPrefs: Map[Int, Seq[String]])\n+  extends RDD[Long](sc, Nil) {\n+\n+  override def getPartitions: Array[Partition] = {\n+    val slices = RangeCollectionRDD.sliceIntoIterator(limit, numSlices).toArray\n+    slices.indices.map(i => new ParallelCollectionPartition(id, i, slices(i).toArray.toSeq)).toArray"
  }, {
    "author": {
      "login": "zsxwing"
    },
    "body": "Good point. +1\n",
    "commit": "cbf52002c0fb2e21c9e86d087e69041281cc411d",
    "createdAt": "2015-05-12T19:00:13Z",
    "diffHunk": "@@ -0,0 +1,69 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd\n+\n+import scala.collection.Map\n+\n+import org.apache.spark._\n+\n+private[spark] class RangeCollectionRDD(\n+    @transient sc: SparkContext,\n+    limit: Long,\n+    numSlices: Int,\n+    locationPrefs: Map[Int, Seq[String]])\n+  extends RDD[Long](sc, Nil) {\n+\n+  override def getPartitions: Array[Partition] = {\n+    val slices = RangeCollectionRDD.sliceIntoIterator(limit, numSlices).toArray\n+    slices.indices.map(i => new ParallelCollectionPartition(id, i, slices(i).toArray.toSeq)).toArray"
  }],
  "prId": 6081
}]