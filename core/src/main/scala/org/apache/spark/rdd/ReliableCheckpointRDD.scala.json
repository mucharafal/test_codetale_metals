[{
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Do you want to catch for this separately then, rather than re-check whether it exists?\n",
    "commit": "fa8c3bebf31a03686972c5e09ea5b7703c9e81fe",
    "createdAt": "2016-07-26T19:00:43Z",
    "diffHunk": "@@ -166,17 +166,25 @@ private[spark] object ReliableCheckpointRDD extends Logging {\n     val tempOutputPath =\n       new Path(outputDir, s\".$finalOutputName-attempt-${ctx.attemptNumber()}\")\n \n-    if (fs.exists(tempOutputPath)) {\n-      throw new IOException(s\"Checkpoint failed: temporary path $tempOutputPath already exists\")\n-    }\n     val bufferSize = env.conf.getInt(\"spark.buffer.size\", 65536)\n \n-    val fileOutputStream = if (blockSize < 0) {\n-      fs.create(tempOutputPath, false, bufferSize)\n-    } else {\n-      // This is mainly for testing purpose\n-      fs.create(tempOutputPath, false, bufferSize,\n-        fs.getDefaultReplication(fs.getWorkingDirectory), blockSize)\n+    val fileOutputStream = try {\n+      if (blockSize < 0) {\n+        fs.create(tempOutputPath, false, bufferSize)\n+      } else {\n+        // This is mainly for testing purpose\n+        fs.create(tempOutputPath, false, bufferSize,\n+          fs.getDefaultReplication(fs.getWorkingDirectory), blockSize)\n+      }\n+    } catch {\n+      case e: IOException =>\n+        // could be FileAlreadyExistsException, or a more fundamental create failure"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "There's some ambiguity on create(), at least with the special case of: destination is a directory, and local filesystems don't throw what is a Hadoop exception. [filesystem spec]\n\nOne thing to consider is: why wrap any IOE here at all? Is it the message or the declaration of the path which matters?\n",
    "commit": "fa8c3bebf31a03686972c5e09ea5b7703c9e81fe",
    "createdAt": "2016-07-27T12:03:28Z",
    "diffHunk": "@@ -166,17 +166,25 @@ private[spark] object ReliableCheckpointRDD extends Logging {\n     val tempOutputPath =\n       new Path(outputDir, s\".$finalOutputName-attempt-${ctx.attemptNumber()}\")\n \n-    if (fs.exists(tempOutputPath)) {\n-      throw new IOException(s\"Checkpoint failed: temporary path $tempOutputPath already exists\")\n-    }\n     val bufferSize = env.conf.getInt(\"spark.buffer.size\", 65536)\n \n-    val fileOutputStream = if (blockSize < 0) {\n-      fs.create(tempOutputPath, false, bufferSize)\n-    } else {\n-      // This is mainly for testing purpose\n-      fs.create(tempOutputPath, false, bufferSize,\n-        fs.getDefaultReplication(fs.getWorkingDirectory), blockSize)\n+    val fileOutputStream = try {\n+      if (blockSize < 0) {\n+        fs.create(tempOutputPath, false, bufferSize)\n+      } else {\n+        // This is mainly for testing purpose\n+        fs.create(tempOutputPath, false, bufferSize,\n+          fs.getDefaultReplication(fs.getWorkingDirectory), blockSize)\n+      }\n+    } catch {\n+      case e: IOException =>\n+        // could be FileAlreadyExistsException, or a more fundamental create failure"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "changing to rely on the underlying layer to do things....create(overwrite=false) will trigger a FileAlreadyExistsException. Note that S3 and swift don't have an atomic check+create, but that's not avoidable.\n",
    "commit": "fa8c3bebf31a03686972c5e09ea5b7703c9e81fe",
    "createdAt": "2016-07-27T14:33:52Z",
    "diffHunk": "@@ -166,17 +166,25 @@ private[spark] object ReliableCheckpointRDD extends Logging {\n     val tempOutputPath =\n       new Path(outputDir, s\".$finalOutputName-attempt-${ctx.attemptNumber()}\")\n \n-    if (fs.exists(tempOutputPath)) {\n-      throw new IOException(s\"Checkpoint failed: temporary path $tempOutputPath already exists\")\n-    }\n     val bufferSize = env.conf.getInt(\"spark.buffer.size\", 65536)\n \n-    val fileOutputStream = if (blockSize < 0) {\n-      fs.create(tempOutputPath, false, bufferSize)\n-    } else {\n-      // This is mainly for testing purpose\n-      fs.create(tempOutputPath, false, bufferSize,\n-        fs.getDefaultReplication(fs.getWorkingDirectory), blockSize)\n+    val fileOutputStream = try {\n+      if (blockSize < 0) {\n+        fs.create(tempOutputPath, false, bufferSize)\n+      } else {\n+        // This is mainly for testing purpose\n+        fs.create(tempOutputPath, false, bufferSize,\n+          fs.getDefaultReplication(fs.getWorkingDirectory), blockSize)\n+      }\n+    } catch {\n+      case e: IOException =>\n+        // could be FileAlreadyExistsException, or a more fundamental create failure"
  }],
  "prId": 14371
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "I suppose this has the small disadvantage of putting more code inside a try block that isn't intended to handle exceptions from the rest. Can the other code be moved after the block, and short-circuit and return None in the exception block?\n",
    "commit": "fa8c3bebf31a03686972c5e09ea5b7703c9e81fe",
    "createdAt": "2016-07-26T19:02:21Z",
    "diffHunk": "@@ -240,7 +248,7 @@ private[spark] object ReliableCheckpointRDD extends Logging {\n       val bufferSize = sc.conf.getInt(\"spark.buffer.size\", 65536)\n       val partitionerFilePath = new Path(checkpointDirPath, checkpointPartitionerFileName)\n       val fs = partitionerFilePath.getFileSystem(sc.hadoopConfiguration)\n-      if (fs.exists(partitionerFilePath)) {\n+      try {"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "Seems to me that it can be pulled into the existing try/catch...its the only place in the codepath which can raise an FNFE; all existing failure modes (permissions, not a file, etc) would be caught there and log at warning...this just converts the FNFE to a log at debug event. \n\n-going to do that but adding the stack trace printing in the debug logs. Noisy, but your support team will love you on the one day it finally happens.\n",
    "commit": "fa8c3bebf31a03686972c5e09ea5b7703c9e81fe",
    "createdAt": "2016-07-27T14:38:18Z",
    "diffHunk": "@@ -240,7 +248,7 @@ private[spark] object ReliableCheckpointRDD extends Logging {\n       val bufferSize = sc.conf.getInt(\"spark.buffer.size\", 65536)\n       val partitionerFilePath = new Path(checkpointDirPath, checkpointPartitionerFileName)\n       val fs = partitionerFilePath.getFileSystem(sc.hadoopConfiguration)\n-      if (fs.exists(partitionerFilePath)) {\n+      try {"
  }],
  "prId": 14371
}]