[{
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "hmm, shouldn't `minPartitions` be used like this?\r\n\r\n```scala\r\nval defaultParallelism = Math.max(sc.defaultParallelism, if (minPartitions == 0) 1 else minPartitions)\r\n```",
    "commit": "5e46efb5f5ce86297c4aeb23bf934fd9942de3de",
    "createdAt": "2018-06-26T01:16:38Z",
    "diffHunk": "@@ -45,7 +45,8 @@ private[spark] abstract class StreamFileInputFormat[T]\n    * which is set through setMaxSplitSize\n    */\n   def setMinPartitions(sc: SparkContext, context: JobContext, minPartitions: Int) {\n-    val defaultMaxSplitBytes = sc.getConf.get(config.FILES_MAX_PARTITION_BYTES)\n+    val defaultMaxSplitBytes = Math.max(\n+      sc.getConf.get(config.FILES_MAX_PARTITION_BYTES), minPartitions)\n     val openCostInBytes = sc.getConf.get(config.FILES_OPEN_COST_IN_BYTES)\n     val defaultParallelism = sc.defaultParallelism"
  }, {
    "author": {
      "login": "MaxGekk"
    },
    "body": "Could you describe the use case when you need to take into account `minPartitions`. By default, `FILES_MAX_PARTITION_BYTES` is 128MB. Let's say it is even set to 1000, and `minPartitions` equals to 10 000. What is the reason to set the max size of splits in **bytes** to the min **number** of partition. Why should bigger number of partitions require bigger split size? Could you add more details to the PR description, please.",
    "commit": "5e46efb5f5ce86297c4aeb23bf934fd9942de3de",
    "createdAt": "2018-06-26T12:18:09Z",
    "diffHunk": "@@ -45,7 +45,8 @@ private[spark] abstract class StreamFileInputFormat[T]\n    * which is set through setMaxSplitSize\n    */\n   def setMinPartitions(sc: SparkContext, context: JobContext, minPartitions: Int) {\n-    val defaultMaxSplitBytes = sc.getConf.get(config.FILES_MAX_PARTITION_BYTES)\n+    val defaultMaxSplitBytes = Math.max(\n+      sc.getConf.get(config.FILES_MAX_PARTITION_BYTES), minPartitions)\n     val openCostInBytes = sc.getConf.get(config.FILES_OPEN_COST_IN_BYTES)\n     val defaultParallelism = sc.defaultParallelism"
  }],
  "prId": 21638
}, {
  "comments": [{
    "author": {
      "login": "MaxGekk"
    },
    "body": "Now it makes much more sense.",
    "commit": "5e46efb5f5ce86297c4aeb23bf934fd9942de3de",
    "createdAt": "2018-06-26T17:59:27Z",
    "diffHunk": "@@ -45,10 +45,9 @@ private[spark] abstract class StreamFileInputFormat[T]\n    * which is set through setMaxSplitSize\n    */\n   def setMinPartitions(sc: SparkContext, context: JobContext, minPartitions: Int) {\n-    val defaultMaxSplitBytes = Math.max(\n-      sc.getConf.get(config.FILES_MAX_PARTITION_BYTES), minPartitions)\n+    val defaultMaxSplitBytes = sc.getConf.get(config.FILES_MAX_PARTITION_BYTES)\n     val openCostInBytes = sc.getConf.get(config.FILES_OPEN_COST_IN_BYTES)\n-    val defaultParallelism = sc.defaultParallelism\n+    val defaultParallelism = Math.max(sc.defaultParallelism, minPartitions)"
  }],
  "prId": 21638
}, {
  "comments": [{
    "author": {
      "login": "jiangxb1987"
    },
    "body": "If `sc.defaultParallelism` < 2, and `minParititions` is not set in `BinaryFileRDD`, then previously `defaultParallelism` shall be the same as `sc.defaultParallelism`, and after the change it will be `2`. Have you already consider this case and feel it's right behavior change to make?",
    "commit": "5e46efb5f5ce86297c4aeb23bf934fd9942de3de",
    "createdAt": "2018-07-16T15:58:50Z",
    "diffHunk": "@@ -47,7 +47,7 @@ private[spark] abstract class StreamFileInputFormat[T]\n   def setMinPartitions(sc: SparkContext, context: JobContext, minPartitions: Int) {\n     val defaultMaxSplitBytes = sc.getConf.get(config.FILES_MAX_PARTITION_BYTES)\n     val openCostInBytes = sc.getConf.get(config.FILES_OPEN_COST_IN_BYTES)\n-    val defaultParallelism = sc.defaultParallelism\n+    val defaultParallelism = Math.max(sc.defaultParallelism, minPartitions)",
    "line": 5
  }, {
    "author": {
      "login": "bomeng"
    },
    "body": "you need to pass in the minPartitions to use this method, what do you mean minParititions is not set? ",
    "commit": "5e46efb5f5ce86297c4aeb23bf934fd9942de3de",
    "createdAt": "2018-07-17T06:38:10Z",
    "diffHunk": "@@ -47,7 +47,7 @@ private[spark] abstract class StreamFileInputFormat[T]\n   def setMinPartitions(sc: SparkContext, context: JobContext, minPartitions: Int) {\n     val defaultMaxSplitBytes = sc.getConf.get(config.FILES_MAX_PARTITION_BYTES)\n     val openCostInBytes = sc.getConf.get(config.FILES_OPEN_COST_IN_BYTES)\n-    val defaultParallelism = sc.defaultParallelism\n+    val defaultParallelism = Math.max(sc.defaultParallelism, minPartitions)",
    "line": 5
  }, {
    "author": {
      "login": "jiangxb1987"
    },
    "body": "I metioned `BinaryFileRDD` not this method, you can check the code to see how it handles the default value.",
    "commit": "5e46efb5f5ce86297c4aeb23bf934fd9942de3de",
    "createdAt": "2018-07-19T02:56:05Z",
    "diffHunk": "@@ -47,7 +47,7 @@ private[spark] abstract class StreamFileInputFormat[T]\n   def setMinPartitions(sc: SparkContext, context: JobContext, minPartitions: Int) {\n     val defaultMaxSplitBytes = sc.getConf.get(config.FILES_MAX_PARTITION_BYTES)\n     val openCostInBytes = sc.getConf.get(config.FILES_OPEN_COST_IN_BYTES)\n-    val defaultParallelism = sc.defaultParallelism\n+    val defaultParallelism = Math.max(sc.defaultParallelism, minPartitions)",
    "line": 5
  }, {
    "author": {
      "login": "bomeng"
    },
    "body": "BinaryFileRDD will set minPartitions, which will either be defaultMinPartitions, or the values you can set via binaryFiles(path, minPartitions) method. Eventually, this minPartitions value will be passed to setMinPartitions() method.",
    "commit": "5e46efb5f5ce86297c4aeb23bf934fd9942de3de",
    "createdAt": "2018-07-23T18:58:13Z",
    "diffHunk": "@@ -47,7 +47,7 @@ private[spark] abstract class StreamFileInputFormat[T]\n   def setMinPartitions(sc: SparkContext, context: JobContext, minPartitions: Int) {\n     val defaultMaxSplitBytes = sc.getConf.get(config.FILES_MAX_PARTITION_BYTES)\n     val openCostInBytes = sc.getConf.get(config.FILES_OPEN_COST_IN_BYTES)\n-    val defaultParallelism = sc.defaultParallelism\n+    val defaultParallelism = Math.max(sc.defaultParallelism, minPartitions)",
    "line": 5
  }],
  "prId": 21638
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "We should have a test case; otherwise, we could hit the same issue again. ",
    "commit": "5e46efb5f5ce86297c4aeb23bf934fd9942de3de",
    "createdAt": "2018-09-03T05:48:38Z",
    "diffHunk": "@@ -47,7 +47,7 @@ private[spark] abstract class StreamFileInputFormat[T]\n   def setMinPartitions(sc: SparkContext, context: JobContext, minPartitions: Int) {\n     val defaultMaxSplitBytes = sc.getConf.get(config.FILES_MAX_PARTITION_BYTES)\n     val openCostInBytes = sc.getConf.get(config.FILES_OPEN_COST_IN_BYTES)\n-    val defaultParallelism = sc.defaultParallelism\n+    val defaultParallelism = Math.max(sc.defaultParallelism, minPartitions)",
    "line": 5
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "BTW, it is easy to add such a test case. We can even test the behaviors of the boundary cases. cc @srowen @HyukjinKwon @MaxGekk @jiangxb1987 ",
    "commit": "5e46efb5f5ce86297c4aeb23bf934fd9942de3de",
    "createdAt": "2018-09-03T05:50:14Z",
    "diffHunk": "@@ -47,7 +47,7 @@ private[spark] abstract class StreamFileInputFormat[T]\n   def setMinPartitions(sc: SparkContext, context: JobContext, minPartitions: Int) {\n     val defaultMaxSplitBytes = sc.getConf.get(config.FILES_MAX_PARTITION_BYTES)\n     val openCostInBytes = sc.getConf.get(config.FILES_OPEN_COST_IN_BYTES)\n-    val defaultParallelism = sc.defaultParallelism\n+    val defaultParallelism = Math.max(sc.defaultParallelism, minPartitions)",
    "line": 5
  }, {
    "author": {
      "login": "srowen"
    },
    "body": "I think it's hard to test, technically, because `setMinPartitions` is only a hint. In the case of `binaryFiles` we know it will put a hard limit on the number of partitions, but it isn't true of other implementations. We can still make a simple test for all of these, it just may be asserting behavior that could change in the future in Hadoop, though I strongly doubt it would.",
    "commit": "5e46efb5f5ce86297c4aeb23bf934fd9942de3de",
    "createdAt": "2018-09-03T13:14:14Z",
    "diffHunk": "@@ -47,7 +47,7 @@ private[spark] abstract class StreamFileInputFormat[T]\n   def setMinPartitions(sc: SparkContext, context: JobContext, minPartitions: Int) {\n     val defaultMaxSplitBytes = sc.getConf.get(config.FILES_MAX_PARTITION_BYTES)\n     val openCostInBytes = sc.getConf.get(config.FILES_OPEN_COST_IN_BYTES)\n-    val defaultParallelism = sc.defaultParallelism\n+    val defaultParallelism = Math.max(sc.defaultParallelism, minPartitions)",
    "line": 5
  }, {
    "author": {
      "login": "bomeng"
    },
    "body": "I agree it is hard to test. I appreciate If anyone can give me some hints of how to do these (how to verify and where to put my test cases). ",
    "commit": "5e46efb5f5ce86297c4aeb23bf934fd9942de3de",
    "createdAt": "2018-09-04T17:55:49Z",
    "diffHunk": "@@ -47,7 +47,7 @@ private[spark] abstract class StreamFileInputFormat[T]\n   def setMinPartitions(sc: SparkContext, context: JobContext, minPartitions: Int) {\n     val defaultMaxSplitBytes = sc.getConf.get(config.FILES_MAX_PARTITION_BYTES)\n     val openCostInBytes = sc.getConf.get(config.FILES_OPEN_COST_IN_BYTES)\n-    val defaultParallelism = sc.defaultParallelism\n+    val defaultParallelism = Math.max(sc.defaultParallelism, minPartitions)",
    "line": 5
  }, {
    "author": {
      "login": "srowen"
    },
    "body": "Would you mind following up with a test that just asserts that asking for, say, 20 partitions results in 20 partitions? This is technically too specific as a test, but is probably fine for now.",
    "commit": "5e46efb5f5ce86297c4aeb23bf934fd9942de3de",
    "createdAt": "2018-09-04T18:17:05Z",
    "diffHunk": "@@ -47,7 +47,7 @@ private[spark] abstract class StreamFileInputFormat[T]\n   def setMinPartitions(sc: SparkContext, context: JobContext, minPartitions: Int) {\n     val defaultMaxSplitBytes = sc.getConf.get(config.FILES_MAX_PARTITION_BYTES)\n     val openCostInBytes = sc.getConf.get(config.FILES_OPEN_COST_IN_BYTES)\n-    val defaultParallelism = sc.defaultParallelism\n+    val defaultParallelism = Math.max(sc.defaultParallelism, minPartitions)",
    "line": 5
  }, {
    "author": {
      "login": "bomeng"
    },
    "body": "From the codes, you can see the calculation is just the intermediate result and this method won't return any value. Checking the split size does not make sense for this test case because it depends on multiple variables and this is just one of them.",
    "commit": "5e46efb5f5ce86297c4aeb23bf934fd9942de3de",
    "createdAt": "2018-09-04T18:35:08Z",
    "diffHunk": "@@ -47,7 +47,7 @@ private[spark] abstract class StreamFileInputFormat[T]\n   def setMinPartitions(sc: SparkContext, context: JobContext, minPartitions: Int) {\n     val defaultMaxSplitBytes = sc.getConf.get(config.FILES_MAX_PARTITION_BYTES)\n     val openCostInBytes = sc.getConf.get(config.FILES_OPEN_COST_IN_BYTES)\n-    val defaultParallelism = sc.defaultParallelism\n+    val defaultParallelism = Math.max(sc.defaultParallelism, minPartitions)",
    "line": 5
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "```\r\n      sc = new SparkContext(new SparkConf().setAppName(\"test\").setMaster(\"local\")\r\n        .set(config.FILES_OPEN_COST_IN_BYTES.key, \"0\")\r\n        .set(\"spark.default.parallelism\", \"1\"))\r\n\r\n      println(sc.binaryFiles(dirpath1, minPartitions = 50).getNumPartitions)\r\n      println(sc.binaryFiles(dirpath1, minPartitions = 1).getNumPartitions)\r\n```\r\n\r\nIt is not hard to verify whether the parameter `minPartitions` takes an effect. Currently, the description of this parameter is not clear. We need to document it clear which factors impact the actual number of partitions; otherwise, users will not understand how to use it. ",
    "commit": "5e46efb5f5ce86297c4aeb23bf934fd9942de3de",
    "createdAt": "2018-09-04T19:00:16Z",
    "diffHunk": "@@ -47,7 +47,7 @@ private[spark] abstract class StreamFileInputFormat[T]\n   def setMinPartitions(sc: SparkContext, context: JobContext, minPartitions: Int) {\n     val defaultMaxSplitBytes = sc.getConf.get(config.FILES_MAX_PARTITION_BYTES)\n     val openCostInBytes = sc.getConf.get(config.FILES_OPEN_COST_IN_BYTES)\n-    val defaultParallelism = sc.defaultParallelism\n+    val defaultParallelism = Math.max(sc.defaultParallelism, minPartitions)",
    "line": 5
  }],
  "prId": 21638
}]