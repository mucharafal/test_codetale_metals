[{
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "Note to self: make sure DefaultFileRegion can be released properly in error conditions.\n",
    "commit": "f9214216fe33e365375a071e522cc915be8acd67",
    "createdAt": "2014-08-12T18:34:32Z",
    "diffHunk": "@@ -0,0 +1,290 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.network.netty\n+\n+import java.io.FileInputStream\n+import java.net.InetSocketAddress\n+import java.nio.channels.FileChannel\n+\n+import io.netty.bootstrap.ServerBootstrap\n+import io.netty.buffer.{ByteBuf, PooledByteBufAllocator}\n+import io.netty.channel._\n+import io.netty.channel.epoll.{EpollEventLoopGroup, EpollServerSocketChannel}\n+import io.netty.channel.nio.NioEventLoopGroup\n+import io.netty.channel.oio.OioEventLoopGroup\n+import io.netty.channel.socket.SocketChannel\n+import io.netty.channel.socket.nio.NioServerSocketChannel\n+import io.netty.channel.socket.oio.OioServerSocketChannel\n+import io.netty.handler.codec.{MessageToByteEncoder, LineBasedFrameDecoder}\n+import io.netty.handler.codec.string.StringDecoder\n+import io.netty.util.CharsetUtil\n+import org.apache.spark.storage.{TestBlockId, FileSegment, BlockId}\n+\n+import org.apache.spark.{Logging, SparkConf}\n+import org.apache.spark.util.Utils\n+\n+\n+// TODO: Remove dependency on BlockId. This layer should not be coupled with storage.\n+\n+// TODO: PathResolver is not general enough. It only works for on-disk blocks.\n+\n+// TODO: Allow user-configured port\n+\n+/** A simple main function for testing the server. */\n+object BlockServer {\n+  def main(args: Array[String]): Unit = {\n+    new BlockServer(new SparkConf, new PathResolver {\n+      override def getBlockLocation(blockId: BlockId): FileSegment = {\n+        val file = new java.io.File(blockId.asInstanceOf[TestBlockId].id)\n+        new FileSegment(file, 0, file.length())\n+      }\n+    })\n+    Thread.sleep(1000000)\n+  }\n+}\n+\n+\n+/**\n+ * Server for serving Spark data blocks. This should be used together with [[BlockFetchingClient]].\n+ *\n+ * Protocol for requesting blocks (client to server):\n+ *   One block id per line, e.g. to request 3 blocks: \"block1\\nblock2\\nblock3\\n\"\n+ *\n+ * Protocol for sending blocks (server to client):\n+ *   frame-length (4 bytes), block-id-length (4 bytes), block-id, block-data.\n+ *\n+ *   frame-length should not include the length of itself.\n+ *   If frame-length is negative, then this is an error message rather than block-data. The real\n+ *   length is the absolute value of the frame-length.\n+ *\n+ */\n+private[spark]\n+class BlockServer(conf: SparkConf, pResolver: PathResolver) extends Logging {\n+\n+  def port: Int = _port\n+\n+  private var _port: Int = 0\n+  private var bootstrap: ServerBootstrap = _\n+  private var channelFuture: ChannelFuture = _\n+\n+  init()\n+\n+  /** Initialize the server. */\n+  private def init(): Unit = {\n+    bootstrap = new ServerBootstrap\n+    val bossThreadFactory = Utils.namedThreadFactory(\"spark-shuffle-server-boss\")\n+    val workerThreadFactory = Utils.namedThreadFactory(\"spark-shuffle-server-worker\")\n+\n+    // Use only one thread to accept connections, and 2 * num_cores for worker.\n+    def initNio(): Unit = {\n+      val bossGroup = new NioEventLoopGroup(1, bossThreadFactory)\n+      val workerGroup = new NioEventLoopGroup(0, workerThreadFactory)\n+      bootstrap.group(bossGroup, workerGroup).channel(classOf[NioServerSocketChannel])\n+    }\n+    def initOio(): Unit = {\n+      val bossGroup = new OioEventLoopGroup(1, bossThreadFactory)\n+      val workerGroup = new OioEventLoopGroup(0, workerThreadFactory)\n+      bootstrap.group(bossGroup, workerGroup).channel(classOf[OioServerSocketChannel])\n+    }\n+    def initEpoll(): Unit = {\n+      val bossGroup = new EpollEventLoopGroup(1, bossThreadFactory)\n+      val workerGroup = new EpollEventLoopGroup(0, workerThreadFactory)\n+      bootstrap.group(bossGroup, workerGroup).channel(classOf[EpollServerSocketChannel])\n+    }\n+\n+    conf.get(\"spark.shuffle.io.mode\", \"auto\").toLowerCase match {\n+      case \"nio\" => initNio()\n+      case \"oio\" => initOio()\n+      case \"epoll\" => initEpoll()\n+      case \"auto\" =>\n+        // For auto mode, first try epoll (only available on Linux), then nio.\n+        try {\n+          initEpoll()\n+        } catch {\n+          // TODO: Should we log the throwable? But that always happen on non-Linux systems.\n+          // Perhaps the right thing to do is to check whether the system is Linux, and then only\n+          // call initEpoll on Linux.\n+          case e: Throwable => initNio()\n+        }\n+    }\n+\n+    // Use pooled buffers to reduce temporary buffer allocation\n+    bootstrap.option(ChannelOption.ALLOCATOR, PooledByteBufAllocator.DEFAULT)\n+    bootstrap.childOption(ChannelOption.ALLOCATOR, PooledByteBufAllocator.DEFAULT)\n+\n+    // Various (advanced) user-configured settings.\n+    conf.getOption(\"spark.shuffle.io.backLog\").foreach { backLog =>\n+      bootstrap.option[java.lang.Integer](ChannelOption.SO_BACKLOG, backLog.toInt)\n+    }\n+    // Note: the optimal size for receive buffer and send buffer should be\n+    //  latency * network_bandwidth.\n+    // Assuming latency = 1ms, network_bandwidth = 10Gbps\n+    // buffer size should be ~ 1.25MB\n+    conf.getOption(\"spark.shuffle.io.receiveBuffer\").foreach { receiveBuf =>\n+      bootstrap.option[java.lang.Integer](ChannelOption.SO_RCVBUF, receiveBuf.toInt)\n+    }\n+    conf.getOption(\"spark.shuffle.io.sendBuffer\").foreach { sendBuf =>\n+      bootstrap.option[java.lang.Integer](ChannelOption.SO_SNDBUF, sendBuf.toInt)\n+    }\n+\n+    bootstrap.childHandler(new ChannelInitializer[SocketChannel] {\n+      override def initChannel(ch: SocketChannel): Unit = {\n+        ch.pipeline\n+          .addLast(\"frameDecoder\", new LineBasedFrameDecoder(1024))  // max block id length 1024\n+          .addLast(\"stringDecoder\", new StringDecoder(CharsetUtil.UTF_8))\n+          //.addLast(\"stringEncoder\", new StringEncoder(CharsetUtil.UTF_8))\n+          .addLast(\"blockHeaderEncoder\", new BlockHeaderEncoder)\n+          .addLast(\"handler\", new BlockServerHandler(pResolver))\n+      }\n+    })\n+\n+    channelFuture = bootstrap.bind(new InetSocketAddress(_port))\n+    channelFuture.sync()\n+\n+    val addr = channelFuture.channel.localAddress.asInstanceOf[InetSocketAddress]\n+    _port = addr.getPort\n+  }\n+\n+  /** Shutdown the server. */\n+  def stop(): Unit = {\n+    if (channelFuture != null) {\n+      channelFuture.channel().close().awaitUninterruptibly()\n+      channelFuture = null\n+    }\n+    if (bootstrap != null && bootstrap.group() != null) {\n+      bootstrap.group().shutdownGracefully()\n+    }\n+    if (bootstrap != null && bootstrap.childGroup() != null) {\n+      bootstrap.childGroup().shutdownGracefully()\n+    }\n+    bootstrap = null\n+  }\n+}\n+\n+\n+/** A handler that writes the content of a block to the channel. */\n+class BlockServerHandler(p: PathResolver)\n+  extends SimpleChannelInboundHandler[String] with Logging {\n+\n+  override def channelRead0(ctx: ChannelHandlerContext, blockId: String): Unit = {\n+    // client in the form of hostname:port\n+    val client = {\n+      val remoteAddr = ctx.channel.remoteAddress.asInstanceOf[InetSocketAddress]\n+      remoteAddr.getHostName + \":\" + remoteAddr.getPort\n+    }\n+\n+    // A helper function to send error message back to the client.\n+    def respondWithError(error: String): Unit = {\n+      ctx.writeAndFlush(new BlockHeader(-1, blockId, Some(error))).addListener(\n+        new ChannelFutureListener {\n+          override def operationComplete(future: ChannelFuture) {\n+            if (!future.isSuccess) {\n+              // TODO: Maybe log the success case as well.\n+              logError(s\"Error sending error back to $client\", future.cause)\n+              ctx.close()\n+            }\n+          }\n+        }\n+      )\n+    }\n+\n+    logTrace(s\"Received request from $client to fetch block $blockId\")\n+\n+    var fileChannel: FileChannel = null\n+    var offset: Long = 0\n+    var blockLength: Long = 0\n+\n+    // First make sure we can find the block.\n+    try {\n+      val segment = p.getBlockLocation(BlockId(blockId))\n+      fileChannel = new FileInputStream(segment.file).getChannel\n+      offset = segment.offset\n+      blockLength = segment.length\n+    } catch {\n+      case e: Exception =>\n+        // If we fail to find the block and get its size, send error back.\n+        logError(s\"Error opening block $blockId for request from $client\", e)\n+        blockLength = -1\n+        respondWithError(e.getMessage)\n+    }\n+\n+    // Send error message back if the block is too large. Even though we are capable of sending\n+    // large (2G+) blocks, the receiving end cannot handle it so let's fail fast.\n+    // Once we fixed the receiving end to be able to process large blocks, this should be removed.\n+    // Also make sure we update BlockHeaderEncoder to support length > 2G.\n+    if (blockLength > Int.MaxValue) {\n+      respondWithError(s\"Block $blockId size ($blockLength) greater than 2G\")\n+    }\n+\n+    // Found the block. Send it back.\n+    if (fileChannel != null && blockLength >= 0) {\n+      val listener = new ChannelFutureListener {\n+        override def operationComplete(future: ChannelFuture) {\n+          if (future.isSuccess) {\n+            logTrace(s\"Sent block $blockId ($blockLength B) back to $client\")\n+          } else {\n+            logError(s\"Error sending block $blockId to $client; closing connection\", future.cause)\n+            ctx.close()\n+          }\n+        }\n+      }\n+      val region = new DefaultFileRegion(fileChannel, offset, blockLength)"
  }],
  "prId": 1907
}, {
  "comments": [{
    "author": {
      "login": "coderplay"
    },
    "body": "call ctx.write(..) instead in order to  save a syscall.\n",
    "commit": "f9214216fe33e365375a071e522cc915be8acd67",
    "createdAt": "2014-08-12T18:49:56Z",
    "diffHunk": "@@ -0,0 +1,290 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.network.netty\n+\n+import java.io.FileInputStream\n+import java.net.InetSocketAddress\n+import java.nio.channels.FileChannel\n+\n+import io.netty.bootstrap.ServerBootstrap\n+import io.netty.buffer.{ByteBuf, PooledByteBufAllocator}\n+import io.netty.channel._\n+import io.netty.channel.epoll.{EpollEventLoopGroup, EpollServerSocketChannel}\n+import io.netty.channel.nio.NioEventLoopGroup\n+import io.netty.channel.oio.OioEventLoopGroup\n+import io.netty.channel.socket.SocketChannel\n+import io.netty.channel.socket.nio.NioServerSocketChannel\n+import io.netty.channel.socket.oio.OioServerSocketChannel\n+import io.netty.handler.codec.{MessageToByteEncoder, LineBasedFrameDecoder}\n+import io.netty.handler.codec.string.StringDecoder\n+import io.netty.util.CharsetUtil\n+import org.apache.spark.storage.{TestBlockId, FileSegment, BlockId}\n+\n+import org.apache.spark.{Logging, SparkConf}\n+import org.apache.spark.util.Utils\n+\n+\n+// TODO: Remove dependency on BlockId. This layer should not be coupled with storage.\n+\n+// TODO: PathResolver is not general enough. It only works for on-disk blocks.\n+\n+// TODO: Allow user-configured port\n+\n+/** A simple main function for testing the server. */\n+object BlockServer {\n+  def main(args: Array[String]): Unit = {\n+    new BlockServer(new SparkConf, new PathResolver {\n+      override def getBlockLocation(blockId: BlockId): FileSegment = {\n+        val file = new java.io.File(blockId.asInstanceOf[TestBlockId].id)\n+        new FileSegment(file, 0, file.length())\n+      }\n+    })\n+    Thread.sleep(1000000)\n+  }\n+}\n+\n+\n+/**\n+ * Server for serving Spark data blocks. This should be used together with [[BlockFetchingClient]].\n+ *\n+ * Protocol for requesting blocks (client to server):\n+ *   One block id per line, e.g. to request 3 blocks: \"block1\\nblock2\\nblock3\\n\"\n+ *\n+ * Protocol for sending blocks (server to client):\n+ *   frame-length (4 bytes), block-id-length (4 bytes), block-id, block-data.\n+ *\n+ *   frame-length should not include the length of itself.\n+ *   If frame-length is negative, then this is an error message rather than block-data. The real\n+ *   length is the absolute value of the frame-length.\n+ *\n+ */\n+private[spark]\n+class BlockServer(conf: SparkConf, pResolver: PathResolver) extends Logging {\n+\n+  def port: Int = _port\n+\n+  private var _port: Int = 0\n+  private var bootstrap: ServerBootstrap = _\n+  private var channelFuture: ChannelFuture = _\n+\n+  init()\n+\n+  /** Initialize the server. */\n+  private def init(): Unit = {\n+    bootstrap = new ServerBootstrap\n+    val bossThreadFactory = Utils.namedThreadFactory(\"spark-shuffle-server-boss\")\n+    val workerThreadFactory = Utils.namedThreadFactory(\"spark-shuffle-server-worker\")\n+\n+    // Use only one thread to accept connections, and 2 * num_cores for worker.\n+    def initNio(): Unit = {\n+      val bossGroup = new NioEventLoopGroup(1, bossThreadFactory)\n+      val workerGroup = new NioEventLoopGroup(0, workerThreadFactory)\n+      bootstrap.group(bossGroup, workerGroup).channel(classOf[NioServerSocketChannel])\n+    }\n+    def initOio(): Unit = {\n+      val bossGroup = new OioEventLoopGroup(1, bossThreadFactory)\n+      val workerGroup = new OioEventLoopGroup(0, workerThreadFactory)\n+      bootstrap.group(bossGroup, workerGroup).channel(classOf[OioServerSocketChannel])\n+    }\n+    def initEpoll(): Unit = {\n+      val bossGroup = new EpollEventLoopGroup(1, bossThreadFactory)\n+      val workerGroup = new EpollEventLoopGroup(0, workerThreadFactory)\n+      bootstrap.group(bossGroup, workerGroup).channel(classOf[EpollServerSocketChannel])\n+    }\n+\n+    conf.get(\"spark.shuffle.io.mode\", \"auto\").toLowerCase match {\n+      case \"nio\" => initNio()\n+      case \"oio\" => initOio()\n+      case \"epoll\" => initEpoll()\n+      case \"auto\" =>\n+        // For auto mode, first try epoll (only available on Linux), then nio.\n+        try {\n+          initEpoll()\n+        } catch {\n+          // TODO: Should we log the throwable? But that always happen on non-Linux systems.\n+          // Perhaps the right thing to do is to check whether the system is Linux, and then only\n+          // call initEpoll on Linux.\n+          case e: Throwable => initNio()\n+        }\n+    }\n+\n+    // Use pooled buffers to reduce temporary buffer allocation\n+    bootstrap.option(ChannelOption.ALLOCATOR, PooledByteBufAllocator.DEFAULT)\n+    bootstrap.childOption(ChannelOption.ALLOCATOR, PooledByteBufAllocator.DEFAULT)\n+\n+    // Various (advanced) user-configured settings.\n+    conf.getOption(\"spark.shuffle.io.backLog\").foreach { backLog =>\n+      bootstrap.option[java.lang.Integer](ChannelOption.SO_BACKLOG, backLog.toInt)\n+    }\n+    // Note: the optimal size for receive buffer and send buffer should be\n+    //  latency * network_bandwidth.\n+    // Assuming latency = 1ms, network_bandwidth = 10Gbps\n+    // buffer size should be ~ 1.25MB\n+    conf.getOption(\"spark.shuffle.io.receiveBuffer\").foreach { receiveBuf =>\n+      bootstrap.option[java.lang.Integer](ChannelOption.SO_RCVBUF, receiveBuf.toInt)\n+    }\n+    conf.getOption(\"spark.shuffle.io.sendBuffer\").foreach { sendBuf =>\n+      bootstrap.option[java.lang.Integer](ChannelOption.SO_SNDBUF, sendBuf.toInt)\n+    }\n+\n+    bootstrap.childHandler(new ChannelInitializer[SocketChannel] {\n+      override def initChannel(ch: SocketChannel): Unit = {\n+        ch.pipeline\n+          .addLast(\"frameDecoder\", new LineBasedFrameDecoder(1024))  // max block id length 1024\n+          .addLast(\"stringDecoder\", new StringDecoder(CharsetUtil.UTF_8))\n+          //.addLast(\"stringEncoder\", new StringEncoder(CharsetUtil.UTF_8))\n+          .addLast(\"blockHeaderEncoder\", new BlockHeaderEncoder)\n+          .addLast(\"handler\", new BlockServerHandler(pResolver))\n+      }\n+    })\n+\n+    channelFuture = bootstrap.bind(new InetSocketAddress(_port))\n+    channelFuture.sync()\n+\n+    val addr = channelFuture.channel.localAddress.asInstanceOf[InetSocketAddress]\n+    _port = addr.getPort\n+  }\n+\n+  /** Shutdown the server. */\n+  def stop(): Unit = {\n+    if (channelFuture != null) {\n+      channelFuture.channel().close().awaitUninterruptibly()\n+      channelFuture = null\n+    }\n+    if (bootstrap != null && bootstrap.group() != null) {\n+      bootstrap.group().shutdownGracefully()\n+    }\n+    if (bootstrap != null && bootstrap.childGroup() != null) {\n+      bootstrap.childGroup().shutdownGracefully()\n+    }\n+    bootstrap = null\n+  }\n+}\n+\n+\n+/** A handler that writes the content of a block to the channel. */\n+class BlockServerHandler(p: PathResolver)\n+  extends SimpleChannelInboundHandler[String] with Logging {\n+\n+  override def channelRead0(ctx: ChannelHandlerContext, blockId: String): Unit = {\n+    // client in the form of hostname:port\n+    val client = {\n+      val remoteAddr = ctx.channel.remoteAddress.asInstanceOf[InetSocketAddress]\n+      remoteAddr.getHostName + \":\" + remoteAddr.getPort\n+    }\n+\n+    // A helper function to send error message back to the client.\n+    def respondWithError(error: String): Unit = {\n+      ctx.writeAndFlush(new BlockHeader(-1, blockId, Some(error))).addListener(\n+        new ChannelFutureListener {\n+          override def operationComplete(future: ChannelFuture) {\n+            if (!future.isSuccess) {\n+              // TODO: Maybe log the success case as well.\n+              logError(s\"Error sending error back to $client\", future.cause)\n+              ctx.close()\n+            }\n+          }\n+        }\n+      )\n+    }\n+\n+    logTrace(s\"Received request from $client to fetch block $blockId\")\n+\n+    var fileChannel: FileChannel = null\n+    var offset: Long = 0\n+    var blockLength: Long = 0\n+\n+    // First make sure we can find the block.\n+    try {\n+      val segment = p.getBlockLocation(BlockId(blockId))\n+      fileChannel = new FileInputStream(segment.file).getChannel\n+      offset = segment.offset\n+      blockLength = segment.length\n+    } catch {\n+      case e: Exception =>\n+        // If we fail to find the block and get its size, send error back.\n+        logError(s\"Error opening block $blockId for request from $client\", e)\n+        blockLength = -1\n+        respondWithError(e.getMessage)\n+    }\n+\n+    // Send error message back if the block is too large. Even though we are capable of sending\n+    // large (2G+) blocks, the receiving end cannot handle it so let's fail fast.\n+    // Once we fixed the receiving end to be able to process large blocks, this should be removed.\n+    // Also make sure we update BlockHeaderEncoder to support length > 2G.\n+    if (blockLength > Int.MaxValue) {\n+      respondWithError(s\"Block $blockId size ($blockLength) greater than 2G\")\n+    }\n+\n+    // Found the block. Send it back.\n+    if (fileChannel != null && blockLength >= 0) {\n+      val listener = new ChannelFutureListener {\n+        override def operationComplete(future: ChannelFuture) {\n+          if (future.isSuccess) {\n+            logTrace(s\"Sent block $blockId ($blockLength B) back to $client\")\n+          } else {\n+            logError(s\"Error sending block $blockId to $client; closing connection\", future.cause)\n+            ctx.close()\n+          }\n+        }\n+      }\n+      val region = new DefaultFileRegion(fileChannel, offset, blockLength)\n+      ctx.writeAndFlush(new BlockHeader(blockLength.toInt, blockId)).addListener(listener)"
  }],
  "prId": 1907
}, {
  "comments": [{
    "author": {
      "login": "coderplay"
    },
    "body": "Because most of the work of this block server is io,  so setIoRatio() be higher will gain a better performance. by default it's 50\n",
    "commit": "f9214216fe33e365375a071e522cc915be8acd67",
    "createdAt": "2014-08-12T18:52:21Z",
    "diffHunk": "@@ -0,0 +1,290 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.network.netty\n+\n+import java.io.FileInputStream\n+import java.net.InetSocketAddress\n+import java.nio.channels.FileChannel\n+\n+import io.netty.bootstrap.ServerBootstrap\n+import io.netty.buffer.{ByteBuf, PooledByteBufAllocator}\n+import io.netty.channel._\n+import io.netty.channel.epoll.{EpollEventLoopGroup, EpollServerSocketChannel}\n+import io.netty.channel.nio.NioEventLoopGroup\n+import io.netty.channel.oio.OioEventLoopGroup\n+import io.netty.channel.socket.SocketChannel\n+import io.netty.channel.socket.nio.NioServerSocketChannel\n+import io.netty.channel.socket.oio.OioServerSocketChannel\n+import io.netty.handler.codec.{MessageToByteEncoder, LineBasedFrameDecoder}\n+import io.netty.handler.codec.string.StringDecoder\n+import io.netty.util.CharsetUtil\n+import org.apache.spark.storage.{TestBlockId, FileSegment, BlockId}\n+\n+import org.apache.spark.{Logging, SparkConf}\n+import org.apache.spark.util.Utils\n+\n+\n+// TODO: Remove dependency on BlockId. This layer should not be coupled with storage.\n+\n+// TODO: PathResolver is not general enough. It only works for on-disk blocks.\n+\n+// TODO: Allow user-configured port\n+\n+/** A simple main function for testing the server. */\n+object BlockServer {\n+  def main(args: Array[String]): Unit = {\n+    new BlockServer(new SparkConf, new PathResolver {\n+      override def getBlockLocation(blockId: BlockId): FileSegment = {\n+        val file = new java.io.File(blockId.asInstanceOf[TestBlockId].id)\n+        new FileSegment(file, 0, file.length())\n+      }\n+    })\n+    Thread.sleep(1000000)\n+  }\n+}\n+\n+\n+/**\n+ * Server for serving Spark data blocks. This should be used together with [[BlockFetchingClient]].\n+ *\n+ * Protocol for requesting blocks (client to server):\n+ *   One block id per line, e.g. to request 3 blocks: \"block1\\nblock2\\nblock3\\n\"\n+ *\n+ * Protocol for sending blocks (server to client):\n+ *   frame-length (4 bytes), block-id-length (4 bytes), block-id, block-data.\n+ *\n+ *   frame-length should not include the length of itself.\n+ *   If frame-length is negative, then this is an error message rather than block-data. The real\n+ *   length is the absolute value of the frame-length.\n+ *\n+ */\n+private[spark]\n+class BlockServer(conf: SparkConf, pResolver: PathResolver) extends Logging {\n+\n+  def port: Int = _port\n+\n+  private var _port: Int = 0\n+  private var bootstrap: ServerBootstrap = _\n+  private var channelFuture: ChannelFuture = _\n+\n+  init()\n+\n+  /** Initialize the server. */\n+  private def init(): Unit = {\n+    bootstrap = new ServerBootstrap\n+    val bossThreadFactory = Utils.namedThreadFactory(\"spark-shuffle-server-boss\")\n+    val workerThreadFactory = Utils.namedThreadFactory(\"spark-shuffle-server-worker\")\n+\n+    // Use only one thread to accept connections, and 2 * num_cores for worker.\n+    def initNio(): Unit = {\n+      val bossGroup = new NioEventLoopGroup(1, bossThreadFactory)\n+      val workerGroup = new NioEventLoopGroup(0, workerThreadFactory)\n+      bootstrap.group(bossGroup, workerGroup).channel(classOf[NioServerSocketChannel])\n+    }\n+    def initOio(): Unit = {\n+      val bossGroup = new OioEventLoopGroup(1, bossThreadFactory)\n+      val workerGroup = new OioEventLoopGroup(0, workerThreadFactory)\n+      bootstrap.group(bossGroup, workerGroup).channel(classOf[OioServerSocketChannel])\n+    }\n+    def initEpoll(): Unit = {\n+      val bossGroup = new EpollEventLoopGroup(1, bossThreadFactory)\n+      val workerGroup = new EpollEventLoopGroup(0, workerThreadFactory)\n+      bootstrap.group(bossGroup, workerGroup).channel(classOf[EpollServerSocketChannel])\n+    }\n+\n+    conf.get(\"spark.shuffle.io.mode\", \"auto\").toLowerCase match {\n+      case \"nio\" => initNio()\n+      case \"oio\" => initOio()\n+      case \"epoll\" => initEpoll()\n+      case \"auto\" =>\n+        // For auto mode, first try epoll (only available on Linux), then nio.\n+        try {\n+          initEpoll()\n+        } catch {\n+          // TODO: Should we log the throwable? But that always happen on non-Linux systems.\n+          // Perhaps the right thing to do is to check whether the system is Linux, and then only\n+          // call initEpoll on Linux.\n+          case e: Throwable => initNio()\n+        }\n+    }\n+\n+    // Use pooled buffers to reduce temporary buffer allocation\n+    bootstrap.option(ChannelOption.ALLOCATOR, PooledByteBufAllocator.DEFAULT)\n+    bootstrap.childOption(ChannelOption.ALLOCATOR, PooledByteBufAllocator.DEFAULT)\n+\n+    // Various (advanced) user-configured settings.\n+    conf.getOption(\"spark.shuffle.io.backLog\").foreach { backLog =>"
  }],
  "prId": 1907
}, {
  "comments": [{
    "author": {
      "login": "coderplay"
    },
    "body": "What will happen if write blockheader fail?  The former listener will close the connection then the latter listener call ctx.close() again? \n",
    "commit": "f9214216fe33e365375a071e522cc915be8acd67",
    "createdAt": "2014-08-12T18:56:18Z",
    "diffHunk": "@@ -0,0 +1,290 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.network.netty\n+\n+import java.io.FileInputStream\n+import java.net.InetSocketAddress\n+import java.nio.channels.FileChannel\n+\n+import io.netty.bootstrap.ServerBootstrap\n+import io.netty.buffer.{ByteBuf, PooledByteBufAllocator}\n+import io.netty.channel._\n+import io.netty.channel.epoll.{EpollEventLoopGroup, EpollServerSocketChannel}\n+import io.netty.channel.nio.NioEventLoopGroup\n+import io.netty.channel.oio.OioEventLoopGroup\n+import io.netty.channel.socket.SocketChannel\n+import io.netty.channel.socket.nio.NioServerSocketChannel\n+import io.netty.channel.socket.oio.OioServerSocketChannel\n+import io.netty.handler.codec.{MessageToByteEncoder, LineBasedFrameDecoder}\n+import io.netty.handler.codec.string.StringDecoder\n+import io.netty.util.CharsetUtil\n+import org.apache.spark.storage.{TestBlockId, FileSegment, BlockId}\n+\n+import org.apache.spark.{Logging, SparkConf}\n+import org.apache.spark.util.Utils\n+\n+\n+// TODO: Remove dependency on BlockId. This layer should not be coupled with storage.\n+\n+// TODO: PathResolver is not general enough. It only works for on-disk blocks.\n+\n+// TODO: Allow user-configured port\n+\n+/** A simple main function for testing the server. */\n+object BlockServer {\n+  def main(args: Array[String]): Unit = {\n+    new BlockServer(new SparkConf, new PathResolver {\n+      override def getBlockLocation(blockId: BlockId): FileSegment = {\n+        val file = new java.io.File(blockId.asInstanceOf[TestBlockId].id)\n+        new FileSegment(file, 0, file.length())\n+      }\n+    })\n+    Thread.sleep(1000000)\n+  }\n+}\n+\n+\n+/**\n+ * Server for serving Spark data blocks. This should be used together with [[BlockFetchingClient]].\n+ *\n+ * Protocol for requesting blocks (client to server):\n+ *   One block id per line, e.g. to request 3 blocks: \"block1\\nblock2\\nblock3\\n\"\n+ *\n+ * Protocol for sending blocks (server to client):\n+ *   frame-length (4 bytes), block-id-length (4 bytes), block-id, block-data.\n+ *\n+ *   frame-length should not include the length of itself.\n+ *   If frame-length is negative, then this is an error message rather than block-data. The real\n+ *   length is the absolute value of the frame-length.\n+ *\n+ */\n+private[spark]\n+class BlockServer(conf: SparkConf, pResolver: PathResolver) extends Logging {\n+\n+  def port: Int = _port\n+\n+  private var _port: Int = 0\n+  private var bootstrap: ServerBootstrap = _\n+  private var channelFuture: ChannelFuture = _\n+\n+  init()\n+\n+  /** Initialize the server. */\n+  private def init(): Unit = {\n+    bootstrap = new ServerBootstrap\n+    val bossThreadFactory = Utils.namedThreadFactory(\"spark-shuffle-server-boss\")\n+    val workerThreadFactory = Utils.namedThreadFactory(\"spark-shuffle-server-worker\")\n+\n+    // Use only one thread to accept connections, and 2 * num_cores for worker.\n+    def initNio(): Unit = {\n+      val bossGroup = new NioEventLoopGroup(1, bossThreadFactory)\n+      val workerGroup = new NioEventLoopGroup(0, workerThreadFactory)\n+      bootstrap.group(bossGroup, workerGroup).channel(classOf[NioServerSocketChannel])\n+    }\n+    def initOio(): Unit = {\n+      val bossGroup = new OioEventLoopGroup(1, bossThreadFactory)\n+      val workerGroup = new OioEventLoopGroup(0, workerThreadFactory)\n+      bootstrap.group(bossGroup, workerGroup).channel(classOf[OioServerSocketChannel])\n+    }\n+    def initEpoll(): Unit = {\n+      val bossGroup = new EpollEventLoopGroup(1, bossThreadFactory)\n+      val workerGroup = new EpollEventLoopGroup(0, workerThreadFactory)\n+      bootstrap.group(bossGroup, workerGroup).channel(classOf[EpollServerSocketChannel])\n+    }\n+\n+    conf.get(\"spark.shuffle.io.mode\", \"auto\").toLowerCase match {\n+      case \"nio\" => initNio()\n+      case \"oio\" => initOio()\n+      case \"epoll\" => initEpoll()\n+      case \"auto\" =>\n+        // For auto mode, first try epoll (only available on Linux), then nio.\n+        try {\n+          initEpoll()\n+        } catch {\n+          // TODO: Should we log the throwable? But that always happen on non-Linux systems.\n+          // Perhaps the right thing to do is to check whether the system is Linux, and then only\n+          // call initEpoll on Linux.\n+          case e: Throwable => initNio()\n+        }\n+    }\n+\n+    // Use pooled buffers to reduce temporary buffer allocation\n+    bootstrap.option(ChannelOption.ALLOCATOR, PooledByteBufAllocator.DEFAULT)\n+    bootstrap.childOption(ChannelOption.ALLOCATOR, PooledByteBufAllocator.DEFAULT)\n+\n+    // Various (advanced) user-configured settings.\n+    conf.getOption(\"spark.shuffle.io.backLog\").foreach { backLog =>\n+      bootstrap.option[java.lang.Integer](ChannelOption.SO_BACKLOG, backLog.toInt)\n+    }\n+    // Note: the optimal size for receive buffer and send buffer should be\n+    //  latency * network_bandwidth.\n+    // Assuming latency = 1ms, network_bandwidth = 10Gbps\n+    // buffer size should be ~ 1.25MB\n+    conf.getOption(\"spark.shuffle.io.receiveBuffer\").foreach { receiveBuf =>\n+      bootstrap.option[java.lang.Integer](ChannelOption.SO_RCVBUF, receiveBuf.toInt)\n+    }\n+    conf.getOption(\"spark.shuffle.io.sendBuffer\").foreach { sendBuf =>\n+      bootstrap.option[java.lang.Integer](ChannelOption.SO_SNDBUF, sendBuf.toInt)\n+    }\n+\n+    bootstrap.childHandler(new ChannelInitializer[SocketChannel] {\n+      override def initChannel(ch: SocketChannel): Unit = {\n+        ch.pipeline\n+          .addLast(\"frameDecoder\", new LineBasedFrameDecoder(1024))  // max block id length 1024\n+          .addLast(\"stringDecoder\", new StringDecoder(CharsetUtil.UTF_8))\n+          //.addLast(\"stringEncoder\", new StringEncoder(CharsetUtil.UTF_8))\n+          .addLast(\"blockHeaderEncoder\", new BlockHeaderEncoder)\n+          .addLast(\"handler\", new BlockServerHandler(pResolver))\n+      }\n+    })\n+\n+    channelFuture = bootstrap.bind(new InetSocketAddress(_port))\n+    channelFuture.sync()\n+\n+    val addr = channelFuture.channel.localAddress.asInstanceOf[InetSocketAddress]\n+    _port = addr.getPort\n+  }\n+\n+  /** Shutdown the server. */\n+  def stop(): Unit = {\n+    if (channelFuture != null) {\n+      channelFuture.channel().close().awaitUninterruptibly()\n+      channelFuture = null\n+    }\n+    if (bootstrap != null && bootstrap.group() != null) {\n+      bootstrap.group().shutdownGracefully()\n+    }\n+    if (bootstrap != null && bootstrap.childGroup() != null) {\n+      bootstrap.childGroup().shutdownGracefully()\n+    }\n+    bootstrap = null\n+  }\n+}\n+\n+\n+/** A handler that writes the content of a block to the channel. */\n+class BlockServerHandler(p: PathResolver)\n+  extends SimpleChannelInboundHandler[String] with Logging {\n+\n+  override def channelRead0(ctx: ChannelHandlerContext, blockId: String): Unit = {\n+    // client in the form of hostname:port\n+    val client = {\n+      val remoteAddr = ctx.channel.remoteAddress.asInstanceOf[InetSocketAddress]\n+      remoteAddr.getHostName + \":\" + remoteAddr.getPort\n+    }\n+\n+    // A helper function to send error message back to the client.\n+    def respondWithError(error: String): Unit = {\n+      ctx.writeAndFlush(new BlockHeader(-1, blockId, Some(error))).addListener(\n+        new ChannelFutureListener {\n+          override def operationComplete(future: ChannelFuture) {\n+            if (!future.isSuccess) {\n+              // TODO: Maybe log the success case as well.\n+              logError(s\"Error sending error back to $client\", future.cause)\n+              ctx.close()\n+            }\n+          }\n+        }\n+      )\n+    }\n+\n+    logTrace(s\"Received request from $client to fetch block $blockId\")\n+\n+    var fileChannel: FileChannel = null\n+    var offset: Long = 0\n+    var blockLength: Long = 0\n+\n+    // First make sure we can find the block.\n+    try {\n+      val segment = p.getBlockLocation(BlockId(blockId))\n+      fileChannel = new FileInputStream(segment.file).getChannel\n+      offset = segment.offset\n+      blockLength = segment.length\n+    } catch {\n+      case e: Exception =>\n+        // If we fail to find the block and get its size, send error back.\n+        logError(s\"Error opening block $blockId for request from $client\", e)\n+        blockLength = -1\n+        respondWithError(e.getMessage)\n+    }\n+\n+    // Send error message back if the block is too large. Even though we are capable of sending\n+    // large (2G+) blocks, the receiving end cannot handle it so let's fail fast.\n+    // Once we fixed the receiving end to be able to process large blocks, this should be removed.\n+    // Also make sure we update BlockHeaderEncoder to support length > 2G.\n+    if (blockLength > Int.MaxValue) {\n+      respondWithError(s\"Block $blockId size ($blockLength) greater than 2G\")\n+    }\n+\n+    // Found the block. Send it back.\n+    if (fileChannel != null && blockLength >= 0) {\n+      val listener = new ChannelFutureListener {\n+        override def operationComplete(future: ChannelFuture) {\n+          if (future.isSuccess) {\n+            logTrace(s\"Sent block $blockId ($blockLength B) back to $client\")\n+          } else {\n+            logError(s\"Error sending block $blockId to $client; closing connection\", future.cause)\n+            ctx.close()\n+          }\n+        }\n+      }\n+      val region = new DefaultFileRegion(fileChannel, offset, blockLength)\n+      ctx.writeAndFlush(new BlockHeader(blockLength.toInt, blockId)).addListener(listener)\n+      ctx.writeAndFlush(region).addListener(listener)"
  }],
  "prId": 1907
}, {
  "comments": [{
    "author": {
      "login": "coderplay"
    },
    "body": "This listener will be used twice. One is for writing block header, the other is for sending block. But the log info will show sent the same block twice?\n",
    "commit": "f9214216fe33e365375a071e522cc915be8acd67",
    "createdAt": "2014-08-12T18:57:46Z",
    "diffHunk": "@@ -0,0 +1,290 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.network.netty\n+\n+import java.io.FileInputStream\n+import java.net.InetSocketAddress\n+import java.nio.channels.FileChannel\n+\n+import io.netty.bootstrap.ServerBootstrap\n+import io.netty.buffer.{ByteBuf, PooledByteBufAllocator}\n+import io.netty.channel._\n+import io.netty.channel.epoll.{EpollEventLoopGroup, EpollServerSocketChannel}\n+import io.netty.channel.nio.NioEventLoopGroup\n+import io.netty.channel.oio.OioEventLoopGroup\n+import io.netty.channel.socket.SocketChannel\n+import io.netty.channel.socket.nio.NioServerSocketChannel\n+import io.netty.channel.socket.oio.OioServerSocketChannel\n+import io.netty.handler.codec.{MessageToByteEncoder, LineBasedFrameDecoder}\n+import io.netty.handler.codec.string.StringDecoder\n+import io.netty.util.CharsetUtil\n+import org.apache.spark.storage.{TestBlockId, FileSegment, BlockId}\n+\n+import org.apache.spark.{Logging, SparkConf}\n+import org.apache.spark.util.Utils\n+\n+\n+// TODO: Remove dependency on BlockId. This layer should not be coupled with storage.\n+\n+// TODO: PathResolver is not general enough. It only works for on-disk blocks.\n+\n+// TODO: Allow user-configured port\n+\n+/** A simple main function for testing the server. */\n+object BlockServer {\n+  def main(args: Array[String]): Unit = {\n+    new BlockServer(new SparkConf, new PathResolver {\n+      override def getBlockLocation(blockId: BlockId): FileSegment = {\n+        val file = new java.io.File(blockId.asInstanceOf[TestBlockId].id)\n+        new FileSegment(file, 0, file.length())\n+      }\n+    })\n+    Thread.sleep(1000000)\n+  }\n+}\n+\n+\n+/**\n+ * Server for serving Spark data blocks. This should be used together with [[BlockFetchingClient]].\n+ *\n+ * Protocol for requesting blocks (client to server):\n+ *   One block id per line, e.g. to request 3 blocks: \"block1\\nblock2\\nblock3\\n\"\n+ *\n+ * Protocol for sending blocks (server to client):\n+ *   frame-length (4 bytes), block-id-length (4 bytes), block-id, block-data.\n+ *\n+ *   frame-length should not include the length of itself.\n+ *   If frame-length is negative, then this is an error message rather than block-data. The real\n+ *   length is the absolute value of the frame-length.\n+ *\n+ */\n+private[spark]\n+class BlockServer(conf: SparkConf, pResolver: PathResolver) extends Logging {\n+\n+  def port: Int = _port\n+\n+  private var _port: Int = 0\n+  private var bootstrap: ServerBootstrap = _\n+  private var channelFuture: ChannelFuture = _\n+\n+  init()\n+\n+  /** Initialize the server. */\n+  private def init(): Unit = {\n+    bootstrap = new ServerBootstrap\n+    val bossThreadFactory = Utils.namedThreadFactory(\"spark-shuffle-server-boss\")\n+    val workerThreadFactory = Utils.namedThreadFactory(\"spark-shuffle-server-worker\")\n+\n+    // Use only one thread to accept connections, and 2 * num_cores for worker.\n+    def initNio(): Unit = {\n+      val bossGroup = new NioEventLoopGroup(1, bossThreadFactory)\n+      val workerGroup = new NioEventLoopGroup(0, workerThreadFactory)\n+      bootstrap.group(bossGroup, workerGroup).channel(classOf[NioServerSocketChannel])\n+    }\n+    def initOio(): Unit = {\n+      val bossGroup = new OioEventLoopGroup(1, bossThreadFactory)\n+      val workerGroup = new OioEventLoopGroup(0, workerThreadFactory)\n+      bootstrap.group(bossGroup, workerGroup).channel(classOf[OioServerSocketChannel])\n+    }\n+    def initEpoll(): Unit = {\n+      val bossGroup = new EpollEventLoopGroup(1, bossThreadFactory)\n+      val workerGroup = new EpollEventLoopGroup(0, workerThreadFactory)\n+      bootstrap.group(bossGroup, workerGroup).channel(classOf[EpollServerSocketChannel])\n+    }\n+\n+    conf.get(\"spark.shuffle.io.mode\", \"auto\").toLowerCase match {\n+      case \"nio\" => initNio()\n+      case \"oio\" => initOio()\n+      case \"epoll\" => initEpoll()\n+      case \"auto\" =>\n+        // For auto mode, first try epoll (only available on Linux), then nio.\n+        try {\n+          initEpoll()\n+        } catch {\n+          // TODO: Should we log the throwable? But that always happen on non-Linux systems.\n+          // Perhaps the right thing to do is to check whether the system is Linux, and then only\n+          // call initEpoll on Linux.\n+          case e: Throwable => initNio()\n+        }\n+    }\n+\n+    // Use pooled buffers to reduce temporary buffer allocation\n+    bootstrap.option(ChannelOption.ALLOCATOR, PooledByteBufAllocator.DEFAULT)\n+    bootstrap.childOption(ChannelOption.ALLOCATOR, PooledByteBufAllocator.DEFAULT)\n+\n+    // Various (advanced) user-configured settings.\n+    conf.getOption(\"spark.shuffle.io.backLog\").foreach { backLog =>\n+      bootstrap.option[java.lang.Integer](ChannelOption.SO_BACKLOG, backLog.toInt)\n+    }\n+    // Note: the optimal size for receive buffer and send buffer should be\n+    //  latency * network_bandwidth.\n+    // Assuming latency = 1ms, network_bandwidth = 10Gbps\n+    // buffer size should be ~ 1.25MB\n+    conf.getOption(\"spark.shuffle.io.receiveBuffer\").foreach { receiveBuf =>\n+      bootstrap.option[java.lang.Integer](ChannelOption.SO_RCVBUF, receiveBuf.toInt)\n+    }\n+    conf.getOption(\"spark.shuffle.io.sendBuffer\").foreach { sendBuf =>\n+      bootstrap.option[java.lang.Integer](ChannelOption.SO_SNDBUF, sendBuf.toInt)\n+    }\n+\n+    bootstrap.childHandler(new ChannelInitializer[SocketChannel] {\n+      override def initChannel(ch: SocketChannel): Unit = {\n+        ch.pipeline\n+          .addLast(\"frameDecoder\", new LineBasedFrameDecoder(1024))  // max block id length 1024\n+          .addLast(\"stringDecoder\", new StringDecoder(CharsetUtil.UTF_8))\n+          //.addLast(\"stringEncoder\", new StringEncoder(CharsetUtil.UTF_8))\n+          .addLast(\"blockHeaderEncoder\", new BlockHeaderEncoder)\n+          .addLast(\"handler\", new BlockServerHandler(pResolver))\n+      }\n+    })\n+\n+    channelFuture = bootstrap.bind(new InetSocketAddress(_port))\n+    channelFuture.sync()\n+\n+    val addr = channelFuture.channel.localAddress.asInstanceOf[InetSocketAddress]\n+    _port = addr.getPort\n+  }\n+\n+  /** Shutdown the server. */\n+  def stop(): Unit = {\n+    if (channelFuture != null) {\n+      channelFuture.channel().close().awaitUninterruptibly()\n+      channelFuture = null\n+    }\n+    if (bootstrap != null && bootstrap.group() != null) {\n+      bootstrap.group().shutdownGracefully()\n+    }\n+    if (bootstrap != null && bootstrap.childGroup() != null) {\n+      bootstrap.childGroup().shutdownGracefully()\n+    }\n+    bootstrap = null\n+  }\n+}\n+\n+\n+/** A handler that writes the content of a block to the channel. */\n+class BlockServerHandler(p: PathResolver)\n+  extends SimpleChannelInboundHandler[String] with Logging {\n+\n+  override def channelRead0(ctx: ChannelHandlerContext, blockId: String): Unit = {\n+    // client in the form of hostname:port\n+    val client = {\n+      val remoteAddr = ctx.channel.remoteAddress.asInstanceOf[InetSocketAddress]\n+      remoteAddr.getHostName + \":\" + remoteAddr.getPort\n+    }\n+\n+    // A helper function to send error message back to the client.\n+    def respondWithError(error: String): Unit = {\n+      ctx.writeAndFlush(new BlockHeader(-1, blockId, Some(error))).addListener(\n+        new ChannelFutureListener {\n+          override def operationComplete(future: ChannelFuture) {\n+            if (!future.isSuccess) {\n+              // TODO: Maybe log the success case as well.\n+              logError(s\"Error sending error back to $client\", future.cause)\n+              ctx.close()\n+            }\n+          }\n+        }\n+      )\n+    }\n+\n+    logTrace(s\"Received request from $client to fetch block $blockId\")\n+\n+    var fileChannel: FileChannel = null\n+    var offset: Long = 0\n+    var blockLength: Long = 0\n+\n+    // First make sure we can find the block.\n+    try {\n+      val segment = p.getBlockLocation(BlockId(blockId))\n+      fileChannel = new FileInputStream(segment.file).getChannel\n+      offset = segment.offset\n+      blockLength = segment.length\n+    } catch {\n+      case e: Exception =>\n+        // If we fail to find the block and get its size, send error back.\n+        logError(s\"Error opening block $blockId for request from $client\", e)\n+        blockLength = -1\n+        respondWithError(e.getMessage)\n+    }\n+\n+    // Send error message back if the block is too large. Even though we are capable of sending\n+    // large (2G+) blocks, the receiving end cannot handle it so let's fail fast.\n+    // Once we fixed the receiving end to be able to process large blocks, this should be removed.\n+    // Also make sure we update BlockHeaderEncoder to support length > 2G.\n+    if (blockLength > Int.MaxValue) {\n+      respondWithError(s\"Block $blockId size ($blockLength) greater than 2G\")\n+    }\n+\n+    // Found the block. Send it back.\n+    if (fileChannel != null && blockLength >= 0) {\n+      val listener = new ChannelFutureListener {"
  }],
  "prId": 1907
}]