[{
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "The scaladoc and this comment look a little confusing. Is this only used for testing, or is it called by non-test Spark code too?\n",
    "commit": "3447bb995b53c4d93154328c7c7c06e08a5ec9b9",
    "createdAt": "2015-04-20T18:52:03Z",
    "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.network.buffer\n+\n+import java.io.OutputStream\n+import java.nio.ByteBuffer\n+\n+import org.apache.spark.util.io.ByteArrayChunkOutputStream\n+\n+private[spark] class LargeByteBufferOutputStream(chunkSize: Int = 65536)\n+  extends OutputStream {\n+\n+  private[buffer] val output = new ByteArrayChunkOutputStream(chunkSize)\n+\n+  override def write(b: Int): Unit = {\n+    output.write(b)\n+  }\n+\n+  override def write(bytes: Array[Byte], offs: Int, len: Int): Unit = {\n+    output.write(bytes, offs, len)\n+  }\n+\n+  def largeBuffer: LargeByteBuffer = {\n+    largeBuffer(LargeByteBufferHelper.MAX_CHUNK)\n+  }\n+\n+  /**\n+   * exposed for testing.  You don't really ever want to call this method -- the returned\n+   * buffer will not implement {{asByteBuffer}} correctly.\n+   */\n+  private[buffer] def largeBuffer(maxChunk: Int): WrappedLargeByteBuffer = {\n+    // LargeByteBuffer is supposed to make a \"best effort\" to get all the data"
  }],
  "prId": 5400
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "Can you also implement this class in Java?\n",
    "commit": "3447bb995b53c4d93154328c7c7c06e08a5ec9b9",
    "createdAt": "2015-05-29T19:26:17Z",
    "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.network.buffer\n+\n+import java.io.OutputStream\n+import java.nio.ByteBuffer\n+\n+import org.apache.spark.util.io.ByteArrayChunkOutputStream\n+\n+private[spark] class LargeByteBufferOutputStream(chunkSize: Int = 65536)"
  }],
  "prId": 5400
}]