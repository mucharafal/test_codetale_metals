[{
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "do you need this here *and* in `onExecutorMetricsUpdate`?  I guess you do, because one is for reading from the logs, and the other is for the live UI?  If so its worth putting in a comment about that.",
    "commit": "571285beace1a0c1df92d9f5127828ed8955c93f",
    "createdAt": "2018-06-27T19:46:47Z",
    "diffHunk": "@@ -669,6 +686,29 @@ private[spark] class AppStatusListener(\n         }\n       }\n     }\n+    event.executorUpdates.foreach { updates: Array[Long] =>\n+      // check if there is a new peak value for any of the executor level memory metrics\n+      liveExecutors.get(event.execId).foreach { exec: LiveExecutor =>\n+        if (exec.peakExecutorMetrics.compareAndUpdate(updates)) {\n+          maybeUpdate(exec, now)\n+        }\n+      }\n+    }\n+  }\n+\n+  override def onStageExecutorMetrics(executorMetrics: SparkListenerStageExecutorMetrics): Unit = {\n+    val now = System.nanoTime()\n+\n+    // check if there is a new peak value for any of the executor level memory metrics"
  }, {
    "author": {
      "login": "edwinalu"
    },
    "body": "Unfortunately, yes. I've added some comments.",
    "commit": "571285beace1a0c1df92d9f5127828ed8955c93f",
    "createdAt": "2018-06-28T01:00:24Z",
    "diffHunk": "@@ -669,6 +686,29 @@ private[spark] class AppStatusListener(\n         }\n       }\n     }\n+    event.executorUpdates.foreach { updates: Array[Long] =>\n+      // check if there is a new peak value for any of the executor level memory metrics\n+      liveExecutors.get(event.execId).foreach { exec: LiveExecutor =>\n+        if (exec.peakExecutorMetrics.compareAndUpdate(updates)) {\n+          maybeUpdate(exec, now)\n+        }\n+      }\n+    }\n+  }\n+\n+  override def onStageExecutorMetrics(executorMetrics: SparkListenerStageExecutorMetrics): Unit = {\n+    val now = System.nanoTime()\n+\n+    // check if there is a new peak value for any of the executor level memory metrics"
  }],
  "prId": 21221
}, {
  "comments": [{
    "author": {
      "login": "mccheah"
    },
    "body": "Don't case compare with `Some` and `None` - for options prefer the functional equivalents and occasionally can use isEmpty / isPresent. We can do this here:\r\n\r\n```\r\nval opt = liveExecutors.get(...).orElse(...)\r\nopt.foreach { ...}\r\nif (opt.isEmpty) {\r\n  logWarning(...)\r\n}\r\n```",
    "commit": "571285beace1a0c1df92d9f5127828ed8955c93f",
    "createdAt": "2018-07-30T21:51:54Z",
    "diffHunk": "@@ -669,6 +686,34 @@ private[spark] class AppStatusListener(\n         }\n       }\n     }\n+\n+    // check if there is a new peak value for any of the executor level memory metrics\n+    // for the live UI. SparkListenerExecutorMetricsUpdate events are only processed\n+    // for the live UI.\n+    event.executorUpdates.foreach { updates: ExecutorMetrics =>\n+      liveExecutors.get(event.execId).foreach { exec: LiveExecutor =>\n+        if (exec.peakExecutorMetrics.compareAndUpdatePeakValues(updates)) {\n+          maybeUpdate(exec, now)\n+        }\n+      }\n+    }\n+  }\n+\n+  override def onStageExecutorMetrics(executorMetrics: SparkListenerStageExecutorMetrics): Unit = {\n+    val now = System.nanoTime()\n+\n+    // check if there is a new peak value for any of the executor level memory metrics,\n+    // while reading from the log. SparkListenerStageExecutorMetrics are only processed\n+    // when reading logs.\n+    liveExecutors.get(executorMetrics.execId)\n+      .orElse(deadExecutors.get(executorMetrics.execId)) match {\n+      case Some(exec) =>"
  }, {
    "author": {
      "login": "squito"
    },
    "body": "I don't think we really make a point about this code style.  In general we prefer the functional versions, but I actually find a match on Some & None totally fine.  There are plenty of other examples of this in the code base (though that doesn't necessarily mean its the right style ...)",
    "commit": "571285beace1a0c1df92d9f5127828ed8955c93f",
    "createdAt": "2018-08-01T19:08:53Z",
    "diffHunk": "@@ -669,6 +686,34 @@ private[spark] class AppStatusListener(\n         }\n       }\n     }\n+\n+    // check if there is a new peak value for any of the executor level memory metrics\n+    // for the live UI. SparkListenerExecutorMetricsUpdate events are only processed\n+    // for the live UI.\n+    event.executorUpdates.foreach { updates: ExecutorMetrics =>\n+      liveExecutors.get(event.execId).foreach { exec: LiveExecutor =>\n+        if (exec.peakExecutorMetrics.compareAndUpdatePeakValues(updates)) {\n+          maybeUpdate(exec, now)\n+        }\n+      }\n+    }\n+  }\n+\n+  override def onStageExecutorMetrics(executorMetrics: SparkListenerStageExecutorMetrics): Unit = {\n+    val now = System.nanoTime()\n+\n+    // check if there is a new peak value for any of the executor level memory metrics,\n+    // while reading from the log. SparkListenerStageExecutorMetrics are only processed\n+    // when reading logs.\n+    liveExecutors.get(executorMetrics.execId)\n+      .orElse(deadExecutors.get(executorMetrics.execId)) match {\n+      case Some(exec) =>"
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "From the [Scaladoc](https://www.scala-lang.org/api/2.10.2/index.html#scala.Option):\r\n\r\n> The most idiomatic way to use an scala.Option instance is to treat it as a collection or monad and use map,flatMap, filter, or foreach\r\n\r\nIt's probably better to follow the Scala conventions here.",
    "commit": "571285beace1a0c1df92d9f5127828ed8955c93f",
    "createdAt": "2018-08-01T19:24:36Z",
    "diffHunk": "@@ -669,6 +686,34 @@ private[spark] class AppStatusListener(\n         }\n       }\n     }\n+\n+    // check if there is a new peak value for any of the executor level memory metrics\n+    // for the live UI. SparkListenerExecutorMetricsUpdate events are only processed\n+    // for the live UI.\n+    event.executorUpdates.foreach { updates: ExecutorMetrics =>\n+      liveExecutors.get(event.execId).foreach { exec: LiveExecutor =>\n+        if (exec.peakExecutorMetrics.compareAndUpdatePeakValues(updates)) {\n+          maybeUpdate(exec, now)\n+        }\n+      }\n+    }\n+  }\n+\n+  override def onStageExecutorMetrics(executorMetrics: SparkListenerStageExecutorMetrics): Unit = {\n+    val now = System.nanoTime()\n+\n+    // check if there is a new peak value for any of the executor level memory metrics,\n+    // while reading from the log. SparkListenerStageExecutorMetrics are only processed\n+    // when reading logs.\n+    liveExecutors.get(executorMetrics.execId)\n+      .orElse(deadExecutors.get(executorMetrics.execId)) match {\n+      case Some(exec) =>"
  }, {
    "author": {
      "login": "squito"
    },
    "body": "yeah, but you're talking about both a `foreach` *and* an `if` together.\r\n\r\nA long time back we discussed using `option.fold` for this, as it is all in one function, but we rejected it as being pretty confusing for most developers.\r\n\r\n```scala\r\nscala> def foo(x: Option[String]) = x.fold(\"nada\")(\"some \" + _)\r\nfoo: (x: Option[String])String\r\n\r\nscala> foo(None)\r\nres0: String = nada\r\n\r\nscala> foo(Some(\"blah\"))\r\nres1: String = some blah\r\n```",
    "commit": "571285beace1a0c1df92d9f5127828ed8955c93f",
    "createdAt": "2018-08-01T21:25:37Z",
    "diffHunk": "@@ -669,6 +686,34 @@ private[spark] class AppStatusListener(\n         }\n       }\n     }\n+\n+    // check if there is a new peak value for any of the executor level memory metrics\n+    // for the live UI. SparkListenerExecutorMetricsUpdate events are only processed\n+    // for the live UI.\n+    event.executorUpdates.foreach { updates: ExecutorMetrics =>\n+      liveExecutors.get(event.execId).foreach { exec: LiveExecutor =>\n+        if (exec.peakExecutorMetrics.compareAndUpdatePeakValues(updates)) {\n+          maybeUpdate(exec, now)\n+        }\n+      }\n+    }\n+  }\n+\n+  override def onStageExecutorMetrics(executorMetrics: SparkListenerStageExecutorMetrics): Unit = {\n+    val now = System.nanoTime()\n+\n+    // check if there is a new peak value for any of the executor level memory metrics,\n+    // while reading from the log. SparkListenerStageExecutorMetrics are only processed\n+    // when reading logs.\n+    liveExecutors.get(executorMetrics.execId)\n+      .orElse(deadExecutors.get(executorMetrics.execId)) match {\n+      case Some(exec) =>"
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "Ok, this can stay as is.",
    "commit": "571285beace1a0c1df92d9f5127828ed8955c93f",
    "createdAt": "2018-08-02T00:42:59Z",
    "diffHunk": "@@ -669,6 +686,34 @@ private[spark] class AppStatusListener(\n         }\n       }\n     }\n+\n+    // check if there is a new peak value for any of the executor level memory metrics\n+    // for the live UI. SparkListenerExecutorMetricsUpdate events are only processed\n+    // for the live UI.\n+    event.executorUpdates.foreach { updates: ExecutorMetrics =>\n+      liveExecutors.get(event.execId).foreach { exec: LiveExecutor =>\n+        if (exec.peakExecutorMetrics.compareAndUpdatePeakValues(updates)) {\n+          maybeUpdate(exec, now)\n+        }\n+      }\n+    }\n+  }\n+\n+  override def onStageExecutorMetrics(executorMetrics: SparkListenerStageExecutorMetrics): Unit = {\n+    val now = System.nanoTime()\n+\n+    // check if there is a new peak value for any of the executor level memory metrics,\n+    // while reading from the log. SparkListenerStageExecutorMetrics are only processed\n+    // when reading logs.\n+    liveExecutors.get(executorMetrics.execId)\n+      .orElse(deadExecutors.get(executorMetrics.execId)) match {\n+      case Some(exec) =>"
  }],
  "prId": 21221
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "this is only for replaying from logs, right?  but then, `maybeUpdate` will be a no-op, as `live` is false.  So, what is the desired effect here?  But maybe I'm missing something, because this is covered by the some of the api tests.  I think maybe you don't need to call update at all here, and it'll just get written to the kvstore as part of the final `flush` call when the kvstore is closed after reading the whole log.\r\n\r\nalso style nit: indentation is off",
    "commit": "571285beace1a0c1df92d9f5127828ed8955c93f",
    "createdAt": "2018-08-01T19:28:53Z",
    "diffHunk": "@@ -669,6 +686,34 @@ private[spark] class AppStatusListener(\n         }\n       }\n     }\n+\n+    // check if there is a new peak value for any of the executor level memory metrics\n+    // for the live UI. SparkListenerExecutorMetricsUpdate events are only processed\n+    // for the live UI.\n+    event.executorUpdates.foreach { updates: ExecutorMetrics =>\n+      liveExecutors.get(event.execId).foreach { exec: LiveExecutor =>\n+        if (exec.peakExecutorMetrics.compareAndUpdatePeakValues(updates)) {\n+          maybeUpdate(exec, now)\n+        }\n+      }\n+    }\n+  }\n+\n+  override def onStageExecutorMetrics(executorMetrics: SparkListenerStageExecutorMetrics): Unit = {\n+    val now = System.nanoTime()\n+\n+    // check if there is a new peak value for any of the executor level memory metrics,\n+    // while reading from the log. SparkListenerStageExecutorMetrics are only processed\n+    // when reading logs.\n+    liveExecutors.get(executorMetrics.execId)\n+      .orElse(deadExecutors.get(executorMetrics.execId)) match {\n+      case Some(exec) =>\n+         if (exec.peakExecutorMetrics.compareAndUpdatePeakValues(executorMetrics.executorMetrics)) {\n+          maybeUpdate(exec, now)"
  }, {
    "author": {
      "login": "edwinalu"
    },
    "body": "Yes, this is called on replay. It should always update the executor metrics, if there is a new peak value for any of them. The test had had live set to true, so did not catch this. I've changed this to update(), and modified the test.",
    "commit": "571285beace1a0c1df92d9f5127828ed8955c93f",
    "createdAt": "2018-08-05T01:41:11Z",
    "diffHunk": "@@ -669,6 +686,34 @@ private[spark] class AppStatusListener(\n         }\n       }\n     }\n+\n+    // check if there is a new peak value for any of the executor level memory metrics\n+    // for the live UI. SparkListenerExecutorMetricsUpdate events are only processed\n+    // for the live UI.\n+    event.executorUpdates.foreach { updates: ExecutorMetrics =>\n+      liveExecutors.get(event.execId).foreach { exec: LiveExecutor =>\n+        if (exec.peakExecutorMetrics.compareAndUpdatePeakValues(updates)) {\n+          maybeUpdate(exec, now)\n+        }\n+      }\n+    }\n+  }\n+\n+  override def onStageExecutorMetrics(executorMetrics: SparkListenerStageExecutorMetrics): Unit = {\n+    val now = System.nanoTime()\n+\n+    // check if there is a new peak value for any of the executor level memory metrics,\n+    // while reading from the log. SparkListenerStageExecutorMetrics are only processed\n+    // when reading logs.\n+    liveExecutors.get(executorMetrics.execId)\n+      .orElse(deadExecutors.get(executorMetrics.execId)) match {\n+      case Some(exec) =>\n+         if (exec.peakExecutorMetrics.compareAndUpdatePeakValues(executorMetrics.executorMetrics)) {\n+          maybeUpdate(exec, now)"
  }],
  "prId": 21221
}, {
  "comments": [{
    "author": {
      "login": "mccheah"
    },
    "body": "Does the type need to be explicitly defined here and in the next line?",
    "commit": "571285beace1a0c1df92d9f5127828ed8955c93f",
    "createdAt": "2018-08-13T18:43:58Z",
    "diffHunk": "@@ -669,6 +686,31 @@ private[spark] class AppStatusListener(\n         }\n       }\n     }\n+\n+    // check if there is a new peak value for any of the executor level memory metrics\n+    // for the live UI. SparkListenerExecutorMetricsUpdate events are only processed\n+    // for the live UI.\n+    event.executorUpdates.foreach { updates: ExecutorMetrics =>"
  }, {
    "author": {
      "login": "edwinalu"
    },
    "body": "No, these don't need to be explicitly defined.",
    "commit": "571285beace1a0c1df92d9f5127828ed8955c93f",
    "createdAt": "2018-08-13T21:51:52Z",
    "diffHunk": "@@ -669,6 +686,31 @@ private[spark] class AppStatusListener(\n         }\n       }\n     }\n+\n+    // check if there is a new peak value for any of the executor level memory metrics\n+    // for the live UI. SparkListenerExecutorMetricsUpdate events are only processed\n+    // for the live UI.\n+    event.executorUpdates.foreach { updates: ExecutorMetrics =>"
  }],
  "prId": 21221
}, {
  "comments": [{
    "author": {
      "login": "mccheah"
    },
    "body": "Should we use a `Clock` instance here for testing?",
    "commit": "571285beace1a0c1df92d9f5127828ed8955c93f",
    "createdAt": "2018-08-13T18:45:52Z",
    "diffHunk": "@@ -669,6 +686,31 @@ private[spark] class AppStatusListener(\n         }\n       }\n     }\n+\n+    // check if there is a new peak value for any of the executor level memory metrics\n+    // for the live UI. SparkListenerExecutorMetricsUpdate events are only processed\n+    // for the live UI.\n+    event.executorUpdates.foreach { updates: ExecutorMetrics =>\n+      liveExecutors.get(event.execId).foreach { exec: LiveExecutor =>\n+        if (exec.peakExecutorMetrics.compareAndUpdatePeakValues(updates)) {\n+          maybeUpdate(exec, now)\n+        }\n+      }\n+    }\n+  }\n+\n+  override def onStageExecutorMetrics(executorMetrics: SparkListenerStageExecutorMetrics): Unit = {\n+    val now = System.nanoTime()",
    "line": 65
  }, {
    "author": {
      "login": "edwinalu"
    },
    "body": "The rest of the file is using System.nanoTime() -- it seems more consistent to keep it the same. Clock has getTimeMillis(), although we could always multiply by 1000, not sure if the precision would matter.",
    "commit": "571285beace1a0c1df92d9f5127828ed8955c93f",
    "createdAt": "2018-08-13T21:55:35Z",
    "diffHunk": "@@ -669,6 +686,31 @@ private[spark] class AppStatusListener(\n         }\n       }\n     }\n+\n+    // check if there is a new peak value for any of the executor level memory metrics\n+    // for the live UI. SparkListenerExecutorMetricsUpdate events are only processed\n+    // for the live UI.\n+    event.executorUpdates.foreach { updates: ExecutorMetrics =>\n+      liveExecutors.get(event.execId).foreach { exec: LiveExecutor =>\n+        if (exec.peakExecutorMetrics.compareAndUpdatePeakValues(updates)) {\n+          maybeUpdate(exec, now)\n+        }\n+      }\n+    }\n+  }\n+\n+  override def onStageExecutorMetrics(executorMetrics: SparkListenerStageExecutorMetrics): Unit = {\n+    val now = System.nanoTime()",
    "line": 65
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "Yup that's fine.",
    "commit": "571285beace1a0c1df92d9f5127828ed8955c93f",
    "createdAt": "2018-08-13T21:59:53Z",
    "diffHunk": "@@ -669,6 +686,31 @@ private[spark] class AppStatusListener(\n         }\n       }\n     }\n+\n+    // check if there is a new peak value for any of the executor level memory metrics\n+    // for the live UI. SparkListenerExecutorMetricsUpdate events are only processed\n+    // for the live UI.\n+    event.executorUpdates.foreach { updates: ExecutorMetrics =>\n+      liveExecutors.get(event.execId).foreach { exec: LiveExecutor =>\n+        if (exec.peakExecutorMetrics.compareAndUpdatePeakValues(updates)) {\n+          maybeUpdate(exec, now)\n+        }\n+      }\n+    }\n+  }\n+\n+  override def onStageExecutorMetrics(executorMetrics: SparkListenerStageExecutorMetrics): Unit = {\n+    val now = System.nanoTime()",
    "line": 65
  }],
  "prId": 21221
}]