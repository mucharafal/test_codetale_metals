[{
  "comments": [{
    "author": {
      "login": "gengliangwang"
    },
    "body": "`finish || processedEventsNum - lastRecordEventsNum >= batchSize`",
    "commit": "a08c3e90c1553f81dae6fcf99e2179cfd95f93f9",
    "createdAt": "2019-08-27T03:24:17Z",
    "diffHunk": "@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.status\n+\n+import java.io.BufferedOutputStream\n+import java.util.concurrent.{CountDownLatch, TimeUnit}\n+\n+import org.apache.hadoop.fs.{FileUtil, Path}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.internal.config.Status._\n+import org.apache.spark.scheduler.EventLoggingListener\n+import org.apache.spark.serializer.JavaSerializer\n+import org.apache.spark.status.api.v1\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+import org.apache.spark.util.kvstore.InMemoryStore\n+\n+case class InMemoryStoreSnapshot(\n+    store: InMemoryStore,\n+    eventsNum: Int,\n+    finished: Boolean) extends Serializable\n+\n+\n+private[spark] class InMemoryStoreCheckpoint(\n+    store: InMemoryStore,\n+    conf: SparkConf,\n+    listener: AppStatusListener) extends Logging {\n+  var lastRecordEventsNum: Int = 0\n+  var finished: Boolean = false\n+\n+  // used to count the number of processed events in a live AppStatusListener\n+  private var processedEventsNum = 0\n+  private val batchSize = conf.get(IMS_CHECKPOINT_BATCH_SIZE)\n+  private val bufferSize = conf.get(IMS_CHECKPOINT_BUFFER_SIZE).toInt\n+  private var latch = new CountDownLatch(0)\n+  @volatile var isDone = true\n+  // a JavaSerializer used to serialize InMemoryStoreSnapshot\n+  private val serializer = new JavaSerializer(conf).newInstance()\n+  private val logBaseDir = Utils.resolveURI(conf.get(EVENT_LOG_DIR).stripSuffix(\"/\"))\n+  private lazy val ckpPath = getCheckpointPath\n+  private val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+  private val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  private val executor = ThreadUtils.newDaemonSingleThreadExecutor(\n+    \"inmemorystore-checkpoint-thread\")\n+  // should be inited before the first time checkpoint\n+  var appInfo: v1.ApplicationInfo = _\n+\n+  private val checkpointTask = new Runnable {\n+    override def run(): Unit = Utils.tryLogNonFatalError(doCheckpoint())\n+  }\n+\n+  private def getCheckpointPath: String = {\n+    val appId = appInfo.id\n+    val appAttemptId = appInfo.attempts.head.attemptId\n+    EventLoggingListener.getLogPath(logBaseDir, appId, appAttemptId, None)\n+  }\n+\n+  def await(): Unit = latch.await()\n+\n+  def eventInc(finish: Boolean = false): Unit = {\n+    processedEventsNum += 1\n+    val shouldCheckpoint = !finished && (processedEventsNum - lastRecordEventsNum >="
  }],
  "prId": 25577
}, {
  "comments": [{
    "author": {
      "login": "gengliangwang"
    },
    "body": "Is it thread-safe to create a new `CountDownLatch` here?",
    "commit": "a08c3e90c1553f81dae6fcf99e2179cfd95f93f9",
    "createdAt": "2019-08-27T03:37:04Z",
    "diffHunk": "@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.status\n+\n+import java.io.BufferedOutputStream\n+import java.util.concurrent.{CountDownLatch, TimeUnit}\n+\n+import org.apache.hadoop.fs.{FileUtil, Path}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.internal.config.Status._\n+import org.apache.spark.scheduler.EventLoggingListener\n+import org.apache.spark.serializer.JavaSerializer\n+import org.apache.spark.status.api.v1\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+import org.apache.spark.util.kvstore.InMemoryStore\n+\n+case class InMemoryStoreSnapshot(\n+    store: InMemoryStore,\n+    eventsNum: Int,\n+    finished: Boolean) extends Serializable\n+\n+\n+private[spark] class InMemoryStoreCheckpoint(\n+    store: InMemoryStore,\n+    conf: SparkConf,\n+    listener: AppStatusListener) extends Logging {\n+  var lastRecordEventsNum: Int = 0\n+  var finished: Boolean = false\n+\n+  // used to count the number of processed events in a live AppStatusListener\n+  private var processedEventsNum = 0\n+  private val batchSize = conf.get(IMS_CHECKPOINT_BATCH_SIZE)\n+  private val bufferSize = conf.get(IMS_CHECKPOINT_BUFFER_SIZE).toInt\n+  private var latch = new CountDownLatch(0)\n+  @volatile var isDone = true\n+  // a JavaSerializer used to serialize InMemoryStoreSnapshot\n+  private val serializer = new JavaSerializer(conf).newInstance()\n+  private val logBaseDir = Utils.resolveURI(conf.get(EVENT_LOG_DIR).stripSuffix(\"/\"))\n+  private lazy val ckpPath = getCheckpointPath\n+  private val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+  private val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  private val executor = ThreadUtils.newDaemonSingleThreadExecutor(\n+    \"inmemorystore-checkpoint-thread\")\n+  // should be inited before the first time checkpoint\n+  var appInfo: v1.ApplicationInfo = _\n+\n+  private val checkpointTask = new Runnable {\n+    override def run(): Unit = Utils.tryLogNonFatalError(doCheckpoint())\n+  }\n+\n+  private def getCheckpointPath: String = {\n+    val appId = appInfo.id\n+    val appAttemptId = appInfo.attempts.head.attemptId\n+    EventLoggingListener.getLogPath(logBaseDir, appId, appAttemptId, None)\n+  }\n+\n+  def await(): Unit = latch.await()\n+\n+  def eventInc(finish: Boolean = false): Unit = {\n+    processedEventsNum += 1\n+    val shouldCheckpoint = !finished && (processedEventsNum - lastRecordEventsNum >=\n+      batchSize || finish)\n+    if (shouldCheckpoint) {\n+      // flush to make sure that all processed events' related data have write into InMemoryStore\n+      listener.flush(listener.update(_, System.nanoTime()))\n+      latch = new CountDownLatch(1)",
    "line": 85
  }, {
    "author": {
      "login": "Ngone51"
    },
    "body": "Since all events are processed in the single dispatchThread, so we'll only have that one thread to create new `CountDownLatch`. But I just realize that variable `latch` maybe override if the number of backlog events exceeds checkpoint batch size while last checkpoint is still in progress. So, I update the `shouldCheckpoint` condition to `isDone && (finish || processedEventsNum - lastRecordEventsNum >= batchSize)` to cover that corner case.",
    "commit": "a08c3e90c1553f81dae6fcf99e2179cfd95f93f9",
    "createdAt": "2019-08-27T12:05:43Z",
    "diffHunk": "@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.status\n+\n+import java.io.BufferedOutputStream\n+import java.util.concurrent.{CountDownLatch, TimeUnit}\n+\n+import org.apache.hadoop.fs.{FileUtil, Path}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.internal.config.Status._\n+import org.apache.spark.scheduler.EventLoggingListener\n+import org.apache.spark.serializer.JavaSerializer\n+import org.apache.spark.status.api.v1\n+import org.apache.spark.util.{ThreadUtils, Utils}\n+import org.apache.spark.util.kvstore.InMemoryStore\n+\n+case class InMemoryStoreSnapshot(\n+    store: InMemoryStore,\n+    eventsNum: Int,\n+    finished: Boolean) extends Serializable\n+\n+\n+private[spark] class InMemoryStoreCheckpoint(\n+    store: InMemoryStore,\n+    conf: SparkConf,\n+    listener: AppStatusListener) extends Logging {\n+  var lastRecordEventsNum: Int = 0\n+  var finished: Boolean = false\n+\n+  // used to count the number of processed events in a live AppStatusListener\n+  private var processedEventsNum = 0\n+  private val batchSize = conf.get(IMS_CHECKPOINT_BATCH_SIZE)\n+  private val bufferSize = conf.get(IMS_CHECKPOINT_BUFFER_SIZE).toInt\n+  private var latch = new CountDownLatch(0)\n+  @volatile var isDone = true\n+  // a JavaSerializer used to serialize InMemoryStoreSnapshot\n+  private val serializer = new JavaSerializer(conf).newInstance()\n+  private val logBaseDir = Utils.resolveURI(conf.get(EVENT_LOG_DIR).stripSuffix(\"/\"))\n+  private lazy val ckpPath = getCheckpointPath\n+  private val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+  private val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  private val executor = ThreadUtils.newDaemonSingleThreadExecutor(\n+    \"inmemorystore-checkpoint-thread\")\n+  // should be inited before the first time checkpoint\n+  var appInfo: v1.ApplicationInfo = _\n+\n+  private val checkpointTask = new Runnable {\n+    override def run(): Unit = Utils.tryLogNonFatalError(doCheckpoint())\n+  }\n+\n+  private def getCheckpointPath: String = {\n+    val appId = appInfo.id\n+    val appAttemptId = appInfo.attempts.head.attemptId\n+    EventLoggingListener.getLogPath(logBaseDir, appId, appAttemptId, None)\n+  }\n+\n+  def await(): Unit = latch.await()\n+\n+  def eventInc(finish: Boolean = false): Unit = {\n+    processedEventsNum += 1\n+    val shouldCheckpoint = !finished && (processedEventsNum - lastRecordEventsNum >=\n+      batchSize || finish)\n+    if (shouldCheckpoint) {\n+      // flush to make sure that all processed events' related data have write into InMemoryStore\n+      listener.flush(listener.update(_, System.nanoTime()))\n+      latch = new CountDownLatch(1)",
    "line": 85
  }],
  "prId": 25577
}]