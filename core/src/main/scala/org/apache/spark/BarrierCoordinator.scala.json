[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "is it better to use `HashMap[(Int, Int), Int]`?",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-07-31T13:00:24Z",
    "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+\n+import scala.collection.mutable.{ArrayBuffer, HashMap}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+\n+class BarrierCoordinator(\n+    timeout: Long,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  // Barrier epoch for each stage attempt, fail a sync request if the barrier epoch in the request\n+  // mismatches the barrier epoch in the coordinator.\n+  private val barrierEpochByStageIdAndAttempt = new HashMap[Int, HashMap[Int, Int]]"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "+1.",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-07-31T18:03:32Z",
    "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+\n+import scala.collection.mutable.{ArrayBuffer, HashMap}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+\n+class BarrierCoordinator(\n+    timeout: Long,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  // Barrier epoch for each stage attempt, fail a sync request if the barrier epoch in the request\n+  // mismatches the barrier epoch in the coordinator.\n+  private val barrierEpochByStageIdAndAttempt = new HashMap[Int, HashMap[Int, Int]]"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "Also, how about using `AtomicLong` to remember the epoch?",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-07-31T22:47:29Z",
    "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+\n+import scala.collection.mutable.{ArrayBuffer, HashMap}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+\n+class BarrierCoordinator(\n+    timeout: Long,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  // Barrier epoch for each stage attempt, fail a sync request if the barrier epoch in the request\n+  // mismatches the barrier epoch in the coordinator.\n+  private val barrierEpochByStageIdAndAttempt = new HashMap[Int, HashMap[Int, Int]]"
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "* package private\r\n* add ScalDoc",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-07-31T18:00:26Z",
    "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+\n+import scala.collection.mutable.{ArrayBuffer, HashMap}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+\n+class BarrierCoordinator("
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "* `Epoch counter for each barrier (stage, attempt).`\r\n* Remove \"fail ...\" because it is not implemented by this variable.",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-07-31T18:03:24Z",
    "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+\n+import scala.collection.mutable.{ArrayBuffer, HashMap}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+\n+class BarrierCoordinator(\n+    timeout: Long,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  // Barrier epoch for each stage attempt, fail a sync request if the barrier epoch in the request"
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Then shall we switch to Java's ConcurrentHashMap?",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-07-31T19:12:07Z",
    "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+\n+import scala.collection.mutable.{ArrayBuffer, HashMap}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+\n+class BarrierCoordinator(\n+    timeout: Long,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  // Barrier epoch for each stage attempt, fail a sync request if the barrier epoch in the request\n+  // mismatches the barrier epoch in the coordinator.\n+  private val barrierEpochByStageIdAndAttempt = new HashMap[Int, HashMap[Int, Int]]\n+\n+  // Any access to this should be synchronized."
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Ditto (stage, attempt) -> contexts.",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-07-31T19:12:42Z",
    "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+\n+import scala.collection.mutable.{ArrayBuffer, HashMap}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+\n+class BarrierCoordinator(\n+    timeout: Long,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  // Barrier epoch for each stage attempt, fail a sync request if the barrier epoch in the request\n+  // mismatches the barrier epoch in the coordinator.\n+  private val barrierEpochByStageIdAndAttempt = new HashMap[Int, HashMap[Int, Int]]\n+\n+  // Any access to this should be synchronized.\n+  private val syncRequestsByStageIdAndAttempt =\n+    new HashMap[Int, HashMap[Int, ArrayBuffer[RpcCallContext]]]"
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "If we use ConcurrentHashMap, this would be a one-liner.",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-07-31T22:44:44Z",
    "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+\n+import scala.collection.mutable.{ArrayBuffer, HashMap}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+\n+class BarrierCoordinator(\n+    timeout: Long,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  // Barrier epoch for each stage attempt, fail a sync request if the barrier epoch in the request\n+  // mismatches the barrier epoch in the coordinator.\n+  private val barrierEpochByStageIdAndAttempt = new HashMap[Int, HashMap[Int, Int]]\n+\n+  // Any access to this should be synchronized.\n+  private val syncRequestsByStageIdAndAttempt =\n+    new HashMap[Int, HashMap[Int, ArrayBuffer[RpcCallContext]]]\n+\n+  /**\n+   * Get the array of [[RpcCallContext]]s that correspond to a barrier sync request from a stage\n+   * attempt.\n+   */\n+  private def getOrInitSyncRequests("
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Ditto. This would be a one-liner.",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-07-31T22:45:30Z",
    "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+\n+import scala.collection.mutable.{ArrayBuffer, HashMap}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+\n+class BarrierCoordinator(\n+    timeout: Long,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  // Barrier epoch for each stage attempt, fail a sync request if the barrier epoch in the request\n+  // mismatches the barrier epoch in the coordinator.\n+  private val barrierEpochByStageIdAndAttempt = new HashMap[Int, HashMap[Int, Int]]\n+\n+  // Any access to this should be synchronized.\n+  private val syncRequestsByStageIdAndAttempt =\n+    new HashMap[Int, HashMap[Int, ArrayBuffer[RpcCallContext]]]\n+\n+  /**\n+   * Get the array of [[RpcCallContext]]s that correspond to a barrier sync request from a stage\n+   * attempt.\n+   */\n+  private def getOrInitSyncRequests(\n+      stageId: Int,\n+      stageAttemptId: Int,\n+      numTasks: Int = 0): ArrayBuffer[RpcCallContext] = synchronized {\n+    val syncRequestsByStage = syncRequestsByStageIdAndAttempt\n+      .getOrElseUpdate(stageId, new HashMap[Int, ArrayBuffer[RpcCallContext]])\n+    syncRequestsByStage.getOrElseUpdate(stageAttemptId, new ArrayBuffer[RpcCallContext](numTasks))\n+  }\n+\n+  /**\n+   * Clean up the array of [[RpcCallContext]]s that correspond to a barrier sync request from a\n+   * stage attempt.\n+   */\n+  private def cleanupSyncRequests(stageId: Int, stageAttemptId: Int): Unit = synchronized {"
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Ditto: one-liner with logging.",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-07-31T22:48:30Z",
    "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+\n+import scala.collection.mutable.{ArrayBuffer, HashMap}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+\n+class BarrierCoordinator(\n+    timeout: Long,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  // Barrier epoch for each stage attempt, fail a sync request if the barrier epoch in the request\n+  // mismatches the barrier epoch in the coordinator.\n+  private val barrierEpochByStageIdAndAttempt = new HashMap[Int, HashMap[Int, Int]]\n+\n+  // Any access to this should be synchronized.\n+  private val syncRequestsByStageIdAndAttempt =\n+    new HashMap[Int, HashMap[Int, ArrayBuffer[RpcCallContext]]]\n+\n+  /**\n+   * Get the array of [[RpcCallContext]]s that correspond to a barrier sync request from a stage\n+   * attempt.\n+   */\n+  private def getOrInitSyncRequests(\n+      stageId: Int,\n+      stageAttemptId: Int,\n+      numTasks: Int = 0): ArrayBuffer[RpcCallContext] = synchronized {\n+    val syncRequestsByStage = syncRequestsByStageIdAndAttempt\n+      .getOrElseUpdate(stageId, new HashMap[Int, ArrayBuffer[RpcCallContext]])\n+    syncRequestsByStage.getOrElseUpdate(stageAttemptId, new ArrayBuffer[RpcCallContext](numTasks))\n+  }\n+\n+  /**\n+   * Clean up the array of [[RpcCallContext]]s that correspond to a barrier sync request from a\n+   * stage attempt.\n+   */\n+  private def cleanupSyncRequests(stageId: Int, stageAttemptId: Int): Unit = synchronized {\n+    syncRequestsByStageIdAndAttempt.get(stageId).foreach { syncRequestByStage =>\n+      syncRequestByStage.get(stageAttemptId).foreach { syncRequests =>\n+        syncRequests.clear()\n+      }\n+      syncRequestByStage -= stageAttemptId\n+      if (syncRequestByStage.isEmpty) {\n+        syncRequestsByStageIdAndAttempt -= stageId\n+      }\n+      logInfo(s\"Removed all the pending barrier sync requests from Stage $stageId(Attempt \" +\n+        s\"$stageAttemptId).\")\n+    }\n+  }\n+\n+  /**\n+   * Get the barrier epoch that correspond to a barrier sync request from a stage attempt.\n+   */\n+  private def getOrInitBarrierEpoch(stageId: Int, stageAttemptId: Int): Int = synchronized {"
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Should include `stageAttemptId` to help debugging.",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-07-31T22:54:18Z",
    "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+\n+import scala.collection.mutable.{ArrayBuffer, HashMap}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+\n+class BarrierCoordinator(\n+    timeout: Long,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  // Barrier epoch for each stage attempt, fail a sync request if the barrier epoch in the request\n+  // mismatches the barrier epoch in the coordinator.\n+  private val barrierEpochByStageIdAndAttempt = new HashMap[Int, HashMap[Int, Int]]\n+\n+  // Any access to this should be synchronized.\n+  private val syncRequestsByStageIdAndAttempt =\n+    new HashMap[Int, HashMap[Int, ArrayBuffer[RpcCallContext]]]\n+\n+  /**\n+   * Get the array of [[RpcCallContext]]s that correspond to a barrier sync request from a stage\n+   * attempt.\n+   */\n+  private def getOrInitSyncRequests(\n+      stageId: Int,\n+      stageAttemptId: Int,\n+      numTasks: Int = 0): ArrayBuffer[RpcCallContext] = synchronized {\n+    val syncRequestsByStage = syncRequestsByStageIdAndAttempt\n+      .getOrElseUpdate(stageId, new HashMap[Int, ArrayBuffer[RpcCallContext]])\n+    syncRequestsByStage.getOrElseUpdate(stageAttemptId, new ArrayBuffer[RpcCallContext](numTasks))\n+  }\n+\n+  /**\n+   * Clean up the array of [[RpcCallContext]]s that correspond to a barrier sync request from a\n+   * stage attempt.\n+   */\n+  private def cleanupSyncRequests(stageId: Int, stageAttemptId: Int): Unit = synchronized {\n+    syncRequestsByStageIdAndAttempt.get(stageId).foreach { syncRequestByStage =>\n+      syncRequestByStage.get(stageAttemptId).foreach { syncRequests =>\n+        syncRequests.clear()\n+      }\n+      syncRequestByStage -= stageAttemptId\n+      if (syncRequestByStage.isEmpty) {\n+        syncRequestsByStageIdAndAttempt -= stageId\n+      }\n+      logInfo(s\"Removed all the pending barrier sync requests from Stage $stageId(Attempt \" +\n+        s\"$stageAttemptId).\")\n+    }\n+  }\n+\n+  /**\n+   * Get the barrier epoch that correspond to a barrier sync request from a stage attempt.\n+   */\n+  private def getOrInitBarrierEpoch(stageId: Int, stageAttemptId: Int): Int = synchronized {\n+    val barrierEpochByStage = barrierEpochByStageIdAndAttempt\n+      .getOrElseUpdate(stageId, new HashMap[Int, Int])\n+    val barrierEpoch = barrierEpochByStage.getOrElseUpdate(stageAttemptId, 0)\n+    logInfo(s\"Current barrier epoch for Stage $stageId(Attempt $stageAttemptId) is $barrierEpoch.\")\n+    barrierEpoch\n+  }\n+\n+  /**\n+   * Update the barrier epoch that correspond to a barrier sync request from a stage attempt.\n+   */\n+  private def updateBarrierEpoch(\n+      stageId: Int,\n+      stageAttemptId: Int,\n+      newBarrierEpoch: Int): Unit = synchronized {\n+    val barrierEpochByStage = barrierEpochByStageIdAndAttempt\n+      .getOrElseUpdate(stageId, new HashMap[Int, Int])\n+    barrierEpochByStage.put(stageAttemptId, newBarrierEpoch)\n+    logInfo(s\"Current barrier epoch for Stage $stageId(Attempt $stageAttemptId) is \" +\n+      s\"$newBarrierEpoch.\")\n+  }\n+\n+  /**\n+   * Send failure to all the blocking barrier sync requests from a stage attempt with proper\n+   * failure message.\n+   */\n+  private def failAllSyncRequests(\n+      syncRequests: ArrayBuffer[RpcCallContext],\n+      message: String): Unit = {\n+    syncRequests.foreach(_.sendFailure(new SparkException(message)))\n+  }\n+\n+  /**\n+   * Finish all the blocking barrier sync requests from a stage attempt successfully if we\n+   * have received all the sync requests.\n+   */\n+  private def maybeFinishAllSyncRequests(\n+      syncRequests: ArrayBuffer[RpcCallContext],\n+      numTasks: Int): Boolean = {\n+    if (syncRequests.size == numTasks) {\n+      syncRequests.foreach(_.reply(()))\n+      return true\n+    }\n+\n+    false\n+  }\n+\n+\n+  override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = {\n+    case RequestToSync(numTasks, stageId, stageAttemptId, taskAttemptId, barrierEpoch) =>\n+      // Check the barrier epoch, fail the sync request if barrier epoch mismatches.\n+      val currentBarrierEpoch = getOrInitBarrierEpoch(stageId, stageAttemptId)\n+      val syncRequests = getOrInitSyncRequests(stageId, stageAttemptId)\n+      if (barrierEpoch != currentBarrierEpoch) {\n+        syncRequests += context\n+        failAllSyncRequests(syncRequests,\n+          \"The request to sync fails due to mismatched barrier epoch, the barrier epoch from \" +\n+            s\"task $taskAttemptId is $barrierEpoch, while the barrier epoch from the \" +"
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Do we have a use case to set an arbitrary epoch number? If not, we might not want to define the method this way. It should be either increment or reset.",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-07-31T22:55:25Z",
    "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+\n+import scala.collection.mutable.{ArrayBuffer, HashMap}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+\n+class BarrierCoordinator(\n+    timeout: Long,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  // Barrier epoch for each stage attempt, fail a sync request if the barrier epoch in the request\n+  // mismatches the barrier epoch in the coordinator.\n+  private val barrierEpochByStageIdAndAttempt = new HashMap[Int, HashMap[Int, Int]]\n+\n+  // Any access to this should be synchronized.\n+  private val syncRequestsByStageIdAndAttempt =\n+    new HashMap[Int, HashMap[Int, ArrayBuffer[RpcCallContext]]]\n+\n+  /**\n+   * Get the array of [[RpcCallContext]]s that correspond to a barrier sync request from a stage\n+   * attempt.\n+   */\n+  private def getOrInitSyncRequests(\n+      stageId: Int,\n+      stageAttemptId: Int,\n+      numTasks: Int = 0): ArrayBuffer[RpcCallContext] = synchronized {\n+    val syncRequestsByStage = syncRequestsByStageIdAndAttempt\n+      .getOrElseUpdate(stageId, new HashMap[Int, ArrayBuffer[RpcCallContext]])\n+    syncRequestsByStage.getOrElseUpdate(stageAttemptId, new ArrayBuffer[RpcCallContext](numTasks))\n+  }\n+\n+  /**\n+   * Clean up the array of [[RpcCallContext]]s that correspond to a barrier sync request from a\n+   * stage attempt.\n+   */\n+  private def cleanupSyncRequests(stageId: Int, stageAttemptId: Int): Unit = synchronized {\n+    syncRequestsByStageIdAndAttempt.get(stageId).foreach { syncRequestByStage =>\n+      syncRequestByStage.get(stageAttemptId).foreach { syncRequests =>\n+        syncRequests.clear()\n+      }\n+      syncRequestByStage -= stageAttemptId\n+      if (syncRequestByStage.isEmpty) {\n+        syncRequestsByStageIdAndAttempt -= stageId\n+      }\n+      logInfo(s\"Removed all the pending barrier sync requests from Stage $stageId(Attempt \" +\n+        s\"$stageAttemptId).\")\n+    }\n+  }\n+\n+  /**\n+   * Get the barrier epoch that correspond to a barrier sync request from a stage attempt.\n+   */\n+  private def getOrInitBarrierEpoch(stageId: Int, stageAttemptId: Int): Int = synchronized {\n+    val barrierEpochByStage = barrierEpochByStageIdAndAttempt\n+      .getOrElseUpdate(stageId, new HashMap[Int, Int])\n+    val barrierEpoch = barrierEpochByStage.getOrElseUpdate(stageAttemptId, 0)\n+    logInfo(s\"Current barrier epoch for Stage $stageId(Attempt $stageAttemptId) is $barrierEpoch.\")\n+    barrierEpoch\n+  }\n+\n+  /**\n+   * Update the barrier epoch that correspond to a barrier sync request from a stage attempt.\n+   */\n+  private def updateBarrierEpoch("
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "* space before `(`\r\n* it would help debug if we can list at least one partition that didn't send the request.",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-07-31T22:57:09Z",
    "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+\n+import scala.collection.mutable.{ArrayBuffer, HashMap}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+\n+class BarrierCoordinator(\n+    timeout: Long,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  // Barrier epoch for each stage attempt, fail a sync request if the barrier epoch in the request\n+  // mismatches the barrier epoch in the coordinator.\n+  private val barrierEpochByStageIdAndAttempt = new HashMap[Int, HashMap[Int, Int]]\n+\n+  // Any access to this should be synchronized.\n+  private val syncRequestsByStageIdAndAttempt =\n+    new HashMap[Int, HashMap[Int, ArrayBuffer[RpcCallContext]]]\n+\n+  /**\n+   * Get the array of [[RpcCallContext]]s that correspond to a barrier sync request from a stage\n+   * attempt.\n+   */\n+  private def getOrInitSyncRequests(\n+      stageId: Int,\n+      stageAttemptId: Int,\n+      numTasks: Int = 0): ArrayBuffer[RpcCallContext] = synchronized {\n+    val syncRequestsByStage = syncRequestsByStageIdAndAttempt\n+      .getOrElseUpdate(stageId, new HashMap[Int, ArrayBuffer[RpcCallContext]])\n+    syncRequestsByStage.getOrElseUpdate(stageAttemptId, new ArrayBuffer[RpcCallContext](numTasks))\n+  }\n+\n+  /**\n+   * Clean up the array of [[RpcCallContext]]s that correspond to a barrier sync request from a\n+   * stage attempt.\n+   */\n+  private def cleanupSyncRequests(stageId: Int, stageAttemptId: Int): Unit = synchronized {\n+    syncRequestsByStageIdAndAttempt.get(stageId).foreach { syncRequestByStage =>\n+      syncRequestByStage.get(stageAttemptId).foreach { syncRequests =>\n+        syncRequests.clear()\n+      }\n+      syncRequestByStage -= stageAttemptId\n+      if (syncRequestByStage.isEmpty) {\n+        syncRequestsByStageIdAndAttempt -= stageId\n+      }\n+      logInfo(s\"Removed all the pending barrier sync requests from Stage $stageId(Attempt \" +\n+        s\"$stageAttemptId).\")\n+    }\n+  }\n+\n+  /**\n+   * Get the barrier epoch that correspond to a barrier sync request from a stage attempt.\n+   */\n+  private def getOrInitBarrierEpoch(stageId: Int, stageAttemptId: Int): Int = synchronized {\n+    val barrierEpochByStage = barrierEpochByStageIdAndAttempt\n+      .getOrElseUpdate(stageId, new HashMap[Int, Int])\n+    val barrierEpoch = barrierEpochByStage.getOrElseUpdate(stageAttemptId, 0)\n+    logInfo(s\"Current barrier epoch for Stage $stageId(Attempt $stageAttemptId) is $barrierEpoch.\")\n+    barrierEpoch\n+  }\n+\n+  /**\n+   * Update the barrier epoch that correspond to a barrier sync request from a stage attempt.\n+   */\n+  private def updateBarrierEpoch(\n+      stageId: Int,\n+      stageAttemptId: Int,\n+      newBarrierEpoch: Int): Unit = synchronized {\n+    val barrierEpochByStage = barrierEpochByStageIdAndAttempt\n+      .getOrElseUpdate(stageId, new HashMap[Int, Int])\n+    barrierEpochByStage.put(stageAttemptId, newBarrierEpoch)\n+    logInfo(s\"Current barrier epoch for Stage $stageId(Attempt $stageAttemptId) is \" +\n+      s\"$newBarrierEpoch.\")\n+  }\n+\n+  /**\n+   * Send failure to all the blocking barrier sync requests from a stage attempt with proper\n+   * failure message.\n+   */\n+  private def failAllSyncRequests(\n+      syncRequests: ArrayBuffer[RpcCallContext],\n+      message: String): Unit = {\n+    syncRequests.foreach(_.sendFailure(new SparkException(message)))\n+  }\n+\n+  /**\n+   * Finish all the blocking barrier sync requests from a stage attempt successfully if we\n+   * have received all the sync requests.\n+   */\n+  private def maybeFinishAllSyncRequests(\n+      syncRequests: ArrayBuffer[RpcCallContext],\n+      numTasks: Int): Boolean = {\n+    if (syncRequests.size == numTasks) {\n+      syncRequests.foreach(_.reply(()))\n+      return true\n+    }\n+\n+    false\n+  }\n+\n+\n+  override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = {\n+    case RequestToSync(numTasks, stageId, stageAttemptId, taskAttemptId, barrierEpoch) =>\n+      // Check the barrier epoch, fail the sync request if barrier epoch mismatches.\n+      val currentBarrierEpoch = getOrInitBarrierEpoch(stageId, stageAttemptId)\n+      val syncRequests = getOrInitSyncRequests(stageId, stageAttemptId)\n+      if (barrierEpoch != currentBarrierEpoch) {\n+        syncRequests += context\n+        failAllSyncRequests(syncRequests,\n+          \"The request to sync fails due to mismatched barrier epoch, the barrier epoch from \" +\n+            s\"task $taskAttemptId is $barrierEpoch, while the barrier epoch from the \" +\n+            s\"coordinator is $currentBarrierEpoch.\")\n+        cleanupSyncRequests(stageId, stageAttemptId)\n+        // The global sync fails so the stage is expected to retry another attempt, all sync\n+        // messages come from current stage attempt shall fail.\n+        updateBarrierEpoch(stageId, stageAttemptId, -1)\n+      } else {\n+        // If this is the first sync message received for a barrier() call, init a timer to ensure\n+        // we may timeout for the sync.\n+        if (syncRequests.isEmpty) {\n+          timer.schedule(new TimerTask {\n+            override def run(): Unit = {\n+              // Timeout for current barrier() call, fail all the sync requests and reset the\n+              // barrier epoch.\n+              val requests = getOrInitSyncRequests(stageId, stageAttemptId)\n+              failAllSyncRequests(requests,\n+                \"The coordinator didn't get all barrier sync requests for barrier epoch \" +\n+                  s\"$barrierEpoch from Stage $stageId(Attempt $stageAttemptId) within $timeout \" +"
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "document `barrierEpoch`",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-07-31T22:59:17Z",
    "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+\n+import scala.collection.mutable.{ArrayBuffer, HashMap}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+\n+class BarrierCoordinator(\n+    timeout: Long,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  // Barrier epoch for each stage attempt, fail a sync request if the barrier epoch in the request\n+  // mismatches the barrier epoch in the coordinator.\n+  private val barrierEpochByStageIdAndAttempt = new HashMap[Int, HashMap[Int, Int]]\n+\n+  // Any access to this should be synchronized.\n+  private val syncRequestsByStageIdAndAttempt =\n+    new HashMap[Int, HashMap[Int, ArrayBuffer[RpcCallContext]]]\n+\n+  /**\n+   * Get the array of [[RpcCallContext]]s that correspond to a barrier sync request from a stage\n+   * attempt.\n+   */\n+  private def getOrInitSyncRequests(\n+      stageId: Int,\n+      stageAttemptId: Int,\n+      numTasks: Int = 0): ArrayBuffer[RpcCallContext] = synchronized {\n+    val syncRequestsByStage = syncRequestsByStageIdAndAttempt\n+      .getOrElseUpdate(stageId, new HashMap[Int, ArrayBuffer[RpcCallContext]])\n+    syncRequestsByStage.getOrElseUpdate(stageAttemptId, new ArrayBuffer[RpcCallContext](numTasks))\n+  }\n+\n+  /**\n+   * Clean up the array of [[RpcCallContext]]s that correspond to a barrier sync request from a\n+   * stage attempt.\n+   */\n+  private def cleanupSyncRequests(stageId: Int, stageAttemptId: Int): Unit = synchronized {\n+    syncRequestsByStageIdAndAttempt.get(stageId).foreach { syncRequestByStage =>\n+      syncRequestByStage.get(stageAttemptId).foreach { syncRequests =>\n+        syncRequests.clear()\n+      }\n+      syncRequestByStage -= stageAttemptId\n+      if (syncRequestByStage.isEmpty) {\n+        syncRequestsByStageIdAndAttempt -= stageId\n+      }\n+      logInfo(s\"Removed all the pending barrier sync requests from Stage $stageId(Attempt \" +\n+        s\"$stageAttemptId).\")\n+    }\n+  }\n+\n+  /**\n+   * Get the barrier epoch that correspond to a barrier sync request from a stage attempt.\n+   */\n+  private def getOrInitBarrierEpoch(stageId: Int, stageAttemptId: Int): Int = synchronized {\n+    val barrierEpochByStage = barrierEpochByStageIdAndAttempt\n+      .getOrElseUpdate(stageId, new HashMap[Int, Int])\n+    val barrierEpoch = barrierEpochByStage.getOrElseUpdate(stageAttemptId, 0)\n+    logInfo(s\"Current barrier epoch for Stage $stageId(Attempt $stageAttemptId) is $barrierEpoch.\")\n+    barrierEpoch\n+  }\n+\n+  /**\n+   * Update the barrier epoch that correspond to a barrier sync request from a stage attempt.\n+   */\n+  private def updateBarrierEpoch(\n+      stageId: Int,\n+      stageAttemptId: Int,\n+      newBarrierEpoch: Int): Unit = synchronized {\n+    val barrierEpochByStage = barrierEpochByStageIdAndAttempt\n+      .getOrElseUpdate(stageId, new HashMap[Int, Int])\n+    barrierEpochByStage.put(stageAttemptId, newBarrierEpoch)\n+    logInfo(s\"Current barrier epoch for Stage $stageId(Attempt $stageAttemptId) is \" +\n+      s\"$newBarrierEpoch.\")\n+  }\n+\n+  /**\n+   * Send failure to all the blocking barrier sync requests from a stage attempt with proper\n+   * failure message.\n+   */\n+  private def failAllSyncRequests(\n+      syncRequests: ArrayBuffer[RpcCallContext],\n+      message: String): Unit = {\n+    syncRequests.foreach(_.sendFailure(new SparkException(message)))\n+  }\n+\n+  /**\n+   * Finish all the blocking barrier sync requests from a stage attempt successfully if we\n+   * have received all the sync requests.\n+   */\n+  private def maybeFinishAllSyncRequests(\n+      syncRequests: ArrayBuffer[RpcCallContext],\n+      numTasks: Int): Boolean = {\n+    if (syncRequests.size == numTasks) {\n+      syncRequests.foreach(_.reply(()))\n+      return true\n+    }\n+\n+    false\n+  }\n+\n+\n+  override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = {\n+    case RequestToSync(numTasks, stageId, stageAttemptId, taskAttemptId, barrierEpoch) =>\n+      // Check the barrier epoch, fail the sync request if barrier epoch mismatches.\n+      val currentBarrierEpoch = getOrInitBarrierEpoch(stageId, stageAttemptId)\n+      val syncRequests = getOrInitSyncRequests(stageId, stageAttemptId)\n+      if (barrierEpoch != currentBarrierEpoch) {\n+        syncRequests += context\n+        failAllSyncRequests(syncRequests,\n+          \"The request to sync fails due to mismatched barrier epoch, the barrier epoch from \" +\n+            s\"task $taskAttemptId is $barrierEpoch, while the barrier epoch from the \" +\n+            s\"coordinator is $currentBarrierEpoch.\")\n+        cleanupSyncRequests(stageId, stageAttemptId)\n+        // The global sync fails so the stage is expected to retry another attempt, all sync\n+        // messages come from current stage attempt shall fail.\n+        updateBarrierEpoch(stageId, stageAttemptId, -1)\n+      } else {\n+        // If this is the first sync message received for a barrier() call, init a timer to ensure\n+        // we may timeout for the sync.\n+        if (syncRequests.isEmpty) {\n+          timer.schedule(new TimerTask {\n+            override def run(): Unit = {\n+              // Timeout for current barrier() call, fail all the sync requests and reset the\n+              // barrier epoch.\n+              val requests = getOrInitSyncRequests(stageId, stageAttemptId)\n+              failAllSyncRequests(requests,\n+                \"The coordinator didn't get all barrier sync requests for barrier epoch \" +\n+                  s\"$barrierEpoch from Stage $stageId(Attempt $stageAttemptId) within $timeout \" +\n+                  \"ms.\")\n+              cleanupSyncRequests(stageId, stageAttemptId)\n+              // The global sync fails so the stage is expected to retry another attempt, all sync\n+              // messages come from current stage attempt shall fail.\n+              updateBarrierEpoch(stageId, stageAttemptId, -1)\n+            }\n+          }, timeout)\n+        }\n+\n+        syncRequests += context\n+        logInfo(s\"Barrier sync epoch $barrierEpoch from Stage $stageId(Attempt $stageAttemptId) \" +\n+          s\"received update from Task $taskAttemptId, current progress: \" +\n+          s\"${syncRequests.size}/$numTasks.\")\n+        if (maybeFinishAllSyncRequests(syncRequests, numTasks)) {\n+          // Finished current barrier() call successfully, clean up internal data and increase the\n+          // barrier epoch.\n+          logInfo(s\"Barrier sync epoch $barrierEpoch from Stage $stageId(Attempt \" +\n+            s\"$stageAttemptId) received all updates from tasks, finished successfully.\")\n+          cleanupSyncRequests(stageId, stageAttemptId)\n+          updateBarrierEpoch(stageId, stageAttemptId, currentBarrierEpoch + 1)\n+        }\n+      }\n+  }\n+\n+  override def onStop(): Unit = timer.cancel()\n+}\n+\n+private[spark] sealed trait BarrierCoordinatorMessage extends Serializable\n+\n+private[spark] case class RequestToSync("
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "is this needed? when we call `syncRequestsByStageIdAndAttempt.remove((stageId, stageAttemptId))`, the array buffer becomes dangling and will be GCed.",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-02T04:06:56Z",
    "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      // Remove internal data from a finished stage attempt.\n+      cleanupSyncRequests(stageInfo.stageId, stageInfo.attemptNumber)\n+      barrierEpochByStageIdAndAttempt.remove((stageInfo.stageId, stageInfo.attemptNumber))\n+    }\n+  }\n+\n+  // Epoch counter for each barrier (stage, attempt).\n+  private val barrierEpochByStageIdAndAttempt = new ConcurrentHashMap[(Int, Int), AtomicInteger]\n+\n+  // Remember all the blocking global sync requests for each barrier (stage, attempt).\n+  private val syncRequestsByStageIdAndAttempt =\n+    new ConcurrentHashMap[(Int, Int), ArrayBuffer[RpcCallContext]]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Get the array of [[RpcCallContext]]s that correspond to a barrier sync request from a stage\n+   * attempt.\n+   */\n+  private def getOrInitSyncRequests(\n+      stageId: Int,\n+      stageAttemptId: Int,\n+      numTasks: Int = 0): ArrayBuffer[RpcCallContext] = {\n+    val requests = syncRequestsByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId),\n+      new ArrayBuffer[RpcCallContext](numTasks))\n+    if (requests == null) {\n+      syncRequestsByStageIdAndAttempt.get((stageId, stageAttemptId))\n+    } else {\n+      requests\n+    }\n+  }\n+\n+  /**\n+   * Clean up the array of [[RpcCallContext]]s that correspond to a barrier sync request from a\n+   * stage attempt.\n+   */\n+  private def cleanupSyncRequests(stageId: Int, stageAttemptId: Int): Unit = {\n+    val requests = syncRequestsByStageIdAndAttempt.remove((stageId, stageAttemptId))\n+    if (requests != null) {\n+      requests.clear()"
  }, {
    "author": {
      "login": "jiangxb1987"
    },
    "body": "This is just to be safe, in case the requests are held in other places, we can still GC the `RpcCallContext`s",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-02T05:30:08Z",
    "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      // Remove internal data from a finished stage attempt.\n+      cleanupSyncRequests(stageInfo.stageId, stageInfo.attemptNumber)\n+      barrierEpochByStageIdAndAttempt.remove((stageInfo.stageId, stageInfo.attemptNumber))\n+    }\n+  }\n+\n+  // Epoch counter for each barrier (stage, attempt).\n+  private val barrierEpochByStageIdAndAttempt = new ConcurrentHashMap[(Int, Int), AtomicInteger]\n+\n+  // Remember all the blocking global sync requests for each barrier (stage, attempt).\n+  private val syncRequestsByStageIdAndAttempt =\n+    new ConcurrentHashMap[(Int, Int), ArrayBuffer[RpcCallContext]]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Get the array of [[RpcCallContext]]s that correspond to a barrier sync request from a stage\n+   * attempt.\n+   */\n+  private def getOrInitSyncRequests(\n+      stageId: Int,\n+      stageAttemptId: Int,\n+      numTasks: Int = 0): ArrayBuffer[RpcCallContext] = {\n+    val requests = syncRequestsByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId),\n+      new ArrayBuffer[RpcCallContext](numTasks))\n+    if (requests == null) {\n+      syncRequestsByStageIdAndAttempt.get((stageId, stageAttemptId))\n+    } else {\n+      requests\n+    }\n+  }\n+\n+  /**\n+   * Clean up the array of [[RpcCallContext]]s that correspond to a barrier sync request from a\n+   * stage attempt.\n+   */\n+  private def cleanupSyncRequests(stageId: Int, stageAttemptId: Int): Unit = {\n+    val requests = syncRequestsByStageIdAndAttempt.remove((stageId, stageAttemptId))\n+    if (requests != null) {\n+      requests.clear()"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "Agree with @cloud-fan that this is not necessary. It only explicitly clears the ArrayBuffer object instead of the contexts.",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-03T05:12:08Z",
    "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      // Remove internal data from a finished stage attempt.\n+      cleanupSyncRequests(stageInfo.stageId, stageInfo.attemptNumber)\n+      barrierEpochByStageIdAndAttempt.remove((stageInfo.stageId, stageInfo.attemptNumber))\n+    }\n+  }\n+\n+  // Epoch counter for each barrier (stage, attempt).\n+  private val barrierEpochByStageIdAndAttempt = new ConcurrentHashMap[(Int, Int), AtomicInteger]\n+\n+  // Remember all the blocking global sync requests for each barrier (stage, attempt).\n+  private val syncRequestsByStageIdAndAttempt =\n+    new ConcurrentHashMap[(Int, Int), ArrayBuffer[RpcCallContext]]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Get the array of [[RpcCallContext]]s that correspond to a barrier sync request from a stage\n+   * attempt.\n+   */\n+  private def getOrInitSyncRequests(\n+      stageId: Int,\n+      stageAttemptId: Int,\n+      numTasks: Int = 0): ArrayBuffer[RpcCallContext] = {\n+    val requests = syncRequestsByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId),\n+      new ArrayBuffer[RpcCallContext](numTasks))\n+    if (requests == null) {\n+      syncRequestsByStageIdAndAttempt.get((stageId, stageAttemptId))\n+    } else {\n+      requests\n+    }\n+  }\n+\n+  /**\n+   * Clean up the array of [[RpcCallContext]]s that correspond to a barrier sync request from a\n+   * stage attempt.\n+   */\n+  private def cleanupSyncRequests(stageId: Int, stageAttemptId: Int): Unit = {\n+    val requests = syncRequestsByStageIdAndAttempt.remove((stageId, stageAttemptId))\n+    if (requests != null) {\n+      requests.clear()"
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "when will we use the default value `0`?",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-02T04:07:53Z",
    "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      // Remove internal data from a finished stage attempt.\n+      cleanupSyncRequests(stageInfo.stageId, stageInfo.attemptNumber)\n+      barrierEpochByStageIdAndAttempt.remove((stageInfo.stageId, stageInfo.attemptNumber))\n+    }\n+  }\n+\n+  // Epoch counter for each barrier (stage, attempt).\n+  private val barrierEpochByStageIdAndAttempt = new ConcurrentHashMap[(Int, Int), AtomicInteger]\n+\n+  // Remember all the blocking global sync requests for each barrier (stage, attempt).\n+  private val syncRequestsByStageIdAndAttempt =\n+    new ConcurrentHashMap[(Int, Int), ArrayBuffer[RpcCallContext]]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Get the array of [[RpcCallContext]]s that correspond to a barrier sync request from a stage\n+   * attempt.\n+   */\n+  private def getOrInitSyncRequests(\n+      stageId: Int,\n+      stageAttemptId: Int,\n+      numTasks: Int = 0): ArrayBuffer[RpcCallContext] = {"
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "nit:\r\n```\r\nif (...) {\r\n  ...\r\n  true\r\n} else {\r\n  false\r\n}\r\n```",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-02T04:09:53Z",
    "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      // Remove internal data from a finished stage attempt.\n+      cleanupSyncRequests(stageInfo.stageId, stageInfo.attemptNumber)\n+      barrierEpochByStageIdAndAttempt.remove((stageInfo.stageId, stageInfo.attemptNumber))\n+    }\n+  }\n+\n+  // Epoch counter for each barrier (stage, attempt).\n+  private val barrierEpochByStageIdAndAttempt = new ConcurrentHashMap[(Int, Int), AtomicInteger]\n+\n+  // Remember all the blocking global sync requests for each barrier (stage, attempt).\n+  private val syncRequestsByStageIdAndAttempt =\n+    new ConcurrentHashMap[(Int, Int), ArrayBuffer[RpcCallContext]]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Get the array of [[RpcCallContext]]s that correspond to a barrier sync request from a stage\n+   * attempt.\n+   */\n+  private def getOrInitSyncRequests(\n+      stageId: Int,\n+      stageAttemptId: Int,\n+      numTasks: Int = 0): ArrayBuffer[RpcCallContext] = {\n+    val requests = syncRequestsByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId),\n+      new ArrayBuffer[RpcCallContext](numTasks))\n+    if (requests == null) {\n+      syncRequestsByStageIdAndAttempt.get((stageId, stageAttemptId))\n+    } else {\n+      requests\n+    }\n+  }\n+\n+  /**\n+   * Clean up the array of [[RpcCallContext]]s that correspond to a barrier sync request from a\n+   * stage attempt.\n+   */\n+  private def cleanupSyncRequests(stageId: Int, stageAttemptId: Int): Unit = {\n+    val requests = syncRequestsByStageIdAndAttempt.remove((stageId, stageAttemptId))\n+    if (requests != null) {\n+      requests.clear()\n+    }\n+    logInfo(s\"Removed all the pending barrier sync requests from Stage $stageId (Attempt \" +\n+      s\"$stageAttemptId).\")\n+  }\n+\n+  /**\n+   * Get the barrier epoch that correspond to a barrier sync request from a stage attempt.\n+   */\n+  private def getOrInitBarrierEpoch(stageId: Int, stageAttemptId: Int): AtomicInteger = {\n+    val barrierEpoch = barrierEpochByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId),\n+      new AtomicInteger(0))\n+    if (barrierEpoch == null) {\n+      barrierEpochByStageIdAndAttempt.get((stageId, stageAttemptId))\n+    } else {\n+      barrierEpoch\n+    }\n+  }\n+\n+  /**\n+   * Send failure to all the blocking barrier sync requests from a stage attempt with proper\n+   * failure message.\n+   */\n+  private def failAllSyncRequests(\n+      syncRequests: ArrayBuffer[RpcCallContext],\n+      message: String): Unit = {\n+    syncRequests.foreach(_.sendFailure(new SparkException(message)))\n+  }\n+\n+  /**\n+   * Finish all the blocking barrier sync requests from a stage attempt successfully if we\n+   * have received all the sync requests.\n+   */\n+  private def maybeFinishAllSyncRequests(\n+      syncRequests: ArrayBuffer[RpcCallContext],\n+      numTasks: Int): Boolean = {\n+    if (syncRequests.size == numTasks) {\n+      syncRequests.foreach(_.reply(()))\n+      return true\n+    }"
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "what if all the sync requests finish before timeout? then here we may init the request array again.",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-02T04:11:45Z",
    "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      // Remove internal data from a finished stage attempt.\n+      cleanupSyncRequests(stageInfo.stageId, stageInfo.attemptNumber)\n+      barrierEpochByStageIdAndAttempt.remove((stageInfo.stageId, stageInfo.attemptNumber))\n+    }\n+  }\n+\n+  // Epoch counter for each barrier (stage, attempt).\n+  private val barrierEpochByStageIdAndAttempt = new ConcurrentHashMap[(Int, Int), AtomicInteger]\n+\n+  // Remember all the blocking global sync requests for each barrier (stage, attempt).\n+  private val syncRequestsByStageIdAndAttempt =\n+    new ConcurrentHashMap[(Int, Int), ArrayBuffer[RpcCallContext]]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Get the array of [[RpcCallContext]]s that correspond to a barrier sync request from a stage\n+   * attempt.\n+   */\n+  private def getOrInitSyncRequests(\n+      stageId: Int,\n+      stageAttemptId: Int,\n+      numTasks: Int = 0): ArrayBuffer[RpcCallContext] = {\n+    val requests = syncRequestsByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId),\n+      new ArrayBuffer[RpcCallContext](numTasks))\n+    if (requests == null) {\n+      syncRequestsByStageIdAndAttempt.get((stageId, stageAttemptId))\n+    } else {\n+      requests\n+    }\n+  }\n+\n+  /**\n+   * Clean up the array of [[RpcCallContext]]s that correspond to a barrier sync request from a\n+   * stage attempt.\n+   */\n+  private def cleanupSyncRequests(stageId: Int, stageAttemptId: Int): Unit = {\n+    val requests = syncRequestsByStageIdAndAttempt.remove((stageId, stageAttemptId))\n+    if (requests != null) {\n+      requests.clear()\n+    }\n+    logInfo(s\"Removed all the pending barrier sync requests from Stage $stageId (Attempt \" +\n+      s\"$stageAttemptId).\")\n+  }\n+\n+  /**\n+   * Get the barrier epoch that correspond to a barrier sync request from a stage attempt.\n+   */\n+  private def getOrInitBarrierEpoch(stageId: Int, stageAttemptId: Int): AtomicInteger = {\n+    val barrierEpoch = barrierEpochByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId),\n+      new AtomicInteger(0))\n+    if (barrierEpoch == null) {\n+      barrierEpochByStageIdAndAttempt.get((stageId, stageAttemptId))\n+    } else {\n+      barrierEpoch\n+    }\n+  }\n+\n+  /**\n+   * Send failure to all the blocking barrier sync requests from a stage attempt with proper\n+   * failure message.\n+   */\n+  private def failAllSyncRequests(\n+      syncRequests: ArrayBuffer[RpcCallContext],\n+      message: String): Unit = {\n+    syncRequests.foreach(_.sendFailure(new SparkException(message)))\n+  }\n+\n+  /**\n+   * Finish all the blocking barrier sync requests from a stage attempt successfully if we\n+   * have received all the sync requests.\n+   */\n+  private def maybeFinishAllSyncRequests(\n+      syncRequests: ArrayBuffer[RpcCallContext],\n+      numTasks: Int): Boolean = {\n+    if (syncRequests.size == numTasks) {\n+      syncRequests.foreach(_.reply(()))\n+      return true\n+    }\n+\n+    false\n+  }\n+\n+\n+  override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = {\n+    case RequestToSync(numTasks, stageId, stageAttemptId, taskAttemptId, barrierEpoch) =>\n+      // Check the barrier epoch, to see which barrier() call we are processing.\n+      val currentBarrierEpoch = getOrInitBarrierEpoch(stageId, stageAttemptId)\n+      logInfo(s\"Current barrier epoch for Stage $stageId (Attempt $stageAttemptId) is\" +\n+        s\"$currentBarrierEpoch.\")\n+      if (currentBarrierEpoch.get() != barrierEpoch) {\n+        context.sendFailure(new SparkException(s\"The request to sync of Stage $stageId (Attempt \" +\n+          s\"$stageAttemptId) with barrier epoch $barrierEpoch has already finished. Maybe task \" +\n+          s\"$taskAttemptId is not properly killed.\"))\n+      } else {\n+        val syncRequests = getOrInitSyncRequests(stageId, stageAttemptId)\n+        // If this is the first sync message received for a barrier() call, init a timer to ensure\n+        // we may timeout for the sync.\n+        if (syncRequests.isEmpty) {\n+          timer.schedule(new TimerTask {\n+            override def run(): Unit = {\n+              // Timeout for current barrier() call, fail all the sync requests.\n+              val requests = getOrInitSyncRequests(stageId, stageAttemptId)"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "we should have some tests for the timeout behavior, by setting a very small timeout.",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-02T04:12:30Z",
    "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      // Remove internal data from a finished stage attempt.\n+      cleanupSyncRequests(stageInfo.stageId, stageInfo.attemptNumber)\n+      barrierEpochByStageIdAndAttempt.remove((stageInfo.stageId, stageInfo.attemptNumber))\n+    }\n+  }\n+\n+  // Epoch counter for each barrier (stage, attempt).\n+  private val barrierEpochByStageIdAndAttempt = new ConcurrentHashMap[(Int, Int), AtomicInteger]\n+\n+  // Remember all the blocking global sync requests for each barrier (stage, attempt).\n+  private val syncRequestsByStageIdAndAttempt =\n+    new ConcurrentHashMap[(Int, Int), ArrayBuffer[RpcCallContext]]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Get the array of [[RpcCallContext]]s that correspond to a barrier sync request from a stage\n+   * attempt.\n+   */\n+  private def getOrInitSyncRequests(\n+      stageId: Int,\n+      stageAttemptId: Int,\n+      numTasks: Int = 0): ArrayBuffer[RpcCallContext] = {\n+    val requests = syncRequestsByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId),\n+      new ArrayBuffer[RpcCallContext](numTasks))\n+    if (requests == null) {\n+      syncRequestsByStageIdAndAttempt.get((stageId, stageAttemptId))\n+    } else {\n+      requests\n+    }\n+  }\n+\n+  /**\n+   * Clean up the array of [[RpcCallContext]]s that correspond to a barrier sync request from a\n+   * stage attempt.\n+   */\n+  private def cleanupSyncRequests(stageId: Int, stageAttemptId: Int): Unit = {\n+    val requests = syncRequestsByStageIdAndAttempt.remove((stageId, stageAttemptId))\n+    if (requests != null) {\n+      requests.clear()\n+    }\n+    logInfo(s\"Removed all the pending barrier sync requests from Stage $stageId (Attempt \" +\n+      s\"$stageAttemptId).\")\n+  }\n+\n+  /**\n+   * Get the barrier epoch that correspond to a barrier sync request from a stage attempt.\n+   */\n+  private def getOrInitBarrierEpoch(stageId: Int, stageAttemptId: Int): AtomicInteger = {\n+    val barrierEpoch = barrierEpochByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId),\n+      new AtomicInteger(0))\n+    if (barrierEpoch == null) {\n+      barrierEpochByStageIdAndAttempt.get((stageId, stageAttemptId))\n+    } else {\n+      barrierEpoch\n+    }\n+  }\n+\n+  /**\n+   * Send failure to all the blocking barrier sync requests from a stage attempt with proper\n+   * failure message.\n+   */\n+  private def failAllSyncRequests(\n+      syncRequests: ArrayBuffer[RpcCallContext],\n+      message: String): Unit = {\n+    syncRequests.foreach(_.sendFailure(new SparkException(message)))\n+  }\n+\n+  /**\n+   * Finish all the blocking barrier sync requests from a stage attempt successfully if we\n+   * have received all the sync requests.\n+   */\n+  private def maybeFinishAllSyncRequests(\n+      syncRequests: ArrayBuffer[RpcCallContext],\n+      numTasks: Int): Boolean = {\n+    if (syncRequests.size == numTasks) {\n+      syncRequests.foreach(_.reply(()))\n+      return true\n+    }\n+\n+    false\n+  }\n+\n+\n+  override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = {\n+    case RequestToSync(numTasks, stageId, stageAttemptId, taskAttemptId, barrierEpoch) =>\n+      // Check the barrier epoch, to see which barrier() call we are processing.\n+      val currentBarrierEpoch = getOrInitBarrierEpoch(stageId, stageAttemptId)\n+      logInfo(s\"Current barrier epoch for Stage $stageId (Attempt $stageAttemptId) is\" +\n+        s\"$currentBarrierEpoch.\")\n+      if (currentBarrierEpoch.get() != barrierEpoch) {\n+        context.sendFailure(new SparkException(s\"The request to sync of Stage $stageId (Attempt \" +\n+          s\"$stageAttemptId) with barrier epoch $barrierEpoch has already finished. Maybe task \" +\n+          s\"$taskAttemptId is not properly killed.\"))\n+      } else {\n+        val syncRequests = getOrInitSyncRequests(stageId, stageAttemptId)\n+        // If this is the first sync message received for a barrier() call, init a timer to ensure\n+        // we may timeout for the sync.\n+        if (syncRequests.isEmpty) {\n+          timer.schedule(new TimerTask {\n+            override def run(): Unit = {\n+              // Timeout for current barrier() call, fail all the sync requests.\n+              val requests = getOrInitSyncRequests(stageId, stageAttemptId)"
  }, {
    "author": {
      "login": "jiangxb1987"
    },
    "body": "Em, how about cancel the TimerTask when sync request finished successfully?",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-02T05:34:05Z",
    "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      // Remove internal data from a finished stage attempt.\n+      cleanupSyncRequests(stageInfo.stageId, stageInfo.attemptNumber)\n+      barrierEpochByStageIdAndAttempt.remove((stageInfo.stageId, stageInfo.attemptNumber))\n+    }\n+  }\n+\n+  // Epoch counter for each barrier (stage, attempt).\n+  private val barrierEpochByStageIdAndAttempt = new ConcurrentHashMap[(Int, Int), AtomicInteger]\n+\n+  // Remember all the blocking global sync requests for each barrier (stage, attempt).\n+  private val syncRequestsByStageIdAndAttempt =\n+    new ConcurrentHashMap[(Int, Int), ArrayBuffer[RpcCallContext]]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Get the array of [[RpcCallContext]]s that correspond to a barrier sync request from a stage\n+   * attempt.\n+   */\n+  private def getOrInitSyncRequests(\n+      stageId: Int,\n+      stageAttemptId: Int,\n+      numTasks: Int = 0): ArrayBuffer[RpcCallContext] = {\n+    val requests = syncRequestsByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId),\n+      new ArrayBuffer[RpcCallContext](numTasks))\n+    if (requests == null) {\n+      syncRequestsByStageIdAndAttempt.get((stageId, stageAttemptId))\n+    } else {\n+      requests\n+    }\n+  }\n+\n+  /**\n+   * Clean up the array of [[RpcCallContext]]s that correspond to a barrier sync request from a\n+   * stage attempt.\n+   */\n+  private def cleanupSyncRequests(stageId: Int, stageAttemptId: Int): Unit = {\n+    val requests = syncRequestsByStageIdAndAttempt.remove((stageId, stageAttemptId))\n+    if (requests != null) {\n+      requests.clear()\n+    }\n+    logInfo(s\"Removed all the pending barrier sync requests from Stage $stageId (Attempt \" +\n+      s\"$stageAttemptId).\")\n+  }\n+\n+  /**\n+   * Get the barrier epoch that correspond to a barrier sync request from a stage attempt.\n+   */\n+  private def getOrInitBarrierEpoch(stageId: Int, stageAttemptId: Int): AtomicInteger = {\n+    val barrierEpoch = barrierEpochByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId),\n+      new AtomicInteger(0))\n+    if (barrierEpoch == null) {\n+      barrierEpochByStageIdAndAttempt.get((stageId, stageAttemptId))\n+    } else {\n+      barrierEpoch\n+    }\n+  }\n+\n+  /**\n+   * Send failure to all the blocking barrier sync requests from a stage attempt with proper\n+   * failure message.\n+   */\n+  private def failAllSyncRequests(\n+      syncRequests: ArrayBuffer[RpcCallContext],\n+      message: String): Unit = {\n+    syncRequests.foreach(_.sendFailure(new SparkException(message)))\n+  }\n+\n+  /**\n+   * Finish all the blocking barrier sync requests from a stage attempt successfully if we\n+   * have received all the sync requests.\n+   */\n+  private def maybeFinishAllSyncRequests(\n+      syncRequests: ArrayBuffer[RpcCallContext],\n+      numTasks: Int): Boolean = {\n+    if (syncRequests.size == numTasks) {\n+      syncRequests.foreach(_.reply(()))\n+      return true\n+    }\n+\n+    false\n+  }\n+\n+\n+  override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = {\n+    case RequestToSync(numTasks, stageId, stageAttemptId, taskAttemptId, barrierEpoch) =>\n+      // Check the barrier epoch, to see which barrier() call we are processing.\n+      val currentBarrierEpoch = getOrInitBarrierEpoch(stageId, stageAttemptId)\n+      logInfo(s\"Current barrier epoch for Stage $stageId (Attempt $stageAttemptId) is\" +\n+        s\"$currentBarrierEpoch.\")\n+      if (currentBarrierEpoch.get() != barrierEpoch) {\n+        context.sendFailure(new SparkException(s\"The request to sync of Stage $stageId (Attempt \" +\n+          s\"$stageAttemptId) with barrier epoch $barrierEpoch has already finished. Maybe task \" +\n+          s\"$taskAttemptId is not properly killed.\"))\n+      } else {\n+        val syncRequests = getOrInitSyncRequests(stageId, stageAttemptId)\n+        // If this is the first sync message received for a barrier() call, init a timer to ensure\n+        // we may timeout for the sync.\n+        if (syncRequests.isEmpty) {\n+          timer.schedule(new TimerTask {\n+            override def run(): Unit = {\n+              // Timeout for current barrier() call, fail all the sync requests.\n+              val requests = getOrInitSyncRequests(stageId, stageAttemptId)"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "yea we should do that, but we also need to consider race like sync request finishes and timer triggers at the same time.",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-02T05:39:25Z",
    "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      // Remove internal data from a finished stage attempt.\n+      cleanupSyncRequests(stageInfo.stageId, stageInfo.attemptNumber)\n+      barrierEpochByStageIdAndAttempt.remove((stageInfo.stageId, stageInfo.attemptNumber))\n+    }\n+  }\n+\n+  // Epoch counter for each barrier (stage, attempt).\n+  private val barrierEpochByStageIdAndAttempt = new ConcurrentHashMap[(Int, Int), AtomicInteger]\n+\n+  // Remember all the blocking global sync requests for each barrier (stage, attempt).\n+  private val syncRequestsByStageIdAndAttempt =\n+    new ConcurrentHashMap[(Int, Int), ArrayBuffer[RpcCallContext]]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Get the array of [[RpcCallContext]]s that correspond to a barrier sync request from a stage\n+   * attempt.\n+   */\n+  private def getOrInitSyncRequests(\n+      stageId: Int,\n+      stageAttemptId: Int,\n+      numTasks: Int = 0): ArrayBuffer[RpcCallContext] = {\n+    val requests = syncRequestsByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId),\n+      new ArrayBuffer[RpcCallContext](numTasks))\n+    if (requests == null) {\n+      syncRequestsByStageIdAndAttempt.get((stageId, stageAttemptId))\n+    } else {\n+      requests\n+    }\n+  }\n+\n+  /**\n+   * Clean up the array of [[RpcCallContext]]s that correspond to a barrier sync request from a\n+   * stage attempt.\n+   */\n+  private def cleanupSyncRequests(stageId: Int, stageAttemptId: Int): Unit = {\n+    val requests = syncRequestsByStageIdAndAttempt.remove((stageId, stageAttemptId))\n+    if (requests != null) {\n+      requests.clear()\n+    }\n+    logInfo(s\"Removed all the pending barrier sync requests from Stage $stageId (Attempt \" +\n+      s\"$stageAttemptId).\")\n+  }\n+\n+  /**\n+   * Get the barrier epoch that correspond to a barrier sync request from a stage attempt.\n+   */\n+  private def getOrInitBarrierEpoch(stageId: Int, stageAttemptId: Int): AtomicInteger = {\n+    val barrierEpoch = barrierEpochByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId),\n+      new AtomicInteger(0))\n+    if (barrierEpoch == null) {\n+      barrierEpochByStageIdAndAttempt.get((stageId, stageAttemptId))\n+    } else {\n+      barrierEpoch\n+    }\n+  }\n+\n+  /**\n+   * Send failure to all the blocking barrier sync requests from a stage attempt with proper\n+   * failure message.\n+   */\n+  private def failAllSyncRequests(\n+      syncRequests: ArrayBuffer[RpcCallContext],\n+      message: String): Unit = {\n+    syncRequests.foreach(_.sendFailure(new SparkException(message)))\n+  }\n+\n+  /**\n+   * Finish all the blocking barrier sync requests from a stage attempt successfully if we\n+   * have received all the sync requests.\n+   */\n+  private def maybeFinishAllSyncRequests(\n+      syncRequests: ArrayBuffer[RpcCallContext],\n+      numTasks: Int): Boolean = {\n+    if (syncRequests.size == numTasks) {\n+      syncRequests.foreach(_.reply(()))\n+      return true\n+    }\n+\n+    false\n+  }\n+\n+\n+  override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = {\n+    case RequestToSync(numTasks, stageId, stageAttemptId, taskAttemptId, barrierEpoch) =>\n+      // Check the barrier epoch, to see which barrier() call we are processing.\n+      val currentBarrierEpoch = getOrInitBarrierEpoch(stageId, stageAttemptId)\n+      logInfo(s\"Current barrier epoch for Stage $stageId (Attempt $stageAttemptId) is\" +\n+        s\"$currentBarrierEpoch.\")\n+      if (currentBarrierEpoch.get() != barrierEpoch) {\n+        context.sendFailure(new SparkException(s\"The request to sync of Stage $stageId (Attempt \" +\n+          s\"$stageAttemptId) with barrier epoch $barrierEpoch has already finished. Maybe task \" +\n+          s\"$taskAttemptId is not properly killed.\"))\n+      } else {\n+        val syncRequests = getOrInitSyncRequests(stageId, stageAttemptId)\n+        // If this is the first sync message received for a barrier() call, init a timer to ensure\n+        // we may timeout for the sync.\n+        if (syncRequests.isEmpty) {\n+          timer.schedule(new TimerTask {\n+            override def run(): Unit = {\n+              // Timeout for current barrier() call, fail all the sync requests.\n+              val requests = getOrInitSyncRequests(stageId, stageAttemptId)"
  }, {
    "author": {
      "login": "jiangxb1987"
    },
    "body": "We will also remove the internal data on stage completed, so I assume the race condition you mentioned won't cause serious issues, the internal data will after all be removed.",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-02T05:52:14Z",
    "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      // Remove internal data from a finished stage attempt.\n+      cleanupSyncRequests(stageInfo.stageId, stageInfo.attemptNumber)\n+      barrierEpochByStageIdAndAttempt.remove((stageInfo.stageId, stageInfo.attemptNumber))\n+    }\n+  }\n+\n+  // Epoch counter for each barrier (stage, attempt).\n+  private val barrierEpochByStageIdAndAttempt = new ConcurrentHashMap[(Int, Int), AtomicInteger]\n+\n+  // Remember all the blocking global sync requests for each barrier (stage, attempt).\n+  private val syncRequestsByStageIdAndAttempt =\n+    new ConcurrentHashMap[(Int, Int), ArrayBuffer[RpcCallContext]]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Get the array of [[RpcCallContext]]s that correspond to a barrier sync request from a stage\n+   * attempt.\n+   */\n+  private def getOrInitSyncRequests(\n+      stageId: Int,\n+      stageAttemptId: Int,\n+      numTasks: Int = 0): ArrayBuffer[RpcCallContext] = {\n+    val requests = syncRequestsByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId),\n+      new ArrayBuffer[RpcCallContext](numTasks))\n+    if (requests == null) {\n+      syncRequestsByStageIdAndAttempt.get((stageId, stageAttemptId))\n+    } else {\n+      requests\n+    }\n+  }\n+\n+  /**\n+   * Clean up the array of [[RpcCallContext]]s that correspond to a barrier sync request from a\n+   * stage attempt.\n+   */\n+  private def cleanupSyncRequests(stageId: Int, stageAttemptId: Int): Unit = {\n+    val requests = syncRequestsByStageIdAndAttempt.remove((stageId, stageAttemptId))\n+    if (requests != null) {\n+      requests.clear()\n+    }\n+    logInfo(s\"Removed all the pending barrier sync requests from Stage $stageId (Attempt \" +\n+      s\"$stageAttemptId).\")\n+  }\n+\n+  /**\n+   * Get the barrier epoch that correspond to a barrier sync request from a stage attempt.\n+   */\n+  private def getOrInitBarrierEpoch(stageId: Int, stageAttemptId: Int): AtomicInteger = {\n+    val barrierEpoch = barrierEpochByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId),\n+      new AtomicInteger(0))\n+    if (barrierEpoch == null) {\n+      barrierEpochByStageIdAndAttempt.get((stageId, stageAttemptId))\n+    } else {\n+      barrierEpoch\n+    }\n+  }\n+\n+  /**\n+   * Send failure to all the blocking barrier sync requests from a stage attempt with proper\n+   * failure message.\n+   */\n+  private def failAllSyncRequests(\n+      syncRequests: ArrayBuffer[RpcCallContext],\n+      message: String): Unit = {\n+    syncRequests.foreach(_.sendFailure(new SparkException(message)))\n+  }\n+\n+  /**\n+   * Finish all the blocking barrier sync requests from a stage attempt successfully if we\n+   * have received all the sync requests.\n+   */\n+  private def maybeFinishAllSyncRequests(\n+      syncRequests: ArrayBuffer[RpcCallContext],\n+      numTasks: Int): Boolean = {\n+    if (syncRequests.size == numTasks) {\n+      syncRequests.foreach(_.reply(()))\n+      return true\n+    }\n+\n+    false\n+  }\n+\n+\n+  override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = {\n+    case RequestToSync(numTasks, stageId, stageAttemptId, taskAttemptId, barrierEpoch) =>\n+      // Check the barrier epoch, to see which barrier() call we are processing.\n+      val currentBarrierEpoch = getOrInitBarrierEpoch(stageId, stageAttemptId)\n+      logInfo(s\"Current barrier epoch for Stage $stageId (Attempt $stageAttemptId) is\" +\n+        s\"$currentBarrierEpoch.\")\n+      if (currentBarrierEpoch.get() != barrierEpoch) {\n+        context.sendFailure(new SparkException(s\"The request to sync of Stage $stageId (Attempt \" +\n+          s\"$stageAttemptId) with barrier epoch $barrierEpoch has already finished. Maybe task \" +\n+          s\"$taskAttemptId is not properly killed.\"))\n+      } else {\n+        val syncRequests = getOrInitSyncRequests(stageId, stageAttemptId)\n+        // If this is the first sync message received for a barrier() call, init a timer to ensure\n+        // we may timeout for the sync.\n+        if (syncRequests.isEmpty) {\n+          timer.schedule(new TimerTask {\n+            override def run(): Unit = {\n+              // Timeout for current barrier() call, fail all the sync requests.\n+              val requests = getOrInitSyncRequests(stageId, stageAttemptId)"
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "although very unlikely, shall we add an assert that `syncRequests.length == numTasks`? Just in case we have a bug and some barrier tasks have a different value of `numTasks`.",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-02T04:14:06Z",
    "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      // Remove internal data from a finished stage attempt.\n+      cleanupSyncRequests(stageInfo.stageId, stageInfo.attemptNumber)\n+      barrierEpochByStageIdAndAttempt.remove((stageInfo.stageId, stageInfo.attemptNumber))\n+    }\n+  }\n+\n+  // Epoch counter for each barrier (stage, attempt).\n+  private val barrierEpochByStageIdAndAttempt = new ConcurrentHashMap[(Int, Int), AtomicInteger]\n+\n+  // Remember all the blocking global sync requests for each barrier (stage, attempt).\n+  private val syncRequestsByStageIdAndAttempt =\n+    new ConcurrentHashMap[(Int, Int), ArrayBuffer[RpcCallContext]]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Get the array of [[RpcCallContext]]s that correspond to a barrier sync request from a stage\n+   * attempt.\n+   */\n+  private def getOrInitSyncRequests(\n+      stageId: Int,\n+      stageAttemptId: Int,\n+      numTasks: Int = 0): ArrayBuffer[RpcCallContext] = {\n+    val requests = syncRequestsByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId),\n+      new ArrayBuffer[RpcCallContext](numTasks))\n+    if (requests == null) {\n+      syncRequestsByStageIdAndAttempt.get((stageId, stageAttemptId))\n+    } else {\n+      requests\n+    }\n+  }\n+\n+  /**\n+   * Clean up the array of [[RpcCallContext]]s that correspond to a barrier sync request from a\n+   * stage attempt.\n+   */\n+  private def cleanupSyncRequests(stageId: Int, stageAttemptId: Int): Unit = {\n+    val requests = syncRequestsByStageIdAndAttempt.remove((stageId, stageAttemptId))\n+    if (requests != null) {\n+      requests.clear()\n+    }\n+    logInfo(s\"Removed all the pending barrier sync requests from Stage $stageId (Attempt \" +\n+      s\"$stageAttemptId).\")\n+  }\n+\n+  /**\n+   * Get the barrier epoch that correspond to a barrier sync request from a stage attempt.\n+   */\n+  private def getOrInitBarrierEpoch(stageId: Int, stageAttemptId: Int): AtomicInteger = {\n+    val barrierEpoch = barrierEpochByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId),\n+      new AtomicInteger(0))\n+    if (barrierEpoch == null) {\n+      barrierEpochByStageIdAndAttempt.get((stageId, stageAttemptId))\n+    } else {\n+      barrierEpoch\n+    }\n+  }\n+\n+  /**\n+   * Send failure to all the blocking barrier sync requests from a stage attempt with proper\n+   * failure message.\n+   */\n+  private def failAllSyncRequests(\n+      syncRequests: ArrayBuffer[RpcCallContext],\n+      message: String): Unit = {\n+    syncRequests.foreach(_.sendFailure(new SparkException(message)))\n+  }\n+\n+  /**\n+   * Finish all the blocking barrier sync requests from a stage attempt successfully if we\n+   * have received all the sync requests.\n+   */\n+  private def maybeFinishAllSyncRequests(\n+      syncRequests: ArrayBuffer[RpcCallContext],\n+      numTasks: Int): Boolean = {\n+    if (syncRequests.size == numTasks) {\n+      syncRequests.foreach(_.reply(()))\n+      return true\n+    }\n+\n+    false\n+  }\n+\n+\n+  override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = {\n+    case RequestToSync(numTasks, stageId, stageAttemptId, taskAttemptId, barrierEpoch) =>\n+      // Check the barrier epoch, to see which barrier() call we are processing.\n+      val currentBarrierEpoch = getOrInitBarrierEpoch(stageId, stageAttemptId)\n+      logInfo(s\"Current barrier epoch for Stage $stageId (Attempt $stageAttemptId) is\" +\n+        s\"$currentBarrierEpoch.\")\n+      if (currentBarrierEpoch.get() != barrierEpoch) {\n+        context.sendFailure(new SparkException(s\"The request to sync of Stage $stageId (Attempt \" +\n+          s\"$stageAttemptId) with barrier epoch $barrierEpoch has already finished. Maybe task \" +\n+          s\"$taskAttemptId is not properly killed.\"))\n+      } else {\n+        val syncRequests = getOrInitSyncRequests(stageId, stageAttemptId)\n+        // If this is the first sync message received for a barrier() call, init a timer to ensure\n+        // we may timeout for the sync.\n+        if (syncRequests.isEmpty) {\n+          timer.schedule(new TimerTask {\n+            override def run(): Unit = {\n+              // Timeout for current barrier() call, fail all the sync requests.\n+              val requests = getOrInitSyncRequests(stageId, stageAttemptId)\n+              failAllSyncRequests(requests, \"The coordinator didn't get all barrier sync \" +\n+                s\"requests for barrier epoch $barrierEpoch from Stage $stageId (Attempt \" +\n+                s\"$stageAttemptId) within ${timeout}s.\")\n+              cleanupSyncRequests(stageId, stageAttemptId)\n+              // The global sync fails so the stage is expected to retry another attempt, all sync\n+              // messages come from current stage attempt shall fail.\n+              currentBarrierEpoch.set(-1)\n+            }\n+          }, timeout * 1000)\n+        }\n+\n+        syncRequests += context"
  }, {
    "author": {
      "login": "jiangxb1987"
    },
    "body": "We don't remember the `numTasks` in BarrierCoordinator, if it really worth that then we have to use another map to store the information.",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-02T05:42:34Z",
    "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      // Remove internal data from a finished stage attempt.\n+      cleanupSyncRequests(stageInfo.stageId, stageInfo.attemptNumber)\n+      barrierEpochByStageIdAndAttempt.remove((stageInfo.stageId, stageInfo.attemptNumber))\n+    }\n+  }\n+\n+  // Epoch counter for each barrier (stage, attempt).\n+  private val barrierEpochByStageIdAndAttempt = new ConcurrentHashMap[(Int, Int), AtomicInteger]\n+\n+  // Remember all the blocking global sync requests for each barrier (stage, attempt).\n+  private val syncRequestsByStageIdAndAttempt =\n+    new ConcurrentHashMap[(Int, Int), ArrayBuffer[RpcCallContext]]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Get the array of [[RpcCallContext]]s that correspond to a barrier sync request from a stage\n+   * attempt.\n+   */\n+  private def getOrInitSyncRequests(\n+      stageId: Int,\n+      stageAttemptId: Int,\n+      numTasks: Int = 0): ArrayBuffer[RpcCallContext] = {\n+    val requests = syncRequestsByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId),\n+      new ArrayBuffer[RpcCallContext](numTasks))\n+    if (requests == null) {\n+      syncRequestsByStageIdAndAttempt.get((stageId, stageAttemptId))\n+    } else {\n+      requests\n+    }\n+  }\n+\n+  /**\n+   * Clean up the array of [[RpcCallContext]]s that correspond to a barrier sync request from a\n+   * stage attempt.\n+   */\n+  private def cleanupSyncRequests(stageId: Int, stageAttemptId: Int): Unit = {\n+    val requests = syncRequestsByStageIdAndAttempt.remove((stageId, stageAttemptId))\n+    if (requests != null) {\n+      requests.clear()\n+    }\n+    logInfo(s\"Removed all the pending barrier sync requests from Stage $stageId (Attempt \" +\n+      s\"$stageAttemptId).\")\n+  }\n+\n+  /**\n+   * Get the barrier epoch that correspond to a barrier sync request from a stage attempt.\n+   */\n+  private def getOrInitBarrierEpoch(stageId: Int, stageAttemptId: Int): AtomicInteger = {\n+    val barrierEpoch = barrierEpochByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId),\n+      new AtomicInteger(0))\n+    if (barrierEpoch == null) {\n+      barrierEpochByStageIdAndAttempt.get((stageId, stageAttemptId))\n+    } else {\n+      barrierEpoch\n+    }\n+  }\n+\n+  /**\n+   * Send failure to all the blocking barrier sync requests from a stage attempt with proper\n+   * failure message.\n+   */\n+  private def failAllSyncRequests(\n+      syncRequests: ArrayBuffer[RpcCallContext],\n+      message: String): Unit = {\n+    syncRequests.foreach(_.sendFailure(new SparkException(message)))\n+  }\n+\n+  /**\n+   * Finish all the blocking barrier sync requests from a stage attempt successfully if we\n+   * have received all the sync requests.\n+   */\n+  private def maybeFinishAllSyncRequests(\n+      syncRequests: ArrayBuffer[RpcCallContext],\n+      numTasks: Int): Boolean = {\n+    if (syncRequests.size == numTasks) {\n+      syncRequests.foreach(_.reply(()))\n+      return true\n+    }\n+\n+    false\n+  }\n+\n+\n+  override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = {\n+    case RequestToSync(numTasks, stageId, stageAttemptId, taskAttemptId, barrierEpoch) =>\n+      // Check the barrier epoch, to see which barrier() call we are processing.\n+      val currentBarrierEpoch = getOrInitBarrierEpoch(stageId, stageAttemptId)\n+      logInfo(s\"Current barrier epoch for Stage $stageId (Attempt $stageAttemptId) is\" +\n+        s\"$currentBarrierEpoch.\")\n+      if (currentBarrierEpoch.get() != barrierEpoch) {\n+        context.sendFailure(new SparkException(s\"The request to sync of Stage $stageId (Attempt \" +\n+          s\"$stageAttemptId) with barrier epoch $barrierEpoch has already finished. Maybe task \" +\n+          s\"$taskAttemptId is not properly killed.\"))\n+      } else {\n+        val syncRequests = getOrInitSyncRequests(stageId, stageAttemptId)\n+        // If this is the first sync message received for a barrier() call, init a timer to ensure\n+        // we may timeout for the sync.\n+        if (syncRequests.isEmpty) {\n+          timer.schedule(new TimerTask {\n+            override def run(): Unit = {\n+              // Timeout for current barrier() call, fail all the sync requests.\n+              val requests = getOrInitSyncRequests(stageId, stageAttemptId)\n+              failAllSyncRequests(requests, \"The coordinator didn't get all barrier sync \" +\n+                s\"requests for barrier epoch $barrierEpoch from Stage $stageId (Attempt \" +\n+                s\"$stageAttemptId) within ${timeout}s.\")\n+              cleanupSyncRequests(stageId, stageAttemptId)\n+              // The global sync fails so the stage is expected to retry another attempt, all sync\n+              // messages come from current stage attempt shall fail.\n+              currentBarrierEpoch.set(-1)\n+            }\n+          }, timeout * 1000)\n+        }\n+\n+        syncRequests += context"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "each barrier task remembers numTask, so here we can make sure  barrier tasks of same group would have same `numTasks`",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-02T07:11:15Z",
    "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      // Remove internal data from a finished stage attempt.\n+      cleanupSyncRequests(stageInfo.stageId, stageInfo.attemptNumber)\n+      barrierEpochByStageIdAndAttempt.remove((stageInfo.stageId, stageInfo.attemptNumber))\n+    }\n+  }\n+\n+  // Epoch counter for each barrier (stage, attempt).\n+  private val barrierEpochByStageIdAndAttempt = new ConcurrentHashMap[(Int, Int), AtomicInteger]\n+\n+  // Remember all the blocking global sync requests for each barrier (stage, attempt).\n+  private val syncRequestsByStageIdAndAttempt =\n+    new ConcurrentHashMap[(Int, Int), ArrayBuffer[RpcCallContext]]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Get the array of [[RpcCallContext]]s that correspond to a barrier sync request from a stage\n+   * attempt.\n+   */\n+  private def getOrInitSyncRequests(\n+      stageId: Int,\n+      stageAttemptId: Int,\n+      numTasks: Int = 0): ArrayBuffer[RpcCallContext] = {\n+    val requests = syncRequestsByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId),\n+      new ArrayBuffer[RpcCallContext](numTasks))\n+    if (requests == null) {\n+      syncRequestsByStageIdAndAttempt.get((stageId, stageAttemptId))\n+    } else {\n+      requests\n+    }\n+  }\n+\n+  /**\n+   * Clean up the array of [[RpcCallContext]]s that correspond to a barrier sync request from a\n+   * stage attempt.\n+   */\n+  private def cleanupSyncRequests(stageId: Int, stageAttemptId: Int): Unit = {\n+    val requests = syncRequestsByStageIdAndAttempt.remove((stageId, stageAttemptId))\n+    if (requests != null) {\n+      requests.clear()\n+    }\n+    logInfo(s\"Removed all the pending barrier sync requests from Stage $stageId (Attempt \" +\n+      s\"$stageAttemptId).\")\n+  }\n+\n+  /**\n+   * Get the barrier epoch that correspond to a barrier sync request from a stage attempt.\n+   */\n+  private def getOrInitBarrierEpoch(stageId: Int, stageAttemptId: Int): AtomicInteger = {\n+    val barrierEpoch = barrierEpochByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId),\n+      new AtomicInteger(0))\n+    if (barrierEpoch == null) {\n+      barrierEpochByStageIdAndAttempt.get((stageId, stageAttemptId))\n+    } else {\n+      barrierEpoch\n+    }\n+  }\n+\n+  /**\n+   * Send failure to all the blocking barrier sync requests from a stage attempt with proper\n+   * failure message.\n+   */\n+  private def failAllSyncRequests(\n+      syncRequests: ArrayBuffer[RpcCallContext],\n+      message: String): Unit = {\n+    syncRequests.foreach(_.sendFailure(new SparkException(message)))\n+  }\n+\n+  /**\n+   * Finish all the blocking barrier sync requests from a stage attempt successfully if we\n+   * have received all the sync requests.\n+   */\n+  private def maybeFinishAllSyncRequests(\n+      syncRequests: ArrayBuffer[RpcCallContext],\n+      numTasks: Int): Boolean = {\n+    if (syncRequests.size == numTasks) {\n+      syncRequests.foreach(_.reply(()))\n+      return true\n+    }\n+\n+    false\n+  }\n+\n+\n+  override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = {\n+    case RequestToSync(numTasks, stageId, stageAttemptId, taskAttemptId, barrierEpoch) =>\n+      // Check the barrier epoch, to see which barrier() call we are processing.\n+      val currentBarrierEpoch = getOrInitBarrierEpoch(stageId, stageAttemptId)\n+      logInfo(s\"Current barrier epoch for Stage $stageId (Attempt $stageAttemptId) is\" +\n+        s\"$currentBarrierEpoch.\")\n+      if (currentBarrierEpoch.get() != barrierEpoch) {\n+        context.sendFailure(new SparkException(s\"The request to sync of Stage $stageId (Attempt \" +\n+          s\"$stageAttemptId) with barrier epoch $barrierEpoch has already finished. Maybe task \" +\n+          s\"$taskAttemptId is not properly killed.\"))\n+      } else {\n+        val syncRequests = getOrInitSyncRequests(stageId, stageAttemptId)\n+        // If this is the first sync message received for a barrier() call, init a timer to ensure\n+        // we may timeout for the sync.\n+        if (syncRequests.isEmpty) {\n+          timer.schedule(new TimerTask {\n+            override def run(): Unit = {\n+              // Timeout for current barrier() call, fail all the sync requests.\n+              val requests = getOrInitSyncRequests(stageId, stageAttemptId)\n+              failAllSyncRequests(requests, \"The coordinator didn't get all barrier sync \" +\n+                s\"requests for barrier epoch $barrierEpoch from Stage $stageId (Attempt \" +\n+                s\"$stageAttemptId) within ${timeout}s.\")\n+              cleanupSyncRequests(stageId, stageAttemptId)\n+              // The global sync fails so the stage is expected to retry another attempt, all sync\n+              // messages come from current stage attempt shall fail.\n+              currentBarrierEpoch.set(-1)\n+            }\n+          }, timeout * 1000)\n+        }\n+\n+        syncRequests += context"
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "do we need to do this? if the barrier sync successes before timeout, the epoch data will be removed from `barrierEpochByStageIdAndAttempt` and get GCed.",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-02T04:16:21Z",
    "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      // Remove internal data from a finished stage attempt.\n+      cleanupSyncRequests(stageInfo.stageId, stageInfo.attemptNumber)\n+      barrierEpochByStageIdAndAttempt.remove((stageInfo.stageId, stageInfo.attemptNumber))\n+    }\n+  }\n+\n+  // Epoch counter for each barrier (stage, attempt).\n+  private val barrierEpochByStageIdAndAttempt = new ConcurrentHashMap[(Int, Int), AtomicInteger]\n+\n+  // Remember all the blocking global sync requests for each barrier (stage, attempt).\n+  private val syncRequestsByStageIdAndAttempt =\n+    new ConcurrentHashMap[(Int, Int), ArrayBuffer[RpcCallContext]]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Get the array of [[RpcCallContext]]s that correspond to a barrier sync request from a stage\n+   * attempt.\n+   */\n+  private def getOrInitSyncRequests(\n+      stageId: Int,\n+      stageAttemptId: Int,\n+      numTasks: Int = 0): ArrayBuffer[RpcCallContext] = {\n+    val requests = syncRequestsByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId),\n+      new ArrayBuffer[RpcCallContext](numTasks))\n+    if (requests == null) {\n+      syncRequestsByStageIdAndAttempt.get((stageId, stageAttemptId))\n+    } else {\n+      requests\n+    }\n+  }\n+\n+  /**\n+   * Clean up the array of [[RpcCallContext]]s that correspond to a barrier sync request from a\n+   * stage attempt.\n+   */\n+  private def cleanupSyncRequests(stageId: Int, stageAttemptId: Int): Unit = {\n+    val requests = syncRequestsByStageIdAndAttempt.remove((stageId, stageAttemptId))\n+    if (requests != null) {\n+      requests.clear()\n+    }\n+    logInfo(s\"Removed all the pending barrier sync requests from Stage $stageId (Attempt \" +\n+      s\"$stageAttemptId).\")\n+  }\n+\n+  /**\n+   * Get the barrier epoch that correspond to a barrier sync request from a stage attempt.\n+   */\n+  private def getOrInitBarrierEpoch(stageId: Int, stageAttemptId: Int): AtomicInteger = {\n+    val barrierEpoch = barrierEpochByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId),\n+      new AtomicInteger(0))\n+    if (barrierEpoch == null) {\n+      barrierEpochByStageIdAndAttempt.get((stageId, stageAttemptId))\n+    } else {\n+      barrierEpoch\n+    }\n+  }\n+\n+  /**\n+   * Send failure to all the blocking barrier sync requests from a stage attempt with proper\n+   * failure message.\n+   */\n+  private def failAllSyncRequests(\n+      syncRequests: ArrayBuffer[RpcCallContext],\n+      message: String): Unit = {\n+    syncRequests.foreach(_.sendFailure(new SparkException(message)))\n+  }\n+\n+  /**\n+   * Finish all the blocking barrier sync requests from a stage attempt successfully if we\n+   * have received all the sync requests.\n+   */\n+  private def maybeFinishAllSyncRequests(\n+      syncRequests: ArrayBuffer[RpcCallContext],\n+      numTasks: Int): Boolean = {\n+    if (syncRequests.size == numTasks) {\n+      syncRequests.foreach(_.reply(()))\n+      return true\n+    }\n+\n+    false\n+  }\n+\n+\n+  override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = {\n+    case RequestToSync(numTasks, stageId, stageAttemptId, taskAttemptId, barrierEpoch) =>\n+      // Check the barrier epoch, to see which barrier() call we are processing.\n+      val currentBarrierEpoch = getOrInitBarrierEpoch(stageId, stageAttemptId)\n+      logInfo(s\"Current barrier epoch for Stage $stageId (Attempt $stageAttemptId) is\" +\n+        s\"$currentBarrierEpoch.\")\n+      if (currentBarrierEpoch.get() != barrierEpoch) {\n+        context.sendFailure(new SparkException(s\"The request to sync of Stage $stageId (Attempt \" +\n+          s\"$stageAttemptId) with barrier epoch $barrierEpoch has already finished. Maybe task \" +\n+          s\"$taskAttemptId is not properly killed.\"))\n+      } else {\n+        val syncRequests = getOrInitSyncRequests(stageId, stageAttemptId)\n+        // If this is the first sync message received for a barrier() call, init a timer to ensure\n+        // we may timeout for the sync.\n+        if (syncRequests.isEmpty) {\n+          timer.schedule(new TimerTask {\n+            override def run(): Unit = {\n+              // Timeout for current barrier() call, fail all the sync requests.\n+              val requests = getOrInitSyncRequests(stageId, stageAttemptId)\n+              failAllSyncRequests(requests, \"The coordinator didn't get all barrier sync \" +\n+                s\"requests for barrier epoch $barrierEpoch from Stage $stageId (Attempt \" +\n+                s\"$stageAttemptId) within ${timeout}s.\")\n+              cleanupSyncRequests(stageId, stageAttemptId)\n+              // The global sync fails so the stage is expected to retry another attempt, all sync\n+              // messages come from current stage attempt shall fail.\n+              currentBarrierEpoch.set(-1)"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "ah i see, it is for any late tasks so we can fail them via epoch mismatch.",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-02T04:19:19Z",
    "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      // Remove internal data from a finished stage attempt.\n+      cleanupSyncRequests(stageInfo.stageId, stageInfo.attemptNumber)\n+      barrierEpochByStageIdAndAttempt.remove((stageInfo.stageId, stageInfo.attemptNumber))\n+    }\n+  }\n+\n+  // Epoch counter for each barrier (stage, attempt).\n+  private val barrierEpochByStageIdAndAttempt = new ConcurrentHashMap[(Int, Int), AtomicInteger]\n+\n+  // Remember all the blocking global sync requests for each barrier (stage, attempt).\n+  private val syncRequestsByStageIdAndAttempt =\n+    new ConcurrentHashMap[(Int, Int), ArrayBuffer[RpcCallContext]]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Get the array of [[RpcCallContext]]s that correspond to a barrier sync request from a stage\n+   * attempt.\n+   */\n+  private def getOrInitSyncRequests(\n+      stageId: Int,\n+      stageAttemptId: Int,\n+      numTasks: Int = 0): ArrayBuffer[RpcCallContext] = {\n+    val requests = syncRequestsByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId),\n+      new ArrayBuffer[RpcCallContext](numTasks))\n+    if (requests == null) {\n+      syncRequestsByStageIdAndAttempt.get((stageId, stageAttemptId))\n+    } else {\n+      requests\n+    }\n+  }\n+\n+  /**\n+   * Clean up the array of [[RpcCallContext]]s that correspond to a barrier sync request from a\n+   * stage attempt.\n+   */\n+  private def cleanupSyncRequests(stageId: Int, stageAttemptId: Int): Unit = {\n+    val requests = syncRequestsByStageIdAndAttempt.remove((stageId, stageAttemptId))\n+    if (requests != null) {\n+      requests.clear()\n+    }\n+    logInfo(s\"Removed all the pending barrier sync requests from Stage $stageId (Attempt \" +\n+      s\"$stageAttemptId).\")\n+  }\n+\n+  /**\n+   * Get the barrier epoch that correspond to a barrier sync request from a stage attempt.\n+   */\n+  private def getOrInitBarrierEpoch(stageId: Int, stageAttemptId: Int): AtomicInteger = {\n+    val barrierEpoch = barrierEpochByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId),\n+      new AtomicInteger(0))\n+    if (barrierEpoch == null) {\n+      barrierEpochByStageIdAndAttempt.get((stageId, stageAttemptId))\n+    } else {\n+      barrierEpoch\n+    }\n+  }\n+\n+  /**\n+   * Send failure to all the blocking barrier sync requests from a stage attempt with proper\n+   * failure message.\n+   */\n+  private def failAllSyncRequests(\n+      syncRequests: ArrayBuffer[RpcCallContext],\n+      message: String): Unit = {\n+    syncRequests.foreach(_.sendFailure(new SparkException(message)))\n+  }\n+\n+  /**\n+   * Finish all the blocking barrier sync requests from a stage attempt successfully if we\n+   * have received all the sync requests.\n+   */\n+  private def maybeFinishAllSyncRequests(\n+      syncRequests: ArrayBuffer[RpcCallContext],\n+      numTasks: Int): Boolean = {\n+    if (syncRequests.size == numTasks) {\n+      syncRequests.foreach(_.reply(()))\n+      return true\n+    }\n+\n+    false\n+  }\n+\n+\n+  override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = {\n+    case RequestToSync(numTasks, stageId, stageAttemptId, taskAttemptId, barrierEpoch) =>\n+      // Check the barrier epoch, to see which barrier() call we are processing.\n+      val currentBarrierEpoch = getOrInitBarrierEpoch(stageId, stageAttemptId)\n+      logInfo(s\"Current barrier epoch for Stage $stageId (Attempt $stageAttemptId) is\" +\n+        s\"$currentBarrierEpoch.\")\n+      if (currentBarrierEpoch.get() != barrierEpoch) {\n+        context.sendFailure(new SparkException(s\"The request to sync of Stage $stageId (Attempt \" +\n+          s\"$stageAttemptId) with barrier epoch $barrierEpoch has already finished. Maybe task \" +\n+          s\"$taskAttemptId is not properly killed.\"))\n+      } else {\n+        val syncRequests = getOrInitSyncRequests(stageId, stageAttemptId)\n+        // If this is the first sync message received for a barrier() call, init a timer to ensure\n+        // we may timeout for the sync.\n+        if (syncRequests.isEmpty) {\n+          timer.schedule(new TimerTask {\n+            override def run(): Unit = {\n+              // Timeout for current barrier() call, fail all the sync requests.\n+              val requests = getOrInitSyncRequests(stageId, stageAttemptId)\n+              failAllSyncRequests(requests, \"The coordinator didn't get all barrier sync \" +\n+                s\"requests for barrier epoch $barrierEpoch from Stage $stageId (Attempt \" +\n+                s\"$stageAttemptId) within ${timeout}s.\")\n+              cleanupSyncRequests(stageId, stageAttemptId)\n+              // The global sync fails so the stage is expected to retry another attempt, all sync\n+              // messages come from current stage attempt shall fail.\n+              currentBarrierEpoch.set(-1)"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "we should not use `currentBarrierEpoch` here, it will hold its reference and make it un-GC-able even the barrier sync is finished before timeout. we should get the epoch id from `barrierEpochByStageIdAndAttempt`",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-02T04:21:08Z",
    "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      // Remove internal data from a finished stage attempt.\n+      cleanupSyncRequests(stageInfo.stageId, stageInfo.attemptNumber)\n+      barrierEpochByStageIdAndAttempt.remove((stageInfo.stageId, stageInfo.attemptNumber))\n+    }\n+  }\n+\n+  // Epoch counter for each barrier (stage, attempt).\n+  private val barrierEpochByStageIdAndAttempt = new ConcurrentHashMap[(Int, Int), AtomicInteger]\n+\n+  // Remember all the blocking global sync requests for each barrier (stage, attempt).\n+  private val syncRequestsByStageIdAndAttempt =\n+    new ConcurrentHashMap[(Int, Int), ArrayBuffer[RpcCallContext]]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Get the array of [[RpcCallContext]]s that correspond to a barrier sync request from a stage\n+   * attempt.\n+   */\n+  private def getOrInitSyncRequests(\n+      stageId: Int,\n+      stageAttemptId: Int,\n+      numTasks: Int = 0): ArrayBuffer[RpcCallContext] = {\n+    val requests = syncRequestsByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId),\n+      new ArrayBuffer[RpcCallContext](numTasks))\n+    if (requests == null) {\n+      syncRequestsByStageIdAndAttempt.get((stageId, stageAttemptId))\n+    } else {\n+      requests\n+    }\n+  }\n+\n+  /**\n+   * Clean up the array of [[RpcCallContext]]s that correspond to a barrier sync request from a\n+   * stage attempt.\n+   */\n+  private def cleanupSyncRequests(stageId: Int, stageAttemptId: Int): Unit = {\n+    val requests = syncRequestsByStageIdAndAttempt.remove((stageId, stageAttemptId))\n+    if (requests != null) {\n+      requests.clear()\n+    }\n+    logInfo(s\"Removed all the pending barrier sync requests from Stage $stageId (Attempt \" +\n+      s\"$stageAttemptId).\")\n+  }\n+\n+  /**\n+   * Get the barrier epoch that correspond to a barrier sync request from a stage attempt.\n+   */\n+  private def getOrInitBarrierEpoch(stageId: Int, stageAttemptId: Int): AtomicInteger = {\n+    val barrierEpoch = barrierEpochByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId),\n+      new AtomicInteger(0))\n+    if (barrierEpoch == null) {\n+      barrierEpochByStageIdAndAttempt.get((stageId, stageAttemptId))\n+    } else {\n+      barrierEpoch\n+    }\n+  }\n+\n+  /**\n+   * Send failure to all the blocking barrier sync requests from a stage attempt with proper\n+   * failure message.\n+   */\n+  private def failAllSyncRequests(\n+      syncRequests: ArrayBuffer[RpcCallContext],\n+      message: String): Unit = {\n+    syncRequests.foreach(_.sendFailure(new SparkException(message)))\n+  }\n+\n+  /**\n+   * Finish all the blocking barrier sync requests from a stage attempt successfully if we\n+   * have received all the sync requests.\n+   */\n+  private def maybeFinishAllSyncRequests(\n+      syncRequests: ArrayBuffer[RpcCallContext],\n+      numTasks: Int): Boolean = {\n+    if (syncRequests.size == numTasks) {\n+      syncRequests.foreach(_.reply(()))\n+      return true\n+    }\n+\n+    false\n+  }\n+\n+\n+  override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = {\n+    case RequestToSync(numTasks, stageId, stageAttemptId, taskAttemptId, barrierEpoch) =>\n+      // Check the barrier epoch, to see which barrier() call we are processing.\n+      val currentBarrierEpoch = getOrInitBarrierEpoch(stageId, stageAttemptId)\n+      logInfo(s\"Current barrier epoch for Stage $stageId (Attempt $stageAttemptId) is\" +\n+        s\"$currentBarrierEpoch.\")\n+      if (currentBarrierEpoch.get() != barrierEpoch) {\n+        context.sendFailure(new SparkException(s\"The request to sync of Stage $stageId (Attempt \" +\n+          s\"$stageAttemptId) with barrier epoch $barrierEpoch has already finished. Maybe task \" +\n+          s\"$taskAttemptId is not properly killed.\"))\n+      } else {\n+        val syncRequests = getOrInitSyncRequests(stageId, stageAttemptId)\n+        // If this is the first sync message received for a barrier() call, init a timer to ensure\n+        // we may timeout for the sync.\n+        if (syncRequests.isEmpty) {\n+          timer.schedule(new TimerTask {\n+            override def run(): Unit = {\n+              // Timeout for current barrier() call, fail all the sync requests.\n+              val requests = getOrInitSyncRequests(stageId, stageAttemptId)\n+              failAllSyncRequests(requests, \"The coordinator didn't get all barrier sync \" +\n+                s\"requests for barrier epoch $barrierEpoch from Stage $stageId (Attempt \" +\n+                s\"$stageAttemptId) within ${timeout}s.\")\n+              cleanupSyncRequests(stageId, stageAttemptId)\n+              // The global sync fails so the stage is expected to retry another attempt, all sync\n+              // messages come from current stage attempt shall fail.\n+              currentBarrierEpoch.set(-1)"
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "kiszk"
    },
    "body": "I think that this code always allocate `ArrayBuffer()` regardless of existence of a value.\r\nIf allocating an array is acceptable, the following code looks simpler.\r\n```\r\nval value = new ArrayBuffer[RpcCallContext](numTasks);\r\nval requests = syncRequestsByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId), value)\r\nif (requests != null) {\r\n  requests\r\n} else {\r\n  value\r\n}\r\n```\r\n\r\nIf allocating an array is not acceptable, we may need to use `computeIfAbsent` like this\r\n```\r\nprivate val allocateArrayBuffer = new JFunction[Int, ArrayBuffer[RpcCallContext]] {\r\n  override def apply(numTasks: Int): ArrayBuffer[RpcCallContext]= new ArrayBuffer[RpcCallContext](numTasks)\r\n}\r\n\r\nprivate def getOrInitSyncRequests(...): ArrayBuffer[RpcCallContext] = {\r\n  syncRequestsByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId), allocateArrayBuffer)\r\n}\r\n```\r\n",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-02T19:38:03Z",
    "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      // Remove internal data from a finished stage attempt.\n+      cleanupSyncRequests(stageInfo.stageId, stageInfo.attemptNumber)\n+      barrierEpochByStageIdAndAttempt.remove((stageInfo.stageId, stageInfo.attemptNumber))\n+      cancelTimerTask(stageInfo.stageId, stageInfo.attemptNumber)\n+    }\n+  }\n+\n+  // Epoch counter for each barrier (stage, attempt).\n+  private val barrierEpochByStageIdAndAttempt = new ConcurrentHashMap[(Int, Int), Int]\n+\n+  // Remember all the blocking global sync requests for each barrier (stage, attempt).\n+  private val syncRequestsByStageIdAndAttempt =\n+    new ConcurrentHashMap[(Int, Int), ArrayBuffer[RpcCallContext]]\n+\n+  // Remember all the TimerTasks for each barrier (stage, attempt).\n+  private val timerTaskByStageIdAndAttempt = new ConcurrentHashMap[(Int, Int), TimerTask]\n+\n+  // Number of tasks for each stage.\n+  private val numTasksByStage = new ConcurrentHashMap[Int, Int]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Get the array of [[RpcCallContext]]s that correspond to a barrier sync request from a stage\n+   * attempt.\n+   */\n+  private def getOrInitSyncRequests(\n+      stageId: Int,\n+      stageAttemptId: Int,\n+      numTasks: Int): ArrayBuffer[RpcCallContext] = {\n+    val requests = syncRequestsByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId),"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "~~~scala\r\nsyncRequestsByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId), new ArrayBuffer[RpcCallContext](numTasks)))\r\nsyncRequestsByStageIdAndAttempt.get((stageId, stageAttemptId))\r\n~~~",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-03T00:32:13Z",
    "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      // Remove internal data from a finished stage attempt.\n+      cleanupSyncRequests(stageInfo.stageId, stageInfo.attemptNumber)\n+      barrierEpochByStageIdAndAttempt.remove((stageInfo.stageId, stageInfo.attemptNumber))\n+      cancelTimerTask(stageInfo.stageId, stageInfo.attemptNumber)\n+    }\n+  }\n+\n+  // Epoch counter for each barrier (stage, attempt).\n+  private val barrierEpochByStageIdAndAttempt = new ConcurrentHashMap[(Int, Int), Int]\n+\n+  // Remember all the blocking global sync requests for each barrier (stage, attempt).\n+  private val syncRequestsByStageIdAndAttempt =\n+    new ConcurrentHashMap[(Int, Int), ArrayBuffer[RpcCallContext]]\n+\n+  // Remember all the TimerTasks for each barrier (stage, attempt).\n+  private val timerTaskByStageIdAndAttempt = new ConcurrentHashMap[(Int, Int), TimerTask]\n+\n+  // Number of tasks for each stage.\n+  private val numTasksByStage = new ConcurrentHashMap[Int, Int]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Get the array of [[RpcCallContext]]s that correspond to a barrier sync request from a stage\n+   * attempt.\n+   */\n+  private def getOrInitSyncRequests(\n+      stageId: Int,\n+      stageAttemptId: Int,\n+      numTasks: Int): ArrayBuffer[RpcCallContext] = {\n+    val requests = syncRequestsByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId),"
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "kiszk"
    },
    "body": "This increment is not atomic. As commented [here](https://github.com/apache/spark/pull/21898#discussion_r206707616), do we need to use `AtomicLong` instead of `int` as `value` of `barrierEpochByStageIdAndAttempt`?",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-02T19:46:46Z",
    "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      // Remove internal data from a finished stage attempt.\n+      cleanupSyncRequests(stageInfo.stageId, stageInfo.attemptNumber)\n+      barrierEpochByStageIdAndAttempt.remove((stageInfo.stageId, stageInfo.attemptNumber))\n+      cancelTimerTask(stageInfo.stageId, stageInfo.attemptNumber)\n+    }\n+  }\n+\n+  // Epoch counter for each barrier (stage, attempt).\n+  private val barrierEpochByStageIdAndAttempt = new ConcurrentHashMap[(Int, Int), Int]\n+\n+  // Remember all the blocking global sync requests for each barrier (stage, attempt).\n+  private val syncRequestsByStageIdAndAttempt =\n+    new ConcurrentHashMap[(Int, Int), ArrayBuffer[RpcCallContext]]\n+\n+  // Remember all the TimerTasks for each barrier (stage, attempt).\n+  private val timerTaskByStageIdAndAttempt = new ConcurrentHashMap[(Int, Int), TimerTask]\n+\n+  // Number of tasks for each stage.\n+  private val numTasksByStage = new ConcurrentHashMap[Int, Int]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Get the array of [[RpcCallContext]]s that correspond to a barrier sync request from a stage\n+   * attempt.\n+   */\n+  private def getOrInitSyncRequests(\n+      stageId: Int,\n+      stageAttemptId: Int,\n+      numTasks: Int): ArrayBuffer[RpcCallContext] = {\n+    val requests = syncRequestsByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId),\n+      new ArrayBuffer[RpcCallContext](numTasks))\n+    if (requests == null) {\n+      syncRequestsByStageIdAndAttempt.get((stageId, stageAttemptId))\n+    } else {\n+      requests\n+    }\n+  }\n+\n+  /**\n+   * Clean up the array of [[RpcCallContext]]s that correspond to a barrier sync request from a\n+   * stage attempt.\n+   */\n+  private def cleanupSyncRequests(stageId: Int, stageAttemptId: Int): Unit = {\n+    val requests = syncRequestsByStageIdAndAttempt.remove((stageId, stageAttemptId))\n+    if (requests != null) {\n+      requests.clear()\n+    }\n+    logInfo(s\"Removed all the pending barrier sync requests from Stage $stageId (Attempt \" +\n+      s\"$stageAttemptId).\")\n+  }\n+\n+  /**\n+   * Get the barrier epoch that correspond to a barrier sync request from a stage attempt.\n+   */\n+  private def getOrInitBarrierEpoch(stageId: Int, stageAttemptId: Int): Int = {\n+    val defaultBarrierEpoch = 0\n+    val barrierEpoch = barrierEpochByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId),\n+      defaultBarrierEpoch)\n+    if (barrierEpoch == null) {\n+      defaultBarrierEpoch\n+    } else {\n+      barrierEpoch\n+    }\n+  }\n+\n+  /**\n+   * Increase the barrier epoch that correspond to a barrier sync request from a stage attempt.\n+   */\n+  private def increaseBarrierEpoch(stageId: Int, stageAttemptId: Int): Unit = {\n+    val barrierEpoch = barrierEpochByStageIdAndAttempt.get((stageId, stageAttemptId))"
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "It doesn't seem safe. getOrDefault is not blocking. You should use `computeIfPresent`.\r\n\r\n",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-03T05:14:04Z",
    "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      // Remove internal data from a finished stage attempt.\n+      cleanupSyncRequests(stageInfo.stageId, stageInfo.attemptNumber)\n+      barrierEpochByStageIdAndAttempt.remove((stageInfo.stageId, stageInfo.attemptNumber))\n+      cancelTimerTask(stageInfo.stageId, stageInfo.attemptNumber)\n+    }\n+  }\n+\n+  // Epoch counter for each barrier (stage, attempt).\n+  private val barrierEpochByStageIdAndAttempt = new ConcurrentHashMap[(Int, Int), Int]\n+\n+  // Remember all the blocking global sync requests for each barrier (stage, attempt).\n+  private val syncRequestsByStageIdAndAttempt =\n+    new ConcurrentHashMap[(Int, Int), ArrayBuffer[RpcCallContext]]\n+\n+  // Remember all the TimerTasks for each barrier (stage, attempt).\n+  private val timerTaskByStageIdAndAttempt = new ConcurrentHashMap[(Int, Int), TimerTask]\n+\n+  // Number of tasks for each stage.\n+  private val numTasksByStage = new ConcurrentHashMap[Int, Int]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Get the array of [[RpcCallContext]]s that correspond to a barrier sync request from a stage\n+   * attempt.\n+   */\n+  private def getOrInitSyncRequests(\n+      stageId: Int,\n+      stageAttemptId: Int,\n+      numTasks: Int): ArrayBuffer[RpcCallContext] = {\n+    syncRequestsByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId),\n+      new ArrayBuffer[RpcCallContext](numTasks))\n+    syncRequestsByStageIdAndAttempt.get((stageId, stageAttemptId))\n+  }\n+\n+  /**\n+   * Clean up the array of [[RpcCallContext]]s that correspond to a barrier sync request from a\n+   * stage attempt.\n+   */\n+  private def cleanupSyncRequests(stageId: Int, stageAttemptId: Int): Unit = {\n+    val requests = syncRequestsByStageIdAndAttempt.remove((stageId, stageAttemptId))\n+    if (requests != null) {\n+      requests.clear()\n+    }\n+    logInfo(s\"Removed all the pending barrier sync requests from Stage $stageId (Attempt \" +\n+      s\"$stageAttemptId).\")\n+  }\n+\n+  /**\n+   * Get the barrier epoch that correspond to a barrier sync request from a stage attempt.\n+   */\n+  private def getOrInitBarrierEpoch(stageId: Int, stageAttemptId: Int): Int = {\n+    barrierEpochByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId), 0)\n+    barrierEpochByStageIdAndAttempt.get((stageId, stageAttemptId))\n+  }\n+\n+  /**\n+   * Increase the barrier epoch that correspond to a barrier sync request from a stage attempt.\n+   */\n+  private def increaseBarrierEpoch(stageId: Int, stageAttemptId: Int): Unit = {\n+    val barrierEpoch = barrierEpochByStageIdAndAttempt.getOrDefault((stageId, stageAttemptId), -1)\n+    if (barrierEpoch >= 0) {"
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Not safe. Use\r\n\r\n~~~scala\r\nval timerTask = timerTaskByStageIdAndAttempt.remove((stageId, stageAttemptId))\r\nif (timerTask != null) {\r\n  timerTask.cancel()\r\n}\r\n~~~",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-03T05:18:19Z",
    "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      // Remove internal data from a finished stage attempt.\n+      cleanupSyncRequests(stageInfo.stageId, stageInfo.attemptNumber)\n+      barrierEpochByStageIdAndAttempt.remove((stageInfo.stageId, stageInfo.attemptNumber))\n+      cancelTimerTask(stageInfo.stageId, stageInfo.attemptNumber)\n+    }\n+  }\n+\n+  // Epoch counter for each barrier (stage, attempt).\n+  private val barrierEpochByStageIdAndAttempt = new ConcurrentHashMap[(Int, Int), Int]\n+\n+  // Remember all the blocking global sync requests for each barrier (stage, attempt).\n+  private val syncRequestsByStageIdAndAttempt =\n+    new ConcurrentHashMap[(Int, Int), ArrayBuffer[RpcCallContext]]\n+\n+  // Remember all the TimerTasks for each barrier (stage, attempt).\n+  private val timerTaskByStageIdAndAttempt = new ConcurrentHashMap[(Int, Int), TimerTask]\n+\n+  // Number of tasks for each stage.\n+  private val numTasksByStage = new ConcurrentHashMap[Int, Int]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Get the array of [[RpcCallContext]]s that correspond to a barrier sync request from a stage\n+   * attempt.\n+   */\n+  private def getOrInitSyncRequests(\n+      stageId: Int,\n+      stageAttemptId: Int,\n+      numTasks: Int): ArrayBuffer[RpcCallContext] = {\n+    syncRequestsByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId),\n+      new ArrayBuffer[RpcCallContext](numTasks))\n+    syncRequestsByStageIdAndAttempt.get((stageId, stageAttemptId))\n+  }\n+\n+  /**\n+   * Clean up the array of [[RpcCallContext]]s that correspond to a barrier sync request from a\n+   * stage attempt.\n+   */\n+  private def cleanupSyncRequests(stageId: Int, stageAttemptId: Int): Unit = {\n+    val requests = syncRequestsByStageIdAndAttempt.remove((stageId, stageAttemptId))\n+    if (requests != null) {\n+      requests.clear()\n+    }\n+    logInfo(s\"Removed all the pending barrier sync requests from Stage $stageId (Attempt \" +\n+      s\"$stageAttemptId).\")\n+  }\n+\n+  /**\n+   * Get the barrier epoch that correspond to a barrier sync request from a stage attempt.\n+   */\n+  private def getOrInitBarrierEpoch(stageId: Int, stageAttemptId: Int): Int = {\n+    barrierEpochByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId), 0)\n+    barrierEpochByStageIdAndAttempt.get((stageId, stageAttemptId))\n+  }\n+\n+  /**\n+   * Increase the barrier epoch that correspond to a barrier sync request from a stage attempt.\n+   */\n+  private def increaseBarrierEpoch(stageId: Int, stageAttemptId: Int): Unit = {\n+    val barrierEpoch = barrierEpochByStageIdAndAttempt.getOrDefault((stageId, stageAttemptId), -1)\n+    if (barrierEpoch >= 0) {\n+      barrierEpochByStageIdAndAttempt.put((stageId, stageAttemptId), barrierEpoch + 1)\n+    } else {\n+      // The stage attempt already finished, don't update barrier epoch.\n+    }\n+  }\n+\n+  /**\n+   * Cancel TimerTask for a stage attempt.\n+   */\n+  private def cancelTimerTask(stageId: Int, stageAttemptId: Int): Unit = {\n+    val timerTask = timerTaskByStageIdAndAttempt.get((stageId, stageAttemptId))\n+    if (timerTask != null) {"
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Overflow, use long. \r\n\r\n~~~scala\r\nscala> 31536000 * 1000\r\nres10: Int = 1471228928\r\n~~~",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-03T05:27:39Z",
    "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      // Remove internal data from a finished stage attempt.\n+      cleanupSyncRequests(stageInfo.stageId, stageInfo.attemptNumber)\n+      barrierEpochByStageIdAndAttempt.remove((stageInfo.stageId, stageInfo.attemptNumber))\n+      cancelTimerTask(stageInfo.stageId, stageInfo.attemptNumber)\n+    }\n+  }\n+\n+  // Epoch counter for each barrier (stage, attempt).\n+  private val barrierEpochByStageIdAndAttempt = new ConcurrentHashMap[(Int, Int), Int]\n+\n+  // Remember all the blocking global sync requests for each barrier (stage, attempt).\n+  private val syncRequestsByStageIdAndAttempt =\n+    new ConcurrentHashMap[(Int, Int), ArrayBuffer[RpcCallContext]]\n+\n+  // Remember all the TimerTasks for each barrier (stage, attempt).\n+  private val timerTaskByStageIdAndAttempt = new ConcurrentHashMap[(Int, Int), TimerTask]\n+\n+  // Number of tasks for each stage.\n+  private val numTasksByStage = new ConcurrentHashMap[Int, Int]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Get the array of [[RpcCallContext]]s that correspond to a barrier sync request from a stage\n+   * attempt.\n+   */\n+  private def getOrInitSyncRequests(\n+      stageId: Int,\n+      stageAttemptId: Int,\n+      numTasks: Int): ArrayBuffer[RpcCallContext] = {\n+    syncRequestsByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId),\n+      new ArrayBuffer[RpcCallContext](numTasks))\n+    syncRequestsByStageIdAndAttempt.get((stageId, stageAttemptId))\n+  }\n+\n+  /**\n+   * Clean up the array of [[RpcCallContext]]s that correspond to a barrier sync request from a\n+   * stage attempt.\n+   */\n+  private def cleanupSyncRequests(stageId: Int, stageAttemptId: Int): Unit = {\n+    val requests = syncRequestsByStageIdAndAttempt.remove((stageId, stageAttemptId))\n+    if (requests != null) {\n+      requests.clear()\n+    }\n+    logInfo(s\"Removed all the pending barrier sync requests from Stage $stageId (Attempt \" +\n+      s\"$stageAttemptId).\")\n+  }\n+\n+  /**\n+   * Get the barrier epoch that correspond to a barrier sync request from a stage attempt.\n+   */\n+  private def getOrInitBarrierEpoch(stageId: Int, stageAttemptId: Int): Int = {\n+    barrierEpochByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId), 0)\n+    barrierEpochByStageIdAndAttempt.get((stageId, stageAttemptId))\n+  }\n+\n+  /**\n+   * Increase the barrier epoch that correspond to a barrier sync request from a stage attempt.\n+   */\n+  private def increaseBarrierEpoch(stageId: Int, stageAttemptId: Int): Unit = {\n+    val barrierEpoch = barrierEpochByStageIdAndAttempt.getOrDefault((stageId, stageAttemptId), -1)\n+    if (barrierEpoch >= 0) {\n+      barrierEpochByStageIdAndAttempt.put((stageId, stageAttemptId), barrierEpoch + 1)\n+    } else {\n+      // The stage attempt already finished, don't update barrier epoch.\n+    }\n+  }\n+\n+  /**\n+   * Cancel TimerTask for a stage attempt.\n+   */\n+  private def cancelTimerTask(stageId: Int, stageAttemptId: Int): Unit = {\n+    val timerTask = timerTaskByStageIdAndAttempt.get((stageId, stageAttemptId))\n+    if (timerTask != null) {\n+      timerTask.cancel()\n+      timerTaskByStageIdAndAttempt.remove((stageId, stageAttemptId))\n+    }\n+  }\n+\n+  /**\n+   * Send failure to all the blocking barrier sync requests from a stage attempt with proper\n+   * failure message.\n+   */\n+  private def failAllSyncRequests(\n+      syncRequests: ArrayBuffer[RpcCallContext],\n+      message: String): Unit = {\n+    syncRequests.foreach(_.sendFailure(new SparkException(message)))\n+  }\n+\n+  /**\n+   * Finish all the blocking barrier sync requests from a stage attempt successfully if we\n+   * have received all the sync requests.\n+   */\n+  private def maybeFinishAllSyncRequests(\n+      syncRequests: ArrayBuffer[RpcCallContext],\n+      numTasks: Int): Boolean = {\n+    if (syncRequests.size == numTasks) {\n+      syncRequests.foreach(_.reply(()))\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+\n+  override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = {\n+    case RequestToSync(numTasks, stageId, stageAttemptId, taskAttemptId, barrierEpoch) =>\n+      // Require the number of tasks is correctly set from the BarrierTaskContext.\n+      val currentNumTasks: Any = numTasksByStage.putIfAbsent(stageId, numTasks)\n+      require(currentNumTasks == null || currentNumTasks == numTasks, \"Number of tasks of \" +\n+        s\"Stage $stageId is $numTasks from Task $taskAttemptId, previously it was \" +\n+        s\"$currentNumTasks.\")\n+\n+      // Check the barrier epoch, to see which barrier() call we are processing.\n+      val currentBarrierEpoch = getOrInitBarrierEpoch(stageId, stageAttemptId)\n+      logInfo(s\"Current barrier epoch for Stage $stageId (Attempt $stageAttemptId) is\" +\n+        s\"$currentBarrierEpoch.\")\n+      if (currentBarrierEpoch != barrierEpoch) {\n+        context.sendFailure(new SparkException(s\"The request to sync of Stage $stageId (Attempt \" +\n+          s\"$stageAttemptId) with barrier epoch $barrierEpoch has already finished. Maybe task \" +\n+          s\"$taskAttemptId is not properly killed.\"))\n+      } else {\n+        val syncRequests = getOrInitSyncRequests(stageId, stageAttemptId, numTasks)\n+        // If this is the first sync message received for a barrier() call, init a timer to ensure\n+        // we may timeout for the sync.\n+        if (syncRequests.isEmpty) {\n+          val timerTask = new TimerTask {\n+            override def run(): Unit = {\n+              // Timeout for current barrier() call, fail all the sync requests.\n+              val requests = getOrInitSyncRequests(stageId, stageAttemptId, numTasks)\n+              failAllSyncRequests(requests, \"The coordinator didn't get all barrier sync \" +\n+                s\"requests for barrier epoch $barrierEpoch from Stage $stageId (Attempt \" +\n+                s\"$stageAttemptId) within ${timeout}s.\")\n+              cleanupSyncRequests(stageId, stageAttemptId)\n+              // The global sync fails so the stage is expected to retry another attempt, all sync\n+              // messages come from current stage attempt shall fail.\n+              barrierEpochByStageIdAndAttempt.put((stageId, stageAttemptId), -1)\n+            }\n+          }\n+          timer.schedule(timerTask, timeout * 1000)"
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "We should do this line before `failAllSyncRequests` and `cleanupSyncRequests`.",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-03T05:28:08Z",
    "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      // Remove internal data from a finished stage attempt.\n+      cleanupSyncRequests(stageInfo.stageId, stageInfo.attemptNumber)\n+      barrierEpochByStageIdAndAttempt.remove((stageInfo.stageId, stageInfo.attemptNumber))\n+      cancelTimerTask(stageInfo.stageId, stageInfo.attemptNumber)\n+    }\n+  }\n+\n+  // Epoch counter for each barrier (stage, attempt).\n+  private val barrierEpochByStageIdAndAttempt = new ConcurrentHashMap[(Int, Int), Int]\n+\n+  // Remember all the blocking global sync requests for each barrier (stage, attempt).\n+  private val syncRequestsByStageIdAndAttempt =\n+    new ConcurrentHashMap[(Int, Int), ArrayBuffer[RpcCallContext]]\n+\n+  // Remember all the TimerTasks for each barrier (stage, attempt).\n+  private val timerTaskByStageIdAndAttempt = new ConcurrentHashMap[(Int, Int), TimerTask]\n+\n+  // Number of tasks for each stage.\n+  private val numTasksByStage = new ConcurrentHashMap[Int, Int]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Get the array of [[RpcCallContext]]s that correspond to a barrier sync request from a stage\n+   * attempt.\n+   */\n+  private def getOrInitSyncRequests(\n+      stageId: Int,\n+      stageAttemptId: Int,\n+      numTasks: Int): ArrayBuffer[RpcCallContext] = {\n+    syncRequestsByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId),\n+      new ArrayBuffer[RpcCallContext](numTasks))\n+    syncRequestsByStageIdAndAttempt.get((stageId, stageAttemptId))\n+  }\n+\n+  /**\n+   * Clean up the array of [[RpcCallContext]]s that correspond to a barrier sync request from a\n+   * stage attempt.\n+   */\n+  private def cleanupSyncRequests(stageId: Int, stageAttemptId: Int): Unit = {\n+    val requests = syncRequestsByStageIdAndAttempt.remove((stageId, stageAttemptId))\n+    if (requests != null) {\n+      requests.clear()\n+    }\n+    logInfo(s\"Removed all the pending barrier sync requests from Stage $stageId (Attempt \" +\n+      s\"$stageAttemptId).\")\n+  }\n+\n+  /**\n+   * Get the barrier epoch that correspond to a barrier sync request from a stage attempt.\n+   */\n+  private def getOrInitBarrierEpoch(stageId: Int, stageAttemptId: Int): Int = {\n+    barrierEpochByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId), 0)\n+    barrierEpochByStageIdAndAttempt.get((stageId, stageAttemptId))\n+  }\n+\n+  /**\n+   * Increase the barrier epoch that correspond to a barrier sync request from a stage attempt.\n+   */\n+  private def increaseBarrierEpoch(stageId: Int, stageAttemptId: Int): Unit = {\n+    val barrierEpoch = barrierEpochByStageIdAndAttempt.getOrDefault((stageId, stageAttemptId), -1)\n+    if (barrierEpoch >= 0) {\n+      barrierEpochByStageIdAndAttempt.put((stageId, stageAttemptId), barrierEpoch + 1)\n+    } else {\n+      // The stage attempt already finished, don't update barrier epoch.\n+    }\n+  }\n+\n+  /**\n+   * Cancel TimerTask for a stage attempt.\n+   */\n+  private def cancelTimerTask(stageId: Int, stageAttemptId: Int): Unit = {\n+    val timerTask = timerTaskByStageIdAndAttempt.get((stageId, stageAttemptId))\n+    if (timerTask != null) {\n+      timerTask.cancel()\n+      timerTaskByStageIdAndAttempt.remove((stageId, stageAttemptId))\n+    }\n+  }\n+\n+  /**\n+   * Send failure to all the blocking barrier sync requests from a stage attempt with proper\n+   * failure message.\n+   */\n+  private def failAllSyncRequests(\n+      syncRequests: ArrayBuffer[RpcCallContext],\n+      message: String): Unit = {\n+    syncRequests.foreach(_.sendFailure(new SparkException(message)))\n+  }\n+\n+  /**\n+   * Finish all the blocking barrier sync requests from a stage attempt successfully if we\n+   * have received all the sync requests.\n+   */\n+  private def maybeFinishAllSyncRequests(\n+      syncRequests: ArrayBuffer[RpcCallContext],\n+      numTasks: Int): Boolean = {\n+    if (syncRequests.size == numTasks) {\n+      syncRequests.foreach(_.reply(()))\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+\n+  override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = {\n+    case RequestToSync(numTasks, stageId, stageAttemptId, taskAttemptId, barrierEpoch) =>\n+      // Require the number of tasks is correctly set from the BarrierTaskContext.\n+      val currentNumTasks: Any = numTasksByStage.putIfAbsent(stageId, numTasks)\n+      require(currentNumTasks == null || currentNumTasks == numTasks, \"Number of tasks of \" +\n+        s\"Stage $stageId is $numTasks from Task $taskAttemptId, previously it was \" +\n+        s\"$currentNumTasks.\")\n+\n+      // Check the barrier epoch, to see which barrier() call we are processing.\n+      val currentBarrierEpoch = getOrInitBarrierEpoch(stageId, stageAttemptId)\n+      logInfo(s\"Current barrier epoch for Stage $stageId (Attempt $stageAttemptId) is\" +\n+        s\"$currentBarrierEpoch.\")\n+      if (currentBarrierEpoch != barrierEpoch) {\n+        context.sendFailure(new SparkException(s\"The request to sync of Stage $stageId (Attempt \" +\n+          s\"$stageAttemptId) with barrier epoch $barrierEpoch has already finished. Maybe task \" +\n+          s\"$taskAttemptId is not properly killed.\"))\n+      } else {\n+        val syncRequests = getOrInitSyncRequests(stageId, stageAttemptId, numTasks)\n+        // If this is the first sync message received for a barrier() call, init a timer to ensure\n+        // we may timeout for the sync.\n+        if (syncRequests.isEmpty) {\n+          val timerTask = new TimerTask {\n+            override def run(): Unit = {\n+              // Timeout for current barrier() call, fail all the sync requests.\n+              val requests = getOrInitSyncRequests(stageId, stageAttemptId, numTasks)\n+              failAllSyncRequests(requests, \"The coordinator didn't get all barrier sync \" +\n+                s\"requests for barrier epoch $barrierEpoch from Stage $stageId (Attempt \" +\n+                s\"$stageAttemptId) within ${timeout}s.\")\n+              cleanupSyncRequests(stageId, stageAttemptId)\n+              // The global sync fails so the stage is expected to retry another attempt, all sync\n+              // messages come from current stage attempt shall fail.\n+              barrierEpochByStageIdAndAttempt.put((stageId, stageAttemptId), -1)"
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Is it thread safe?",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-03T05:31:31Z",
    "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      // Remove internal data from a finished stage attempt.\n+      cleanupSyncRequests(stageInfo.stageId, stageInfo.attemptNumber)\n+      barrierEpochByStageIdAndAttempt.remove((stageInfo.stageId, stageInfo.attemptNumber))\n+      cancelTimerTask(stageInfo.stageId, stageInfo.attemptNumber)\n+    }\n+  }\n+\n+  // Epoch counter for each barrier (stage, attempt).\n+  private val barrierEpochByStageIdAndAttempt = new ConcurrentHashMap[(Int, Int), Int]\n+\n+  // Remember all the blocking global sync requests for each barrier (stage, attempt).\n+  private val syncRequestsByStageIdAndAttempt =\n+    new ConcurrentHashMap[(Int, Int), ArrayBuffer[RpcCallContext]]\n+\n+  // Remember all the TimerTasks for each barrier (stage, attempt).\n+  private val timerTaskByStageIdAndAttempt = new ConcurrentHashMap[(Int, Int), TimerTask]\n+\n+  // Number of tasks for each stage.\n+  private val numTasksByStage = new ConcurrentHashMap[Int, Int]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Get the array of [[RpcCallContext]]s that correspond to a barrier sync request from a stage\n+   * attempt.\n+   */\n+  private def getOrInitSyncRequests(\n+      stageId: Int,\n+      stageAttemptId: Int,\n+      numTasks: Int): ArrayBuffer[RpcCallContext] = {\n+    syncRequestsByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId),\n+      new ArrayBuffer[RpcCallContext](numTasks))\n+    syncRequestsByStageIdAndAttempt.get((stageId, stageAttemptId))\n+  }\n+\n+  /**\n+   * Clean up the array of [[RpcCallContext]]s that correspond to a barrier sync request from a\n+   * stage attempt.\n+   */\n+  private def cleanupSyncRequests(stageId: Int, stageAttemptId: Int): Unit = {\n+    val requests = syncRequestsByStageIdAndAttempt.remove((stageId, stageAttemptId))\n+    if (requests != null) {\n+      requests.clear()\n+    }\n+    logInfo(s\"Removed all the pending barrier sync requests from Stage $stageId (Attempt \" +\n+      s\"$stageAttemptId).\")\n+  }\n+\n+  /**\n+   * Get the barrier epoch that correspond to a barrier sync request from a stage attempt.\n+   */\n+  private def getOrInitBarrierEpoch(stageId: Int, stageAttemptId: Int): Int = {\n+    barrierEpochByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId), 0)\n+    barrierEpochByStageIdAndAttempt.get((stageId, stageAttemptId))\n+  }\n+\n+  /**\n+   * Increase the barrier epoch that correspond to a barrier sync request from a stage attempt.\n+   */\n+  private def increaseBarrierEpoch(stageId: Int, stageAttemptId: Int): Unit = {\n+    val barrierEpoch = barrierEpochByStageIdAndAttempt.getOrDefault((stageId, stageAttemptId), -1)\n+    if (barrierEpoch >= 0) {\n+      barrierEpochByStageIdAndAttempt.put((stageId, stageAttemptId), barrierEpoch + 1)\n+    } else {\n+      // The stage attempt already finished, don't update barrier epoch.\n+    }\n+  }\n+\n+  /**\n+   * Cancel TimerTask for a stage attempt.\n+   */\n+  private def cancelTimerTask(stageId: Int, stageAttemptId: Int): Unit = {\n+    val timerTask = timerTaskByStageIdAndAttempt.get((stageId, stageAttemptId))\n+    if (timerTask != null) {\n+      timerTask.cancel()\n+      timerTaskByStageIdAndAttempt.remove((stageId, stageAttemptId))\n+    }\n+  }\n+\n+  /**\n+   * Send failure to all the blocking barrier sync requests from a stage attempt with proper\n+   * failure message.\n+   */\n+  private def failAllSyncRequests(\n+      syncRequests: ArrayBuffer[RpcCallContext],\n+      message: String): Unit = {\n+    syncRequests.foreach(_.sendFailure(new SparkException(message)))\n+  }\n+\n+  /**\n+   * Finish all the blocking barrier sync requests from a stage attempt successfully if we\n+   * have received all the sync requests.\n+   */\n+  private def maybeFinishAllSyncRequests(\n+      syncRequests: ArrayBuffer[RpcCallContext],\n+      numTasks: Int): Boolean = {\n+    if (syncRequests.size == numTasks) {\n+      syncRequests.foreach(_.reply(()))\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+\n+  override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = {\n+    case RequestToSync(numTasks, stageId, stageAttemptId, taskAttemptId, barrierEpoch) =>\n+      // Require the number of tasks is correctly set from the BarrierTaskContext.\n+      val currentNumTasks: Any = numTasksByStage.putIfAbsent(stageId, numTasks)\n+      require(currentNumTasks == null || currentNumTasks == numTasks, \"Number of tasks of \" +\n+        s\"Stage $stageId is $numTasks from Task $taskAttemptId, previously it was \" +\n+        s\"$currentNumTasks.\")\n+\n+      // Check the barrier epoch, to see which barrier() call we are processing.\n+      val currentBarrierEpoch = getOrInitBarrierEpoch(stageId, stageAttemptId)\n+      logInfo(s\"Current barrier epoch for Stage $stageId (Attempt $stageAttemptId) is\" +\n+        s\"$currentBarrierEpoch.\")\n+      if (currentBarrierEpoch != barrierEpoch) {\n+        context.sendFailure(new SparkException(s\"The request to sync of Stage $stageId (Attempt \" +\n+          s\"$stageAttemptId) with barrier epoch $barrierEpoch has already finished. Maybe task \" +\n+          s\"$taskAttemptId is not properly killed.\"))\n+      } else {\n+        val syncRequests = getOrInitSyncRequests(stageId, stageAttemptId, numTasks)\n+        // If this is the first sync message received for a barrier() call, init a timer to ensure\n+        // we may timeout for the sync.\n+        if (syncRequests.isEmpty) {\n+          val timerTask = new TimerTask {\n+            override def run(): Unit = {\n+              // Timeout for current barrier() call, fail all the sync requests.\n+              val requests = getOrInitSyncRequests(stageId, stageAttemptId, numTasks)\n+              failAllSyncRequests(requests, \"The coordinator didn't get all barrier sync \" +\n+                s\"requests for barrier epoch $barrierEpoch from Stage $stageId (Attempt \" +\n+                s\"$stageAttemptId) within ${timeout}s.\")\n+              cleanupSyncRequests(stageId, stageAttemptId)\n+              // The global sync fails so the stage is expected to retry another attempt, all sync\n+              // messages come from current stage attempt shall fail.\n+              barrierEpochByStageIdAndAttempt.put((stageId, stageAttemptId), -1)\n+            }\n+          }\n+          timer.schedule(timerTask, timeout * 1000)\n+          timerTaskByStageIdAndAttempt.putIfAbsent((stageId, stageAttemptId), timerTask)\n+        }\n+\n+        syncRequests += context"
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Should we assign `timerTask` directly inside this method instead of returning a new instance?",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-03T15:52:23Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.function.Consumer\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * Only one barrier() call shall happen on a barrier stage attempt at each time, we can use\n+ * (stageId, stageAttemptId) to identify the stage attempt where the barrier() call is from.\n+ */\n+private case class ContextBarrierId(stageId: Int, stageAttemptId: Int) {\n+  override def toString: String = s\"Stage $stageId (Attempt $stageAttemptId)\"\n+}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  // Listen to StageCompleted event, clear corresponding ContextBarrierState.\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      val barrierId = ContextBarrierId(stageInfo.stageId, stageInfo.attemptNumber)\n+      // Clear ContextBarrierState from a finished stage attempt.\n+      val barrierState = states.remove(barrierId)\n+      if (barrierState != null) {\n+        barrierState.clear()\n+      }\n+    }\n+  }\n+\n+  // Remember all active stage attempts that make barrier() call(s), and the corresponding\n+  // internal state.\n+  private val states = new ConcurrentHashMap[ContextBarrierId, ContextBarrierState]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Provide current state of a barrier() call, the state is created when a new stage attempt send\n+   * out a barrier() call, and recycled on stage completed.\n+   *\n+   * @param barrierId Identifier of the barrier stage that make a barrier() call.\n+   * @param numTasks Number of tasks of the barrier stage, all barrier() calls from the stage shall\n+   *                 collect `numTasks` requests to succeed.\n+   */\n+  private class ContextBarrierState(\n+      val barrierId: ContextBarrierId,\n+      val numTasks: Int) {\n+\n+    // There may be multiple barrier() calls from a barrier stage attempt, `barrierEpoch` is used\n+    // to identify each barrier() call. It shall get increased when a barrier() call succeed, or\n+    // reset when a barrier() call fail due to timeout.\n+    private var barrierEpoch: Int = 0\n+\n+    // An array of RPCCallContexts for barrier tasks that are waiting for reply of a barrier()\n+    // call.\n+    private val requesters: ArrayBuffer[RpcCallContext] = new ArrayBuffer[RpcCallContext](numTasks)\n+\n+    // A timer task that ensures we may timeout for a barrier() call.\n+    private var timerTask: TimerTask = null\n+\n+    // Init a TimerTask for a barrier() call.\n+    private def initTimerTask(): TimerTask = new TimerTask {"
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "minor: this could go inside ContextBarrierStage",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-03T15:54:16Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.function.Consumer\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * Only one barrier() call shall happen on a barrier stage attempt at each time, we can use\n+ * (stageId, stageAttemptId) to identify the stage attempt where the barrier() call is from.\n+ */\n+private case class ContextBarrierId(stageId: Int, stageAttemptId: Int) {\n+  override def toString: String = s\"Stage $stageId (Attempt $stageAttemptId)\"\n+}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  // Listen to StageCompleted event, clear corresponding ContextBarrierState.\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      val barrierId = ContextBarrierId(stageInfo.stageId, stageInfo.attemptNumber)\n+      // Clear ContextBarrierState from a finished stage attempt.\n+      val barrierState = states.remove(barrierId)\n+      if (barrierState != null) {\n+        barrierState.clear()\n+      }\n+    }\n+  }\n+\n+  // Remember all active stage attempts that make barrier() call(s), and the corresponding\n+  // internal state.\n+  private val states = new ConcurrentHashMap[ContextBarrierId, ContextBarrierState]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Provide current state of a barrier() call, the state is created when a new stage attempt send\n+   * out a barrier() call, and recycled on stage completed.\n+   *\n+   * @param barrierId Identifier of the barrier stage that make a barrier() call.\n+   * @param numTasks Number of tasks of the barrier stage, all barrier() calls from the stage shall\n+   *                 collect `numTasks` requests to succeed.\n+   */\n+  private class ContextBarrierState(\n+      val barrierId: ContextBarrierId,\n+      val numTasks: Int) {\n+\n+    // There may be multiple barrier() calls from a barrier stage attempt, `barrierEpoch` is used\n+    // to identify each barrier() call. It shall get increased when a barrier() call succeed, or\n+    // reset when a barrier() call fail due to timeout.\n+    private var barrierEpoch: Int = 0\n+\n+    // An array of RPCCallContexts for barrier tasks that are waiting for reply of a barrier()\n+    // call.\n+    private val requesters: ArrayBuffer[RpcCallContext] = new ArrayBuffer[RpcCallContext](numTasks)\n+\n+    // A timer task that ensures we may timeout for a barrier() call.\n+    private var timerTask: TimerTask = null\n+\n+    // Init a TimerTask for a barrier() call.\n+    private def initTimerTask(): TimerTask = new TimerTask {\n+      override def run(): Unit = {\n+        // Timeout current barrier() call, fail all the sync requests.\n+        failAllRequesters(requesters, \"The coordinator didn't get all barrier sync \" +\n+          s\"requests for barrier epoch $barrierEpoch from $barrierId within ${timeout}s.\")\n+        cleanupBarrierStage(barrierId)\n+      }\n+    }\n+\n+    // Cancel the current active TimerTask and release resources.\n+    private def cancelTimerTask(): Unit = {\n+      if (timerTask != null) {\n+        timerTask.cancel()\n+        timerTask = null\n+      }\n+    }\n+\n+    // Process the global sync request. The barrier() call succeed if collected enough requests\n+    // within a configured time, otherwise fail all the pending requests.\n+    def handleRequest(requester: RpcCallContext, epoch: Int, taskId: Long): Unit = synchronized {\n+      // Check whether the epoch from the barrier tasks matches current barrierEpoch.\n+      logInfo(s\"Current barrier epoch for $barrierId is $barrierEpoch.\")\n+      if (epoch != barrierEpoch) {\n+        requester.sendFailure(new SparkException(s\"The request to sync of $barrierId with \" +\n+          s\"barrier epoch $barrierEpoch has already finished. Maybe task $taskId is not \" +\n+          \"properly killed.\"))\n+      } else {\n+        // If this is the first sync message received for a barrier() call, start timer to ensure\n+        // we may timeout for the sync.\n+        if (requesters.isEmpty) {\n+          timerTask = initTimerTask()\n+          timer.schedule(timerTask, timeout * 1000L)\n+        }\n+        // Add the requester to array of RPCCallContexts pending for reply.\n+        requesters += requester\n+        logInfo(s\"Barrier sync epoch $barrierEpoch from $barrierId received update from Task \" +\n+          s\"$taskId, current progress: ${requesters.size}/$numTasks.\")\n+        if (maybeFinishAllRequesters(requesters, numTasks)) {\n+          // Finished current barrier() call successfully, clean up ContextBarrierState and\n+          // increase the barrier epoch.\n+          logInfo(s\"Barrier sync epoch $barrierEpoch from $barrierId received all updates from \" +\n+            s\"tasks, finished successfully.\")\n+          barrierEpoch += 1\n+          requesters.clear()\n+          cancelTimerTask()\n+        }\n+      }\n+    }\n+\n+    // Cleanup the internal state of a barrier stage attempt.\n+    def clear(): Unit = synchronized {\n+      // The global sync fails so the stage is expected to retry another attempt, all sync\n+      // messages come from current stage attempt shall fail.\n+      barrierEpoch = -1\n+      requesters.clear()\n+      cancelTimerTask()\n+    }\n+  }\n+\n+  /**\n+   * Clean up the [[ContextBarrierState]] that correspond to a stage attempt.\n+   */\n+  private def cleanupBarrierStage(barrierId: ContextBarrierId): Unit = {\n+    val barrierState = states.remove(barrierId)\n+    if (barrierState != null) {\n+      barrierState.clear()\n+    }\n+  }\n+\n+  /**\n+   * Send failure to all the blocking barrier sync requests from a stage attempt with proper\n+   * failure message.\n+   */\n+  private def failAllRequesters("
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Ditto.",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-03T15:54:27Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.function.Consumer\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * Only one barrier() call shall happen on a barrier stage attempt at each time, we can use\n+ * (stageId, stageAttemptId) to identify the stage attempt where the barrier() call is from.\n+ */\n+private case class ContextBarrierId(stageId: Int, stageAttemptId: Int) {\n+  override def toString: String = s\"Stage $stageId (Attempt $stageAttemptId)\"\n+}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  // Listen to StageCompleted event, clear corresponding ContextBarrierState.\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      val barrierId = ContextBarrierId(stageInfo.stageId, stageInfo.attemptNumber)\n+      // Clear ContextBarrierState from a finished stage attempt.\n+      val barrierState = states.remove(barrierId)\n+      if (barrierState != null) {\n+        barrierState.clear()\n+      }\n+    }\n+  }\n+\n+  // Remember all active stage attempts that make barrier() call(s), and the corresponding\n+  // internal state.\n+  private val states = new ConcurrentHashMap[ContextBarrierId, ContextBarrierState]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Provide current state of a barrier() call, the state is created when a new stage attempt send\n+   * out a barrier() call, and recycled on stage completed.\n+   *\n+   * @param barrierId Identifier of the barrier stage that make a barrier() call.\n+   * @param numTasks Number of tasks of the barrier stage, all barrier() calls from the stage shall\n+   *                 collect `numTasks` requests to succeed.\n+   */\n+  private class ContextBarrierState(\n+      val barrierId: ContextBarrierId,\n+      val numTasks: Int) {\n+\n+    // There may be multiple barrier() calls from a barrier stage attempt, `barrierEpoch` is used\n+    // to identify each barrier() call. It shall get increased when a barrier() call succeed, or\n+    // reset when a barrier() call fail due to timeout.\n+    private var barrierEpoch: Int = 0\n+\n+    // An array of RPCCallContexts for barrier tasks that are waiting for reply of a barrier()\n+    // call.\n+    private val requesters: ArrayBuffer[RpcCallContext] = new ArrayBuffer[RpcCallContext](numTasks)\n+\n+    // A timer task that ensures we may timeout for a barrier() call.\n+    private var timerTask: TimerTask = null\n+\n+    // Init a TimerTask for a barrier() call.\n+    private def initTimerTask(): TimerTask = new TimerTask {\n+      override def run(): Unit = {\n+        // Timeout current barrier() call, fail all the sync requests.\n+        failAllRequesters(requesters, \"The coordinator didn't get all barrier sync \" +\n+          s\"requests for barrier epoch $barrierEpoch from $barrierId within ${timeout}s.\")\n+        cleanupBarrierStage(barrierId)\n+      }\n+    }\n+\n+    // Cancel the current active TimerTask and release resources.\n+    private def cancelTimerTask(): Unit = {\n+      if (timerTask != null) {\n+        timerTask.cancel()\n+        timerTask = null\n+      }\n+    }\n+\n+    // Process the global sync request. The barrier() call succeed if collected enough requests\n+    // within a configured time, otherwise fail all the pending requests.\n+    def handleRequest(requester: RpcCallContext, epoch: Int, taskId: Long): Unit = synchronized {\n+      // Check whether the epoch from the barrier tasks matches current barrierEpoch.\n+      logInfo(s\"Current barrier epoch for $barrierId is $barrierEpoch.\")\n+      if (epoch != barrierEpoch) {\n+        requester.sendFailure(new SparkException(s\"The request to sync of $barrierId with \" +\n+          s\"barrier epoch $barrierEpoch has already finished. Maybe task $taskId is not \" +\n+          \"properly killed.\"))\n+      } else {\n+        // If this is the first sync message received for a barrier() call, start timer to ensure\n+        // we may timeout for the sync.\n+        if (requesters.isEmpty) {\n+          timerTask = initTimerTask()\n+          timer.schedule(timerTask, timeout * 1000L)\n+        }\n+        // Add the requester to array of RPCCallContexts pending for reply.\n+        requesters += requester\n+        logInfo(s\"Barrier sync epoch $barrierEpoch from $barrierId received update from Task \" +\n+          s\"$taskId, current progress: ${requesters.size}/$numTasks.\")\n+        if (maybeFinishAllRequesters(requesters, numTasks)) {\n+          // Finished current barrier() call successfully, clean up ContextBarrierState and\n+          // increase the barrier epoch.\n+          logInfo(s\"Barrier sync epoch $barrierEpoch from $barrierId received all updates from \" +\n+            s\"tasks, finished successfully.\")\n+          barrierEpoch += 1\n+          requesters.clear()\n+          cancelTimerTask()\n+        }\n+      }\n+    }\n+\n+    // Cleanup the internal state of a barrier stage attempt.\n+    def clear(): Unit = synchronized {\n+      // The global sync fails so the stage is expected to retry another attempt, all sync\n+      // messages come from current stage attempt shall fail.\n+      barrierEpoch = -1\n+      requesters.clear()\n+      cancelTimerTask()\n+    }\n+  }\n+\n+  /**\n+   * Clean up the [[ContextBarrierState]] that correspond to a stage attempt.\n+   */\n+  private def cleanupBarrierStage(barrierId: ContextBarrierId): Unit = {\n+    val barrierState = states.remove(barrierId)\n+    if (barrierState != null) {\n+      barrierState.clear()\n+    }\n+  }\n+\n+  /**\n+   * Send failure to all the blocking barrier sync requests from a stage attempt with proper\n+   * failure message.\n+   */\n+  private def failAllRequesters(\n+      requesters: ArrayBuffer[RpcCallContext],\n+      message: String): Unit = {\n+    requesters.foreach(_.sendFailure(new SparkException(message)))\n+  }\n+\n+  /**\n+   * Finish all the blocking barrier sync requests from a stage attempt successfully if we\n+   * have received all the sync requests.\n+   */\n+  private def maybeFinishAllRequesters("
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "This can go into ContextBarrierState too. Basically it maintains valid state by itself. You might pass both `context` and the entire `RequestToSync` request to `handleRequest`.",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-03T15:56:09Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.function.Consumer\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * Only one barrier() call shall happen on a barrier stage attempt at each time, we can use\n+ * (stageId, stageAttemptId) to identify the stage attempt where the barrier() call is from.\n+ */\n+private case class ContextBarrierId(stageId: Int, stageAttemptId: Int) {\n+  override def toString: String = s\"Stage $stageId (Attempt $stageAttemptId)\"\n+}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  // Listen to StageCompleted event, clear corresponding ContextBarrierState.\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      val barrierId = ContextBarrierId(stageInfo.stageId, stageInfo.attemptNumber)\n+      // Clear ContextBarrierState from a finished stage attempt.\n+      val barrierState = states.remove(barrierId)\n+      if (barrierState != null) {\n+        barrierState.clear()\n+      }\n+    }\n+  }\n+\n+  // Remember all active stage attempts that make barrier() call(s), and the corresponding\n+  // internal state.\n+  private val states = new ConcurrentHashMap[ContextBarrierId, ContextBarrierState]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Provide current state of a barrier() call, the state is created when a new stage attempt send\n+   * out a barrier() call, and recycled on stage completed.\n+   *\n+   * @param barrierId Identifier of the barrier stage that make a barrier() call.\n+   * @param numTasks Number of tasks of the barrier stage, all barrier() calls from the stage shall\n+   *                 collect `numTasks` requests to succeed.\n+   */\n+  private class ContextBarrierState(\n+      val barrierId: ContextBarrierId,\n+      val numTasks: Int) {\n+\n+    // There may be multiple barrier() calls from a barrier stage attempt, `barrierEpoch` is used\n+    // to identify each barrier() call. It shall get increased when a barrier() call succeed, or\n+    // reset when a barrier() call fail due to timeout.\n+    private var barrierEpoch: Int = 0\n+\n+    // An array of RPCCallContexts for barrier tasks that are waiting for reply of a barrier()\n+    // call.\n+    private val requesters: ArrayBuffer[RpcCallContext] = new ArrayBuffer[RpcCallContext](numTasks)\n+\n+    // A timer task that ensures we may timeout for a barrier() call.\n+    private var timerTask: TimerTask = null\n+\n+    // Init a TimerTask for a barrier() call.\n+    private def initTimerTask(): TimerTask = new TimerTask {\n+      override def run(): Unit = {\n+        // Timeout current barrier() call, fail all the sync requests.\n+        failAllRequesters(requesters, \"The coordinator didn't get all barrier sync \" +\n+          s\"requests for barrier epoch $barrierEpoch from $barrierId within ${timeout}s.\")\n+        cleanupBarrierStage(barrierId)\n+      }\n+    }\n+\n+    // Cancel the current active TimerTask and release resources.\n+    private def cancelTimerTask(): Unit = {\n+      if (timerTask != null) {\n+        timerTask.cancel()\n+        timerTask = null\n+      }\n+    }\n+\n+    // Process the global sync request. The barrier() call succeed if collected enough requests\n+    // within a configured time, otherwise fail all the pending requests.\n+    def handleRequest(requester: RpcCallContext, epoch: Int, taskId: Long): Unit = synchronized {\n+      // Check whether the epoch from the barrier tasks matches current barrierEpoch.\n+      logInfo(s\"Current barrier epoch for $barrierId is $barrierEpoch.\")\n+      if (epoch != barrierEpoch) {\n+        requester.sendFailure(new SparkException(s\"The request to sync of $barrierId with \" +\n+          s\"barrier epoch $barrierEpoch has already finished. Maybe task $taskId is not \" +\n+          \"properly killed.\"))\n+      } else {\n+        // If this is the first sync message received for a barrier() call, start timer to ensure\n+        // we may timeout for the sync.\n+        if (requesters.isEmpty) {\n+          timerTask = initTimerTask()\n+          timer.schedule(timerTask, timeout * 1000L)\n+        }\n+        // Add the requester to array of RPCCallContexts pending for reply.\n+        requesters += requester\n+        logInfo(s\"Barrier sync epoch $barrierEpoch from $barrierId received update from Task \" +\n+          s\"$taskId, current progress: ${requesters.size}/$numTasks.\")\n+        if (maybeFinishAllRequesters(requesters, numTasks)) {\n+          // Finished current barrier() call successfully, clean up ContextBarrierState and\n+          // increase the barrier epoch.\n+          logInfo(s\"Barrier sync epoch $barrierEpoch from $barrierId received all updates from \" +\n+            s\"tasks, finished successfully.\")\n+          barrierEpoch += 1\n+          requesters.clear()\n+          cancelTimerTask()\n+        }\n+      }\n+    }\n+\n+    // Cleanup the internal state of a barrier stage attempt.\n+    def clear(): Unit = synchronized {\n+      // The global sync fails so the stage is expected to retry another attempt, all sync\n+      // messages come from current stage attempt shall fail.\n+      barrierEpoch = -1\n+      requesters.clear()\n+      cancelTimerTask()\n+    }\n+  }\n+\n+  /**\n+   * Clean up the [[ContextBarrierState]] that correspond to a stage attempt.\n+   */\n+  private def cleanupBarrierStage(barrierId: ContextBarrierId): Unit = {\n+    val barrierState = states.remove(barrierId)\n+    if (barrierState != null) {\n+      barrierState.clear()\n+    }\n+  }\n+\n+  /**\n+   * Send failure to all the blocking barrier sync requests from a stage attempt with proper\n+   * failure message.\n+   */\n+  private def failAllRequesters(\n+      requesters: ArrayBuffer[RpcCallContext],\n+      message: String): Unit = {\n+    requesters.foreach(_.sendFailure(new SparkException(message)))\n+  }\n+\n+  /**\n+   * Finish all the blocking barrier sync requests from a stage attempt successfully if we\n+   * have received all the sync requests.\n+   */\n+  private def maybeFinishAllRequesters(\n+      requesters: ArrayBuffer[RpcCallContext],\n+      numTasks: Int): Boolean = {\n+    if (requesters.size == numTasks) {\n+      requesters.foreach(_.reply(()))\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = {\n+    case RequestToSync(numTasks, stageId, stageAttemptId, taskAttemptId, barrierEpoch) =>\n+      // Get or init the ContextBarrierState correspond to the stage attempt.\n+      val barrierId = ContextBarrierId(stageId, stageAttemptId)\n+      states.putIfAbsent(barrierId, new ContextBarrierState(barrierId, numTasks))\n+      val barrierState = states.get(barrierId)\n+\n+      // Require the number of tasks is correctly set from the BarrierTaskContext."
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "minor: `stateConsumer` -> `clearState` or `clearStateConsumer` (it is useful to keep the type info in the variable name, but it is more important to name it based on what it does)",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-03T15:59:39Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.function.Consumer\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * Only one barrier() call shall happen on a barrier stage attempt at each time, we can use\n+ * (stageId, stageAttemptId) to identify the stage attempt where the barrier() call is from.\n+ */\n+private case class ContextBarrierId(stageId: Int, stageAttemptId: Int) {\n+  override def toString: String = s\"Stage $stageId (Attempt $stageAttemptId)\"\n+}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  // Listen to StageCompleted event, clear corresponding ContextBarrierState.\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      val barrierId = ContextBarrierId(stageInfo.stageId, stageInfo.attemptNumber)\n+      // Clear ContextBarrierState from a finished stage attempt.\n+      val barrierState = states.remove(barrierId)\n+      if (barrierState != null) {\n+        barrierState.clear()\n+      }\n+    }\n+  }\n+\n+  // Remember all active stage attempts that make barrier() call(s), and the corresponding\n+  // internal state.\n+  private val states = new ConcurrentHashMap[ContextBarrierId, ContextBarrierState]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Provide current state of a barrier() call, the state is created when a new stage attempt send\n+   * out a barrier() call, and recycled on stage completed.\n+   *\n+   * @param barrierId Identifier of the barrier stage that make a barrier() call.\n+   * @param numTasks Number of tasks of the barrier stage, all barrier() calls from the stage shall\n+   *                 collect `numTasks` requests to succeed.\n+   */\n+  private class ContextBarrierState(\n+      val barrierId: ContextBarrierId,\n+      val numTasks: Int) {\n+\n+    // There may be multiple barrier() calls from a barrier stage attempt, `barrierEpoch` is used\n+    // to identify each barrier() call. It shall get increased when a barrier() call succeed, or\n+    // reset when a barrier() call fail due to timeout.\n+    private var barrierEpoch: Int = 0\n+\n+    // An array of RPCCallContexts for barrier tasks that are waiting for reply of a barrier()\n+    // call.\n+    private val requesters: ArrayBuffer[RpcCallContext] = new ArrayBuffer[RpcCallContext](numTasks)\n+\n+    // A timer task that ensures we may timeout for a barrier() call.\n+    private var timerTask: TimerTask = null\n+\n+    // Init a TimerTask for a barrier() call.\n+    private def initTimerTask(): TimerTask = new TimerTask {\n+      override def run(): Unit = {\n+        // Timeout current barrier() call, fail all the sync requests.\n+        failAllRequesters(requesters, \"The coordinator didn't get all barrier sync \" +\n+          s\"requests for barrier epoch $barrierEpoch from $barrierId within ${timeout}s.\")\n+        cleanupBarrierStage(barrierId)\n+      }\n+    }\n+\n+    // Cancel the current active TimerTask and release resources.\n+    private def cancelTimerTask(): Unit = {\n+      if (timerTask != null) {\n+        timerTask.cancel()\n+        timerTask = null\n+      }\n+    }\n+\n+    // Process the global sync request. The barrier() call succeed if collected enough requests\n+    // within a configured time, otherwise fail all the pending requests.\n+    def handleRequest(requester: RpcCallContext, epoch: Int, taskId: Long): Unit = synchronized {\n+      // Check whether the epoch from the barrier tasks matches current barrierEpoch.\n+      logInfo(s\"Current barrier epoch for $barrierId is $barrierEpoch.\")\n+      if (epoch != barrierEpoch) {\n+        requester.sendFailure(new SparkException(s\"The request to sync of $barrierId with \" +\n+          s\"barrier epoch $barrierEpoch has already finished. Maybe task $taskId is not \" +\n+          \"properly killed.\"))\n+      } else {\n+        // If this is the first sync message received for a barrier() call, start timer to ensure\n+        // we may timeout for the sync.\n+        if (requesters.isEmpty) {\n+          timerTask = initTimerTask()\n+          timer.schedule(timerTask, timeout * 1000L)\n+        }\n+        // Add the requester to array of RPCCallContexts pending for reply.\n+        requesters += requester\n+        logInfo(s\"Barrier sync epoch $barrierEpoch from $barrierId received update from Task \" +\n+          s\"$taskId, current progress: ${requesters.size}/$numTasks.\")\n+        if (maybeFinishAllRequesters(requesters, numTasks)) {\n+          // Finished current barrier() call successfully, clean up ContextBarrierState and\n+          // increase the barrier epoch.\n+          logInfo(s\"Barrier sync epoch $barrierEpoch from $barrierId received all updates from \" +\n+            s\"tasks, finished successfully.\")\n+          barrierEpoch += 1\n+          requesters.clear()\n+          cancelTimerTask()\n+        }\n+      }\n+    }\n+\n+    // Cleanup the internal state of a barrier stage attempt.\n+    def clear(): Unit = synchronized {\n+      // The global sync fails so the stage is expected to retry another attempt, all sync\n+      // messages come from current stage attempt shall fail.\n+      barrierEpoch = -1\n+      requesters.clear()\n+      cancelTimerTask()\n+    }\n+  }\n+\n+  /**\n+   * Clean up the [[ContextBarrierState]] that correspond to a stage attempt.\n+   */\n+  private def cleanupBarrierStage(barrierId: ContextBarrierId): Unit = {\n+    val barrierState = states.remove(barrierId)\n+    if (barrierState != null) {\n+      barrierState.clear()\n+    }\n+  }\n+\n+  /**\n+   * Send failure to all the blocking barrier sync requests from a stage attempt with proper\n+   * failure message.\n+   */\n+  private def failAllRequesters(\n+      requesters: ArrayBuffer[RpcCallContext],\n+      message: String): Unit = {\n+    requesters.foreach(_.sendFailure(new SparkException(message)))\n+  }\n+\n+  /**\n+   * Finish all the blocking barrier sync requests from a stage attempt successfully if we\n+   * have received all the sync requests.\n+   */\n+  private def maybeFinishAllRequesters(\n+      requesters: ArrayBuffer[RpcCallContext],\n+      numTasks: Int): Boolean = {\n+    if (requesters.size == numTasks) {\n+      requesters.foreach(_.reply(()))\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = {\n+    case RequestToSync(numTasks, stageId, stageAttemptId, taskAttemptId, barrierEpoch) =>\n+      // Get or init the ContextBarrierState correspond to the stage attempt.\n+      val barrierId = ContextBarrierId(stageId, stageAttemptId)\n+      states.putIfAbsent(barrierId, new ContextBarrierState(barrierId, numTasks))\n+      val barrierState = states.get(barrierId)\n+\n+      // Require the number of tasks is correctly set from the BarrierTaskContext.\n+      require(barrierState.numTasks == numTasks, s\"Number of tasks of $barrierId is $numTasks \" +\n+        s\"from Task $taskAttemptId, previously it was ${barrierState.numTasks}.\")\n+\n+      barrierState.handleRequest(context, barrierEpoch, taskAttemptId)\n+  }\n+\n+  private val stateConsumer = new Consumer[ContextBarrierState] {"
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "we can move this method into `ContextBarrierState`",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-04T09:04:58Z",
    "diffHunk": "@@ -0,0 +1,233 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.function.Consumer\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * Only one barrier() call shall happen on a barrier stage attempt at each time, we can use\n+ * (stageId, stageAttemptId) to identify the stage attempt where the barrier() call is from.\n+ */\n+private case class ContextBarrierId(stageId: Int, stageAttemptId: Int) {\n+  override def toString: String = s\"Stage $stageId (Attempt $stageAttemptId)\"\n+}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  // Listen to StageCompleted event, clear corresponding ContextBarrierState.\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      val barrierId = ContextBarrierId(stageInfo.stageId, stageInfo.attemptNumber)\n+      // Clear ContextBarrierState from a finished stage attempt.\n+      val barrierState = states.remove(barrierId)\n+      if (barrierState != null) {\n+        barrierState.clear()\n+      }\n+    }\n+  }\n+\n+  // Remember all active stage attempts that make barrier() call(s), and the corresponding\n+  // internal state.\n+  private val states = new ConcurrentHashMap[ContextBarrierId, ContextBarrierState]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Provide current state of a barrier() call, the state is created when a new stage attempt send\n+   * out a barrier() call, and recycled on stage completed.\n+   *\n+   * @param barrierId Identifier of the barrier stage that make a barrier() call.\n+   * @param numTasks Number of tasks of the barrier stage, all barrier() calls from the stage shall\n+   *                 collect `numTasks` requests to succeed.\n+   */\n+  private class ContextBarrierState(\n+      val barrierId: ContextBarrierId,\n+      val numTasks: Int) {\n+\n+    // There may be multiple barrier() calls from a barrier stage attempt, `barrierEpoch` is used\n+    // to identify each barrier() call. It shall get increased when a barrier() call succeed, or\n+    // reset when a barrier() call fail due to timeout.\n+    private var barrierEpoch: Int = 0\n+\n+    // An array of RPCCallContexts for barrier tasks that are waiting for reply of a barrier()\n+    // call.\n+    private val requesters: ArrayBuffer[RpcCallContext] = new ArrayBuffer[RpcCallContext](numTasks)\n+\n+    // A timer task that ensures we may timeout for a barrier() call.\n+    private var timerTask: TimerTask = null\n+\n+    // Init a TimerTask for a barrier() call.\n+    private def initTimerTask(): Unit = {\n+      timerTask = new TimerTask {\n+        override def run(): Unit = {\n+          // Timeout current barrier() call, fail all the sync requests.\n+          failAllRequesters(requesters, \"The coordinator didn't get all barrier sync \" +\n+            s\"requests for barrier epoch $barrierEpoch from $barrierId within ${timeout}s.\")\n+          cleanupBarrierStage(barrierId)\n+        }\n+      }\n+    }\n+\n+    // Cancel the current active TimerTask and release resources.\n+    private def cancelTimerTask(): Unit = {\n+      if (timerTask != null) {\n+        timerTask.cancel()\n+        timerTask = null\n+      }\n+    }\n+\n+    // Process the global sync request. The barrier() call succeed if collected enough requests\n+    // within a configured time, otherwise fail all the pending requests.\n+    def handleRequest(requester: RpcCallContext, request: RequestToSync): Unit = synchronized {\n+      val taskId = request.taskAttemptId\n+      val epoch = request.barrierEpoch\n+\n+      // Require the number of tasks is correctly set from the BarrierTaskContext.\n+      require(request.numTasks == numTasks, s\"Number of tasks of $barrierId is \" +\n+        s\"${request.numTasks} from Task $taskId, previously it was $numTasks.\")\n+\n+      // Check whether the epoch from the barrier tasks matches current barrierEpoch.\n+      logInfo(s\"Current barrier epoch for $barrierId is $barrierEpoch.\")\n+      if (epoch != barrierEpoch) {\n+        requester.sendFailure(new SparkException(s\"The request to sync of $barrierId with \" +\n+          s\"barrier epoch $barrierEpoch has already finished. Maybe task $taskId is not \" +\n+          \"properly killed.\"))\n+      } else {\n+        // If this is the first sync message received for a barrier() call, start timer to ensure\n+        // we may timeout for the sync.\n+        if (requesters.isEmpty) {\n+          initTimerTask()\n+          timer.schedule(timerTask, timeout * 1000L)\n+        }\n+        // Add the requester to array of RPCCallContexts pending for reply.\n+        requesters += requester\n+        logInfo(s\"Barrier sync epoch $barrierEpoch from $barrierId received update from Task \" +\n+          s\"$taskId, current progress: ${requesters.size}/$numTasks.\")\n+        if (maybeFinishAllRequesters(requesters, numTasks)) {\n+          // Finished current barrier() call successfully, clean up ContextBarrierState and\n+          // increase the barrier epoch.\n+          logInfo(s\"Barrier sync epoch $barrierEpoch from $barrierId received all updates from \" +\n+            s\"tasks, finished successfully.\")\n+          barrierEpoch += 1\n+          requesters.clear()\n+          cancelTimerTask()\n+        }\n+      }\n+    }\n+\n+    // Send failure to all the blocking barrier sync requests from a stage attempt with proper\n+    // failure message.\n+    private def failAllRequesters("
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "actually it's just one line and only called once, shall we inline it?",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-04T09:07:16Z",
    "diffHunk": "@@ -0,0 +1,233 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.function.Consumer\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * Only one barrier() call shall happen on a barrier stage attempt at each time, we can use\n+ * (stageId, stageAttemptId) to identify the stage attempt where the barrier() call is from.\n+ */\n+private case class ContextBarrierId(stageId: Int, stageAttemptId: Int) {\n+  override def toString: String = s\"Stage $stageId (Attempt $stageAttemptId)\"\n+}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  // Listen to StageCompleted event, clear corresponding ContextBarrierState.\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      val barrierId = ContextBarrierId(stageInfo.stageId, stageInfo.attemptNumber)\n+      // Clear ContextBarrierState from a finished stage attempt.\n+      val barrierState = states.remove(barrierId)\n+      if (barrierState != null) {\n+        barrierState.clear()\n+      }\n+    }\n+  }\n+\n+  // Remember all active stage attempts that make barrier() call(s), and the corresponding\n+  // internal state.\n+  private val states = new ConcurrentHashMap[ContextBarrierId, ContextBarrierState]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Provide current state of a barrier() call, the state is created when a new stage attempt send\n+   * out a barrier() call, and recycled on stage completed.\n+   *\n+   * @param barrierId Identifier of the barrier stage that make a barrier() call.\n+   * @param numTasks Number of tasks of the barrier stage, all barrier() calls from the stage shall\n+   *                 collect `numTasks` requests to succeed.\n+   */\n+  private class ContextBarrierState(\n+      val barrierId: ContextBarrierId,\n+      val numTasks: Int) {\n+\n+    // There may be multiple barrier() calls from a barrier stage attempt, `barrierEpoch` is used\n+    // to identify each barrier() call. It shall get increased when a barrier() call succeed, or\n+    // reset when a barrier() call fail due to timeout.\n+    private var barrierEpoch: Int = 0\n+\n+    // An array of RPCCallContexts for barrier tasks that are waiting for reply of a barrier()\n+    // call.\n+    private val requesters: ArrayBuffer[RpcCallContext] = new ArrayBuffer[RpcCallContext](numTasks)\n+\n+    // A timer task that ensures we may timeout for a barrier() call.\n+    private var timerTask: TimerTask = null\n+\n+    // Init a TimerTask for a barrier() call.\n+    private def initTimerTask(): Unit = {\n+      timerTask = new TimerTask {\n+        override def run(): Unit = {\n+          // Timeout current barrier() call, fail all the sync requests.\n+          failAllRequesters(requesters, \"The coordinator didn't get all barrier sync \" +\n+            s\"requests for barrier epoch $barrierEpoch from $barrierId within ${timeout}s.\")\n+          cleanupBarrierStage(barrierId)\n+        }\n+      }\n+    }\n+\n+    // Cancel the current active TimerTask and release resources.\n+    private def cancelTimerTask(): Unit = {\n+      if (timerTask != null) {\n+        timerTask.cancel()\n+        timerTask = null\n+      }\n+    }\n+\n+    // Process the global sync request. The barrier() call succeed if collected enough requests\n+    // within a configured time, otherwise fail all the pending requests.\n+    def handleRequest(requester: RpcCallContext, request: RequestToSync): Unit = synchronized {\n+      val taskId = request.taskAttemptId\n+      val epoch = request.barrierEpoch\n+\n+      // Require the number of tasks is correctly set from the BarrierTaskContext.\n+      require(request.numTasks == numTasks, s\"Number of tasks of $barrierId is \" +\n+        s\"${request.numTasks} from Task $taskId, previously it was $numTasks.\")\n+\n+      // Check whether the epoch from the barrier tasks matches current barrierEpoch.\n+      logInfo(s\"Current barrier epoch for $barrierId is $barrierEpoch.\")\n+      if (epoch != barrierEpoch) {\n+        requester.sendFailure(new SparkException(s\"The request to sync of $barrierId with \" +\n+          s\"barrier epoch $barrierEpoch has already finished. Maybe task $taskId is not \" +\n+          \"properly killed.\"))\n+      } else {\n+        // If this is the first sync message received for a barrier() call, start timer to ensure\n+        // we may timeout for the sync.\n+        if (requesters.isEmpty) {\n+          initTimerTask()\n+          timer.schedule(timerTask, timeout * 1000L)\n+        }\n+        // Add the requester to array of RPCCallContexts pending for reply.\n+        requesters += requester\n+        logInfo(s\"Barrier sync epoch $barrierEpoch from $barrierId received update from Task \" +\n+          s\"$taskId, current progress: ${requesters.size}/$numTasks.\")\n+        if (maybeFinishAllRequesters(requesters, numTasks)) {\n+          // Finished current barrier() call successfully, clean up ContextBarrierState and\n+          // increase the barrier epoch.\n+          logInfo(s\"Barrier sync epoch $barrierEpoch from $barrierId received all updates from \" +\n+            s\"tasks, finished successfully.\")\n+          barrierEpoch += 1\n+          requesters.clear()\n+          cancelTimerTask()\n+        }\n+      }\n+    }\n+\n+    // Send failure to all the blocking barrier sync requests from a stage attempt with proper\n+    // failure message.\n+    private def failAllRequesters("
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "we can call `cleanupBarrierStage` here",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-04T09:11:40Z",
    "diffHunk": "@@ -0,0 +1,233 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.function.Consumer\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * Only one barrier() call shall happen on a barrier stage attempt at each time, we can use\n+ * (stageId, stageAttemptId) to identify the stage attempt where the barrier() call is from.\n+ */\n+private case class ContextBarrierId(stageId: Int, stageAttemptId: Int) {\n+  override def toString: String = s\"Stage $stageId (Attempt $stageAttemptId)\"\n+}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  // Listen to StageCompleted event, clear corresponding ContextBarrierState.\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      val barrierId = ContextBarrierId(stageInfo.stageId, stageInfo.attemptNumber)\n+      // Clear ContextBarrierState from a finished stage attempt.\n+      val barrierState = states.remove(barrierId)"
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "We create `ContextBarrierState` when we receive the first sync message, I think it's more clear to create the timer when creating `ContextBarrierState`, so that we don't need the `if` here.",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-04T09:15:39Z",
    "diffHunk": "@@ -0,0 +1,233 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.function.Consumer\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * Only one barrier() call shall happen on a barrier stage attempt at each time, we can use\n+ * (stageId, stageAttemptId) to identify the stage attempt where the barrier() call is from.\n+ */\n+private case class ContextBarrierId(stageId: Int, stageAttemptId: Int) {\n+  override def toString: String = s\"Stage $stageId (Attempt $stageAttemptId)\"\n+}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  // Listen to StageCompleted event, clear corresponding ContextBarrierState.\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      val barrierId = ContextBarrierId(stageInfo.stageId, stageInfo.attemptNumber)\n+      // Clear ContextBarrierState from a finished stage attempt.\n+      val barrierState = states.remove(barrierId)\n+      if (barrierState != null) {\n+        barrierState.clear()\n+      }\n+    }\n+  }\n+\n+  // Remember all active stage attempts that make barrier() call(s), and the corresponding\n+  // internal state.\n+  private val states = new ConcurrentHashMap[ContextBarrierId, ContextBarrierState]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Provide current state of a barrier() call, the state is created when a new stage attempt send\n+   * out a barrier() call, and recycled on stage completed.\n+   *\n+   * @param barrierId Identifier of the barrier stage that make a barrier() call.\n+   * @param numTasks Number of tasks of the barrier stage, all barrier() calls from the stage shall\n+   *                 collect `numTasks` requests to succeed.\n+   */\n+  private class ContextBarrierState(\n+      val barrierId: ContextBarrierId,\n+      val numTasks: Int) {\n+\n+    // There may be multiple barrier() calls from a barrier stage attempt, `barrierEpoch` is used\n+    // to identify each barrier() call. It shall get increased when a barrier() call succeed, or\n+    // reset when a barrier() call fail due to timeout.\n+    private var barrierEpoch: Int = 0\n+\n+    // An array of RPCCallContexts for barrier tasks that are waiting for reply of a barrier()\n+    // call.\n+    private val requesters: ArrayBuffer[RpcCallContext] = new ArrayBuffer[RpcCallContext](numTasks)\n+\n+    // A timer task that ensures we may timeout for a barrier() call.\n+    private var timerTask: TimerTask = null\n+\n+    // Init a TimerTask for a barrier() call.\n+    private def initTimerTask(): Unit = {\n+      timerTask = new TimerTask {\n+        override def run(): Unit = {\n+          // Timeout current barrier() call, fail all the sync requests.\n+          failAllRequesters(requesters, \"The coordinator didn't get all barrier sync \" +\n+            s\"requests for barrier epoch $barrierEpoch from $barrierId within ${timeout}s.\")\n+          cleanupBarrierStage(barrierId)\n+        }\n+      }\n+    }\n+\n+    // Cancel the current active TimerTask and release resources.\n+    private def cancelTimerTask(): Unit = {\n+      if (timerTask != null) {\n+        timerTask.cancel()\n+        timerTask = null\n+      }\n+    }\n+\n+    // Process the global sync request. The barrier() call succeed if collected enough requests\n+    // within a configured time, otherwise fail all the pending requests.\n+    def handleRequest(requester: RpcCallContext, request: RequestToSync): Unit = synchronized {\n+      val taskId = request.taskAttemptId\n+      val epoch = request.barrierEpoch\n+\n+      // Require the number of tasks is correctly set from the BarrierTaskContext.\n+      require(request.numTasks == numTasks, s\"Number of tasks of $barrierId is \" +\n+        s\"${request.numTasks} from Task $taskId, previously it was $numTasks.\")\n+\n+      // Check whether the epoch from the barrier tasks matches current barrierEpoch.\n+      logInfo(s\"Current barrier epoch for $barrierId is $barrierEpoch.\")\n+      if (epoch != barrierEpoch) {\n+        requester.sendFailure(new SparkException(s\"The request to sync of $barrierId with \" +\n+          s\"barrier epoch $barrierEpoch has already finished. Maybe task $taskId is not \" +\n+          \"properly killed.\"))\n+      } else {\n+        // If this is the first sync message received for a barrier() call, start timer to ensure\n+        // we may timeout for the sync.",
    "line": 148
  }, {
    "author": {
      "login": "jiangxb1987"
    },
    "body": "This is true if you have only one `barrier()` call in a barrier task. Otherwise, if the first `barrier()` call succeed, the RPCCallContext array shall get cleared and the barrier epoch increased, but we still reuse the same `ContextBarrierState` for forthcoming `barrier()` calls.",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-04T14:41:52Z",
    "diffHunk": "@@ -0,0 +1,233 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.function.Consumer\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * Only one barrier() call shall happen on a barrier stage attempt at each time, we can use\n+ * (stageId, stageAttemptId) to identify the stage attempt where the barrier() call is from.\n+ */\n+private case class ContextBarrierId(stageId: Int, stageAttemptId: Int) {\n+  override def toString: String = s\"Stage $stageId (Attempt $stageAttemptId)\"\n+}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  // Listen to StageCompleted event, clear corresponding ContextBarrierState.\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      val barrierId = ContextBarrierId(stageInfo.stageId, stageInfo.attemptNumber)\n+      // Clear ContextBarrierState from a finished stage attempt.\n+      val barrierState = states.remove(barrierId)\n+      if (barrierState != null) {\n+        barrierState.clear()\n+      }\n+    }\n+  }\n+\n+  // Remember all active stage attempts that make barrier() call(s), and the corresponding\n+  // internal state.\n+  private val states = new ConcurrentHashMap[ContextBarrierId, ContextBarrierState]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Provide current state of a barrier() call, the state is created when a new stage attempt send\n+   * out a barrier() call, and recycled on stage completed.\n+   *\n+   * @param barrierId Identifier of the barrier stage that make a barrier() call.\n+   * @param numTasks Number of tasks of the barrier stage, all barrier() calls from the stage shall\n+   *                 collect `numTasks` requests to succeed.\n+   */\n+  private class ContextBarrierState(\n+      val barrierId: ContextBarrierId,\n+      val numTasks: Int) {\n+\n+    // There may be multiple barrier() calls from a barrier stage attempt, `barrierEpoch` is used\n+    // to identify each barrier() call. It shall get increased when a barrier() call succeed, or\n+    // reset when a barrier() call fail due to timeout.\n+    private var barrierEpoch: Int = 0\n+\n+    // An array of RPCCallContexts for barrier tasks that are waiting for reply of a barrier()\n+    // call.\n+    private val requesters: ArrayBuffer[RpcCallContext] = new ArrayBuffer[RpcCallContext](numTasks)\n+\n+    // A timer task that ensures we may timeout for a barrier() call.\n+    private var timerTask: TimerTask = null\n+\n+    // Init a TimerTask for a barrier() call.\n+    private def initTimerTask(): Unit = {\n+      timerTask = new TimerTask {\n+        override def run(): Unit = {\n+          // Timeout current barrier() call, fail all the sync requests.\n+          failAllRequesters(requesters, \"The coordinator didn't get all barrier sync \" +\n+            s\"requests for barrier epoch $barrierEpoch from $barrierId within ${timeout}s.\")\n+          cleanupBarrierStage(barrierId)\n+        }\n+      }\n+    }\n+\n+    // Cancel the current active TimerTask and release resources.\n+    private def cancelTimerTask(): Unit = {\n+      if (timerTask != null) {\n+        timerTask.cancel()\n+        timerTask = null\n+      }\n+    }\n+\n+    // Process the global sync request. The barrier() call succeed if collected enough requests\n+    // within a configured time, otherwise fail all the pending requests.\n+    def handleRequest(requester: RpcCallContext, request: RequestToSync): Unit = synchronized {\n+      val taskId = request.taskAttemptId\n+      val epoch = request.barrierEpoch\n+\n+      // Require the number of tasks is correctly set from the BarrierTaskContext.\n+      require(request.numTasks == numTasks, s\"Number of tasks of $barrierId is \" +\n+        s\"${request.numTasks} from Task $taskId, previously it was $numTasks.\")\n+\n+      // Check whether the epoch from the barrier tasks matches current barrierEpoch.\n+      logInfo(s\"Current barrier epoch for $barrierId is $barrierEpoch.\")\n+      if (epoch != barrierEpoch) {\n+        requester.sendFailure(new SparkException(s\"The request to sync of $barrierId with \" +\n+          s\"barrier epoch $barrierEpoch has already finished. Maybe task $taskId is not \" +\n+          \"properly killed.\"))\n+      } else {\n+        // If this is the first sync message received for a barrier() call, start timer to ensure\n+        // we may timeout for the sync.",
    "line": 148
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "ah i see, makes sense",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-05T02:56:25Z",
    "diffHunk": "@@ -0,0 +1,233 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.function.Consumer\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * Only one barrier() call shall happen on a barrier stage attempt at each time, we can use\n+ * (stageId, stageAttemptId) to identify the stage attempt where the barrier() call is from.\n+ */\n+private case class ContextBarrierId(stageId: Int, stageAttemptId: Int) {\n+  override def toString: String = s\"Stage $stageId (Attempt $stageAttemptId)\"\n+}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  // Listen to StageCompleted event, clear corresponding ContextBarrierState.\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      val barrierId = ContextBarrierId(stageInfo.stageId, stageInfo.attemptNumber)\n+      // Clear ContextBarrierState from a finished stage attempt.\n+      val barrierState = states.remove(barrierId)\n+      if (barrierState != null) {\n+        barrierState.clear()\n+      }\n+    }\n+  }\n+\n+  // Remember all active stage attempts that make barrier() call(s), and the corresponding\n+  // internal state.\n+  private val states = new ConcurrentHashMap[ContextBarrierId, ContextBarrierState]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Provide current state of a barrier() call, the state is created when a new stage attempt send\n+   * out a barrier() call, and recycled on stage completed.\n+   *\n+   * @param barrierId Identifier of the barrier stage that make a barrier() call.\n+   * @param numTasks Number of tasks of the barrier stage, all barrier() calls from the stage shall\n+   *                 collect `numTasks` requests to succeed.\n+   */\n+  private class ContextBarrierState(\n+      val barrierId: ContextBarrierId,\n+      val numTasks: Int) {\n+\n+    // There may be multiple barrier() calls from a barrier stage attempt, `barrierEpoch` is used\n+    // to identify each barrier() call. It shall get increased when a barrier() call succeed, or\n+    // reset when a barrier() call fail due to timeout.\n+    private var barrierEpoch: Int = 0\n+\n+    // An array of RPCCallContexts for barrier tasks that are waiting for reply of a barrier()\n+    // call.\n+    private val requesters: ArrayBuffer[RpcCallContext] = new ArrayBuffer[RpcCallContext](numTasks)\n+\n+    // A timer task that ensures we may timeout for a barrier() call.\n+    private var timerTask: TimerTask = null\n+\n+    // Init a TimerTask for a barrier() call.\n+    private def initTimerTask(): Unit = {\n+      timerTask = new TimerTask {\n+        override def run(): Unit = {\n+          // Timeout current barrier() call, fail all the sync requests.\n+          failAllRequesters(requesters, \"The coordinator didn't get all barrier sync \" +\n+            s\"requests for barrier epoch $barrierEpoch from $barrierId within ${timeout}s.\")\n+          cleanupBarrierStage(barrierId)\n+        }\n+      }\n+    }\n+\n+    // Cancel the current active TimerTask and release resources.\n+    private def cancelTimerTask(): Unit = {\n+      if (timerTask != null) {\n+        timerTask.cancel()\n+        timerTask = null\n+      }\n+    }\n+\n+    // Process the global sync request. The barrier() call succeed if collected enough requests\n+    // within a configured time, otherwise fail all the pending requests.\n+    def handleRequest(requester: RpcCallContext, request: RequestToSync): Unit = synchronized {\n+      val taskId = request.taskAttemptId\n+      val epoch = request.barrierEpoch\n+\n+      // Require the number of tasks is correctly set from the BarrierTaskContext.\n+      require(request.numTasks == numTasks, s\"Number of tasks of $barrierId is \" +\n+        s\"${request.numTasks} from Task $taskId, previously it was $numTasks.\")\n+\n+      // Check whether the epoch from the barrier tasks matches current barrierEpoch.\n+      logInfo(s\"Current barrier epoch for $barrierId is $barrierEpoch.\")\n+      if (epoch != barrierEpoch) {\n+        requester.sendFailure(new SparkException(s\"The request to sync of $barrierId with \" +\n+          s\"barrier epoch $barrierEpoch has already finished. Maybe task $taskId is not \" +\n+          \"properly killed.\"))\n+      } else {\n+        // If this is the first sync message received for a barrier() call, start timer to ensure\n+        // we may timeout for the sync.",
    "line": 148
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "why `synchronized`? I think we only access `ContextBarrierState` in the RPC thread, which is single thread.",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-04T09:18:21Z",
    "diffHunk": "@@ -0,0 +1,233 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.function.Consumer\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * Only one barrier() call shall happen on a barrier stage attempt at each time, we can use\n+ * (stageId, stageAttemptId) to identify the stage attempt where the barrier() call is from.\n+ */\n+private case class ContextBarrierId(stageId: Int, stageAttemptId: Int) {\n+  override def toString: String = s\"Stage $stageId (Attempt $stageAttemptId)\"\n+}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  // Listen to StageCompleted event, clear corresponding ContextBarrierState.\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      val barrierId = ContextBarrierId(stageInfo.stageId, stageInfo.attemptNumber)\n+      // Clear ContextBarrierState from a finished stage attempt.\n+      val barrierState = states.remove(barrierId)\n+      if (barrierState != null) {\n+        barrierState.clear()\n+      }\n+    }\n+  }\n+\n+  // Remember all active stage attempts that make barrier() call(s), and the corresponding\n+  // internal state.\n+  private val states = new ConcurrentHashMap[ContextBarrierId, ContextBarrierState]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Provide current state of a barrier() call, the state is created when a new stage attempt send\n+   * out a barrier() call, and recycled on stage completed.\n+   *\n+   * @param barrierId Identifier of the barrier stage that make a barrier() call.\n+   * @param numTasks Number of tasks of the barrier stage, all barrier() calls from the stage shall\n+   *                 collect `numTasks` requests to succeed.\n+   */\n+  private class ContextBarrierState(\n+      val barrierId: ContextBarrierId,\n+      val numTasks: Int) {\n+\n+    // There may be multiple barrier() calls from a barrier stage attempt, `barrierEpoch` is used\n+    // to identify each barrier() call. It shall get increased when a barrier() call succeed, or\n+    // reset when a barrier() call fail due to timeout.\n+    private var barrierEpoch: Int = 0\n+\n+    // An array of RPCCallContexts for barrier tasks that are waiting for reply of a barrier()\n+    // call.\n+    private val requesters: ArrayBuffer[RpcCallContext] = new ArrayBuffer[RpcCallContext](numTasks)\n+\n+    // A timer task that ensures we may timeout for a barrier() call.\n+    private var timerTask: TimerTask = null\n+\n+    // Init a TimerTask for a barrier() call.\n+    private def initTimerTask(): Unit = {\n+      timerTask = new TimerTask {\n+        override def run(): Unit = {\n+          // Timeout current barrier() call, fail all the sync requests.\n+          failAllRequesters(requesters, \"The coordinator didn't get all barrier sync \" +\n+            s\"requests for barrier epoch $barrierEpoch from $barrierId within ${timeout}s.\")\n+          cleanupBarrierStage(barrierId)\n+        }\n+      }\n+    }\n+\n+    // Cancel the current active TimerTask and release resources.\n+    private def cancelTimerTask(): Unit = {\n+      if (timerTask != null) {\n+        timerTask.cancel()\n+        timerTask = null\n+      }\n+    }\n+\n+    // Process the global sync request. The barrier() call succeed if collected enough requests\n+    // within a configured time, otherwise fail all the pending requests.\n+    def handleRequest(requester: RpcCallContext, request: RequestToSync): Unit = synchronized {",
    "line": 132
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "ah i see, the timer thread also accesses `ContextBarrierState`, the code in timer thread also need to be synchronized.",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-04T09:20:34Z",
    "diffHunk": "@@ -0,0 +1,233 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.function.Consumer\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * Only one barrier() call shall happen on a barrier stage attempt at each time, we can use\n+ * (stageId, stageAttemptId) to identify the stage attempt where the barrier() call is from.\n+ */\n+private case class ContextBarrierId(stageId: Int, stageAttemptId: Int) {\n+  override def toString: String = s\"Stage $stageId (Attempt $stageAttemptId)\"\n+}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  // Listen to StageCompleted event, clear corresponding ContextBarrierState.\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      val barrierId = ContextBarrierId(stageInfo.stageId, stageInfo.attemptNumber)\n+      // Clear ContextBarrierState from a finished stage attempt.\n+      val barrierState = states.remove(barrierId)\n+      if (barrierState != null) {\n+        barrierState.clear()\n+      }\n+    }\n+  }\n+\n+  // Remember all active stage attempts that make barrier() call(s), and the corresponding\n+  // internal state.\n+  private val states = new ConcurrentHashMap[ContextBarrierId, ContextBarrierState]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Provide current state of a barrier() call, the state is created when a new stage attempt send\n+   * out a barrier() call, and recycled on stage completed.\n+   *\n+   * @param barrierId Identifier of the barrier stage that make a barrier() call.\n+   * @param numTasks Number of tasks of the barrier stage, all barrier() calls from the stage shall\n+   *                 collect `numTasks` requests to succeed.\n+   */\n+  private class ContextBarrierState(\n+      val barrierId: ContextBarrierId,\n+      val numTasks: Int) {\n+\n+    // There may be multiple barrier() calls from a barrier stage attempt, `barrierEpoch` is used\n+    // to identify each barrier() call. It shall get increased when a barrier() call succeed, or\n+    // reset when a barrier() call fail due to timeout.\n+    private var barrierEpoch: Int = 0\n+\n+    // An array of RPCCallContexts for barrier tasks that are waiting for reply of a barrier()\n+    // call.\n+    private val requesters: ArrayBuffer[RpcCallContext] = new ArrayBuffer[RpcCallContext](numTasks)\n+\n+    // A timer task that ensures we may timeout for a barrier() call.\n+    private var timerTask: TimerTask = null\n+\n+    // Init a TimerTask for a barrier() call.\n+    private def initTimerTask(): Unit = {\n+      timerTask = new TimerTask {\n+        override def run(): Unit = {\n+          // Timeout current barrier() call, fail all the sync requests.\n+          failAllRequesters(requesters, \"The coordinator didn't get all barrier sync \" +\n+            s\"requests for barrier epoch $barrierEpoch from $barrierId within ${timeout}s.\")\n+          cleanupBarrierStage(barrierId)\n+        }\n+      }\n+    }\n+\n+    // Cancel the current active TimerTask and release resources.\n+    private def cancelTimerTask(): Unit = {\n+      if (timerTask != null) {\n+        timerTask.cancel()\n+        timerTask = null\n+      }\n+    }\n+\n+    // Process the global sync request. The barrier() call succeed if collected enough requests\n+    // within a configured time, otherwise fail all the pending requests.\n+    def handleRequest(requester: RpcCallContext, request: RequestToSync): Unit = synchronized {",
    "line": 132
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "nit: `within $timeout seconds`",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-04T09:25:56Z",
    "diffHunk": "@@ -0,0 +1,233 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.function.Consumer\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * Only one barrier() call shall happen on a barrier stage attempt at each time, we can use\n+ * (stageId, stageAttemptId) to identify the stage attempt where the barrier() call is from.\n+ */\n+private case class ContextBarrierId(stageId: Int, stageAttemptId: Int) {\n+  override def toString: String = s\"Stage $stageId (Attempt $stageAttemptId)\"\n+}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  // Listen to StageCompleted event, clear corresponding ContextBarrierState.\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      val barrierId = ContextBarrierId(stageInfo.stageId, stageInfo.attemptNumber)\n+      // Clear ContextBarrierState from a finished stage attempt.\n+      val barrierState = states.remove(barrierId)\n+      if (barrierState != null) {\n+        barrierState.clear()\n+      }\n+    }\n+  }\n+\n+  // Remember all active stage attempts that make barrier() call(s), and the corresponding\n+  // internal state.\n+  private val states = new ConcurrentHashMap[ContextBarrierId, ContextBarrierState]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Provide current state of a barrier() call, the state is created when a new stage attempt send\n+   * out a barrier() call, and recycled on stage completed.\n+   *\n+   * @param barrierId Identifier of the barrier stage that make a barrier() call.\n+   * @param numTasks Number of tasks of the barrier stage, all barrier() calls from the stage shall\n+   *                 collect `numTasks` requests to succeed.\n+   */\n+  private class ContextBarrierState(\n+      val barrierId: ContextBarrierId,\n+      val numTasks: Int) {\n+\n+    // There may be multiple barrier() calls from a barrier stage attempt, `barrierEpoch` is used\n+    // to identify each barrier() call. It shall get increased when a barrier() call succeed, or\n+    // reset when a barrier() call fail due to timeout.\n+    private var barrierEpoch: Int = 0\n+\n+    // An array of RPCCallContexts for barrier tasks that are waiting for reply of a barrier()\n+    // call.\n+    private val requesters: ArrayBuffer[RpcCallContext] = new ArrayBuffer[RpcCallContext](numTasks)\n+\n+    // A timer task that ensures we may timeout for a barrier() call.\n+    private var timerTask: TimerTask = null\n+\n+    // Init a TimerTask for a barrier() call.\n+    private def initTimerTask(): Unit = {\n+      timerTask = new TimerTask {\n+        override def run(): Unit = {\n+          // Timeout current barrier() call, fail all the sync requests.\n+          failAllRequesters(requesters, \"The coordinator didn't get all barrier sync \" +\n+            s\"requests for barrier epoch $barrierEpoch from $barrierId within ${timeout}s.\")"
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "Will it be safer to do something like?\r\n\r\n```\r\ntry {\r\n  ...\r\n} finally {\r\n  super.onStop()\r\n}\r\n```",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-05T09:50:12Z",
    "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.function.Consumer\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * Only one barrier() call shall happen on a barrier stage attempt at each time, we can use\n+ * (stageId, stageAttemptId) to identify the stage attempt where the barrier() call is from.\n+ */\n+private case class ContextBarrierId(stageId: Int, stageAttemptId: Int) {\n+  override def toString: String = s\"Stage $stageId (Attempt $stageAttemptId)\"\n+}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  // Listen to StageCompleted event, clear corresponding ContextBarrierState.\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      val barrierId = ContextBarrierId(stageInfo.stageId, stageInfo.attemptNumber)\n+      // Clear ContextBarrierState from a finished stage attempt.\n+      cleanupBarrierStage(barrierId)\n+    }\n+  }\n+\n+  // Remember all active stage attempts that make barrier() call(s), and the corresponding\n+  // internal state.\n+  private val states = new ConcurrentHashMap[ContextBarrierId, ContextBarrierState]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Provide current state of a barrier() call, the state is created when a new stage attempt send\n+   * out a barrier() call, and recycled on stage completed.\n+   *\n+   * @param barrierId Identifier of the barrier stage that make a barrier() call.\n+   * @param numTasks Number of tasks of the barrier stage, all barrier() calls from the stage shall\n+   *                 collect `numTasks` requests to succeed.\n+   */\n+  private class ContextBarrierState(\n+      val barrierId: ContextBarrierId,\n+      val numTasks: Int) {\n+\n+    // There may be multiple barrier() calls from a barrier stage attempt, `barrierEpoch` is used\n+    // to identify each barrier() call. It shall get increased when a barrier() call succeed, or\n+    // reset when a barrier() call fail due to timeout.\n+    private var barrierEpoch: Int = 0\n+\n+    // An array of RPCCallContexts for barrier tasks that are waiting for reply of a barrier()\n+    // call.\n+    private val requesters: ArrayBuffer[RpcCallContext] = new ArrayBuffer[RpcCallContext](numTasks)\n+\n+    // A timer task that ensures we may timeout for a barrier() call.\n+    private var timerTask: TimerTask = null\n+\n+    // Init a TimerTask for a barrier() call.\n+    private def initTimerTask(): Unit = {\n+      timerTask = new TimerTask {\n+        override def run(): Unit = synchronized {\n+          // Timeout current barrier() call, fail all the sync requests.\n+          requesters.foreach(_.sendFailure(new SparkException(\"The coordinator didn't get all \" +\n+            s\"barrier sync requests for barrier epoch $barrierEpoch from $barrierId within \" +\n+            s\"$timeout second(s).\")))\n+          cleanupBarrierStage(barrierId)\n+        }\n+      }\n+    }\n+\n+    // Cancel the current active TimerTask and release resources.\n+    private def cancelTimerTask(): Unit = {\n+      if (timerTask != null) {\n+        timerTask.cancel()\n+        timerTask = null\n+      }\n+    }\n+\n+    // Process the global sync request. The barrier() call succeed if collected enough requests\n+    // within a configured time, otherwise fail all the pending requests.\n+    def handleRequest(requester: RpcCallContext, request: RequestToSync): Unit = synchronized {\n+      val taskId = request.taskAttemptId\n+      val epoch = request.barrierEpoch\n+\n+      // Require the number of tasks is correctly set from the BarrierTaskContext.\n+      require(request.numTasks == numTasks, s\"Number of tasks of $barrierId is \" +\n+        s\"${request.numTasks} from Task $taskId, previously it was $numTasks.\")\n+\n+      // Check whether the epoch from the barrier tasks matches current barrierEpoch.\n+      logInfo(s\"Current barrier epoch for $barrierId is $barrierEpoch.\")\n+      if (epoch != barrierEpoch) {\n+        requester.sendFailure(new SparkException(s\"The request to sync of $barrierId with \" +\n+          s\"barrier epoch $barrierEpoch has already finished. Maybe task $taskId is not \" +\n+          \"properly killed.\"))\n+      } else {\n+        // If this is the first sync message received for a barrier() call, start timer to ensure\n+        // we may timeout for the sync.\n+        if (requesters.isEmpty) {\n+          initTimerTask()\n+          timer.schedule(timerTask, timeout * 1000L)\n+        }\n+        // Add the requester to array of RPCCallContexts pending for reply.\n+        requesters += requester\n+        logInfo(s\"Barrier sync epoch $barrierEpoch from $barrierId received update from Task \" +\n+          s\"$taskId, current progress: ${requesters.size}/$numTasks.\")\n+        if (maybeFinishAllRequesters(requesters, numTasks)) {\n+          // Finished current barrier() call successfully, clean up ContextBarrierState and\n+          // increase the barrier epoch.\n+          logInfo(s\"Barrier sync epoch $barrierEpoch from $barrierId received all updates from \" +\n+            s\"tasks, finished successfully.\")\n+          barrierEpoch += 1\n+          requesters.clear()\n+          cancelTimerTask()\n+        }\n+      }\n+    }\n+\n+    // Finish all the blocking barrier sync requests from a stage attempt successfully if we\n+    // have received all the sync requests.\n+    private def maybeFinishAllRequesters(\n+        requesters: ArrayBuffer[RpcCallContext],\n+        numTasks: Int): Boolean = {\n+      if (requesters.size == numTasks) {\n+        requesters.foreach(_.reply(()))\n+        true\n+      } else {\n+        false\n+      }\n+    }\n+\n+    // Cleanup the internal state of a barrier stage attempt.\n+    def clear(): Unit = synchronized {\n+      // The global sync fails so the stage is expected to retry another attempt, all sync\n+      // messages come from current stage attempt shall fail.\n+      barrierEpoch = -1\n+      requesters.clear()\n+      cancelTimerTask()\n+    }\n+  }\n+\n+  // Clean up the [[ContextBarrierState]] that correspond to a stage attempt.\n+  private def cleanupBarrierStage(barrierId: ContextBarrierId): Unit = {\n+    val barrierState = states.remove(barrierId)\n+    if (barrierState != null) {\n+      barrierState.clear()\n+    }\n+  }\n+\n+  override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = {\n+    case request @ RequestToSync(numTasks, stageId, stageAttemptId, _, _) =>\n+      // Get or init the ContextBarrierState correspond to the stage attempt.\n+      val barrierId = ContextBarrierId(stageId, stageAttemptId)\n+      states.putIfAbsent(barrierId, new ContextBarrierState(barrierId, numTasks))\n+      val barrierState = states.get(barrierId)\n+\n+      barrierState.handleRequest(context, request)\n+  }\n+\n+  private val clearStateConsumer = new Consumer[ContextBarrierState] {\n+    override def accept(state: ContextBarrierState) = state.clear()\n+  }\n+\n+  override def onStop(): Unit = {\n+    states.forEachValue(1, clearStateConsumer)\n+    states.clear()\n+    listenerBus.removeListener(listener)\n+    super.onStop()"
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "Move it just after `onStart`?",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-05T09:53:23Z",
    "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.function.Consumer\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * Only one barrier() call shall happen on a barrier stage attempt at each time, we can use\n+ * (stageId, stageAttemptId) to identify the stage attempt where the barrier() call is from.\n+ */\n+private case class ContextBarrierId(stageId: Int, stageAttemptId: Int) {\n+  override def toString: String = s\"Stage $stageId (Attempt $stageAttemptId)\"\n+}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  // Listen to StageCompleted event, clear corresponding ContextBarrierState.\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      val barrierId = ContextBarrierId(stageInfo.stageId, stageInfo.attemptNumber)\n+      // Clear ContextBarrierState from a finished stage attempt.\n+      cleanupBarrierStage(barrierId)\n+    }\n+  }\n+\n+  // Remember all active stage attempts that make barrier() call(s), and the corresponding\n+  // internal state.\n+  private val states = new ConcurrentHashMap[ContextBarrierId, ContextBarrierState]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Provide current state of a barrier() call, the state is created when a new stage attempt send\n+   * out a barrier() call, and recycled on stage completed.\n+   *\n+   * @param barrierId Identifier of the barrier stage that make a barrier() call.\n+   * @param numTasks Number of tasks of the barrier stage, all barrier() calls from the stage shall\n+   *                 collect `numTasks` requests to succeed.\n+   */\n+  private class ContextBarrierState(\n+      val barrierId: ContextBarrierId,\n+      val numTasks: Int) {\n+\n+    // There may be multiple barrier() calls from a barrier stage attempt, `barrierEpoch` is used\n+    // to identify each barrier() call. It shall get increased when a barrier() call succeed, or\n+    // reset when a barrier() call fail due to timeout.\n+    private var barrierEpoch: Int = 0\n+\n+    // An array of RPCCallContexts for barrier tasks that are waiting for reply of a barrier()\n+    // call.\n+    private val requesters: ArrayBuffer[RpcCallContext] = new ArrayBuffer[RpcCallContext](numTasks)\n+\n+    // A timer task that ensures we may timeout for a barrier() call.\n+    private var timerTask: TimerTask = null\n+\n+    // Init a TimerTask for a barrier() call.\n+    private def initTimerTask(): Unit = {\n+      timerTask = new TimerTask {\n+        override def run(): Unit = synchronized {\n+          // Timeout current barrier() call, fail all the sync requests.\n+          requesters.foreach(_.sendFailure(new SparkException(\"The coordinator didn't get all \" +\n+            s\"barrier sync requests for barrier epoch $barrierEpoch from $barrierId within \" +\n+            s\"$timeout second(s).\")))\n+          cleanupBarrierStage(barrierId)\n+        }\n+      }\n+    }\n+\n+    // Cancel the current active TimerTask and release resources.\n+    private def cancelTimerTask(): Unit = {\n+      if (timerTask != null) {\n+        timerTask.cancel()\n+        timerTask = null\n+      }\n+    }\n+\n+    // Process the global sync request. The barrier() call succeed if collected enough requests\n+    // within a configured time, otherwise fail all the pending requests.\n+    def handleRequest(requester: RpcCallContext, request: RequestToSync): Unit = synchronized {\n+      val taskId = request.taskAttemptId\n+      val epoch = request.barrierEpoch\n+\n+      // Require the number of tasks is correctly set from the BarrierTaskContext.\n+      require(request.numTasks == numTasks, s\"Number of tasks of $barrierId is \" +\n+        s\"${request.numTasks} from Task $taskId, previously it was $numTasks.\")\n+\n+      // Check whether the epoch from the barrier tasks matches current barrierEpoch.\n+      logInfo(s\"Current barrier epoch for $barrierId is $barrierEpoch.\")\n+      if (epoch != barrierEpoch) {\n+        requester.sendFailure(new SparkException(s\"The request to sync of $barrierId with \" +\n+          s\"barrier epoch $barrierEpoch has already finished. Maybe task $taskId is not \" +\n+          \"properly killed.\"))\n+      } else {\n+        // If this is the first sync message received for a barrier() call, start timer to ensure\n+        // we may timeout for the sync.\n+        if (requesters.isEmpty) {\n+          initTimerTask()\n+          timer.schedule(timerTask, timeout * 1000L)\n+        }\n+        // Add the requester to array of RPCCallContexts pending for reply.\n+        requesters += requester\n+        logInfo(s\"Barrier sync epoch $barrierEpoch from $barrierId received update from Task \" +\n+          s\"$taskId, current progress: ${requesters.size}/$numTasks.\")\n+        if (maybeFinishAllRequesters(requesters, numTasks)) {\n+          // Finished current barrier() call successfully, clean up ContextBarrierState and\n+          // increase the barrier epoch.\n+          logInfo(s\"Barrier sync epoch $barrierEpoch from $barrierId received all updates from \" +\n+            s\"tasks, finished successfully.\")\n+          barrierEpoch += 1\n+          requesters.clear()\n+          cancelTimerTask()\n+        }\n+      }\n+    }\n+\n+    // Finish all the blocking barrier sync requests from a stage attempt successfully if we\n+    // have received all the sync requests.\n+    private def maybeFinishAllRequesters(\n+        requesters: ArrayBuffer[RpcCallContext],\n+        numTasks: Int): Boolean = {\n+      if (requesters.size == numTasks) {\n+        requesters.foreach(_.reply(()))\n+        true\n+      } else {\n+        false\n+      }\n+    }\n+\n+    // Cleanup the internal state of a barrier stage attempt.\n+    def clear(): Unit = synchronized {\n+      // The global sync fails so the stage is expected to retry another attempt, all sync\n+      // messages come from current stage attempt shall fail.\n+      barrierEpoch = -1\n+      requesters.clear()\n+      cancelTimerTask()\n+    }\n+  }\n+\n+  // Clean up the [[ContextBarrierState]] that correspond to a stage attempt.\n+  private def cleanupBarrierStage(barrierId: ContextBarrierId): Unit = {\n+    val barrierState = states.remove(barrierId)\n+    if (barrierState != null) {\n+      barrierState.clear()\n+    }\n+  }\n+\n+  override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = {\n+    case request @ RequestToSync(numTasks, stageId, stageAttemptId, _, _) =>\n+      // Get or init the ContextBarrierState correspond to the stage attempt.\n+      val barrierId = ContextBarrierId(stageId, stageAttemptId)\n+      states.putIfAbsent(barrierId, new ContextBarrierState(barrierId, numTasks))\n+      val barrierState = states.get(barrierId)\n+\n+      barrierState.handleRequest(context, request)\n+  }\n+\n+  private val clearStateConsumer = new Consumer[ContextBarrierState] {\n+    override def accept(state: ContextBarrierState) = state.clear()\n+  }\n+\n+  override def onStop(): Unit = {"
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "`correspond to a` -> `corresponds to a specific`",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-05T10:01:30Z",
    "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.function.Consumer\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * Only one barrier() call shall happen on a barrier stage attempt at each time, we can use\n+ * (stageId, stageAttemptId) to identify the stage attempt where the barrier() call is from.\n+ */\n+private case class ContextBarrierId(stageId: Int, stageAttemptId: Int) {\n+  override def toString: String = s\"Stage $stageId (Attempt $stageAttemptId)\"\n+}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  // Listen to StageCompleted event, clear corresponding ContextBarrierState.\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      val barrierId = ContextBarrierId(stageInfo.stageId, stageInfo.attemptNumber)\n+      // Clear ContextBarrierState from a finished stage attempt.\n+      cleanupBarrierStage(barrierId)\n+    }\n+  }\n+\n+  // Remember all active stage attempts that make barrier() call(s), and the corresponding\n+  // internal state.\n+  private val states = new ConcurrentHashMap[ContextBarrierId, ContextBarrierState]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Provide current state of a barrier() call, the state is created when a new stage attempt send\n+   * out a barrier() call, and recycled on stage completed.\n+   *\n+   * @param barrierId Identifier of the barrier stage that make a barrier() call.\n+   * @param numTasks Number of tasks of the barrier stage, all barrier() calls from the stage shall\n+   *                 collect `numTasks` requests to succeed.\n+   */\n+  private class ContextBarrierState(\n+      val barrierId: ContextBarrierId,\n+      val numTasks: Int) {\n+\n+    // There may be multiple barrier() calls from a barrier stage attempt, `barrierEpoch` is used\n+    // to identify each barrier() call. It shall get increased when a barrier() call succeed, or\n+    // reset when a barrier() call fail due to timeout.\n+    private var barrierEpoch: Int = 0\n+\n+    // An array of RPCCallContexts for barrier tasks that are waiting for reply of a barrier()\n+    // call.\n+    private val requesters: ArrayBuffer[RpcCallContext] = new ArrayBuffer[RpcCallContext](numTasks)\n+\n+    // A timer task that ensures we may timeout for a barrier() call.\n+    private var timerTask: TimerTask = null\n+\n+    // Init a TimerTask for a barrier() call.\n+    private def initTimerTask(): Unit = {\n+      timerTask = new TimerTask {\n+        override def run(): Unit = synchronized {\n+          // Timeout current barrier() call, fail all the sync requests.\n+          requesters.foreach(_.sendFailure(new SparkException(\"The coordinator didn't get all \" +\n+            s\"barrier sync requests for barrier epoch $barrierEpoch from $barrierId within \" +\n+            s\"$timeout second(s).\")))\n+          cleanupBarrierStage(barrierId)\n+        }\n+      }\n+    }\n+\n+    // Cancel the current active TimerTask and release resources.\n+    private def cancelTimerTask(): Unit = {\n+      if (timerTask != null) {\n+        timerTask.cancel()\n+        timerTask = null\n+      }\n+    }\n+\n+    // Process the global sync request. The barrier() call succeed if collected enough requests\n+    // within a configured time, otherwise fail all the pending requests.\n+    def handleRequest(requester: RpcCallContext, request: RequestToSync): Unit = synchronized {\n+      val taskId = request.taskAttemptId\n+      val epoch = request.barrierEpoch\n+\n+      // Require the number of tasks is correctly set from the BarrierTaskContext.\n+      require(request.numTasks == numTasks, s\"Number of tasks of $barrierId is \" +\n+        s\"${request.numTasks} from Task $taskId, previously it was $numTasks.\")\n+\n+      // Check whether the epoch from the barrier tasks matches current barrierEpoch.\n+      logInfo(s\"Current barrier epoch for $barrierId is $barrierEpoch.\")\n+      if (epoch != barrierEpoch) {\n+        requester.sendFailure(new SparkException(s\"The request to sync of $barrierId with \" +\n+          s\"barrier epoch $barrierEpoch has already finished. Maybe task $taskId is not \" +\n+          \"properly killed.\"))\n+      } else {\n+        // If this is the first sync message received for a barrier() call, start timer to ensure\n+        // we may timeout for the sync.\n+        if (requesters.isEmpty) {\n+          initTimerTask()\n+          timer.schedule(timerTask, timeout * 1000L)\n+        }\n+        // Add the requester to array of RPCCallContexts pending for reply.\n+        requesters += requester\n+        logInfo(s\"Barrier sync epoch $barrierEpoch from $barrierId received update from Task \" +\n+          s\"$taskId, current progress: ${requesters.size}/$numTasks.\")\n+        if (maybeFinishAllRequesters(requesters, numTasks)) {\n+          // Finished current barrier() call successfully, clean up ContextBarrierState and\n+          // increase the barrier epoch.\n+          logInfo(s\"Barrier sync epoch $barrierEpoch from $barrierId received all updates from \" +\n+            s\"tasks, finished successfully.\")\n+          barrierEpoch += 1\n+          requesters.clear()\n+          cancelTimerTask()\n+        }\n+      }\n+    }\n+\n+    // Finish all the blocking barrier sync requests from a stage attempt successfully if we\n+    // have received all the sync requests.\n+    private def maybeFinishAllRequesters(\n+        requesters: ArrayBuffer[RpcCallContext],\n+        numTasks: Int): Boolean = {\n+      if (requesters.size == numTasks) {\n+        requesters.foreach(_.reply(()))\n+        true\n+      } else {\n+        false\n+      }\n+    }\n+\n+    // Cleanup the internal state of a barrier stage attempt.\n+    def clear(): Unit = synchronized {\n+      // The global sync fails so the stage is expected to retry another attempt, all sync\n+      // messages come from current stage attempt shall fail.\n+      barrierEpoch = -1\n+      requesters.clear()\n+      cancelTimerTask()\n+    }\n+  }\n+\n+  // Clean up the [[ContextBarrierState]] that correspond to a stage attempt."
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "```Scala\r\nSince only one barrier() call is triggered by each barrier stage attempt, we use\r\n(stageId, stageAttemptId) to identify the stage attempt which the barrier() call is from.\r\n```",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-05T10:06:46Z",
    "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.function.Consumer\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * Only one barrier() call shall happen on a barrier stage attempt at each time, we can use\n+ * (stageId, stageAttemptId) to identify the stage attempt where the barrier() call is from."
  }, {
    "author": {
      "login": "jiangxb1987"
    },
    "body": "Actually, a barrier stage attempt can trigger multiple barrier() calls, but only one of them can be active at any time.",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-05T13:32:45Z",
    "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.function.Consumer\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * Only one barrier() call shall happen on a barrier stage attempt at each time, we can use\n+ * (stageId, stageAttemptId) to identify the stage attempt where the barrier() call is from."
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "`received all the requests for a group of `barrier()` calls`\r\n->\r\n`all the requests for a group of `barrier()` calls are received`",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-05T10:09:15Z",
    "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.function.Consumer\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * Only one barrier() call shall happen on a barrier stage attempt at each time, we can use\n+ * (stageId, stageAttemptId) to identify the stage attempt where the barrier() call is from.\n+ */\n+private case class ContextBarrierId(stageId: Int, stageAttemptId: Int) {\n+  override def toString: String = s\"Stage $stageId (Attempt $stageAttemptId)\"\n+}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect"
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "`doesn't collect` -> `is unable to collect`",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-05T10:10:14Z",
    "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.function.Consumer\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * Only one barrier() call shall happen on a barrier stage attempt at each time, we can use\n+ * (stageId, stageAttemptId) to identify the stage attempt where the barrier() call is from.\n+ */\n+private case class ContextBarrierId(stageId: Int, stageAttemptId: Int) {\n+  override def toString: String = s\"Stage $stageId (Attempt $stageAttemptId)\"\n+}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect"
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "`due to timeout` -> `and return a timeout exception`",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-05T10:17:51Z",
    "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.function.Consumer\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * Only one barrier() call shall happen on a barrier stage attempt at each time, we can use\n+ * (stageId, stageAttemptId) to identify the stage attempt where the barrier() call is from.\n+ */\n+private case class ContextBarrierId(stageId: Int, stageAttemptId: Int) {\n+  override def toString: String = s\"Stage $stageId (Attempt $stageAttemptId)\"\n+}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout."
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "`Remember` -> `Record`",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-05T10:19:46Z",
    "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.function.Consumer\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * Only one barrier() call shall happen on a barrier stage attempt at each time, we can use\n+ * (stageId, stageAttemptId) to identify the stage attempt where the barrier() call is from.\n+ */\n+private case class ContextBarrierId(stageId: Int, stageAttemptId: Int) {\n+  override def toString: String = s\"Stage $stageId (Attempt $stageAttemptId)\"\n+}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  // Listen to StageCompleted event, clear corresponding ContextBarrierState.\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      val barrierId = ContextBarrierId(stageInfo.stageId, stageInfo.attemptNumber)\n+      // Clear ContextBarrierState from a finished stage attempt.\n+      cleanupBarrierStage(barrierId)\n+    }\n+  }\n+\n+  // Remember all active stage attempts that make barrier() call(s), and the corresponding"
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "`Provide current state of a barrier() call, the state`\r\n-> `Provide the current state of a barrier() call. A state`\r\n\r\n`send` -> `sends`",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-05T10:25:15Z",
    "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.function.Consumer\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * Only one barrier() call shall happen on a barrier stage attempt at each time, we can use\n+ * (stageId, stageAttemptId) to identify the stage attempt where the barrier() call is from.\n+ */\n+private case class ContextBarrierId(stageId: Int, stageAttemptId: Int) {\n+  override def toString: String = s\"Stage $stageId (Attempt $stageAttemptId)\"\n+}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  // Listen to StageCompleted event, clear corresponding ContextBarrierState.\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      val barrierId = ContextBarrierId(stageInfo.stageId, stageInfo.attemptNumber)\n+      // Clear ContextBarrierState from a finished stage attempt.\n+      cleanupBarrierStage(barrierId)\n+    }\n+  }\n+\n+  // Remember all active stage attempts that make barrier() call(s), and the corresponding\n+  // internal state.\n+  private val states = new ConcurrentHashMap[ContextBarrierId, ContextBarrierState]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Provide current state of a barrier() call, the state is created when a new stage attempt send"
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "-> `timeoutInSecs`",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-05T10:45:01Z",
    "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.function.Consumer\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * Only one barrier() call shall happen on a barrier stage attempt at each time, we can use\n+ * (stageId, stageAttemptId) to identify the stage attempt where the barrier() call is from.\n+ */\n+private case class ContextBarrierId(stageId: Int, stageAttemptId: Int) {\n+  override def toString: String = s\"Stage $stageId (Attempt $stageAttemptId)\"\n+}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,"
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "Here, you expect we will issue the following exception after we change it to `-1`? \r\n```\r\nnew SparkException(s\"The request to sync of $barrierId with \" +\r\n          s\"barrier epoch $barrierEpoch has already finished. Maybe task $taskId is not \" +\r\n          \"properly killed.\")\r\n```",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-05T11:02:49Z",
    "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.function.Consumer\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * Only one barrier() call shall happen on a barrier stage attempt at each time, we can use\n+ * (stageId, stageAttemptId) to identify the stage attempt where the barrier() call is from.\n+ */\n+private case class ContextBarrierId(stageId: Int, stageAttemptId: Int) {\n+  override def toString: String = s\"Stage $stageId (Attempt $stageAttemptId)\"\n+}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  // Listen to StageCompleted event, clear corresponding ContextBarrierState.\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      val barrierId = ContextBarrierId(stageInfo.stageId, stageInfo.attemptNumber)\n+      // Clear ContextBarrierState from a finished stage attempt.\n+      cleanupBarrierStage(barrierId)\n+    }\n+  }\n+\n+  // Remember all active stage attempts that make barrier() call(s), and the corresponding\n+  // internal state.\n+  private val states = new ConcurrentHashMap[ContextBarrierId, ContextBarrierState]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Provide current state of a barrier() call, the state is created when a new stage attempt send\n+   * out a barrier() call, and recycled on stage completed.\n+   *\n+   * @param barrierId Identifier of the barrier stage that make a barrier() call.\n+   * @param numTasks Number of tasks of the barrier stage, all barrier() calls from the stage shall\n+   *                 collect `numTasks` requests to succeed.\n+   */\n+  private class ContextBarrierState(\n+      val barrierId: ContextBarrierId,\n+      val numTasks: Int) {\n+\n+    // There may be multiple barrier() calls from a barrier stage attempt, `barrierEpoch` is used\n+    // to identify each barrier() call. It shall get increased when a barrier() call succeed, or\n+    // reset when a barrier() call fail due to timeout.\n+    private var barrierEpoch: Int = 0\n+\n+    // An array of RPCCallContexts for barrier tasks that are waiting for reply of a barrier()\n+    // call.\n+    private val requesters: ArrayBuffer[RpcCallContext] = new ArrayBuffer[RpcCallContext](numTasks)\n+\n+    // A timer task that ensures we may timeout for a barrier() call.\n+    private var timerTask: TimerTask = null\n+\n+    // Init a TimerTask for a barrier() call.\n+    private def initTimerTask(): Unit = {\n+      timerTask = new TimerTask {\n+        override def run(): Unit = synchronized {\n+          // Timeout current barrier() call, fail all the sync requests.\n+          requesters.foreach(_.sendFailure(new SparkException(\"The coordinator didn't get all \" +\n+            s\"barrier sync requests for barrier epoch $barrierEpoch from $barrierId within \" +\n+            s\"$timeout second(s).\")))\n+          cleanupBarrierStage(barrierId)\n+        }\n+      }\n+    }\n+\n+    // Cancel the current active TimerTask and release resources.\n+    private def cancelTimerTask(): Unit = {\n+      if (timerTask != null) {\n+        timerTask.cancel()\n+        timerTask = null\n+      }\n+    }\n+\n+    // Process the global sync request. The barrier() call succeed if collected enough requests\n+    // within a configured time, otherwise fail all the pending requests.\n+    def handleRequest(requester: RpcCallContext, request: RequestToSync): Unit = synchronized {\n+      val taskId = request.taskAttemptId\n+      val epoch = request.barrierEpoch\n+\n+      // Require the number of tasks is correctly set from the BarrierTaskContext.\n+      require(request.numTasks == numTasks, s\"Number of tasks of $barrierId is \" +\n+        s\"${request.numTasks} from Task $taskId, previously it was $numTasks.\")\n+\n+      // Check whether the epoch from the barrier tasks matches current barrierEpoch.\n+      logInfo(s\"Current barrier epoch for $barrierId is $barrierEpoch.\")\n+      if (epoch != barrierEpoch) {\n+        requester.sendFailure(new SparkException(s\"The request to sync of $barrierId with \" +\n+          s\"barrier epoch $barrierEpoch has already finished. Maybe task $taskId is not \" +\n+          \"properly killed.\"))\n+      } else {\n+        // If this is the first sync message received for a barrier() call, start timer to ensure\n+        // we may timeout for the sync.\n+        if (requesters.isEmpty) {\n+          initTimerTask()\n+          timer.schedule(timerTask, timeout * 1000L)\n+        }\n+        // Add the requester to array of RPCCallContexts pending for reply.\n+        requesters += requester\n+        logInfo(s\"Barrier sync epoch $barrierEpoch from $barrierId received update from Task \" +\n+          s\"$taskId, current progress: ${requesters.size}/$numTasks.\")\n+        if (maybeFinishAllRequesters(requesters, numTasks)) {\n+          // Finished current barrier() call successfully, clean up ContextBarrierState and\n+          // increase the barrier epoch.\n+          logInfo(s\"Barrier sync epoch $barrierEpoch from $barrierId received all updates from \" +\n+            s\"tasks, finished successfully.\")\n+          barrierEpoch += 1\n+          requesters.clear()\n+          cancelTimerTask()\n+        }\n+      }\n+    }\n+\n+    // Finish all the blocking barrier sync requests from a stage attempt successfully if we\n+    // have received all the sync requests.\n+    private def maybeFinishAllRequesters(\n+        requesters: ArrayBuffer[RpcCallContext],\n+        numTasks: Int): Boolean = {\n+      if (requesters.size == numTasks) {\n+        requesters.foreach(_.reply(()))\n+        true\n+      } else {\n+        false\n+      }\n+    }\n+\n+    // Cleanup the internal state of a barrier stage attempt.\n+    def clear(): Unit = synchronized {\n+      // The global sync fails so the stage is expected to retry another attempt, all sync\n+      // messages come from current stage attempt shall fail.\n+      barrierEpoch = -1",
    "line": 186
  }, {
    "author": {
      "login": "jiangxb1987"
    },
    "body": "Yea",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-05T13:43:36Z",
    "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.function.Consumer\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * Only one barrier() call shall happen on a barrier stage attempt at each time, we can use\n+ * (stageId, stageAttemptId) to identify the stage attempt where the barrier() call is from.\n+ */\n+private case class ContextBarrierId(stageId: Int, stageAttemptId: Int) {\n+  override def toString: String = s\"Stage $stageId (Attempt $stageAttemptId)\"\n+}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * received all the requests for a group of `barrier()` calls. If the coordinator doesn't collect\n+ * enough global sync requests within a configured time, fail all the requests due to timeout.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeout: Int,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  // Listen to StageCompleted event, clear corresponding ContextBarrierState.\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      val barrierId = ContextBarrierId(stageInfo.stageId, stageInfo.attemptNumber)\n+      // Clear ContextBarrierState from a finished stage attempt.\n+      cleanupBarrierStage(barrierId)\n+    }\n+  }\n+\n+  // Remember all active stage attempts that make barrier() call(s), and the corresponding\n+  // internal state.\n+  private val states = new ConcurrentHashMap[ContextBarrierId, ContextBarrierState]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  /**\n+   * Provide current state of a barrier() call, the state is created when a new stage attempt send\n+   * out a barrier() call, and recycled on stage completed.\n+   *\n+   * @param barrierId Identifier of the barrier stage that make a barrier() call.\n+   * @param numTasks Number of tasks of the barrier stage, all barrier() calls from the stage shall\n+   *                 collect `numTasks` requests to succeed.\n+   */\n+  private class ContextBarrierState(\n+      val barrierId: ContextBarrierId,\n+      val numTasks: Int) {\n+\n+    // There may be multiple barrier() calls from a barrier stage attempt, `barrierEpoch` is used\n+    // to identify each barrier() call. It shall get increased when a barrier() call succeed, or\n+    // reset when a barrier() call fail due to timeout.\n+    private var barrierEpoch: Int = 0\n+\n+    // An array of RPCCallContexts for barrier tasks that are waiting for reply of a barrier()\n+    // call.\n+    private val requesters: ArrayBuffer[RpcCallContext] = new ArrayBuffer[RpcCallContext](numTasks)\n+\n+    // A timer task that ensures we may timeout for a barrier() call.\n+    private var timerTask: TimerTask = null\n+\n+    // Init a TimerTask for a barrier() call.\n+    private def initTimerTask(): Unit = {\n+      timerTask = new TimerTask {\n+        override def run(): Unit = synchronized {\n+          // Timeout current barrier() call, fail all the sync requests.\n+          requesters.foreach(_.sendFailure(new SparkException(\"The coordinator didn't get all \" +\n+            s\"barrier sync requests for barrier epoch $barrierEpoch from $barrierId within \" +\n+            s\"$timeout second(s).\")))\n+          cleanupBarrierStage(barrierId)\n+        }\n+      }\n+    }\n+\n+    // Cancel the current active TimerTask and release resources.\n+    private def cancelTimerTask(): Unit = {\n+      if (timerTask != null) {\n+        timerTask.cancel()\n+        timerTask = null\n+      }\n+    }\n+\n+    // Process the global sync request. The barrier() call succeed if collected enough requests\n+    // within a configured time, otherwise fail all the pending requests.\n+    def handleRequest(requester: RpcCallContext, request: RequestToSync): Unit = synchronized {\n+      val taskId = request.taskAttemptId\n+      val epoch = request.barrierEpoch\n+\n+      // Require the number of tasks is correctly set from the BarrierTaskContext.\n+      require(request.numTasks == numTasks, s\"Number of tasks of $barrierId is \" +\n+        s\"${request.numTasks} from Task $taskId, previously it was $numTasks.\")\n+\n+      // Check whether the epoch from the barrier tasks matches current barrierEpoch.\n+      logInfo(s\"Current barrier epoch for $barrierId is $barrierEpoch.\")\n+      if (epoch != barrierEpoch) {\n+        requester.sendFailure(new SparkException(s\"The request to sync of $barrierId with \" +\n+          s\"barrier epoch $barrierEpoch has already finished. Maybe task $taskId is not \" +\n+          \"properly killed.\"))\n+      } else {\n+        // If this is the first sync message received for a barrier() call, start timer to ensure\n+        // we may timeout for the sync.\n+        if (requesters.isEmpty) {\n+          initTimerTask()\n+          timer.schedule(timerTask, timeout * 1000L)\n+        }\n+        // Add the requester to array of RPCCallContexts pending for reply.\n+        requesters += requester\n+        logInfo(s\"Barrier sync epoch $barrierEpoch from $barrierId received update from Task \" +\n+          s\"$taskId, current progress: ${requesters.size}/$numTasks.\")\n+        if (maybeFinishAllRequesters(requesters, numTasks)) {\n+          // Finished current barrier() call successfully, clean up ContextBarrierState and\n+          // increase the barrier epoch.\n+          logInfo(s\"Barrier sync epoch $barrierEpoch from $barrierId received all updates from \" +\n+            s\"tasks, finished successfully.\")\n+          barrierEpoch += 1\n+          requesters.clear()\n+          cancelTimerTask()\n+        }\n+      }\n+    }\n+\n+    // Finish all the blocking barrier sync requests from a stage attempt successfully if we\n+    // have received all the sync requests.\n+    private def maybeFinishAllRequesters(\n+        requesters: ArrayBuffer[RpcCallContext],\n+        numTasks: Int): Boolean = {\n+      if (requesters.size == numTasks) {\n+        requesters.foreach(_.reply(()))\n+        true\n+      } else {\n+        false\n+      }\n+    }\n+\n+    // Cleanup the internal state of a barrier stage attempt.\n+    def clear(): Unit = synchronized {\n+      // The global sync fails so the stage is expected to retry another attempt, all sync\n+      // messages come from current stage attempt shall fail.\n+      barrierEpoch = -1",
    "line": 186
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "kiszk"
    },
    "body": "Is it better to use `computeIfAbsent` to avoid allocating `ContextBarrierState` when an entry with `barrierId` exists?",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-05T17:34:45Z",
    "diffHunk": "@@ -0,0 +1,230 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.function.Consumer\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * For each barrier stage attempt, only at most one barrier() call can be active at any time, thus\n+ * we can use (stageId, stageAttemptId) to identify the stage attempt where the barrier() call is\n+ * from.\n+ */\n+private case class ContextBarrierId(stageId: Int, stageAttemptId: Int) {\n+  override def toString: String = s\"Stage $stageId (Attempt $stageAttemptId)\"\n+}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * all the requests for a group of `barrier()` calls are received. If the coordinator is unable to\n+ * collect enough global sync requests within a configured time, fail all the requests and return\n+ * an Exception with timeout message.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeoutInSecs: Long,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  // Listen to StageCompleted event, clear corresponding ContextBarrierState.\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      val barrierId = ContextBarrierId(stageInfo.stageId, stageInfo.attemptNumber)\n+      // Clear ContextBarrierState from a finished stage attempt.\n+      cleanupBarrierStage(barrierId)\n+    }\n+  }\n+\n+  // Record all active stage attempts that make barrier() call(s), and the corresponding internal\n+  // state.\n+  private val states = new ConcurrentHashMap[ContextBarrierId, ContextBarrierState]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  override def onStop(): Unit = {\n+    try {\n+      states.forEachValue(1, clearStateConsumer)\n+      states.clear()\n+      listenerBus.removeListener(listener)\n+    } finally {\n+      super.onStop()\n+    }\n+  }\n+\n+  /**\n+   * Provide the current state of a barrier() call. A state is created when a new stage attempt\n+   * sends out a barrier() call, and recycled on stage completed.\n+   *\n+   * @param barrierId Identifier of the barrier stage that make a barrier() call.\n+   * @param numTasks Number of tasks of the barrier stage, all barrier() calls from the stage shall\n+   *                 collect `numTasks` requests to succeed.\n+   */\n+  private class ContextBarrierState(\n+      val barrierId: ContextBarrierId,\n+      val numTasks: Int) {\n+\n+    // There may be multiple barrier() calls from a barrier stage attempt, `barrierEpoch` is used\n+    // to identify each barrier() call. It shall get increased when a barrier() call succeed, or\n+    // reset when a barrier() call fail due to timeout.\n+    private var barrierEpoch: Int = 0\n+\n+    // An array of RPCCallContexts for barrier tasks that are waiting for reply of a barrier()\n+    // call.\n+    private val requesters: ArrayBuffer[RpcCallContext] = new ArrayBuffer[RpcCallContext](numTasks)\n+\n+    // A timer task that ensures we may timeout for a barrier() call.\n+    private var timerTask: TimerTask = null\n+\n+    // Init a TimerTask for a barrier() call.\n+    private def initTimerTask(): Unit = {\n+      timerTask = new TimerTask {\n+        override def run(): Unit = synchronized {\n+          // Timeout current barrier() call, fail all the sync requests.\n+          requesters.foreach(_.sendFailure(new SparkException(\"The coordinator didn't get all \" +\n+            s\"barrier sync requests for barrier epoch $barrierEpoch from $barrierId within \" +\n+            s\"$timeoutInSecs second(s).\")))\n+          cleanupBarrierStage(barrierId)\n+        }\n+      }\n+    }\n+\n+    // Cancel the current active TimerTask and release resources.\n+    private def cancelTimerTask(): Unit = {\n+      if (timerTask != null) {\n+        timerTask.cancel()\n+        timerTask = null\n+      }\n+    }\n+\n+    // Process the global sync request. The barrier() call succeed if collected enough requests\n+    // within a configured time, otherwise fail all the pending requests.\n+    def handleRequest(requester: RpcCallContext, request: RequestToSync): Unit = synchronized {\n+      val taskId = request.taskAttemptId\n+      val epoch = request.barrierEpoch\n+\n+      // Require the number of tasks is correctly set from the BarrierTaskContext.\n+      require(request.numTasks == numTasks, s\"Number of tasks of $barrierId is \" +\n+        s\"${request.numTasks} from Task $taskId, previously it was $numTasks.\")\n+\n+      // Check whether the epoch from the barrier tasks matches current barrierEpoch.\n+      logInfo(s\"Current barrier epoch for $barrierId is $barrierEpoch.\")\n+      if (epoch != barrierEpoch) {\n+        requester.sendFailure(new SparkException(s\"The request to sync of $barrierId with \" +\n+          s\"barrier epoch $barrierEpoch has already finished. Maybe task $taskId is not \" +\n+          \"properly killed.\"))\n+      } else {\n+        // If this is the first sync message received for a barrier() call, start timer to ensure\n+        // we may timeout for the sync.\n+        if (requesters.isEmpty) {\n+          initTimerTask()\n+          timer.schedule(timerTask, timeoutInSecs * 1000)\n+        }\n+        // Add the requester to array of RPCCallContexts pending for reply.\n+        requesters += requester\n+        logInfo(s\"Barrier sync epoch $barrierEpoch from $barrierId received update from Task \" +\n+          s\"$taskId, current progress: ${requesters.size}/$numTasks.\")\n+        if (maybeFinishAllRequesters(requesters, numTasks)) {\n+          // Finished current barrier() call successfully, clean up ContextBarrierState and\n+          // increase the barrier epoch.\n+          logInfo(s\"Barrier sync epoch $barrierEpoch from $barrierId received all updates from \" +\n+            s\"tasks, finished successfully.\")\n+          barrierEpoch += 1\n+          requesters.clear()\n+          cancelTimerTask()\n+        }\n+      }\n+    }\n+\n+    // Finish all the blocking barrier sync requests from a stage attempt successfully if we\n+    // have received all the sync requests.\n+    private def maybeFinishAllRequesters(\n+        requesters: ArrayBuffer[RpcCallContext],\n+        numTasks: Int): Boolean = {\n+      if (requesters.size == numTasks) {\n+        requesters.foreach(_.reply(()))\n+        true\n+      } else {\n+        false\n+      }\n+    }\n+\n+    // Cleanup the internal state of a barrier stage attempt.\n+    def clear(): Unit = synchronized {\n+      // The global sync fails so the stage is expected to retry another attempt, all sync\n+      // messages come from current stage attempt shall fail.\n+      barrierEpoch = -1\n+      requesters.clear()\n+      cancelTimerTask()\n+    }\n+  }\n+\n+  // Clean up the [[ContextBarrierState]] that correspond to a specific stage attempt.\n+  private def cleanupBarrierStage(barrierId: ContextBarrierId): Unit = {\n+    val barrierState = states.remove(barrierId)\n+    if (barrierState != null) {\n+      barrierState.clear()\n+    }\n+  }\n+\n+  override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = {\n+    case request @ RequestToSync(numTasks, stageId, stageAttemptId, _, _) =>\n+      // Get or init the ContextBarrierState correspond to the stage attempt.\n+      val barrierId = ContextBarrierId(stageId, stageAttemptId)\n+      states.putIfAbsent(barrierId, new ContextBarrierState(barrierId, numTasks))"
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "kiszk"
    },
    "body": "nit: `succeed` ->`succeeds`",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-05T17:37:03Z",
    "diffHunk": "@@ -0,0 +1,230 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.function.Consumer\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * For each barrier stage attempt, only at most one barrier() call can be active at any time, thus\n+ * we can use (stageId, stageAttemptId) to identify the stage attempt where the barrier() call is\n+ * from.\n+ */\n+private case class ContextBarrierId(stageId: Int, stageAttemptId: Int) {\n+  override def toString: String = s\"Stage $stageId (Attempt $stageAttemptId)\"\n+}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * all the requests for a group of `barrier()` calls are received. If the coordinator is unable to\n+ * collect enough global sync requests within a configured time, fail all the requests and return\n+ * an Exception with timeout message.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeoutInSecs: Long,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  // Listen to StageCompleted event, clear corresponding ContextBarrierState.\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      val barrierId = ContextBarrierId(stageInfo.stageId, stageInfo.attemptNumber)\n+      // Clear ContextBarrierState from a finished stage attempt.\n+      cleanupBarrierStage(barrierId)\n+    }\n+  }\n+\n+  // Record all active stage attempts that make barrier() call(s), and the corresponding internal\n+  // state.\n+  private val states = new ConcurrentHashMap[ContextBarrierId, ContextBarrierState]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  override def onStop(): Unit = {\n+    try {\n+      states.forEachValue(1, clearStateConsumer)\n+      states.clear()\n+      listenerBus.removeListener(listener)\n+    } finally {\n+      super.onStop()\n+    }\n+  }\n+\n+  /**\n+   * Provide the current state of a barrier() call. A state is created when a new stage attempt\n+   * sends out a barrier() call, and recycled on stage completed.\n+   *\n+   * @param barrierId Identifier of the barrier stage that make a barrier() call.\n+   * @param numTasks Number of tasks of the barrier stage, all barrier() calls from the stage shall\n+   *                 collect `numTasks` requests to succeed.\n+   */\n+  private class ContextBarrierState(\n+      val barrierId: ContextBarrierId,\n+      val numTasks: Int) {\n+\n+    // There may be multiple barrier() calls from a barrier stage attempt, `barrierEpoch` is used\n+    // to identify each barrier() call. It shall get increased when a barrier() call succeed, or"
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "kiszk"
    },
    "body": "nit: `fail` -> `fails`",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-05T17:37:24Z",
    "diffHunk": "@@ -0,0 +1,230 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.function.Consumer\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * For each barrier stage attempt, only at most one barrier() call can be active at any time, thus\n+ * we can use (stageId, stageAttemptId) to identify the stage attempt where the barrier() call is\n+ * from.\n+ */\n+private case class ContextBarrierId(stageId: Int, stageAttemptId: Int) {\n+  override def toString: String = s\"Stage $stageId (Attempt $stageAttemptId)\"\n+}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * all the requests for a group of `barrier()` calls are received. If the coordinator is unable to\n+ * collect enough global sync requests within a configured time, fail all the requests and return\n+ * an Exception with timeout message.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeoutInSecs: Long,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  // Listen to StageCompleted event, clear corresponding ContextBarrierState.\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      val barrierId = ContextBarrierId(stageInfo.stageId, stageInfo.attemptNumber)\n+      // Clear ContextBarrierState from a finished stage attempt.\n+      cleanupBarrierStage(barrierId)\n+    }\n+  }\n+\n+  // Record all active stage attempts that make barrier() call(s), and the corresponding internal\n+  // state.\n+  private val states = new ConcurrentHashMap[ContextBarrierId, ContextBarrierState]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  override def onStop(): Unit = {\n+    try {\n+      states.forEachValue(1, clearStateConsumer)\n+      states.clear()\n+      listenerBus.removeListener(listener)\n+    } finally {\n+      super.onStop()\n+    }\n+  }\n+\n+  /**\n+   * Provide the current state of a barrier() call. A state is created when a new stage attempt\n+   * sends out a barrier() call, and recycled on stage completed.\n+   *\n+   * @param barrierId Identifier of the barrier stage that make a barrier() call.\n+   * @param numTasks Number of tasks of the barrier stage, all barrier() calls from the stage shall\n+   *                 collect `numTasks` requests to succeed.\n+   */\n+  private class ContextBarrierState(\n+      val barrierId: ContextBarrierId,\n+      val numTasks: Int) {\n+\n+    // There may be multiple barrier() calls from a barrier stage attempt, `barrierEpoch` is used\n+    // to identify each barrier() call. It shall get increased when a barrier() call succeed, or\n+    // reset when a barrier() call fail due to timeout."
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "kiszk"
    },
    "body": "nit: `consists` -> `consist`",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-05T17:41:42Z",
    "diffHunk": "@@ -0,0 +1,230 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.function.Consumer\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * For each barrier stage attempt, only at most one barrier() call can be active at any time, thus\n+ * we can use (stageId, stageAttemptId) to identify the stage attempt where the barrier() call is\n+ * from.\n+ */\n+private case class ContextBarrierId(stageId: Int, stageAttemptId: Int) {\n+  override def toString: String = s\"Stage $stageId (Attempt $stageAttemptId)\"\n+}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * all the requests for a group of `barrier()` calls are received. If the coordinator is unable to\n+ * collect enough global sync requests within a configured time, fail all the requests and return\n+ * an Exception with timeout message.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeoutInSecs: Long,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")\n+\n+  // Listen to StageCompleted event, clear corresponding ContextBarrierState.\n+  private val listener = new SparkListener {\n+    override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n+      val stageInfo = stageCompleted.stageInfo\n+      val barrierId = ContextBarrierId(stageInfo.stageId, stageInfo.attemptNumber)\n+      // Clear ContextBarrierState from a finished stage attempt.\n+      cleanupBarrierStage(barrierId)\n+    }\n+  }\n+\n+  // Record all active stage attempts that make barrier() call(s), and the corresponding internal\n+  // state.\n+  private val states = new ConcurrentHashMap[ContextBarrierId, ContextBarrierState]\n+\n+  override def onStart(): Unit = {\n+    super.onStart()\n+    listenerBus.addToStatusQueue(listener)\n+  }\n+\n+  override def onStop(): Unit = {\n+    try {\n+      states.forEachValue(1, clearStateConsumer)\n+      states.clear()\n+      listenerBus.removeListener(listener)\n+    } finally {\n+      super.onStop()\n+    }\n+  }\n+\n+  /**\n+   * Provide the current state of a barrier() call. A state is created when a new stage attempt\n+   * sends out a barrier() call, and recycled on stage completed.\n+   *\n+   * @param barrierId Identifier of the barrier stage that make a barrier() call.\n+   * @param numTasks Number of tasks of the barrier stage, all barrier() calls from the stage shall\n+   *                 collect `numTasks` requests to succeed.\n+   */\n+  private class ContextBarrierState(\n+      val barrierId: ContextBarrierId,\n+      val numTasks: Int) {\n+\n+    // There may be multiple barrier() calls from a barrier stage attempt, `barrierEpoch` is used\n+    // to identify each barrier() call. It shall get increased when a barrier() call succeed, or\n+    // reset when a barrier() call fail due to timeout.\n+    private var barrierEpoch: Int = 0\n+\n+    // An array of RPCCallContexts for barrier tasks that are waiting for reply of a barrier()\n+    // call.\n+    private val requesters: ArrayBuffer[RpcCallContext] = new ArrayBuffer[RpcCallContext](numTasks)\n+\n+    // A timer task that ensures we may timeout for a barrier() call.\n+    private var timerTask: TimerTask = null\n+\n+    // Init a TimerTask for a barrier() call.\n+    private def initTimerTask(): Unit = {\n+      timerTask = new TimerTask {\n+        override def run(): Unit = synchronized {\n+          // Timeout current barrier() call, fail all the sync requests.\n+          requesters.foreach(_.sendFailure(new SparkException(\"The coordinator didn't get all \" +\n+            s\"barrier sync requests for barrier epoch $barrierEpoch from $barrierId within \" +\n+            s\"$timeoutInSecs second(s).\")))\n+          cleanupBarrierStage(barrierId)\n+        }\n+      }\n+    }\n+\n+    // Cancel the current active TimerTask and release resources.\n+    private def cancelTimerTask(): Unit = {\n+      if (timerTask != null) {\n+        timerTask.cancel()\n+        timerTask = null\n+      }\n+    }\n+\n+    // Process the global sync request. The barrier() call succeed if collected enough requests\n+    // within a configured time, otherwise fail all the pending requests.\n+    def handleRequest(requester: RpcCallContext, request: RequestToSync): Unit = synchronized {\n+      val taskId = request.taskAttemptId\n+      val epoch = request.barrierEpoch\n+\n+      // Require the number of tasks is correctly set from the BarrierTaskContext.\n+      require(request.numTasks == numTasks, s\"Number of tasks of $barrierId is \" +\n+        s\"${request.numTasks} from Task $taskId, previously it was $numTasks.\")\n+\n+      // Check whether the epoch from the barrier tasks matches current barrierEpoch.\n+      logInfo(s\"Current barrier epoch for $barrierId is $barrierEpoch.\")\n+      if (epoch != barrierEpoch) {\n+        requester.sendFailure(new SparkException(s\"The request to sync of $barrierId with \" +\n+          s\"barrier epoch $barrierEpoch has already finished. Maybe task $taskId is not \" +\n+          \"properly killed.\"))\n+      } else {\n+        // If this is the first sync message received for a barrier() call, start timer to ensure\n+        // we may timeout for the sync.\n+        if (requesters.isEmpty) {\n+          initTimerTask()\n+          timer.schedule(timerTask, timeoutInSecs * 1000)\n+        }\n+        // Add the requester to array of RPCCallContexts pending for reply.\n+        requesters += requester\n+        logInfo(s\"Barrier sync epoch $barrierEpoch from $barrierId received update from Task \" +\n+          s\"$taskId, current progress: ${requesters.size}/$numTasks.\")\n+        if (maybeFinishAllRequesters(requesters, numTasks)) {\n+          // Finished current barrier() call successfully, clean up ContextBarrierState and\n+          // increase the barrier epoch.\n+          logInfo(s\"Barrier sync epoch $barrierEpoch from $barrierId received all updates from \" +\n+            s\"tasks, finished successfully.\")\n+          barrierEpoch += 1\n+          requesters.clear()\n+          cancelTimerTask()\n+        }\n+      }\n+    }\n+\n+    // Finish all the blocking barrier sync requests from a stage attempt successfully if we\n+    // have received all the sync requests.\n+    private def maybeFinishAllRequesters(\n+        requesters: ArrayBuffer[RpcCallContext],\n+        numTasks: Int): Boolean = {\n+      if (requesters.size == numTasks) {\n+        requesters.foreach(_.reply(()))\n+        true\n+      } else {\n+        false\n+      }\n+    }\n+\n+    // Cleanup the internal state of a barrier stage attempt.\n+    def clear(): Unit = synchronized {\n+      // The global sync fails so the stage is expected to retry another attempt, all sync\n+      // messages come from current stage attempt shall fail.\n+      barrierEpoch = -1\n+      requesters.clear()\n+      cancelTimerTask()\n+    }\n+  }\n+\n+  // Clean up the [[ContextBarrierState]] that correspond to a specific stage attempt.\n+  private def cleanupBarrierStage(barrierId: ContextBarrierId): Unit = {\n+    val barrierState = states.remove(barrierId)\n+    if (barrierState != null) {\n+      barrierState.clear()\n+    }\n+  }\n+\n+  override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = {\n+    case request @ RequestToSync(numTasks, stageId, stageAttemptId, _, _) =>\n+      // Get or init the ContextBarrierState correspond to the stage attempt.\n+      val barrierId = ContextBarrierId(stageId, stageAttemptId)\n+      states.putIfAbsent(barrierId, new ContextBarrierState(barrierId, numTasks))\n+      val barrierState = states.get(barrierId)\n+\n+      barrierState.handleRequest(context, request)\n+  }\n+\n+  private val clearStateConsumer = new Consumer[ContextBarrierState] {\n+    override def accept(state: ContextBarrierState) = state.clear()\n+  }\n+}\n+\n+private[spark] sealed trait BarrierCoordinatorMessage extends Serializable\n+\n+/**\n+ * A global sync request message from BarrierTaskContext, by `barrier()` call. Each request is\n+ * identified by stageId + stageAttemptId + barrierEpoch.\n+ *\n+ * @param numTasks The number of global sync requests the BarrierCoordinator shall receive\n+ * @param stageId ID of current stage\n+ * @param stageAttemptId ID of current stage attempt\n+ * @param taskAttemptId Unique ID of current task\n+ * @param barrierEpoch ID of the `barrier()` call, a task may consists multiple `barrier()` calls."
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "kiszk"
    },
    "body": "Will we identify the underlying reason before merging to master? ",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-06T08:45:51Z",
    "diffHunk": "@@ -0,0 +1,230 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.function.Consumer\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * For each barrier stage attempt, only at most one barrier() call can be active at any time, thus\n+ * we can use (stageId, stageAttemptId) to identify the stage attempt where the barrier() call is\n+ * from.\n+ */\n+private case class ContextBarrierId(stageId: Int, stageAttemptId: Int) {\n+  override def toString: String = s\"Stage $stageId (Attempt $stageAttemptId)\"\n+}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * all the requests for a group of `barrier()` calls are received. If the coordinator is unable to\n+ * collect enough global sync requests within a configured time, fail all the requests and return\n+ * an Exception with timeout message.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeoutInSecs: Long,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private lazy val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")"
  }, {
    "author": {
      "login": "jiangxb1987"
    },
    "body": "This is certainly a potential bug in `SparkSubmit` and not related to the changes made in this PR, I don't feel it shall block this PR.",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-06T09:17:07Z",
    "diffHunk": "@@ -0,0 +1,230 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.function.Consumer\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * For each barrier stage attempt, only at most one barrier() call can be active at any time, thus\n+ * we can use (stageId, stageAttemptId) to identify the stage attempt where the barrier() call is\n+ * from.\n+ */\n+private case class ContextBarrierId(stageId: Int, stageAttemptId: Int) {\n+  override def toString: String = s\"Stage $stageId (Attempt $stageAttemptId)\"\n+}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * all the requests for a group of `barrier()` calls are received. If the coordinator is unable to\n+ * collect enough global sync requests within a configured time, fail all the requests and return\n+ * an Exception with timeout message.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeoutInSecs: Long,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private lazy val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")"
  }, {
    "author": {
      "login": "jiangxb1987"
    },
    "body": "I opened https://issues.apache.org/jira/browse/SPARK-25030 to track the issue.",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-06T09:29:34Z",
    "diffHunk": "@@ -0,0 +1,230 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.function.Consumer\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * For each barrier stage attempt, only at most one barrier() call can be active at any time, thus\n+ * we can use (stageId, stageAttemptId) to identify the stage attempt where the barrier() call is\n+ * from.\n+ */\n+private case class ContextBarrierId(stageId: Int, stageAttemptId: Int) {\n+  override def toString: String = s\"Stage $stageId (Attempt $stageAttemptId)\"\n+}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * all the requests for a group of `barrier()` calls are received. If the coordinator is unable to\n+ * collect enough global sync requests within a configured time, fail all the requests and return\n+ * an Exception with timeout message.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeoutInSecs: Long,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private lazy val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")"
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "Add a comment above this line?",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-06T10:36:01Z",
    "diffHunk": "@@ -0,0 +1,230 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.{Timer, TimerTask}\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.function.Consumer\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}\n+import org.apache.spark.scheduler.{LiveListenerBus, SparkListener, SparkListenerStageCompleted}\n+\n+/**\n+ * For each barrier stage attempt, only at most one barrier() call can be active at any time, thus\n+ * we can use (stageId, stageAttemptId) to identify the stage attempt where the barrier() call is\n+ * from.\n+ */\n+private case class ContextBarrierId(stageId: Int, stageAttemptId: Int) {\n+  override def toString: String = s\"Stage $stageId (Attempt $stageAttemptId)\"\n+}\n+\n+/**\n+ * A coordinator that handles all global sync requests from BarrierTaskContext. Each global sync\n+ * request is generated by `BarrierTaskContext.barrier()`, and identified by\n+ * stageId + stageAttemptId + barrierEpoch. Reply all the blocking global sync requests upon\n+ * all the requests for a group of `barrier()` calls are received. If the coordinator is unable to\n+ * collect enough global sync requests within a configured time, fail all the requests and return\n+ * an Exception with timeout message.\n+ */\n+private[spark] class BarrierCoordinator(\n+    timeoutInSecs: Long,\n+    listenerBus: LiveListenerBus,\n+    override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint with Logging {\n+\n+  private lazy val timer = new Timer(\"BarrierCoordinator barrier epoch increment timer\")"
  }],
  "prId": 21898
}]