[{
  "comments": [{
    "author": {
      "login": "davies"
    },
    "body": "createAll() ?\n",
    "commit": "9f964f26effd66af83acb44c1e531d0c0df09f6b",
    "createdAt": "2016-01-20T20:10:45Z",
    "diffHunk": "@@ -17,42 +17,193 @@\n \n package org.apache.spark\n \n+import org.apache.spark.storage.{BlockId, BlockStatus}\n \n-// This is moved to its own file because many more things will be added to it in SPARK-10620.\n+\n+/**\n+ * A collection of fields and methods concerned with internal accumulators that represent\n+ * task level metrics.\n+ */\n private[spark] object InternalAccumulator {\n-  val PEAK_EXECUTION_MEMORY = \"peakExecutionMemory\"\n-  val TEST_ACCUMULATOR = \"testAccumulator\"\n-\n-  // For testing only.\n-  // This needs to be a def since we don't want to reuse the same accumulator across stages.\n-  private def maybeTestAccumulator: Option[Accumulator[Long]] = {\n-    if (sys.props.contains(\"spark.testing\")) {\n-      Some(new Accumulator(\n-        0L, AccumulatorParam.LongAccumulatorParam, Some(TEST_ACCUMULATOR), internal = true))\n-    } else {\n-      None\n+\n+  import AccumulatorParam._\n+\n+  // Prefixes used in names of internal task level metrics\n+  val METRICS_PREFIX = \"internal.metrics.\"\n+  val SHUFFLE_READ_METRICS_PREFIX = METRICS_PREFIX + \"shuffle.read.\"\n+  val SHUFFLE_WRITE_METRICS_PREFIX = METRICS_PREFIX + \"shuffle.write.\"\n+  val OUTPUT_METRICS_PREFIX = METRICS_PREFIX + \"output.\"\n+  val INPUT_METRICS_PREFIX = METRICS_PREFIX + \"input.\"\n+\n+  // Names of internal task level metrics\n+  val EXECUTOR_DESERIALIZE_TIME = METRICS_PREFIX + \"executorDeserializeTime\"\n+  val EXECUTOR_RUN_TIME = METRICS_PREFIX + \"executorRunTime\"\n+  val RESULT_SIZE = METRICS_PREFIX + \"resultSize\"\n+  val JVM_GC_TIME = METRICS_PREFIX + \"jvmGCTime\"\n+  val RESULT_SERIALIZATION_TIME = METRICS_PREFIX + \"resultSerializationTime\"\n+  val MEMORY_BYTES_SPILLED = METRICS_PREFIX + \"memoryBytesSpilled\"\n+  val DISK_BYTES_SPILLED = METRICS_PREFIX + \"diskBytesSpilled\"\n+  val PEAK_EXECUTION_MEMORY = METRICS_PREFIX + \"peakExecutionMemory\"\n+  val UPDATED_BLOCK_STATUSES = METRICS_PREFIX + \"updatedBlockStatuses\"\n+  val TEST_ACCUM = METRICS_PREFIX + \"testAccumulator\"\n+\n+  // scalastyle:off\n+\n+  // Names of shuffle read metrics\n+  object shuffleRead {\n+    val REMOTE_BLOCKS_FETCHED = SHUFFLE_READ_METRICS_PREFIX + \"remoteBlocksFetched\"\n+    val LOCAL_BLOCKS_FETCHED = SHUFFLE_READ_METRICS_PREFIX + \"localBlocksFetched\"\n+    val REMOTE_BYTES_READ = SHUFFLE_READ_METRICS_PREFIX + \"remoteBytesRead\"\n+    val LOCAL_BYTES_READ = SHUFFLE_READ_METRICS_PREFIX + \"localBytesRead\"\n+    val FETCH_WAIT_TIME = SHUFFLE_READ_METRICS_PREFIX + \"fetchWaitTime\"\n+    val RECORDS_READ = SHUFFLE_READ_METRICS_PREFIX + \"recordsRead\"\n+  }\n+\n+  // Names of shuffle write metrics\n+  object shuffleWrite {\n+    val BYTES_WRITTEN = SHUFFLE_WRITE_METRICS_PREFIX + \"bytesWritten\"\n+    val RECORDS_WRITTEN = SHUFFLE_WRITE_METRICS_PREFIX + \"recordsWritten\"\n+    val WRITE_TIME = SHUFFLE_WRITE_METRICS_PREFIX + \"writeTime\"\n+  }\n+\n+  // Names of output metrics\n+  object output {\n+    val WRITE_METHOD = OUTPUT_METRICS_PREFIX + \"writeMethod\"\n+    val BYTES_WRITTEN = OUTPUT_METRICS_PREFIX + \"bytesWritten\"\n+    val RECORDS_WRITTEN = OUTPUT_METRICS_PREFIX + \"recordsWritten\"\n+  }\n+\n+  // Names of input metrics\n+  object input {\n+    val READ_METHOD = INPUT_METRICS_PREFIX + \"readMethod\"\n+    val BYTES_READ = INPUT_METRICS_PREFIX + \"bytesRead\"\n+    val RECORDS_READ = INPUT_METRICS_PREFIX + \"recordsRead\"\n+  }\n+\n+  // scalastyle:on\n+\n+  /**\n+   * Create an internal [[Accumulator]] by name, which must begin with [[METRICS_PREFIX]].\n+   */\n+  def create(name: String): Accumulator[_] = {\n+    assert(name.startsWith(METRICS_PREFIX),\n+      s\"internal accumulator name must start with '$METRICS_PREFIX': $name\")\n+    getParam(name) match {\n+      case p @ LongAccumulatorParam => newMetric[Long](0L, name, p)\n+      case p @ IntAccumulatorParam => newMetric[Int](0, name, p)\n+      case p @ StringAccumulatorParam => newMetric[String](\"\", name, p)\n+      case p @ UpdatedBlockStatusesAccumulatorParam =>\n+        newMetric[Seq[(BlockId, BlockStatus)]](Seq(), name, p)\n+      case p => throw new IllegalArgumentException(\n+        s\"unsupported accumulator param '${p.getClass.getSimpleName}' for metric '$name'.\")\n+    }\n+  }\n+\n+  /**\n+   * Get the [[AccumulatorParam]] associated with the internal metric name,\n+   * which must begin with [[METRICS_PREFIX]].\n+   */\n+  def getParam(name: String): AccumulatorParam[_] = {\n+    assert(name.startsWith(METRICS_PREFIX),\n+      s\"internal accumulator name must start with '$METRICS_PREFIX': $name\")\n+    name match {\n+      case UPDATED_BLOCK_STATUSES => UpdatedBlockStatusesAccumulatorParam\n+      case shuffleRead.LOCAL_BLOCKS_FETCHED => IntAccumulatorParam\n+      case shuffleRead.REMOTE_BLOCKS_FETCHED => IntAccumulatorParam\n+      case input.READ_METHOD => StringAccumulatorParam\n+      case output.WRITE_METHOD => StringAccumulatorParam\n+      case _ => LongAccumulatorParam\n     }\n   }\n \n   /**\n    * Accumulators for tracking internal metrics.\n+   */\n+  def create(): Seq[Accumulator[_]] = {",
    "line": 118
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "I'd also be cool with renaming this from `create` to `createAll` or `createAccumulators` or something plural, since right now I suppose that `InternalAccumulator.create()` might be mistook for a factory method which returns a single instance of `InternalAccumulator`. This is a really minor nit, though.\n",
    "commit": "9f964f26effd66af83acb44c1e531d0c0df09f6b",
    "createdAt": "2016-01-26T07:44:55Z",
    "diffHunk": "@@ -17,42 +17,193 @@\n \n package org.apache.spark\n \n+import org.apache.spark.storage.{BlockId, BlockStatus}\n \n-// This is moved to its own file because many more things will be added to it in SPARK-10620.\n+\n+/**\n+ * A collection of fields and methods concerned with internal accumulators that represent\n+ * task level metrics.\n+ */\n private[spark] object InternalAccumulator {\n-  val PEAK_EXECUTION_MEMORY = \"peakExecutionMemory\"\n-  val TEST_ACCUMULATOR = \"testAccumulator\"\n-\n-  // For testing only.\n-  // This needs to be a def since we don't want to reuse the same accumulator across stages.\n-  private def maybeTestAccumulator: Option[Accumulator[Long]] = {\n-    if (sys.props.contains(\"spark.testing\")) {\n-      Some(new Accumulator(\n-        0L, AccumulatorParam.LongAccumulatorParam, Some(TEST_ACCUMULATOR), internal = true))\n-    } else {\n-      None\n+\n+  import AccumulatorParam._\n+\n+  // Prefixes used in names of internal task level metrics\n+  val METRICS_PREFIX = \"internal.metrics.\"\n+  val SHUFFLE_READ_METRICS_PREFIX = METRICS_PREFIX + \"shuffle.read.\"\n+  val SHUFFLE_WRITE_METRICS_PREFIX = METRICS_PREFIX + \"shuffle.write.\"\n+  val OUTPUT_METRICS_PREFIX = METRICS_PREFIX + \"output.\"\n+  val INPUT_METRICS_PREFIX = METRICS_PREFIX + \"input.\"\n+\n+  // Names of internal task level metrics\n+  val EXECUTOR_DESERIALIZE_TIME = METRICS_PREFIX + \"executorDeserializeTime\"\n+  val EXECUTOR_RUN_TIME = METRICS_PREFIX + \"executorRunTime\"\n+  val RESULT_SIZE = METRICS_PREFIX + \"resultSize\"\n+  val JVM_GC_TIME = METRICS_PREFIX + \"jvmGCTime\"\n+  val RESULT_SERIALIZATION_TIME = METRICS_PREFIX + \"resultSerializationTime\"\n+  val MEMORY_BYTES_SPILLED = METRICS_PREFIX + \"memoryBytesSpilled\"\n+  val DISK_BYTES_SPILLED = METRICS_PREFIX + \"diskBytesSpilled\"\n+  val PEAK_EXECUTION_MEMORY = METRICS_PREFIX + \"peakExecutionMemory\"\n+  val UPDATED_BLOCK_STATUSES = METRICS_PREFIX + \"updatedBlockStatuses\"\n+  val TEST_ACCUM = METRICS_PREFIX + \"testAccumulator\"\n+\n+  // scalastyle:off\n+\n+  // Names of shuffle read metrics\n+  object shuffleRead {\n+    val REMOTE_BLOCKS_FETCHED = SHUFFLE_READ_METRICS_PREFIX + \"remoteBlocksFetched\"\n+    val LOCAL_BLOCKS_FETCHED = SHUFFLE_READ_METRICS_PREFIX + \"localBlocksFetched\"\n+    val REMOTE_BYTES_READ = SHUFFLE_READ_METRICS_PREFIX + \"remoteBytesRead\"\n+    val LOCAL_BYTES_READ = SHUFFLE_READ_METRICS_PREFIX + \"localBytesRead\"\n+    val FETCH_WAIT_TIME = SHUFFLE_READ_METRICS_PREFIX + \"fetchWaitTime\"\n+    val RECORDS_READ = SHUFFLE_READ_METRICS_PREFIX + \"recordsRead\"\n+  }\n+\n+  // Names of shuffle write metrics\n+  object shuffleWrite {\n+    val BYTES_WRITTEN = SHUFFLE_WRITE_METRICS_PREFIX + \"bytesWritten\"\n+    val RECORDS_WRITTEN = SHUFFLE_WRITE_METRICS_PREFIX + \"recordsWritten\"\n+    val WRITE_TIME = SHUFFLE_WRITE_METRICS_PREFIX + \"writeTime\"\n+  }\n+\n+  // Names of output metrics\n+  object output {\n+    val WRITE_METHOD = OUTPUT_METRICS_PREFIX + \"writeMethod\"\n+    val BYTES_WRITTEN = OUTPUT_METRICS_PREFIX + \"bytesWritten\"\n+    val RECORDS_WRITTEN = OUTPUT_METRICS_PREFIX + \"recordsWritten\"\n+  }\n+\n+  // Names of input metrics\n+  object input {\n+    val READ_METHOD = INPUT_METRICS_PREFIX + \"readMethod\"\n+    val BYTES_READ = INPUT_METRICS_PREFIX + \"bytesRead\"\n+    val RECORDS_READ = INPUT_METRICS_PREFIX + \"recordsRead\"\n+  }\n+\n+  // scalastyle:on\n+\n+  /**\n+   * Create an internal [[Accumulator]] by name, which must begin with [[METRICS_PREFIX]].\n+   */\n+  def create(name: String): Accumulator[_] = {\n+    assert(name.startsWith(METRICS_PREFIX),\n+      s\"internal accumulator name must start with '$METRICS_PREFIX': $name\")\n+    getParam(name) match {\n+      case p @ LongAccumulatorParam => newMetric[Long](0L, name, p)\n+      case p @ IntAccumulatorParam => newMetric[Int](0, name, p)\n+      case p @ StringAccumulatorParam => newMetric[String](\"\", name, p)\n+      case p @ UpdatedBlockStatusesAccumulatorParam =>\n+        newMetric[Seq[(BlockId, BlockStatus)]](Seq(), name, p)\n+      case p => throw new IllegalArgumentException(\n+        s\"unsupported accumulator param '${p.getClass.getSimpleName}' for metric '$name'.\")\n+    }\n+  }\n+\n+  /**\n+   * Get the [[AccumulatorParam]] associated with the internal metric name,\n+   * which must begin with [[METRICS_PREFIX]].\n+   */\n+  def getParam(name: String): AccumulatorParam[_] = {\n+    assert(name.startsWith(METRICS_PREFIX),\n+      s\"internal accumulator name must start with '$METRICS_PREFIX': $name\")\n+    name match {\n+      case UPDATED_BLOCK_STATUSES => UpdatedBlockStatusesAccumulatorParam\n+      case shuffleRead.LOCAL_BLOCKS_FETCHED => IntAccumulatorParam\n+      case shuffleRead.REMOTE_BLOCKS_FETCHED => IntAccumulatorParam\n+      case input.READ_METHOD => StringAccumulatorParam\n+      case output.WRITE_METHOD => StringAccumulatorParam\n+      case _ => LongAccumulatorParam\n     }\n   }\n \n   /**\n    * Accumulators for tracking internal metrics.\n+   */\n+  def create(): Seq[Accumulator[_]] = {",
    "line": 118
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "Also, noticed that we have a `create(name: String)` further up which _is_ a factory method for a single accumulator, so there is a bit of confusion potential here.\n",
    "commit": "9f964f26effd66af83acb44c1e531d0c0df09f6b",
    "createdAt": "2016-01-26T08:31:16Z",
    "diffHunk": "@@ -17,42 +17,193 @@\n \n package org.apache.spark\n \n+import org.apache.spark.storage.{BlockId, BlockStatus}\n \n-// This is moved to its own file because many more things will be added to it in SPARK-10620.\n+\n+/**\n+ * A collection of fields and methods concerned with internal accumulators that represent\n+ * task level metrics.\n+ */\n private[spark] object InternalAccumulator {\n-  val PEAK_EXECUTION_MEMORY = \"peakExecutionMemory\"\n-  val TEST_ACCUMULATOR = \"testAccumulator\"\n-\n-  // For testing only.\n-  // This needs to be a def since we don't want to reuse the same accumulator across stages.\n-  private def maybeTestAccumulator: Option[Accumulator[Long]] = {\n-    if (sys.props.contains(\"spark.testing\")) {\n-      Some(new Accumulator(\n-        0L, AccumulatorParam.LongAccumulatorParam, Some(TEST_ACCUMULATOR), internal = true))\n-    } else {\n-      None\n+\n+  import AccumulatorParam._\n+\n+  // Prefixes used in names of internal task level metrics\n+  val METRICS_PREFIX = \"internal.metrics.\"\n+  val SHUFFLE_READ_METRICS_PREFIX = METRICS_PREFIX + \"shuffle.read.\"\n+  val SHUFFLE_WRITE_METRICS_PREFIX = METRICS_PREFIX + \"shuffle.write.\"\n+  val OUTPUT_METRICS_PREFIX = METRICS_PREFIX + \"output.\"\n+  val INPUT_METRICS_PREFIX = METRICS_PREFIX + \"input.\"\n+\n+  // Names of internal task level metrics\n+  val EXECUTOR_DESERIALIZE_TIME = METRICS_PREFIX + \"executorDeserializeTime\"\n+  val EXECUTOR_RUN_TIME = METRICS_PREFIX + \"executorRunTime\"\n+  val RESULT_SIZE = METRICS_PREFIX + \"resultSize\"\n+  val JVM_GC_TIME = METRICS_PREFIX + \"jvmGCTime\"\n+  val RESULT_SERIALIZATION_TIME = METRICS_PREFIX + \"resultSerializationTime\"\n+  val MEMORY_BYTES_SPILLED = METRICS_PREFIX + \"memoryBytesSpilled\"\n+  val DISK_BYTES_SPILLED = METRICS_PREFIX + \"diskBytesSpilled\"\n+  val PEAK_EXECUTION_MEMORY = METRICS_PREFIX + \"peakExecutionMemory\"\n+  val UPDATED_BLOCK_STATUSES = METRICS_PREFIX + \"updatedBlockStatuses\"\n+  val TEST_ACCUM = METRICS_PREFIX + \"testAccumulator\"\n+\n+  // scalastyle:off\n+\n+  // Names of shuffle read metrics\n+  object shuffleRead {\n+    val REMOTE_BLOCKS_FETCHED = SHUFFLE_READ_METRICS_PREFIX + \"remoteBlocksFetched\"\n+    val LOCAL_BLOCKS_FETCHED = SHUFFLE_READ_METRICS_PREFIX + \"localBlocksFetched\"\n+    val REMOTE_BYTES_READ = SHUFFLE_READ_METRICS_PREFIX + \"remoteBytesRead\"\n+    val LOCAL_BYTES_READ = SHUFFLE_READ_METRICS_PREFIX + \"localBytesRead\"\n+    val FETCH_WAIT_TIME = SHUFFLE_READ_METRICS_PREFIX + \"fetchWaitTime\"\n+    val RECORDS_READ = SHUFFLE_READ_METRICS_PREFIX + \"recordsRead\"\n+  }\n+\n+  // Names of shuffle write metrics\n+  object shuffleWrite {\n+    val BYTES_WRITTEN = SHUFFLE_WRITE_METRICS_PREFIX + \"bytesWritten\"\n+    val RECORDS_WRITTEN = SHUFFLE_WRITE_METRICS_PREFIX + \"recordsWritten\"\n+    val WRITE_TIME = SHUFFLE_WRITE_METRICS_PREFIX + \"writeTime\"\n+  }\n+\n+  // Names of output metrics\n+  object output {\n+    val WRITE_METHOD = OUTPUT_METRICS_PREFIX + \"writeMethod\"\n+    val BYTES_WRITTEN = OUTPUT_METRICS_PREFIX + \"bytesWritten\"\n+    val RECORDS_WRITTEN = OUTPUT_METRICS_PREFIX + \"recordsWritten\"\n+  }\n+\n+  // Names of input metrics\n+  object input {\n+    val READ_METHOD = INPUT_METRICS_PREFIX + \"readMethod\"\n+    val BYTES_READ = INPUT_METRICS_PREFIX + \"bytesRead\"\n+    val RECORDS_READ = INPUT_METRICS_PREFIX + \"recordsRead\"\n+  }\n+\n+  // scalastyle:on\n+\n+  /**\n+   * Create an internal [[Accumulator]] by name, which must begin with [[METRICS_PREFIX]].\n+   */\n+  def create(name: String): Accumulator[_] = {\n+    assert(name.startsWith(METRICS_PREFIX),\n+      s\"internal accumulator name must start with '$METRICS_PREFIX': $name\")\n+    getParam(name) match {\n+      case p @ LongAccumulatorParam => newMetric[Long](0L, name, p)\n+      case p @ IntAccumulatorParam => newMetric[Int](0, name, p)\n+      case p @ StringAccumulatorParam => newMetric[String](\"\", name, p)\n+      case p @ UpdatedBlockStatusesAccumulatorParam =>\n+        newMetric[Seq[(BlockId, BlockStatus)]](Seq(), name, p)\n+      case p => throw new IllegalArgumentException(\n+        s\"unsupported accumulator param '${p.getClass.getSimpleName}' for metric '$name'.\")\n+    }\n+  }\n+\n+  /**\n+   * Get the [[AccumulatorParam]] associated with the internal metric name,\n+   * which must begin with [[METRICS_PREFIX]].\n+   */\n+  def getParam(name: String): AccumulatorParam[_] = {\n+    assert(name.startsWith(METRICS_PREFIX),\n+      s\"internal accumulator name must start with '$METRICS_PREFIX': $name\")\n+    name match {\n+      case UPDATED_BLOCK_STATUSES => UpdatedBlockStatusesAccumulatorParam\n+      case shuffleRead.LOCAL_BLOCKS_FETCHED => IntAccumulatorParam\n+      case shuffleRead.REMOTE_BLOCKS_FETCHED => IntAccumulatorParam\n+      case input.READ_METHOD => StringAccumulatorParam\n+      case output.WRITE_METHOD => StringAccumulatorParam\n+      case _ => LongAccumulatorParam\n     }\n   }\n \n   /**\n    * Accumulators for tracking internal metrics.\n+   */\n+  def create(): Seq[Accumulator[_]] = {",
    "line": 118
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "oops looks like I missed this. I will address it in https://github.com/apache/spark/pull/10958\n",
    "commit": "9f964f26effd66af83acb44c1e531d0c0df09f6b",
    "createdAt": "2016-01-27T22:37:56Z",
    "diffHunk": "@@ -17,42 +17,193 @@\n \n package org.apache.spark\n \n+import org.apache.spark.storage.{BlockId, BlockStatus}\n \n-// This is moved to its own file because many more things will be added to it in SPARK-10620.\n+\n+/**\n+ * A collection of fields and methods concerned with internal accumulators that represent\n+ * task level metrics.\n+ */\n private[spark] object InternalAccumulator {\n-  val PEAK_EXECUTION_MEMORY = \"peakExecutionMemory\"\n-  val TEST_ACCUMULATOR = \"testAccumulator\"\n-\n-  // For testing only.\n-  // This needs to be a def since we don't want to reuse the same accumulator across stages.\n-  private def maybeTestAccumulator: Option[Accumulator[Long]] = {\n-    if (sys.props.contains(\"spark.testing\")) {\n-      Some(new Accumulator(\n-        0L, AccumulatorParam.LongAccumulatorParam, Some(TEST_ACCUMULATOR), internal = true))\n-    } else {\n-      None\n+\n+  import AccumulatorParam._\n+\n+  // Prefixes used in names of internal task level metrics\n+  val METRICS_PREFIX = \"internal.metrics.\"\n+  val SHUFFLE_READ_METRICS_PREFIX = METRICS_PREFIX + \"shuffle.read.\"\n+  val SHUFFLE_WRITE_METRICS_PREFIX = METRICS_PREFIX + \"shuffle.write.\"\n+  val OUTPUT_METRICS_PREFIX = METRICS_PREFIX + \"output.\"\n+  val INPUT_METRICS_PREFIX = METRICS_PREFIX + \"input.\"\n+\n+  // Names of internal task level metrics\n+  val EXECUTOR_DESERIALIZE_TIME = METRICS_PREFIX + \"executorDeserializeTime\"\n+  val EXECUTOR_RUN_TIME = METRICS_PREFIX + \"executorRunTime\"\n+  val RESULT_SIZE = METRICS_PREFIX + \"resultSize\"\n+  val JVM_GC_TIME = METRICS_PREFIX + \"jvmGCTime\"\n+  val RESULT_SERIALIZATION_TIME = METRICS_PREFIX + \"resultSerializationTime\"\n+  val MEMORY_BYTES_SPILLED = METRICS_PREFIX + \"memoryBytesSpilled\"\n+  val DISK_BYTES_SPILLED = METRICS_PREFIX + \"diskBytesSpilled\"\n+  val PEAK_EXECUTION_MEMORY = METRICS_PREFIX + \"peakExecutionMemory\"\n+  val UPDATED_BLOCK_STATUSES = METRICS_PREFIX + \"updatedBlockStatuses\"\n+  val TEST_ACCUM = METRICS_PREFIX + \"testAccumulator\"\n+\n+  // scalastyle:off\n+\n+  // Names of shuffle read metrics\n+  object shuffleRead {\n+    val REMOTE_BLOCKS_FETCHED = SHUFFLE_READ_METRICS_PREFIX + \"remoteBlocksFetched\"\n+    val LOCAL_BLOCKS_FETCHED = SHUFFLE_READ_METRICS_PREFIX + \"localBlocksFetched\"\n+    val REMOTE_BYTES_READ = SHUFFLE_READ_METRICS_PREFIX + \"remoteBytesRead\"\n+    val LOCAL_BYTES_READ = SHUFFLE_READ_METRICS_PREFIX + \"localBytesRead\"\n+    val FETCH_WAIT_TIME = SHUFFLE_READ_METRICS_PREFIX + \"fetchWaitTime\"\n+    val RECORDS_READ = SHUFFLE_READ_METRICS_PREFIX + \"recordsRead\"\n+  }\n+\n+  // Names of shuffle write metrics\n+  object shuffleWrite {\n+    val BYTES_WRITTEN = SHUFFLE_WRITE_METRICS_PREFIX + \"bytesWritten\"\n+    val RECORDS_WRITTEN = SHUFFLE_WRITE_METRICS_PREFIX + \"recordsWritten\"\n+    val WRITE_TIME = SHUFFLE_WRITE_METRICS_PREFIX + \"writeTime\"\n+  }\n+\n+  // Names of output metrics\n+  object output {\n+    val WRITE_METHOD = OUTPUT_METRICS_PREFIX + \"writeMethod\"\n+    val BYTES_WRITTEN = OUTPUT_METRICS_PREFIX + \"bytesWritten\"\n+    val RECORDS_WRITTEN = OUTPUT_METRICS_PREFIX + \"recordsWritten\"\n+  }\n+\n+  // Names of input metrics\n+  object input {\n+    val READ_METHOD = INPUT_METRICS_PREFIX + \"readMethod\"\n+    val BYTES_READ = INPUT_METRICS_PREFIX + \"bytesRead\"\n+    val RECORDS_READ = INPUT_METRICS_PREFIX + \"recordsRead\"\n+  }\n+\n+  // scalastyle:on\n+\n+  /**\n+   * Create an internal [[Accumulator]] by name, which must begin with [[METRICS_PREFIX]].\n+   */\n+  def create(name: String): Accumulator[_] = {\n+    assert(name.startsWith(METRICS_PREFIX),\n+      s\"internal accumulator name must start with '$METRICS_PREFIX': $name\")\n+    getParam(name) match {\n+      case p @ LongAccumulatorParam => newMetric[Long](0L, name, p)\n+      case p @ IntAccumulatorParam => newMetric[Int](0, name, p)\n+      case p @ StringAccumulatorParam => newMetric[String](\"\", name, p)\n+      case p @ UpdatedBlockStatusesAccumulatorParam =>\n+        newMetric[Seq[(BlockId, BlockStatus)]](Seq(), name, p)\n+      case p => throw new IllegalArgumentException(\n+        s\"unsupported accumulator param '${p.getClass.getSimpleName}' for metric '$name'.\")\n+    }\n+  }\n+\n+  /**\n+   * Get the [[AccumulatorParam]] associated with the internal metric name,\n+   * which must begin with [[METRICS_PREFIX]].\n+   */\n+  def getParam(name: String): AccumulatorParam[_] = {\n+    assert(name.startsWith(METRICS_PREFIX),\n+      s\"internal accumulator name must start with '$METRICS_PREFIX': $name\")\n+    name match {\n+      case UPDATED_BLOCK_STATUSES => UpdatedBlockStatusesAccumulatorParam\n+      case shuffleRead.LOCAL_BLOCKS_FETCHED => IntAccumulatorParam\n+      case shuffleRead.REMOTE_BLOCKS_FETCHED => IntAccumulatorParam\n+      case input.READ_METHOD => StringAccumulatorParam\n+      case output.WRITE_METHOD => StringAccumulatorParam\n+      case _ => LongAccumulatorParam\n     }\n   }\n \n   /**\n    * Accumulators for tracking internal metrics.\n+   */\n+  def create(): Seq[Accumulator[_]] = {",
    "line": 118
  }],
  "prId": 10835
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "Nit: you can turn off individual scalastyle inspections, such as line-length, without a blanket `scalastyle:off`, and it would be better to use that more granular style when writing new code.\n",
    "commit": "9f964f26effd66af83acb44c1e531d0c0df09f6b",
    "createdAt": "2016-01-24T22:02:03Z",
    "diffHunk": "@@ -17,42 +17,193 @@\n \n package org.apache.spark\n \n+import org.apache.spark.storage.{BlockId, BlockStatus}\n \n-// This is moved to its own file because many more things will be added to it in SPARK-10620.\n+\n+/**\n+ * A collection of fields and methods concerned with internal accumulators that represent\n+ * task level metrics.\n+ */\n private[spark] object InternalAccumulator {\n-  val PEAK_EXECUTION_MEMORY = \"peakExecutionMemory\"\n-  val TEST_ACCUMULATOR = \"testAccumulator\"\n-\n-  // For testing only.\n-  // This needs to be a def since we don't want to reuse the same accumulator across stages.\n-  private def maybeTestAccumulator: Option[Accumulator[Long]] = {\n-    if (sys.props.contains(\"spark.testing\")) {\n-      Some(new Accumulator(\n-        0L, AccumulatorParam.LongAccumulatorParam, Some(TEST_ACCUMULATOR), internal = true))\n-    } else {\n-      None\n+\n+  import AccumulatorParam._\n+\n+  // Prefixes used in names of internal task level metrics\n+  val METRICS_PREFIX = \"internal.metrics.\"\n+  val SHUFFLE_READ_METRICS_PREFIX = METRICS_PREFIX + \"shuffle.read.\"\n+  val SHUFFLE_WRITE_METRICS_PREFIX = METRICS_PREFIX + \"shuffle.write.\"\n+  val OUTPUT_METRICS_PREFIX = METRICS_PREFIX + \"output.\"\n+  val INPUT_METRICS_PREFIX = METRICS_PREFIX + \"input.\"\n+\n+  // Names of internal task level metrics\n+  val EXECUTOR_DESERIALIZE_TIME = METRICS_PREFIX + \"executorDeserializeTime\"\n+  val EXECUTOR_RUN_TIME = METRICS_PREFIX + \"executorRunTime\"\n+  val RESULT_SIZE = METRICS_PREFIX + \"resultSize\"\n+  val JVM_GC_TIME = METRICS_PREFIX + \"jvmGCTime\"\n+  val RESULT_SERIALIZATION_TIME = METRICS_PREFIX + \"resultSerializationTime\"\n+  val MEMORY_BYTES_SPILLED = METRICS_PREFIX + \"memoryBytesSpilled\"\n+  val DISK_BYTES_SPILLED = METRICS_PREFIX + \"diskBytesSpilled\"\n+  val PEAK_EXECUTION_MEMORY = METRICS_PREFIX + \"peakExecutionMemory\"\n+  val UPDATED_BLOCK_STATUSES = METRICS_PREFIX + \"updatedBlockStatuses\"\n+  val TEST_ACCUM = METRICS_PREFIX + \"testAccumulator\"\n+\n+  // scalastyle:off",
    "line": 46
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "what do you mean? The style it's complaining about is object names must begin with upper case, but I don't want that here.\n",
    "commit": "9f964f26effd66af83acb44c1e531d0c0df09f6b",
    "createdAt": "2016-01-25T19:28:09Z",
    "diffHunk": "@@ -17,42 +17,193 @@\n \n package org.apache.spark\n \n+import org.apache.spark.storage.{BlockId, BlockStatus}\n \n-// This is moved to its own file because many more things will be added to it in SPARK-10620.\n+\n+/**\n+ * A collection of fields and methods concerned with internal accumulators that represent\n+ * task level metrics.\n+ */\n private[spark] object InternalAccumulator {\n-  val PEAK_EXECUTION_MEMORY = \"peakExecutionMemory\"\n-  val TEST_ACCUMULATOR = \"testAccumulator\"\n-\n-  // For testing only.\n-  // This needs to be a def since we don't want to reuse the same accumulator across stages.\n-  private def maybeTestAccumulator: Option[Accumulator[Long]] = {\n-    if (sys.props.contains(\"spark.testing\")) {\n-      Some(new Accumulator(\n-        0L, AccumulatorParam.LongAccumulatorParam, Some(TEST_ACCUMULATOR), internal = true))\n-    } else {\n-      None\n+\n+  import AccumulatorParam._\n+\n+  // Prefixes used in names of internal task level metrics\n+  val METRICS_PREFIX = \"internal.metrics.\"\n+  val SHUFFLE_READ_METRICS_PREFIX = METRICS_PREFIX + \"shuffle.read.\"\n+  val SHUFFLE_WRITE_METRICS_PREFIX = METRICS_PREFIX + \"shuffle.write.\"\n+  val OUTPUT_METRICS_PREFIX = METRICS_PREFIX + \"output.\"\n+  val INPUT_METRICS_PREFIX = METRICS_PREFIX + \"input.\"\n+\n+  // Names of internal task level metrics\n+  val EXECUTOR_DESERIALIZE_TIME = METRICS_PREFIX + \"executorDeserializeTime\"\n+  val EXECUTOR_RUN_TIME = METRICS_PREFIX + \"executorRunTime\"\n+  val RESULT_SIZE = METRICS_PREFIX + \"resultSize\"\n+  val JVM_GC_TIME = METRICS_PREFIX + \"jvmGCTime\"\n+  val RESULT_SERIALIZATION_TIME = METRICS_PREFIX + \"resultSerializationTime\"\n+  val MEMORY_BYTES_SPILLED = METRICS_PREFIX + \"memoryBytesSpilled\"\n+  val DISK_BYTES_SPILLED = METRICS_PREFIX + \"diskBytesSpilled\"\n+  val PEAK_EXECUTION_MEMORY = METRICS_PREFIX + \"peakExecutionMemory\"\n+  val UPDATED_BLOCK_STATUSES = METRICS_PREFIX + \"updatedBlockStatuses\"\n+  val TEST_ACCUM = METRICS_PREFIX + \"testAccumulator\"\n+\n+  // scalastyle:off",
    "line": 46
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "I meant that you can use syntax like\n\n```\n// scalastyle:off println\n```\n\nto turn off an individual inspection. See `scalastyle-config.xml` and the `customId=\"println\"` parameter on how to be able to configure this for this checker.\n",
    "commit": "9f964f26effd66af83acb44c1e531d0c0df09f6b",
    "createdAt": "2016-01-25T22:50:47Z",
    "diffHunk": "@@ -17,42 +17,193 @@\n \n package org.apache.spark\n \n+import org.apache.spark.storage.{BlockId, BlockStatus}\n \n-// This is moved to its own file because many more things will be added to it in SPARK-10620.\n+\n+/**\n+ * A collection of fields and methods concerned with internal accumulators that represent\n+ * task level metrics.\n+ */\n private[spark] object InternalAccumulator {\n-  val PEAK_EXECUTION_MEMORY = \"peakExecutionMemory\"\n-  val TEST_ACCUMULATOR = \"testAccumulator\"\n-\n-  // For testing only.\n-  // This needs to be a def since we don't want to reuse the same accumulator across stages.\n-  private def maybeTestAccumulator: Option[Accumulator[Long]] = {\n-    if (sys.props.contains(\"spark.testing\")) {\n-      Some(new Accumulator(\n-        0L, AccumulatorParam.LongAccumulatorParam, Some(TEST_ACCUMULATOR), internal = true))\n-    } else {\n-      None\n+\n+  import AccumulatorParam._\n+\n+  // Prefixes used in names of internal task level metrics\n+  val METRICS_PREFIX = \"internal.metrics.\"\n+  val SHUFFLE_READ_METRICS_PREFIX = METRICS_PREFIX + \"shuffle.read.\"\n+  val SHUFFLE_WRITE_METRICS_PREFIX = METRICS_PREFIX + \"shuffle.write.\"\n+  val OUTPUT_METRICS_PREFIX = METRICS_PREFIX + \"output.\"\n+  val INPUT_METRICS_PREFIX = METRICS_PREFIX + \"input.\"\n+\n+  // Names of internal task level metrics\n+  val EXECUTOR_DESERIALIZE_TIME = METRICS_PREFIX + \"executorDeserializeTime\"\n+  val EXECUTOR_RUN_TIME = METRICS_PREFIX + \"executorRunTime\"\n+  val RESULT_SIZE = METRICS_PREFIX + \"resultSize\"\n+  val JVM_GC_TIME = METRICS_PREFIX + \"jvmGCTime\"\n+  val RESULT_SERIALIZATION_TIME = METRICS_PREFIX + \"resultSerializationTime\"\n+  val MEMORY_BYTES_SPILLED = METRICS_PREFIX + \"memoryBytesSpilled\"\n+  val DISK_BYTES_SPILLED = METRICS_PREFIX + \"diskBytesSpilled\"\n+  val PEAK_EXECUTION_MEMORY = METRICS_PREFIX + \"peakExecutionMemory\"\n+  val UPDATED_BLOCK_STATUSES = METRICS_PREFIX + \"updatedBlockStatuses\"\n+  val TEST_ACCUM = METRICS_PREFIX + \"testAccumulator\"\n+\n+  // scalastyle:off",
    "line": 46
  }],
  "prId": 10835
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "`assert` -> `require` so that the caller is blamed?\n",
    "commit": "9f964f26effd66af83acb44c1e531d0c0df09f6b",
    "createdAt": "2016-01-24T22:02:53Z",
    "diffHunk": "@@ -17,42 +17,193 @@\n \n package org.apache.spark\n \n+import org.apache.spark.storage.{BlockId, BlockStatus}\n \n-// This is moved to its own file because many more things will be added to it in SPARK-10620.\n+\n+/**\n+ * A collection of fields and methods concerned with internal accumulators that represent\n+ * task level metrics.\n+ */\n private[spark] object InternalAccumulator {\n-  val PEAK_EXECUTION_MEMORY = \"peakExecutionMemory\"\n-  val TEST_ACCUMULATOR = \"testAccumulator\"\n-\n-  // For testing only.\n-  // This needs to be a def since we don't want to reuse the same accumulator across stages.\n-  private def maybeTestAccumulator: Option[Accumulator[Long]] = {\n-    if (sys.props.contains(\"spark.testing\")) {\n-      Some(new Accumulator(\n-        0L, AccumulatorParam.LongAccumulatorParam, Some(TEST_ACCUMULATOR), internal = true))\n-    } else {\n-      None\n+\n+  import AccumulatorParam._\n+\n+  // Prefixes used in names of internal task level metrics\n+  val METRICS_PREFIX = \"internal.metrics.\"\n+  val SHUFFLE_READ_METRICS_PREFIX = METRICS_PREFIX + \"shuffle.read.\"\n+  val SHUFFLE_WRITE_METRICS_PREFIX = METRICS_PREFIX + \"shuffle.write.\"\n+  val OUTPUT_METRICS_PREFIX = METRICS_PREFIX + \"output.\"\n+  val INPUT_METRICS_PREFIX = METRICS_PREFIX + \"input.\"\n+\n+  // Names of internal task level metrics\n+  val EXECUTOR_DESERIALIZE_TIME = METRICS_PREFIX + \"executorDeserializeTime\"\n+  val EXECUTOR_RUN_TIME = METRICS_PREFIX + \"executorRunTime\"\n+  val RESULT_SIZE = METRICS_PREFIX + \"resultSize\"\n+  val JVM_GC_TIME = METRICS_PREFIX + \"jvmGCTime\"\n+  val RESULT_SERIALIZATION_TIME = METRICS_PREFIX + \"resultSerializationTime\"\n+  val MEMORY_BYTES_SPILLED = METRICS_PREFIX + \"memoryBytesSpilled\"\n+  val DISK_BYTES_SPILLED = METRICS_PREFIX + \"diskBytesSpilled\"\n+  val PEAK_EXECUTION_MEMORY = METRICS_PREFIX + \"peakExecutionMemory\"\n+  val UPDATED_BLOCK_STATUSES = METRICS_PREFIX + \"updatedBlockStatuses\"\n+  val TEST_ACCUM = METRICS_PREFIX + \"testAccumulator\"\n+\n+  // scalastyle:off\n+\n+  // Names of shuffle read metrics\n+  object shuffleRead {\n+    val REMOTE_BLOCKS_FETCHED = SHUFFLE_READ_METRICS_PREFIX + \"remoteBlocksFetched\"\n+    val LOCAL_BLOCKS_FETCHED = SHUFFLE_READ_METRICS_PREFIX + \"localBlocksFetched\"\n+    val REMOTE_BYTES_READ = SHUFFLE_READ_METRICS_PREFIX + \"remoteBytesRead\"\n+    val LOCAL_BYTES_READ = SHUFFLE_READ_METRICS_PREFIX + \"localBytesRead\"\n+    val FETCH_WAIT_TIME = SHUFFLE_READ_METRICS_PREFIX + \"fetchWaitTime\"\n+    val RECORDS_READ = SHUFFLE_READ_METRICS_PREFIX + \"recordsRead\"\n+  }\n+\n+  // Names of shuffle write metrics\n+  object shuffleWrite {\n+    val BYTES_WRITTEN = SHUFFLE_WRITE_METRICS_PREFIX + \"bytesWritten\"\n+    val RECORDS_WRITTEN = SHUFFLE_WRITE_METRICS_PREFIX + \"recordsWritten\"\n+    val WRITE_TIME = SHUFFLE_WRITE_METRICS_PREFIX + \"writeTime\"\n+  }\n+\n+  // Names of output metrics\n+  object output {\n+    val WRITE_METHOD = OUTPUT_METRICS_PREFIX + \"writeMethod\"\n+    val BYTES_WRITTEN = OUTPUT_METRICS_PREFIX + \"bytesWritten\"\n+    val RECORDS_WRITTEN = OUTPUT_METRICS_PREFIX + \"recordsWritten\"\n+  }\n+\n+  // Names of input metrics\n+  object input {\n+    val READ_METHOD = INPUT_METRICS_PREFIX + \"readMethod\"\n+    val BYTES_READ = INPUT_METRICS_PREFIX + \"bytesRead\"\n+    val RECORDS_READ = INPUT_METRICS_PREFIX + \"recordsRead\"\n+  }\n+\n+  // scalastyle:on\n+\n+  /**\n+   * Create an internal [[Accumulator]] by name, which must begin with [[METRICS_PREFIX]].\n+   */\n+  def create(name: String): Accumulator[_] = {\n+    assert(name.startsWith(METRICS_PREFIX),"
  }],
  "prId": 10835
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "require?\n",
    "commit": "9f964f26effd66af83acb44c1e531d0c0df09f6b",
    "createdAt": "2016-01-24T22:03:42Z",
    "diffHunk": "@@ -17,42 +17,193 @@\n \n package org.apache.spark\n \n+import org.apache.spark.storage.{BlockId, BlockStatus}\n \n-// This is moved to its own file because many more things will be added to it in SPARK-10620.\n+\n+/**\n+ * A collection of fields and methods concerned with internal accumulators that represent\n+ * task level metrics.\n+ */\n private[spark] object InternalAccumulator {\n-  val PEAK_EXECUTION_MEMORY = \"peakExecutionMemory\"\n-  val TEST_ACCUMULATOR = \"testAccumulator\"\n-\n-  // For testing only.\n-  // This needs to be a def since we don't want to reuse the same accumulator across stages.\n-  private def maybeTestAccumulator: Option[Accumulator[Long]] = {\n-    if (sys.props.contains(\"spark.testing\")) {\n-      Some(new Accumulator(\n-        0L, AccumulatorParam.LongAccumulatorParam, Some(TEST_ACCUMULATOR), internal = true))\n-    } else {\n-      None\n+\n+  import AccumulatorParam._\n+\n+  // Prefixes used in names of internal task level metrics\n+  val METRICS_PREFIX = \"internal.metrics.\"\n+  val SHUFFLE_READ_METRICS_PREFIX = METRICS_PREFIX + \"shuffle.read.\"\n+  val SHUFFLE_WRITE_METRICS_PREFIX = METRICS_PREFIX + \"shuffle.write.\"\n+  val OUTPUT_METRICS_PREFIX = METRICS_PREFIX + \"output.\"\n+  val INPUT_METRICS_PREFIX = METRICS_PREFIX + \"input.\"\n+\n+  // Names of internal task level metrics\n+  val EXECUTOR_DESERIALIZE_TIME = METRICS_PREFIX + \"executorDeserializeTime\"\n+  val EXECUTOR_RUN_TIME = METRICS_PREFIX + \"executorRunTime\"\n+  val RESULT_SIZE = METRICS_PREFIX + \"resultSize\"\n+  val JVM_GC_TIME = METRICS_PREFIX + \"jvmGCTime\"\n+  val RESULT_SERIALIZATION_TIME = METRICS_PREFIX + \"resultSerializationTime\"\n+  val MEMORY_BYTES_SPILLED = METRICS_PREFIX + \"memoryBytesSpilled\"\n+  val DISK_BYTES_SPILLED = METRICS_PREFIX + \"diskBytesSpilled\"\n+  val PEAK_EXECUTION_MEMORY = METRICS_PREFIX + \"peakExecutionMemory\"\n+  val UPDATED_BLOCK_STATUSES = METRICS_PREFIX + \"updatedBlockStatuses\"\n+  val TEST_ACCUM = METRICS_PREFIX + \"testAccumulator\"\n+\n+  // scalastyle:off\n+\n+  // Names of shuffle read metrics\n+  object shuffleRead {\n+    val REMOTE_BLOCKS_FETCHED = SHUFFLE_READ_METRICS_PREFIX + \"remoteBlocksFetched\"\n+    val LOCAL_BLOCKS_FETCHED = SHUFFLE_READ_METRICS_PREFIX + \"localBlocksFetched\"\n+    val REMOTE_BYTES_READ = SHUFFLE_READ_METRICS_PREFIX + \"remoteBytesRead\"\n+    val LOCAL_BYTES_READ = SHUFFLE_READ_METRICS_PREFIX + \"localBytesRead\"\n+    val FETCH_WAIT_TIME = SHUFFLE_READ_METRICS_PREFIX + \"fetchWaitTime\"\n+    val RECORDS_READ = SHUFFLE_READ_METRICS_PREFIX + \"recordsRead\"\n+  }\n+\n+  // Names of shuffle write metrics\n+  object shuffleWrite {\n+    val BYTES_WRITTEN = SHUFFLE_WRITE_METRICS_PREFIX + \"bytesWritten\"\n+    val RECORDS_WRITTEN = SHUFFLE_WRITE_METRICS_PREFIX + \"recordsWritten\"\n+    val WRITE_TIME = SHUFFLE_WRITE_METRICS_PREFIX + \"writeTime\"\n+  }\n+\n+  // Names of output metrics\n+  object output {\n+    val WRITE_METHOD = OUTPUT_METRICS_PREFIX + \"writeMethod\"\n+    val BYTES_WRITTEN = OUTPUT_METRICS_PREFIX + \"bytesWritten\"\n+    val RECORDS_WRITTEN = OUTPUT_METRICS_PREFIX + \"recordsWritten\"\n+  }\n+\n+  // Names of input metrics\n+  object input {\n+    val READ_METHOD = INPUT_METRICS_PREFIX + \"readMethod\"\n+    val BYTES_READ = INPUT_METRICS_PREFIX + \"bytesRead\"\n+    val RECORDS_READ = INPUT_METRICS_PREFIX + \"recordsRead\"\n+  }\n+\n+  // scalastyle:on\n+\n+  /**\n+   * Create an internal [[Accumulator]] by name, which must begin with [[METRICS_PREFIX]].\n+   */\n+  def create(name: String): Accumulator[_] = {\n+    assert(name.startsWith(METRICS_PREFIX),\n+      s\"internal accumulator name must start with '$METRICS_PREFIX': $name\")\n+    getParam(name) match {\n+      case p @ LongAccumulatorParam => newMetric[Long](0L, name, p)\n+      case p @ IntAccumulatorParam => newMetric[Int](0, name, p)\n+      case p @ StringAccumulatorParam => newMetric[String](\"\", name, p)\n+      case p @ UpdatedBlockStatusesAccumulatorParam =>\n+        newMetric[Seq[(BlockId, BlockStatus)]](Seq(), name, p)\n+      case p => throw new IllegalArgumentException(\n+        s\"unsupported accumulator param '${p.getClass.getSimpleName}' for metric '$name'.\")\n+    }\n+  }\n+\n+  /**\n+   * Get the [[AccumulatorParam]] associated with the internal metric name,\n+   * which must begin with [[METRICS_PREFIX]].\n+   */\n+  def getParam(name: String): AccumulatorParam[_] = {\n+    assert(name.startsWith(METRICS_PREFIX),"
  }],
  "prId": 10835
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "`sc.cleaner` should definitely be defined by this point, right? Just curious about whether a `assert(sc.cleaner.isDefined)` would be good here.\n",
    "commit": "9f964f26effd66af83acb44c1e531d0c0df09f6b",
    "createdAt": "2016-01-24T22:06:20Z",
    "diffHunk": "@@ -17,42 +17,193 @@\n \n package org.apache.spark\n \n+import org.apache.spark.storage.{BlockId, BlockStatus}\n \n-// This is moved to its own file because many more things will be added to it in SPARK-10620.\n+\n+/**\n+ * A collection of fields and methods concerned with internal accumulators that represent\n+ * task level metrics.\n+ */\n private[spark] object InternalAccumulator {\n-  val PEAK_EXECUTION_MEMORY = \"peakExecutionMemory\"\n-  val TEST_ACCUMULATOR = \"testAccumulator\"\n-\n-  // For testing only.\n-  // This needs to be a def since we don't want to reuse the same accumulator across stages.\n-  private def maybeTestAccumulator: Option[Accumulator[Long]] = {\n-    if (sys.props.contains(\"spark.testing\")) {\n-      Some(new Accumulator(\n-        0L, AccumulatorParam.LongAccumulatorParam, Some(TEST_ACCUMULATOR), internal = true))\n-    } else {\n-      None\n+\n+  import AccumulatorParam._\n+\n+  // Prefixes used in names of internal task level metrics\n+  val METRICS_PREFIX = \"internal.metrics.\"\n+  val SHUFFLE_READ_METRICS_PREFIX = METRICS_PREFIX + \"shuffle.read.\"\n+  val SHUFFLE_WRITE_METRICS_PREFIX = METRICS_PREFIX + \"shuffle.write.\"\n+  val OUTPUT_METRICS_PREFIX = METRICS_PREFIX + \"output.\"\n+  val INPUT_METRICS_PREFIX = METRICS_PREFIX + \"input.\"\n+\n+  // Names of internal task level metrics\n+  val EXECUTOR_DESERIALIZE_TIME = METRICS_PREFIX + \"executorDeserializeTime\"\n+  val EXECUTOR_RUN_TIME = METRICS_PREFIX + \"executorRunTime\"\n+  val RESULT_SIZE = METRICS_PREFIX + \"resultSize\"\n+  val JVM_GC_TIME = METRICS_PREFIX + \"jvmGCTime\"\n+  val RESULT_SERIALIZATION_TIME = METRICS_PREFIX + \"resultSerializationTime\"\n+  val MEMORY_BYTES_SPILLED = METRICS_PREFIX + \"memoryBytesSpilled\"\n+  val DISK_BYTES_SPILLED = METRICS_PREFIX + \"diskBytesSpilled\"\n+  val PEAK_EXECUTION_MEMORY = METRICS_PREFIX + \"peakExecutionMemory\"\n+  val UPDATED_BLOCK_STATUSES = METRICS_PREFIX + \"updatedBlockStatuses\"\n+  val TEST_ACCUM = METRICS_PREFIX + \"testAccumulator\"\n+\n+  // scalastyle:off\n+\n+  // Names of shuffle read metrics\n+  object shuffleRead {\n+    val REMOTE_BLOCKS_FETCHED = SHUFFLE_READ_METRICS_PREFIX + \"remoteBlocksFetched\"\n+    val LOCAL_BLOCKS_FETCHED = SHUFFLE_READ_METRICS_PREFIX + \"localBlocksFetched\"\n+    val REMOTE_BYTES_READ = SHUFFLE_READ_METRICS_PREFIX + \"remoteBytesRead\"\n+    val LOCAL_BYTES_READ = SHUFFLE_READ_METRICS_PREFIX + \"localBytesRead\"\n+    val FETCH_WAIT_TIME = SHUFFLE_READ_METRICS_PREFIX + \"fetchWaitTime\"\n+    val RECORDS_READ = SHUFFLE_READ_METRICS_PREFIX + \"recordsRead\"\n+  }\n+\n+  // Names of shuffle write metrics\n+  object shuffleWrite {\n+    val BYTES_WRITTEN = SHUFFLE_WRITE_METRICS_PREFIX + \"bytesWritten\"\n+    val RECORDS_WRITTEN = SHUFFLE_WRITE_METRICS_PREFIX + \"recordsWritten\"\n+    val WRITE_TIME = SHUFFLE_WRITE_METRICS_PREFIX + \"writeTime\"\n+  }\n+\n+  // Names of output metrics\n+  object output {\n+    val WRITE_METHOD = OUTPUT_METRICS_PREFIX + \"writeMethod\"\n+    val BYTES_WRITTEN = OUTPUT_METRICS_PREFIX + \"bytesWritten\"\n+    val RECORDS_WRITTEN = OUTPUT_METRICS_PREFIX + \"recordsWritten\"\n+  }\n+\n+  // Names of input metrics\n+  object input {\n+    val READ_METHOD = INPUT_METRICS_PREFIX + \"readMethod\"\n+    val BYTES_READ = INPUT_METRICS_PREFIX + \"bytesRead\"\n+    val RECORDS_READ = INPUT_METRICS_PREFIX + \"recordsRead\"\n+  }\n+\n+  // scalastyle:on\n+\n+  /**\n+   * Create an internal [[Accumulator]] by name, which must begin with [[METRICS_PREFIX]].\n+   */\n+  def create(name: String): Accumulator[_] = {\n+    assert(name.startsWith(METRICS_PREFIX),\n+      s\"internal accumulator name must start with '$METRICS_PREFIX': $name\")\n+    getParam(name) match {\n+      case p @ LongAccumulatorParam => newMetric[Long](0L, name, p)\n+      case p @ IntAccumulatorParam => newMetric[Int](0, name, p)\n+      case p @ StringAccumulatorParam => newMetric[String](\"\", name, p)\n+      case p @ UpdatedBlockStatusesAccumulatorParam =>\n+        newMetric[Seq[(BlockId, BlockStatus)]](Seq(), name, p)\n+      case p => throw new IllegalArgumentException(\n+        s\"unsupported accumulator param '${p.getClass.getSimpleName}' for metric '$name'.\")\n+    }\n+  }\n+\n+  /**\n+   * Get the [[AccumulatorParam]] associated with the internal metric name,\n+   * which must begin with [[METRICS_PREFIX]].\n+   */\n+  def getParam(name: String): AccumulatorParam[_] = {\n+    assert(name.startsWith(METRICS_PREFIX),\n+      s\"internal accumulator name must start with '$METRICS_PREFIX': $name\")\n+    name match {\n+      case UPDATED_BLOCK_STATUSES => UpdatedBlockStatusesAccumulatorParam\n+      case shuffleRead.LOCAL_BLOCKS_FETCHED => IntAccumulatorParam\n+      case shuffleRead.REMOTE_BLOCKS_FETCHED => IntAccumulatorParam\n+      case input.READ_METHOD => StringAccumulatorParam\n+      case output.WRITE_METHOD => StringAccumulatorParam\n+      case _ => LongAccumulatorParam\n     }\n   }\n \n   /**\n    * Accumulators for tracking internal metrics.\n+   */\n+  def create(): Seq[Accumulator[_]] = {\n+    Seq[String](\n+      EXECUTOR_DESERIALIZE_TIME,\n+      EXECUTOR_RUN_TIME,\n+      RESULT_SIZE,\n+      JVM_GC_TIME,\n+      RESULT_SERIALIZATION_TIME,\n+      MEMORY_BYTES_SPILLED,\n+      DISK_BYTES_SPILLED,\n+      PEAK_EXECUTION_MEMORY,\n+      UPDATED_BLOCK_STATUSES).map(create) ++\n+      createShuffleReadAccums() ++\n+      createShuffleWriteAccums() ++\n+      createInputAccums() ++\n+      createOutputAccums() ++\n+      sys.props.get(\"spark.testing\").map(_ => create(TEST_ACCUM)).toSeq\n+  }\n+\n+  /**\n+   * Accumulators for tracking shuffle read metrics.\n+   */\n+  def createShuffleReadAccums(): Seq[Accumulator[_]] = {\n+    Seq[String](\n+      shuffleRead.REMOTE_BLOCKS_FETCHED,\n+      shuffleRead.LOCAL_BLOCKS_FETCHED,\n+      shuffleRead.REMOTE_BYTES_READ,\n+      shuffleRead.LOCAL_BYTES_READ,\n+      shuffleRead.FETCH_WAIT_TIME,\n+      shuffleRead.RECORDS_READ).map(create)\n+  }\n+\n+  /**\n+   * Accumulators for tracking shuffle write metrics.\n+   */\n+  def createShuffleWriteAccums(): Seq[Accumulator[_]] = {\n+    Seq[String](\n+      shuffleWrite.BYTES_WRITTEN,\n+      shuffleWrite.RECORDS_WRITTEN,\n+      shuffleWrite.WRITE_TIME).map(create)\n+  }\n+\n+  /**\n+   * Accumulators for tracking input metrics.\n+   */\n+  def createInputAccums(): Seq[Accumulator[_]] = {\n+    Seq[String](\n+      input.READ_METHOD,\n+      input.BYTES_READ,\n+      input.RECORDS_READ).map(create)\n+  }\n+\n+  /**\n+   * Accumulators for tracking output metrics.\n+   */\n+  def createOutputAccums(): Seq[Accumulator[_]] = {\n+    Seq[String](\n+      output.WRITE_METHOD,\n+      output.BYTES_WRITTEN,\n+      output.RECORDS_WRITTEN).map(create)\n+  }\n+\n+  /**\n+   * Accumulators for tracking internal metrics.\n    *\n    * These accumulators are created with the stage such that all tasks in the stage will\n    * add to the same set of accumulators. We do this to report the distribution of accumulator\n    * values across all tasks within each stage.\n    */\n-  def create(sc: SparkContext): Seq[Accumulator[Long]] = {\n-    val internalAccumulators = Seq(\n-      // Execution memory refers to the memory used by internal data structures created\n-      // during shuffles, aggregations and joins. The value of this accumulator should be\n-      // approximately the sum of the peak sizes across all such data structures created\n-      // in this task. For SQL jobs, this only tracks all unsafe operators and ExternalSort.\n-      new Accumulator(\n-        0L, AccumulatorParam.LongAccumulatorParam, Some(PEAK_EXECUTION_MEMORY), internal = true)\n-    ) ++ maybeTestAccumulator.toSeq\n-    internalAccumulators.foreach { accumulator =>\n-      sc.cleaner.foreach(_.registerAccumulatorForCleanup(accumulator))\n+  def create(sc: SparkContext): Seq[Accumulator[_]] = {\n+    val accums = create()\n+    accums.foreach { accum =>\n+      Accumulators.register(accum)\n+      sc.cleaner.foreach(_.registerAccumulatorForCleanup(accum))",
    "line": 201
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "Also, would it make sense to move the `sc.cleaner.foreach(_.registerAccumulatorForCleanup(acc))` call into the body of `Accumulators.register()`? I ask because I've noticed these two calls paired together in multiple places and worry that someone might forget to pair them in the future.\n",
    "commit": "9f964f26effd66af83acb44c1e531d0c0df09f6b",
    "createdAt": "2016-01-25T00:05:34Z",
    "diffHunk": "@@ -17,42 +17,193 @@\n \n package org.apache.spark\n \n+import org.apache.spark.storage.{BlockId, BlockStatus}\n \n-// This is moved to its own file because many more things will be added to it in SPARK-10620.\n+\n+/**\n+ * A collection of fields and methods concerned with internal accumulators that represent\n+ * task level metrics.\n+ */\n private[spark] object InternalAccumulator {\n-  val PEAK_EXECUTION_MEMORY = \"peakExecutionMemory\"\n-  val TEST_ACCUMULATOR = \"testAccumulator\"\n-\n-  // For testing only.\n-  // This needs to be a def since we don't want to reuse the same accumulator across stages.\n-  private def maybeTestAccumulator: Option[Accumulator[Long]] = {\n-    if (sys.props.contains(\"spark.testing\")) {\n-      Some(new Accumulator(\n-        0L, AccumulatorParam.LongAccumulatorParam, Some(TEST_ACCUMULATOR), internal = true))\n-    } else {\n-      None\n+\n+  import AccumulatorParam._\n+\n+  // Prefixes used in names of internal task level metrics\n+  val METRICS_PREFIX = \"internal.metrics.\"\n+  val SHUFFLE_READ_METRICS_PREFIX = METRICS_PREFIX + \"shuffle.read.\"\n+  val SHUFFLE_WRITE_METRICS_PREFIX = METRICS_PREFIX + \"shuffle.write.\"\n+  val OUTPUT_METRICS_PREFIX = METRICS_PREFIX + \"output.\"\n+  val INPUT_METRICS_PREFIX = METRICS_PREFIX + \"input.\"\n+\n+  // Names of internal task level metrics\n+  val EXECUTOR_DESERIALIZE_TIME = METRICS_PREFIX + \"executorDeserializeTime\"\n+  val EXECUTOR_RUN_TIME = METRICS_PREFIX + \"executorRunTime\"\n+  val RESULT_SIZE = METRICS_PREFIX + \"resultSize\"\n+  val JVM_GC_TIME = METRICS_PREFIX + \"jvmGCTime\"\n+  val RESULT_SERIALIZATION_TIME = METRICS_PREFIX + \"resultSerializationTime\"\n+  val MEMORY_BYTES_SPILLED = METRICS_PREFIX + \"memoryBytesSpilled\"\n+  val DISK_BYTES_SPILLED = METRICS_PREFIX + \"diskBytesSpilled\"\n+  val PEAK_EXECUTION_MEMORY = METRICS_PREFIX + \"peakExecutionMemory\"\n+  val UPDATED_BLOCK_STATUSES = METRICS_PREFIX + \"updatedBlockStatuses\"\n+  val TEST_ACCUM = METRICS_PREFIX + \"testAccumulator\"\n+\n+  // scalastyle:off\n+\n+  // Names of shuffle read metrics\n+  object shuffleRead {\n+    val REMOTE_BLOCKS_FETCHED = SHUFFLE_READ_METRICS_PREFIX + \"remoteBlocksFetched\"\n+    val LOCAL_BLOCKS_FETCHED = SHUFFLE_READ_METRICS_PREFIX + \"localBlocksFetched\"\n+    val REMOTE_BYTES_READ = SHUFFLE_READ_METRICS_PREFIX + \"remoteBytesRead\"\n+    val LOCAL_BYTES_READ = SHUFFLE_READ_METRICS_PREFIX + \"localBytesRead\"\n+    val FETCH_WAIT_TIME = SHUFFLE_READ_METRICS_PREFIX + \"fetchWaitTime\"\n+    val RECORDS_READ = SHUFFLE_READ_METRICS_PREFIX + \"recordsRead\"\n+  }\n+\n+  // Names of shuffle write metrics\n+  object shuffleWrite {\n+    val BYTES_WRITTEN = SHUFFLE_WRITE_METRICS_PREFIX + \"bytesWritten\"\n+    val RECORDS_WRITTEN = SHUFFLE_WRITE_METRICS_PREFIX + \"recordsWritten\"\n+    val WRITE_TIME = SHUFFLE_WRITE_METRICS_PREFIX + \"writeTime\"\n+  }\n+\n+  // Names of output metrics\n+  object output {\n+    val WRITE_METHOD = OUTPUT_METRICS_PREFIX + \"writeMethod\"\n+    val BYTES_WRITTEN = OUTPUT_METRICS_PREFIX + \"bytesWritten\"\n+    val RECORDS_WRITTEN = OUTPUT_METRICS_PREFIX + \"recordsWritten\"\n+  }\n+\n+  // Names of input metrics\n+  object input {\n+    val READ_METHOD = INPUT_METRICS_PREFIX + \"readMethod\"\n+    val BYTES_READ = INPUT_METRICS_PREFIX + \"bytesRead\"\n+    val RECORDS_READ = INPUT_METRICS_PREFIX + \"recordsRead\"\n+  }\n+\n+  // scalastyle:on\n+\n+  /**\n+   * Create an internal [[Accumulator]] by name, which must begin with [[METRICS_PREFIX]].\n+   */\n+  def create(name: String): Accumulator[_] = {\n+    assert(name.startsWith(METRICS_PREFIX),\n+      s\"internal accumulator name must start with '$METRICS_PREFIX': $name\")\n+    getParam(name) match {\n+      case p @ LongAccumulatorParam => newMetric[Long](0L, name, p)\n+      case p @ IntAccumulatorParam => newMetric[Int](0, name, p)\n+      case p @ StringAccumulatorParam => newMetric[String](\"\", name, p)\n+      case p @ UpdatedBlockStatusesAccumulatorParam =>\n+        newMetric[Seq[(BlockId, BlockStatus)]](Seq(), name, p)\n+      case p => throw new IllegalArgumentException(\n+        s\"unsupported accumulator param '${p.getClass.getSimpleName}' for metric '$name'.\")\n+    }\n+  }\n+\n+  /**\n+   * Get the [[AccumulatorParam]] associated with the internal metric name,\n+   * which must begin with [[METRICS_PREFIX]].\n+   */\n+  def getParam(name: String): AccumulatorParam[_] = {\n+    assert(name.startsWith(METRICS_PREFIX),\n+      s\"internal accumulator name must start with '$METRICS_PREFIX': $name\")\n+    name match {\n+      case UPDATED_BLOCK_STATUSES => UpdatedBlockStatusesAccumulatorParam\n+      case shuffleRead.LOCAL_BLOCKS_FETCHED => IntAccumulatorParam\n+      case shuffleRead.REMOTE_BLOCKS_FETCHED => IntAccumulatorParam\n+      case input.READ_METHOD => StringAccumulatorParam\n+      case output.WRITE_METHOD => StringAccumulatorParam\n+      case _ => LongAccumulatorParam\n     }\n   }\n \n   /**\n    * Accumulators for tracking internal metrics.\n+   */\n+  def create(): Seq[Accumulator[_]] = {\n+    Seq[String](\n+      EXECUTOR_DESERIALIZE_TIME,\n+      EXECUTOR_RUN_TIME,\n+      RESULT_SIZE,\n+      JVM_GC_TIME,\n+      RESULT_SERIALIZATION_TIME,\n+      MEMORY_BYTES_SPILLED,\n+      DISK_BYTES_SPILLED,\n+      PEAK_EXECUTION_MEMORY,\n+      UPDATED_BLOCK_STATUSES).map(create) ++\n+      createShuffleReadAccums() ++\n+      createShuffleWriteAccums() ++\n+      createInputAccums() ++\n+      createOutputAccums() ++\n+      sys.props.get(\"spark.testing\").map(_ => create(TEST_ACCUM)).toSeq\n+  }\n+\n+  /**\n+   * Accumulators for tracking shuffle read metrics.\n+   */\n+  def createShuffleReadAccums(): Seq[Accumulator[_]] = {\n+    Seq[String](\n+      shuffleRead.REMOTE_BLOCKS_FETCHED,\n+      shuffleRead.LOCAL_BLOCKS_FETCHED,\n+      shuffleRead.REMOTE_BYTES_READ,\n+      shuffleRead.LOCAL_BYTES_READ,\n+      shuffleRead.FETCH_WAIT_TIME,\n+      shuffleRead.RECORDS_READ).map(create)\n+  }\n+\n+  /**\n+   * Accumulators for tracking shuffle write metrics.\n+   */\n+  def createShuffleWriteAccums(): Seq[Accumulator[_]] = {\n+    Seq[String](\n+      shuffleWrite.BYTES_WRITTEN,\n+      shuffleWrite.RECORDS_WRITTEN,\n+      shuffleWrite.WRITE_TIME).map(create)\n+  }\n+\n+  /**\n+   * Accumulators for tracking input metrics.\n+   */\n+  def createInputAccums(): Seq[Accumulator[_]] = {\n+    Seq[String](\n+      input.READ_METHOD,\n+      input.BYTES_READ,\n+      input.RECORDS_READ).map(create)\n+  }\n+\n+  /**\n+   * Accumulators for tracking output metrics.\n+   */\n+  def createOutputAccums(): Seq[Accumulator[_]] = {\n+    Seq[String](\n+      output.WRITE_METHOD,\n+      output.BYTES_WRITTEN,\n+      output.RECORDS_WRITTEN).map(create)\n+  }\n+\n+  /**\n+   * Accumulators for tracking internal metrics.\n    *\n    * These accumulators are created with the stage such that all tasks in the stage will\n    * add to the same set of accumulators. We do this to report the distribution of accumulator\n    * values across all tasks within each stage.\n    */\n-  def create(sc: SparkContext): Seq[Accumulator[Long]] = {\n-    val internalAccumulators = Seq(\n-      // Execution memory refers to the memory used by internal data structures created\n-      // during shuffles, aggregations and joins. The value of this accumulator should be\n-      // approximately the sum of the peak sizes across all such data structures created\n-      // in this task. For SQL jobs, this only tracks all unsafe operators and ExternalSort.\n-      new Accumulator(\n-        0L, AccumulatorParam.LongAccumulatorParam, Some(PEAK_EXECUTION_MEMORY), internal = true)\n-    ) ++ maybeTestAccumulator.toSeq\n-    internalAccumulators.foreach { accumulator =>\n-      sc.cleaner.foreach(_.registerAccumulatorForCleanup(accumulator))\n+  def create(sc: SparkContext): Seq[Accumulator[_]] = {\n+    val accums = create()\n+    accums.foreach { accum =>\n+      Accumulators.register(accum)\n+      sc.cleaner.foreach(_.registerAccumulatorForCleanup(accum))",
    "line": 201
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "I don't want to make `Accumulators.register` take in `SparkContext`. Let's do the cleanup later.\n",
    "commit": "9f964f26effd66af83acb44c1e531d0c0df09f6b",
    "createdAt": "2016-01-25T19:29:43Z",
    "diffHunk": "@@ -17,42 +17,193 @@\n \n package org.apache.spark\n \n+import org.apache.spark.storage.{BlockId, BlockStatus}\n \n-// This is moved to its own file because many more things will be added to it in SPARK-10620.\n+\n+/**\n+ * A collection of fields and methods concerned with internal accumulators that represent\n+ * task level metrics.\n+ */\n private[spark] object InternalAccumulator {\n-  val PEAK_EXECUTION_MEMORY = \"peakExecutionMemory\"\n-  val TEST_ACCUMULATOR = \"testAccumulator\"\n-\n-  // For testing only.\n-  // This needs to be a def since we don't want to reuse the same accumulator across stages.\n-  private def maybeTestAccumulator: Option[Accumulator[Long]] = {\n-    if (sys.props.contains(\"spark.testing\")) {\n-      Some(new Accumulator(\n-        0L, AccumulatorParam.LongAccumulatorParam, Some(TEST_ACCUMULATOR), internal = true))\n-    } else {\n-      None\n+\n+  import AccumulatorParam._\n+\n+  // Prefixes used in names of internal task level metrics\n+  val METRICS_PREFIX = \"internal.metrics.\"\n+  val SHUFFLE_READ_METRICS_PREFIX = METRICS_PREFIX + \"shuffle.read.\"\n+  val SHUFFLE_WRITE_METRICS_PREFIX = METRICS_PREFIX + \"shuffle.write.\"\n+  val OUTPUT_METRICS_PREFIX = METRICS_PREFIX + \"output.\"\n+  val INPUT_METRICS_PREFIX = METRICS_PREFIX + \"input.\"\n+\n+  // Names of internal task level metrics\n+  val EXECUTOR_DESERIALIZE_TIME = METRICS_PREFIX + \"executorDeserializeTime\"\n+  val EXECUTOR_RUN_TIME = METRICS_PREFIX + \"executorRunTime\"\n+  val RESULT_SIZE = METRICS_PREFIX + \"resultSize\"\n+  val JVM_GC_TIME = METRICS_PREFIX + \"jvmGCTime\"\n+  val RESULT_SERIALIZATION_TIME = METRICS_PREFIX + \"resultSerializationTime\"\n+  val MEMORY_BYTES_SPILLED = METRICS_PREFIX + \"memoryBytesSpilled\"\n+  val DISK_BYTES_SPILLED = METRICS_PREFIX + \"diskBytesSpilled\"\n+  val PEAK_EXECUTION_MEMORY = METRICS_PREFIX + \"peakExecutionMemory\"\n+  val UPDATED_BLOCK_STATUSES = METRICS_PREFIX + \"updatedBlockStatuses\"\n+  val TEST_ACCUM = METRICS_PREFIX + \"testAccumulator\"\n+\n+  // scalastyle:off\n+\n+  // Names of shuffle read metrics\n+  object shuffleRead {\n+    val REMOTE_BLOCKS_FETCHED = SHUFFLE_READ_METRICS_PREFIX + \"remoteBlocksFetched\"\n+    val LOCAL_BLOCKS_FETCHED = SHUFFLE_READ_METRICS_PREFIX + \"localBlocksFetched\"\n+    val REMOTE_BYTES_READ = SHUFFLE_READ_METRICS_PREFIX + \"remoteBytesRead\"\n+    val LOCAL_BYTES_READ = SHUFFLE_READ_METRICS_PREFIX + \"localBytesRead\"\n+    val FETCH_WAIT_TIME = SHUFFLE_READ_METRICS_PREFIX + \"fetchWaitTime\"\n+    val RECORDS_READ = SHUFFLE_READ_METRICS_PREFIX + \"recordsRead\"\n+  }\n+\n+  // Names of shuffle write metrics\n+  object shuffleWrite {\n+    val BYTES_WRITTEN = SHUFFLE_WRITE_METRICS_PREFIX + \"bytesWritten\"\n+    val RECORDS_WRITTEN = SHUFFLE_WRITE_METRICS_PREFIX + \"recordsWritten\"\n+    val WRITE_TIME = SHUFFLE_WRITE_METRICS_PREFIX + \"writeTime\"\n+  }\n+\n+  // Names of output metrics\n+  object output {\n+    val WRITE_METHOD = OUTPUT_METRICS_PREFIX + \"writeMethod\"\n+    val BYTES_WRITTEN = OUTPUT_METRICS_PREFIX + \"bytesWritten\"\n+    val RECORDS_WRITTEN = OUTPUT_METRICS_PREFIX + \"recordsWritten\"\n+  }\n+\n+  // Names of input metrics\n+  object input {\n+    val READ_METHOD = INPUT_METRICS_PREFIX + \"readMethod\"\n+    val BYTES_READ = INPUT_METRICS_PREFIX + \"bytesRead\"\n+    val RECORDS_READ = INPUT_METRICS_PREFIX + \"recordsRead\"\n+  }\n+\n+  // scalastyle:on\n+\n+  /**\n+   * Create an internal [[Accumulator]] by name, which must begin with [[METRICS_PREFIX]].\n+   */\n+  def create(name: String): Accumulator[_] = {\n+    assert(name.startsWith(METRICS_PREFIX),\n+      s\"internal accumulator name must start with '$METRICS_PREFIX': $name\")\n+    getParam(name) match {\n+      case p @ LongAccumulatorParam => newMetric[Long](0L, name, p)\n+      case p @ IntAccumulatorParam => newMetric[Int](0, name, p)\n+      case p @ StringAccumulatorParam => newMetric[String](\"\", name, p)\n+      case p @ UpdatedBlockStatusesAccumulatorParam =>\n+        newMetric[Seq[(BlockId, BlockStatus)]](Seq(), name, p)\n+      case p => throw new IllegalArgumentException(\n+        s\"unsupported accumulator param '${p.getClass.getSimpleName}' for metric '$name'.\")\n+    }\n+  }\n+\n+  /**\n+   * Get the [[AccumulatorParam]] associated with the internal metric name,\n+   * which must begin with [[METRICS_PREFIX]].\n+   */\n+  def getParam(name: String): AccumulatorParam[_] = {\n+    assert(name.startsWith(METRICS_PREFIX),\n+      s\"internal accumulator name must start with '$METRICS_PREFIX': $name\")\n+    name match {\n+      case UPDATED_BLOCK_STATUSES => UpdatedBlockStatusesAccumulatorParam\n+      case shuffleRead.LOCAL_BLOCKS_FETCHED => IntAccumulatorParam\n+      case shuffleRead.REMOTE_BLOCKS_FETCHED => IntAccumulatorParam\n+      case input.READ_METHOD => StringAccumulatorParam\n+      case output.WRITE_METHOD => StringAccumulatorParam\n+      case _ => LongAccumulatorParam\n     }\n   }\n \n   /**\n    * Accumulators for tracking internal metrics.\n+   */\n+  def create(): Seq[Accumulator[_]] = {\n+    Seq[String](\n+      EXECUTOR_DESERIALIZE_TIME,\n+      EXECUTOR_RUN_TIME,\n+      RESULT_SIZE,\n+      JVM_GC_TIME,\n+      RESULT_SERIALIZATION_TIME,\n+      MEMORY_BYTES_SPILLED,\n+      DISK_BYTES_SPILLED,\n+      PEAK_EXECUTION_MEMORY,\n+      UPDATED_BLOCK_STATUSES).map(create) ++\n+      createShuffleReadAccums() ++\n+      createShuffleWriteAccums() ++\n+      createInputAccums() ++\n+      createOutputAccums() ++\n+      sys.props.get(\"spark.testing\").map(_ => create(TEST_ACCUM)).toSeq\n+  }\n+\n+  /**\n+   * Accumulators for tracking shuffle read metrics.\n+   */\n+  def createShuffleReadAccums(): Seq[Accumulator[_]] = {\n+    Seq[String](\n+      shuffleRead.REMOTE_BLOCKS_FETCHED,\n+      shuffleRead.LOCAL_BLOCKS_FETCHED,\n+      shuffleRead.REMOTE_BYTES_READ,\n+      shuffleRead.LOCAL_BYTES_READ,\n+      shuffleRead.FETCH_WAIT_TIME,\n+      shuffleRead.RECORDS_READ).map(create)\n+  }\n+\n+  /**\n+   * Accumulators for tracking shuffle write metrics.\n+   */\n+  def createShuffleWriteAccums(): Seq[Accumulator[_]] = {\n+    Seq[String](\n+      shuffleWrite.BYTES_WRITTEN,\n+      shuffleWrite.RECORDS_WRITTEN,\n+      shuffleWrite.WRITE_TIME).map(create)\n+  }\n+\n+  /**\n+   * Accumulators for tracking input metrics.\n+   */\n+  def createInputAccums(): Seq[Accumulator[_]] = {\n+    Seq[String](\n+      input.READ_METHOD,\n+      input.BYTES_READ,\n+      input.RECORDS_READ).map(create)\n+  }\n+\n+  /**\n+   * Accumulators for tracking output metrics.\n+   */\n+  def createOutputAccums(): Seq[Accumulator[_]] = {\n+    Seq[String](\n+      output.WRITE_METHOD,\n+      output.BYTES_WRITTEN,\n+      output.RECORDS_WRITTEN).map(create)\n+  }\n+\n+  /**\n+   * Accumulators for tracking internal metrics.\n    *\n    * These accumulators are created with the stage such that all tasks in the stage will\n    * add to the same set of accumulators. We do this to report the distribution of accumulator\n    * values across all tasks within each stage.\n    */\n-  def create(sc: SparkContext): Seq[Accumulator[Long]] = {\n-    val internalAccumulators = Seq(\n-      // Execution memory refers to the memory used by internal data structures created\n-      // during shuffles, aggregations and joins. The value of this accumulator should be\n-      // approximately the sum of the peak sizes across all such data structures created\n-      // in this task. For SQL jobs, this only tracks all unsafe operators and ExternalSort.\n-      new Accumulator(\n-        0L, AccumulatorParam.LongAccumulatorParam, Some(PEAK_EXECUTION_MEMORY), internal = true)\n-    ) ++ maybeTestAccumulator.toSeq\n-    internalAccumulators.foreach { accumulator =>\n-      sc.cleaner.foreach(_.registerAccumulatorForCleanup(accumulator))\n+  def create(sc: SparkContext): Seq[Accumulator[_]] = {\n+    val accums = create()\n+    accums.foreach { accum =>\n+      Accumulators.register(accum)\n+      sc.cleaner.foreach(_.registerAccumulatorForCleanup(accum))",
    "line": 201
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "also, `sc.cleaner` can be disabled, so we need to do a check\n",
    "commit": "9f964f26effd66af83acb44c1e531d0c0df09f6b",
    "createdAt": "2016-01-25T21:17:07Z",
    "diffHunk": "@@ -17,42 +17,193 @@\n \n package org.apache.spark\n \n+import org.apache.spark.storage.{BlockId, BlockStatus}\n \n-// This is moved to its own file because many more things will be added to it in SPARK-10620.\n+\n+/**\n+ * A collection of fields and methods concerned with internal accumulators that represent\n+ * task level metrics.\n+ */\n private[spark] object InternalAccumulator {\n-  val PEAK_EXECUTION_MEMORY = \"peakExecutionMemory\"\n-  val TEST_ACCUMULATOR = \"testAccumulator\"\n-\n-  // For testing only.\n-  // This needs to be a def since we don't want to reuse the same accumulator across stages.\n-  private def maybeTestAccumulator: Option[Accumulator[Long]] = {\n-    if (sys.props.contains(\"spark.testing\")) {\n-      Some(new Accumulator(\n-        0L, AccumulatorParam.LongAccumulatorParam, Some(TEST_ACCUMULATOR), internal = true))\n-    } else {\n-      None\n+\n+  import AccumulatorParam._\n+\n+  // Prefixes used in names of internal task level metrics\n+  val METRICS_PREFIX = \"internal.metrics.\"\n+  val SHUFFLE_READ_METRICS_PREFIX = METRICS_PREFIX + \"shuffle.read.\"\n+  val SHUFFLE_WRITE_METRICS_PREFIX = METRICS_PREFIX + \"shuffle.write.\"\n+  val OUTPUT_METRICS_PREFIX = METRICS_PREFIX + \"output.\"\n+  val INPUT_METRICS_PREFIX = METRICS_PREFIX + \"input.\"\n+\n+  // Names of internal task level metrics\n+  val EXECUTOR_DESERIALIZE_TIME = METRICS_PREFIX + \"executorDeserializeTime\"\n+  val EXECUTOR_RUN_TIME = METRICS_PREFIX + \"executorRunTime\"\n+  val RESULT_SIZE = METRICS_PREFIX + \"resultSize\"\n+  val JVM_GC_TIME = METRICS_PREFIX + \"jvmGCTime\"\n+  val RESULT_SERIALIZATION_TIME = METRICS_PREFIX + \"resultSerializationTime\"\n+  val MEMORY_BYTES_SPILLED = METRICS_PREFIX + \"memoryBytesSpilled\"\n+  val DISK_BYTES_SPILLED = METRICS_PREFIX + \"diskBytesSpilled\"\n+  val PEAK_EXECUTION_MEMORY = METRICS_PREFIX + \"peakExecutionMemory\"\n+  val UPDATED_BLOCK_STATUSES = METRICS_PREFIX + \"updatedBlockStatuses\"\n+  val TEST_ACCUM = METRICS_PREFIX + \"testAccumulator\"\n+\n+  // scalastyle:off\n+\n+  // Names of shuffle read metrics\n+  object shuffleRead {\n+    val REMOTE_BLOCKS_FETCHED = SHUFFLE_READ_METRICS_PREFIX + \"remoteBlocksFetched\"\n+    val LOCAL_BLOCKS_FETCHED = SHUFFLE_READ_METRICS_PREFIX + \"localBlocksFetched\"\n+    val REMOTE_BYTES_READ = SHUFFLE_READ_METRICS_PREFIX + \"remoteBytesRead\"\n+    val LOCAL_BYTES_READ = SHUFFLE_READ_METRICS_PREFIX + \"localBytesRead\"\n+    val FETCH_WAIT_TIME = SHUFFLE_READ_METRICS_PREFIX + \"fetchWaitTime\"\n+    val RECORDS_READ = SHUFFLE_READ_METRICS_PREFIX + \"recordsRead\"\n+  }\n+\n+  // Names of shuffle write metrics\n+  object shuffleWrite {\n+    val BYTES_WRITTEN = SHUFFLE_WRITE_METRICS_PREFIX + \"bytesWritten\"\n+    val RECORDS_WRITTEN = SHUFFLE_WRITE_METRICS_PREFIX + \"recordsWritten\"\n+    val WRITE_TIME = SHUFFLE_WRITE_METRICS_PREFIX + \"writeTime\"\n+  }\n+\n+  // Names of output metrics\n+  object output {\n+    val WRITE_METHOD = OUTPUT_METRICS_PREFIX + \"writeMethod\"\n+    val BYTES_WRITTEN = OUTPUT_METRICS_PREFIX + \"bytesWritten\"\n+    val RECORDS_WRITTEN = OUTPUT_METRICS_PREFIX + \"recordsWritten\"\n+  }\n+\n+  // Names of input metrics\n+  object input {\n+    val READ_METHOD = INPUT_METRICS_PREFIX + \"readMethod\"\n+    val BYTES_READ = INPUT_METRICS_PREFIX + \"bytesRead\"\n+    val RECORDS_READ = INPUT_METRICS_PREFIX + \"recordsRead\"\n+  }\n+\n+  // scalastyle:on\n+\n+  /**\n+   * Create an internal [[Accumulator]] by name, which must begin with [[METRICS_PREFIX]].\n+   */\n+  def create(name: String): Accumulator[_] = {\n+    assert(name.startsWith(METRICS_PREFIX),\n+      s\"internal accumulator name must start with '$METRICS_PREFIX': $name\")\n+    getParam(name) match {\n+      case p @ LongAccumulatorParam => newMetric[Long](0L, name, p)\n+      case p @ IntAccumulatorParam => newMetric[Int](0, name, p)\n+      case p @ StringAccumulatorParam => newMetric[String](\"\", name, p)\n+      case p @ UpdatedBlockStatusesAccumulatorParam =>\n+        newMetric[Seq[(BlockId, BlockStatus)]](Seq(), name, p)\n+      case p => throw new IllegalArgumentException(\n+        s\"unsupported accumulator param '${p.getClass.getSimpleName}' for metric '$name'.\")\n+    }\n+  }\n+\n+  /**\n+   * Get the [[AccumulatorParam]] associated with the internal metric name,\n+   * which must begin with [[METRICS_PREFIX]].\n+   */\n+  def getParam(name: String): AccumulatorParam[_] = {\n+    assert(name.startsWith(METRICS_PREFIX),\n+      s\"internal accumulator name must start with '$METRICS_PREFIX': $name\")\n+    name match {\n+      case UPDATED_BLOCK_STATUSES => UpdatedBlockStatusesAccumulatorParam\n+      case shuffleRead.LOCAL_BLOCKS_FETCHED => IntAccumulatorParam\n+      case shuffleRead.REMOTE_BLOCKS_FETCHED => IntAccumulatorParam\n+      case input.READ_METHOD => StringAccumulatorParam\n+      case output.WRITE_METHOD => StringAccumulatorParam\n+      case _ => LongAccumulatorParam\n     }\n   }\n \n   /**\n    * Accumulators for tracking internal metrics.\n+   */\n+  def create(): Seq[Accumulator[_]] = {\n+    Seq[String](\n+      EXECUTOR_DESERIALIZE_TIME,\n+      EXECUTOR_RUN_TIME,\n+      RESULT_SIZE,\n+      JVM_GC_TIME,\n+      RESULT_SERIALIZATION_TIME,\n+      MEMORY_BYTES_SPILLED,\n+      DISK_BYTES_SPILLED,\n+      PEAK_EXECUTION_MEMORY,\n+      UPDATED_BLOCK_STATUSES).map(create) ++\n+      createShuffleReadAccums() ++\n+      createShuffleWriteAccums() ++\n+      createInputAccums() ++\n+      createOutputAccums() ++\n+      sys.props.get(\"spark.testing\").map(_ => create(TEST_ACCUM)).toSeq\n+  }\n+\n+  /**\n+   * Accumulators for tracking shuffle read metrics.\n+   */\n+  def createShuffleReadAccums(): Seq[Accumulator[_]] = {\n+    Seq[String](\n+      shuffleRead.REMOTE_BLOCKS_FETCHED,\n+      shuffleRead.LOCAL_BLOCKS_FETCHED,\n+      shuffleRead.REMOTE_BYTES_READ,\n+      shuffleRead.LOCAL_BYTES_READ,\n+      shuffleRead.FETCH_WAIT_TIME,\n+      shuffleRead.RECORDS_READ).map(create)\n+  }\n+\n+  /**\n+   * Accumulators for tracking shuffle write metrics.\n+   */\n+  def createShuffleWriteAccums(): Seq[Accumulator[_]] = {\n+    Seq[String](\n+      shuffleWrite.BYTES_WRITTEN,\n+      shuffleWrite.RECORDS_WRITTEN,\n+      shuffleWrite.WRITE_TIME).map(create)\n+  }\n+\n+  /**\n+   * Accumulators for tracking input metrics.\n+   */\n+  def createInputAccums(): Seq[Accumulator[_]] = {\n+    Seq[String](\n+      input.READ_METHOD,\n+      input.BYTES_READ,\n+      input.RECORDS_READ).map(create)\n+  }\n+\n+  /**\n+   * Accumulators for tracking output metrics.\n+   */\n+  def createOutputAccums(): Seq[Accumulator[_]] = {\n+    Seq[String](\n+      output.WRITE_METHOD,\n+      output.BYTES_WRITTEN,\n+      output.RECORDS_WRITTEN).map(create)\n+  }\n+\n+  /**\n+   * Accumulators for tracking internal metrics.\n    *\n    * These accumulators are created with the stage such that all tasks in the stage will\n    * add to the same set of accumulators. We do this to report the distribution of accumulator\n    * values across all tasks within each stage.\n    */\n-  def create(sc: SparkContext): Seq[Accumulator[Long]] = {\n-    val internalAccumulators = Seq(\n-      // Execution memory refers to the memory used by internal data structures created\n-      // during shuffles, aggregations and joins. The value of this accumulator should be\n-      // approximately the sum of the peak sizes across all such data structures created\n-      // in this task. For SQL jobs, this only tracks all unsafe operators and ExternalSort.\n-      new Accumulator(\n-        0L, AccumulatorParam.LongAccumulatorParam, Some(PEAK_EXECUTION_MEMORY), internal = true)\n-    ) ++ maybeTestAccumulator.toSeq\n-    internalAccumulators.foreach { accumulator =>\n-      sc.cleaner.foreach(_.registerAccumulatorForCleanup(accumulator))\n+  def create(sc: SparkContext): Seq[Accumulator[_]] = {\n+    val accums = create()\n+    accums.foreach { accum =>\n+      Accumulators.register(accum)\n+      sc.cleaner.foreach(_.registerAccumulatorForCleanup(accum))",
    "line": 201
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "Sure, it seems fine to deal with this later.\n",
    "commit": "9f964f26effd66af83acb44c1e531d0c0df09f6b",
    "createdAt": "2016-01-25T22:51:44Z",
    "diffHunk": "@@ -17,42 +17,193 @@\n \n package org.apache.spark\n \n+import org.apache.spark.storage.{BlockId, BlockStatus}\n \n-// This is moved to its own file because many more things will be added to it in SPARK-10620.\n+\n+/**\n+ * A collection of fields and methods concerned with internal accumulators that represent\n+ * task level metrics.\n+ */\n private[spark] object InternalAccumulator {\n-  val PEAK_EXECUTION_MEMORY = \"peakExecutionMemory\"\n-  val TEST_ACCUMULATOR = \"testAccumulator\"\n-\n-  // For testing only.\n-  // This needs to be a def since we don't want to reuse the same accumulator across stages.\n-  private def maybeTestAccumulator: Option[Accumulator[Long]] = {\n-    if (sys.props.contains(\"spark.testing\")) {\n-      Some(new Accumulator(\n-        0L, AccumulatorParam.LongAccumulatorParam, Some(TEST_ACCUMULATOR), internal = true))\n-    } else {\n-      None\n+\n+  import AccumulatorParam._\n+\n+  // Prefixes used in names of internal task level metrics\n+  val METRICS_PREFIX = \"internal.metrics.\"\n+  val SHUFFLE_READ_METRICS_PREFIX = METRICS_PREFIX + \"shuffle.read.\"\n+  val SHUFFLE_WRITE_METRICS_PREFIX = METRICS_PREFIX + \"shuffle.write.\"\n+  val OUTPUT_METRICS_PREFIX = METRICS_PREFIX + \"output.\"\n+  val INPUT_METRICS_PREFIX = METRICS_PREFIX + \"input.\"\n+\n+  // Names of internal task level metrics\n+  val EXECUTOR_DESERIALIZE_TIME = METRICS_PREFIX + \"executorDeserializeTime\"\n+  val EXECUTOR_RUN_TIME = METRICS_PREFIX + \"executorRunTime\"\n+  val RESULT_SIZE = METRICS_PREFIX + \"resultSize\"\n+  val JVM_GC_TIME = METRICS_PREFIX + \"jvmGCTime\"\n+  val RESULT_SERIALIZATION_TIME = METRICS_PREFIX + \"resultSerializationTime\"\n+  val MEMORY_BYTES_SPILLED = METRICS_PREFIX + \"memoryBytesSpilled\"\n+  val DISK_BYTES_SPILLED = METRICS_PREFIX + \"diskBytesSpilled\"\n+  val PEAK_EXECUTION_MEMORY = METRICS_PREFIX + \"peakExecutionMemory\"\n+  val UPDATED_BLOCK_STATUSES = METRICS_PREFIX + \"updatedBlockStatuses\"\n+  val TEST_ACCUM = METRICS_PREFIX + \"testAccumulator\"\n+\n+  // scalastyle:off\n+\n+  // Names of shuffle read metrics\n+  object shuffleRead {\n+    val REMOTE_BLOCKS_FETCHED = SHUFFLE_READ_METRICS_PREFIX + \"remoteBlocksFetched\"\n+    val LOCAL_BLOCKS_FETCHED = SHUFFLE_READ_METRICS_PREFIX + \"localBlocksFetched\"\n+    val REMOTE_BYTES_READ = SHUFFLE_READ_METRICS_PREFIX + \"remoteBytesRead\"\n+    val LOCAL_BYTES_READ = SHUFFLE_READ_METRICS_PREFIX + \"localBytesRead\"\n+    val FETCH_WAIT_TIME = SHUFFLE_READ_METRICS_PREFIX + \"fetchWaitTime\"\n+    val RECORDS_READ = SHUFFLE_READ_METRICS_PREFIX + \"recordsRead\"\n+  }\n+\n+  // Names of shuffle write metrics\n+  object shuffleWrite {\n+    val BYTES_WRITTEN = SHUFFLE_WRITE_METRICS_PREFIX + \"bytesWritten\"\n+    val RECORDS_WRITTEN = SHUFFLE_WRITE_METRICS_PREFIX + \"recordsWritten\"\n+    val WRITE_TIME = SHUFFLE_WRITE_METRICS_PREFIX + \"writeTime\"\n+  }\n+\n+  // Names of output metrics\n+  object output {\n+    val WRITE_METHOD = OUTPUT_METRICS_PREFIX + \"writeMethod\"\n+    val BYTES_WRITTEN = OUTPUT_METRICS_PREFIX + \"bytesWritten\"\n+    val RECORDS_WRITTEN = OUTPUT_METRICS_PREFIX + \"recordsWritten\"\n+  }\n+\n+  // Names of input metrics\n+  object input {\n+    val READ_METHOD = INPUT_METRICS_PREFIX + \"readMethod\"\n+    val BYTES_READ = INPUT_METRICS_PREFIX + \"bytesRead\"\n+    val RECORDS_READ = INPUT_METRICS_PREFIX + \"recordsRead\"\n+  }\n+\n+  // scalastyle:on\n+\n+  /**\n+   * Create an internal [[Accumulator]] by name, which must begin with [[METRICS_PREFIX]].\n+   */\n+  def create(name: String): Accumulator[_] = {\n+    assert(name.startsWith(METRICS_PREFIX),\n+      s\"internal accumulator name must start with '$METRICS_PREFIX': $name\")\n+    getParam(name) match {\n+      case p @ LongAccumulatorParam => newMetric[Long](0L, name, p)\n+      case p @ IntAccumulatorParam => newMetric[Int](0, name, p)\n+      case p @ StringAccumulatorParam => newMetric[String](\"\", name, p)\n+      case p @ UpdatedBlockStatusesAccumulatorParam =>\n+        newMetric[Seq[(BlockId, BlockStatus)]](Seq(), name, p)\n+      case p => throw new IllegalArgumentException(\n+        s\"unsupported accumulator param '${p.getClass.getSimpleName}' for metric '$name'.\")\n+    }\n+  }\n+\n+  /**\n+   * Get the [[AccumulatorParam]] associated with the internal metric name,\n+   * which must begin with [[METRICS_PREFIX]].\n+   */\n+  def getParam(name: String): AccumulatorParam[_] = {\n+    assert(name.startsWith(METRICS_PREFIX),\n+      s\"internal accumulator name must start with '$METRICS_PREFIX': $name\")\n+    name match {\n+      case UPDATED_BLOCK_STATUSES => UpdatedBlockStatusesAccumulatorParam\n+      case shuffleRead.LOCAL_BLOCKS_FETCHED => IntAccumulatorParam\n+      case shuffleRead.REMOTE_BLOCKS_FETCHED => IntAccumulatorParam\n+      case input.READ_METHOD => StringAccumulatorParam\n+      case output.WRITE_METHOD => StringAccumulatorParam\n+      case _ => LongAccumulatorParam\n     }\n   }\n \n   /**\n    * Accumulators for tracking internal metrics.\n+   */\n+  def create(): Seq[Accumulator[_]] = {\n+    Seq[String](\n+      EXECUTOR_DESERIALIZE_TIME,\n+      EXECUTOR_RUN_TIME,\n+      RESULT_SIZE,\n+      JVM_GC_TIME,\n+      RESULT_SERIALIZATION_TIME,\n+      MEMORY_BYTES_SPILLED,\n+      DISK_BYTES_SPILLED,\n+      PEAK_EXECUTION_MEMORY,\n+      UPDATED_BLOCK_STATUSES).map(create) ++\n+      createShuffleReadAccums() ++\n+      createShuffleWriteAccums() ++\n+      createInputAccums() ++\n+      createOutputAccums() ++\n+      sys.props.get(\"spark.testing\").map(_ => create(TEST_ACCUM)).toSeq\n+  }\n+\n+  /**\n+   * Accumulators for tracking shuffle read metrics.\n+   */\n+  def createShuffleReadAccums(): Seq[Accumulator[_]] = {\n+    Seq[String](\n+      shuffleRead.REMOTE_BLOCKS_FETCHED,\n+      shuffleRead.LOCAL_BLOCKS_FETCHED,\n+      shuffleRead.REMOTE_BYTES_READ,\n+      shuffleRead.LOCAL_BYTES_READ,\n+      shuffleRead.FETCH_WAIT_TIME,\n+      shuffleRead.RECORDS_READ).map(create)\n+  }\n+\n+  /**\n+   * Accumulators for tracking shuffle write metrics.\n+   */\n+  def createShuffleWriteAccums(): Seq[Accumulator[_]] = {\n+    Seq[String](\n+      shuffleWrite.BYTES_WRITTEN,\n+      shuffleWrite.RECORDS_WRITTEN,\n+      shuffleWrite.WRITE_TIME).map(create)\n+  }\n+\n+  /**\n+   * Accumulators for tracking input metrics.\n+   */\n+  def createInputAccums(): Seq[Accumulator[_]] = {\n+    Seq[String](\n+      input.READ_METHOD,\n+      input.BYTES_READ,\n+      input.RECORDS_READ).map(create)\n+  }\n+\n+  /**\n+   * Accumulators for tracking output metrics.\n+   */\n+  def createOutputAccums(): Seq[Accumulator[_]] = {\n+    Seq[String](\n+      output.WRITE_METHOD,\n+      output.BYTES_WRITTEN,\n+      output.RECORDS_WRITTEN).map(create)\n+  }\n+\n+  /**\n+   * Accumulators for tracking internal metrics.\n    *\n    * These accumulators are created with the stage such that all tasks in the stage will\n    * add to the same set of accumulators. We do this to report the distribution of accumulator\n    * values across all tasks within each stage.\n    */\n-  def create(sc: SparkContext): Seq[Accumulator[Long]] = {\n-    val internalAccumulators = Seq(\n-      // Execution memory refers to the memory used by internal data structures created\n-      // during shuffles, aggregations and joins. The value of this accumulator should be\n-      // approximately the sum of the peak sizes across all such data structures created\n-      // in this task. For SQL jobs, this only tracks all unsafe operators and ExternalSort.\n-      new Accumulator(\n-        0L, AccumulatorParam.LongAccumulatorParam, Some(PEAK_EXECUTION_MEMORY), internal = true)\n-    ) ++ maybeTestAccumulator.toSeq\n-    internalAccumulators.foreach { accumulator =>\n-      sc.cleaner.foreach(_.registerAccumulatorForCleanup(accumulator))\n+  def create(sc: SparkContext): Seq[Accumulator[_]] = {\n+    val accums = create()\n+    accums.foreach { accum =>\n+      Accumulators.register(accum)\n+      sc.cleaner.foreach(_.registerAccumulatorForCleanup(accum))",
    "line": 201
  }],
  "prId": 10835
}]