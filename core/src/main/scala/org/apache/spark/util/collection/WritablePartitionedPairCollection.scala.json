[{
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "Can you remove the extension of `SizeTracker` here? Most of the implementations of this themselves extend `SizeTracker`, so it's duplicated. But also I think logically they are decoupled. You could implement this interface and not use the `SizeTracker` functionality to estimate your size.\n\nYou may need to add it then to `PartitionedPairBuffer`.\n",
    "commit": "8c70dd909c20af6dacaf4091e964a1239a221016",
    "createdAt": "2015-04-26T18:02:52Z",
    "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.collection\n+\n+import java.util.Comparator\n+\n+import org.apache.spark.storage.BlockObjectWriter\n+\n+/**\n+ * A common interface for size-tracking collections of key-value pairs that\n+ * - Have an associated partition for each key-value pair.\n+ * - Support a memory-efficient sorted iterator\n+ * - Support a WritablePartitionedIterator for writing the contents directly as bytes.\n+ */\n+private[spark] trait WritablePartitionedPairCollection[K, V] extends SizeTracker {"
  }],
  "prId": 4450
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "Can you use `Option[Comparator[K]]` to more explicitly model the fact that this key comparator is optional? Right now it looks like you pass nulls around.\n",
    "commit": "8c70dd909c20af6dacaf4091e964a1239a221016",
    "createdAt": "2015-04-26T18:13:52Z",
    "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.collection\n+\n+import java.util.Comparator\n+\n+import org.apache.spark.storage.BlockObjectWriter\n+\n+/**\n+ * A common interface for size-tracking collections of key-value pairs that\n+ * - Have an associated partition for each key-value pair.\n+ * - Support a memory-efficient sorted iterator\n+ * - Support a WritablePartitionedIterator for writing the contents directly as bytes.\n+ */\n+private[spark] trait WritablePartitionedPairCollection[K, V] extends SizeTracker {\n+  /**\n+   * Insert a key-value pair with a partition into the collection\n+   */\n+  def insert(partition: Int, key: K, value: V): Unit\n+\n+  /**\n+   * Estimate the collection's current memory usage in bytes.\n+   */\n+  def estimateSize(): Long\n+\n+  /**\n+   * Iterate through the data in order of partition ID and then the given comparator. This may\n+   * destroy the underlying collection.\n+   */\n+  def partitionedDestructiveSortedIterator(keyComparator: Comparator[K]): Iterator[((Int, K), V)]"
  }, {
    "author": {
      "login": "pwendell"
    },
    "body": "Also maybe to shorten this we can just name it `partitionedIterator`.\n",
    "commit": "8c70dd909c20af6dacaf4091e964a1239a221016",
    "createdAt": "2015-04-26T18:16:51Z",
    "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.collection\n+\n+import java.util.Comparator\n+\n+import org.apache.spark.storage.BlockObjectWriter\n+\n+/**\n+ * A common interface for size-tracking collections of key-value pairs that\n+ * - Have an associated partition for each key-value pair.\n+ * - Support a memory-efficient sorted iterator\n+ * - Support a WritablePartitionedIterator for writing the contents directly as bytes.\n+ */\n+private[spark] trait WritablePartitionedPairCollection[K, V] extends SizeTracker {\n+  /**\n+   * Insert a key-value pair with a partition into the collection\n+   */\n+  def insert(partition: Int, key: K, value: V): Unit\n+\n+  /**\n+   * Estimate the collection's current memory usage in bytes.\n+   */\n+  def estimateSize(): Long\n+\n+  /**\n+   * Iterate through the data in order of partition ID and then the given comparator. This may\n+   * destroy the underlying collection.\n+   */\n+  def partitionedDestructiveSortedIterator(keyComparator: Comparator[K]): Iterator[((Int, K), V)]"
  }, {
    "author": {
      "login": "sryza"
    },
    "body": "Agree this is an obnoxiously long name.  However, if we rename `partitionedDestructiveSortedIterator` to `partitionedIterator`, then we probably also want to rename `destructiveSortedWritablePartitionedIterator` to `writablePartitionedIterator`.  But a method named `writablePartitionedIterator` exists as well (which is not destructive or sorted).\n",
    "commit": "8c70dd909c20af6dacaf4091e964a1239a221016",
    "createdAt": "2015-04-27T21:24:56Z",
    "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.collection\n+\n+import java.util.Comparator\n+\n+import org.apache.spark.storage.BlockObjectWriter\n+\n+/**\n+ * A common interface for size-tracking collections of key-value pairs that\n+ * - Have an associated partition for each key-value pair.\n+ * - Support a memory-efficient sorted iterator\n+ * - Support a WritablePartitionedIterator for writing the contents directly as bytes.\n+ */\n+private[spark] trait WritablePartitionedPairCollection[K, V] extends SizeTracker {\n+  /**\n+   * Insert a key-value pair with a partition into the collection\n+   */\n+  def insert(partition: Int, key: K, value: V): Unit\n+\n+  /**\n+   * Estimate the collection's current memory usage in bytes.\n+   */\n+  def estimateSize(): Long\n+\n+  /**\n+   * Iterate through the data in order of partition ID and then the given comparator. This may\n+   * destroy the underlying collection.\n+   */\n+  def partitionedDestructiveSortedIterator(keyComparator: Comparator[K]): Iterator[((Int, K), V)]"
  }],
  "prId": 4450
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "This is a bit awkward the way this iterator works since it's threading the BlockObjectWriter through everywhere. It would be nicer if this WritablePartitionedPairCollection could be written in a way that's less coupled.\n\nWhat if you allow WritablePartitionedPairCollection to return an iterator over serialized data and then the callers can pass that through to the block object writer? Then you'd probably just call it `PartitionedPairCollection`.\n",
    "commit": "8c70dd909c20af6dacaf4091e964a1239a221016",
    "createdAt": "2015-04-26T18:19:21Z",
    "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.collection\n+\n+import java.util.Comparator\n+\n+import org.apache.spark.storage.BlockObjectWriter\n+\n+/**\n+ * A common interface for size-tracking collections of key-value pairs that\n+ * - Have an associated partition for each key-value pair.\n+ * - Support a memory-efficient sorted iterator\n+ * - Support a WritablePartitionedIterator for writing the contents directly as bytes.\n+ */\n+private[spark] trait WritablePartitionedPairCollection[K, V] extends SizeTracker {\n+  /**\n+   * Insert a key-value pair with a partition into the collection\n+   */\n+  def insert(partition: Int, key: K, value: V): Unit\n+\n+  /**\n+   * Estimate the collection's current memory usage in bytes.\n+   */\n+  def estimateSize(): Long\n+\n+  /**\n+   * Iterate through the data in order of partition ID and then the given comparator. This may\n+   * destroy the underlying collection.\n+   */\n+  def partitionedDestructiveSortedIterator(keyComparator: Comparator[K]): Iterator[((Int, K), V)]\n+\n+  /**\n+   * Iterate through the data and write out the elements instead of returning them. Records are\n+   * returned in order of their partition ID and then the given comparator.\n+   * This may destroy the underlying collection.\n+   */\n+  def destructiveSortedWritablePartitionedIterator(keyComparator: Comparator[K])\n+    : WritablePartitionedIterator = {\n+    WritablePartitionedIterator.fromIterator(partitionedDestructiveSortedIterator(keyComparator))\n+  }\n+\n+  /**\n+   * Iterate through the data and write out the elements instead of returning them.\n+   */\n+  def writablePartitionedIterator(): WritablePartitionedIterator\n+}\n+\n+private[spark] object WritablePartitionedPairCollection {\n+  /**\n+   * A comparator for (Int, K) pairs that orders them by only their partition ID.\n+   */\n+  def partitionComparator[K]: Comparator[(Int, K)] = new Comparator[(Int, K)] {\n+    override def compare(a: (Int, K), b: (Int, K)): Int = {\n+      a._1 - b._1\n+    }\n+  }\n+\n+  /**\n+   * A comparator for (Int, K) pairs that orders them both by their partition ID and a key ordering.\n+   */\n+  def partitionKeyComparator[K](keyComparator: Comparator[K]): Comparator[(Int, K)] = {\n+    new Comparator[(Int, K)] {\n+      override def compare(a: (Int, K), b: (Int, K)): Int = {\n+        val partitionDiff = a._1 - b._1\n+        if (partitionDiff != 0) {\n+          partitionDiff\n+        } else {\n+          keyComparator.compare(a._2, b._2)\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * Iterator that writes elements to a BlockObjectWriter instead of returning them. Each element\n+ * has an associated partition.\n+ */\n+private[spark] trait WritablePartitionedIterator {",
    "line": 90
  }, {
    "author": {
      "login": "pwendell"
    },
    "body": "After looking a bit more, I do wonder if we should just have two methods in what is now WritablePartitionedPairCollection. `partitionedIterator` and `partitionedSerializedIterator`. Where you get back serialized key/value pairs. You'd then deal with the distinction explicitly in the caller.\n",
    "commit": "8c70dd909c20af6dacaf4091e964a1239a221016",
    "createdAt": "2015-04-26T18:44:35Z",
    "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.collection\n+\n+import java.util.Comparator\n+\n+import org.apache.spark.storage.BlockObjectWriter\n+\n+/**\n+ * A common interface for size-tracking collections of key-value pairs that\n+ * - Have an associated partition for each key-value pair.\n+ * - Support a memory-efficient sorted iterator\n+ * - Support a WritablePartitionedIterator for writing the contents directly as bytes.\n+ */\n+private[spark] trait WritablePartitionedPairCollection[K, V] extends SizeTracker {\n+  /**\n+   * Insert a key-value pair with a partition into the collection\n+   */\n+  def insert(partition: Int, key: K, value: V): Unit\n+\n+  /**\n+   * Estimate the collection's current memory usage in bytes.\n+   */\n+  def estimateSize(): Long\n+\n+  /**\n+   * Iterate through the data in order of partition ID and then the given comparator. This may\n+   * destroy the underlying collection.\n+   */\n+  def partitionedDestructiveSortedIterator(keyComparator: Comparator[K]): Iterator[((Int, K), V)]\n+\n+  /**\n+   * Iterate through the data and write out the elements instead of returning them. Records are\n+   * returned in order of their partition ID and then the given comparator.\n+   * This may destroy the underlying collection.\n+   */\n+  def destructiveSortedWritablePartitionedIterator(keyComparator: Comparator[K])\n+    : WritablePartitionedIterator = {\n+    WritablePartitionedIterator.fromIterator(partitionedDestructiveSortedIterator(keyComparator))\n+  }\n+\n+  /**\n+   * Iterate through the data and write out the elements instead of returning them.\n+   */\n+  def writablePartitionedIterator(): WritablePartitionedIterator\n+}\n+\n+private[spark] object WritablePartitionedPairCollection {\n+  /**\n+   * A comparator for (Int, K) pairs that orders them by only their partition ID.\n+   */\n+  def partitionComparator[K]: Comparator[(Int, K)] = new Comparator[(Int, K)] {\n+    override def compare(a: (Int, K), b: (Int, K)): Int = {\n+      a._1 - b._1\n+    }\n+  }\n+\n+  /**\n+   * A comparator for (Int, K) pairs that orders them both by their partition ID and a key ordering.\n+   */\n+  def partitionKeyComparator[K](keyComparator: Comparator[K]): Comparator[(Int, K)] = {\n+    new Comparator[(Int, K)] {\n+      override def compare(a: (Int, K), b: (Int, K)): Int = {\n+        val partitionDiff = a._1 - b._1\n+        if (partitionDiff != 0) {\n+          partitionDiff\n+        } else {\n+          keyComparator.compare(a._2, b._2)\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * Iterator that writes elements to a BlockObjectWriter instead of returning them. Each element\n+ * has an associated partition.\n+ */\n+private[spark] trait WritablePartitionedIterator {",
    "line": 90
  }, {
    "author": {
      "login": "sryza"
    },
    "body": "I agree that it's a bit awkward.  The main issue is that objects can span multiple chunks of the `ChainedBuffer`, so `next` can't always return a single contiguous segment of a byte buffer unless we're willing to force an extra copy.\n\nOne way around this would be to add a `def singleSegmentChunk(startPos: Int, maxLen: Int): (Array[Byte], Int, Int)` method to `ChainedBuffer`, which would return a pointer to a contiguous segment of bytes with length possibly smaller than requested.  This would then be called multiple times per object.\n",
    "commit": "8c70dd909c20af6dacaf4091e964a1239a221016",
    "createdAt": "2015-04-26T19:05:09Z",
    "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.collection\n+\n+import java.util.Comparator\n+\n+import org.apache.spark.storage.BlockObjectWriter\n+\n+/**\n+ * A common interface for size-tracking collections of key-value pairs that\n+ * - Have an associated partition for each key-value pair.\n+ * - Support a memory-efficient sorted iterator\n+ * - Support a WritablePartitionedIterator for writing the contents directly as bytes.\n+ */\n+private[spark] trait WritablePartitionedPairCollection[K, V] extends SizeTracker {\n+  /**\n+   * Insert a key-value pair with a partition into the collection\n+   */\n+  def insert(partition: Int, key: K, value: V): Unit\n+\n+  /**\n+   * Estimate the collection's current memory usage in bytes.\n+   */\n+  def estimateSize(): Long\n+\n+  /**\n+   * Iterate through the data in order of partition ID and then the given comparator. This may\n+   * destroy the underlying collection.\n+   */\n+  def partitionedDestructiveSortedIterator(keyComparator: Comparator[K]): Iterator[((Int, K), V)]\n+\n+  /**\n+   * Iterate through the data and write out the elements instead of returning them. Records are\n+   * returned in order of their partition ID and then the given comparator.\n+   * This may destroy the underlying collection.\n+   */\n+  def destructiveSortedWritablePartitionedIterator(keyComparator: Comparator[K])\n+    : WritablePartitionedIterator = {\n+    WritablePartitionedIterator.fromIterator(partitionedDestructiveSortedIterator(keyComparator))\n+  }\n+\n+  /**\n+   * Iterate through the data and write out the elements instead of returning them.\n+   */\n+  def writablePartitionedIterator(): WritablePartitionedIterator\n+}\n+\n+private[spark] object WritablePartitionedPairCollection {\n+  /**\n+   * A comparator for (Int, K) pairs that orders them by only their partition ID.\n+   */\n+  def partitionComparator[K]: Comparator[(Int, K)] = new Comparator[(Int, K)] {\n+    override def compare(a: (Int, K), b: (Int, K)): Int = {\n+      a._1 - b._1\n+    }\n+  }\n+\n+  /**\n+   * A comparator for (Int, K) pairs that orders them both by their partition ID and a key ordering.\n+   */\n+  def partitionKeyComparator[K](keyComparator: Comparator[K]): Comparator[(Int, K)] = {\n+    new Comparator[(Int, K)] {\n+      override def compare(a: (Int, K), b: (Int, K)): Int = {\n+        val partitionDiff = a._1 - b._1\n+        if (partitionDiff != 0) {\n+          partitionDiff\n+        } else {\n+          keyComparator.compare(a._2, b._2)\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * Iterator that writes elements to a BlockObjectWriter instead of returning them. Each element\n+ * has an associated partition.\n+ */\n+private[spark] trait WritablePartitionedIterator {",
    "line": 90
  }],
  "prId": 4450
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "Instead of having this can you just have consumers of this use a `PeekingIterator`?\n",
    "commit": "8c70dd909c20af6dacaf4091e964a1239a221016",
    "createdAt": "2015-04-26T18:19:38Z",
    "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.collection\n+\n+import java.util.Comparator\n+\n+import org.apache.spark.storage.BlockObjectWriter\n+\n+/**\n+ * A common interface for size-tracking collections of key-value pairs that\n+ * - Have an associated partition for each key-value pair.\n+ * - Support a memory-efficient sorted iterator\n+ * - Support a WritablePartitionedIterator for writing the contents directly as bytes.\n+ */\n+private[spark] trait WritablePartitionedPairCollection[K, V] extends SizeTracker {\n+  /**\n+   * Insert a key-value pair with a partition into the collection\n+   */\n+  def insert(partition: Int, key: K, value: V): Unit\n+\n+  /**\n+   * Estimate the collection's current memory usage in bytes.\n+   */\n+  def estimateSize(): Long\n+\n+  /**\n+   * Iterate through the data in order of partition ID and then the given comparator. This may\n+   * destroy the underlying collection.\n+   */\n+  def partitionedDestructiveSortedIterator(keyComparator: Comparator[K]): Iterator[((Int, K), V)]\n+\n+  /**\n+   * Iterate through the data and write out the elements instead of returning them. Records are\n+   * returned in order of their partition ID and then the given comparator.\n+   * This may destroy the underlying collection.\n+   */\n+  def destructiveSortedWritablePartitionedIterator(keyComparator: Comparator[K])\n+    : WritablePartitionedIterator = {\n+    WritablePartitionedIterator.fromIterator(partitionedDestructiveSortedIterator(keyComparator))\n+  }\n+\n+  /**\n+   * Iterate through the data and write out the elements instead of returning them.\n+   */\n+  def writablePartitionedIterator(): WritablePartitionedIterator\n+}\n+\n+private[spark] object WritablePartitionedPairCollection {\n+  /**\n+   * A comparator for (Int, K) pairs that orders them by only their partition ID.\n+   */\n+  def partitionComparator[K]: Comparator[(Int, K)] = new Comparator[(Int, K)] {\n+    override def compare(a: (Int, K), b: (Int, K)): Int = {\n+      a._1 - b._1\n+    }\n+  }\n+\n+  /**\n+   * A comparator for (Int, K) pairs that orders them both by their partition ID and a key ordering.\n+   */\n+  def partitionKeyComparator[K](keyComparator: Comparator[K]): Comparator[(Int, K)] = {\n+    new Comparator[(Int, K)] {\n+      override def compare(a: (Int, K), b: (Int, K)): Int = {\n+        val partitionDiff = a._1 - b._1\n+        if (partitionDiff != 0) {\n+          partitionDiff\n+        } else {\n+          keyComparator.compare(a._2, b._2)\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * Iterator that writes elements to a BlockObjectWriter instead of returning them. Each element\n+ * has an associated partition.\n+ */\n+private[spark] trait WritablePartitionedIterator {\n+  def writeNext(writer: BlockObjectWriter): Unit\n+\n+  def hasNext(): Boolean\n+\n+  def nextPartition(): Int",
    "line": 95
  }, {
    "author": {
      "login": "sryza"
    },
    "body": "`WritablePartitionedIterator` doesn't have a `next` method (just `writeNext`), so I don't think there would be anything to peek at.\n",
    "commit": "8c70dd909c20af6dacaf4091e964a1239a221016",
    "createdAt": "2015-04-27T21:27:56Z",
    "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.collection\n+\n+import java.util.Comparator\n+\n+import org.apache.spark.storage.BlockObjectWriter\n+\n+/**\n+ * A common interface for size-tracking collections of key-value pairs that\n+ * - Have an associated partition for each key-value pair.\n+ * - Support a memory-efficient sorted iterator\n+ * - Support a WritablePartitionedIterator for writing the contents directly as bytes.\n+ */\n+private[spark] trait WritablePartitionedPairCollection[K, V] extends SizeTracker {\n+  /**\n+   * Insert a key-value pair with a partition into the collection\n+   */\n+  def insert(partition: Int, key: K, value: V): Unit\n+\n+  /**\n+   * Estimate the collection's current memory usage in bytes.\n+   */\n+  def estimateSize(): Long\n+\n+  /**\n+   * Iterate through the data in order of partition ID and then the given comparator. This may\n+   * destroy the underlying collection.\n+   */\n+  def partitionedDestructiveSortedIterator(keyComparator: Comparator[K]): Iterator[((Int, K), V)]\n+\n+  /**\n+   * Iterate through the data and write out the elements instead of returning them. Records are\n+   * returned in order of their partition ID and then the given comparator.\n+   * This may destroy the underlying collection.\n+   */\n+  def destructiveSortedWritablePartitionedIterator(keyComparator: Comparator[K])\n+    : WritablePartitionedIterator = {\n+    WritablePartitionedIterator.fromIterator(partitionedDestructiveSortedIterator(keyComparator))\n+  }\n+\n+  /**\n+   * Iterate through the data and write out the elements instead of returning them.\n+   */\n+  def writablePartitionedIterator(): WritablePartitionedIterator\n+}\n+\n+private[spark] object WritablePartitionedPairCollection {\n+  /**\n+   * A comparator for (Int, K) pairs that orders them by only their partition ID.\n+   */\n+  def partitionComparator[K]: Comparator[(Int, K)] = new Comparator[(Int, K)] {\n+    override def compare(a: (Int, K), b: (Int, K)): Int = {\n+      a._1 - b._1\n+    }\n+  }\n+\n+  /**\n+   * A comparator for (Int, K) pairs that orders them both by their partition ID and a key ordering.\n+   */\n+  def partitionKeyComparator[K](keyComparator: Comparator[K]): Comparator[(Int, K)] = {\n+    new Comparator[(Int, K)] {\n+      override def compare(a: (Int, K), b: (Int, K)): Int = {\n+        val partitionDiff = a._1 - b._1\n+        if (partitionDiff != 0) {\n+          partitionDiff\n+        } else {\n+          keyComparator.compare(a._2, b._2)\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * Iterator that writes elements to a BlockObjectWriter instead of returning them. Each element\n+ * has an associated partition.\n+ */\n+private[spark] trait WritablePartitionedIterator {\n+  def writeNext(writer: BlockObjectWriter): Unit\n+\n+  def hasNext(): Boolean\n+\n+  def nextPartition(): Int",
    "line": 95
  }],
  "prId": 4450
}]