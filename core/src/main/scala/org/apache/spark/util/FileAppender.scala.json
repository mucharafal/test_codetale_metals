[{
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "This and one other comment reference `log4j`, but I think these classes are actually inside of Spark.\n",
    "commit": "fd8f87fde302824813222e1a81003dea32010da7",
    "createdAt": "2014-06-05T00:08:41Z",
    "diffHunk": "@@ -0,0 +1,429 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util\n+\n+import java.io.{File, FileFilter, FileOutputStream, InputStream}\n+import java.text.SimpleDateFormat\n+import java.util.Calendar\n+import java.util.concurrent.{Executors, ThreadFactory}\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.spark.{Logging, SparkConf}\n+\n+/**\n+ * Continuously appends the data from an input stream into the given file.\n+ */\n+private[spark] class FileAppender(inputStream: InputStream, file: File, bufferSize: Int = 8192)\n+  extends Logging {\n+  @volatile private var outputStream: FileOutputStream = null\n+  @volatile private var markedForStop = false     // has the appender been asked to stopped\n+  @volatile private var stopped = false           // has the appender stopped\n+\n+  // Thread that reads the input stream and writes to file\n+  private val writingThread = new Thread(\"File appending thread for \" + file) {\n+    setDaemon(true)\n+    override def run() {\n+      Utils.logUncaughtExceptions {\n+        appendStreamToFile()\n+      }\n+    }\n+  }\n+  writingThread.start()\n+\n+  /**\n+   * Wait for the appender to stop appending, either because input stream is closed\n+   * or because of any error in appending\n+   */\n+  def awaitTermination() {\n+    synchronized {\n+      if (!stopped) {\n+        wait()\n+      }\n+    }\n+  }\n+\n+  /** Stop the appender */\n+  def stop() {\n+    markedForStop = true\n+  }\n+\n+  /** Continuously read chunks from the input stream and append to the file */\n+  protected def appendStreamToFile() {\n+    try {\n+      logDebug(\"Started appending thread\")\n+      openFile()\n+      val buf = new Array[Byte](bufferSize)\n+      var n = 0\n+      while (!markedForStop && n != -1) {\n+        n = inputStream.read(buf)\n+        if (n != -1) {\n+          appendToFile(buf, n)\n+        }\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Error writing stream to file $file\", e)\n+    } finally {\n+      closeFile()\n+      synchronized {\n+        stopped = true\n+        notifyAll()\n+      }\n+    }\n+  }\n+\n+  /** Append bytes to the file output stream */\n+  protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (outputStream == null) {\n+      openFile()\n+    }\n+    outputStream.write(bytes, 0, len)\n+  }\n+\n+  /** Open the file output stream */\n+  protected def openFile() {\n+    outputStream = new FileOutputStream(file, true)\n+    logDebug(s\"Opened file $file\")\n+  }\n+\n+  /** Close the file output stream */\n+  protected def closeFile() {\n+    outputStream.flush()\n+    outputStream.close()\n+    logDebug(s\"Closed file $file\")\n+  }\n+}\n+\n+/**\n+ * Companion object to [[org.apache.log4j.FileAppender]] which has helper\n+ * functions to choose the write type of FileAppender based on SparkConf configuration.\n+ */\n+private[spark] object FileAppender extends Logging {\n+\n+  /** Create the right appender based on Spark configuration */\n+  def apply(inputStream: InputStream, file: File, conf: SparkConf): FileAppender = {\n+\n+    import RollingFileAppender._\n+\n+    val rolloverEnabled = conf.getBoolean(ENABLE_PROPERTY, false)\n+    logDebug(\"Log rollover enabled = \" + rolloverEnabled)\n+    if (rolloverEnabled) {\n+\n+      val rolloverSizeOption = conf.getOption(SIZE_PROPERTY)\n+      val rolloverIntervalOption = conf.getOption(INTERVAL_PROPERTY)\n+\n+      (rolloverIntervalOption, rolloverSizeOption) match {\n+\n+        case (Some(rolloverInterval), Some(rolloverSize)) =>              // if both size and interval have been set\n+          logWarning(s\"Rollover interval [$rolloverInterval] and size [$rolloverSize] \" +\n+            s\"both set for executor logs, rolling logs not enabled\")\n+          new FileAppender(inputStream, file)\n+\n+        case (Some(rolloverInterval), None) =>  // if interval has been set\n+          rolloverInterval match {\n+            case \"daily\" =>\n+              logInfo(s\"Rolling executor logs enabled for $file with daily rolling\")\n+              new DailyRollingFileAppender(inputStream, file, conf)\n+            case \"hourly\" =>\n+              logInfo(s\"Rolling executor logs enabled for $file with hourly rolling\")\n+              new HourlyRollingFileAppender(inputStream, file, conf)\n+            case \"minutely\" =>\n+              logInfo(s\"Rolling executor logs enabled for $file with rolling every minute\")\n+              new MinutelyRollingFileAppender(inputStream, file, conf)\n+            case IntParam(millis) =>\n+              logInfo(s\"Rolling executor logs enabled for $file with rolling every minute\")\n+              new RollingFileAppender(inputStream, file, new TimeBasedRollingPolicy(millis),\n+                s\"--YYYY-MM-dd--HH-mm-ss-SSSS\", conf)\n+            case _ =>\n+              logWarning(\n+                s\"Illegal interval for rolling executor logs [$rolloverInterval], \" +\n+                  s\"rolling logs not enabled\")\n+              new FileAppender(inputStream, file)\n+          }\n+\n+        case (None, Some(rolloverSize)) =>    // if size has been set\n+          rolloverSize match {\n+            case IntParam(bytes) =>\n+              logInfo(s\"Rolling executor logs enabled for $file with rolling every $bytes bytes\")\n+              new RollingFileAppender(inputStream, file, new SizeBasedRollingPolicy(bytes),\n+                s\"--YYYY-MM-dd--HH-mm-ss-SSSS\", conf)\n+            case _ =>\n+              logWarning(\n+                s\"Illegal size for rolling executor logs [$rolloverSize], \" +\n+                  s\"rolling logs not enabled\")\n+              new FileAppender(inputStream, file)\n+          }\n+\n+        case (None, None) =>                // if neither size nor interval has been set\n+          logWarning(s\"Interval and size for rolling executor logs not set, \" +\n+            s\"rolling logs enabled with daily rolling.\")\n+          new DailyRollingFileAppender(inputStream, file, conf)\n+      }\n+    } else {\n+      new FileAppender(inputStream, file)\n+    }\n+  }\n+}\n+\n+/**\n+ * Defines the policy based on which [[org.apache.log4j.RollingFileAppender]] will"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Ooops, thats the wrong RollingFileAppender. Damn you autocompletion!\n",
    "commit": "fd8f87fde302824813222e1a81003dea32010da7",
    "createdAt": "2014-06-05T00:15:48Z",
    "diffHunk": "@@ -0,0 +1,429 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util\n+\n+import java.io.{File, FileFilter, FileOutputStream, InputStream}\n+import java.text.SimpleDateFormat\n+import java.util.Calendar\n+import java.util.concurrent.{Executors, ThreadFactory}\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.spark.{Logging, SparkConf}\n+\n+/**\n+ * Continuously appends the data from an input stream into the given file.\n+ */\n+private[spark] class FileAppender(inputStream: InputStream, file: File, bufferSize: Int = 8192)\n+  extends Logging {\n+  @volatile private var outputStream: FileOutputStream = null\n+  @volatile private var markedForStop = false     // has the appender been asked to stopped\n+  @volatile private var stopped = false           // has the appender stopped\n+\n+  // Thread that reads the input stream and writes to file\n+  private val writingThread = new Thread(\"File appending thread for \" + file) {\n+    setDaemon(true)\n+    override def run() {\n+      Utils.logUncaughtExceptions {\n+        appendStreamToFile()\n+      }\n+    }\n+  }\n+  writingThread.start()\n+\n+  /**\n+   * Wait for the appender to stop appending, either because input stream is closed\n+   * or because of any error in appending\n+   */\n+  def awaitTermination() {\n+    synchronized {\n+      if (!stopped) {\n+        wait()\n+      }\n+    }\n+  }\n+\n+  /** Stop the appender */\n+  def stop() {\n+    markedForStop = true\n+  }\n+\n+  /** Continuously read chunks from the input stream and append to the file */\n+  protected def appendStreamToFile() {\n+    try {\n+      logDebug(\"Started appending thread\")\n+      openFile()\n+      val buf = new Array[Byte](bufferSize)\n+      var n = 0\n+      while (!markedForStop && n != -1) {\n+        n = inputStream.read(buf)\n+        if (n != -1) {\n+          appendToFile(buf, n)\n+        }\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Error writing stream to file $file\", e)\n+    } finally {\n+      closeFile()\n+      synchronized {\n+        stopped = true\n+        notifyAll()\n+      }\n+    }\n+  }\n+\n+  /** Append bytes to the file output stream */\n+  protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (outputStream == null) {\n+      openFile()\n+    }\n+    outputStream.write(bytes, 0, len)\n+  }\n+\n+  /** Open the file output stream */\n+  protected def openFile() {\n+    outputStream = new FileOutputStream(file, true)\n+    logDebug(s\"Opened file $file\")\n+  }\n+\n+  /** Close the file output stream */\n+  protected def closeFile() {\n+    outputStream.flush()\n+    outputStream.close()\n+    logDebug(s\"Closed file $file\")\n+  }\n+}\n+\n+/**\n+ * Companion object to [[org.apache.log4j.FileAppender]] which has helper\n+ * functions to choose the write type of FileAppender based on SparkConf configuration.\n+ */\n+private[spark] object FileAppender extends Logging {\n+\n+  /** Create the right appender based on Spark configuration */\n+  def apply(inputStream: InputStream, file: File, conf: SparkConf): FileAppender = {\n+\n+    import RollingFileAppender._\n+\n+    val rolloverEnabled = conf.getBoolean(ENABLE_PROPERTY, false)\n+    logDebug(\"Log rollover enabled = \" + rolloverEnabled)\n+    if (rolloverEnabled) {\n+\n+      val rolloverSizeOption = conf.getOption(SIZE_PROPERTY)\n+      val rolloverIntervalOption = conf.getOption(INTERVAL_PROPERTY)\n+\n+      (rolloverIntervalOption, rolloverSizeOption) match {\n+\n+        case (Some(rolloverInterval), Some(rolloverSize)) =>              // if both size and interval have been set\n+          logWarning(s\"Rollover interval [$rolloverInterval] and size [$rolloverSize] \" +\n+            s\"both set for executor logs, rolling logs not enabled\")\n+          new FileAppender(inputStream, file)\n+\n+        case (Some(rolloverInterval), None) =>  // if interval has been set\n+          rolloverInterval match {\n+            case \"daily\" =>\n+              logInfo(s\"Rolling executor logs enabled for $file with daily rolling\")\n+              new DailyRollingFileAppender(inputStream, file, conf)\n+            case \"hourly\" =>\n+              logInfo(s\"Rolling executor logs enabled for $file with hourly rolling\")\n+              new HourlyRollingFileAppender(inputStream, file, conf)\n+            case \"minutely\" =>\n+              logInfo(s\"Rolling executor logs enabled for $file with rolling every minute\")\n+              new MinutelyRollingFileAppender(inputStream, file, conf)\n+            case IntParam(millis) =>\n+              logInfo(s\"Rolling executor logs enabled for $file with rolling every minute\")\n+              new RollingFileAppender(inputStream, file, new TimeBasedRollingPolicy(millis),\n+                s\"--YYYY-MM-dd--HH-mm-ss-SSSS\", conf)\n+            case _ =>\n+              logWarning(\n+                s\"Illegal interval for rolling executor logs [$rolloverInterval], \" +\n+                  s\"rolling logs not enabled\")\n+              new FileAppender(inputStream, file)\n+          }\n+\n+        case (None, Some(rolloverSize)) =>    // if size has been set\n+          rolloverSize match {\n+            case IntParam(bytes) =>\n+              logInfo(s\"Rolling executor logs enabled for $file with rolling every $bytes bytes\")\n+              new RollingFileAppender(inputStream, file, new SizeBasedRollingPolicy(bytes),\n+                s\"--YYYY-MM-dd--HH-mm-ss-SSSS\", conf)\n+            case _ =>\n+              logWarning(\n+                s\"Illegal size for rolling executor logs [$rolloverSize], \" +\n+                  s\"rolling logs not enabled\")\n+              new FileAppender(inputStream, file)\n+          }\n+\n+        case (None, None) =>                // if neither size nor interval has been set\n+          logWarning(s\"Interval and size for rolling executor logs not set, \" +\n+            s\"rolling logs enabled with daily rolling.\")\n+          new DailyRollingFileAppender(inputStream, file, conf)\n+      }\n+    } else {\n+      new FileAppender(inputStream, file)\n+    }\n+  }\n+}\n+\n+/**\n+ * Defines the policy based on which [[org.apache.log4j.RollingFileAppender]] will"
  }],
  "prId": 895
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "Are these individual classes really necessary? The main thing here seems to be the string format... I'd actually prefer anyways to keep the string format the same for all of them and just use one that includes all the way down to seconds (d, m, y, h, m, s). That way if people switch back and fourth among log intervals, they don't have to change their tools that are reading logs.\n",
    "commit": "fd8f87fde302824813222e1a81003dea32010da7",
    "createdAt": "2014-06-05T00:26:30Z",
    "diffHunk": "@@ -0,0 +1,429 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util\n+\n+import java.io.{File, FileFilter, FileOutputStream, InputStream}\n+import java.text.SimpleDateFormat\n+import java.util.Calendar\n+import java.util.concurrent.{Executors, ThreadFactory}\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.spark.{Logging, SparkConf}\n+\n+/**\n+ * Continuously appends the data from an input stream into the given file.\n+ */\n+private[spark] class FileAppender(inputStream: InputStream, file: File, bufferSize: Int = 8192)\n+  extends Logging {\n+  @volatile private var outputStream: FileOutputStream = null\n+  @volatile private var markedForStop = false     // has the appender been asked to stopped\n+  @volatile private var stopped = false           // has the appender stopped\n+\n+  // Thread that reads the input stream and writes to file\n+  private val writingThread = new Thread(\"File appending thread for \" + file) {\n+    setDaemon(true)\n+    override def run() {\n+      Utils.logUncaughtExceptions {\n+        appendStreamToFile()\n+      }\n+    }\n+  }\n+  writingThread.start()\n+\n+  /**\n+   * Wait for the appender to stop appending, either because input stream is closed\n+   * or because of any error in appending\n+   */\n+  def awaitTermination() {\n+    synchronized {\n+      if (!stopped) {\n+        wait()\n+      }\n+    }\n+  }\n+\n+  /** Stop the appender */\n+  def stop() {\n+    markedForStop = true\n+  }\n+\n+  /** Continuously read chunks from the input stream and append to the file */\n+  protected def appendStreamToFile() {\n+    try {\n+      logDebug(\"Started appending thread\")\n+      openFile()\n+      val buf = new Array[Byte](bufferSize)\n+      var n = 0\n+      while (!markedForStop && n != -1) {\n+        n = inputStream.read(buf)\n+        if (n != -1) {\n+          appendToFile(buf, n)\n+        }\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Error writing stream to file $file\", e)\n+    } finally {\n+      closeFile()\n+      synchronized {\n+        stopped = true\n+        notifyAll()\n+      }\n+    }\n+  }\n+\n+  /** Append bytes to the file output stream */\n+  protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (outputStream == null) {\n+      openFile()\n+    }\n+    outputStream.write(bytes, 0, len)\n+  }\n+\n+  /** Open the file output stream */\n+  protected def openFile() {\n+    outputStream = new FileOutputStream(file, true)\n+    logDebug(s\"Opened file $file\")\n+  }\n+\n+  /** Close the file output stream */\n+  protected def closeFile() {\n+    outputStream.flush()\n+    outputStream.close()\n+    logDebug(s\"Closed file $file\")\n+  }\n+}\n+\n+/**\n+ * Companion object to [[org.apache.log4j.FileAppender]] which has helper\n+ * functions to choose the write type of FileAppender based on SparkConf configuration.\n+ */\n+private[spark] object FileAppender extends Logging {\n+\n+  /** Create the right appender based on Spark configuration */\n+  def apply(inputStream: InputStream, file: File, conf: SparkConf): FileAppender = {\n+\n+    import RollingFileAppender._\n+\n+    val rolloverEnabled = conf.getBoolean(ENABLE_PROPERTY, false)\n+    logDebug(\"Log rollover enabled = \" + rolloverEnabled)\n+    if (rolloverEnabled) {\n+\n+      val rolloverSizeOption = conf.getOption(SIZE_PROPERTY)\n+      val rolloverIntervalOption = conf.getOption(INTERVAL_PROPERTY)\n+\n+      (rolloverIntervalOption, rolloverSizeOption) match {\n+\n+        case (Some(rolloverInterval), Some(rolloverSize)) =>              // if both size and interval have been set\n+          logWarning(s\"Rollover interval [$rolloverInterval] and size [$rolloverSize] \" +\n+            s\"both set for executor logs, rolling logs not enabled\")\n+          new FileAppender(inputStream, file)\n+\n+        case (Some(rolloverInterval), None) =>  // if interval has been set\n+          rolloverInterval match {\n+            case \"daily\" =>\n+              logInfo(s\"Rolling executor logs enabled for $file with daily rolling\")\n+              new DailyRollingFileAppender(inputStream, file, conf)\n+            case \"hourly\" =>\n+              logInfo(s\"Rolling executor logs enabled for $file with hourly rolling\")\n+              new HourlyRollingFileAppender(inputStream, file, conf)\n+            case \"minutely\" =>\n+              logInfo(s\"Rolling executor logs enabled for $file with rolling every minute\")\n+              new MinutelyRollingFileAppender(inputStream, file, conf)\n+            case IntParam(millis) =>\n+              logInfo(s\"Rolling executor logs enabled for $file with rolling every minute\")\n+              new RollingFileAppender(inputStream, file, new TimeBasedRollingPolicy(millis),\n+                s\"--YYYY-MM-dd--HH-mm-ss-SSSS\", conf)\n+            case _ =>\n+              logWarning(\n+                s\"Illegal interval for rolling executor logs [$rolloverInterval], \" +\n+                  s\"rolling logs not enabled\")\n+              new FileAppender(inputStream, file)\n+          }\n+\n+        case (None, Some(rolloverSize)) =>    // if size has been set\n+          rolloverSize match {\n+            case IntParam(bytes) =>\n+              logInfo(s\"Rolling executor logs enabled for $file with rolling every $bytes bytes\")\n+              new RollingFileAppender(inputStream, file, new SizeBasedRollingPolicy(bytes),\n+                s\"--YYYY-MM-dd--HH-mm-ss-SSSS\", conf)\n+            case _ =>\n+              logWarning(\n+                s\"Illegal size for rolling executor logs [$rolloverSize], \" +\n+                  s\"rolling logs not enabled\")\n+              new FileAppender(inputStream, file)\n+          }\n+\n+        case (None, None) =>                // if neither size nor interval has been set\n+          logWarning(s\"Interval and size for rolling executor logs not set, \" +\n+            s\"rolling logs enabled with daily rolling.\")\n+          new DailyRollingFileAppender(inputStream, file, conf)\n+      }\n+    } else {\n+      new FileAppender(inputStream, file)\n+    }\n+  }\n+}\n+\n+/**\n+ * Defines the policy based on which [[org.apache.log4j.RollingFileAppender]] will\n+ * generate rolling files.\n+ */\n+private[spark] trait RollingPolicy {\n+\n+  /** Whether rollover should be initiated at this moment */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean\n+\n+  /** Notify that rollover has occurred */\n+  def rolledOver()\n+\n+  /** Notify that bytes have been written */\n+  def bytesWritten(bytes: Long)\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] by which files will be rolled\n+ * over at a fixed interval.\n+ */\n+private[spark] class TimeBasedRollingPolicy(val rolloverIntervalMillis: Long)\n+  extends RollingPolicy with Logging {\n+\n+  require(rolloverIntervalMillis >= 100)\n+\n+  @volatile private var nextRolloverTime = calculateNextRolloverTime()\n+\n+  /** Should rollover if current time has exceeded next rollover time */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean = {\n+    System.currentTimeMillis > nextRolloverTime\n+  }\n+\n+  /** Rollover has occurred, so find the next time to rollover */\n+  def rolledOver() {\n+    nextRolloverTime = calculateNextRolloverTime()\n+    logDebug(s\"Current time: ${System.currentTimeMillis}, next rollover time: \" + nextRolloverTime)\n+  }\n+\n+  def bytesWritten(bytes: Long) { }  // nothing to do\n+\n+  private def calculateNextRolloverTime(): Long = {\n+    val now = System.currentTimeMillis()\n+    val targetTime = math.ceil(now.toDouble / rolloverIntervalMillis) * rolloverIntervalMillis\n+    targetTime.toLong + 1  // +1 to make sure this falls in the next interval\n+  }\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] bywhich files will be rolled\n+ * over after reaching a particular size.\n+ */\n+private[spark] class SizeBasedRollingPolicy(val rolloverSizeBytes: Long)\n+  extends RollingPolicy with Logging {\n+\n+  require(rolloverSizeBytes >= 1000)\n+\n+  @volatile private var bytesWrittenTillNow = 0L\n+\n+  /** Should rollover if the next set of bytes is going to exceed the size limit */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean = {\n+    bytesToBeWritten + bytesWrittenTillNow > rolloverSizeBytes\n+  }\n+\n+  /** Rollover has occurred, so reset the counter */\n+  def rolledOver() {\n+    bytesWrittenTillNow = 0\n+  }\n+\n+  /** Increment the bytes that have been written in the current file */\n+  def bytesWritten(bytes: Long) {\n+    bytesWrittenTillNow += bytes\n+  }\n+}\n+\n+/**\n+ * Continuously appends data from input stream into the given file, and rolls\n+ * over the file after the given interval. The rolled over files are named\n+ * based on the given pattern.\n+ *\n+ * @param inputStream             Input stream to read data from\n+ * @param activeFile              File to write data to\n+ * @param rollingPolicy           Policy based on which files will be rolled over.\n+ * @param rollingFilePattern      Pattern based on which the rolled over files will be named.\n+ *                                Uses SimpleDataFormat pattern.\n+ * @param conf                    SparkConf that is used to pass on extra configurations\n+ * @param bufferSize              Optional buffer size. Used mainly for testing.\n+ */\n+private[spark] class RollingFileAppender(\n+    inputStream: InputStream,\n+    activeFile: File,\n+    val rollingPolicy: RollingPolicy,\n+    rollingFilePattern: String,\n+    conf: SparkConf,\n+    bufferSize: Int = 8192\n+  ) extends FileAppender(inputStream, activeFile, bufferSize) {\n+\n+  import RollingFileAppender._\n+\n+  private val retainCount = conf.getInt(KEEP_LAST_PROPERTY, -1)\n+  private val formatter = new SimpleDateFormat(rollingFilePattern)\n+\n+  private val executor = Executors.newFixedThreadPool(1, new ThreadFactory{\n+    def newThread(r: Runnable): Thread = {\n+      val t = new Thread(r)\n+      t.setDaemon(true)\n+      t.setName(s\"Threadpool of ${RollingFileAppender.this.getClass.getSimpleName} for $activeFile\")\n+      t\n+    }\n+  })\n+\n+  /** Stop the appender */\n+  override def stop() {\n+    super.stop()\n+    //cleanupTask.cancel(true)\n+    executor.shutdownNow()\n+  }\n+\n+  /** Append bytes to file after rolling over is necessary */\n+  override protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (rollingPolicy.shouldRollover(len)) {\n+      rollover()\n+      rollingPolicy.rolledOver()\n+    }\n+    super.appendToFile(bytes, len)\n+    rollingPolicy.bytesWritten(len)\n+  }\n+\n+  /** Rollover the file, by closing the output stream and moving it over */\n+  private def rollover() {\n+    val rolloverSuffix = formatter.format(Calendar.getInstance.getTime)\n+    val rolloverFile = new File(\n+      activeFile.getParentFile, activeFile.getName + rolloverSuffix).getAbsoluteFile\n+    logDebug(\"Attempting to rollover at \" + System.currentTimeMillis + \" to file \" + rolloverFile)\n+\n+    try {\n+      closeFile()\n+      if (activeFile.exists) {\n+        if (!rolloverFile.exists) {\n+          FileUtils.moveFile(activeFile, rolloverFile)\n+          logInfo(s\"Rolled over $activeFile to $rolloverFile\")\n+        } else {\n+          // In case the rollover file name clashes, make a unique file name.\n+          // The resultant file names are long and ugly, so this is used only\n+          // if there is a name collision. This can be avoided by the using\n+          // the right pattern such that name collisions do not occur.\n+          var i = 0\n+          var altRolloverFile: File = null\n+          do {\n+            altRolloverFile = new File(activeFile.getParent,\n+              s\"${activeFile.getName}$rolloverSuffix--$i\").getAbsoluteFile\n+            i += 1\n+          } while (i < 10000 && altRolloverFile.exists)\n+\n+          logWarning(s\"Rollover file $rolloverFile already exists, \" +\n+            s\"rolled over $activeFile to file $altRolloverFile\")\n+          logWarning(s\"Make sure that the given file name pattern [$rollingFilePattern]\")\n+          FileUtils.moveFile(activeFile, altRolloverFile)\n+        }\n+      } else {\n+        logWarning(s\"File $activeFile does not exist\")\n+      }\n+      openFile()\n+      scheduleCleanup()\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Error rolling over $activeFile to $rolloverFile\", e)\n+    }\n+  }\n+\n+  /** Schedule cleaning up for older rolled over files */\n+  private def scheduleCleanup() {\n+    if (retainCount > 0) {\n+      executor.submit(new Runnable() { override def run() { cleanup() }})\n+    }\n+  }\n+\n+  /** Retain only last few files */\n+  private[util] def cleanup() {\n+    try {\n+      val rolledoverFiles = activeFile.getParentFile.listFiles(new FileFilter {\n+        def accept(f: File): Boolean = {\n+          f.getName.startsWith(activeFile.getName) && f != activeFile\n+        }\n+      }).sorted\n+      val filesToBeDeleted = rolledoverFiles.take(\n+        math.max(0, rolledoverFiles.size - retainCount))\n+      filesToBeDeleted.foreach { file =>\n+        logInfo(s\"Deleting file executor log file ${file.getAbsolutePath}\")\n+        file.delete()\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(\"Error cleaning logs in directory \" + activeFile.getParentFile.getAbsolutePath, e)\n+    }\n+  }\n+}\n+\n+/**\n+ * Companion object to [[org.apache.spark.util.RollingFileAppender]]. Defines\n+ * names of configurations that configure rolling file appenders.\n+ */\n+private[spark] object RollingFileAppender {\n+  val ENABLE_PROPERTY = \"spark.executor.rollingLogs.enabled\"\n+  val INTERVAL_PROPERTY = \"spark.executor.rollingLogs.interval\"\n+  val SIZE_PROPERTY = \"spark.executor.rollingLogs.size\"\n+  val KEEP_LAST_PROPERTY = \"spark.executor.rollingLogs.keepLastN\"\n+\n+}\n+\n+/** RollingFileAppender that rolls over every minute */\n+private[spark] class MinutelyRollingFileAppender("
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "I have the same dilemma, as well, and was waiting for someone to comment on\nit. The only reason for having the these different formats is that if you\nare doing daily rolling, it looks a little ugly to have the h,m,s be\nattached to the name. A case in point is log4j's [RollingFileAppender](http://logging.apache.org/log4j/extras/apidocs/org/apache/log4j/rolling/RollingFileAppender.html).\nThey tie up the rolling interval with the file naming pattern - if the\npattern has hour in it, then the rolling is done every hour. If the pattern\nhas only day in it, then rolling is done every day.\n\nIf people are interested in accessing the rolling log files directly for\ntheir processing, its pretty easy to find all the files as all of them start\nwith `stdout` or `stderr` irrespective of the interval.\n",
    "commit": "fd8f87fde302824813222e1a81003dea32010da7",
    "createdAt": "2014-06-05T00:39:47Z",
    "diffHunk": "@@ -0,0 +1,429 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util\n+\n+import java.io.{File, FileFilter, FileOutputStream, InputStream}\n+import java.text.SimpleDateFormat\n+import java.util.Calendar\n+import java.util.concurrent.{Executors, ThreadFactory}\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.spark.{Logging, SparkConf}\n+\n+/**\n+ * Continuously appends the data from an input stream into the given file.\n+ */\n+private[spark] class FileAppender(inputStream: InputStream, file: File, bufferSize: Int = 8192)\n+  extends Logging {\n+  @volatile private var outputStream: FileOutputStream = null\n+  @volatile private var markedForStop = false     // has the appender been asked to stopped\n+  @volatile private var stopped = false           // has the appender stopped\n+\n+  // Thread that reads the input stream and writes to file\n+  private val writingThread = new Thread(\"File appending thread for \" + file) {\n+    setDaemon(true)\n+    override def run() {\n+      Utils.logUncaughtExceptions {\n+        appendStreamToFile()\n+      }\n+    }\n+  }\n+  writingThread.start()\n+\n+  /**\n+   * Wait for the appender to stop appending, either because input stream is closed\n+   * or because of any error in appending\n+   */\n+  def awaitTermination() {\n+    synchronized {\n+      if (!stopped) {\n+        wait()\n+      }\n+    }\n+  }\n+\n+  /** Stop the appender */\n+  def stop() {\n+    markedForStop = true\n+  }\n+\n+  /** Continuously read chunks from the input stream and append to the file */\n+  protected def appendStreamToFile() {\n+    try {\n+      logDebug(\"Started appending thread\")\n+      openFile()\n+      val buf = new Array[Byte](bufferSize)\n+      var n = 0\n+      while (!markedForStop && n != -1) {\n+        n = inputStream.read(buf)\n+        if (n != -1) {\n+          appendToFile(buf, n)\n+        }\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Error writing stream to file $file\", e)\n+    } finally {\n+      closeFile()\n+      synchronized {\n+        stopped = true\n+        notifyAll()\n+      }\n+    }\n+  }\n+\n+  /** Append bytes to the file output stream */\n+  protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (outputStream == null) {\n+      openFile()\n+    }\n+    outputStream.write(bytes, 0, len)\n+  }\n+\n+  /** Open the file output stream */\n+  protected def openFile() {\n+    outputStream = new FileOutputStream(file, true)\n+    logDebug(s\"Opened file $file\")\n+  }\n+\n+  /** Close the file output stream */\n+  protected def closeFile() {\n+    outputStream.flush()\n+    outputStream.close()\n+    logDebug(s\"Closed file $file\")\n+  }\n+}\n+\n+/**\n+ * Companion object to [[org.apache.log4j.FileAppender]] which has helper\n+ * functions to choose the write type of FileAppender based on SparkConf configuration.\n+ */\n+private[spark] object FileAppender extends Logging {\n+\n+  /** Create the right appender based on Spark configuration */\n+  def apply(inputStream: InputStream, file: File, conf: SparkConf): FileAppender = {\n+\n+    import RollingFileAppender._\n+\n+    val rolloverEnabled = conf.getBoolean(ENABLE_PROPERTY, false)\n+    logDebug(\"Log rollover enabled = \" + rolloverEnabled)\n+    if (rolloverEnabled) {\n+\n+      val rolloverSizeOption = conf.getOption(SIZE_PROPERTY)\n+      val rolloverIntervalOption = conf.getOption(INTERVAL_PROPERTY)\n+\n+      (rolloverIntervalOption, rolloverSizeOption) match {\n+\n+        case (Some(rolloverInterval), Some(rolloverSize)) =>              // if both size and interval have been set\n+          logWarning(s\"Rollover interval [$rolloverInterval] and size [$rolloverSize] \" +\n+            s\"both set for executor logs, rolling logs not enabled\")\n+          new FileAppender(inputStream, file)\n+\n+        case (Some(rolloverInterval), None) =>  // if interval has been set\n+          rolloverInterval match {\n+            case \"daily\" =>\n+              logInfo(s\"Rolling executor logs enabled for $file with daily rolling\")\n+              new DailyRollingFileAppender(inputStream, file, conf)\n+            case \"hourly\" =>\n+              logInfo(s\"Rolling executor logs enabled for $file with hourly rolling\")\n+              new HourlyRollingFileAppender(inputStream, file, conf)\n+            case \"minutely\" =>\n+              logInfo(s\"Rolling executor logs enabled for $file with rolling every minute\")\n+              new MinutelyRollingFileAppender(inputStream, file, conf)\n+            case IntParam(millis) =>\n+              logInfo(s\"Rolling executor logs enabled for $file with rolling every minute\")\n+              new RollingFileAppender(inputStream, file, new TimeBasedRollingPolicy(millis),\n+                s\"--YYYY-MM-dd--HH-mm-ss-SSSS\", conf)\n+            case _ =>\n+              logWarning(\n+                s\"Illegal interval for rolling executor logs [$rolloverInterval], \" +\n+                  s\"rolling logs not enabled\")\n+              new FileAppender(inputStream, file)\n+          }\n+\n+        case (None, Some(rolloverSize)) =>    // if size has been set\n+          rolloverSize match {\n+            case IntParam(bytes) =>\n+              logInfo(s\"Rolling executor logs enabled for $file with rolling every $bytes bytes\")\n+              new RollingFileAppender(inputStream, file, new SizeBasedRollingPolicy(bytes),\n+                s\"--YYYY-MM-dd--HH-mm-ss-SSSS\", conf)\n+            case _ =>\n+              logWarning(\n+                s\"Illegal size for rolling executor logs [$rolloverSize], \" +\n+                  s\"rolling logs not enabled\")\n+              new FileAppender(inputStream, file)\n+          }\n+\n+        case (None, None) =>                // if neither size nor interval has been set\n+          logWarning(s\"Interval and size for rolling executor logs not set, \" +\n+            s\"rolling logs enabled with daily rolling.\")\n+          new DailyRollingFileAppender(inputStream, file, conf)\n+      }\n+    } else {\n+      new FileAppender(inputStream, file)\n+    }\n+  }\n+}\n+\n+/**\n+ * Defines the policy based on which [[org.apache.log4j.RollingFileAppender]] will\n+ * generate rolling files.\n+ */\n+private[spark] trait RollingPolicy {\n+\n+  /** Whether rollover should be initiated at this moment */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean\n+\n+  /** Notify that rollover has occurred */\n+  def rolledOver()\n+\n+  /** Notify that bytes have been written */\n+  def bytesWritten(bytes: Long)\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] by which files will be rolled\n+ * over at a fixed interval.\n+ */\n+private[spark] class TimeBasedRollingPolicy(val rolloverIntervalMillis: Long)\n+  extends RollingPolicy with Logging {\n+\n+  require(rolloverIntervalMillis >= 100)\n+\n+  @volatile private var nextRolloverTime = calculateNextRolloverTime()\n+\n+  /** Should rollover if current time has exceeded next rollover time */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean = {\n+    System.currentTimeMillis > nextRolloverTime\n+  }\n+\n+  /** Rollover has occurred, so find the next time to rollover */\n+  def rolledOver() {\n+    nextRolloverTime = calculateNextRolloverTime()\n+    logDebug(s\"Current time: ${System.currentTimeMillis}, next rollover time: \" + nextRolloverTime)\n+  }\n+\n+  def bytesWritten(bytes: Long) { }  // nothing to do\n+\n+  private def calculateNextRolloverTime(): Long = {\n+    val now = System.currentTimeMillis()\n+    val targetTime = math.ceil(now.toDouble / rolloverIntervalMillis) * rolloverIntervalMillis\n+    targetTime.toLong + 1  // +1 to make sure this falls in the next interval\n+  }\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] bywhich files will be rolled\n+ * over after reaching a particular size.\n+ */\n+private[spark] class SizeBasedRollingPolicy(val rolloverSizeBytes: Long)\n+  extends RollingPolicy with Logging {\n+\n+  require(rolloverSizeBytes >= 1000)\n+\n+  @volatile private var bytesWrittenTillNow = 0L\n+\n+  /** Should rollover if the next set of bytes is going to exceed the size limit */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean = {\n+    bytesToBeWritten + bytesWrittenTillNow > rolloverSizeBytes\n+  }\n+\n+  /** Rollover has occurred, so reset the counter */\n+  def rolledOver() {\n+    bytesWrittenTillNow = 0\n+  }\n+\n+  /** Increment the bytes that have been written in the current file */\n+  def bytesWritten(bytes: Long) {\n+    bytesWrittenTillNow += bytes\n+  }\n+}\n+\n+/**\n+ * Continuously appends data from input stream into the given file, and rolls\n+ * over the file after the given interval. The rolled over files are named\n+ * based on the given pattern.\n+ *\n+ * @param inputStream             Input stream to read data from\n+ * @param activeFile              File to write data to\n+ * @param rollingPolicy           Policy based on which files will be rolled over.\n+ * @param rollingFilePattern      Pattern based on which the rolled over files will be named.\n+ *                                Uses SimpleDataFormat pattern.\n+ * @param conf                    SparkConf that is used to pass on extra configurations\n+ * @param bufferSize              Optional buffer size. Used mainly for testing.\n+ */\n+private[spark] class RollingFileAppender(\n+    inputStream: InputStream,\n+    activeFile: File,\n+    val rollingPolicy: RollingPolicy,\n+    rollingFilePattern: String,\n+    conf: SparkConf,\n+    bufferSize: Int = 8192\n+  ) extends FileAppender(inputStream, activeFile, bufferSize) {\n+\n+  import RollingFileAppender._\n+\n+  private val retainCount = conf.getInt(KEEP_LAST_PROPERTY, -1)\n+  private val formatter = new SimpleDateFormat(rollingFilePattern)\n+\n+  private val executor = Executors.newFixedThreadPool(1, new ThreadFactory{\n+    def newThread(r: Runnable): Thread = {\n+      val t = new Thread(r)\n+      t.setDaemon(true)\n+      t.setName(s\"Threadpool of ${RollingFileAppender.this.getClass.getSimpleName} for $activeFile\")\n+      t\n+    }\n+  })\n+\n+  /** Stop the appender */\n+  override def stop() {\n+    super.stop()\n+    //cleanupTask.cancel(true)\n+    executor.shutdownNow()\n+  }\n+\n+  /** Append bytes to file after rolling over is necessary */\n+  override protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (rollingPolicy.shouldRollover(len)) {\n+      rollover()\n+      rollingPolicy.rolledOver()\n+    }\n+    super.appendToFile(bytes, len)\n+    rollingPolicy.bytesWritten(len)\n+  }\n+\n+  /** Rollover the file, by closing the output stream and moving it over */\n+  private def rollover() {\n+    val rolloverSuffix = formatter.format(Calendar.getInstance.getTime)\n+    val rolloverFile = new File(\n+      activeFile.getParentFile, activeFile.getName + rolloverSuffix).getAbsoluteFile\n+    logDebug(\"Attempting to rollover at \" + System.currentTimeMillis + \" to file \" + rolloverFile)\n+\n+    try {\n+      closeFile()\n+      if (activeFile.exists) {\n+        if (!rolloverFile.exists) {\n+          FileUtils.moveFile(activeFile, rolloverFile)\n+          logInfo(s\"Rolled over $activeFile to $rolloverFile\")\n+        } else {\n+          // In case the rollover file name clashes, make a unique file name.\n+          // The resultant file names are long and ugly, so this is used only\n+          // if there is a name collision. This can be avoided by the using\n+          // the right pattern such that name collisions do not occur.\n+          var i = 0\n+          var altRolloverFile: File = null\n+          do {\n+            altRolloverFile = new File(activeFile.getParent,\n+              s\"${activeFile.getName}$rolloverSuffix--$i\").getAbsoluteFile\n+            i += 1\n+          } while (i < 10000 && altRolloverFile.exists)\n+\n+          logWarning(s\"Rollover file $rolloverFile already exists, \" +\n+            s\"rolled over $activeFile to file $altRolloverFile\")\n+          logWarning(s\"Make sure that the given file name pattern [$rollingFilePattern]\")\n+          FileUtils.moveFile(activeFile, altRolloverFile)\n+        }\n+      } else {\n+        logWarning(s\"File $activeFile does not exist\")\n+      }\n+      openFile()\n+      scheduleCleanup()\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Error rolling over $activeFile to $rolloverFile\", e)\n+    }\n+  }\n+\n+  /** Schedule cleaning up for older rolled over files */\n+  private def scheduleCleanup() {\n+    if (retainCount > 0) {\n+      executor.submit(new Runnable() { override def run() { cleanup() }})\n+    }\n+  }\n+\n+  /** Retain only last few files */\n+  private[util] def cleanup() {\n+    try {\n+      val rolledoverFiles = activeFile.getParentFile.listFiles(new FileFilter {\n+        def accept(f: File): Boolean = {\n+          f.getName.startsWith(activeFile.getName) && f != activeFile\n+        }\n+      }).sorted\n+      val filesToBeDeleted = rolledoverFiles.take(\n+        math.max(0, rolledoverFiles.size - retainCount))\n+      filesToBeDeleted.foreach { file =>\n+        logInfo(s\"Deleting file executor log file ${file.getAbsolutePath}\")\n+        file.delete()\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(\"Error cleaning logs in directory \" + activeFile.getParentFile.getAbsolutePath, e)\n+    }\n+  }\n+}\n+\n+/**\n+ * Companion object to [[org.apache.spark.util.RollingFileAppender]]. Defines\n+ * names of configurations that configure rolling file appenders.\n+ */\n+private[spark] object RollingFileAppender {\n+  val ENABLE_PROPERTY = \"spark.executor.rollingLogs.enabled\"\n+  val INTERVAL_PROPERTY = \"spark.executor.rollingLogs.interval\"\n+  val SIZE_PROPERTY = \"spark.executor.rollingLogs.size\"\n+  val KEEP_LAST_PROPERTY = \"spark.executor.rollingLogs.keepLastN\"\n+\n+}\n+\n+/** RollingFileAppender that rolls over every minute */\n+private[spark] class MinutelyRollingFileAppender("
  }],
  "prId": 895
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "I did some checking to make sure this works. Turns out there are no \"leap seconds\" or anything else that interfere with this.\n\n```\nscala> val now = System.currentTimeMillis()\nscala> val rolloverIntervalMillis = 3600 * 1000\nscala> val targetTime = math.ceil(now.toDouble / rolloverIntervalMillis) * rolloverIntervalMillis\nscala> new Date(targetTime.toLong)\nres2: java.util.Date = Wed Jun 04 18:00:00 PDT 2014\n```\n",
    "commit": "fd8f87fde302824813222e1a81003dea32010da7",
    "createdAt": "2014-06-05T00:29:33Z",
    "diffHunk": "@@ -0,0 +1,429 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util\n+\n+import java.io.{File, FileFilter, FileOutputStream, InputStream}\n+import java.text.SimpleDateFormat\n+import java.util.Calendar\n+import java.util.concurrent.{Executors, ThreadFactory}\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.spark.{Logging, SparkConf}\n+\n+/**\n+ * Continuously appends the data from an input stream into the given file.\n+ */\n+private[spark] class FileAppender(inputStream: InputStream, file: File, bufferSize: Int = 8192)\n+  extends Logging {\n+  @volatile private var outputStream: FileOutputStream = null\n+  @volatile private var markedForStop = false     // has the appender been asked to stopped\n+  @volatile private var stopped = false           // has the appender stopped\n+\n+  // Thread that reads the input stream and writes to file\n+  private val writingThread = new Thread(\"File appending thread for \" + file) {\n+    setDaemon(true)\n+    override def run() {\n+      Utils.logUncaughtExceptions {\n+        appendStreamToFile()\n+      }\n+    }\n+  }\n+  writingThread.start()\n+\n+  /**\n+   * Wait for the appender to stop appending, either because input stream is closed\n+   * or because of any error in appending\n+   */\n+  def awaitTermination() {\n+    synchronized {\n+      if (!stopped) {\n+        wait()\n+      }\n+    }\n+  }\n+\n+  /** Stop the appender */\n+  def stop() {\n+    markedForStop = true\n+  }\n+\n+  /** Continuously read chunks from the input stream and append to the file */\n+  protected def appendStreamToFile() {\n+    try {\n+      logDebug(\"Started appending thread\")\n+      openFile()\n+      val buf = new Array[Byte](bufferSize)\n+      var n = 0\n+      while (!markedForStop && n != -1) {\n+        n = inputStream.read(buf)\n+        if (n != -1) {\n+          appendToFile(buf, n)\n+        }\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Error writing stream to file $file\", e)\n+    } finally {\n+      closeFile()\n+      synchronized {\n+        stopped = true\n+        notifyAll()\n+      }\n+    }\n+  }\n+\n+  /** Append bytes to the file output stream */\n+  protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (outputStream == null) {\n+      openFile()\n+    }\n+    outputStream.write(bytes, 0, len)\n+  }\n+\n+  /** Open the file output stream */\n+  protected def openFile() {\n+    outputStream = new FileOutputStream(file, true)\n+    logDebug(s\"Opened file $file\")\n+  }\n+\n+  /** Close the file output stream */\n+  protected def closeFile() {\n+    outputStream.flush()\n+    outputStream.close()\n+    logDebug(s\"Closed file $file\")\n+  }\n+}\n+\n+/**\n+ * Companion object to [[org.apache.log4j.FileAppender]] which has helper\n+ * functions to choose the write type of FileAppender based on SparkConf configuration.\n+ */\n+private[spark] object FileAppender extends Logging {\n+\n+  /** Create the right appender based on Spark configuration */\n+  def apply(inputStream: InputStream, file: File, conf: SparkConf): FileAppender = {\n+\n+    import RollingFileAppender._\n+\n+    val rolloverEnabled = conf.getBoolean(ENABLE_PROPERTY, false)\n+    logDebug(\"Log rollover enabled = \" + rolloverEnabled)\n+    if (rolloverEnabled) {\n+\n+      val rolloverSizeOption = conf.getOption(SIZE_PROPERTY)\n+      val rolloverIntervalOption = conf.getOption(INTERVAL_PROPERTY)\n+\n+      (rolloverIntervalOption, rolloverSizeOption) match {\n+\n+        case (Some(rolloverInterval), Some(rolloverSize)) =>              // if both size and interval have been set\n+          logWarning(s\"Rollover interval [$rolloverInterval] and size [$rolloverSize] \" +\n+            s\"both set for executor logs, rolling logs not enabled\")\n+          new FileAppender(inputStream, file)\n+\n+        case (Some(rolloverInterval), None) =>  // if interval has been set\n+          rolloverInterval match {\n+            case \"daily\" =>\n+              logInfo(s\"Rolling executor logs enabled for $file with daily rolling\")\n+              new DailyRollingFileAppender(inputStream, file, conf)\n+            case \"hourly\" =>\n+              logInfo(s\"Rolling executor logs enabled for $file with hourly rolling\")\n+              new HourlyRollingFileAppender(inputStream, file, conf)\n+            case \"minutely\" =>\n+              logInfo(s\"Rolling executor logs enabled for $file with rolling every minute\")\n+              new MinutelyRollingFileAppender(inputStream, file, conf)\n+            case IntParam(millis) =>\n+              logInfo(s\"Rolling executor logs enabled for $file with rolling every minute\")\n+              new RollingFileAppender(inputStream, file, new TimeBasedRollingPolicy(millis),\n+                s\"--YYYY-MM-dd--HH-mm-ss-SSSS\", conf)\n+            case _ =>\n+              logWarning(\n+                s\"Illegal interval for rolling executor logs [$rolloverInterval], \" +\n+                  s\"rolling logs not enabled\")\n+              new FileAppender(inputStream, file)\n+          }\n+\n+        case (None, Some(rolloverSize)) =>    // if size has been set\n+          rolloverSize match {\n+            case IntParam(bytes) =>\n+              logInfo(s\"Rolling executor logs enabled for $file with rolling every $bytes bytes\")\n+              new RollingFileAppender(inputStream, file, new SizeBasedRollingPolicy(bytes),\n+                s\"--YYYY-MM-dd--HH-mm-ss-SSSS\", conf)\n+            case _ =>\n+              logWarning(\n+                s\"Illegal size for rolling executor logs [$rolloverSize], \" +\n+                  s\"rolling logs not enabled\")\n+              new FileAppender(inputStream, file)\n+          }\n+\n+        case (None, None) =>                // if neither size nor interval has been set\n+          logWarning(s\"Interval and size for rolling executor logs not set, \" +\n+            s\"rolling logs enabled with daily rolling.\")\n+          new DailyRollingFileAppender(inputStream, file, conf)\n+      }\n+    } else {\n+      new FileAppender(inputStream, file)\n+    }\n+  }\n+}\n+\n+/**\n+ * Defines the policy based on which [[org.apache.log4j.RollingFileAppender]] will\n+ * generate rolling files.\n+ */\n+private[spark] trait RollingPolicy {\n+\n+  /** Whether rollover should be initiated at this moment */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean\n+\n+  /** Notify that rollover has occurred */\n+  def rolledOver()\n+\n+  /** Notify that bytes have been written */\n+  def bytesWritten(bytes: Long)\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] by which files will be rolled\n+ * over at a fixed interval.\n+ */\n+private[spark] class TimeBasedRollingPolicy(val rolloverIntervalMillis: Long)\n+  extends RollingPolicy with Logging {\n+\n+  require(rolloverIntervalMillis >= 100)\n+\n+  @volatile private var nextRolloverTime = calculateNextRolloverTime()\n+\n+  /** Should rollover if current time has exceeded next rollover time */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean = {\n+    System.currentTimeMillis > nextRolloverTime\n+  }\n+\n+  /** Rollover has occurred, so find the next time to rollover */\n+  def rolledOver() {\n+    nextRolloverTime = calculateNextRolloverTime()\n+    logDebug(s\"Current time: ${System.currentTimeMillis}, next rollover time: \" + nextRolloverTime)\n+  }\n+\n+  def bytesWritten(bytes: Long) { }  // nothing to do\n+\n+  private def calculateNextRolloverTime(): Long = {\n+    val now = System.currentTimeMillis()\n+    val targetTime = math.ceil(now.toDouble / rolloverIntervalMillis) * rolloverIntervalMillis"
  }, {
    "author": {
      "login": "pwendell"
    },
    "body": "@tdas Ah okay I actually found one potential issue. When rolling on days... I guess this will just always use the UTC day? Might be good to document that.\n\n```\nscala> val now = System.currentTimeMillis()\nnow: Long = 1401931771561\n\nscala> val rolloverIntervalMillis = 24 * 3600 * 1000\nrolloverIntervalMillis: Int = 86400000\n\nscala> val targetTime = math.ceil(now.toDouble / rolloverIntervalMillis) * rolloverIntervalMillis\ntargetTime: Double = 1.4020128E12\n\nscala> new Date(targetTime.toLong)\nres2: java.util.Date = Thu Jun 05 17:00:00 PDT 2014\n```\n",
    "commit": "fd8f87fde302824813222e1a81003dea32010da7",
    "createdAt": "2014-06-05T01:35:03Z",
    "diffHunk": "@@ -0,0 +1,429 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util\n+\n+import java.io.{File, FileFilter, FileOutputStream, InputStream}\n+import java.text.SimpleDateFormat\n+import java.util.Calendar\n+import java.util.concurrent.{Executors, ThreadFactory}\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.spark.{Logging, SparkConf}\n+\n+/**\n+ * Continuously appends the data from an input stream into the given file.\n+ */\n+private[spark] class FileAppender(inputStream: InputStream, file: File, bufferSize: Int = 8192)\n+  extends Logging {\n+  @volatile private var outputStream: FileOutputStream = null\n+  @volatile private var markedForStop = false     // has the appender been asked to stopped\n+  @volatile private var stopped = false           // has the appender stopped\n+\n+  // Thread that reads the input stream and writes to file\n+  private val writingThread = new Thread(\"File appending thread for \" + file) {\n+    setDaemon(true)\n+    override def run() {\n+      Utils.logUncaughtExceptions {\n+        appendStreamToFile()\n+      }\n+    }\n+  }\n+  writingThread.start()\n+\n+  /**\n+   * Wait for the appender to stop appending, either because input stream is closed\n+   * or because of any error in appending\n+   */\n+  def awaitTermination() {\n+    synchronized {\n+      if (!stopped) {\n+        wait()\n+      }\n+    }\n+  }\n+\n+  /** Stop the appender */\n+  def stop() {\n+    markedForStop = true\n+  }\n+\n+  /** Continuously read chunks from the input stream and append to the file */\n+  protected def appendStreamToFile() {\n+    try {\n+      logDebug(\"Started appending thread\")\n+      openFile()\n+      val buf = new Array[Byte](bufferSize)\n+      var n = 0\n+      while (!markedForStop && n != -1) {\n+        n = inputStream.read(buf)\n+        if (n != -1) {\n+          appendToFile(buf, n)\n+        }\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Error writing stream to file $file\", e)\n+    } finally {\n+      closeFile()\n+      synchronized {\n+        stopped = true\n+        notifyAll()\n+      }\n+    }\n+  }\n+\n+  /** Append bytes to the file output stream */\n+  protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (outputStream == null) {\n+      openFile()\n+    }\n+    outputStream.write(bytes, 0, len)\n+  }\n+\n+  /** Open the file output stream */\n+  protected def openFile() {\n+    outputStream = new FileOutputStream(file, true)\n+    logDebug(s\"Opened file $file\")\n+  }\n+\n+  /** Close the file output stream */\n+  protected def closeFile() {\n+    outputStream.flush()\n+    outputStream.close()\n+    logDebug(s\"Closed file $file\")\n+  }\n+}\n+\n+/**\n+ * Companion object to [[org.apache.log4j.FileAppender]] which has helper\n+ * functions to choose the write type of FileAppender based on SparkConf configuration.\n+ */\n+private[spark] object FileAppender extends Logging {\n+\n+  /** Create the right appender based on Spark configuration */\n+  def apply(inputStream: InputStream, file: File, conf: SparkConf): FileAppender = {\n+\n+    import RollingFileAppender._\n+\n+    val rolloverEnabled = conf.getBoolean(ENABLE_PROPERTY, false)\n+    logDebug(\"Log rollover enabled = \" + rolloverEnabled)\n+    if (rolloverEnabled) {\n+\n+      val rolloverSizeOption = conf.getOption(SIZE_PROPERTY)\n+      val rolloverIntervalOption = conf.getOption(INTERVAL_PROPERTY)\n+\n+      (rolloverIntervalOption, rolloverSizeOption) match {\n+\n+        case (Some(rolloverInterval), Some(rolloverSize)) =>              // if both size and interval have been set\n+          logWarning(s\"Rollover interval [$rolloverInterval] and size [$rolloverSize] \" +\n+            s\"both set for executor logs, rolling logs not enabled\")\n+          new FileAppender(inputStream, file)\n+\n+        case (Some(rolloverInterval), None) =>  // if interval has been set\n+          rolloverInterval match {\n+            case \"daily\" =>\n+              logInfo(s\"Rolling executor logs enabled for $file with daily rolling\")\n+              new DailyRollingFileAppender(inputStream, file, conf)\n+            case \"hourly\" =>\n+              logInfo(s\"Rolling executor logs enabled for $file with hourly rolling\")\n+              new HourlyRollingFileAppender(inputStream, file, conf)\n+            case \"minutely\" =>\n+              logInfo(s\"Rolling executor logs enabled for $file with rolling every minute\")\n+              new MinutelyRollingFileAppender(inputStream, file, conf)\n+            case IntParam(millis) =>\n+              logInfo(s\"Rolling executor logs enabled for $file with rolling every minute\")\n+              new RollingFileAppender(inputStream, file, new TimeBasedRollingPolicy(millis),\n+                s\"--YYYY-MM-dd--HH-mm-ss-SSSS\", conf)\n+            case _ =>\n+              logWarning(\n+                s\"Illegal interval for rolling executor logs [$rolloverInterval], \" +\n+                  s\"rolling logs not enabled\")\n+              new FileAppender(inputStream, file)\n+          }\n+\n+        case (None, Some(rolloverSize)) =>    // if size has been set\n+          rolloverSize match {\n+            case IntParam(bytes) =>\n+              logInfo(s\"Rolling executor logs enabled for $file with rolling every $bytes bytes\")\n+              new RollingFileAppender(inputStream, file, new SizeBasedRollingPolicy(bytes),\n+                s\"--YYYY-MM-dd--HH-mm-ss-SSSS\", conf)\n+            case _ =>\n+              logWarning(\n+                s\"Illegal size for rolling executor logs [$rolloverSize], \" +\n+                  s\"rolling logs not enabled\")\n+              new FileAppender(inputStream, file)\n+          }\n+\n+        case (None, None) =>                // if neither size nor interval has been set\n+          logWarning(s\"Interval and size for rolling executor logs not set, \" +\n+            s\"rolling logs enabled with daily rolling.\")\n+          new DailyRollingFileAppender(inputStream, file, conf)\n+      }\n+    } else {\n+      new FileAppender(inputStream, file)\n+    }\n+  }\n+}\n+\n+/**\n+ * Defines the policy based on which [[org.apache.log4j.RollingFileAppender]] will\n+ * generate rolling files.\n+ */\n+private[spark] trait RollingPolicy {\n+\n+  /** Whether rollover should be initiated at this moment */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean\n+\n+  /** Notify that rollover has occurred */\n+  def rolledOver()\n+\n+  /** Notify that bytes have been written */\n+  def bytesWritten(bytes: Long)\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] by which files will be rolled\n+ * over at a fixed interval.\n+ */\n+private[spark] class TimeBasedRollingPolicy(val rolloverIntervalMillis: Long)\n+  extends RollingPolicy with Logging {\n+\n+  require(rolloverIntervalMillis >= 100)\n+\n+  @volatile private var nextRolloverTime = calculateNextRolloverTime()\n+\n+  /** Should rollover if current time has exceeded next rollover time */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean = {\n+    System.currentTimeMillis > nextRolloverTime\n+  }\n+\n+  /** Rollover has occurred, so find the next time to rollover */\n+  def rolledOver() {\n+    nextRolloverTime = calculateNextRolloverTime()\n+    logDebug(s\"Current time: ${System.currentTimeMillis}, next rollover time: \" + nextRolloverTime)\n+  }\n+\n+  def bytesWritten(bytes: Long) { }  // nothing to do\n+\n+  private def calculateNextRolloverTime(): Long = {\n+    val now = System.currentTimeMillis()\n+    val targetTime = math.ceil(now.toDouble / rolloverIntervalMillis) * rolloverIntervalMillis"
  }],
  "prId": 895
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "This could would get a lot simpler if we just used the same (second-granularity) string format for everything. We could also just ask the user to specify the rolling interval in seconds instead of having special strings. Finally, I don't really see a great use case for having < 1s rolling interval, so to keep things simple we could just drop support for that.\n",
    "commit": "fd8f87fde302824813222e1a81003dea32010da7",
    "createdAt": "2014-06-05T00:33:13Z",
    "diffHunk": "@@ -0,0 +1,429 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util\n+\n+import java.io.{File, FileFilter, FileOutputStream, InputStream}\n+import java.text.SimpleDateFormat\n+import java.util.Calendar\n+import java.util.concurrent.{Executors, ThreadFactory}\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.spark.{Logging, SparkConf}\n+\n+/**\n+ * Continuously appends the data from an input stream into the given file.\n+ */\n+private[spark] class FileAppender(inputStream: InputStream, file: File, bufferSize: Int = 8192)\n+  extends Logging {\n+  @volatile private var outputStream: FileOutputStream = null\n+  @volatile private var markedForStop = false     // has the appender been asked to stopped\n+  @volatile private var stopped = false           // has the appender stopped\n+\n+  // Thread that reads the input stream and writes to file\n+  private val writingThread = new Thread(\"File appending thread for \" + file) {\n+    setDaemon(true)\n+    override def run() {\n+      Utils.logUncaughtExceptions {\n+        appendStreamToFile()\n+      }\n+    }\n+  }\n+  writingThread.start()\n+\n+  /**\n+   * Wait for the appender to stop appending, either because input stream is closed\n+   * or because of any error in appending\n+   */\n+  def awaitTermination() {\n+    synchronized {\n+      if (!stopped) {\n+        wait()\n+      }\n+    }\n+  }\n+\n+  /** Stop the appender */\n+  def stop() {\n+    markedForStop = true\n+  }\n+\n+  /** Continuously read chunks from the input stream and append to the file */\n+  protected def appendStreamToFile() {\n+    try {\n+      logDebug(\"Started appending thread\")\n+      openFile()\n+      val buf = new Array[Byte](bufferSize)\n+      var n = 0\n+      while (!markedForStop && n != -1) {\n+        n = inputStream.read(buf)\n+        if (n != -1) {\n+          appendToFile(buf, n)\n+        }\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Error writing stream to file $file\", e)\n+    } finally {\n+      closeFile()\n+      synchronized {\n+        stopped = true\n+        notifyAll()\n+      }\n+    }\n+  }\n+\n+  /** Append bytes to the file output stream */\n+  protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (outputStream == null) {\n+      openFile()\n+    }\n+    outputStream.write(bytes, 0, len)\n+  }\n+\n+  /** Open the file output stream */\n+  protected def openFile() {\n+    outputStream = new FileOutputStream(file, true)\n+    logDebug(s\"Opened file $file\")\n+  }\n+\n+  /** Close the file output stream */\n+  protected def closeFile() {\n+    outputStream.flush()\n+    outputStream.close()\n+    logDebug(s\"Closed file $file\")\n+  }\n+}\n+\n+/**\n+ * Companion object to [[org.apache.log4j.FileAppender]] which has helper\n+ * functions to choose the write type of FileAppender based on SparkConf configuration.\n+ */\n+private[spark] object FileAppender extends Logging {\n+\n+  /** Create the right appender based on Spark configuration */\n+  def apply(inputStream: InputStream, file: File, conf: SparkConf): FileAppender = {\n+\n+    import RollingFileAppender._\n+\n+    val rolloverEnabled = conf.getBoolean(ENABLE_PROPERTY, false)\n+    logDebug(\"Log rollover enabled = \" + rolloverEnabled)\n+    if (rolloverEnabled) {\n+\n+      val rolloverSizeOption = conf.getOption(SIZE_PROPERTY)\n+      val rolloverIntervalOption = conf.getOption(INTERVAL_PROPERTY)\n+\n+      (rolloverIntervalOption, rolloverSizeOption) match {\n+\n+        case (Some(rolloverInterval), Some(rolloverSize)) =>              // if both size and interval have been set\n+          logWarning(s\"Rollover interval [$rolloverInterval] and size [$rolloverSize] \" +\n+            s\"both set for executor logs, rolling logs not enabled\")\n+          new FileAppender(inputStream, file)\n+\n+        case (Some(rolloverInterval), None) =>  // if interval has been set\n+          rolloverInterval match {\n+            case \"daily\" =>"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "I think being able to say \"daily\" or \"hourly\" is pretty convenient. It doesnt really add much complexity. Its a different call we can take on whether to have dedicated classes for daily, hourly, and minutely rolling.\n\nYes, Rolling < 1s is not really practical. However it is used in the testsuites.\n",
    "commit": "fd8f87fde302824813222e1a81003dea32010da7",
    "createdAt": "2014-06-05T00:43:50Z",
    "diffHunk": "@@ -0,0 +1,429 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util\n+\n+import java.io.{File, FileFilter, FileOutputStream, InputStream}\n+import java.text.SimpleDateFormat\n+import java.util.Calendar\n+import java.util.concurrent.{Executors, ThreadFactory}\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.spark.{Logging, SparkConf}\n+\n+/**\n+ * Continuously appends the data from an input stream into the given file.\n+ */\n+private[spark] class FileAppender(inputStream: InputStream, file: File, bufferSize: Int = 8192)\n+  extends Logging {\n+  @volatile private var outputStream: FileOutputStream = null\n+  @volatile private var markedForStop = false     // has the appender been asked to stopped\n+  @volatile private var stopped = false           // has the appender stopped\n+\n+  // Thread that reads the input stream and writes to file\n+  private val writingThread = new Thread(\"File appending thread for \" + file) {\n+    setDaemon(true)\n+    override def run() {\n+      Utils.logUncaughtExceptions {\n+        appendStreamToFile()\n+      }\n+    }\n+  }\n+  writingThread.start()\n+\n+  /**\n+   * Wait for the appender to stop appending, either because input stream is closed\n+   * or because of any error in appending\n+   */\n+  def awaitTermination() {\n+    synchronized {\n+      if (!stopped) {\n+        wait()\n+      }\n+    }\n+  }\n+\n+  /** Stop the appender */\n+  def stop() {\n+    markedForStop = true\n+  }\n+\n+  /** Continuously read chunks from the input stream and append to the file */\n+  protected def appendStreamToFile() {\n+    try {\n+      logDebug(\"Started appending thread\")\n+      openFile()\n+      val buf = new Array[Byte](bufferSize)\n+      var n = 0\n+      while (!markedForStop && n != -1) {\n+        n = inputStream.read(buf)\n+        if (n != -1) {\n+          appendToFile(buf, n)\n+        }\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Error writing stream to file $file\", e)\n+    } finally {\n+      closeFile()\n+      synchronized {\n+        stopped = true\n+        notifyAll()\n+      }\n+    }\n+  }\n+\n+  /** Append bytes to the file output stream */\n+  protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (outputStream == null) {\n+      openFile()\n+    }\n+    outputStream.write(bytes, 0, len)\n+  }\n+\n+  /** Open the file output stream */\n+  protected def openFile() {\n+    outputStream = new FileOutputStream(file, true)\n+    logDebug(s\"Opened file $file\")\n+  }\n+\n+  /** Close the file output stream */\n+  protected def closeFile() {\n+    outputStream.flush()\n+    outputStream.close()\n+    logDebug(s\"Closed file $file\")\n+  }\n+}\n+\n+/**\n+ * Companion object to [[org.apache.log4j.FileAppender]] which has helper\n+ * functions to choose the write type of FileAppender based on SparkConf configuration.\n+ */\n+private[spark] object FileAppender extends Logging {\n+\n+  /** Create the right appender based on Spark configuration */\n+  def apply(inputStream: InputStream, file: File, conf: SparkConf): FileAppender = {\n+\n+    import RollingFileAppender._\n+\n+    val rolloverEnabled = conf.getBoolean(ENABLE_PROPERTY, false)\n+    logDebug(\"Log rollover enabled = \" + rolloverEnabled)\n+    if (rolloverEnabled) {\n+\n+      val rolloverSizeOption = conf.getOption(SIZE_PROPERTY)\n+      val rolloverIntervalOption = conf.getOption(INTERVAL_PROPERTY)\n+\n+      (rolloverIntervalOption, rolloverSizeOption) match {\n+\n+        case (Some(rolloverInterval), Some(rolloverSize)) =>              // if both size and interval have been set\n+          logWarning(s\"Rollover interval [$rolloverInterval] and size [$rolloverSize] \" +\n+            s\"both set for executor logs, rolling logs not enabled\")\n+          new FileAppender(inputStream, file)\n+\n+        case (Some(rolloverInterval), None) =>  // if interval has been set\n+          rolloverInterval match {\n+            case \"daily\" =>"
  }],
  "prId": 895
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "It seems like since the check above says `System.currentTimeMillis > nextRolloverTime` (i.e. it's strictly greater) I'm having trouble constructing a case where this `+ 1` is necessary. What is the case?\n\nIf `shouldRollover` was not strict inequality, I could see a case where, in the same millisecond, you finished one interval and rolled over to another and then just re-calculated the same one. But that doesn't seem to be the case here.\n",
    "commit": "fd8f87fde302824813222e1a81003dea32010da7",
    "createdAt": "2014-06-05T00:54:19Z",
    "diffHunk": "@@ -0,0 +1,429 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util\n+\n+import java.io.{File, FileFilter, FileOutputStream, InputStream}\n+import java.text.SimpleDateFormat\n+import java.util.Calendar\n+import java.util.concurrent.{Executors, ThreadFactory}\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.spark.{Logging, SparkConf}\n+\n+/**\n+ * Continuously appends the data from an input stream into the given file.\n+ */\n+private[spark] class FileAppender(inputStream: InputStream, file: File, bufferSize: Int = 8192)\n+  extends Logging {\n+  @volatile private var outputStream: FileOutputStream = null\n+  @volatile private var markedForStop = false     // has the appender been asked to stopped\n+  @volatile private var stopped = false           // has the appender stopped\n+\n+  // Thread that reads the input stream and writes to file\n+  private val writingThread = new Thread(\"File appending thread for \" + file) {\n+    setDaemon(true)\n+    override def run() {\n+      Utils.logUncaughtExceptions {\n+        appendStreamToFile()\n+      }\n+    }\n+  }\n+  writingThread.start()\n+\n+  /**\n+   * Wait for the appender to stop appending, either because input stream is closed\n+   * or because of any error in appending\n+   */\n+  def awaitTermination() {\n+    synchronized {\n+      if (!stopped) {\n+        wait()\n+      }\n+    }\n+  }\n+\n+  /** Stop the appender */\n+  def stop() {\n+    markedForStop = true\n+  }\n+\n+  /** Continuously read chunks from the input stream and append to the file */\n+  protected def appendStreamToFile() {\n+    try {\n+      logDebug(\"Started appending thread\")\n+      openFile()\n+      val buf = new Array[Byte](bufferSize)\n+      var n = 0\n+      while (!markedForStop && n != -1) {\n+        n = inputStream.read(buf)\n+        if (n != -1) {\n+          appendToFile(buf, n)\n+        }\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Error writing stream to file $file\", e)\n+    } finally {\n+      closeFile()\n+      synchronized {\n+        stopped = true\n+        notifyAll()\n+      }\n+    }\n+  }\n+\n+  /** Append bytes to the file output stream */\n+  protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (outputStream == null) {\n+      openFile()\n+    }\n+    outputStream.write(bytes, 0, len)\n+  }\n+\n+  /** Open the file output stream */\n+  protected def openFile() {\n+    outputStream = new FileOutputStream(file, true)\n+    logDebug(s\"Opened file $file\")\n+  }\n+\n+  /** Close the file output stream */\n+  protected def closeFile() {\n+    outputStream.flush()\n+    outputStream.close()\n+    logDebug(s\"Closed file $file\")\n+  }\n+}\n+\n+/**\n+ * Companion object to [[org.apache.log4j.FileAppender]] which has helper\n+ * functions to choose the write type of FileAppender based on SparkConf configuration.\n+ */\n+private[spark] object FileAppender extends Logging {\n+\n+  /** Create the right appender based on Spark configuration */\n+  def apply(inputStream: InputStream, file: File, conf: SparkConf): FileAppender = {\n+\n+    import RollingFileAppender._\n+\n+    val rolloverEnabled = conf.getBoolean(ENABLE_PROPERTY, false)\n+    logDebug(\"Log rollover enabled = \" + rolloverEnabled)\n+    if (rolloverEnabled) {\n+\n+      val rolloverSizeOption = conf.getOption(SIZE_PROPERTY)\n+      val rolloverIntervalOption = conf.getOption(INTERVAL_PROPERTY)\n+\n+      (rolloverIntervalOption, rolloverSizeOption) match {\n+\n+        case (Some(rolloverInterval), Some(rolloverSize)) =>              // if both size and interval have been set\n+          logWarning(s\"Rollover interval [$rolloverInterval] and size [$rolloverSize] \" +\n+            s\"both set for executor logs, rolling logs not enabled\")\n+          new FileAppender(inputStream, file)\n+\n+        case (Some(rolloverInterval), None) =>  // if interval has been set\n+          rolloverInterval match {\n+            case \"daily\" =>\n+              logInfo(s\"Rolling executor logs enabled for $file with daily rolling\")\n+              new DailyRollingFileAppender(inputStream, file, conf)\n+            case \"hourly\" =>\n+              logInfo(s\"Rolling executor logs enabled for $file with hourly rolling\")\n+              new HourlyRollingFileAppender(inputStream, file, conf)\n+            case \"minutely\" =>\n+              logInfo(s\"Rolling executor logs enabled for $file with rolling every minute\")\n+              new MinutelyRollingFileAppender(inputStream, file, conf)\n+            case IntParam(millis) =>\n+              logInfo(s\"Rolling executor logs enabled for $file with rolling every minute\")\n+              new RollingFileAppender(inputStream, file, new TimeBasedRollingPolicy(millis),\n+                s\"--YYYY-MM-dd--HH-mm-ss-SSSS\", conf)\n+            case _ =>\n+              logWarning(\n+                s\"Illegal interval for rolling executor logs [$rolloverInterval], \" +\n+                  s\"rolling logs not enabled\")\n+              new FileAppender(inputStream, file)\n+          }\n+\n+        case (None, Some(rolloverSize)) =>    // if size has been set\n+          rolloverSize match {\n+            case IntParam(bytes) =>\n+              logInfo(s\"Rolling executor logs enabled for $file with rolling every $bytes bytes\")\n+              new RollingFileAppender(inputStream, file, new SizeBasedRollingPolicy(bytes),\n+                s\"--YYYY-MM-dd--HH-mm-ss-SSSS\", conf)\n+            case _ =>\n+              logWarning(\n+                s\"Illegal size for rolling executor logs [$rolloverSize], \" +\n+                  s\"rolling logs not enabled\")\n+              new FileAppender(inputStream, file)\n+          }\n+\n+        case (None, None) =>                // if neither size nor interval has been set\n+          logWarning(s\"Interval and size for rolling executor logs not set, \" +\n+            s\"rolling logs enabled with daily rolling.\")\n+          new DailyRollingFileAppender(inputStream, file, conf)\n+      }\n+    } else {\n+      new FileAppender(inputStream, file)\n+    }\n+  }\n+}\n+\n+/**\n+ * Defines the policy based on which [[org.apache.log4j.RollingFileAppender]] will\n+ * generate rolling files.\n+ */\n+private[spark] trait RollingPolicy {\n+\n+  /** Whether rollover should be initiated at this moment */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean\n+\n+  /** Notify that rollover has occurred */\n+  def rolledOver()\n+\n+  /** Notify that bytes have been written */\n+  def bytesWritten(bytes: Long)\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] by which files will be rolled\n+ * over at a fixed interval.\n+ */\n+private[spark] class TimeBasedRollingPolicy(val rolloverIntervalMillis: Long)\n+  extends RollingPolicy with Logging {\n+\n+  require(rolloverIntervalMillis >= 100)\n+\n+  @volatile private var nextRolloverTime = calculateNextRolloverTime()\n+\n+  /** Should rollover if current time has exceeded next rollover time */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean = {\n+    System.currentTimeMillis > nextRolloverTime\n+  }\n+\n+  /** Rollover has occurred, so find the next time to rollover */\n+  def rolledOver() {\n+    nextRolloverTime = calculateNextRolloverTime()\n+    logDebug(s\"Current time: ${System.currentTimeMillis}, next rollover time: \" + nextRolloverTime)\n+  }\n+\n+  def bytesWritten(bytes: Long) { }  // nothing to do\n+\n+  private def calculateNextRolloverTime(): Long = {\n+    val now = System.currentTimeMillis()\n+    val targetTime = math.ceil(now.toDouble / rolloverIntervalMillis) * rolloverIntervalMillis\n+    targetTime.toLong + 1  // +1 to make sure this falls in the next interval"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Right, good catch. Had changed `>=` to `>` and forgot about the other. \n",
    "commit": "fd8f87fde302824813222e1a81003dea32010da7",
    "createdAt": "2014-06-05T00:55:42Z",
    "diffHunk": "@@ -0,0 +1,429 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util\n+\n+import java.io.{File, FileFilter, FileOutputStream, InputStream}\n+import java.text.SimpleDateFormat\n+import java.util.Calendar\n+import java.util.concurrent.{Executors, ThreadFactory}\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.spark.{Logging, SparkConf}\n+\n+/**\n+ * Continuously appends the data from an input stream into the given file.\n+ */\n+private[spark] class FileAppender(inputStream: InputStream, file: File, bufferSize: Int = 8192)\n+  extends Logging {\n+  @volatile private var outputStream: FileOutputStream = null\n+  @volatile private var markedForStop = false     // has the appender been asked to stopped\n+  @volatile private var stopped = false           // has the appender stopped\n+\n+  // Thread that reads the input stream and writes to file\n+  private val writingThread = new Thread(\"File appending thread for \" + file) {\n+    setDaemon(true)\n+    override def run() {\n+      Utils.logUncaughtExceptions {\n+        appendStreamToFile()\n+      }\n+    }\n+  }\n+  writingThread.start()\n+\n+  /**\n+   * Wait for the appender to stop appending, either because input stream is closed\n+   * or because of any error in appending\n+   */\n+  def awaitTermination() {\n+    synchronized {\n+      if (!stopped) {\n+        wait()\n+      }\n+    }\n+  }\n+\n+  /** Stop the appender */\n+  def stop() {\n+    markedForStop = true\n+  }\n+\n+  /** Continuously read chunks from the input stream and append to the file */\n+  protected def appendStreamToFile() {\n+    try {\n+      logDebug(\"Started appending thread\")\n+      openFile()\n+      val buf = new Array[Byte](bufferSize)\n+      var n = 0\n+      while (!markedForStop && n != -1) {\n+        n = inputStream.read(buf)\n+        if (n != -1) {\n+          appendToFile(buf, n)\n+        }\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Error writing stream to file $file\", e)\n+    } finally {\n+      closeFile()\n+      synchronized {\n+        stopped = true\n+        notifyAll()\n+      }\n+    }\n+  }\n+\n+  /** Append bytes to the file output stream */\n+  protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (outputStream == null) {\n+      openFile()\n+    }\n+    outputStream.write(bytes, 0, len)\n+  }\n+\n+  /** Open the file output stream */\n+  protected def openFile() {\n+    outputStream = new FileOutputStream(file, true)\n+    logDebug(s\"Opened file $file\")\n+  }\n+\n+  /** Close the file output stream */\n+  protected def closeFile() {\n+    outputStream.flush()\n+    outputStream.close()\n+    logDebug(s\"Closed file $file\")\n+  }\n+}\n+\n+/**\n+ * Companion object to [[org.apache.log4j.FileAppender]] which has helper\n+ * functions to choose the write type of FileAppender based on SparkConf configuration.\n+ */\n+private[spark] object FileAppender extends Logging {\n+\n+  /** Create the right appender based on Spark configuration */\n+  def apply(inputStream: InputStream, file: File, conf: SparkConf): FileAppender = {\n+\n+    import RollingFileAppender._\n+\n+    val rolloverEnabled = conf.getBoolean(ENABLE_PROPERTY, false)\n+    logDebug(\"Log rollover enabled = \" + rolloverEnabled)\n+    if (rolloverEnabled) {\n+\n+      val rolloverSizeOption = conf.getOption(SIZE_PROPERTY)\n+      val rolloverIntervalOption = conf.getOption(INTERVAL_PROPERTY)\n+\n+      (rolloverIntervalOption, rolloverSizeOption) match {\n+\n+        case (Some(rolloverInterval), Some(rolloverSize)) =>              // if both size and interval have been set\n+          logWarning(s\"Rollover interval [$rolloverInterval] and size [$rolloverSize] \" +\n+            s\"both set for executor logs, rolling logs not enabled\")\n+          new FileAppender(inputStream, file)\n+\n+        case (Some(rolloverInterval), None) =>  // if interval has been set\n+          rolloverInterval match {\n+            case \"daily\" =>\n+              logInfo(s\"Rolling executor logs enabled for $file with daily rolling\")\n+              new DailyRollingFileAppender(inputStream, file, conf)\n+            case \"hourly\" =>\n+              logInfo(s\"Rolling executor logs enabled for $file with hourly rolling\")\n+              new HourlyRollingFileAppender(inputStream, file, conf)\n+            case \"minutely\" =>\n+              logInfo(s\"Rolling executor logs enabled for $file with rolling every minute\")\n+              new MinutelyRollingFileAppender(inputStream, file, conf)\n+            case IntParam(millis) =>\n+              logInfo(s\"Rolling executor logs enabled for $file with rolling every minute\")\n+              new RollingFileAppender(inputStream, file, new TimeBasedRollingPolicy(millis),\n+                s\"--YYYY-MM-dd--HH-mm-ss-SSSS\", conf)\n+            case _ =>\n+              logWarning(\n+                s\"Illegal interval for rolling executor logs [$rolloverInterval], \" +\n+                  s\"rolling logs not enabled\")\n+              new FileAppender(inputStream, file)\n+          }\n+\n+        case (None, Some(rolloverSize)) =>    // if size has been set\n+          rolloverSize match {\n+            case IntParam(bytes) =>\n+              logInfo(s\"Rolling executor logs enabled for $file with rolling every $bytes bytes\")\n+              new RollingFileAppender(inputStream, file, new SizeBasedRollingPolicy(bytes),\n+                s\"--YYYY-MM-dd--HH-mm-ss-SSSS\", conf)\n+            case _ =>\n+              logWarning(\n+                s\"Illegal size for rolling executor logs [$rolloverSize], \" +\n+                  s\"rolling logs not enabled\")\n+              new FileAppender(inputStream, file)\n+          }\n+\n+        case (None, None) =>                // if neither size nor interval has been set\n+          logWarning(s\"Interval and size for rolling executor logs not set, \" +\n+            s\"rolling logs enabled with daily rolling.\")\n+          new DailyRollingFileAppender(inputStream, file, conf)\n+      }\n+    } else {\n+      new FileAppender(inputStream, file)\n+    }\n+  }\n+}\n+\n+/**\n+ * Defines the policy based on which [[org.apache.log4j.RollingFileAppender]] will\n+ * generate rolling files.\n+ */\n+private[spark] trait RollingPolicy {\n+\n+  /** Whether rollover should be initiated at this moment */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean\n+\n+  /** Notify that rollover has occurred */\n+  def rolledOver()\n+\n+  /** Notify that bytes have been written */\n+  def bytesWritten(bytes: Long)\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] by which files will be rolled\n+ * over at a fixed interval.\n+ */\n+private[spark] class TimeBasedRollingPolicy(val rolloverIntervalMillis: Long)\n+  extends RollingPolicy with Logging {\n+\n+  require(rolloverIntervalMillis >= 100)\n+\n+  @volatile private var nextRolloverTime = calculateNextRolloverTime()\n+\n+  /** Should rollover if current time has exceeded next rollover time */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean = {\n+    System.currentTimeMillis > nextRolloverTime\n+  }\n+\n+  /** Rollover has occurred, so find the next time to rollover */\n+  def rolledOver() {\n+    nextRolloverTime = calculateNextRolloverTime()\n+    logDebug(s\"Current time: ${System.currentTimeMillis}, next rollover time: \" + nextRolloverTime)\n+  }\n+\n+  def bytesWritten(bytes: Long) { }  // nothing to do\n+\n+  private def calculateNextRolloverTime(): Long = {\n+    val now = System.currentTimeMillis()\n+    val targetTime = math.ceil(now.toDouble / rolloverIntervalMillis) * rolloverIntervalMillis\n+    targetTime.toLong + 1  // +1 to make sure this falls in the next interval"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Fixed. Will push after addressing all the comments.\n",
    "commit": "fd8f87fde302824813222e1a81003dea32010da7",
    "createdAt": "2014-06-05T01:09:44Z",
    "diffHunk": "@@ -0,0 +1,429 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util\n+\n+import java.io.{File, FileFilter, FileOutputStream, InputStream}\n+import java.text.SimpleDateFormat\n+import java.util.Calendar\n+import java.util.concurrent.{Executors, ThreadFactory}\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.spark.{Logging, SparkConf}\n+\n+/**\n+ * Continuously appends the data from an input stream into the given file.\n+ */\n+private[spark] class FileAppender(inputStream: InputStream, file: File, bufferSize: Int = 8192)\n+  extends Logging {\n+  @volatile private var outputStream: FileOutputStream = null\n+  @volatile private var markedForStop = false     // has the appender been asked to stopped\n+  @volatile private var stopped = false           // has the appender stopped\n+\n+  // Thread that reads the input stream and writes to file\n+  private val writingThread = new Thread(\"File appending thread for \" + file) {\n+    setDaemon(true)\n+    override def run() {\n+      Utils.logUncaughtExceptions {\n+        appendStreamToFile()\n+      }\n+    }\n+  }\n+  writingThread.start()\n+\n+  /**\n+   * Wait for the appender to stop appending, either because input stream is closed\n+   * or because of any error in appending\n+   */\n+  def awaitTermination() {\n+    synchronized {\n+      if (!stopped) {\n+        wait()\n+      }\n+    }\n+  }\n+\n+  /** Stop the appender */\n+  def stop() {\n+    markedForStop = true\n+  }\n+\n+  /** Continuously read chunks from the input stream and append to the file */\n+  protected def appendStreamToFile() {\n+    try {\n+      logDebug(\"Started appending thread\")\n+      openFile()\n+      val buf = new Array[Byte](bufferSize)\n+      var n = 0\n+      while (!markedForStop && n != -1) {\n+        n = inputStream.read(buf)\n+        if (n != -1) {\n+          appendToFile(buf, n)\n+        }\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Error writing stream to file $file\", e)\n+    } finally {\n+      closeFile()\n+      synchronized {\n+        stopped = true\n+        notifyAll()\n+      }\n+    }\n+  }\n+\n+  /** Append bytes to the file output stream */\n+  protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (outputStream == null) {\n+      openFile()\n+    }\n+    outputStream.write(bytes, 0, len)\n+  }\n+\n+  /** Open the file output stream */\n+  protected def openFile() {\n+    outputStream = new FileOutputStream(file, true)\n+    logDebug(s\"Opened file $file\")\n+  }\n+\n+  /** Close the file output stream */\n+  protected def closeFile() {\n+    outputStream.flush()\n+    outputStream.close()\n+    logDebug(s\"Closed file $file\")\n+  }\n+}\n+\n+/**\n+ * Companion object to [[org.apache.log4j.FileAppender]] which has helper\n+ * functions to choose the write type of FileAppender based on SparkConf configuration.\n+ */\n+private[spark] object FileAppender extends Logging {\n+\n+  /** Create the right appender based on Spark configuration */\n+  def apply(inputStream: InputStream, file: File, conf: SparkConf): FileAppender = {\n+\n+    import RollingFileAppender._\n+\n+    val rolloverEnabled = conf.getBoolean(ENABLE_PROPERTY, false)\n+    logDebug(\"Log rollover enabled = \" + rolloverEnabled)\n+    if (rolloverEnabled) {\n+\n+      val rolloverSizeOption = conf.getOption(SIZE_PROPERTY)\n+      val rolloverIntervalOption = conf.getOption(INTERVAL_PROPERTY)\n+\n+      (rolloverIntervalOption, rolloverSizeOption) match {\n+\n+        case (Some(rolloverInterval), Some(rolloverSize)) =>              // if both size and interval have been set\n+          logWarning(s\"Rollover interval [$rolloverInterval] and size [$rolloverSize] \" +\n+            s\"both set for executor logs, rolling logs not enabled\")\n+          new FileAppender(inputStream, file)\n+\n+        case (Some(rolloverInterval), None) =>  // if interval has been set\n+          rolloverInterval match {\n+            case \"daily\" =>\n+              logInfo(s\"Rolling executor logs enabled for $file with daily rolling\")\n+              new DailyRollingFileAppender(inputStream, file, conf)\n+            case \"hourly\" =>\n+              logInfo(s\"Rolling executor logs enabled for $file with hourly rolling\")\n+              new HourlyRollingFileAppender(inputStream, file, conf)\n+            case \"minutely\" =>\n+              logInfo(s\"Rolling executor logs enabled for $file with rolling every minute\")\n+              new MinutelyRollingFileAppender(inputStream, file, conf)\n+            case IntParam(millis) =>\n+              logInfo(s\"Rolling executor logs enabled for $file with rolling every minute\")\n+              new RollingFileAppender(inputStream, file, new TimeBasedRollingPolicy(millis),\n+                s\"--YYYY-MM-dd--HH-mm-ss-SSSS\", conf)\n+            case _ =>\n+              logWarning(\n+                s\"Illegal interval for rolling executor logs [$rolloverInterval], \" +\n+                  s\"rolling logs not enabled\")\n+              new FileAppender(inputStream, file)\n+          }\n+\n+        case (None, Some(rolloverSize)) =>    // if size has been set\n+          rolloverSize match {\n+            case IntParam(bytes) =>\n+              logInfo(s\"Rolling executor logs enabled for $file with rolling every $bytes bytes\")\n+              new RollingFileAppender(inputStream, file, new SizeBasedRollingPolicy(bytes),\n+                s\"--YYYY-MM-dd--HH-mm-ss-SSSS\", conf)\n+            case _ =>\n+              logWarning(\n+                s\"Illegal size for rolling executor logs [$rolloverSize], \" +\n+                  s\"rolling logs not enabled\")\n+              new FileAppender(inputStream, file)\n+          }\n+\n+        case (None, None) =>                // if neither size nor interval has been set\n+          logWarning(s\"Interval and size for rolling executor logs not set, \" +\n+            s\"rolling logs enabled with daily rolling.\")\n+          new DailyRollingFileAppender(inputStream, file, conf)\n+      }\n+    } else {\n+      new FileAppender(inputStream, file)\n+    }\n+  }\n+}\n+\n+/**\n+ * Defines the policy based on which [[org.apache.log4j.RollingFileAppender]] will\n+ * generate rolling files.\n+ */\n+private[spark] trait RollingPolicy {\n+\n+  /** Whether rollover should be initiated at this moment */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean\n+\n+  /** Notify that rollover has occurred */\n+  def rolledOver()\n+\n+  /** Notify that bytes have been written */\n+  def bytesWritten(bytes: Long)\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] by which files will be rolled\n+ * over at a fixed interval.\n+ */\n+private[spark] class TimeBasedRollingPolicy(val rolloverIntervalMillis: Long)\n+  extends RollingPolicy with Logging {\n+\n+  require(rolloverIntervalMillis >= 100)\n+\n+  @volatile private var nextRolloverTime = calculateNextRolloverTime()\n+\n+  /** Should rollover if current time has exceeded next rollover time */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean = {\n+    System.currentTimeMillis > nextRolloverTime\n+  }\n+\n+  /** Rollover has occurred, so find the next time to rollover */\n+  def rolledOver() {\n+    nextRolloverTime = calculateNextRolloverTime()\n+    logDebug(s\"Current time: ${System.currentTimeMillis}, next rollover time: \" + nextRolloverTime)\n+  }\n+\n+  def bytesWritten(bytes: Long) { }  // nothing to do\n+\n+  private def calculateNextRolloverTime(): Long = {\n+    val now = System.currentTimeMillis()\n+    val targetTime = math.ceil(now.toDouble / rolloverIntervalMillis) * rolloverIntervalMillis\n+    targetTime.toLong + 1  // +1 to make sure this falls in the next interval"
  }],
  "prId": 895
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "It seems like `rolloverSizeBytes` must be larger than the buffer size. I guess it's hard to see where that wouldn't be the case, so probably not worth noting.\n",
    "commit": "fd8f87fde302824813222e1a81003dea32010da7",
    "createdAt": "2014-06-05T01:37:51Z",
    "diffHunk": "@@ -0,0 +1,429 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util\n+\n+import java.io.{File, FileFilter, FileOutputStream, InputStream}\n+import java.text.SimpleDateFormat\n+import java.util.Calendar\n+import java.util.concurrent.{Executors, ThreadFactory}\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.spark.{Logging, SparkConf}\n+\n+/**\n+ * Continuously appends the data from an input stream into the given file.\n+ */\n+private[spark] class FileAppender(inputStream: InputStream, file: File, bufferSize: Int = 8192)\n+  extends Logging {\n+  @volatile private var outputStream: FileOutputStream = null\n+  @volatile private var markedForStop = false     // has the appender been asked to stopped\n+  @volatile private var stopped = false           // has the appender stopped\n+\n+  // Thread that reads the input stream and writes to file\n+  private val writingThread = new Thread(\"File appending thread for \" + file) {\n+    setDaemon(true)\n+    override def run() {\n+      Utils.logUncaughtExceptions {\n+        appendStreamToFile()\n+      }\n+    }\n+  }\n+  writingThread.start()\n+\n+  /**\n+   * Wait for the appender to stop appending, either because input stream is closed\n+   * or because of any error in appending\n+   */\n+  def awaitTermination() {\n+    synchronized {\n+      if (!stopped) {\n+        wait()\n+      }\n+    }\n+  }\n+\n+  /** Stop the appender */\n+  def stop() {\n+    markedForStop = true\n+  }\n+\n+  /** Continuously read chunks from the input stream and append to the file */\n+  protected def appendStreamToFile() {\n+    try {\n+      logDebug(\"Started appending thread\")\n+      openFile()\n+      val buf = new Array[Byte](bufferSize)\n+      var n = 0\n+      while (!markedForStop && n != -1) {\n+        n = inputStream.read(buf)\n+        if (n != -1) {\n+          appendToFile(buf, n)\n+        }\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Error writing stream to file $file\", e)\n+    } finally {\n+      closeFile()\n+      synchronized {\n+        stopped = true\n+        notifyAll()\n+      }\n+    }\n+  }\n+\n+  /** Append bytes to the file output stream */\n+  protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (outputStream == null) {\n+      openFile()\n+    }\n+    outputStream.write(bytes, 0, len)\n+  }\n+\n+  /** Open the file output stream */\n+  protected def openFile() {\n+    outputStream = new FileOutputStream(file, true)\n+    logDebug(s\"Opened file $file\")\n+  }\n+\n+  /** Close the file output stream */\n+  protected def closeFile() {\n+    outputStream.flush()\n+    outputStream.close()\n+    logDebug(s\"Closed file $file\")\n+  }\n+}\n+\n+/**\n+ * Companion object to [[org.apache.log4j.FileAppender]] which has helper\n+ * functions to choose the write type of FileAppender based on SparkConf configuration.\n+ */\n+private[spark] object FileAppender extends Logging {\n+\n+  /** Create the right appender based on Spark configuration */\n+  def apply(inputStream: InputStream, file: File, conf: SparkConf): FileAppender = {\n+\n+    import RollingFileAppender._\n+\n+    val rolloverEnabled = conf.getBoolean(ENABLE_PROPERTY, false)\n+    logDebug(\"Log rollover enabled = \" + rolloverEnabled)\n+    if (rolloverEnabled) {\n+\n+      val rolloverSizeOption = conf.getOption(SIZE_PROPERTY)\n+      val rolloverIntervalOption = conf.getOption(INTERVAL_PROPERTY)\n+\n+      (rolloverIntervalOption, rolloverSizeOption) match {\n+\n+        case (Some(rolloverInterval), Some(rolloverSize)) =>              // if both size and interval have been set\n+          logWarning(s\"Rollover interval [$rolloverInterval] and size [$rolloverSize] \" +\n+            s\"both set for executor logs, rolling logs not enabled\")\n+          new FileAppender(inputStream, file)\n+\n+        case (Some(rolloverInterval), None) =>  // if interval has been set\n+          rolloverInterval match {\n+            case \"daily\" =>\n+              logInfo(s\"Rolling executor logs enabled for $file with daily rolling\")\n+              new DailyRollingFileAppender(inputStream, file, conf)\n+            case \"hourly\" =>\n+              logInfo(s\"Rolling executor logs enabled for $file with hourly rolling\")\n+              new HourlyRollingFileAppender(inputStream, file, conf)\n+            case \"minutely\" =>\n+              logInfo(s\"Rolling executor logs enabled for $file with rolling every minute\")\n+              new MinutelyRollingFileAppender(inputStream, file, conf)\n+            case IntParam(millis) =>\n+              logInfo(s\"Rolling executor logs enabled for $file with rolling every minute\")\n+              new RollingFileAppender(inputStream, file, new TimeBasedRollingPolicy(millis),\n+                s\"--YYYY-MM-dd--HH-mm-ss-SSSS\", conf)\n+            case _ =>\n+              logWarning(\n+                s\"Illegal interval for rolling executor logs [$rolloverInterval], \" +\n+                  s\"rolling logs not enabled\")\n+              new FileAppender(inputStream, file)\n+          }\n+\n+        case (None, Some(rolloverSize)) =>    // if size has been set\n+          rolloverSize match {\n+            case IntParam(bytes) =>\n+              logInfo(s\"Rolling executor logs enabled for $file with rolling every $bytes bytes\")\n+              new RollingFileAppender(inputStream, file, new SizeBasedRollingPolicy(bytes),\n+                s\"--YYYY-MM-dd--HH-mm-ss-SSSS\", conf)\n+            case _ =>\n+              logWarning(\n+                s\"Illegal size for rolling executor logs [$rolloverSize], \" +\n+                  s\"rolling logs not enabled\")\n+              new FileAppender(inputStream, file)\n+          }\n+\n+        case (None, None) =>                // if neither size nor interval has been set\n+          logWarning(s\"Interval and size for rolling executor logs not set, \" +\n+            s\"rolling logs enabled with daily rolling.\")\n+          new DailyRollingFileAppender(inputStream, file, conf)\n+      }\n+    } else {\n+      new FileAppender(inputStream, file)\n+    }\n+  }\n+}\n+\n+/**\n+ * Defines the policy based on which [[org.apache.log4j.RollingFileAppender]] will\n+ * generate rolling files.\n+ */\n+private[spark] trait RollingPolicy {\n+\n+  /** Whether rollover should be initiated at this moment */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean\n+\n+  /** Notify that rollover has occurred */\n+  def rolledOver()\n+\n+  /** Notify that bytes have been written */\n+  def bytesWritten(bytes: Long)\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] by which files will be rolled\n+ * over at a fixed interval.\n+ */\n+private[spark] class TimeBasedRollingPolicy(val rolloverIntervalMillis: Long)\n+  extends RollingPolicy with Logging {\n+\n+  require(rolloverIntervalMillis >= 100)\n+\n+  @volatile private var nextRolloverTime = calculateNextRolloverTime()\n+\n+  /** Should rollover if current time has exceeded next rollover time */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean = {\n+    System.currentTimeMillis > nextRolloverTime\n+  }\n+\n+  /** Rollover has occurred, so find the next time to rollover */\n+  def rolledOver() {\n+    nextRolloverTime = calculateNextRolloverTime()\n+    logDebug(s\"Current time: ${System.currentTimeMillis}, next rollover time: \" + nextRolloverTime)\n+  }\n+\n+  def bytesWritten(bytes: Long) { }  // nothing to do\n+\n+  private def calculateNextRolloverTime(): Long = {\n+    val now = System.currentTimeMillis()\n+    val targetTime = math.ceil(now.toDouble / rolloverIntervalMillis) * rolloverIntervalMillis\n+    targetTime.toLong + 1  // +1 to make sure this falls in the next interval\n+  }\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] bywhich files will be rolled\n+ * over after reaching a particular size.\n+ */\n+private[spark] class SizeBasedRollingPolicy(val rolloverSizeBytes: Long)"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Addressed this reasonably cleanly. \n",
    "commit": "fd8f87fde302824813222e1a81003dea32010da7",
    "createdAt": "2014-06-05T02:59:37Z",
    "diffHunk": "@@ -0,0 +1,429 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util\n+\n+import java.io.{File, FileFilter, FileOutputStream, InputStream}\n+import java.text.SimpleDateFormat\n+import java.util.Calendar\n+import java.util.concurrent.{Executors, ThreadFactory}\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.spark.{Logging, SparkConf}\n+\n+/**\n+ * Continuously appends the data from an input stream into the given file.\n+ */\n+private[spark] class FileAppender(inputStream: InputStream, file: File, bufferSize: Int = 8192)\n+  extends Logging {\n+  @volatile private var outputStream: FileOutputStream = null\n+  @volatile private var markedForStop = false     // has the appender been asked to stopped\n+  @volatile private var stopped = false           // has the appender stopped\n+\n+  // Thread that reads the input stream and writes to file\n+  private val writingThread = new Thread(\"File appending thread for \" + file) {\n+    setDaemon(true)\n+    override def run() {\n+      Utils.logUncaughtExceptions {\n+        appendStreamToFile()\n+      }\n+    }\n+  }\n+  writingThread.start()\n+\n+  /**\n+   * Wait for the appender to stop appending, either because input stream is closed\n+   * or because of any error in appending\n+   */\n+  def awaitTermination() {\n+    synchronized {\n+      if (!stopped) {\n+        wait()\n+      }\n+    }\n+  }\n+\n+  /** Stop the appender */\n+  def stop() {\n+    markedForStop = true\n+  }\n+\n+  /** Continuously read chunks from the input stream and append to the file */\n+  protected def appendStreamToFile() {\n+    try {\n+      logDebug(\"Started appending thread\")\n+      openFile()\n+      val buf = new Array[Byte](bufferSize)\n+      var n = 0\n+      while (!markedForStop && n != -1) {\n+        n = inputStream.read(buf)\n+        if (n != -1) {\n+          appendToFile(buf, n)\n+        }\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Error writing stream to file $file\", e)\n+    } finally {\n+      closeFile()\n+      synchronized {\n+        stopped = true\n+        notifyAll()\n+      }\n+    }\n+  }\n+\n+  /** Append bytes to the file output stream */\n+  protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (outputStream == null) {\n+      openFile()\n+    }\n+    outputStream.write(bytes, 0, len)\n+  }\n+\n+  /** Open the file output stream */\n+  protected def openFile() {\n+    outputStream = new FileOutputStream(file, true)\n+    logDebug(s\"Opened file $file\")\n+  }\n+\n+  /** Close the file output stream */\n+  protected def closeFile() {\n+    outputStream.flush()\n+    outputStream.close()\n+    logDebug(s\"Closed file $file\")\n+  }\n+}\n+\n+/**\n+ * Companion object to [[org.apache.log4j.FileAppender]] which has helper\n+ * functions to choose the write type of FileAppender based on SparkConf configuration.\n+ */\n+private[spark] object FileAppender extends Logging {\n+\n+  /** Create the right appender based on Spark configuration */\n+  def apply(inputStream: InputStream, file: File, conf: SparkConf): FileAppender = {\n+\n+    import RollingFileAppender._\n+\n+    val rolloverEnabled = conf.getBoolean(ENABLE_PROPERTY, false)\n+    logDebug(\"Log rollover enabled = \" + rolloverEnabled)\n+    if (rolloverEnabled) {\n+\n+      val rolloverSizeOption = conf.getOption(SIZE_PROPERTY)\n+      val rolloverIntervalOption = conf.getOption(INTERVAL_PROPERTY)\n+\n+      (rolloverIntervalOption, rolloverSizeOption) match {\n+\n+        case (Some(rolloverInterval), Some(rolloverSize)) =>              // if both size and interval have been set\n+          logWarning(s\"Rollover interval [$rolloverInterval] and size [$rolloverSize] \" +\n+            s\"both set for executor logs, rolling logs not enabled\")\n+          new FileAppender(inputStream, file)\n+\n+        case (Some(rolloverInterval), None) =>  // if interval has been set\n+          rolloverInterval match {\n+            case \"daily\" =>\n+              logInfo(s\"Rolling executor logs enabled for $file with daily rolling\")\n+              new DailyRollingFileAppender(inputStream, file, conf)\n+            case \"hourly\" =>\n+              logInfo(s\"Rolling executor logs enabled for $file with hourly rolling\")\n+              new HourlyRollingFileAppender(inputStream, file, conf)\n+            case \"minutely\" =>\n+              logInfo(s\"Rolling executor logs enabled for $file with rolling every minute\")\n+              new MinutelyRollingFileAppender(inputStream, file, conf)\n+            case IntParam(millis) =>\n+              logInfo(s\"Rolling executor logs enabled for $file with rolling every minute\")\n+              new RollingFileAppender(inputStream, file, new TimeBasedRollingPolicy(millis),\n+                s\"--YYYY-MM-dd--HH-mm-ss-SSSS\", conf)\n+            case _ =>\n+              logWarning(\n+                s\"Illegal interval for rolling executor logs [$rolloverInterval], \" +\n+                  s\"rolling logs not enabled\")\n+              new FileAppender(inputStream, file)\n+          }\n+\n+        case (None, Some(rolloverSize)) =>    // if size has been set\n+          rolloverSize match {\n+            case IntParam(bytes) =>\n+              logInfo(s\"Rolling executor logs enabled for $file with rolling every $bytes bytes\")\n+              new RollingFileAppender(inputStream, file, new SizeBasedRollingPolicy(bytes),\n+                s\"--YYYY-MM-dd--HH-mm-ss-SSSS\", conf)\n+            case _ =>\n+              logWarning(\n+                s\"Illegal size for rolling executor logs [$rolloverSize], \" +\n+                  s\"rolling logs not enabled\")\n+              new FileAppender(inputStream, file)\n+          }\n+\n+        case (None, None) =>                // if neither size nor interval has been set\n+          logWarning(s\"Interval and size for rolling executor logs not set, \" +\n+            s\"rolling logs enabled with daily rolling.\")\n+          new DailyRollingFileAppender(inputStream, file, conf)\n+      }\n+    } else {\n+      new FileAppender(inputStream, file)\n+    }\n+  }\n+}\n+\n+/**\n+ * Defines the policy based on which [[org.apache.log4j.RollingFileAppender]] will\n+ * generate rolling files.\n+ */\n+private[spark] trait RollingPolicy {\n+\n+  /** Whether rollover should be initiated at this moment */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean\n+\n+  /** Notify that rollover has occurred */\n+  def rolledOver()\n+\n+  /** Notify that bytes have been written */\n+  def bytesWritten(bytes: Long)\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] by which files will be rolled\n+ * over at a fixed interval.\n+ */\n+private[spark] class TimeBasedRollingPolicy(val rolloverIntervalMillis: Long)\n+  extends RollingPolicy with Logging {\n+\n+  require(rolloverIntervalMillis >= 100)\n+\n+  @volatile private var nextRolloverTime = calculateNextRolloverTime()\n+\n+  /** Should rollover if current time has exceeded next rollover time */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean = {\n+    System.currentTimeMillis > nextRolloverTime\n+  }\n+\n+  /** Rollover has occurred, so find the next time to rollover */\n+  def rolledOver() {\n+    nextRolloverTime = calculateNextRolloverTime()\n+    logDebug(s\"Current time: ${System.currentTimeMillis}, next rollover time: \" + nextRolloverTime)\n+  }\n+\n+  def bytesWritten(bytes: Long) { }  // nothing to do\n+\n+  private def calculateNextRolloverTime(): Long = {\n+    val now = System.currentTimeMillis()\n+    val targetTime = math.ceil(now.toDouble / rolloverIntervalMillis) * rolloverIntervalMillis\n+    targetTime.toLong + 1  // +1 to make sure this falls in the next interval\n+  }\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] bywhich files will be rolled\n+ * over after reaching a particular size.\n+ */\n+private[spark] class SizeBasedRollingPolicy(val rolloverSizeBytes: Long)"
  }],
  "prId": 895
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "Minor: but would it be better to call this `bytesWrittenSinceRollover`? \"Till now\" could also be interpreted as the total number of bytes ever written.\n",
    "commit": "fd8f87fde302824813222e1a81003dea32010da7",
    "createdAt": "2014-06-05T22:24:05Z",
    "diffHunk": "@@ -0,0 +1,410 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util\n+\n+import java.io.{File, FileFilter, FileOutputStream, InputStream}\n+import java.text.SimpleDateFormat\n+import java.util.Calendar\n+import java.util.concurrent.{Executors, ThreadFactory}\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.spark.{Logging, SparkConf}\n+import RollingFileAppender._\n+\n+/**\n+ * Continuously appends the data from an input stream into the given file.\n+ */\n+private[spark] class FileAppender(inputStream: InputStream, file: File, bufferSize: Int = 8192)\n+  extends Logging {\n+  @volatile private var outputStream: FileOutputStream = null\n+  @volatile private var markedForStop = false     // has the appender been asked to stopped\n+  @volatile private var stopped = false           // has the appender stopped\n+\n+  // Thread that reads the input stream and writes to file\n+  private val writingThread = new Thread(\"File appending thread for \" + file) {\n+    setDaemon(true)\n+    override def run() {\n+      Utils.logUncaughtExceptions {\n+        appendStreamToFile()\n+      }\n+    }\n+  }\n+  writingThread.start()\n+\n+  /**\n+   * Wait for the appender to stop appending, either because input stream is closed\n+   * or because of any error in appending\n+   */\n+  def awaitTermination() {\n+    synchronized {\n+      if (!stopped) {\n+        wait()\n+      }\n+    }\n+  }\n+\n+  /** Stop the appender */\n+  def stop() {\n+    markedForStop = true\n+  }\n+\n+  /** Continuously read chunks from the input stream and append to the file */\n+  protected def appendStreamToFile() {\n+    try {\n+      logDebug(\"Started appending thread\")\n+      openFile()\n+      val buf = new Array[Byte](bufferSize)\n+      var n = 0\n+      while (!markedForStop && n != -1) {\n+        n = inputStream.read(buf)\n+        if (n != -1) {\n+          appendToFile(buf, n)\n+        }\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Error writing stream to file $file\", e)\n+    } finally {\n+      closeFile()\n+      synchronized {\n+        stopped = true\n+        notifyAll()\n+      }\n+    }\n+  }\n+\n+  /** Append bytes to the file output stream */\n+  protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (outputStream == null) {\n+      openFile()\n+    }\n+    outputStream.write(bytes, 0, len)\n+  }\n+\n+  /** Open the file output stream */\n+  protected def openFile() {\n+    outputStream = new FileOutputStream(file, true)\n+    logDebug(s\"Opened file $file\")\n+  }\n+\n+  /** Close the file output stream */\n+  protected def closeFile() {\n+    outputStream.flush()\n+    outputStream.close()\n+    logDebug(s\"Closed file $file\")\n+  }\n+}\n+\n+/**\n+ * Companion object to [[org.apache.log4j.FileAppender]] which has helper\n+ * functions to choose the write type of FileAppender based on SparkConf configuration.\n+ */\n+private[spark] object FileAppender extends Logging {\n+\n+  /** Create the right appender based on Spark configuration */\n+  def apply(inputStream: InputStream, file: File, conf: SparkConf): FileAppender = {\n+\n+    import RollingFileAppender._\n+\n+    val rolloverSizeOption = conf.getOption(SIZE_PROPERTY)\n+    val rolloverIntervalOption = conf.getOption(INTERVAL_PROPERTY)\n+\n+    (rolloverIntervalOption, rolloverSizeOption) match {\n+\n+      case (Some(rolloverInterval), Some(rolloverSize)) =>\n+        // if both size and interval have been set\n+        logWarning(s\"Rollover interval [$rolloverInterval] and size [$rolloverSize] \" +\n+          s\"both set for executor logs, rolling logs not enabled\")\n+        new FileAppender(inputStream, file)\n+\n+      case (Some(rolloverInterval), None) =>  // if interval has been set\n+        val validatedParams: Option[(Long, String)] = rolloverInterval match {\n+          case \"daily\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with daily rolling\")\n+            Some(24 * 60 * 60 * 1000L, s\"--YYYY-MM-dd\")\n+          case \"hourly\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with hourly rolling\")\n+            Some(60 * 60 * 1000L, s\"--YYYY-MM-dd--HH\")\n+          case \"minutely\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling every minute\")\n+            Some(60 * 1000L, s\"--YYYY-MM-dd--HH-mm\")\n+          case IntParam(seconds) =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling $seconds seconds\")\n+            Some(seconds * 1000L, s\"--YYYY-MM-dd--HH-mm-ss\")\n+          case _ =>\n+            logWarning(s\"Illegal interval for rolling executor logs [$rolloverInterval], \" +\n+                s\"rolling logs not enabled\")\n+            None\n+        }\n+        validatedParams.map {\n+          case (interval, pattern) =>\n+            new RollingFileAppender(\n+              inputStream, file, new TimeBasedRollingPolicy(interval),pattern, conf)\n+        }.getOrElse {\n+          new FileAppender(inputStream, file)\n+        }\n+\n+      case (None, Some(rolloverSize)) =>    // if size has been set\n+        rolloverSize match {\n+          case IntParam(bytes) =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling every $bytes bytes\")\n+            new RollingFileAppender(inputStream, file, new SizeBasedRollingPolicy(bytes),\n+              s\"--YYYY-MM-dd--HH-mm-ss-SSSS\", conf)\n+          case _ =>\n+            logWarning(\n+              s\"Illegal size for rolling executor logs [$rolloverSize], \" +\n+                s\"rolling logs not enabled\")\n+            new FileAppender(inputStream, file)\n+        }\n+\n+      case (None, None) =>                // if neither size nor interval has been set\n+        new FileAppender(inputStream, file)\n+    }\n+  }\n+}\n+\n+/**\n+ * Defines the policy based on which [[org.apache.spark.util.RollingFileAppender]] will\n+ * generate rolling files.\n+ */\n+private[spark] trait RollingPolicy {\n+\n+  /** Whether rollover should be initiated at this moment */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean\n+\n+  /** Notify that rollover has occurred */\n+  def rolledOver()\n+\n+  /** Notify that bytes have been written */\n+  def bytesWritten(bytes: Long)\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] by which files will be rolled\n+ * over at a fixed interval.\n+ */\n+private[spark] class TimeBasedRollingPolicy(\n+    var rolloverIntervalMillis: Long,\n+    checkIntervalConstraint: Boolean = true   // set to false while testing\n+  ) extends RollingPolicy with Logging {\n+\n+  import TimeBasedRollingPolicy._\n+  if (checkIntervalConstraint && rolloverIntervalMillis < MINIMUM_INTERVAL_SECONDS * 1000L) {\n+    logWarning(s\"Rolling interval [${rolloverIntervalMillis/1000L} seconds] is too small. \" +\n+      s\"Setting the interval to the acceptable minimum of $MINIMUM_INTERVAL_SECONDS seconds.\")\n+    rolloverIntervalMillis = MINIMUM_INTERVAL_SECONDS * 1000L\n+  }\n+\n+  @volatile private var nextRolloverTime = calculateNextRolloverTime()\n+\n+  /** Should rollover if current time has exceeded next rollover time */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean = {\n+    System.currentTimeMillis > nextRolloverTime\n+  }\n+\n+  /** Rollover has occurred, so find the next time to rollover */\n+  def rolledOver() {\n+    nextRolloverTime = calculateNextRolloverTime()\n+    logDebug(s\"Current time: ${System.currentTimeMillis}, next rollover time: \" + nextRolloverTime)\n+  }\n+\n+  def bytesWritten(bytes: Long) { }  // nothing to do\n+\n+  private def calculateNextRolloverTime(): Long = {\n+    val now = System.currentTimeMillis()\n+    val targetTime = (\n+      math.ceil(now.toDouble / rolloverIntervalMillis) * rolloverIntervalMillis\n+    ).toLong\n+    logDebug(s\"Next rollover time is $targetTime\")\n+    targetTime\n+  }\n+}\n+\n+private[spark] object TimeBasedRollingPolicy {\n+  val MINIMUM_INTERVAL_SECONDS = 60L  // 1 minute\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] by which files will be rolled\n+ * over after reaching a particular size.\n+ */\n+private[spark] class SizeBasedRollingPolicy(\n+    var rolloverSizeBytes: Long,\n+    checkSizeConstraint: Boolean = true     // set to false while testing\n+  ) extends RollingPolicy with Logging {\n+\n+  import SizeBasedRollingPolicy._\n+  if (checkSizeConstraint && rolloverSizeBytes < MINIMUM_SIZE_BYTES) {\n+    logWarning(s\"Rolling size [$rolloverSizeBytes bytes] is too small. \" +\n+      s\"Setting the size to the acceptable minimum of $MINIMUM_SIZE_BYTES bytes.\")\n+    rolloverSizeBytes = MINIMUM_SIZE_BYTES * 1000L\n+  }\n+\n+  @volatile private var bytesWrittenTillNow = 0L\n+\n+  /** Should rollover if the next set of bytes is going to exceed the size limit */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean = {\n+    bytesToBeWritten + bytesWrittenTillNow > rolloverSizeBytes\n+  }\n+\n+  /** Rollover has occurred, so reset the counter */\n+  def rolledOver() {\n+    bytesWrittenTillNow = 0"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Much better. \n",
    "commit": "fd8f87fde302824813222e1a81003dea32010da7",
    "createdAt": "2014-06-07T01:47:34Z",
    "diffHunk": "@@ -0,0 +1,410 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util\n+\n+import java.io.{File, FileFilter, FileOutputStream, InputStream}\n+import java.text.SimpleDateFormat\n+import java.util.Calendar\n+import java.util.concurrent.{Executors, ThreadFactory}\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.spark.{Logging, SparkConf}\n+import RollingFileAppender._\n+\n+/**\n+ * Continuously appends the data from an input stream into the given file.\n+ */\n+private[spark] class FileAppender(inputStream: InputStream, file: File, bufferSize: Int = 8192)\n+  extends Logging {\n+  @volatile private var outputStream: FileOutputStream = null\n+  @volatile private var markedForStop = false     // has the appender been asked to stopped\n+  @volatile private var stopped = false           // has the appender stopped\n+\n+  // Thread that reads the input stream and writes to file\n+  private val writingThread = new Thread(\"File appending thread for \" + file) {\n+    setDaemon(true)\n+    override def run() {\n+      Utils.logUncaughtExceptions {\n+        appendStreamToFile()\n+      }\n+    }\n+  }\n+  writingThread.start()\n+\n+  /**\n+   * Wait for the appender to stop appending, either because input stream is closed\n+   * or because of any error in appending\n+   */\n+  def awaitTermination() {\n+    synchronized {\n+      if (!stopped) {\n+        wait()\n+      }\n+    }\n+  }\n+\n+  /** Stop the appender */\n+  def stop() {\n+    markedForStop = true\n+  }\n+\n+  /** Continuously read chunks from the input stream and append to the file */\n+  protected def appendStreamToFile() {\n+    try {\n+      logDebug(\"Started appending thread\")\n+      openFile()\n+      val buf = new Array[Byte](bufferSize)\n+      var n = 0\n+      while (!markedForStop && n != -1) {\n+        n = inputStream.read(buf)\n+        if (n != -1) {\n+          appendToFile(buf, n)\n+        }\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Error writing stream to file $file\", e)\n+    } finally {\n+      closeFile()\n+      synchronized {\n+        stopped = true\n+        notifyAll()\n+      }\n+    }\n+  }\n+\n+  /** Append bytes to the file output stream */\n+  protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (outputStream == null) {\n+      openFile()\n+    }\n+    outputStream.write(bytes, 0, len)\n+  }\n+\n+  /** Open the file output stream */\n+  protected def openFile() {\n+    outputStream = new FileOutputStream(file, true)\n+    logDebug(s\"Opened file $file\")\n+  }\n+\n+  /** Close the file output stream */\n+  protected def closeFile() {\n+    outputStream.flush()\n+    outputStream.close()\n+    logDebug(s\"Closed file $file\")\n+  }\n+}\n+\n+/**\n+ * Companion object to [[org.apache.log4j.FileAppender]] which has helper\n+ * functions to choose the write type of FileAppender based on SparkConf configuration.\n+ */\n+private[spark] object FileAppender extends Logging {\n+\n+  /** Create the right appender based on Spark configuration */\n+  def apply(inputStream: InputStream, file: File, conf: SparkConf): FileAppender = {\n+\n+    import RollingFileAppender._\n+\n+    val rolloverSizeOption = conf.getOption(SIZE_PROPERTY)\n+    val rolloverIntervalOption = conf.getOption(INTERVAL_PROPERTY)\n+\n+    (rolloverIntervalOption, rolloverSizeOption) match {\n+\n+      case (Some(rolloverInterval), Some(rolloverSize)) =>\n+        // if both size and interval have been set\n+        logWarning(s\"Rollover interval [$rolloverInterval] and size [$rolloverSize] \" +\n+          s\"both set for executor logs, rolling logs not enabled\")\n+        new FileAppender(inputStream, file)\n+\n+      case (Some(rolloverInterval), None) =>  // if interval has been set\n+        val validatedParams: Option[(Long, String)] = rolloverInterval match {\n+          case \"daily\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with daily rolling\")\n+            Some(24 * 60 * 60 * 1000L, s\"--YYYY-MM-dd\")\n+          case \"hourly\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with hourly rolling\")\n+            Some(60 * 60 * 1000L, s\"--YYYY-MM-dd--HH\")\n+          case \"minutely\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling every minute\")\n+            Some(60 * 1000L, s\"--YYYY-MM-dd--HH-mm\")\n+          case IntParam(seconds) =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling $seconds seconds\")\n+            Some(seconds * 1000L, s\"--YYYY-MM-dd--HH-mm-ss\")\n+          case _ =>\n+            logWarning(s\"Illegal interval for rolling executor logs [$rolloverInterval], \" +\n+                s\"rolling logs not enabled\")\n+            None\n+        }\n+        validatedParams.map {\n+          case (interval, pattern) =>\n+            new RollingFileAppender(\n+              inputStream, file, new TimeBasedRollingPolicy(interval),pattern, conf)\n+        }.getOrElse {\n+          new FileAppender(inputStream, file)\n+        }\n+\n+      case (None, Some(rolloverSize)) =>    // if size has been set\n+        rolloverSize match {\n+          case IntParam(bytes) =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling every $bytes bytes\")\n+            new RollingFileAppender(inputStream, file, new SizeBasedRollingPolicy(bytes),\n+              s\"--YYYY-MM-dd--HH-mm-ss-SSSS\", conf)\n+          case _ =>\n+            logWarning(\n+              s\"Illegal size for rolling executor logs [$rolloverSize], \" +\n+                s\"rolling logs not enabled\")\n+            new FileAppender(inputStream, file)\n+        }\n+\n+      case (None, None) =>                // if neither size nor interval has been set\n+        new FileAppender(inputStream, file)\n+    }\n+  }\n+}\n+\n+/**\n+ * Defines the policy based on which [[org.apache.spark.util.RollingFileAppender]] will\n+ * generate rolling files.\n+ */\n+private[spark] trait RollingPolicy {\n+\n+  /** Whether rollover should be initiated at this moment */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean\n+\n+  /** Notify that rollover has occurred */\n+  def rolledOver()\n+\n+  /** Notify that bytes have been written */\n+  def bytesWritten(bytes: Long)\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] by which files will be rolled\n+ * over at a fixed interval.\n+ */\n+private[spark] class TimeBasedRollingPolicy(\n+    var rolloverIntervalMillis: Long,\n+    checkIntervalConstraint: Boolean = true   // set to false while testing\n+  ) extends RollingPolicy with Logging {\n+\n+  import TimeBasedRollingPolicy._\n+  if (checkIntervalConstraint && rolloverIntervalMillis < MINIMUM_INTERVAL_SECONDS * 1000L) {\n+    logWarning(s\"Rolling interval [${rolloverIntervalMillis/1000L} seconds] is too small. \" +\n+      s\"Setting the interval to the acceptable minimum of $MINIMUM_INTERVAL_SECONDS seconds.\")\n+    rolloverIntervalMillis = MINIMUM_INTERVAL_SECONDS * 1000L\n+  }\n+\n+  @volatile private var nextRolloverTime = calculateNextRolloverTime()\n+\n+  /** Should rollover if current time has exceeded next rollover time */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean = {\n+    System.currentTimeMillis > nextRolloverTime\n+  }\n+\n+  /** Rollover has occurred, so find the next time to rollover */\n+  def rolledOver() {\n+    nextRolloverTime = calculateNextRolloverTime()\n+    logDebug(s\"Current time: ${System.currentTimeMillis}, next rollover time: \" + nextRolloverTime)\n+  }\n+\n+  def bytesWritten(bytes: Long) { }  // nothing to do\n+\n+  private def calculateNextRolloverTime(): Long = {\n+    val now = System.currentTimeMillis()\n+    val targetTime = (\n+      math.ceil(now.toDouble / rolloverIntervalMillis) * rolloverIntervalMillis\n+    ).toLong\n+    logDebug(s\"Next rollover time is $targetTime\")\n+    targetTime\n+  }\n+}\n+\n+private[spark] object TimeBasedRollingPolicy {\n+  val MINIMUM_INTERVAL_SECONDS = 60L  // 1 minute\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] by which files will be rolled\n+ * over after reaching a particular size.\n+ */\n+private[spark] class SizeBasedRollingPolicy(\n+    var rolloverSizeBytes: Long,\n+    checkSizeConstraint: Boolean = true     // set to false while testing\n+  ) extends RollingPolicy with Logging {\n+\n+  import SizeBasedRollingPolicy._\n+  if (checkSizeConstraint && rolloverSizeBytes < MINIMUM_SIZE_BYTES) {\n+    logWarning(s\"Rolling size [$rolloverSizeBytes bytes] is too small. \" +\n+      s\"Setting the size to the acceptable minimum of $MINIMUM_SIZE_BYTES bytes.\")\n+    rolloverSizeBytes = MINIMUM_SIZE_BYTES * 1000L\n+  }\n+\n+  @volatile private var bytesWrittenTillNow = 0L\n+\n+  /** Should rollover if the next set of bytes is going to exceed the size limit */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean = {\n+    bytesToBeWritten + bytesWrittenTillNow > rolloverSizeBytes\n+  }\n+\n+  /** Rollover has occurred, so reset the counter */\n+  def rolledOver() {\n+    bytesWrittenTillNow = 0"
  }],
  "prId": 895
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "Why multiply by 1000? This seems inconsistent with the warning message.\n",
    "commit": "fd8f87fde302824813222e1a81003dea32010da7",
    "createdAt": "2014-06-05T22:25:08Z",
    "diffHunk": "@@ -0,0 +1,410 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util\n+\n+import java.io.{File, FileFilter, FileOutputStream, InputStream}\n+import java.text.SimpleDateFormat\n+import java.util.Calendar\n+import java.util.concurrent.{Executors, ThreadFactory}\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.spark.{Logging, SparkConf}\n+import RollingFileAppender._\n+\n+/**\n+ * Continuously appends the data from an input stream into the given file.\n+ */\n+private[spark] class FileAppender(inputStream: InputStream, file: File, bufferSize: Int = 8192)\n+  extends Logging {\n+  @volatile private var outputStream: FileOutputStream = null\n+  @volatile private var markedForStop = false     // has the appender been asked to stopped\n+  @volatile private var stopped = false           // has the appender stopped\n+\n+  // Thread that reads the input stream and writes to file\n+  private val writingThread = new Thread(\"File appending thread for \" + file) {\n+    setDaemon(true)\n+    override def run() {\n+      Utils.logUncaughtExceptions {\n+        appendStreamToFile()\n+      }\n+    }\n+  }\n+  writingThread.start()\n+\n+  /**\n+   * Wait for the appender to stop appending, either because input stream is closed\n+   * or because of any error in appending\n+   */\n+  def awaitTermination() {\n+    synchronized {\n+      if (!stopped) {\n+        wait()\n+      }\n+    }\n+  }\n+\n+  /** Stop the appender */\n+  def stop() {\n+    markedForStop = true\n+  }\n+\n+  /** Continuously read chunks from the input stream and append to the file */\n+  protected def appendStreamToFile() {\n+    try {\n+      logDebug(\"Started appending thread\")\n+      openFile()\n+      val buf = new Array[Byte](bufferSize)\n+      var n = 0\n+      while (!markedForStop && n != -1) {\n+        n = inputStream.read(buf)\n+        if (n != -1) {\n+          appendToFile(buf, n)\n+        }\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Error writing stream to file $file\", e)\n+    } finally {\n+      closeFile()\n+      synchronized {\n+        stopped = true\n+        notifyAll()\n+      }\n+    }\n+  }\n+\n+  /** Append bytes to the file output stream */\n+  protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (outputStream == null) {\n+      openFile()\n+    }\n+    outputStream.write(bytes, 0, len)\n+  }\n+\n+  /** Open the file output stream */\n+  protected def openFile() {\n+    outputStream = new FileOutputStream(file, true)\n+    logDebug(s\"Opened file $file\")\n+  }\n+\n+  /** Close the file output stream */\n+  protected def closeFile() {\n+    outputStream.flush()\n+    outputStream.close()\n+    logDebug(s\"Closed file $file\")\n+  }\n+}\n+\n+/**\n+ * Companion object to [[org.apache.log4j.FileAppender]] which has helper\n+ * functions to choose the write type of FileAppender based on SparkConf configuration.\n+ */\n+private[spark] object FileAppender extends Logging {\n+\n+  /** Create the right appender based on Spark configuration */\n+  def apply(inputStream: InputStream, file: File, conf: SparkConf): FileAppender = {\n+\n+    import RollingFileAppender._\n+\n+    val rolloverSizeOption = conf.getOption(SIZE_PROPERTY)\n+    val rolloverIntervalOption = conf.getOption(INTERVAL_PROPERTY)\n+\n+    (rolloverIntervalOption, rolloverSizeOption) match {\n+\n+      case (Some(rolloverInterval), Some(rolloverSize)) =>\n+        // if both size and interval have been set\n+        logWarning(s\"Rollover interval [$rolloverInterval] and size [$rolloverSize] \" +\n+          s\"both set for executor logs, rolling logs not enabled\")\n+        new FileAppender(inputStream, file)\n+\n+      case (Some(rolloverInterval), None) =>  // if interval has been set\n+        val validatedParams: Option[(Long, String)] = rolloverInterval match {\n+          case \"daily\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with daily rolling\")\n+            Some(24 * 60 * 60 * 1000L, s\"--YYYY-MM-dd\")\n+          case \"hourly\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with hourly rolling\")\n+            Some(60 * 60 * 1000L, s\"--YYYY-MM-dd--HH\")\n+          case \"minutely\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling every minute\")\n+            Some(60 * 1000L, s\"--YYYY-MM-dd--HH-mm\")\n+          case IntParam(seconds) =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling $seconds seconds\")\n+            Some(seconds * 1000L, s\"--YYYY-MM-dd--HH-mm-ss\")\n+          case _ =>\n+            logWarning(s\"Illegal interval for rolling executor logs [$rolloverInterval], \" +\n+                s\"rolling logs not enabled\")\n+            None\n+        }\n+        validatedParams.map {\n+          case (interval, pattern) =>\n+            new RollingFileAppender(\n+              inputStream, file, new TimeBasedRollingPolicy(interval),pattern, conf)\n+        }.getOrElse {\n+          new FileAppender(inputStream, file)\n+        }\n+\n+      case (None, Some(rolloverSize)) =>    // if size has been set\n+        rolloverSize match {\n+          case IntParam(bytes) =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling every $bytes bytes\")\n+            new RollingFileAppender(inputStream, file, new SizeBasedRollingPolicy(bytes),\n+              s\"--YYYY-MM-dd--HH-mm-ss-SSSS\", conf)\n+          case _ =>\n+            logWarning(\n+              s\"Illegal size for rolling executor logs [$rolloverSize], \" +\n+                s\"rolling logs not enabled\")\n+            new FileAppender(inputStream, file)\n+        }\n+\n+      case (None, None) =>                // if neither size nor interval has been set\n+        new FileAppender(inputStream, file)\n+    }\n+  }\n+}\n+\n+/**\n+ * Defines the policy based on which [[org.apache.spark.util.RollingFileAppender]] will\n+ * generate rolling files.\n+ */\n+private[spark] trait RollingPolicy {\n+\n+  /** Whether rollover should be initiated at this moment */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean\n+\n+  /** Notify that rollover has occurred */\n+  def rolledOver()\n+\n+  /** Notify that bytes have been written */\n+  def bytesWritten(bytes: Long)\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] by which files will be rolled\n+ * over at a fixed interval.\n+ */\n+private[spark] class TimeBasedRollingPolicy(\n+    var rolloverIntervalMillis: Long,\n+    checkIntervalConstraint: Boolean = true   // set to false while testing\n+  ) extends RollingPolicy with Logging {\n+\n+  import TimeBasedRollingPolicy._\n+  if (checkIntervalConstraint && rolloverIntervalMillis < MINIMUM_INTERVAL_SECONDS * 1000L) {\n+    logWarning(s\"Rolling interval [${rolloverIntervalMillis/1000L} seconds] is too small. \" +\n+      s\"Setting the interval to the acceptable minimum of $MINIMUM_INTERVAL_SECONDS seconds.\")\n+    rolloverIntervalMillis = MINIMUM_INTERVAL_SECONDS * 1000L\n+  }\n+\n+  @volatile private var nextRolloverTime = calculateNextRolloverTime()\n+\n+  /** Should rollover if current time has exceeded next rollover time */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean = {\n+    System.currentTimeMillis > nextRolloverTime\n+  }\n+\n+  /** Rollover has occurred, so find the next time to rollover */\n+  def rolledOver() {\n+    nextRolloverTime = calculateNextRolloverTime()\n+    logDebug(s\"Current time: ${System.currentTimeMillis}, next rollover time: \" + nextRolloverTime)\n+  }\n+\n+  def bytesWritten(bytes: Long) { }  // nothing to do\n+\n+  private def calculateNextRolloverTime(): Long = {\n+    val now = System.currentTimeMillis()\n+    val targetTime = (\n+      math.ceil(now.toDouble / rolloverIntervalMillis) * rolloverIntervalMillis\n+    ).toLong\n+    logDebug(s\"Next rollover time is $targetTime\")\n+    targetTime\n+  }\n+}\n+\n+private[spark] object TimeBasedRollingPolicy {\n+  val MINIMUM_INTERVAL_SECONDS = 60L  // 1 minute\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] by which files will be rolled\n+ * over after reaching a particular size.\n+ */\n+private[spark] class SizeBasedRollingPolicy(\n+    var rolloverSizeBytes: Long,\n+    checkSizeConstraint: Boolean = true     // set to false while testing\n+  ) extends RollingPolicy with Logging {\n+\n+  import SizeBasedRollingPolicy._\n+  if (checkSizeConstraint && rolloverSizeBytes < MINIMUM_SIZE_BYTES) {\n+    logWarning(s\"Rolling size [$rolloverSizeBytes bytes] is too small. \" +\n+      s\"Setting the size to the acceptable minimum of $MINIMUM_SIZE_BYTES bytes.\")\n+    rolloverSizeBytes = MINIMUM_SIZE_BYTES * 1000L"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Yikes!\n",
    "commit": "fd8f87fde302824813222e1a81003dea32010da7",
    "createdAt": "2014-06-07T01:48:15Z",
    "diffHunk": "@@ -0,0 +1,410 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util\n+\n+import java.io.{File, FileFilter, FileOutputStream, InputStream}\n+import java.text.SimpleDateFormat\n+import java.util.Calendar\n+import java.util.concurrent.{Executors, ThreadFactory}\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.spark.{Logging, SparkConf}\n+import RollingFileAppender._\n+\n+/**\n+ * Continuously appends the data from an input stream into the given file.\n+ */\n+private[spark] class FileAppender(inputStream: InputStream, file: File, bufferSize: Int = 8192)\n+  extends Logging {\n+  @volatile private var outputStream: FileOutputStream = null\n+  @volatile private var markedForStop = false     // has the appender been asked to stopped\n+  @volatile private var stopped = false           // has the appender stopped\n+\n+  // Thread that reads the input stream and writes to file\n+  private val writingThread = new Thread(\"File appending thread for \" + file) {\n+    setDaemon(true)\n+    override def run() {\n+      Utils.logUncaughtExceptions {\n+        appendStreamToFile()\n+      }\n+    }\n+  }\n+  writingThread.start()\n+\n+  /**\n+   * Wait for the appender to stop appending, either because input stream is closed\n+   * or because of any error in appending\n+   */\n+  def awaitTermination() {\n+    synchronized {\n+      if (!stopped) {\n+        wait()\n+      }\n+    }\n+  }\n+\n+  /** Stop the appender */\n+  def stop() {\n+    markedForStop = true\n+  }\n+\n+  /** Continuously read chunks from the input stream and append to the file */\n+  protected def appendStreamToFile() {\n+    try {\n+      logDebug(\"Started appending thread\")\n+      openFile()\n+      val buf = new Array[Byte](bufferSize)\n+      var n = 0\n+      while (!markedForStop && n != -1) {\n+        n = inputStream.read(buf)\n+        if (n != -1) {\n+          appendToFile(buf, n)\n+        }\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Error writing stream to file $file\", e)\n+    } finally {\n+      closeFile()\n+      synchronized {\n+        stopped = true\n+        notifyAll()\n+      }\n+    }\n+  }\n+\n+  /** Append bytes to the file output stream */\n+  protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (outputStream == null) {\n+      openFile()\n+    }\n+    outputStream.write(bytes, 0, len)\n+  }\n+\n+  /** Open the file output stream */\n+  protected def openFile() {\n+    outputStream = new FileOutputStream(file, true)\n+    logDebug(s\"Opened file $file\")\n+  }\n+\n+  /** Close the file output stream */\n+  protected def closeFile() {\n+    outputStream.flush()\n+    outputStream.close()\n+    logDebug(s\"Closed file $file\")\n+  }\n+}\n+\n+/**\n+ * Companion object to [[org.apache.log4j.FileAppender]] which has helper\n+ * functions to choose the write type of FileAppender based on SparkConf configuration.\n+ */\n+private[spark] object FileAppender extends Logging {\n+\n+  /** Create the right appender based on Spark configuration */\n+  def apply(inputStream: InputStream, file: File, conf: SparkConf): FileAppender = {\n+\n+    import RollingFileAppender._\n+\n+    val rolloverSizeOption = conf.getOption(SIZE_PROPERTY)\n+    val rolloverIntervalOption = conf.getOption(INTERVAL_PROPERTY)\n+\n+    (rolloverIntervalOption, rolloverSizeOption) match {\n+\n+      case (Some(rolloverInterval), Some(rolloverSize)) =>\n+        // if both size and interval have been set\n+        logWarning(s\"Rollover interval [$rolloverInterval] and size [$rolloverSize] \" +\n+          s\"both set for executor logs, rolling logs not enabled\")\n+        new FileAppender(inputStream, file)\n+\n+      case (Some(rolloverInterval), None) =>  // if interval has been set\n+        val validatedParams: Option[(Long, String)] = rolloverInterval match {\n+          case \"daily\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with daily rolling\")\n+            Some(24 * 60 * 60 * 1000L, s\"--YYYY-MM-dd\")\n+          case \"hourly\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with hourly rolling\")\n+            Some(60 * 60 * 1000L, s\"--YYYY-MM-dd--HH\")\n+          case \"minutely\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling every minute\")\n+            Some(60 * 1000L, s\"--YYYY-MM-dd--HH-mm\")\n+          case IntParam(seconds) =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling $seconds seconds\")\n+            Some(seconds * 1000L, s\"--YYYY-MM-dd--HH-mm-ss\")\n+          case _ =>\n+            logWarning(s\"Illegal interval for rolling executor logs [$rolloverInterval], \" +\n+                s\"rolling logs not enabled\")\n+            None\n+        }\n+        validatedParams.map {\n+          case (interval, pattern) =>\n+            new RollingFileAppender(\n+              inputStream, file, new TimeBasedRollingPolicy(interval),pattern, conf)\n+        }.getOrElse {\n+          new FileAppender(inputStream, file)\n+        }\n+\n+      case (None, Some(rolloverSize)) =>    // if size has been set\n+        rolloverSize match {\n+          case IntParam(bytes) =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling every $bytes bytes\")\n+            new RollingFileAppender(inputStream, file, new SizeBasedRollingPolicy(bytes),\n+              s\"--YYYY-MM-dd--HH-mm-ss-SSSS\", conf)\n+          case _ =>\n+            logWarning(\n+              s\"Illegal size for rolling executor logs [$rolloverSize], \" +\n+                s\"rolling logs not enabled\")\n+            new FileAppender(inputStream, file)\n+        }\n+\n+      case (None, None) =>                // if neither size nor interval has been set\n+        new FileAppender(inputStream, file)\n+    }\n+  }\n+}\n+\n+/**\n+ * Defines the policy based on which [[org.apache.spark.util.RollingFileAppender]] will\n+ * generate rolling files.\n+ */\n+private[spark] trait RollingPolicy {\n+\n+  /** Whether rollover should be initiated at this moment */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean\n+\n+  /** Notify that rollover has occurred */\n+  def rolledOver()\n+\n+  /** Notify that bytes have been written */\n+  def bytesWritten(bytes: Long)\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] by which files will be rolled\n+ * over at a fixed interval.\n+ */\n+private[spark] class TimeBasedRollingPolicy(\n+    var rolloverIntervalMillis: Long,\n+    checkIntervalConstraint: Boolean = true   // set to false while testing\n+  ) extends RollingPolicy with Logging {\n+\n+  import TimeBasedRollingPolicy._\n+  if (checkIntervalConstraint && rolloverIntervalMillis < MINIMUM_INTERVAL_SECONDS * 1000L) {\n+    logWarning(s\"Rolling interval [${rolloverIntervalMillis/1000L} seconds] is too small. \" +\n+      s\"Setting the interval to the acceptable minimum of $MINIMUM_INTERVAL_SECONDS seconds.\")\n+    rolloverIntervalMillis = MINIMUM_INTERVAL_SECONDS * 1000L\n+  }\n+\n+  @volatile private var nextRolloverTime = calculateNextRolloverTime()\n+\n+  /** Should rollover if current time has exceeded next rollover time */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean = {\n+    System.currentTimeMillis > nextRolloverTime\n+  }\n+\n+  /** Rollover has occurred, so find the next time to rollover */\n+  def rolledOver() {\n+    nextRolloverTime = calculateNextRolloverTime()\n+    logDebug(s\"Current time: ${System.currentTimeMillis}, next rollover time: \" + nextRolloverTime)\n+  }\n+\n+  def bytesWritten(bytes: Long) { }  // nothing to do\n+\n+  private def calculateNextRolloverTime(): Long = {\n+    val now = System.currentTimeMillis()\n+    val targetTime = (\n+      math.ceil(now.toDouble / rolloverIntervalMillis) * rolloverIntervalMillis\n+    ).toLong\n+    logDebug(s\"Next rollover time is $targetTime\")\n+    targetTime\n+  }\n+}\n+\n+private[spark] object TimeBasedRollingPolicy {\n+  val MINIMUM_INTERVAL_SECONDS = 60L  // 1 minute\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] by which files will be rolled\n+ * over after reaching a particular size.\n+ */\n+private[spark] class SizeBasedRollingPolicy(\n+    var rolloverSizeBytes: Long,\n+    checkSizeConstraint: Boolean = true     // set to false while testing\n+  ) extends RollingPolicy with Logging {\n+\n+  import SizeBasedRollingPolicy._\n+  if (checkSizeConstraint && rolloverSizeBytes < MINIMUM_SIZE_BYTES) {\n+    logWarning(s\"Rolling size [$rolloverSizeBytes bytes] is too small. \" +\n+      s\"Setting the size to the acceptable minimum of $MINIMUM_SIZE_BYTES bytes.\")\n+    rolloverSizeBytes = MINIMUM_SIZE_BYTES * 1000L"
  }],
  "prId": 895
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "minor: to avoid the name overloading, mind calling this `threadPool`?\n",
    "commit": "fd8f87fde302824813222e1a81003dea32010da7",
    "createdAt": "2014-06-05T22:26:45Z",
    "diffHunk": "@@ -0,0 +1,410 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util\n+\n+import java.io.{File, FileFilter, FileOutputStream, InputStream}\n+import java.text.SimpleDateFormat\n+import java.util.Calendar\n+import java.util.concurrent.{Executors, ThreadFactory}\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.spark.{Logging, SparkConf}\n+import RollingFileAppender._\n+\n+/**\n+ * Continuously appends the data from an input stream into the given file.\n+ */\n+private[spark] class FileAppender(inputStream: InputStream, file: File, bufferSize: Int = 8192)\n+  extends Logging {\n+  @volatile private var outputStream: FileOutputStream = null\n+  @volatile private var markedForStop = false     // has the appender been asked to stopped\n+  @volatile private var stopped = false           // has the appender stopped\n+\n+  // Thread that reads the input stream and writes to file\n+  private val writingThread = new Thread(\"File appending thread for \" + file) {\n+    setDaemon(true)\n+    override def run() {\n+      Utils.logUncaughtExceptions {\n+        appendStreamToFile()\n+      }\n+    }\n+  }\n+  writingThread.start()\n+\n+  /**\n+   * Wait for the appender to stop appending, either because input stream is closed\n+   * or because of any error in appending\n+   */\n+  def awaitTermination() {\n+    synchronized {\n+      if (!stopped) {\n+        wait()\n+      }\n+    }\n+  }\n+\n+  /** Stop the appender */\n+  def stop() {\n+    markedForStop = true\n+  }\n+\n+  /** Continuously read chunks from the input stream and append to the file */\n+  protected def appendStreamToFile() {\n+    try {\n+      logDebug(\"Started appending thread\")\n+      openFile()\n+      val buf = new Array[Byte](bufferSize)\n+      var n = 0\n+      while (!markedForStop && n != -1) {\n+        n = inputStream.read(buf)\n+        if (n != -1) {\n+          appendToFile(buf, n)\n+        }\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Error writing stream to file $file\", e)\n+    } finally {\n+      closeFile()\n+      synchronized {\n+        stopped = true\n+        notifyAll()\n+      }\n+    }\n+  }\n+\n+  /** Append bytes to the file output stream */\n+  protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (outputStream == null) {\n+      openFile()\n+    }\n+    outputStream.write(bytes, 0, len)\n+  }\n+\n+  /** Open the file output stream */\n+  protected def openFile() {\n+    outputStream = new FileOutputStream(file, true)\n+    logDebug(s\"Opened file $file\")\n+  }\n+\n+  /** Close the file output stream */\n+  protected def closeFile() {\n+    outputStream.flush()\n+    outputStream.close()\n+    logDebug(s\"Closed file $file\")\n+  }\n+}\n+\n+/**\n+ * Companion object to [[org.apache.log4j.FileAppender]] which has helper\n+ * functions to choose the write type of FileAppender based on SparkConf configuration.\n+ */\n+private[spark] object FileAppender extends Logging {\n+\n+  /** Create the right appender based on Spark configuration */\n+  def apply(inputStream: InputStream, file: File, conf: SparkConf): FileAppender = {\n+\n+    import RollingFileAppender._\n+\n+    val rolloverSizeOption = conf.getOption(SIZE_PROPERTY)\n+    val rolloverIntervalOption = conf.getOption(INTERVAL_PROPERTY)\n+\n+    (rolloverIntervalOption, rolloverSizeOption) match {\n+\n+      case (Some(rolloverInterval), Some(rolloverSize)) =>\n+        // if both size and interval have been set\n+        logWarning(s\"Rollover interval [$rolloverInterval] and size [$rolloverSize] \" +\n+          s\"both set for executor logs, rolling logs not enabled\")\n+        new FileAppender(inputStream, file)\n+\n+      case (Some(rolloverInterval), None) =>  // if interval has been set\n+        val validatedParams: Option[(Long, String)] = rolloverInterval match {\n+          case \"daily\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with daily rolling\")\n+            Some(24 * 60 * 60 * 1000L, s\"--YYYY-MM-dd\")\n+          case \"hourly\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with hourly rolling\")\n+            Some(60 * 60 * 1000L, s\"--YYYY-MM-dd--HH\")\n+          case \"minutely\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling every minute\")\n+            Some(60 * 1000L, s\"--YYYY-MM-dd--HH-mm\")\n+          case IntParam(seconds) =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling $seconds seconds\")\n+            Some(seconds * 1000L, s\"--YYYY-MM-dd--HH-mm-ss\")\n+          case _ =>\n+            logWarning(s\"Illegal interval for rolling executor logs [$rolloverInterval], \" +\n+                s\"rolling logs not enabled\")\n+            None\n+        }\n+        validatedParams.map {\n+          case (interval, pattern) =>\n+            new RollingFileAppender(\n+              inputStream, file, new TimeBasedRollingPolicy(interval),pattern, conf)\n+        }.getOrElse {\n+          new FileAppender(inputStream, file)\n+        }\n+\n+      case (None, Some(rolloverSize)) =>    // if size has been set\n+        rolloverSize match {\n+          case IntParam(bytes) =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling every $bytes bytes\")\n+            new RollingFileAppender(inputStream, file, new SizeBasedRollingPolicy(bytes),\n+              s\"--YYYY-MM-dd--HH-mm-ss-SSSS\", conf)\n+          case _ =>\n+            logWarning(\n+              s\"Illegal size for rolling executor logs [$rolloverSize], \" +\n+                s\"rolling logs not enabled\")\n+            new FileAppender(inputStream, file)\n+        }\n+\n+      case (None, None) =>                // if neither size nor interval has been set\n+        new FileAppender(inputStream, file)\n+    }\n+  }\n+}\n+\n+/**\n+ * Defines the policy based on which [[org.apache.spark.util.RollingFileAppender]] will\n+ * generate rolling files.\n+ */\n+private[spark] trait RollingPolicy {\n+\n+  /** Whether rollover should be initiated at this moment */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean\n+\n+  /** Notify that rollover has occurred */\n+  def rolledOver()\n+\n+  /** Notify that bytes have been written */\n+  def bytesWritten(bytes: Long)\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] by which files will be rolled\n+ * over at a fixed interval.\n+ */\n+private[spark] class TimeBasedRollingPolicy(\n+    var rolloverIntervalMillis: Long,\n+    checkIntervalConstraint: Boolean = true   // set to false while testing\n+  ) extends RollingPolicy with Logging {\n+\n+  import TimeBasedRollingPolicy._\n+  if (checkIntervalConstraint && rolloverIntervalMillis < MINIMUM_INTERVAL_SECONDS * 1000L) {\n+    logWarning(s\"Rolling interval [${rolloverIntervalMillis/1000L} seconds] is too small. \" +\n+      s\"Setting the interval to the acceptable minimum of $MINIMUM_INTERVAL_SECONDS seconds.\")\n+    rolloverIntervalMillis = MINIMUM_INTERVAL_SECONDS * 1000L\n+  }\n+\n+  @volatile private var nextRolloverTime = calculateNextRolloverTime()\n+\n+  /** Should rollover if current time has exceeded next rollover time */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean = {\n+    System.currentTimeMillis > nextRolloverTime\n+  }\n+\n+  /** Rollover has occurred, so find the next time to rollover */\n+  def rolledOver() {\n+    nextRolloverTime = calculateNextRolloverTime()\n+    logDebug(s\"Current time: ${System.currentTimeMillis}, next rollover time: \" + nextRolloverTime)\n+  }\n+\n+  def bytesWritten(bytes: Long) { }  // nothing to do\n+\n+  private def calculateNextRolloverTime(): Long = {\n+    val now = System.currentTimeMillis()\n+    val targetTime = (\n+      math.ceil(now.toDouble / rolloverIntervalMillis) * rolloverIntervalMillis\n+    ).toLong\n+    logDebug(s\"Next rollover time is $targetTime\")\n+    targetTime\n+  }\n+}\n+\n+private[spark] object TimeBasedRollingPolicy {\n+  val MINIMUM_INTERVAL_SECONDS = 60L  // 1 minute\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] by which files will be rolled\n+ * over after reaching a particular size.\n+ */\n+private[spark] class SizeBasedRollingPolicy(\n+    var rolloverSizeBytes: Long,\n+    checkSizeConstraint: Boolean = true     // set to false while testing\n+  ) extends RollingPolicy with Logging {\n+\n+  import SizeBasedRollingPolicy._\n+  if (checkSizeConstraint && rolloverSizeBytes < MINIMUM_SIZE_BYTES) {\n+    logWarning(s\"Rolling size [$rolloverSizeBytes bytes] is too small. \" +\n+      s\"Setting the size to the acceptable minimum of $MINIMUM_SIZE_BYTES bytes.\")\n+    rolloverSizeBytes = MINIMUM_SIZE_BYTES * 1000L\n+  }\n+\n+  @volatile private var bytesWrittenTillNow = 0L\n+\n+  /** Should rollover if the next set of bytes is going to exceed the size limit */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean = {\n+    bytesToBeWritten + bytesWrittenTillNow > rolloverSizeBytes\n+  }\n+\n+  /** Rollover has occurred, so reset the counter */\n+  def rolledOver() {\n+    bytesWrittenTillNow = 0\n+  }\n+\n+  /** Increment the bytes that have been written in the current file */\n+  def bytesWritten(bytes: Long) {\n+    bytesWrittenTillNow += bytes\n+  }\n+}\n+\n+private[spark] object SizeBasedRollingPolicy {\n+  val MINIMUM_SIZE_BYTES = RollingFileAppender.DEFAULT_BUFFER_SIZE * 10\n+}\n+\n+\n+/**\n+ * Continuously appends data from input stream into the given file, and rolls\n+ * over the file after the given interval. The rolled over files are named\n+ * based on the given pattern.\n+ *\n+ * @param inputStream             Input stream to read data from\n+ * @param activeFile              File to write data to\n+ * @param rollingPolicy           Policy based on which files will be rolled over.\n+ * @param rollingFilePattern      Pattern based on which the rolled over files will be named.\n+ *                                Uses SimpleDataFormat pattern.\n+ * @param conf                    SparkConf that is used to pass on extra configurations\n+ * @param bufferSize              Optional buffer size. Used mainly for testing.\n+ */\n+private[spark] class RollingFileAppender(\n+    inputStream: InputStream,\n+    activeFile: File,\n+    val rollingPolicy: RollingPolicy,\n+    rollingFilePattern: String,\n+    conf: SparkConf,\n+    bufferSize: Int = DEFAULT_BUFFER_SIZE\n+  ) extends FileAppender(inputStream, activeFile, bufferSize) {\n+\n+  private val retainCount = conf.getInt(KEEP_LAST_PROPERTY, -1)\n+  private val formatter = new SimpleDateFormat(rollingFilePattern)\n+\n+  private val executor = Executors.newFixedThreadPool(1, new ThreadFactory{"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Executor removed completely\n",
    "commit": "fd8f87fde302824813222e1a81003dea32010da7",
    "createdAt": "2014-06-09T21:12:49Z",
    "diffHunk": "@@ -0,0 +1,410 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util\n+\n+import java.io.{File, FileFilter, FileOutputStream, InputStream}\n+import java.text.SimpleDateFormat\n+import java.util.Calendar\n+import java.util.concurrent.{Executors, ThreadFactory}\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.spark.{Logging, SparkConf}\n+import RollingFileAppender._\n+\n+/**\n+ * Continuously appends the data from an input stream into the given file.\n+ */\n+private[spark] class FileAppender(inputStream: InputStream, file: File, bufferSize: Int = 8192)\n+  extends Logging {\n+  @volatile private var outputStream: FileOutputStream = null\n+  @volatile private var markedForStop = false     // has the appender been asked to stopped\n+  @volatile private var stopped = false           // has the appender stopped\n+\n+  // Thread that reads the input stream and writes to file\n+  private val writingThread = new Thread(\"File appending thread for \" + file) {\n+    setDaemon(true)\n+    override def run() {\n+      Utils.logUncaughtExceptions {\n+        appendStreamToFile()\n+      }\n+    }\n+  }\n+  writingThread.start()\n+\n+  /**\n+   * Wait for the appender to stop appending, either because input stream is closed\n+   * or because of any error in appending\n+   */\n+  def awaitTermination() {\n+    synchronized {\n+      if (!stopped) {\n+        wait()\n+      }\n+    }\n+  }\n+\n+  /** Stop the appender */\n+  def stop() {\n+    markedForStop = true\n+  }\n+\n+  /** Continuously read chunks from the input stream and append to the file */\n+  protected def appendStreamToFile() {\n+    try {\n+      logDebug(\"Started appending thread\")\n+      openFile()\n+      val buf = new Array[Byte](bufferSize)\n+      var n = 0\n+      while (!markedForStop && n != -1) {\n+        n = inputStream.read(buf)\n+        if (n != -1) {\n+          appendToFile(buf, n)\n+        }\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Error writing stream to file $file\", e)\n+    } finally {\n+      closeFile()\n+      synchronized {\n+        stopped = true\n+        notifyAll()\n+      }\n+    }\n+  }\n+\n+  /** Append bytes to the file output stream */\n+  protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (outputStream == null) {\n+      openFile()\n+    }\n+    outputStream.write(bytes, 0, len)\n+  }\n+\n+  /** Open the file output stream */\n+  protected def openFile() {\n+    outputStream = new FileOutputStream(file, true)\n+    logDebug(s\"Opened file $file\")\n+  }\n+\n+  /** Close the file output stream */\n+  protected def closeFile() {\n+    outputStream.flush()\n+    outputStream.close()\n+    logDebug(s\"Closed file $file\")\n+  }\n+}\n+\n+/**\n+ * Companion object to [[org.apache.log4j.FileAppender]] which has helper\n+ * functions to choose the write type of FileAppender based on SparkConf configuration.\n+ */\n+private[spark] object FileAppender extends Logging {\n+\n+  /** Create the right appender based on Spark configuration */\n+  def apply(inputStream: InputStream, file: File, conf: SparkConf): FileAppender = {\n+\n+    import RollingFileAppender._\n+\n+    val rolloverSizeOption = conf.getOption(SIZE_PROPERTY)\n+    val rolloverIntervalOption = conf.getOption(INTERVAL_PROPERTY)\n+\n+    (rolloverIntervalOption, rolloverSizeOption) match {\n+\n+      case (Some(rolloverInterval), Some(rolloverSize)) =>\n+        // if both size and interval have been set\n+        logWarning(s\"Rollover interval [$rolloverInterval] and size [$rolloverSize] \" +\n+          s\"both set for executor logs, rolling logs not enabled\")\n+        new FileAppender(inputStream, file)\n+\n+      case (Some(rolloverInterval), None) =>  // if interval has been set\n+        val validatedParams: Option[(Long, String)] = rolloverInterval match {\n+          case \"daily\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with daily rolling\")\n+            Some(24 * 60 * 60 * 1000L, s\"--YYYY-MM-dd\")\n+          case \"hourly\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with hourly rolling\")\n+            Some(60 * 60 * 1000L, s\"--YYYY-MM-dd--HH\")\n+          case \"minutely\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling every minute\")\n+            Some(60 * 1000L, s\"--YYYY-MM-dd--HH-mm\")\n+          case IntParam(seconds) =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling $seconds seconds\")\n+            Some(seconds * 1000L, s\"--YYYY-MM-dd--HH-mm-ss\")\n+          case _ =>\n+            logWarning(s\"Illegal interval for rolling executor logs [$rolloverInterval], \" +\n+                s\"rolling logs not enabled\")\n+            None\n+        }\n+        validatedParams.map {\n+          case (interval, pattern) =>\n+            new RollingFileAppender(\n+              inputStream, file, new TimeBasedRollingPolicy(interval),pattern, conf)\n+        }.getOrElse {\n+          new FileAppender(inputStream, file)\n+        }\n+\n+      case (None, Some(rolloverSize)) =>    // if size has been set\n+        rolloverSize match {\n+          case IntParam(bytes) =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling every $bytes bytes\")\n+            new RollingFileAppender(inputStream, file, new SizeBasedRollingPolicy(bytes),\n+              s\"--YYYY-MM-dd--HH-mm-ss-SSSS\", conf)\n+          case _ =>\n+            logWarning(\n+              s\"Illegal size for rolling executor logs [$rolloverSize], \" +\n+                s\"rolling logs not enabled\")\n+            new FileAppender(inputStream, file)\n+        }\n+\n+      case (None, None) =>                // if neither size nor interval has been set\n+        new FileAppender(inputStream, file)\n+    }\n+  }\n+}\n+\n+/**\n+ * Defines the policy based on which [[org.apache.spark.util.RollingFileAppender]] will\n+ * generate rolling files.\n+ */\n+private[spark] trait RollingPolicy {\n+\n+  /** Whether rollover should be initiated at this moment */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean\n+\n+  /** Notify that rollover has occurred */\n+  def rolledOver()\n+\n+  /** Notify that bytes have been written */\n+  def bytesWritten(bytes: Long)\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] by which files will be rolled\n+ * over at a fixed interval.\n+ */\n+private[spark] class TimeBasedRollingPolicy(\n+    var rolloverIntervalMillis: Long,\n+    checkIntervalConstraint: Boolean = true   // set to false while testing\n+  ) extends RollingPolicy with Logging {\n+\n+  import TimeBasedRollingPolicy._\n+  if (checkIntervalConstraint && rolloverIntervalMillis < MINIMUM_INTERVAL_SECONDS * 1000L) {\n+    logWarning(s\"Rolling interval [${rolloverIntervalMillis/1000L} seconds] is too small. \" +\n+      s\"Setting the interval to the acceptable minimum of $MINIMUM_INTERVAL_SECONDS seconds.\")\n+    rolloverIntervalMillis = MINIMUM_INTERVAL_SECONDS * 1000L\n+  }\n+\n+  @volatile private var nextRolloverTime = calculateNextRolloverTime()\n+\n+  /** Should rollover if current time has exceeded next rollover time */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean = {\n+    System.currentTimeMillis > nextRolloverTime\n+  }\n+\n+  /** Rollover has occurred, so find the next time to rollover */\n+  def rolledOver() {\n+    nextRolloverTime = calculateNextRolloverTime()\n+    logDebug(s\"Current time: ${System.currentTimeMillis}, next rollover time: \" + nextRolloverTime)\n+  }\n+\n+  def bytesWritten(bytes: Long) { }  // nothing to do\n+\n+  private def calculateNextRolloverTime(): Long = {\n+    val now = System.currentTimeMillis()\n+    val targetTime = (\n+      math.ceil(now.toDouble / rolloverIntervalMillis) * rolloverIntervalMillis\n+    ).toLong\n+    logDebug(s\"Next rollover time is $targetTime\")\n+    targetTime\n+  }\n+}\n+\n+private[spark] object TimeBasedRollingPolicy {\n+  val MINIMUM_INTERVAL_SECONDS = 60L  // 1 minute\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] by which files will be rolled\n+ * over after reaching a particular size.\n+ */\n+private[spark] class SizeBasedRollingPolicy(\n+    var rolloverSizeBytes: Long,\n+    checkSizeConstraint: Boolean = true     // set to false while testing\n+  ) extends RollingPolicy with Logging {\n+\n+  import SizeBasedRollingPolicy._\n+  if (checkSizeConstraint && rolloverSizeBytes < MINIMUM_SIZE_BYTES) {\n+    logWarning(s\"Rolling size [$rolloverSizeBytes bytes] is too small. \" +\n+      s\"Setting the size to the acceptable minimum of $MINIMUM_SIZE_BYTES bytes.\")\n+    rolloverSizeBytes = MINIMUM_SIZE_BYTES * 1000L\n+  }\n+\n+  @volatile private var bytesWrittenTillNow = 0L\n+\n+  /** Should rollover if the next set of bytes is going to exceed the size limit */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean = {\n+    bytesToBeWritten + bytesWrittenTillNow > rolloverSizeBytes\n+  }\n+\n+  /** Rollover has occurred, so reset the counter */\n+  def rolledOver() {\n+    bytesWrittenTillNow = 0\n+  }\n+\n+  /** Increment the bytes that have been written in the current file */\n+  def bytesWritten(bytes: Long) {\n+    bytesWrittenTillNow += bytes\n+  }\n+}\n+\n+private[spark] object SizeBasedRollingPolicy {\n+  val MINIMUM_SIZE_BYTES = RollingFileAppender.DEFAULT_BUFFER_SIZE * 10\n+}\n+\n+\n+/**\n+ * Continuously appends data from input stream into the given file, and rolls\n+ * over the file after the given interval. The rolled over files are named\n+ * based on the given pattern.\n+ *\n+ * @param inputStream             Input stream to read data from\n+ * @param activeFile              File to write data to\n+ * @param rollingPolicy           Policy based on which files will be rolled over.\n+ * @param rollingFilePattern      Pattern based on which the rolled over files will be named.\n+ *                                Uses SimpleDataFormat pattern.\n+ * @param conf                    SparkConf that is used to pass on extra configurations\n+ * @param bufferSize              Optional buffer size. Used mainly for testing.\n+ */\n+private[spark] class RollingFileAppender(\n+    inputStream: InputStream,\n+    activeFile: File,\n+    val rollingPolicy: RollingPolicy,\n+    rollingFilePattern: String,\n+    conf: SparkConf,\n+    bufferSize: Int = DEFAULT_BUFFER_SIZE\n+  ) extends FileAppender(inputStream, activeFile, bufferSize) {\n+\n+  private val retainCount = conf.getInt(KEEP_LAST_PROPERTY, -1)\n+  private val formatter = new SimpleDateFormat(rollingFilePattern)\n+\n+  private val executor = Executors.newFixedThreadPool(1, new ThreadFactory{"
  }],
  "prId": 895
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "Is there any case in which we anticipate duplicate file names? If not, isn't this just an error condition? I wonder if we should just throw an exception if this happens, since it indicates some assumption is broken.\n",
    "commit": "fd8f87fde302824813222e1a81003dea32010da7",
    "createdAt": "2014-06-05T22:30:32Z",
    "diffHunk": "@@ -0,0 +1,410 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util\n+\n+import java.io.{File, FileFilter, FileOutputStream, InputStream}\n+import java.text.SimpleDateFormat\n+import java.util.Calendar\n+import java.util.concurrent.{Executors, ThreadFactory}\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.spark.{Logging, SparkConf}\n+import RollingFileAppender._\n+\n+/**\n+ * Continuously appends the data from an input stream into the given file.\n+ */\n+private[spark] class FileAppender(inputStream: InputStream, file: File, bufferSize: Int = 8192)\n+  extends Logging {\n+  @volatile private var outputStream: FileOutputStream = null\n+  @volatile private var markedForStop = false     // has the appender been asked to stopped\n+  @volatile private var stopped = false           // has the appender stopped\n+\n+  // Thread that reads the input stream and writes to file\n+  private val writingThread = new Thread(\"File appending thread for \" + file) {\n+    setDaemon(true)\n+    override def run() {\n+      Utils.logUncaughtExceptions {\n+        appendStreamToFile()\n+      }\n+    }\n+  }\n+  writingThread.start()\n+\n+  /**\n+   * Wait for the appender to stop appending, either because input stream is closed\n+   * or because of any error in appending\n+   */\n+  def awaitTermination() {\n+    synchronized {\n+      if (!stopped) {\n+        wait()\n+      }\n+    }\n+  }\n+\n+  /** Stop the appender */\n+  def stop() {\n+    markedForStop = true\n+  }\n+\n+  /** Continuously read chunks from the input stream and append to the file */\n+  protected def appendStreamToFile() {\n+    try {\n+      logDebug(\"Started appending thread\")\n+      openFile()\n+      val buf = new Array[Byte](bufferSize)\n+      var n = 0\n+      while (!markedForStop && n != -1) {\n+        n = inputStream.read(buf)\n+        if (n != -1) {\n+          appendToFile(buf, n)\n+        }\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Error writing stream to file $file\", e)\n+    } finally {\n+      closeFile()\n+      synchronized {\n+        stopped = true\n+        notifyAll()\n+      }\n+    }\n+  }\n+\n+  /** Append bytes to the file output stream */\n+  protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (outputStream == null) {\n+      openFile()\n+    }\n+    outputStream.write(bytes, 0, len)\n+  }\n+\n+  /** Open the file output stream */\n+  protected def openFile() {\n+    outputStream = new FileOutputStream(file, true)\n+    logDebug(s\"Opened file $file\")\n+  }\n+\n+  /** Close the file output stream */\n+  protected def closeFile() {\n+    outputStream.flush()\n+    outputStream.close()\n+    logDebug(s\"Closed file $file\")\n+  }\n+}\n+\n+/**\n+ * Companion object to [[org.apache.log4j.FileAppender]] which has helper\n+ * functions to choose the write type of FileAppender based on SparkConf configuration.\n+ */\n+private[spark] object FileAppender extends Logging {\n+\n+  /** Create the right appender based on Spark configuration */\n+  def apply(inputStream: InputStream, file: File, conf: SparkConf): FileAppender = {\n+\n+    import RollingFileAppender._\n+\n+    val rolloverSizeOption = conf.getOption(SIZE_PROPERTY)\n+    val rolloverIntervalOption = conf.getOption(INTERVAL_PROPERTY)\n+\n+    (rolloverIntervalOption, rolloverSizeOption) match {\n+\n+      case (Some(rolloverInterval), Some(rolloverSize)) =>\n+        // if both size and interval have been set\n+        logWarning(s\"Rollover interval [$rolloverInterval] and size [$rolloverSize] \" +\n+          s\"both set for executor logs, rolling logs not enabled\")\n+        new FileAppender(inputStream, file)\n+\n+      case (Some(rolloverInterval), None) =>  // if interval has been set\n+        val validatedParams: Option[(Long, String)] = rolloverInterval match {\n+          case \"daily\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with daily rolling\")\n+            Some(24 * 60 * 60 * 1000L, s\"--YYYY-MM-dd\")\n+          case \"hourly\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with hourly rolling\")\n+            Some(60 * 60 * 1000L, s\"--YYYY-MM-dd--HH\")\n+          case \"minutely\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling every minute\")\n+            Some(60 * 1000L, s\"--YYYY-MM-dd--HH-mm\")\n+          case IntParam(seconds) =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling $seconds seconds\")\n+            Some(seconds * 1000L, s\"--YYYY-MM-dd--HH-mm-ss\")\n+          case _ =>\n+            logWarning(s\"Illegal interval for rolling executor logs [$rolloverInterval], \" +\n+                s\"rolling logs not enabled\")\n+            None\n+        }\n+        validatedParams.map {\n+          case (interval, pattern) =>\n+            new RollingFileAppender(\n+              inputStream, file, new TimeBasedRollingPolicy(interval),pattern, conf)\n+        }.getOrElse {\n+          new FileAppender(inputStream, file)\n+        }\n+\n+      case (None, Some(rolloverSize)) =>    // if size has been set\n+        rolloverSize match {\n+          case IntParam(bytes) =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling every $bytes bytes\")\n+            new RollingFileAppender(inputStream, file, new SizeBasedRollingPolicy(bytes),\n+              s\"--YYYY-MM-dd--HH-mm-ss-SSSS\", conf)\n+          case _ =>\n+            logWarning(\n+              s\"Illegal size for rolling executor logs [$rolloverSize], \" +\n+                s\"rolling logs not enabled\")\n+            new FileAppender(inputStream, file)\n+        }\n+\n+      case (None, None) =>                // if neither size nor interval has been set\n+        new FileAppender(inputStream, file)\n+    }\n+  }\n+}\n+\n+/**\n+ * Defines the policy based on which [[org.apache.spark.util.RollingFileAppender]] will\n+ * generate rolling files.\n+ */\n+private[spark] trait RollingPolicy {\n+\n+  /** Whether rollover should be initiated at this moment */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean\n+\n+  /** Notify that rollover has occurred */\n+  def rolledOver()\n+\n+  /** Notify that bytes have been written */\n+  def bytesWritten(bytes: Long)\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] by which files will be rolled\n+ * over at a fixed interval.\n+ */\n+private[spark] class TimeBasedRollingPolicy(\n+    var rolloverIntervalMillis: Long,\n+    checkIntervalConstraint: Boolean = true   // set to false while testing\n+  ) extends RollingPolicy with Logging {\n+\n+  import TimeBasedRollingPolicy._\n+  if (checkIntervalConstraint && rolloverIntervalMillis < MINIMUM_INTERVAL_SECONDS * 1000L) {\n+    logWarning(s\"Rolling interval [${rolloverIntervalMillis/1000L} seconds] is too small. \" +\n+      s\"Setting the interval to the acceptable minimum of $MINIMUM_INTERVAL_SECONDS seconds.\")\n+    rolloverIntervalMillis = MINIMUM_INTERVAL_SECONDS * 1000L\n+  }\n+\n+  @volatile private var nextRolloverTime = calculateNextRolloverTime()\n+\n+  /** Should rollover if current time has exceeded next rollover time */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean = {\n+    System.currentTimeMillis > nextRolloverTime\n+  }\n+\n+  /** Rollover has occurred, so find the next time to rollover */\n+  def rolledOver() {\n+    nextRolloverTime = calculateNextRolloverTime()\n+    logDebug(s\"Current time: ${System.currentTimeMillis}, next rollover time: \" + nextRolloverTime)\n+  }\n+\n+  def bytesWritten(bytes: Long) { }  // nothing to do\n+\n+  private def calculateNextRolloverTime(): Long = {\n+    val now = System.currentTimeMillis()\n+    val targetTime = (\n+      math.ceil(now.toDouble / rolloverIntervalMillis) * rolloverIntervalMillis\n+    ).toLong\n+    logDebug(s\"Next rollover time is $targetTime\")\n+    targetTime\n+  }\n+}\n+\n+private[spark] object TimeBasedRollingPolicy {\n+  val MINIMUM_INTERVAL_SECONDS = 60L  // 1 minute\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] by which files will be rolled\n+ * over after reaching a particular size.\n+ */\n+private[spark] class SizeBasedRollingPolicy(\n+    var rolloverSizeBytes: Long,\n+    checkSizeConstraint: Boolean = true     // set to false while testing\n+  ) extends RollingPolicy with Logging {\n+\n+  import SizeBasedRollingPolicy._\n+  if (checkSizeConstraint && rolloverSizeBytes < MINIMUM_SIZE_BYTES) {\n+    logWarning(s\"Rolling size [$rolloverSizeBytes bytes] is too small. \" +\n+      s\"Setting the size to the acceptable minimum of $MINIMUM_SIZE_BYTES bytes.\")\n+    rolloverSizeBytes = MINIMUM_SIZE_BYTES * 1000L\n+  }\n+\n+  @volatile private var bytesWrittenTillNow = 0L\n+\n+  /** Should rollover if the next set of bytes is going to exceed the size limit */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean = {\n+    bytesToBeWritten + bytesWrittenTillNow > rolloverSizeBytes\n+  }\n+\n+  /** Rollover has occurred, so reset the counter */\n+  def rolledOver() {\n+    bytesWrittenTillNow = 0\n+  }\n+\n+  /** Increment the bytes that have been written in the current file */\n+  def bytesWritten(bytes: Long) {\n+    bytesWrittenTillNow += bytes\n+  }\n+}\n+\n+private[spark] object SizeBasedRollingPolicy {\n+  val MINIMUM_SIZE_BYTES = RollingFileAppender.DEFAULT_BUFFER_SIZE * 10\n+}\n+\n+\n+/**\n+ * Continuously appends data from input stream into the given file, and rolls\n+ * over the file after the given interval. The rolled over files are named\n+ * based on the given pattern.\n+ *\n+ * @param inputStream             Input stream to read data from\n+ * @param activeFile              File to write data to\n+ * @param rollingPolicy           Policy based on which files will be rolled over.\n+ * @param rollingFilePattern      Pattern based on which the rolled over files will be named.\n+ *                                Uses SimpleDataFormat pattern.\n+ * @param conf                    SparkConf that is used to pass on extra configurations\n+ * @param bufferSize              Optional buffer size. Used mainly for testing.\n+ */\n+private[spark] class RollingFileAppender(\n+    inputStream: InputStream,\n+    activeFile: File,\n+    val rollingPolicy: RollingPolicy,\n+    rollingFilePattern: String,\n+    conf: SparkConf,\n+    bufferSize: Int = DEFAULT_BUFFER_SIZE\n+  ) extends FileAppender(inputStream, activeFile, bufferSize) {\n+\n+  private val retainCount = conf.getInt(KEEP_LAST_PROPERTY, -1)\n+  private val formatter = new SimpleDateFormat(rollingFilePattern)\n+\n+  private val executor = Executors.newFixedThreadPool(1, new ThreadFactory{\n+    def newThread(r: Runnable): Thread = {\n+      val t = new Thread(r)\n+      t.setDaemon(true)\n+      t.setName(s\"Threadpool of ${RollingFileAppender.this.getClass.getSimpleName} for $activeFile\")\n+      t\n+    }\n+  })\n+\n+  /** Stop the appender */\n+  override def stop() {\n+    super.stop()\n+    executor.shutdownNow()\n+  }\n+\n+  /** Append bytes to file after rolling over is necessary */\n+  override protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (rollingPolicy.shouldRollover(len)) {\n+      rollover()\n+      rollingPolicy.rolledOver()\n+    }\n+    super.appendToFile(bytes, len)\n+    rollingPolicy.bytesWritten(len)\n+  }\n+\n+  /** Rollover the file, by closing the output stream and moving it over */\n+  private def rollover() {\n+    val rolloverSuffix = formatter.format(Calendar.getInstance.getTime)\n+    val rolloverFile = new File(\n+      activeFile.getParentFile, activeFile.getName + rolloverSuffix).getAbsoluteFile\n+    logDebug(\"Attempting to rollover at \" + System.currentTimeMillis + \" to file \" + rolloverFile)\n+\n+    try {\n+      closeFile()\n+      if (activeFile.exists) {\n+        if (!rolloverFile.exists) {\n+          FileUtils.moveFile(activeFile, rolloverFile)\n+          logInfo(s\"Rolled over $activeFile to $rolloverFile\")\n+        } else {\n+          // In case the rollover file name clashes, make a unique file name."
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "I dont think it is a good idea throw an exception and stop logging any more. One should not stop generating logs at all costs, even if there is a rollover naming error. That's why i think its important to just show a warning and continue writing the logs out, even if it is too a unexpectedly named (but uniquely named) file.\n\nThe only reason there could be naming conflict is in scenarios where we rollover too fast for the name pattern. Like we rollover every minute, but the file pattern has been set to use hour as the finest granularity. Though wont happen because of the parameters we expose (\"hourly\" rolling will have hour in the file pattern name). \n",
    "commit": "fd8f87fde302824813222e1a81003dea32010da7",
    "createdAt": "2014-06-08T23:28:01Z",
    "diffHunk": "@@ -0,0 +1,410 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util\n+\n+import java.io.{File, FileFilter, FileOutputStream, InputStream}\n+import java.text.SimpleDateFormat\n+import java.util.Calendar\n+import java.util.concurrent.{Executors, ThreadFactory}\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.spark.{Logging, SparkConf}\n+import RollingFileAppender._\n+\n+/**\n+ * Continuously appends the data from an input stream into the given file.\n+ */\n+private[spark] class FileAppender(inputStream: InputStream, file: File, bufferSize: Int = 8192)\n+  extends Logging {\n+  @volatile private var outputStream: FileOutputStream = null\n+  @volatile private var markedForStop = false     // has the appender been asked to stopped\n+  @volatile private var stopped = false           // has the appender stopped\n+\n+  // Thread that reads the input stream and writes to file\n+  private val writingThread = new Thread(\"File appending thread for \" + file) {\n+    setDaemon(true)\n+    override def run() {\n+      Utils.logUncaughtExceptions {\n+        appendStreamToFile()\n+      }\n+    }\n+  }\n+  writingThread.start()\n+\n+  /**\n+   * Wait for the appender to stop appending, either because input stream is closed\n+   * or because of any error in appending\n+   */\n+  def awaitTermination() {\n+    synchronized {\n+      if (!stopped) {\n+        wait()\n+      }\n+    }\n+  }\n+\n+  /** Stop the appender */\n+  def stop() {\n+    markedForStop = true\n+  }\n+\n+  /** Continuously read chunks from the input stream and append to the file */\n+  protected def appendStreamToFile() {\n+    try {\n+      logDebug(\"Started appending thread\")\n+      openFile()\n+      val buf = new Array[Byte](bufferSize)\n+      var n = 0\n+      while (!markedForStop && n != -1) {\n+        n = inputStream.read(buf)\n+        if (n != -1) {\n+          appendToFile(buf, n)\n+        }\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Error writing stream to file $file\", e)\n+    } finally {\n+      closeFile()\n+      synchronized {\n+        stopped = true\n+        notifyAll()\n+      }\n+    }\n+  }\n+\n+  /** Append bytes to the file output stream */\n+  protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (outputStream == null) {\n+      openFile()\n+    }\n+    outputStream.write(bytes, 0, len)\n+  }\n+\n+  /** Open the file output stream */\n+  protected def openFile() {\n+    outputStream = new FileOutputStream(file, true)\n+    logDebug(s\"Opened file $file\")\n+  }\n+\n+  /** Close the file output stream */\n+  protected def closeFile() {\n+    outputStream.flush()\n+    outputStream.close()\n+    logDebug(s\"Closed file $file\")\n+  }\n+}\n+\n+/**\n+ * Companion object to [[org.apache.log4j.FileAppender]] which has helper\n+ * functions to choose the write type of FileAppender based on SparkConf configuration.\n+ */\n+private[spark] object FileAppender extends Logging {\n+\n+  /** Create the right appender based on Spark configuration */\n+  def apply(inputStream: InputStream, file: File, conf: SparkConf): FileAppender = {\n+\n+    import RollingFileAppender._\n+\n+    val rolloverSizeOption = conf.getOption(SIZE_PROPERTY)\n+    val rolloverIntervalOption = conf.getOption(INTERVAL_PROPERTY)\n+\n+    (rolloverIntervalOption, rolloverSizeOption) match {\n+\n+      case (Some(rolloverInterval), Some(rolloverSize)) =>\n+        // if both size and interval have been set\n+        logWarning(s\"Rollover interval [$rolloverInterval] and size [$rolloverSize] \" +\n+          s\"both set for executor logs, rolling logs not enabled\")\n+        new FileAppender(inputStream, file)\n+\n+      case (Some(rolloverInterval), None) =>  // if interval has been set\n+        val validatedParams: Option[(Long, String)] = rolloverInterval match {\n+          case \"daily\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with daily rolling\")\n+            Some(24 * 60 * 60 * 1000L, s\"--YYYY-MM-dd\")\n+          case \"hourly\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with hourly rolling\")\n+            Some(60 * 60 * 1000L, s\"--YYYY-MM-dd--HH\")\n+          case \"minutely\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling every minute\")\n+            Some(60 * 1000L, s\"--YYYY-MM-dd--HH-mm\")\n+          case IntParam(seconds) =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling $seconds seconds\")\n+            Some(seconds * 1000L, s\"--YYYY-MM-dd--HH-mm-ss\")\n+          case _ =>\n+            logWarning(s\"Illegal interval for rolling executor logs [$rolloverInterval], \" +\n+                s\"rolling logs not enabled\")\n+            None\n+        }\n+        validatedParams.map {\n+          case (interval, pattern) =>\n+            new RollingFileAppender(\n+              inputStream, file, new TimeBasedRollingPolicy(interval),pattern, conf)\n+        }.getOrElse {\n+          new FileAppender(inputStream, file)\n+        }\n+\n+      case (None, Some(rolloverSize)) =>    // if size has been set\n+        rolloverSize match {\n+          case IntParam(bytes) =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling every $bytes bytes\")\n+            new RollingFileAppender(inputStream, file, new SizeBasedRollingPolicy(bytes),\n+              s\"--YYYY-MM-dd--HH-mm-ss-SSSS\", conf)\n+          case _ =>\n+            logWarning(\n+              s\"Illegal size for rolling executor logs [$rolloverSize], \" +\n+                s\"rolling logs not enabled\")\n+            new FileAppender(inputStream, file)\n+        }\n+\n+      case (None, None) =>                // if neither size nor interval has been set\n+        new FileAppender(inputStream, file)\n+    }\n+  }\n+}\n+\n+/**\n+ * Defines the policy based on which [[org.apache.spark.util.RollingFileAppender]] will\n+ * generate rolling files.\n+ */\n+private[spark] trait RollingPolicy {\n+\n+  /** Whether rollover should be initiated at this moment */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean\n+\n+  /** Notify that rollover has occurred */\n+  def rolledOver()\n+\n+  /** Notify that bytes have been written */\n+  def bytesWritten(bytes: Long)\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] by which files will be rolled\n+ * over at a fixed interval.\n+ */\n+private[spark] class TimeBasedRollingPolicy(\n+    var rolloverIntervalMillis: Long,\n+    checkIntervalConstraint: Boolean = true   // set to false while testing\n+  ) extends RollingPolicy with Logging {\n+\n+  import TimeBasedRollingPolicy._\n+  if (checkIntervalConstraint && rolloverIntervalMillis < MINIMUM_INTERVAL_SECONDS * 1000L) {\n+    logWarning(s\"Rolling interval [${rolloverIntervalMillis/1000L} seconds] is too small. \" +\n+      s\"Setting the interval to the acceptable minimum of $MINIMUM_INTERVAL_SECONDS seconds.\")\n+    rolloverIntervalMillis = MINIMUM_INTERVAL_SECONDS * 1000L\n+  }\n+\n+  @volatile private var nextRolloverTime = calculateNextRolloverTime()\n+\n+  /** Should rollover if current time has exceeded next rollover time */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean = {\n+    System.currentTimeMillis > nextRolloverTime\n+  }\n+\n+  /** Rollover has occurred, so find the next time to rollover */\n+  def rolledOver() {\n+    nextRolloverTime = calculateNextRolloverTime()\n+    logDebug(s\"Current time: ${System.currentTimeMillis}, next rollover time: \" + nextRolloverTime)\n+  }\n+\n+  def bytesWritten(bytes: Long) { }  // nothing to do\n+\n+  private def calculateNextRolloverTime(): Long = {\n+    val now = System.currentTimeMillis()\n+    val targetTime = (\n+      math.ceil(now.toDouble / rolloverIntervalMillis) * rolloverIntervalMillis\n+    ).toLong\n+    logDebug(s\"Next rollover time is $targetTime\")\n+    targetTime\n+  }\n+}\n+\n+private[spark] object TimeBasedRollingPolicy {\n+  val MINIMUM_INTERVAL_SECONDS = 60L  // 1 minute\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] by which files will be rolled\n+ * over after reaching a particular size.\n+ */\n+private[spark] class SizeBasedRollingPolicy(\n+    var rolloverSizeBytes: Long,\n+    checkSizeConstraint: Boolean = true     // set to false while testing\n+  ) extends RollingPolicy with Logging {\n+\n+  import SizeBasedRollingPolicy._\n+  if (checkSizeConstraint && rolloverSizeBytes < MINIMUM_SIZE_BYTES) {\n+    logWarning(s\"Rolling size [$rolloverSizeBytes bytes] is too small. \" +\n+      s\"Setting the size to the acceptable minimum of $MINIMUM_SIZE_BYTES bytes.\")\n+    rolloverSizeBytes = MINIMUM_SIZE_BYTES * 1000L\n+  }\n+\n+  @volatile private var bytesWrittenTillNow = 0L\n+\n+  /** Should rollover if the next set of bytes is going to exceed the size limit */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean = {\n+    bytesToBeWritten + bytesWrittenTillNow > rolloverSizeBytes\n+  }\n+\n+  /** Rollover has occurred, so reset the counter */\n+  def rolledOver() {\n+    bytesWrittenTillNow = 0\n+  }\n+\n+  /** Increment the bytes that have been written in the current file */\n+  def bytesWritten(bytes: Long) {\n+    bytesWrittenTillNow += bytes\n+  }\n+}\n+\n+private[spark] object SizeBasedRollingPolicy {\n+  val MINIMUM_SIZE_BYTES = RollingFileAppender.DEFAULT_BUFFER_SIZE * 10\n+}\n+\n+\n+/**\n+ * Continuously appends data from input stream into the given file, and rolls\n+ * over the file after the given interval. The rolled over files are named\n+ * based on the given pattern.\n+ *\n+ * @param inputStream             Input stream to read data from\n+ * @param activeFile              File to write data to\n+ * @param rollingPolicy           Policy based on which files will be rolled over.\n+ * @param rollingFilePattern      Pattern based on which the rolled over files will be named.\n+ *                                Uses SimpleDataFormat pattern.\n+ * @param conf                    SparkConf that is used to pass on extra configurations\n+ * @param bufferSize              Optional buffer size. Used mainly for testing.\n+ */\n+private[spark] class RollingFileAppender(\n+    inputStream: InputStream,\n+    activeFile: File,\n+    val rollingPolicy: RollingPolicy,\n+    rollingFilePattern: String,\n+    conf: SparkConf,\n+    bufferSize: Int = DEFAULT_BUFFER_SIZE\n+  ) extends FileAppender(inputStream, activeFile, bufferSize) {\n+\n+  private val retainCount = conf.getInt(KEEP_LAST_PROPERTY, -1)\n+  private val formatter = new SimpleDateFormat(rollingFilePattern)\n+\n+  private val executor = Executors.newFixedThreadPool(1, new ThreadFactory{\n+    def newThread(r: Runnable): Thread = {\n+      val t = new Thread(r)\n+      t.setDaemon(true)\n+      t.setName(s\"Threadpool of ${RollingFileAppender.this.getClass.getSimpleName} for $activeFile\")\n+      t\n+    }\n+  })\n+\n+  /** Stop the appender */\n+  override def stop() {\n+    super.stop()\n+    executor.shutdownNow()\n+  }\n+\n+  /** Append bytes to file after rolling over is necessary */\n+  override protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (rollingPolicy.shouldRollover(len)) {\n+      rollover()\n+      rollingPolicy.rolledOver()\n+    }\n+    super.appendToFile(bytes, len)\n+    rollingPolicy.bytesWritten(len)\n+  }\n+\n+  /** Rollover the file, by closing the output stream and moving it over */\n+  private def rollover() {\n+    val rolloverSuffix = formatter.format(Calendar.getInstance.getTime)\n+    val rolloverFile = new File(\n+      activeFile.getParentFile, activeFile.getName + rolloverSuffix).getAbsoluteFile\n+    logDebug(\"Attempting to rollover at \" + System.currentTimeMillis + \" to file \" + rolloverFile)\n+\n+    try {\n+      closeFile()\n+      if (activeFile.exists) {\n+        if (!rolloverFile.exists) {\n+          FileUtils.moveFile(activeFile, rolloverFile)\n+          logInfo(s\"Rolled over $activeFile to $rolloverFile\")\n+        } else {\n+          // In case the rollover file name clashes, make a unique file name."
  }, {
    "author": {
      "login": "pwendell"
    },
    "body": "What about just appending the millisecond or nanosecond time stamp to the end of the file name then? That would simplify the rollover suffix.\n",
    "commit": "fd8f87fde302824813222e1a81003dea32010da7",
    "createdAt": "2014-06-09T06:09:49Z",
    "diffHunk": "@@ -0,0 +1,410 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util\n+\n+import java.io.{File, FileFilter, FileOutputStream, InputStream}\n+import java.text.SimpleDateFormat\n+import java.util.Calendar\n+import java.util.concurrent.{Executors, ThreadFactory}\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.spark.{Logging, SparkConf}\n+import RollingFileAppender._\n+\n+/**\n+ * Continuously appends the data from an input stream into the given file.\n+ */\n+private[spark] class FileAppender(inputStream: InputStream, file: File, bufferSize: Int = 8192)\n+  extends Logging {\n+  @volatile private var outputStream: FileOutputStream = null\n+  @volatile private var markedForStop = false     // has the appender been asked to stopped\n+  @volatile private var stopped = false           // has the appender stopped\n+\n+  // Thread that reads the input stream and writes to file\n+  private val writingThread = new Thread(\"File appending thread for \" + file) {\n+    setDaemon(true)\n+    override def run() {\n+      Utils.logUncaughtExceptions {\n+        appendStreamToFile()\n+      }\n+    }\n+  }\n+  writingThread.start()\n+\n+  /**\n+   * Wait for the appender to stop appending, either because input stream is closed\n+   * or because of any error in appending\n+   */\n+  def awaitTermination() {\n+    synchronized {\n+      if (!stopped) {\n+        wait()\n+      }\n+    }\n+  }\n+\n+  /** Stop the appender */\n+  def stop() {\n+    markedForStop = true\n+  }\n+\n+  /** Continuously read chunks from the input stream and append to the file */\n+  protected def appendStreamToFile() {\n+    try {\n+      logDebug(\"Started appending thread\")\n+      openFile()\n+      val buf = new Array[Byte](bufferSize)\n+      var n = 0\n+      while (!markedForStop && n != -1) {\n+        n = inputStream.read(buf)\n+        if (n != -1) {\n+          appendToFile(buf, n)\n+        }\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Error writing stream to file $file\", e)\n+    } finally {\n+      closeFile()\n+      synchronized {\n+        stopped = true\n+        notifyAll()\n+      }\n+    }\n+  }\n+\n+  /** Append bytes to the file output stream */\n+  protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (outputStream == null) {\n+      openFile()\n+    }\n+    outputStream.write(bytes, 0, len)\n+  }\n+\n+  /** Open the file output stream */\n+  protected def openFile() {\n+    outputStream = new FileOutputStream(file, true)\n+    logDebug(s\"Opened file $file\")\n+  }\n+\n+  /** Close the file output stream */\n+  protected def closeFile() {\n+    outputStream.flush()\n+    outputStream.close()\n+    logDebug(s\"Closed file $file\")\n+  }\n+}\n+\n+/**\n+ * Companion object to [[org.apache.log4j.FileAppender]] which has helper\n+ * functions to choose the write type of FileAppender based on SparkConf configuration.\n+ */\n+private[spark] object FileAppender extends Logging {\n+\n+  /** Create the right appender based on Spark configuration */\n+  def apply(inputStream: InputStream, file: File, conf: SparkConf): FileAppender = {\n+\n+    import RollingFileAppender._\n+\n+    val rolloverSizeOption = conf.getOption(SIZE_PROPERTY)\n+    val rolloverIntervalOption = conf.getOption(INTERVAL_PROPERTY)\n+\n+    (rolloverIntervalOption, rolloverSizeOption) match {\n+\n+      case (Some(rolloverInterval), Some(rolloverSize)) =>\n+        // if both size and interval have been set\n+        logWarning(s\"Rollover interval [$rolloverInterval] and size [$rolloverSize] \" +\n+          s\"both set for executor logs, rolling logs not enabled\")\n+        new FileAppender(inputStream, file)\n+\n+      case (Some(rolloverInterval), None) =>  // if interval has been set\n+        val validatedParams: Option[(Long, String)] = rolloverInterval match {\n+          case \"daily\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with daily rolling\")\n+            Some(24 * 60 * 60 * 1000L, s\"--YYYY-MM-dd\")\n+          case \"hourly\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with hourly rolling\")\n+            Some(60 * 60 * 1000L, s\"--YYYY-MM-dd--HH\")\n+          case \"minutely\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling every minute\")\n+            Some(60 * 1000L, s\"--YYYY-MM-dd--HH-mm\")\n+          case IntParam(seconds) =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling $seconds seconds\")\n+            Some(seconds * 1000L, s\"--YYYY-MM-dd--HH-mm-ss\")\n+          case _ =>\n+            logWarning(s\"Illegal interval for rolling executor logs [$rolloverInterval], \" +\n+                s\"rolling logs not enabled\")\n+            None\n+        }\n+        validatedParams.map {\n+          case (interval, pattern) =>\n+            new RollingFileAppender(\n+              inputStream, file, new TimeBasedRollingPolicy(interval),pattern, conf)\n+        }.getOrElse {\n+          new FileAppender(inputStream, file)\n+        }\n+\n+      case (None, Some(rolloverSize)) =>    // if size has been set\n+        rolloverSize match {\n+          case IntParam(bytes) =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling every $bytes bytes\")\n+            new RollingFileAppender(inputStream, file, new SizeBasedRollingPolicy(bytes),\n+              s\"--YYYY-MM-dd--HH-mm-ss-SSSS\", conf)\n+          case _ =>\n+            logWarning(\n+              s\"Illegal size for rolling executor logs [$rolloverSize], \" +\n+                s\"rolling logs not enabled\")\n+            new FileAppender(inputStream, file)\n+        }\n+\n+      case (None, None) =>                // if neither size nor interval has been set\n+        new FileAppender(inputStream, file)\n+    }\n+  }\n+}\n+\n+/**\n+ * Defines the policy based on which [[org.apache.spark.util.RollingFileAppender]] will\n+ * generate rolling files.\n+ */\n+private[spark] trait RollingPolicy {\n+\n+  /** Whether rollover should be initiated at this moment */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean\n+\n+  /** Notify that rollover has occurred */\n+  def rolledOver()\n+\n+  /** Notify that bytes have been written */\n+  def bytesWritten(bytes: Long)\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] by which files will be rolled\n+ * over at a fixed interval.\n+ */\n+private[spark] class TimeBasedRollingPolicy(\n+    var rolloverIntervalMillis: Long,\n+    checkIntervalConstraint: Boolean = true   // set to false while testing\n+  ) extends RollingPolicy with Logging {\n+\n+  import TimeBasedRollingPolicy._\n+  if (checkIntervalConstraint && rolloverIntervalMillis < MINIMUM_INTERVAL_SECONDS * 1000L) {\n+    logWarning(s\"Rolling interval [${rolloverIntervalMillis/1000L} seconds] is too small. \" +\n+      s\"Setting the interval to the acceptable minimum of $MINIMUM_INTERVAL_SECONDS seconds.\")\n+    rolloverIntervalMillis = MINIMUM_INTERVAL_SECONDS * 1000L\n+  }\n+\n+  @volatile private var nextRolloverTime = calculateNextRolloverTime()\n+\n+  /** Should rollover if current time has exceeded next rollover time */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean = {\n+    System.currentTimeMillis > nextRolloverTime\n+  }\n+\n+  /** Rollover has occurred, so find the next time to rollover */\n+  def rolledOver() {\n+    nextRolloverTime = calculateNextRolloverTime()\n+    logDebug(s\"Current time: ${System.currentTimeMillis}, next rollover time: \" + nextRolloverTime)\n+  }\n+\n+  def bytesWritten(bytes: Long) { }  // nothing to do\n+\n+  private def calculateNextRolloverTime(): Long = {\n+    val now = System.currentTimeMillis()\n+    val targetTime = (\n+      math.ceil(now.toDouble / rolloverIntervalMillis) * rolloverIntervalMillis\n+    ).toLong\n+    logDebug(s\"Next rollover time is $targetTime\")\n+    targetTime\n+  }\n+}\n+\n+private[spark] object TimeBasedRollingPolicy {\n+  val MINIMUM_INTERVAL_SECONDS = 60L  // 1 minute\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] by which files will be rolled\n+ * over after reaching a particular size.\n+ */\n+private[spark] class SizeBasedRollingPolicy(\n+    var rolloverSizeBytes: Long,\n+    checkSizeConstraint: Boolean = true     // set to false while testing\n+  ) extends RollingPolicy with Logging {\n+\n+  import SizeBasedRollingPolicy._\n+  if (checkSizeConstraint && rolloverSizeBytes < MINIMUM_SIZE_BYTES) {\n+    logWarning(s\"Rolling size [$rolloverSizeBytes bytes] is too small. \" +\n+      s\"Setting the size to the acceptable minimum of $MINIMUM_SIZE_BYTES bytes.\")\n+    rolloverSizeBytes = MINIMUM_SIZE_BYTES * 1000L\n+  }\n+\n+  @volatile private var bytesWrittenTillNow = 0L\n+\n+  /** Should rollover if the next set of bytes is going to exceed the size limit */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean = {\n+    bytesToBeWritten + bytesWrittenTillNow > rolloverSizeBytes\n+  }\n+\n+  /** Rollover has occurred, so reset the counter */\n+  def rolledOver() {\n+    bytesWrittenTillNow = 0\n+  }\n+\n+  /** Increment the bytes that have been written in the current file */\n+  def bytesWritten(bytes: Long) {\n+    bytesWrittenTillNow += bytes\n+  }\n+}\n+\n+private[spark] object SizeBasedRollingPolicy {\n+  val MINIMUM_SIZE_BYTES = RollingFileAppender.DEFAULT_BUFFER_SIZE * 10\n+}\n+\n+\n+/**\n+ * Continuously appends data from input stream into the given file, and rolls\n+ * over the file after the given interval. The rolled over files are named\n+ * based on the given pattern.\n+ *\n+ * @param inputStream             Input stream to read data from\n+ * @param activeFile              File to write data to\n+ * @param rollingPolicy           Policy based on which files will be rolled over.\n+ * @param rollingFilePattern      Pattern based on which the rolled over files will be named.\n+ *                                Uses SimpleDataFormat pattern.\n+ * @param conf                    SparkConf that is used to pass on extra configurations\n+ * @param bufferSize              Optional buffer size. Used mainly for testing.\n+ */\n+private[spark] class RollingFileAppender(\n+    inputStream: InputStream,\n+    activeFile: File,\n+    val rollingPolicy: RollingPolicy,\n+    rollingFilePattern: String,\n+    conf: SparkConf,\n+    bufferSize: Int = DEFAULT_BUFFER_SIZE\n+  ) extends FileAppender(inputStream, activeFile, bufferSize) {\n+\n+  private val retainCount = conf.getInt(KEEP_LAST_PROPERTY, -1)\n+  private val formatter = new SimpleDateFormat(rollingFilePattern)\n+\n+  private val executor = Executors.newFixedThreadPool(1, new ThreadFactory{\n+    def newThread(r: Runnable): Thread = {\n+      val t = new Thread(r)\n+      t.setDaemon(true)\n+      t.setName(s\"Threadpool of ${RollingFileAppender.this.getClass.getSimpleName} for $activeFile\")\n+      t\n+    }\n+  })\n+\n+  /** Stop the appender */\n+  override def stop() {\n+    super.stop()\n+    executor.shutdownNow()\n+  }\n+\n+  /** Append bytes to file after rolling over is necessary */\n+  override protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (rollingPolicy.shouldRollover(len)) {\n+      rollover()\n+      rollingPolicy.rolledOver()\n+    }\n+    super.appendToFile(bytes, len)\n+    rollingPolicy.bytesWritten(len)\n+  }\n+\n+  /** Rollover the file, by closing the output stream and moving it over */\n+  private def rollover() {\n+    val rolloverSuffix = formatter.format(Calendar.getInstance.getTime)\n+    val rolloverFile = new File(\n+      activeFile.getParentFile, activeFile.getName + rolloverSuffix).getAbsoluteFile\n+    logDebug(\"Attempting to rollover at \" + System.currentTimeMillis + \" to file \" + rolloverFile)\n+\n+    try {\n+      closeFile()\n+      if (activeFile.exists) {\n+        if (!rolloverFile.exists) {\n+          FileUtils.moveFile(activeFile, rolloverFile)\n+          logInfo(s\"Rolled over $activeFile to $rolloverFile\")\n+        } else {\n+          // In case the rollover file name clashes, make a unique file name."
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "This was to guard against those remote corner cases where multiple rollovers may occur within the same millisecond (rolling over by size, can occur multiple times within 1 ms, if rollover size is low). This may not practically occur, but an extra \"--0\" suffix safe guards against that while keep the names semantically useful (time of log in the name of the log is useful).\n\nWe could use nanoseconds... and then the format can be either\n`stdout-NNNNNNNNNNNN` (not semantically useful name) or \n`stdout-YYYY-MM-dd--HH-mm-ss--NNNNNNNNNNNNNNNN` (semantically useful, but ugly in my opinion).\n",
    "commit": "fd8f87fde302824813222e1a81003dea32010da7",
    "createdAt": "2014-06-09T21:11:07Z",
    "diffHunk": "@@ -0,0 +1,410 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util\n+\n+import java.io.{File, FileFilter, FileOutputStream, InputStream}\n+import java.text.SimpleDateFormat\n+import java.util.Calendar\n+import java.util.concurrent.{Executors, ThreadFactory}\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.spark.{Logging, SparkConf}\n+import RollingFileAppender._\n+\n+/**\n+ * Continuously appends the data from an input stream into the given file.\n+ */\n+private[spark] class FileAppender(inputStream: InputStream, file: File, bufferSize: Int = 8192)\n+  extends Logging {\n+  @volatile private var outputStream: FileOutputStream = null\n+  @volatile private var markedForStop = false     // has the appender been asked to stopped\n+  @volatile private var stopped = false           // has the appender stopped\n+\n+  // Thread that reads the input stream and writes to file\n+  private val writingThread = new Thread(\"File appending thread for \" + file) {\n+    setDaemon(true)\n+    override def run() {\n+      Utils.logUncaughtExceptions {\n+        appendStreamToFile()\n+      }\n+    }\n+  }\n+  writingThread.start()\n+\n+  /**\n+   * Wait for the appender to stop appending, either because input stream is closed\n+   * or because of any error in appending\n+   */\n+  def awaitTermination() {\n+    synchronized {\n+      if (!stopped) {\n+        wait()\n+      }\n+    }\n+  }\n+\n+  /** Stop the appender */\n+  def stop() {\n+    markedForStop = true\n+  }\n+\n+  /** Continuously read chunks from the input stream and append to the file */\n+  protected def appendStreamToFile() {\n+    try {\n+      logDebug(\"Started appending thread\")\n+      openFile()\n+      val buf = new Array[Byte](bufferSize)\n+      var n = 0\n+      while (!markedForStop && n != -1) {\n+        n = inputStream.read(buf)\n+        if (n != -1) {\n+          appendToFile(buf, n)\n+        }\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Error writing stream to file $file\", e)\n+    } finally {\n+      closeFile()\n+      synchronized {\n+        stopped = true\n+        notifyAll()\n+      }\n+    }\n+  }\n+\n+  /** Append bytes to the file output stream */\n+  protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (outputStream == null) {\n+      openFile()\n+    }\n+    outputStream.write(bytes, 0, len)\n+  }\n+\n+  /** Open the file output stream */\n+  protected def openFile() {\n+    outputStream = new FileOutputStream(file, true)\n+    logDebug(s\"Opened file $file\")\n+  }\n+\n+  /** Close the file output stream */\n+  protected def closeFile() {\n+    outputStream.flush()\n+    outputStream.close()\n+    logDebug(s\"Closed file $file\")\n+  }\n+}\n+\n+/**\n+ * Companion object to [[org.apache.log4j.FileAppender]] which has helper\n+ * functions to choose the write type of FileAppender based on SparkConf configuration.\n+ */\n+private[spark] object FileAppender extends Logging {\n+\n+  /** Create the right appender based on Spark configuration */\n+  def apply(inputStream: InputStream, file: File, conf: SparkConf): FileAppender = {\n+\n+    import RollingFileAppender._\n+\n+    val rolloverSizeOption = conf.getOption(SIZE_PROPERTY)\n+    val rolloverIntervalOption = conf.getOption(INTERVAL_PROPERTY)\n+\n+    (rolloverIntervalOption, rolloverSizeOption) match {\n+\n+      case (Some(rolloverInterval), Some(rolloverSize)) =>\n+        // if both size and interval have been set\n+        logWarning(s\"Rollover interval [$rolloverInterval] and size [$rolloverSize] \" +\n+          s\"both set for executor logs, rolling logs not enabled\")\n+        new FileAppender(inputStream, file)\n+\n+      case (Some(rolloverInterval), None) =>  // if interval has been set\n+        val validatedParams: Option[(Long, String)] = rolloverInterval match {\n+          case \"daily\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with daily rolling\")\n+            Some(24 * 60 * 60 * 1000L, s\"--YYYY-MM-dd\")\n+          case \"hourly\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with hourly rolling\")\n+            Some(60 * 60 * 1000L, s\"--YYYY-MM-dd--HH\")\n+          case \"minutely\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling every minute\")\n+            Some(60 * 1000L, s\"--YYYY-MM-dd--HH-mm\")\n+          case IntParam(seconds) =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling $seconds seconds\")\n+            Some(seconds * 1000L, s\"--YYYY-MM-dd--HH-mm-ss\")\n+          case _ =>\n+            logWarning(s\"Illegal interval for rolling executor logs [$rolloverInterval], \" +\n+                s\"rolling logs not enabled\")\n+            None\n+        }\n+        validatedParams.map {\n+          case (interval, pattern) =>\n+            new RollingFileAppender(\n+              inputStream, file, new TimeBasedRollingPolicy(interval),pattern, conf)\n+        }.getOrElse {\n+          new FileAppender(inputStream, file)\n+        }\n+\n+      case (None, Some(rolloverSize)) =>    // if size has been set\n+        rolloverSize match {\n+          case IntParam(bytes) =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling every $bytes bytes\")\n+            new RollingFileAppender(inputStream, file, new SizeBasedRollingPolicy(bytes),\n+              s\"--YYYY-MM-dd--HH-mm-ss-SSSS\", conf)\n+          case _ =>\n+            logWarning(\n+              s\"Illegal size for rolling executor logs [$rolloverSize], \" +\n+                s\"rolling logs not enabled\")\n+            new FileAppender(inputStream, file)\n+        }\n+\n+      case (None, None) =>                // if neither size nor interval has been set\n+        new FileAppender(inputStream, file)\n+    }\n+  }\n+}\n+\n+/**\n+ * Defines the policy based on which [[org.apache.spark.util.RollingFileAppender]] will\n+ * generate rolling files.\n+ */\n+private[spark] trait RollingPolicy {\n+\n+  /** Whether rollover should be initiated at this moment */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean\n+\n+  /** Notify that rollover has occurred */\n+  def rolledOver()\n+\n+  /** Notify that bytes have been written */\n+  def bytesWritten(bytes: Long)\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] by which files will be rolled\n+ * over at a fixed interval.\n+ */\n+private[spark] class TimeBasedRollingPolicy(\n+    var rolloverIntervalMillis: Long,\n+    checkIntervalConstraint: Boolean = true   // set to false while testing\n+  ) extends RollingPolicy with Logging {\n+\n+  import TimeBasedRollingPolicy._\n+  if (checkIntervalConstraint && rolloverIntervalMillis < MINIMUM_INTERVAL_SECONDS * 1000L) {\n+    logWarning(s\"Rolling interval [${rolloverIntervalMillis/1000L} seconds] is too small. \" +\n+      s\"Setting the interval to the acceptable minimum of $MINIMUM_INTERVAL_SECONDS seconds.\")\n+    rolloverIntervalMillis = MINIMUM_INTERVAL_SECONDS * 1000L\n+  }\n+\n+  @volatile private var nextRolloverTime = calculateNextRolloverTime()\n+\n+  /** Should rollover if current time has exceeded next rollover time */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean = {\n+    System.currentTimeMillis > nextRolloverTime\n+  }\n+\n+  /** Rollover has occurred, so find the next time to rollover */\n+  def rolledOver() {\n+    nextRolloverTime = calculateNextRolloverTime()\n+    logDebug(s\"Current time: ${System.currentTimeMillis}, next rollover time: \" + nextRolloverTime)\n+  }\n+\n+  def bytesWritten(bytes: Long) { }  // nothing to do\n+\n+  private def calculateNextRolloverTime(): Long = {\n+    val now = System.currentTimeMillis()\n+    val targetTime = (\n+      math.ceil(now.toDouble / rolloverIntervalMillis) * rolloverIntervalMillis\n+    ).toLong\n+    logDebug(s\"Next rollover time is $targetTime\")\n+    targetTime\n+  }\n+}\n+\n+private[spark] object TimeBasedRollingPolicy {\n+  val MINIMUM_INTERVAL_SECONDS = 60L  // 1 minute\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] by which files will be rolled\n+ * over after reaching a particular size.\n+ */\n+private[spark] class SizeBasedRollingPolicy(\n+    var rolloverSizeBytes: Long,\n+    checkSizeConstraint: Boolean = true     // set to false while testing\n+  ) extends RollingPolicy with Logging {\n+\n+  import SizeBasedRollingPolicy._\n+  if (checkSizeConstraint && rolloverSizeBytes < MINIMUM_SIZE_BYTES) {\n+    logWarning(s\"Rolling size [$rolloverSizeBytes bytes] is too small. \" +\n+      s\"Setting the size to the acceptable minimum of $MINIMUM_SIZE_BYTES bytes.\")\n+    rolloverSizeBytes = MINIMUM_SIZE_BYTES * 1000L\n+  }\n+\n+  @volatile private var bytesWrittenTillNow = 0L\n+\n+  /** Should rollover if the next set of bytes is going to exceed the size limit */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean = {\n+    bytesToBeWritten + bytesWrittenTillNow > rolloverSizeBytes\n+  }\n+\n+  /** Rollover has occurred, so reset the counter */\n+  def rolledOver() {\n+    bytesWrittenTillNow = 0\n+  }\n+\n+  /** Increment the bytes that have been written in the current file */\n+  def bytesWritten(bytes: Long) {\n+    bytesWrittenTillNow += bytes\n+  }\n+}\n+\n+private[spark] object SizeBasedRollingPolicy {\n+  val MINIMUM_SIZE_BYTES = RollingFileAppender.DEFAULT_BUFFER_SIZE * 10\n+}\n+\n+\n+/**\n+ * Continuously appends data from input stream into the given file, and rolls\n+ * over the file after the given interval. The rolled over files are named\n+ * based on the given pattern.\n+ *\n+ * @param inputStream             Input stream to read data from\n+ * @param activeFile              File to write data to\n+ * @param rollingPolicy           Policy based on which files will be rolled over.\n+ * @param rollingFilePattern      Pattern based on which the rolled over files will be named.\n+ *                                Uses SimpleDataFormat pattern.\n+ * @param conf                    SparkConf that is used to pass on extra configurations\n+ * @param bufferSize              Optional buffer size. Used mainly for testing.\n+ */\n+private[spark] class RollingFileAppender(\n+    inputStream: InputStream,\n+    activeFile: File,\n+    val rollingPolicy: RollingPolicy,\n+    rollingFilePattern: String,\n+    conf: SparkConf,\n+    bufferSize: Int = DEFAULT_BUFFER_SIZE\n+  ) extends FileAppender(inputStream, activeFile, bufferSize) {\n+\n+  private val retainCount = conf.getInt(KEEP_LAST_PROPERTY, -1)\n+  private val formatter = new SimpleDateFormat(rollingFilePattern)\n+\n+  private val executor = Executors.newFixedThreadPool(1, new ThreadFactory{\n+    def newThread(r: Runnable): Thread = {\n+      val t = new Thread(r)\n+      t.setDaemon(true)\n+      t.setName(s\"Threadpool of ${RollingFileAppender.this.getClass.getSimpleName} for $activeFile\")\n+      t\n+    }\n+  })\n+\n+  /** Stop the appender */\n+  override def stop() {\n+    super.stop()\n+    executor.shutdownNow()\n+  }\n+\n+  /** Append bytes to file after rolling over is necessary */\n+  override protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (rollingPolicy.shouldRollover(len)) {\n+      rollover()\n+      rollingPolicy.rolledOver()\n+    }\n+    super.appendToFile(bytes, len)\n+    rollingPolicy.bytesWritten(len)\n+  }\n+\n+  /** Rollover the file, by closing the output stream and moving it over */\n+  private def rollover() {\n+    val rolloverSuffix = formatter.format(Calendar.getInstance.getTime)\n+    val rolloverFile = new File(\n+      activeFile.getParentFile, activeFile.getName + rolloverSuffix).getAbsoluteFile\n+    logDebug(\"Attempting to rollover at \" + System.currentTimeMillis + \" to file \" + rolloverFile)\n+\n+    try {\n+      closeFile()\n+      if (activeFile.exists) {\n+        if (!rolloverFile.exists) {\n+          FileUtils.moveFile(activeFile, rolloverFile)\n+          logInfo(s\"Rolled over $activeFile to $rolloverFile\")\n+        } else {\n+          // In case the rollover file name clashes, make a unique file name."
  }],
  "prId": 895
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "Why does this clean-up need to happen in it's own thread pool? It doesn't seem more expensive than any of the other operations that happen inside of this main thread (i.e. it has to list the directory and potentially do a small number of file-system operations).\n\nIf this were moved into the main thread could you get rid of the `executor` entirely?\n",
    "commit": "fd8f87fde302824813222e1a81003dea32010da7",
    "createdAt": "2014-06-05T22:38:41Z",
    "diffHunk": "@@ -0,0 +1,410 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util\n+\n+import java.io.{File, FileFilter, FileOutputStream, InputStream}\n+import java.text.SimpleDateFormat\n+import java.util.Calendar\n+import java.util.concurrent.{Executors, ThreadFactory}\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.spark.{Logging, SparkConf}\n+import RollingFileAppender._\n+\n+/**\n+ * Continuously appends the data from an input stream into the given file.\n+ */\n+private[spark] class FileAppender(inputStream: InputStream, file: File, bufferSize: Int = 8192)\n+  extends Logging {\n+  @volatile private var outputStream: FileOutputStream = null\n+  @volatile private var markedForStop = false     // has the appender been asked to stopped\n+  @volatile private var stopped = false           // has the appender stopped\n+\n+  // Thread that reads the input stream and writes to file\n+  private val writingThread = new Thread(\"File appending thread for \" + file) {\n+    setDaemon(true)\n+    override def run() {\n+      Utils.logUncaughtExceptions {\n+        appendStreamToFile()\n+      }\n+    }\n+  }\n+  writingThread.start()\n+\n+  /**\n+   * Wait for the appender to stop appending, either because input stream is closed\n+   * or because of any error in appending\n+   */\n+  def awaitTermination() {\n+    synchronized {\n+      if (!stopped) {\n+        wait()\n+      }\n+    }\n+  }\n+\n+  /** Stop the appender */\n+  def stop() {\n+    markedForStop = true\n+  }\n+\n+  /** Continuously read chunks from the input stream and append to the file */\n+  protected def appendStreamToFile() {\n+    try {\n+      logDebug(\"Started appending thread\")\n+      openFile()\n+      val buf = new Array[Byte](bufferSize)\n+      var n = 0\n+      while (!markedForStop && n != -1) {\n+        n = inputStream.read(buf)\n+        if (n != -1) {\n+          appendToFile(buf, n)\n+        }\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Error writing stream to file $file\", e)\n+    } finally {\n+      closeFile()\n+      synchronized {\n+        stopped = true\n+        notifyAll()\n+      }\n+    }\n+  }\n+\n+  /** Append bytes to the file output stream */\n+  protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (outputStream == null) {\n+      openFile()\n+    }\n+    outputStream.write(bytes, 0, len)\n+  }\n+\n+  /** Open the file output stream */\n+  protected def openFile() {\n+    outputStream = new FileOutputStream(file, true)\n+    logDebug(s\"Opened file $file\")\n+  }\n+\n+  /** Close the file output stream */\n+  protected def closeFile() {\n+    outputStream.flush()\n+    outputStream.close()\n+    logDebug(s\"Closed file $file\")\n+  }\n+}\n+\n+/**\n+ * Companion object to [[org.apache.log4j.FileAppender]] which has helper\n+ * functions to choose the write type of FileAppender based on SparkConf configuration.\n+ */\n+private[spark] object FileAppender extends Logging {\n+\n+  /** Create the right appender based on Spark configuration */\n+  def apply(inputStream: InputStream, file: File, conf: SparkConf): FileAppender = {\n+\n+    import RollingFileAppender._\n+\n+    val rolloverSizeOption = conf.getOption(SIZE_PROPERTY)\n+    val rolloverIntervalOption = conf.getOption(INTERVAL_PROPERTY)\n+\n+    (rolloverIntervalOption, rolloverSizeOption) match {\n+\n+      case (Some(rolloverInterval), Some(rolloverSize)) =>\n+        // if both size and interval have been set\n+        logWarning(s\"Rollover interval [$rolloverInterval] and size [$rolloverSize] \" +\n+          s\"both set for executor logs, rolling logs not enabled\")\n+        new FileAppender(inputStream, file)\n+\n+      case (Some(rolloverInterval), None) =>  // if interval has been set\n+        val validatedParams: Option[(Long, String)] = rolloverInterval match {\n+          case \"daily\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with daily rolling\")\n+            Some(24 * 60 * 60 * 1000L, s\"--YYYY-MM-dd\")\n+          case \"hourly\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with hourly rolling\")\n+            Some(60 * 60 * 1000L, s\"--YYYY-MM-dd--HH\")\n+          case \"minutely\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling every minute\")\n+            Some(60 * 1000L, s\"--YYYY-MM-dd--HH-mm\")\n+          case IntParam(seconds) =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling $seconds seconds\")\n+            Some(seconds * 1000L, s\"--YYYY-MM-dd--HH-mm-ss\")\n+          case _ =>\n+            logWarning(s\"Illegal interval for rolling executor logs [$rolloverInterval], \" +\n+                s\"rolling logs not enabled\")\n+            None\n+        }\n+        validatedParams.map {\n+          case (interval, pattern) =>\n+            new RollingFileAppender(\n+              inputStream, file, new TimeBasedRollingPolicy(interval),pattern, conf)\n+        }.getOrElse {\n+          new FileAppender(inputStream, file)\n+        }\n+\n+      case (None, Some(rolloverSize)) =>    // if size has been set\n+        rolloverSize match {\n+          case IntParam(bytes) =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling every $bytes bytes\")\n+            new RollingFileAppender(inputStream, file, new SizeBasedRollingPolicy(bytes),\n+              s\"--YYYY-MM-dd--HH-mm-ss-SSSS\", conf)\n+          case _ =>\n+            logWarning(\n+              s\"Illegal size for rolling executor logs [$rolloverSize], \" +\n+                s\"rolling logs not enabled\")\n+            new FileAppender(inputStream, file)\n+        }\n+\n+      case (None, None) =>                // if neither size nor interval has been set\n+        new FileAppender(inputStream, file)\n+    }\n+  }\n+}\n+\n+/**\n+ * Defines the policy based on which [[org.apache.spark.util.RollingFileAppender]] will\n+ * generate rolling files.\n+ */\n+private[spark] trait RollingPolicy {\n+\n+  /** Whether rollover should be initiated at this moment */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean\n+\n+  /** Notify that rollover has occurred */\n+  def rolledOver()\n+\n+  /** Notify that bytes have been written */\n+  def bytesWritten(bytes: Long)\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] by which files will be rolled\n+ * over at a fixed interval.\n+ */\n+private[spark] class TimeBasedRollingPolicy(\n+    var rolloverIntervalMillis: Long,\n+    checkIntervalConstraint: Boolean = true   // set to false while testing\n+  ) extends RollingPolicy with Logging {\n+\n+  import TimeBasedRollingPolicy._\n+  if (checkIntervalConstraint && rolloverIntervalMillis < MINIMUM_INTERVAL_SECONDS * 1000L) {\n+    logWarning(s\"Rolling interval [${rolloverIntervalMillis/1000L} seconds] is too small. \" +\n+      s\"Setting the interval to the acceptable minimum of $MINIMUM_INTERVAL_SECONDS seconds.\")\n+    rolloverIntervalMillis = MINIMUM_INTERVAL_SECONDS * 1000L\n+  }\n+\n+  @volatile private var nextRolloverTime = calculateNextRolloverTime()\n+\n+  /** Should rollover if current time has exceeded next rollover time */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean = {\n+    System.currentTimeMillis > nextRolloverTime\n+  }\n+\n+  /** Rollover has occurred, so find the next time to rollover */\n+  def rolledOver() {\n+    nextRolloverTime = calculateNextRolloverTime()\n+    logDebug(s\"Current time: ${System.currentTimeMillis}, next rollover time: \" + nextRolloverTime)\n+  }\n+\n+  def bytesWritten(bytes: Long) { }  // nothing to do\n+\n+  private def calculateNextRolloverTime(): Long = {\n+    val now = System.currentTimeMillis()\n+    val targetTime = (\n+      math.ceil(now.toDouble / rolloverIntervalMillis) * rolloverIntervalMillis\n+    ).toLong\n+    logDebug(s\"Next rollover time is $targetTime\")\n+    targetTime\n+  }\n+}\n+\n+private[spark] object TimeBasedRollingPolicy {\n+  val MINIMUM_INTERVAL_SECONDS = 60L  // 1 minute\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] by which files will be rolled\n+ * over after reaching a particular size.\n+ */\n+private[spark] class SizeBasedRollingPolicy(\n+    var rolloverSizeBytes: Long,\n+    checkSizeConstraint: Boolean = true     // set to false while testing\n+  ) extends RollingPolicy with Logging {\n+\n+  import SizeBasedRollingPolicy._\n+  if (checkSizeConstraint && rolloverSizeBytes < MINIMUM_SIZE_BYTES) {\n+    logWarning(s\"Rolling size [$rolloverSizeBytes bytes] is too small. \" +\n+      s\"Setting the size to the acceptable minimum of $MINIMUM_SIZE_BYTES bytes.\")\n+    rolloverSizeBytes = MINIMUM_SIZE_BYTES * 1000L\n+  }\n+\n+  @volatile private var bytesWrittenTillNow = 0L\n+\n+  /** Should rollover if the next set of bytes is going to exceed the size limit */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean = {\n+    bytesToBeWritten + bytesWrittenTillNow > rolloverSizeBytes\n+  }\n+\n+  /** Rollover has occurred, so reset the counter */\n+  def rolledOver() {\n+    bytesWrittenTillNow = 0\n+  }\n+\n+  /** Increment the bytes that have been written in the current file */\n+  def bytesWritten(bytes: Long) {\n+    bytesWrittenTillNow += bytes\n+  }\n+}\n+\n+private[spark] object SizeBasedRollingPolicy {\n+  val MINIMUM_SIZE_BYTES = RollingFileAppender.DEFAULT_BUFFER_SIZE * 10\n+}\n+\n+\n+/**\n+ * Continuously appends data from input stream into the given file, and rolls\n+ * over the file after the given interval. The rolled over files are named\n+ * based on the given pattern.\n+ *\n+ * @param inputStream             Input stream to read data from\n+ * @param activeFile              File to write data to\n+ * @param rollingPolicy           Policy based on which files will be rolled over.\n+ * @param rollingFilePattern      Pattern based on which the rolled over files will be named.\n+ *                                Uses SimpleDataFormat pattern.\n+ * @param conf                    SparkConf that is used to pass on extra configurations\n+ * @param bufferSize              Optional buffer size. Used mainly for testing.\n+ */\n+private[spark] class RollingFileAppender(\n+    inputStream: InputStream,\n+    activeFile: File,\n+    val rollingPolicy: RollingPolicy,\n+    rollingFilePattern: String,\n+    conf: SparkConf,\n+    bufferSize: Int = DEFAULT_BUFFER_SIZE\n+  ) extends FileAppender(inputStream, activeFile, bufferSize) {\n+\n+  private val retainCount = conf.getInt(KEEP_LAST_PROPERTY, -1)\n+  private val formatter = new SimpleDateFormat(rollingFilePattern)\n+\n+  private val executor = Executors.newFixedThreadPool(1, new ThreadFactory{\n+    def newThread(r: Runnable): Thread = {\n+      val t = new Thread(r)\n+      t.setDaemon(true)\n+      t.setName(s\"Threadpool of ${RollingFileAppender.this.getClass.getSimpleName} for $activeFile\")\n+      t\n+    }\n+  })\n+\n+  /** Stop the appender */\n+  override def stop() {\n+    super.stop()\n+    executor.shutdownNow()\n+  }\n+\n+  /** Append bytes to file after rolling over is necessary */\n+  override protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (rollingPolicy.shouldRollover(len)) {\n+      rollover()\n+      rollingPolicy.rolledOver()\n+    }\n+    super.appendToFile(bytes, len)\n+    rollingPolicy.bytesWritten(len)\n+  }\n+\n+  /** Rollover the file, by closing the output stream and moving it over */\n+  private def rollover() {\n+    val rolloverSuffix = formatter.format(Calendar.getInstance.getTime)\n+    val rolloverFile = new File(\n+      activeFile.getParentFile, activeFile.getName + rolloverSuffix).getAbsoluteFile\n+    logDebug(\"Attempting to rollover at \" + System.currentTimeMillis + \" to file \" + rolloverFile)\n+\n+    try {\n+      closeFile()\n+      if (activeFile.exists) {\n+        if (!rolloverFile.exists) {\n+          FileUtils.moveFile(activeFile, rolloverFile)\n+          logInfo(s\"Rolled over $activeFile to $rolloverFile\")\n+        } else {\n+          // In case the rollover file name clashes, make a unique file name.\n+          // The resultant file names are long and ugly, so this is used only\n+          // if there is a name collision. This can be avoided by the using\n+          // the right pattern such that name collisions do not occur.\n+          var i = 0\n+          var altRolloverFile: File = null\n+          do {\n+            altRolloverFile = new File(activeFile.getParent,\n+              s\"${activeFile.getName}$rolloverSuffix--$i\").getAbsoluteFile\n+            i += 1\n+          } while (i < 10000 && altRolloverFile.exists)\n+\n+          logWarning(s\"Rollover file $rolloverFile already exists, \" +\n+            s\"rolled over $activeFile to file $altRolloverFile\")\n+          logWarning(s\"Make sure that the given file name pattern [$rollingFilePattern]\")\n+          FileUtils.moveFile(activeFile, altRolloverFile)\n+        }\n+      } else {\n+        logWarning(s\"File $activeFile does not exist\")\n+      }\n+      openFile()\n+      scheduleCleanup()\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Error rolling over $activeFile to $rolloverFile\", e)\n+    }\n+  }\n+\n+  /** Schedule cleaning up for older rolled over files */\n+  private def scheduleCleanup() {\n+    if (retainCount > 0) {\n+      executor.submit(new Runnable() { override def run() { cleanup() }})"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "I was divided on that. I had it that way initially. I wanted to minimize the number of operations that i have to do because i am doing the rolling (closing file, renaming file, opening new file) on the data path (pausing the pulling from the stream and doing the rolling). If i delay more than necessary, I am not sure what the effect of pausing that stream-pulling would be. I can change it very easily if you think that's okay.\n",
    "commit": "fd8f87fde302824813222e1a81003dea32010da7",
    "createdAt": "2014-06-07T01:44:38Z",
    "diffHunk": "@@ -0,0 +1,410 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util\n+\n+import java.io.{File, FileFilter, FileOutputStream, InputStream}\n+import java.text.SimpleDateFormat\n+import java.util.Calendar\n+import java.util.concurrent.{Executors, ThreadFactory}\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.spark.{Logging, SparkConf}\n+import RollingFileAppender._\n+\n+/**\n+ * Continuously appends the data from an input stream into the given file.\n+ */\n+private[spark] class FileAppender(inputStream: InputStream, file: File, bufferSize: Int = 8192)\n+  extends Logging {\n+  @volatile private var outputStream: FileOutputStream = null\n+  @volatile private var markedForStop = false     // has the appender been asked to stopped\n+  @volatile private var stopped = false           // has the appender stopped\n+\n+  // Thread that reads the input stream and writes to file\n+  private val writingThread = new Thread(\"File appending thread for \" + file) {\n+    setDaemon(true)\n+    override def run() {\n+      Utils.logUncaughtExceptions {\n+        appendStreamToFile()\n+      }\n+    }\n+  }\n+  writingThread.start()\n+\n+  /**\n+   * Wait for the appender to stop appending, either because input stream is closed\n+   * or because of any error in appending\n+   */\n+  def awaitTermination() {\n+    synchronized {\n+      if (!stopped) {\n+        wait()\n+      }\n+    }\n+  }\n+\n+  /** Stop the appender */\n+  def stop() {\n+    markedForStop = true\n+  }\n+\n+  /** Continuously read chunks from the input stream and append to the file */\n+  protected def appendStreamToFile() {\n+    try {\n+      logDebug(\"Started appending thread\")\n+      openFile()\n+      val buf = new Array[Byte](bufferSize)\n+      var n = 0\n+      while (!markedForStop && n != -1) {\n+        n = inputStream.read(buf)\n+        if (n != -1) {\n+          appendToFile(buf, n)\n+        }\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Error writing stream to file $file\", e)\n+    } finally {\n+      closeFile()\n+      synchronized {\n+        stopped = true\n+        notifyAll()\n+      }\n+    }\n+  }\n+\n+  /** Append bytes to the file output stream */\n+  protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (outputStream == null) {\n+      openFile()\n+    }\n+    outputStream.write(bytes, 0, len)\n+  }\n+\n+  /** Open the file output stream */\n+  protected def openFile() {\n+    outputStream = new FileOutputStream(file, true)\n+    logDebug(s\"Opened file $file\")\n+  }\n+\n+  /** Close the file output stream */\n+  protected def closeFile() {\n+    outputStream.flush()\n+    outputStream.close()\n+    logDebug(s\"Closed file $file\")\n+  }\n+}\n+\n+/**\n+ * Companion object to [[org.apache.log4j.FileAppender]] which has helper\n+ * functions to choose the write type of FileAppender based on SparkConf configuration.\n+ */\n+private[spark] object FileAppender extends Logging {\n+\n+  /** Create the right appender based on Spark configuration */\n+  def apply(inputStream: InputStream, file: File, conf: SparkConf): FileAppender = {\n+\n+    import RollingFileAppender._\n+\n+    val rolloverSizeOption = conf.getOption(SIZE_PROPERTY)\n+    val rolloverIntervalOption = conf.getOption(INTERVAL_PROPERTY)\n+\n+    (rolloverIntervalOption, rolloverSizeOption) match {\n+\n+      case (Some(rolloverInterval), Some(rolloverSize)) =>\n+        // if both size and interval have been set\n+        logWarning(s\"Rollover interval [$rolloverInterval] and size [$rolloverSize] \" +\n+          s\"both set for executor logs, rolling logs not enabled\")\n+        new FileAppender(inputStream, file)\n+\n+      case (Some(rolloverInterval), None) =>  // if interval has been set\n+        val validatedParams: Option[(Long, String)] = rolloverInterval match {\n+          case \"daily\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with daily rolling\")\n+            Some(24 * 60 * 60 * 1000L, s\"--YYYY-MM-dd\")\n+          case \"hourly\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with hourly rolling\")\n+            Some(60 * 60 * 1000L, s\"--YYYY-MM-dd--HH\")\n+          case \"minutely\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling every minute\")\n+            Some(60 * 1000L, s\"--YYYY-MM-dd--HH-mm\")\n+          case IntParam(seconds) =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling $seconds seconds\")\n+            Some(seconds * 1000L, s\"--YYYY-MM-dd--HH-mm-ss\")\n+          case _ =>\n+            logWarning(s\"Illegal interval for rolling executor logs [$rolloverInterval], \" +\n+                s\"rolling logs not enabled\")\n+            None\n+        }\n+        validatedParams.map {\n+          case (interval, pattern) =>\n+            new RollingFileAppender(\n+              inputStream, file, new TimeBasedRollingPolicy(interval),pattern, conf)\n+        }.getOrElse {\n+          new FileAppender(inputStream, file)\n+        }\n+\n+      case (None, Some(rolloverSize)) =>    // if size has been set\n+        rolloverSize match {\n+          case IntParam(bytes) =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling every $bytes bytes\")\n+            new RollingFileAppender(inputStream, file, new SizeBasedRollingPolicy(bytes),\n+              s\"--YYYY-MM-dd--HH-mm-ss-SSSS\", conf)\n+          case _ =>\n+            logWarning(\n+              s\"Illegal size for rolling executor logs [$rolloverSize], \" +\n+                s\"rolling logs not enabled\")\n+            new FileAppender(inputStream, file)\n+        }\n+\n+      case (None, None) =>                // if neither size nor interval has been set\n+        new FileAppender(inputStream, file)\n+    }\n+  }\n+}\n+\n+/**\n+ * Defines the policy based on which [[org.apache.spark.util.RollingFileAppender]] will\n+ * generate rolling files.\n+ */\n+private[spark] trait RollingPolicy {\n+\n+  /** Whether rollover should be initiated at this moment */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean\n+\n+  /** Notify that rollover has occurred */\n+  def rolledOver()\n+\n+  /** Notify that bytes have been written */\n+  def bytesWritten(bytes: Long)\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] by which files will be rolled\n+ * over at a fixed interval.\n+ */\n+private[spark] class TimeBasedRollingPolicy(\n+    var rolloverIntervalMillis: Long,\n+    checkIntervalConstraint: Boolean = true   // set to false while testing\n+  ) extends RollingPolicy with Logging {\n+\n+  import TimeBasedRollingPolicy._\n+  if (checkIntervalConstraint && rolloverIntervalMillis < MINIMUM_INTERVAL_SECONDS * 1000L) {\n+    logWarning(s\"Rolling interval [${rolloverIntervalMillis/1000L} seconds] is too small. \" +\n+      s\"Setting the interval to the acceptable minimum of $MINIMUM_INTERVAL_SECONDS seconds.\")\n+    rolloverIntervalMillis = MINIMUM_INTERVAL_SECONDS * 1000L\n+  }\n+\n+  @volatile private var nextRolloverTime = calculateNextRolloverTime()\n+\n+  /** Should rollover if current time has exceeded next rollover time */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean = {\n+    System.currentTimeMillis > nextRolloverTime\n+  }\n+\n+  /** Rollover has occurred, so find the next time to rollover */\n+  def rolledOver() {\n+    nextRolloverTime = calculateNextRolloverTime()\n+    logDebug(s\"Current time: ${System.currentTimeMillis}, next rollover time: \" + nextRolloverTime)\n+  }\n+\n+  def bytesWritten(bytes: Long) { }  // nothing to do\n+\n+  private def calculateNextRolloverTime(): Long = {\n+    val now = System.currentTimeMillis()\n+    val targetTime = (\n+      math.ceil(now.toDouble / rolloverIntervalMillis) * rolloverIntervalMillis\n+    ).toLong\n+    logDebug(s\"Next rollover time is $targetTime\")\n+    targetTime\n+  }\n+}\n+\n+private[spark] object TimeBasedRollingPolicy {\n+  val MINIMUM_INTERVAL_SECONDS = 60L  // 1 minute\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] by which files will be rolled\n+ * over after reaching a particular size.\n+ */\n+private[spark] class SizeBasedRollingPolicy(\n+    var rolloverSizeBytes: Long,\n+    checkSizeConstraint: Boolean = true     // set to false while testing\n+  ) extends RollingPolicy with Logging {\n+\n+  import SizeBasedRollingPolicy._\n+  if (checkSizeConstraint && rolloverSizeBytes < MINIMUM_SIZE_BYTES) {\n+    logWarning(s\"Rolling size [$rolloverSizeBytes bytes] is too small. \" +\n+      s\"Setting the size to the acceptable minimum of $MINIMUM_SIZE_BYTES bytes.\")\n+    rolloverSizeBytes = MINIMUM_SIZE_BYTES * 1000L\n+  }\n+\n+  @volatile private var bytesWrittenTillNow = 0L\n+\n+  /** Should rollover if the next set of bytes is going to exceed the size limit */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean = {\n+    bytesToBeWritten + bytesWrittenTillNow > rolloverSizeBytes\n+  }\n+\n+  /** Rollover has occurred, so reset the counter */\n+  def rolledOver() {\n+    bytesWrittenTillNow = 0\n+  }\n+\n+  /** Increment the bytes that have been written in the current file */\n+  def bytesWritten(bytes: Long) {\n+    bytesWrittenTillNow += bytes\n+  }\n+}\n+\n+private[spark] object SizeBasedRollingPolicy {\n+  val MINIMUM_SIZE_BYTES = RollingFileAppender.DEFAULT_BUFFER_SIZE * 10\n+}\n+\n+\n+/**\n+ * Continuously appends data from input stream into the given file, and rolls\n+ * over the file after the given interval. The rolled over files are named\n+ * based on the given pattern.\n+ *\n+ * @param inputStream             Input stream to read data from\n+ * @param activeFile              File to write data to\n+ * @param rollingPolicy           Policy based on which files will be rolled over.\n+ * @param rollingFilePattern      Pattern based on which the rolled over files will be named.\n+ *                                Uses SimpleDataFormat pattern.\n+ * @param conf                    SparkConf that is used to pass on extra configurations\n+ * @param bufferSize              Optional buffer size. Used mainly for testing.\n+ */\n+private[spark] class RollingFileAppender(\n+    inputStream: InputStream,\n+    activeFile: File,\n+    val rollingPolicy: RollingPolicy,\n+    rollingFilePattern: String,\n+    conf: SparkConf,\n+    bufferSize: Int = DEFAULT_BUFFER_SIZE\n+  ) extends FileAppender(inputStream, activeFile, bufferSize) {\n+\n+  private val retainCount = conf.getInt(KEEP_LAST_PROPERTY, -1)\n+  private val formatter = new SimpleDateFormat(rollingFilePattern)\n+\n+  private val executor = Executors.newFixedThreadPool(1, new ThreadFactory{\n+    def newThread(r: Runnable): Thread = {\n+      val t = new Thread(r)\n+      t.setDaemon(true)\n+      t.setName(s\"Threadpool of ${RollingFileAppender.this.getClass.getSimpleName} for $activeFile\")\n+      t\n+    }\n+  })\n+\n+  /** Stop the appender */\n+  override def stop() {\n+    super.stop()\n+    executor.shutdownNow()\n+  }\n+\n+  /** Append bytes to file after rolling over is necessary */\n+  override protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (rollingPolicy.shouldRollover(len)) {\n+      rollover()\n+      rollingPolicy.rolledOver()\n+    }\n+    super.appendToFile(bytes, len)\n+    rollingPolicy.bytesWritten(len)\n+  }\n+\n+  /** Rollover the file, by closing the output stream and moving it over */\n+  private def rollover() {\n+    val rolloverSuffix = formatter.format(Calendar.getInstance.getTime)\n+    val rolloverFile = new File(\n+      activeFile.getParentFile, activeFile.getName + rolloverSuffix).getAbsoluteFile\n+    logDebug(\"Attempting to rollover at \" + System.currentTimeMillis + \" to file \" + rolloverFile)\n+\n+    try {\n+      closeFile()\n+      if (activeFile.exists) {\n+        if (!rolloverFile.exists) {\n+          FileUtils.moveFile(activeFile, rolloverFile)\n+          logInfo(s\"Rolled over $activeFile to $rolloverFile\")\n+        } else {\n+          // In case the rollover file name clashes, make a unique file name.\n+          // The resultant file names are long and ugly, so this is used only\n+          // if there is a name collision. This can be avoided by the using\n+          // the right pattern such that name collisions do not occur.\n+          var i = 0\n+          var altRolloverFile: File = null\n+          do {\n+            altRolloverFile = new File(activeFile.getParent,\n+              s\"${activeFile.getName}$rolloverSuffix--$i\").getAbsoluteFile\n+            i += 1\n+          } while (i < 10000 && altRolloverFile.exists)\n+\n+          logWarning(s\"Rollover file $rolloverFile already exists, \" +\n+            s\"rolled over $activeFile to file $altRolloverFile\")\n+          logWarning(s\"Make sure that the given file name pattern [$rollingFilePattern]\")\n+          FileUtils.moveFile(activeFile, altRolloverFile)\n+        }\n+      } else {\n+        logWarning(s\"File $activeFile does not exist\")\n+      }\n+      openFile()\n+      scheduleCleanup()\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Error rolling over $activeFile to $rolloverFile\", e)\n+    }\n+  }\n+\n+  /** Schedule cleaning up for older rolled over files */\n+  private def scheduleCleanup() {\n+    if (retainCount > 0) {\n+      executor.submit(new Runnable() { override def run() { cleanup() }})"
  }, {
    "author": {
      "login": "pwendell"
    },
    "body": "Yeah the thing is - you already do a bunch of filesystem operations in the main thread, so I don't think the rollover is a big marginal difference (all of the meta-data operations are going to be roughly the same cost at the FS). In general these are going to be a small number of O(ms) operations that occur pretty infrequently, so probably not a huge deal.\n\nSo to keep it simpler I'd just block on the rollover for now.\n",
    "commit": "fd8f87fde302824813222e1a81003dea32010da7",
    "createdAt": "2014-06-09T06:12:42Z",
    "diffHunk": "@@ -0,0 +1,410 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util\n+\n+import java.io.{File, FileFilter, FileOutputStream, InputStream}\n+import java.text.SimpleDateFormat\n+import java.util.Calendar\n+import java.util.concurrent.{Executors, ThreadFactory}\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.spark.{Logging, SparkConf}\n+import RollingFileAppender._\n+\n+/**\n+ * Continuously appends the data from an input stream into the given file.\n+ */\n+private[spark] class FileAppender(inputStream: InputStream, file: File, bufferSize: Int = 8192)\n+  extends Logging {\n+  @volatile private var outputStream: FileOutputStream = null\n+  @volatile private var markedForStop = false     // has the appender been asked to stopped\n+  @volatile private var stopped = false           // has the appender stopped\n+\n+  // Thread that reads the input stream and writes to file\n+  private val writingThread = new Thread(\"File appending thread for \" + file) {\n+    setDaemon(true)\n+    override def run() {\n+      Utils.logUncaughtExceptions {\n+        appendStreamToFile()\n+      }\n+    }\n+  }\n+  writingThread.start()\n+\n+  /**\n+   * Wait for the appender to stop appending, either because input stream is closed\n+   * or because of any error in appending\n+   */\n+  def awaitTermination() {\n+    synchronized {\n+      if (!stopped) {\n+        wait()\n+      }\n+    }\n+  }\n+\n+  /** Stop the appender */\n+  def stop() {\n+    markedForStop = true\n+  }\n+\n+  /** Continuously read chunks from the input stream and append to the file */\n+  protected def appendStreamToFile() {\n+    try {\n+      logDebug(\"Started appending thread\")\n+      openFile()\n+      val buf = new Array[Byte](bufferSize)\n+      var n = 0\n+      while (!markedForStop && n != -1) {\n+        n = inputStream.read(buf)\n+        if (n != -1) {\n+          appendToFile(buf, n)\n+        }\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Error writing stream to file $file\", e)\n+    } finally {\n+      closeFile()\n+      synchronized {\n+        stopped = true\n+        notifyAll()\n+      }\n+    }\n+  }\n+\n+  /** Append bytes to the file output stream */\n+  protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (outputStream == null) {\n+      openFile()\n+    }\n+    outputStream.write(bytes, 0, len)\n+  }\n+\n+  /** Open the file output stream */\n+  protected def openFile() {\n+    outputStream = new FileOutputStream(file, true)\n+    logDebug(s\"Opened file $file\")\n+  }\n+\n+  /** Close the file output stream */\n+  protected def closeFile() {\n+    outputStream.flush()\n+    outputStream.close()\n+    logDebug(s\"Closed file $file\")\n+  }\n+}\n+\n+/**\n+ * Companion object to [[org.apache.log4j.FileAppender]] which has helper\n+ * functions to choose the write type of FileAppender based on SparkConf configuration.\n+ */\n+private[spark] object FileAppender extends Logging {\n+\n+  /** Create the right appender based on Spark configuration */\n+  def apply(inputStream: InputStream, file: File, conf: SparkConf): FileAppender = {\n+\n+    import RollingFileAppender._\n+\n+    val rolloverSizeOption = conf.getOption(SIZE_PROPERTY)\n+    val rolloverIntervalOption = conf.getOption(INTERVAL_PROPERTY)\n+\n+    (rolloverIntervalOption, rolloverSizeOption) match {\n+\n+      case (Some(rolloverInterval), Some(rolloverSize)) =>\n+        // if both size and interval have been set\n+        logWarning(s\"Rollover interval [$rolloverInterval] and size [$rolloverSize] \" +\n+          s\"both set for executor logs, rolling logs not enabled\")\n+        new FileAppender(inputStream, file)\n+\n+      case (Some(rolloverInterval), None) =>  // if interval has been set\n+        val validatedParams: Option[(Long, String)] = rolloverInterval match {\n+          case \"daily\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with daily rolling\")\n+            Some(24 * 60 * 60 * 1000L, s\"--YYYY-MM-dd\")\n+          case \"hourly\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with hourly rolling\")\n+            Some(60 * 60 * 1000L, s\"--YYYY-MM-dd--HH\")\n+          case \"minutely\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling every minute\")\n+            Some(60 * 1000L, s\"--YYYY-MM-dd--HH-mm\")\n+          case IntParam(seconds) =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling $seconds seconds\")\n+            Some(seconds * 1000L, s\"--YYYY-MM-dd--HH-mm-ss\")\n+          case _ =>\n+            logWarning(s\"Illegal interval for rolling executor logs [$rolloverInterval], \" +\n+                s\"rolling logs not enabled\")\n+            None\n+        }\n+        validatedParams.map {\n+          case (interval, pattern) =>\n+            new RollingFileAppender(\n+              inputStream, file, new TimeBasedRollingPolicy(interval),pattern, conf)\n+        }.getOrElse {\n+          new FileAppender(inputStream, file)\n+        }\n+\n+      case (None, Some(rolloverSize)) =>    // if size has been set\n+        rolloverSize match {\n+          case IntParam(bytes) =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling every $bytes bytes\")\n+            new RollingFileAppender(inputStream, file, new SizeBasedRollingPolicy(bytes),\n+              s\"--YYYY-MM-dd--HH-mm-ss-SSSS\", conf)\n+          case _ =>\n+            logWarning(\n+              s\"Illegal size for rolling executor logs [$rolloverSize], \" +\n+                s\"rolling logs not enabled\")\n+            new FileAppender(inputStream, file)\n+        }\n+\n+      case (None, None) =>                // if neither size nor interval has been set\n+        new FileAppender(inputStream, file)\n+    }\n+  }\n+}\n+\n+/**\n+ * Defines the policy based on which [[org.apache.spark.util.RollingFileAppender]] will\n+ * generate rolling files.\n+ */\n+private[spark] trait RollingPolicy {\n+\n+  /** Whether rollover should be initiated at this moment */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean\n+\n+  /** Notify that rollover has occurred */\n+  def rolledOver()\n+\n+  /** Notify that bytes have been written */\n+  def bytesWritten(bytes: Long)\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] by which files will be rolled\n+ * over at a fixed interval.\n+ */\n+private[spark] class TimeBasedRollingPolicy(\n+    var rolloverIntervalMillis: Long,\n+    checkIntervalConstraint: Boolean = true   // set to false while testing\n+  ) extends RollingPolicy with Logging {\n+\n+  import TimeBasedRollingPolicy._\n+  if (checkIntervalConstraint && rolloverIntervalMillis < MINIMUM_INTERVAL_SECONDS * 1000L) {\n+    logWarning(s\"Rolling interval [${rolloverIntervalMillis/1000L} seconds] is too small. \" +\n+      s\"Setting the interval to the acceptable minimum of $MINIMUM_INTERVAL_SECONDS seconds.\")\n+    rolloverIntervalMillis = MINIMUM_INTERVAL_SECONDS * 1000L\n+  }\n+\n+  @volatile private var nextRolloverTime = calculateNextRolloverTime()\n+\n+  /** Should rollover if current time has exceeded next rollover time */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean = {\n+    System.currentTimeMillis > nextRolloverTime\n+  }\n+\n+  /** Rollover has occurred, so find the next time to rollover */\n+  def rolledOver() {\n+    nextRolloverTime = calculateNextRolloverTime()\n+    logDebug(s\"Current time: ${System.currentTimeMillis}, next rollover time: \" + nextRolloverTime)\n+  }\n+\n+  def bytesWritten(bytes: Long) { }  // nothing to do\n+\n+  private def calculateNextRolloverTime(): Long = {\n+    val now = System.currentTimeMillis()\n+    val targetTime = (\n+      math.ceil(now.toDouble / rolloverIntervalMillis) * rolloverIntervalMillis\n+    ).toLong\n+    logDebug(s\"Next rollover time is $targetTime\")\n+    targetTime\n+  }\n+}\n+\n+private[spark] object TimeBasedRollingPolicy {\n+  val MINIMUM_INTERVAL_SECONDS = 60L  // 1 minute\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] by which files will be rolled\n+ * over after reaching a particular size.\n+ */\n+private[spark] class SizeBasedRollingPolicy(\n+    var rolloverSizeBytes: Long,\n+    checkSizeConstraint: Boolean = true     // set to false while testing\n+  ) extends RollingPolicy with Logging {\n+\n+  import SizeBasedRollingPolicy._\n+  if (checkSizeConstraint && rolloverSizeBytes < MINIMUM_SIZE_BYTES) {\n+    logWarning(s\"Rolling size [$rolloverSizeBytes bytes] is too small. \" +\n+      s\"Setting the size to the acceptable minimum of $MINIMUM_SIZE_BYTES bytes.\")\n+    rolloverSizeBytes = MINIMUM_SIZE_BYTES * 1000L\n+  }\n+\n+  @volatile private var bytesWrittenTillNow = 0L\n+\n+  /** Should rollover if the next set of bytes is going to exceed the size limit */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean = {\n+    bytesToBeWritten + bytesWrittenTillNow > rolloverSizeBytes\n+  }\n+\n+  /** Rollover has occurred, so reset the counter */\n+  def rolledOver() {\n+    bytesWrittenTillNow = 0\n+  }\n+\n+  /** Increment the bytes that have been written in the current file */\n+  def bytesWritten(bytes: Long) {\n+    bytesWrittenTillNow += bytes\n+  }\n+}\n+\n+private[spark] object SizeBasedRollingPolicy {\n+  val MINIMUM_SIZE_BYTES = RollingFileAppender.DEFAULT_BUFFER_SIZE * 10\n+}\n+\n+\n+/**\n+ * Continuously appends data from input stream into the given file, and rolls\n+ * over the file after the given interval. The rolled over files are named\n+ * based on the given pattern.\n+ *\n+ * @param inputStream             Input stream to read data from\n+ * @param activeFile              File to write data to\n+ * @param rollingPolicy           Policy based on which files will be rolled over.\n+ * @param rollingFilePattern      Pattern based on which the rolled over files will be named.\n+ *                                Uses SimpleDataFormat pattern.\n+ * @param conf                    SparkConf that is used to pass on extra configurations\n+ * @param bufferSize              Optional buffer size. Used mainly for testing.\n+ */\n+private[spark] class RollingFileAppender(\n+    inputStream: InputStream,\n+    activeFile: File,\n+    val rollingPolicy: RollingPolicy,\n+    rollingFilePattern: String,\n+    conf: SparkConf,\n+    bufferSize: Int = DEFAULT_BUFFER_SIZE\n+  ) extends FileAppender(inputStream, activeFile, bufferSize) {\n+\n+  private val retainCount = conf.getInt(KEEP_LAST_PROPERTY, -1)\n+  private val formatter = new SimpleDateFormat(rollingFilePattern)\n+\n+  private val executor = Executors.newFixedThreadPool(1, new ThreadFactory{\n+    def newThread(r: Runnable): Thread = {\n+      val t = new Thread(r)\n+      t.setDaemon(true)\n+      t.setName(s\"Threadpool of ${RollingFileAppender.this.getClass.getSimpleName} for $activeFile\")\n+      t\n+    }\n+  })\n+\n+  /** Stop the appender */\n+  override def stop() {\n+    super.stop()\n+    executor.shutdownNow()\n+  }\n+\n+  /** Append bytes to file after rolling over is necessary */\n+  override protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (rollingPolicy.shouldRollover(len)) {\n+      rollover()\n+      rollingPolicy.rolledOver()\n+    }\n+    super.appendToFile(bytes, len)\n+    rollingPolicy.bytesWritten(len)\n+  }\n+\n+  /** Rollover the file, by closing the output stream and moving it over */\n+  private def rollover() {\n+    val rolloverSuffix = formatter.format(Calendar.getInstance.getTime)\n+    val rolloverFile = new File(\n+      activeFile.getParentFile, activeFile.getName + rolloverSuffix).getAbsoluteFile\n+    logDebug(\"Attempting to rollover at \" + System.currentTimeMillis + \" to file \" + rolloverFile)\n+\n+    try {\n+      closeFile()\n+      if (activeFile.exists) {\n+        if (!rolloverFile.exists) {\n+          FileUtils.moveFile(activeFile, rolloverFile)\n+          logInfo(s\"Rolled over $activeFile to $rolloverFile\")\n+        } else {\n+          // In case the rollover file name clashes, make a unique file name.\n+          // The resultant file names are long and ugly, so this is used only\n+          // if there is a name collision. This can be avoided by the using\n+          // the right pattern such that name collisions do not occur.\n+          var i = 0\n+          var altRolloverFile: File = null\n+          do {\n+            altRolloverFile = new File(activeFile.getParent,\n+              s\"${activeFile.getName}$rolloverSuffix--$i\").getAbsoluteFile\n+            i += 1\n+          } while (i < 10000 && altRolloverFile.exists)\n+\n+          logWarning(s\"Rollover file $rolloverFile already exists, \" +\n+            s\"rolled over $activeFile to file $altRolloverFile\")\n+          logWarning(s\"Make sure that the given file name pattern [$rollingFilePattern]\")\n+          FileUtils.moveFile(activeFile, altRolloverFile)\n+        }\n+      } else {\n+        logWarning(s\"File $activeFile does not exist\")\n+      }\n+      openFile()\n+      scheduleCleanup()\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Error rolling over $activeFile to $rolloverFile\", e)\n+    }\n+  }\n+\n+  /** Schedule cleaning up for older rolled over files */\n+  private def scheduleCleanup() {\n+    if (retainCount > 0) {\n+      executor.submit(new Runnable() { override def run() { cleanup() }})"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Changed!\n",
    "commit": "fd8f87fde302824813222e1a81003dea32010da7",
    "createdAt": "2014-06-09T20:52:24Z",
    "diffHunk": "@@ -0,0 +1,410 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util\n+\n+import java.io.{File, FileFilter, FileOutputStream, InputStream}\n+import java.text.SimpleDateFormat\n+import java.util.Calendar\n+import java.util.concurrent.{Executors, ThreadFactory}\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.spark.{Logging, SparkConf}\n+import RollingFileAppender._\n+\n+/**\n+ * Continuously appends the data from an input stream into the given file.\n+ */\n+private[spark] class FileAppender(inputStream: InputStream, file: File, bufferSize: Int = 8192)\n+  extends Logging {\n+  @volatile private var outputStream: FileOutputStream = null\n+  @volatile private var markedForStop = false     // has the appender been asked to stopped\n+  @volatile private var stopped = false           // has the appender stopped\n+\n+  // Thread that reads the input stream and writes to file\n+  private val writingThread = new Thread(\"File appending thread for \" + file) {\n+    setDaemon(true)\n+    override def run() {\n+      Utils.logUncaughtExceptions {\n+        appendStreamToFile()\n+      }\n+    }\n+  }\n+  writingThread.start()\n+\n+  /**\n+   * Wait for the appender to stop appending, either because input stream is closed\n+   * or because of any error in appending\n+   */\n+  def awaitTermination() {\n+    synchronized {\n+      if (!stopped) {\n+        wait()\n+      }\n+    }\n+  }\n+\n+  /** Stop the appender */\n+  def stop() {\n+    markedForStop = true\n+  }\n+\n+  /** Continuously read chunks from the input stream and append to the file */\n+  protected def appendStreamToFile() {\n+    try {\n+      logDebug(\"Started appending thread\")\n+      openFile()\n+      val buf = new Array[Byte](bufferSize)\n+      var n = 0\n+      while (!markedForStop && n != -1) {\n+        n = inputStream.read(buf)\n+        if (n != -1) {\n+          appendToFile(buf, n)\n+        }\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Error writing stream to file $file\", e)\n+    } finally {\n+      closeFile()\n+      synchronized {\n+        stopped = true\n+        notifyAll()\n+      }\n+    }\n+  }\n+\n+  /** Append bytes to the file output stream */\n+  protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (outputStream == null) {\n+      openFile()\n+    }\n+    outputStream.write(bytes, 0, len)\n+  }\n+\n+  /** Open the file output stream */\n+  protected def openFile() {\n+    outputStream = new FileOutputStream(file, true)\n+    logDebug(s\"Opened file $file\")\n+  }\n+\n+  /** Close the file output stream */\n+  protected def closeFile() {\n+    outputStream.flush()\n+    outputStream.close()\n+    logDebug(s\"Closed file $file\")\n+  }\n+}\n+\n+/**\n+ * Companion object to [[org.apache.log4j.FileAppender]] which has helper\n+ * functions to choose the write type of FileAppender based on SparkConf configuration.\n+ */\n+private[spark] object FileAppender extends Logging {\n+\n+  /** Create the right appender based on Spark configuration */\n+  def apply(inputStream: InputStream, file: File, conf: SparkConf): FileAppender = {\n+\n+    import RollingFileAppender._\n+\n+    val rolloverSizeOption = conf.getOption(SIZE_PROPERTY)\n+    val rolloverIntervalOption = conf.getOption(INTERVAL_PROPERTY)\n+\n+    (rolloverIntervalOption, rolloverSizeOption) match {\n+\n+      case (Some(rolloverInterval), Some(rolloverSize)) =>\n+        // if both size and interval have been set\n+        logWarning(s\"Rollover interval [$rolloverInterval] and size [$rolloverSize] \" +\n+          s\"both set for executor logs, rolling logs not enabled\")\n+        new FileAppender(inputStream, file)\n+\n+      case (Some(rolloverInterval), None) =>  // if interval has been set\n+        val validatedParams: Option[(Long, String)] = rolloverInterval match {\n+          case \"daily\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with daily rolling\")\n+            Some(24 * 60 * 60 * 1000L, s\"--YYYY-MM-dd\")\n+          case \"hourly\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with hourly rolling\")\n+            Some(60 * 60 * 1000L, s\"--YYYY-MM-dd--HH\")\n+          case \"minutely\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling every minute\")\n+            Some(60 * 1000L, s\"--YYYY-MM-dd--HH-mm\")\n+          case IntParam(seconds) =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling $seconds seconds\")\n+            Some(seconds * 1000L, s\"--YYYY-MM-dd--HH-mm-ss\")\n+          case _ =>\n+            logWarning(s\"Illegal interval for rolling executor logs [$rolloverInterval], \" +\n+                s\"rolling logs not enabled\")\n+            None\n+        }\n+        validatedParams.map {\n+          case (interval, pattern) =>\n+            new RollingFileAppender(\n+              inputStream, file, new TimeBasedRollingPolicy(interval),pattern, conf)\n+        }.getOrElse {\n+          new FileAppender(inputStream, file)\n+        }\n+\n+      case (None, Some(rolloverSize)) =>    // if size has been set\n+        rolloverSize match {\n+          case IntParam(bytes) =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling every $bytes bytes\")\n+            new RollingFileAppender(inputStream, file, new SizeBasedRollingPolicy(bytes),\n+              s\"--YYYY-MM-dd--HH-mm-ss-SSSS\", conf)\n+          case _ =>\n+            logWarning(\n+              s\"Illegal size for rolling executor logs [$rolloverSize], \" +\n+                s\"rolling logs not enabled\")\n+            new FileAppender(inputStream, file)\n+        }\n+\n+      case (None, None) =>                // if neither size nor interval has been set\n+        new FileAppender(inputStream, file)\n+    }\n+  }\n+}\n+\n+/**\n+ * Defines the policy based on which [[org.apache.spark.util.RollingFileAppender]] will\n+ * generate rolling files.\n+ */\n+private[spark] trait RollingPolicy {\n+\n+  /** Whether rollover should be initiated at this moment */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean\n+\n+  /** Notify that rollover has occurred */\n+  def rolledOver()\n+\n+  /** Notify that bytes have been written */\n+  def bytesWritten(bytes: Long)\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] by which files will be rolled\n+ * over at a fixed interval.\n+ */\n+private[spark] class TimeBasedRollingPolicy(\n+    var rolloverIntervalMillis: Long,\n+    checkIntervalConstraint: Boolean = true   // set to false while testing\n+  ) extends RollingPolicy with Logging {\n+\n+  import TimeBasedRollingPolicy._\n+  if (checkIntervalConstraint && rolloverIntervalMillis < MINIMUM_INTERVAL_SECONDS * 1000L) {\n+    logWarning(s\"Rolling interval [${rolloverIntervalMillis/1000L} seconds] is too small. \" +\n+      s\"Setting the interval to the acceptable minimum of $MINIMUM_INTERVAL_SECONDS seconds.\")\n+    rolloverIntervalMillis = MINIMUM_INTERVAL_SECONDS * 1000L\n+  }\n+\n+  @volatile private var nextRolloverTime = calculateNextRolloverTime()\n+\n+  /** Should rollover if current time has exceeded next rollover time */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean = {\n+    System.currentTimeMillis > nextRolloverTime\n+  }\n+\n+  /** Rollover has occurred, so find the next time to rollover */\n+  def rolledOver() {\n+    nextRolloverTime = calculateNextRolloverTime()\n+    logDebug(s\"Current time: ${System.currentTimeMillis}, next rollover time: \" + nextRolloverTime)\n+  }\n+\n+  def bytesWritten(bytes: Long) { }  // nothing to do\n+\n+  private def calculateNextRolloverTime(): Long = {\n+    val now = System.currentTimeMillis()\n+    val targetTime = (\n+      math.ceil(now.toDouble / rolloverIntervalMillis) * rolloverIntervalMillis\n+    ).toLong\n+    logDebug(s\"Next rollover time is $targetTime\")\n+    targetTime\n+  }\n+}\n+\n+private[spark] object TimeBasedRollingPolicy {\n+  val MINIMUM_INTERVAL_SECONDS = 60L  // 1 minute\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] by which files will be rolled\n+ * over after reaching a particular size.\n+ */\n+private[spark] class SizeBasedRollingPolicy(\n+    var rolloverSizeBytes: Long,\n+    checkSizeConstraint: Boolean = true     // set to false while testing\n+  ) extends RollingPolicy with Logging {\n+\n+  import SizeBasedRollingPolicy._\n+  if (checkSizeConstraint && rolloverSizeBytes < MINIMUM_SIZE_BYTES) {\n+    logWarning(s\"Rolling size [$rolloverSizeBytes bytes] is too small. \" +\n+      s\"Setting the size to the acceptable minimum of $MINIMUM_SIZE_BYTES bytes.\")\n+    rolloverSizeBytes = MINIMUM_SIZE_BYTES * 1000L\n+  }\n+\n+  @volatile private var bytesWrittenTillNow = 0L\n+\n+  /** Should rollover if the next set of bytes is going to exceed the size limit */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean = {\n+    bytesToBeWritten + bytesWrittenTillNow > rolloverSizeBytes\n+  }\n+\n+  /** Rollover has occurred, so reset the counter */\n+  def rolledOver() {\n+    bytesWrittenTillNow = 0\n+  }\n+\n+  /** Increment the bytes that have been written in the current file */\n+  def bytesWritten(bytes: Long) {\n+    bytesWrittenTillNow += bytes\n+  }\n+}\n+\n+private[spark] object SizeBasedRollingPolicy {\n+  val MINIMUM_SIZE_BYTES = RollingFileAppender.DEFAULT_BUFFER_SIZE * 10\n+}\n+\n+\n+/**\n+ * Continuously appends data from input stream into the given file, and rolls\n+ * over the file after the given interval. The rolled over files are named\n+ * based on the given pattern.\n+ *\n+ * @param inputStream             Input stream to read data from\n+ * @param activeFile              File to write data to\n+ * @param rollingPolicy           Policy based on which files will be rolled over.\n+ * @param rollingFilePattern      Pattern based on which the rolled over files will be named.\n+ *                                Uses SimpleDataFormat pattern.\n+ * @param conf                    SparkConf that is used to pass on extra configurations\n+ * @param bufferSize              Optional buffer size. Used mainly for testing.\n+ */\n+private[spark] class RollingFileAppender(\n+    inputStream: InputStream,\n+    activeFile: File,\n+    val rollingPolicy: RollingPolicy,\n+    rollingFilePattern: String,\n+    conf: SparkConf,\n+    bufferSize: Int = DEFAULT_BUFFER_SIZE\n+  ) extends FileAppender(inputStream, activeFile, bufferSize) {\n+\n+  private val retainCount = conf.getInt(KEEP_LAST_PROPERTY, -1)\n+  private val formatter = new SimpleDateFormat(rollingFilePattern)\n+\n+  private val executor = Executors.newFixedThreadPool(1, new ThreadFactory{\n+    def newThread(r: Runnable): Thread = {\n+      val t = new Thread(r)\n+      t.setDaemon(true)\n+      t.setName(s\"Threadpool of ${RollingFileAppender.this.getClass.getSimpleName} for $activeFile\")\n+      t\n+    }\n+  })\n+\n+  /** Stop the appender */\n+  override def stop() {\n+    super.stop()\n+    executor.shutdownNow()\n+  }\n+\n+  /** Append bytes to file after rolling over is necessary */\n+  override protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (rollingPolicy.shouldRollover(len)) {\n+      rollover()\n+      rollingPolicy.rolledOver()\n+    }\n+    super.appendToFile(bytes, len)\n+    rollingPolicy.bytesWritten(len)\n+  }\n+\n+  /** Rollover the file, by closing the output stream and moving it over */\n+  private def rollover() {\n+    val rolloverSuffix = formatter.format(Calendar.getInstance.getTime)\n+    val rolloverFile = new File(\n+      activeFile.getParentFile, activeFile.getName + rolloverSuffix).getAbsoluteFile\n+    logDebug(\"Attempting to rollover at \" + System.currentTimeMillis + \" to file \" + rolloverFile)\n+\n+    try {\n+      closeFile()\n+      if (activeFile.exists) {\n+        if (!rolloverFile.exists) {\n+          FileUtils.moveFile(activeFile, rolloverFile)\n+          logInfo(s\"Rolled over $activeFile to $rolloverFile\")\n+        } else {\n+          // In case the rollover file name clashes, make a unique file name.\n+          // The resultant file names are long and ugly, so this is used only\n+          // if there is a name collision. This can be avoided by the using\n+          // the right pattern such that name collisions do not occur.\n+          var i = 0\n+          var altRolloverFile: File = null\n+          do {\n+            altRolloverFile = new File(activeFile.getParent,\n+              s\"${activeFile.getName}$rolloverSuffix--$i\").getAbsoluteFile\n+            i += 1\n+          } while (i < 10000 && altRolloverFile.exists)\n+\n+          logWarning(s\"Rollover file $rolloverFile already exists, \" +\n+            s\"rolled over $activeFile to file $altRolloverFile\")\n+          logWarning(s\"Make sure that the given file name pattern [$rollingFilePattern]\")\n+          FileUtils.moveFile(activeFile, altRolloverFile)\n+        }\n+      } else {\n+        logWarning(s\"File $activeFile does not exist\")\n+      }\n+      openFile()\n+      scheduleCleanup()\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Error rolling over $activeFile to $rolloverFile\", e)\n+    }\n+  }\n+\n+  /** Schedule cleaning up for older rolled over files */\n+  private def scheduleCleanup() {\n+    if (retainCount > 0) {\n+      executor.submit(new Runnable() { override def run() { cleanup() }})"
  }],
  "prId": 895
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "This still says log4j.\n",
    "commit": "fd8f87fde302824813222e1a81003dea32010da7",
    "createdAt": "2014-06-05T22:58:10Z",
    "diffHunk": "@@ -0,0 +1,410 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util\n+\n+import java.io.{File, FileFilter, FileOutputStream, InputStream}\n+import java.text.SimpleDateFormat\n+import java.util.Calendar\n+import java.util.concurrent.{Executors, ThreadFactory}\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.spark.{Logging, SparkConf}\n+import RollingFileAppender._\n+\n+/**\n+ * Continuously appends the data from an input stream into the given file.\n+ */\n+private[spark] class FileAppender(inputStream: InputStream, file: File, bufferSize: Int = 8192)\n+  extends Logging {\n+  @volatile private var outputStream: FileOutputStream = null\n+  @volatile private var markedForStop = false     // has the appender been asked to stopped\n+  @volatile private var stopped = false           // has the appender stopped\n+\n+  // Thread that reads the input stream and writes to file\n+  private val writingThread = new Thread(\"File appending thread for \" + file) {\n+    setDaemon(true)\n+    override def run() {\n+      Utils.logUncaughtExceptions {\n+        appendStreamToFile()\n+      }\n+    }\n+  }\n+  writingThread.start()\n+\n+  /**\n+   * Wait for the appender to stop appending, either because input stream is closed\n+   * or because of any error in appending\n+   */\n+  def awaitTermination() {\n+    synchronized {\n+      if (!stopped) {\n+        wait()\n+      }\n+    }\n+  }\n+\n+  /** Stop the appender */\n+  def stop() {\n+    markedForStop = true\n+  }\n+\n+  /** Continuously read chunks from the input stream and append to the file */\n+  protected def appendStreamToFile() {\n+    try {\n+      logDebug(\"Started appending thread\")\n+      openFile()\n+      val buf = new Array[Byte](bufferSize)\n+      var n = 0\n+      while (!markedForStop && n != -1) {\n+        n = inputStream.read(buf)\n+        if (n != -1) {\n+          appendToFile(buf, n)\n+        }\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Error writing stream to file $file\", e)\n+    } finally {\n+      closeFile()\n+      synchronized {\n+        stopped = true\n+        notifyAll()\n+      }\n+    }\n+  }\n+\n+  /** Append bytes to the file output stream */\n+  protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (outputStream == null) {\n+      openFile()\n+    }\n+    outputStream.write(bytes, 0, len)\n+  }\n+\n+  /** Open the file output stream */\n+  protected def openFile() {\n+    outputStream = new FileOutputStream(file, true)\n+    logDebug(s\"Opened file $file\")\n+  }\n+\n+  /** Close the file output stream */\n+  protected def closeFile() {\n+    outputStream.flush()\n+    outputStream.close()\n+    logDebug(s\"Closed file $file\")\n+  }\n+}\n+\n+/**\n+ * Companion object to [[org.apache.log4j.FileAppender]] which has helper"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Fixed.\n",
    "commit": "fd8f87fde302824813222e1a81003dea32010da7",
    "createdAt": "2014-06-09T21:11:16Z",
    "diffHunk": "@@ -0,0 +1,410 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util\n+\n+import java.io.{File, FileFilter, FileOutputStream, InputStream}\n+import java.text.SimpleDateFormat\n+import java.util.Calendar\n+import java.util.concurrent.{Executors, ThreadFactory}\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.spark.{Logging, SparkConf}\n+import RollingFileAppender._\n+\n+/**\n+ * Continuously appends the data from an input stream into the given file.\n+ */\n+private[spark] class FileAppender(inputStream: InputStream, file: File, bufferSize: Int = 8192)\n+  extends Logging {\n+  @volatile private var outputStream: FileOutputStream = null\n+  @volatile private var markedForStop = false     // has the appender been asked to stopped\n+  @volatile private var stopped = false           // has the appender stopped\n+\n+  // Thread that reads the input stream and writes to file\n+  private val writingThread = new Thread(\"File appending thread for \" + file) {\n+    setDaemon(true)\n+    override def run() {\n+      Utils.logUncaughtExceptions {\n+        appendStreamToFile()\n+      }\n+    }\n+  }\n+  writingThread.start()\n+\n+  /**\n+   * Wait for the appender to stop appending, either because input stream is closed\n+   * or because of any error in appending\n+   */\n+  def awaitTermination() {\n+    synchronized {\n+      if (!stopped) {\n+        wait()\n+      }\n+    }\n+  }\n+\n+  /** Stop the appender */\n+  def stop() {\n+    markedForStop = true\n+  }\n+\n+  /** Continuously read chunks from the input stream and append to the file */\n+  protected def appendStreamToFile() {\n+    try {\n+      logDebug(\"Started appending thread\")\n+      openFile()\n+      val buf = new Array[Byte](bufferSize)\n+      var n = 0\n+      while (!markedForStop && n != -1) {\n+        n = inputStream.read(buf)\n+        if (n != -1) {\n+          appendToFile(buf, n)\n+        }\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Error writing stream to file $file\", e)\n+    } finally {\n+      closeFile()\n+      synchronized {\n+        stopped = true\n+        notifyAll()\n+      }\n+    }\n+  }\n+\n+  /** Append bytes to the file output stream */\n+  protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (outputStream == null) {\n+      openFile()\n+    }\n+    outputStream.write(bytes, 0, len)\n+  }\n+\n+  /** Open the file output stream */\n+  protected def openFile() {\n+    outputStream = new FileOutputStream(file, true)\n+    logDebug(s\"Opened file $file\")\n+  }\n+\n+  /** Close the file output stream */\n+  protected def closeFile() {\n+    outputStream.flush()\n+    outputStream.close()\n+    logDebug(s\"Closed file $file\")\n+  }\n+}\n+\n+/**\n+ * Companion object to [[org.apache.log4j.FileAppender]] which has helper"
  }],
  "prId": 895
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "\"write type\" -> \"right type\" or maybe \"correct type\"\n",
    "commit": "fd8f87fde302824813222e1a81003dea32010da7",
    "createdAt": "2014-06-05T22:59:04Z",
    "diffHunk": "@@ -0,0 +1,410 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util\n+\n+import java.io.{File, FileFilter, FileOutputStream, InputStream}\n+import java.text.SimpleDateFormat\n+import java.util.Calendar\n+import java.util.concurrent.{Executors, ThreadFactory}\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.spark.{Logging, SparkConf}\n+import RollingFileAppender._\n+\n+/**\n+ * Continuously appends the data from an input stream into the given file.\n+ */\n+private[spark] class FileAppender(inputStream: InputStream, file: File, bufferSize: Int = 8192)\n+  extends Logging {\n+  @volatile private var outputStream: FileOutputStream = null\n+  @volatile private var markedForStop = false     // has the appender been asked to stopped\n+  @volatile private var stopped = false           // has the appender stopped\n+\n+  // Thread that reads the input stream and writes to file\n+  private val writingThread = new Thread(\"File appending thread for \" + file) {\n+    setDaemon(true)\n+    override def run() {\n+      Utils.logUncaughtExceptions {\n+        appendStreamToFile()\n+      }\n+    }\n+  }\n+  writingThread.start()\n+\n+  /**\n+   * Wait for the appender to stop appending, either because input stream is closed\n+   * or because of any error in appending\n+   */\n+  def awaitTermination() {\n+    synchronized {\n+      if (!stopped) {\n+        wait()\n+      }\n+    }\n+  }\n+\n+  /** Stop the appender */\n+  def stop() {\n+    markedForStop = true\n+  }\n+\n+  /** Continuously read chunks from the input stream and append to the file */\n+  protected def appendStreamToFile() {\n+    try {\n+      logDebug(\"Started appending thread\")\n+      openFile()\n+      val buf = new Array[Byte](bufferSize)\n+      var n = 0\n+      while (!markedForStop && n != -1) {\n+        n = inputStream.read(buf)\n+        if (n != -1) {\n+          appendToFile(buf, n)\n+        }\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Error writing stream to file $file\", e)\n+    } finally {\n+      closeFile()\n+      synchronized {\n+        stopped = true\n+        notifyAll()\n+      }\n+    }\n+  }\n+\n+  /** Append bytes to the file output stream */\n+  protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (outputStream == null) {\n+      openFile()\n+    }\n+    outputStream.write(bytes, 0, len)\n+  }\n+\n+  /** Open the file output stream */\n+  protected def openFile() {\n+    outputStream = new FileOutputStream(file, true)\n+    logDebug(s\"Opened file $file\")\n+  }\n+\n+  /** Close the file output stream */\n+  protected def closeFile() {\n+    outputStream.flush()\n+    outputStream.close()\n+    logDebug(s\"Closed file $file\")\n+  }\n+}\n+\n+/**\n+ * Companion object to [[org.apache.log4j.FileAppender]] which has helper\n+ * functions to choose the write type of FileAppender based on SparkConf configuration."
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "yikes!\n",
    "commit": "fd8f87fde302824813222e1a81003dea32010da7",
    "createdAt": "2014-06-07T01:05:51Z",
    "diffHunk": "@@ -0,0 +1,410 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util\n+\n+import java.io.{File, FileFilter, FileOutputStream, InputStream}\n+import java.text.SimpleDateFormat\n+import java.util.Calendar\n+import java.util.concurrent.{Executors, ThreadFactory}\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.spark.{Logging, SparkConf}\n+import RollingFileAppender._\n+\n+/**\n+ * Continuously appends the data from an input stream into the given file.\n+ */\n+private[spark] class FileAppender(inputStream: InputStream, file: File, bufferSize: Int = 8192)\n+  extends Logging {\n+  @volatile private var outputStream: FileOutputStream = null\n+  @volatile private var markedForStop = false     // has the appender been asked to stopped\n+  @volatile private var stopped = false           // has the appender stopped\n+\n+  // Thread that reads the input stream and writes to file\n+  private val writingThread = new Thread(\"File appending thread for \" + file) {\n+    setDaemon(true)\n+    override def run() {\n+      Utils.logUncaughtExceptions {\n+        appendStreamToFile()\n+      }\n+    }\n+  }\n+  writingThread.start()\n+\n+  /**\n+   * Wait for the appender to stop appending, either because input stream is closed\n+   * or because of any error in appending\n+   */\n+  def awaitTermination() {\n+    synchronized {\n+      if (!stopped) {\n+        wait()\n+      }\n+    }\n+  }\n+\n+  /** Stop the appender */\n+  def stop() {\n+    markedForStop = true\n+  }\n+\n+  /** Continuously read chunks from the input stream and append to the file */\n+  protected def appendStreamToFile() {\n+    try {\n+      logDebug(\"Started appending thread\")\n+      openFile()\n+      val buf = new Array[Byte](bufferSize)\n+      var n = 0\n+      while (!markedForStop && n != -1) {\n+        n = inputStream.read(buf)\n+        if (n != -1) {\n+          appendToFile(buf, n)\n+        }\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Error writing stream to file $file\", e)\n+    } finally {\n+      closeFile()\n+      synchronized {\n+        stopped = true\n+        notifyAll()\n+      }\n+    }\n+  }\n+\n+  /** Append bytes to the file output stream */\n+  protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (outputStream == null) {\n+      openFile()\n+    }\n+    outputStream.write(bytes, 0, len)\n+  }\n+\n+  /** Open the file output stream */\n+  protected def openFile() {\n+    outputStream = new FileOutputStream(file, true)\n+    logDebug(s\"Opened file $file\")\n+  }\n+\n+  /** Close the file output stream */\n+  protected def closeFile() {\n+    outputStream.flush()\n+    outputStream.close()\n+    logDebug(s\"Closed file $file\")\n+  }\n+}\n+\n+/**\n+ * Companion object to [[org.apache.log4j.FileAppender]] which has helper\n+ * functions to choose the write type of FileAppender based on SparkConf configuration."
  }],
  "prId": 895
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "It seems a bit weird that this extends FileAppender. Why not instead have a `RollingFileAppender` just encapsulate multiple instances of a `FileAppender`. Logically, a rolling appender is not really a sub type of an appender that appends to one file.\n\nThe fact that `outputFile` is a `var` makes the semantics of the `FileAppender` pretty confusing. Because a single FileAppender will be used to append to many files. It seems more intuitive to have the rolling appender simply create a new FileAppender when a new file is created. You'd just need to make sure you stop and flush the old appender before switching over.\n",
    "commit": "fd8f87fde302824813222e1a81003dea32010da7",
    "createdAt": "2014-06-05T23:13:03Z",
    "diffHunk": "@@ -0,0 +1,410 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util\n+\n+import java.io.{File, FileFilter, FileOutputStream, InputStream}\n+import java.text.SimpleDateFormat\n+import java.util.Calendar\n+import java.util.concurrent.{Executors, ThreadFactory}\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.spark.{Logging, SparkConf}\n+import RollingFileAppender._\n+\n+/**\n+ * Continuously appends the data from an input stream into the given file.\n+ */\n+private[spark] class FileAppender(inputStream: InputStream, file: File, bufferSize: Int = 8192)\n+  extends Logging {\n+  @volatile private var outputStream: FileOutputStream = null\n+  @volatile private var markedForStop = false     // has the appender been asked to stopped\n+  @volatile private var stopped = false           // has the appender stopped\n+\n+  // Thread that reads the input stream and writes to file\n+  private val writingThread = new Thread(\"File appending thread for \" + file) {\n+    setDaemon(true)\n+    override def run() {\n+      Utils.logUncaughtExceptions {\n+        appendStreamToFile()\n+      }\n+    }\n+  }\n+  writingThread.start()\n+\n+  /**\n+   * Wait for the appender to stop appending, either because input stream is closed\n+   * or because of any error in appending\n+   */\n+  def awaitTermination() {\n+    synchronized {\n+      if (!stopped) {\n+        wait()\n+      }\n+    }\n+  }\n+\n+  /** Stop the appender */\n+  def stop() {\n+    markedForStop = true\n+  }\n+\n+  /** Continuously read chunks from the input stream and append to the file */\n+  protected def appendStreamToFile() {\n+    try {\n+      logDebug(\"Started appending thread\")\n+      openFile()\n+      val buf = new Array[Byte](bufferSize)\n+      var n = 0\n+      while (!markedForStop && n != -1) {\n+        n = inputStream.read(buf)\n+        if (n != -1) {\n+          appendToFile(buf, n)\n+        }\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Error writing stream to file $file\", e)\n+    } finally {\n+      closeFile()\n+      synchronized {\n+        stopped = true\n+        notifyAll()\n+      }\n+    }\n+  }\n+\n+  /** Append bytes to the file output stream */\n+  protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (outputStream == null) {\n+      openFile()\n+    }\n+    outputStream.write(bytes, 0, len)\n+  }\n+\n+  /** Open the file output stream */\n+  protected def openFile() {\n+    outputStream = new FileOutputStream(file, true)\n+    logDebug(s\"Opened file $file\")\n+  }\n+\n+  /** Close the file output stream */\n+  protected def closeFile() {\n+    outputStream.flush()\n+    outputStream.close()\n+    logDebug(s\"Closed file $file\")\n+  }\n+}\n+\n+/**\n+ * Companion object to [[org.apache.log4j.FileAppender]] which has helper\n+ * functions to choose the write type of FileAppender based on SparkConf configuration.\n+ */\n+private[spark] object FileAppender extends Logging {\n+\n+  /** Create the right appender based on Spark configuration */\n+  def apply(inputStream: InputStream, file: File, conf: SparkConf): FileAppender = {\n+\n+    import RollingFileAppender._\n+\n+    val rolloverSizeOption = conf.getOption(SIZE_PROPERTY)\n+    val rolloverIntervalOption = conf.getOption(INTERVAL_PROPERTY)\n+\n+    (rolloverIntervalOption, rolloverSizeOption) match {\n+\n+      case (Some(rolloverInterval), Some(rolloverSize)) =>\n+        // if both size and interval have been set\n+        logWarning(s\"Rollover interval [$rolloverInterval] and size [$rolloverSize] \" +\n+          s\"both set for executor logs, rolling logs not enabled\")\n+        new FileAppender(inputStream, file)\n+\n+      case (Some(rolloverInterval), None) =>  // if interval has been set\n+        val validatedParams: Option[(Long, String)] = rolloverInterval match {\n+          case \"daily\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with daily rolling\")\n+            Some(24 * 60 * 60 * 1000L, s\"--YYYY-MM-dd\")\n+          case \"hourly\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with hourly rolling\")\n+            Some(60 * 60 * 1000L, s\"--YYYY-MM-dd--HH\")\n+          case \"minutely\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling every minute\")\n+            Some(60 * 1000L, s\"--YYYY-MM-dd--HH-mm\")\n+          case IntParam(seconds) =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling $seconds seconds\")\n+            Some(seconds * 1000L, s\"--YYYY-MM-dd--HH-mm-ss\")\n+          case _ =>\n+            logWarning(s\"Illegal interval for rolling executor logs [$rolloverInterval], \" +\n+                s\"rolling logs not enabled\")\n+            None\n+        }\n+        validatedParams.map {\n+          case (interval, pattern) =>\n+            new RollingFileAppender(\n+              inputStream, file, new TimeBasedRollingPolicy(interval),pattern, conf)\n+        }.getOrElse {\n+          new FileAppender(inputStream, file)\n+        }\n+\n+      case (None, Some(rolloverSize)) =>    // if size has been set\n+        rolloverSize match {\n+          case IntParam(bytes) =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling every $bytes bytes\")\n+            new RollingFileAppender(inputStream, file, new SizeBasedRollingPolicy(bytes),\n+              s\"--YYYY-MM-dd--HH-mm-ss-SSSS\", conf)\n+          case _ =>\n+            logWarning(\n+              s\"Illegal size for rolling executor logs [$rolloverSize], \" +\n+                s\"rolling logs not enabled\")\n+            new FileAppender(inputStream, file)\n+        }\n+\n+      case (None, None) =>                // if neither size nor interval has been set\n+        new FileAppender(inputStream, file)\n+    }\n+  }\n+}\n+\n+/**\n+ * Defines the policy based on which [[org.apache.spark.util.RollingFileAppender]] will\n+ * generate rolling files.\n+ */\n+private[spark] trait RollingPolicy {\n+\n+  /** Whether rollover should be initiated at this moment */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean\n+\n+  /** Notify that rollover has occurred */\n+  def rolledOver()\n+\n+  /** Notify that bytes have been written */\n+  def bytesWritten(bytes: Long)\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] by which files will be rolled\n+ * over at a fixed interval.\n+ */\n+private[spark] class TimeBasedRollingPolicy(\n+    var rolloverIntervalMillis: Long,\n+    checkIntervalConstraint: Boolean = true   // set to false while testing\n+  ) extends RollingPolicy with Logging {\n+\n+  import TimeBasedRollingPolicy._\n+  if (checkIntervalConstraint && rolloverIntervalMillis < MINIMUM_INTERVAL_SECONDS * 1000L) {\n+    logWarning(s\"Rolling interval [${rolloverIntervalMillis/1000L} seconds] is too small. \" +\n+      s\"Setting the interval to the acceptable minimum of $MINIMUM_INTERVAL_SECONDS seconds.\")\n+    rolloverIntervalMillis = MINIMUM_INTERVAL_SECONDS * 1000L\n+  }\n+\n+  @volatile private var nextRolloverTime = calculateNextRolloverTime()\n+\n+  /** Should rollover if current time has exceeded next rollover time */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean = {\n+    System.currentTimeMillis > nextRolloverTime\n+  }\n+\n+  /** Rollover has occurred, so find the next time to rollover */\n+  def rolledOver() {\n+    nextRolloverTime = calculateNextRolloverTime()\n+    logDebug(s\"Current time: ${System.currentTimeMillis}, next rollover time: \" + nextRolloverTime)\n+  }\n+\n+  def bytesWritten(bytes: Long) { }  // nothing to do\n+\n+  private def calculateNextRolloverTime(): Long = {\n+    val now = System.currentTimeMillis()\n+    val targetTime = (\n+      math.ceil(now.toDouble / rolloverIntervalMillis) * rolloverIntervalMillis\n+    ).toLong\n+    logDebug(s\"Next rollover time is $targetTime\")\n+    targetTime\n+  }\n+}\n+\n+private[spark] object TimeBasedRollingPolicy {\n+  val MINIMUM_INTERVAL_SECONDS = 60L  // 1 minute\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] by which files will be rolled\n+ * over after reaching a particular size.\n+ */\n+private[spark] class SizeBasedRollingPolicy(\n+    var rolloverSizeBytes: Long,\n+    checkSizeConstraint: Boolean = true     // set to false while testing\n+  ) extends RollingPolicy with Logging {\n+\n+  import SizeBasedRollingPolicy._\n+  if (checkSizeConstraint && rolloverSizeBytes < MINIMUM_SIZE_BYTES) {\n+    logWarning(s\"Rolling size [$rolloverSizeBytes bytes] is too small. \" +\n+      s\"Setting the size to the acceptable minimum of $MINIMUM_SIZE_BYTES bytes.\")\n+    rolloverSizeBytes = MINIMUM_SIZE_BYTES * 1000L\n+  }\n+\n+  @volatile private var bytesWrittenTillNow = 0L\n+\n+  /** Should rollover if the next set of bytes is going to exceed the size limit */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean = {\n+    bytesToBeWritten + bytesWrittenTillNow > rolloverSizeBytes\n+  }\n+\n+  /** Rollover has occurred, so reset the counter */\n+  def rolledOver() {\n+    bytesWrittenTillNow = 0\n+  }\n+\n+  /** Increment the bytes that have been written in the current file */\n+  def bytesWritten(bytes: Long) {\n+    bytesWrittenTillNow += bytes\n+  }\n+}\n+\n+private[spark] object SizeBasedRollingPolicy {\n+  val MINIMUM_SIZE_BYTES = RollingFileAppender.DEFAULT_BUFFER_SIZE * 10\n+}\n+\n+\n+/**\n+ * Continuously appends data from input stream into the given file, and rolls\n+ * over the file after the given interval. The rolled over files are named\n+ * based on the given pattern.\n+ *\n+ * @param inputStream             Input stream to read data from\n+ * @param activeFile              File to write data to\n+ * @param rollingPolicy           Policy based on which files will be rolled over.\n+ * @param rollingFilePattern      Pattern based on which the rolled over files will be named.\n+ *                                Uses SimpleDataFormat pattern.\n+ * @param conf                    SparkConf that is used to pass on extra configurations\n+ * @param bufferSize              Optional buffer size. Used mainly for testing.\n+ */\n+private[spark] class RollingFileAppender(\n+    inputStream: InputStream,\n+    activeFile: File,\n+    val rollingPolicy: RollingPolicy,\n+    rollingFilePattern: String,\n+    conf: SparkConf,\n+    bufferSize: Int = DEFAULT_BUFFER_SIZE\n+  ) extends FileAppender(inputStream, activeFile, bufferSize) {"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Actually, I was inspired by the Log4j's model where [RollingFileAppender](https://logging.apache.org/log4j/extras/apidocs/org/apache/log4j/rolling/RollingFileAppender.html) is subclass of FileAppender. And it makes sense if you look at it this way. FileAppender has all the functionality to append a stream to a file. RollingFileAppender \"inherits\" that functionality to write a stream to file and extends it to write to multiple files. \n",
    "commit": "fd8f87fde302824813222e1a81003dea32010da7",
    "createdAt": "2014-06-07T01:05:26Z",
    "diffHunk": "@@ -0,0 +1,410 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util\n+\n+import java.io.{File, FileFilter, FileOutputStream, InputStream}\n+import java.text.SimpleDateFormat\n+import java.util.Calendar\n+import java.util.concurrent.{Executors, ThreadFactory}\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.spark.{Logging, SparkConf}\n+import RollingFileAppender._\n+\n+/**\n+ * Continuously appends the data from an input stream into the given file.\n+ */\n+private[spark] class FileAppender(inputStream: InputStream, file: File, bufferSize: Int = 8192)\n+  extends Logging {\n+  @volatile private var outputStream: FileOutputStream = null\n+  @volatile private var markedForStop = false     // has the appender been asked to stopped\n+  @volatile private var stopped = false           // has the appender stopped\n+\n+  // Thread that reads the input stream and writes to file\n+  private val writingThread = new Thread(\"File appending thread for \" + file) {\n+    setDaemon(true)\n+    override def run() {\n+      Utils.logUncaughtExceptions {\n+        appendStreamToFile()\n+      }\n+    }\n+  }\n+  writingThread.start()\n+\n+  /**\n+   * Wait for the appender to stop appending, either because input stream is closed\n+   * or because of any error in appending\n+   */\n+  def awaitTermination() {\n+    synchronized {\n+      if (!stopped) {\n+        wait()\n+      }\n+    }\n+  }\n+\n+  /** Stop the appender */\n+  def stop() {\n+    markedForStop = true\n+  }\n+\n+  /** Continuously read chunks from the input stream and append to the file */\n+  protected def appendStreamToFile() {\n+    try {\n+      logDebug(\"Started appending thread\")\n+      openFile()\n+      val buf = new Array[Byte](bufferSize)\n+      var n = 0\n+      while (!markedForStop && n != -1) {\n+        n = inputStream.read(buf)\n+        if (n != -1) {\n+          appendToFile(buf, n)\n+        }\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Error writing stream to file $file\", e)\n+    } finally {\n+      closeFile()\n+      synchronized {\n+        stopped = true\n+        notifyAll()\n+      }\n+    }\n+  }\n+\n+  /** Append bytes to the file output stream */\n+  protected def appendToFile(bytes: Array[Byte], len: Int) {\n+    if (outputStream == null) {\n+      openFile()\n+    }\n+    outputStream.write(bytes, 0, len)\n+  }\n+\n+  /** Open the file output stream */\n+  protected def openFile() {\n+    outputStream = new FileOutputStream(file, true)\n+    logDebug(s\"Opened file $file\")\n+  }\n+\n+  /** Close the file output stream */\n+  protected def closeFile() {\n+    outputStream.flush()\n+    outputStream.close()\n+    logDebug(s\"Closed file $file\")\n+  }\n+}\n+\n+/**\n+ * Companion object to [[org.apache.log4j.FileAppender]] which has helper\n+ * functions to choose the write type of FileAppender based on SparkConf configuration.\n+ */\n+private[spark] object FileAppender extends Logging {\n+\n+  /** Create the right appender based on Spark configuration */\n+  def apply(inputStream: InputStream, file: File, conf: SparkConf): FileAppender = {\n+\n+    import RollingFileAppender._\n+\n+    val rolloverSizeOption = conf.getOption(SIZE_PROPERTY)\n+    val rolloverIntervalOption = conf.getOption(INTERVAL_PROPERTY)\n+\n+    (rolloverIntervalOption, rolloverSizeOption) match {\n+\n+      case (Some(rolloverInterval), Some(rolloverSize)) =>\n+        // if both size and interval have been set\n+        logWarning(s\"Rollover interval [$rolloverInterval] and size [$rolloverSize] \" +\n+          s\"both set for executor logs, rolling logs not enabled\")\n+        new FileAppender(inputStream, file)\n+\n+      case (Some(rolloverInterval), None) =>  // if interval has been set\n+        val validatedParams: Option[(Long, String)] = rolloverInterval match {\n+          case \"daily\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with daily rolling\")\n+            Some(24 * 60 * 60 * 1000L, s\"--YYYY-MM-dd\")\n+          case \"hourly\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with hourly rolling\")\n+            Some(60 * 60 * 1000L, s\"--YYYY-MM-dd--HH\")\n+          case \"minutely\" =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling every minute\")\n+            Some(60 * 1000L, s\"--YYYY-MM-dd--HH-mm\")\n+          case IntParam(seconds) =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling $seconds seconds\")\n+            Some(seconds * 1000L, s\"--YYYY-MM-dd--HH-mm-ss\")\n+          case _ =>\n+            logWarning(s\"Illegal interval for rolling executor logs [$rolloverInterval], \" +\n+                s\"rolling logs not enabled\")\n+            None\n+        }\n+        validatedParams.map {\n+          case (interval, pattern) =>\n+            new RollingFileAppender(\n+              inputStream, file, new TimeBasedRollingPolicy(interval),pattern, conf)\n+        }.getOrElse {\n+          new FileAppender(inputStream, file)\n+        }\n+\n+      case (None, Some(rolloverSize)) =>    // if size has been set\n+        rolloverSize match {\n+          case IntParam(bytes) =>\n+            logInfo(s\"Rolling executor logs enabled for $file with rolling every $bytes bytes\")\n+            new RollingFileAppender(inputStream, file, new SizeBasedRollingPolicy(bytes),\n+              s\"--YYYY-MM-dd--HH-mm-ss-SSSS\", conf)\n+          case _ =>\n+            logWarning(\n+              s\"Illegal size for rolling executor logs [$rolloverSize], \" +\n+                s\"rolling logs not enabled\")\n+            new FileAppender(inputStream, file)\n+        }\n+\n+      case (None, None) =>                // if neither size nor interval has been set\n+        new FileAppender(inputStream, file)\n+    }\n+  }\n+}\n+\n+/**\n+ * Defines the policy based on which [[org.apache.spark.util.RollingFileAppender]] will\n+ * generate rolling files.\n+ */\n+private[spark] trait RollingPolicy {\n+\n+  /** Whether rollover should be initiated at this moment */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean\n+\n+  /** Notify that rollover has occurred */\n+  def rolledOver()\n+\n+  /** Notify that bytes have been written */\n+  def bytesWritten(bytes: Long)\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] by which files will be rolled\n+ * over at a fixed interval.\n+ */\n+private[spark] class TimeBasedRollingPolicy(\n+    var rolloverIntervalMillis: Long,\n+    checkIntervalConstraint: Boolean = true   // set to false while testing\n+  ) extends RollingPolicy with Logging {\n+\n+  import TimeBasedRollingPolicy._\n+  if (checkIntervalConstraint && rolloverIntervalMillis < MINIMUM_INTERVAL_SECONDS * 1000L) {\n+    logWarning(s\"Rolling interval [${rolloverIntervalMillis/1000L} seconds] is too small. \" +\n+      s\"Setting the interval to the acceptable minimum of $MINIMUM_INTERVAL_SECONDS seconds.\")\n+    rolloverIntervalMillis = MINIMUM_INTERVAL_SECONDS * 1000L\n+  }\n+\n+  @volatile private var nextRolloverTime = calculateNextRolloverTime()\n+\n+  /** Should rollover if current time has exceeded next rollover time */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean = {\n+    System.currentTimeMillis > nextRolloverTime\n+  }\n+\n+  /** Rollover has occurred, so find the next time to rollover */\n+  def rolledOver() {\n+    nextRolloverTime = calculateNextRolloverTime()\n+    logDebug(s\"Current time: ${System.currentTimeMillis}, next rollover time: \" + nextRolloverTime)\n+  }\n+\n+  def bytesWritten(bytes: Long) { }  // nothing to do\n+\n+  private def calculateNextRolloverTime(): Long = {\n+    val now = System.currentTimeMillis()\n+    val targetTime = (\n+      math.ceil(now.toDouble / rolloverIntervalMillis) * rolloverIntervalMillis\n+    ).toLong\n+    logDebug(s\"Next rollover time is $targetTime\")\n+    targetTime\n+  }\n+}\n+\n+private[spark] object TimeBasedRollingPolicy {\n+  val MINIMUM_INTERVAL_SECONDS = 60L  // 1 minute\n+}\n+\n+/**\n+ * Defines a [[org.apache.spark.util.RollingPolicy]] by which files will be rolled\n+ * over after reaching a particular size.\n+ */\n+private[spark] class SizeBasedRollingPolicy(\n+    var rolloverSizeBytes: Long,\n+    checkSizeConstraint: Boolean = true     // set to false while testing\n+  ) extends RollingPolicy with Logging {\n+\n+  import SizeBasedRollingPolicy._\n+  if (checkSizeConstraint && rolloverSizeBytes < MINIMUM_SIZE_BYTES) {\n+    logWarning(s\"Rolling size [$rolloverSizeBytes bytes] is too small. \" +\n+      s\"Setting the size to the acceptable minimum of $MINIMUM_SIZE_BYTES bytes.\")\n+    rolloverSizeBytes = MINIMUM_SIZE_BYTES * 1000L\n+  }\n+\n+  @volatile private var bytesWrittenTillNow = 0L\n+\n+  /** Should rollover if the next set of bytes is going to exceed the size limit */\n+  def shouldRollover(bytesToBeWritten: Long): Boolean = {\n+    bytesToBeWritten + bytesWrittenTillNow > rolloverSizeBytes\n+  }\n+\n+  /** Rollover has occurred, so reset the counter */\n+  def rolledOver() {\n+    bytesWrittenTillNow = 0\n+  }\n+\n+  /** Increment the bytes that have been written in the current file */\n+  def bytesWritten(bytes: Long) {\n+    bytesWrittenTillNow += bytes\n+  }\n+}\n+\n+private[spark] object SizeBasedRollingPolicy {\n+  val MINIMUM_SIZE_BYTES = RollingFileAppender.DEFAULT_BUFFER_SIZE * 10\n+}\n+\n+\n+/**\n+ * Continuously appends data from input stream into the given file, and rolls\n+ * over the file after the given interval. The rolled over files are named\n+ * based on the given pattern.\n+ *\n+ * @param inputStream             Input stream to read data from\n+ * @param activeFile              File to write data to\n+ * @param rollingPolicy           Policy based on which files will be rolled over.\n+ * @param rollingFilePattern      Pattern based on which the rolled over files will be named.\n+ *                                Uses SimpleDataFormat pattern.\n+ * @param conf                    SparkConf that is used to pass on extra configurations\n+ * @param bufferSize              Optional buffer size. Used mainly for testing.\n+ */\n+private[spark] class RollingFileAppender(\n+    inputStream: InputStream,\n+    activeFile: File,\n+    val rollingPolicy: RollingPolicy,\n+    rollingFilePattern: String,\n+    conf: SparkConf,\n+    bufferSize: Int = DEFAULT_BUFFER_SIZE\n+  ) extends FileAppender(inputStream, activeFile, bufferSize) {"
  }],
  "prId": 895
}]