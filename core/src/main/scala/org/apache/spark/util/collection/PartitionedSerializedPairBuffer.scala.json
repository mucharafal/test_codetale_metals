[{
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "I wondered whether flushing so often like this might be an issue - but I think it should be fine since in Kryo flushing just does a simple write to the underlying stream, and the underlying stream here is your own thing that just stores data in memory.\n",
    "commit": "8c70dd909c20af6dacaf4091e964a1239a221016",
    "createdAt": "2015-04-22T15:15:32Z",
    "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.collection\n+\n+import java.io.InputStream\n+import java.nio.IntBuffer\n+import java.util.Comparator\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.serializer.{JavaSerializerInstance, SerializerInstance}\n+import org.apache.spark.storage.BlockObjectWriter\n+import org.apache.spark.util.collection.PartitionedSerializedPairBuffer._\n+\n+/**\n+ * Append-only buffer of key-value pairs, each with a corresponding partition ID, that serializes\n+ * its records upon insert and stores them as raw bytes.\n+ *\n+ * We use two data-structures to store the contents. The serialized records are stored in a\n+ * ChainedBuffer that can expand gracefully as records are added. This buffer is accompanied by a\n+ * metadata buffer that stores pointers into the data buffer as well as the partition ID of each\n+ * record. Each entry in the metadata buffer takes up a fixed amount of space.\n+ *\n+ * Sorting the collection means swapping entries in the metadata buffer - the record buffer need not\n+ * be modified at all. Storing the partition IDs in the metadata buffer means that comparisons can\n+ * happen without following any pointers, which should minimize cache misses.\n+ *\n+ * Currently, only sorting by partition is supported.\n+ *\n+ * @param metaInitialRecords The initial number of entries in the metadata buffer.\n+ * @param kvBlockSize The size of each byte buffer in the ChainedBuffer used to store the records.\n+ * @param serializerInstance the serializer used for serializing inserted records.\n+ */\n+private[spark] class PartitionedSerializedPairBuffer[K, V](\n+    metaInitialRecords: Int,\n+    kvBlockSize: Int,\n+    serializerInstance: SerializerInstance = SparkEnv.get.serializer.newInstance)\n+  extends WritablePartitionedPairCollection[K, V] {\n+\n+  if (serializerInstance.isInstanceOf[JavaSerializerInstance]) {\n+    throw new IllegalArgumentException(\"PartitionedSerializedPairBuffer does not support\" +\n+      \" Java-serialized objects.\")\n+  }\n+\n+  private var metaBuffer = IntBuffer.allocate(metaInitialRecords * NMETA)\n+\n+  private val kvBuffer: ChainedBuffer = new ChainedBuffer(kvBlockSize)\n+  private val kvOutputStream = new ChainedBufferOutputStream(kvBuffer)\n+  private val kvSerializationStream = serializerInstance.serializeStream(kvOutputStream)\n+\n+  def insert(partition: Int, key: K, value: V): Unit = {\n+    if (metaBuffer.position == metaBuffer.capacity) {\n+      growMetaBuffer()\n+    }\n+\n+    val keyStart = kvBuffer.size\n+    if (keyStart < 0) {\n+      throw new Exception(s\"Can't grow buffer beyond ${1 << 31} bytes\")\n+    }\n+    kvSerializationStream.writeObject[Any](key)\n+    kvSerializationStream.flush()",
    "line": 75
  }, {
    "author": {
      "login": "sryza"
    },
    "body": "I actually benchmarked this.  Results are in a comment on the JIRA and don't seem to indicate an issue.\n",
    "commit": "8c70dd909c20af6dacaf4091e964a1239a221016",
    "createdAt": "2015-04-22T16:58:28Z",
    "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.collection\n+\n+import java.io.InputStream\n+import java.nio.IntBuffer\n+import java.util.Comparator\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.serializer.{JavaSerializerInstance, SerializerInstance}\n+import org.apache.spark.storage.BlockObjectWriter\n+import org.apache.spark.util.collection.PartitionedSerializedPairBuffer._\n+\n+/**\n+ * Append-only buffer of key-value pairs, each with a corresponding partition ID, that serializes\n+ * its records upon insert and stores them as raw bytes.\n+ *\n+ * We use two data-structures to store the contents. The serialized records are stored in a\n+ * ChainedBuffer that can expand gracefully as records are added. This buffer is accompanied by a\n+ * metadata buffer that stores pointers into the data buffer as well as the partition ID of each\n+ * record. Each entry in the metadata buffer takes up a fixed amount of space.\n+ *\n+ * Sorting the collection means swapping entries in the metadata buffer - the record buffer need not\n+ * be modified at all. Storing the partition IDs in the metadata buffer means that comparisons can\n+ * happen without following any pointers, which should minimize cache misses.\n+ *\n+ * Currently, only sorting by partition is supported.\n+ *\n+ * @param metaInitialRecords The initial number of entries in the metadata buffer.\n+ * @param kvBlockSize The size of each byte buffer in the ChainedBuffer used to store the records.\n+ * @param serializerInstance the serializer used for serializing inserted records.\n+ */\n+private[spark] class PartitionedSerializedPairBuffer[K, V](\n+    metaInitialRecords: Int,\n+    kvBlockSize: Int,\n+    serializerInstance: SerializerInstance = SparkEnv.get.serializer.newInstance)\n+  extends WritablePartitionedPairCollection[K, V] {\n+\n+  if (serializerInstance.isInstanceOf[JavaSerializerInstance]) {\n+    throw new IllegalArgumentException(\"PartitionedSerializedPairBuffer does not support\" +\n+      \" Java-serialized objects.\")\n+  }\n+\n+  private var metaBuffer = IntBuffer.allocate(metaInitialRecords * NMETA)\n+\n+  private val kvBuffer: ChainedBuffer = new ChainedBuffer(kvBlockSize)\n+  private val kvOutputStream = new ChainedBufferOutputStream(kvBuffer)\n+  private val kvSerializationStream = serializerInstance.serializeStream(kvOutputStream)\n+\n+  def insert(partition: Int, key: K, value: V): Unit = {\n+    if (metaBuffer.position == metaBuffer.capacity) {\n+      growMetaBuffer()\n+    }\n+\n+    val keyStart = kvBuffer.size\n+    if (keyStart < 0) {\n+      throw new Exception(s\"Can't grow buffer beyond ${1 << 31} bytes\")\n+    }\n+    kvSerializationStream.writeObject[Any](key)\n+    kvSerializationStream.flush()",
    "line": 75
  }],
  "prId": 4450
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "Any reason not to just check for `metaBuffer.capacity.toLong * 2 > Integer.MAX_VALUE` - seems easier to reason about.\n",
    "commit": "8c70dd909c20af6dacaf4091e964a1239a221016",
    "createdAt": "2015-04-27T02:45:10Z",
    "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.collection\n+\n+import java.io.InputStream\n+import java.nio.IntBuffer\n+import java.util.Comparator\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.serializer.{JavaSerializerInstance, SerializerInstance}\n+import org.apache.spark.storage.BlockObjectWriter\n+import org.apache.spark.util.collection.PartitionedSerializedPairBuffer._\n+\n+/**\n+ * Append-only buffer of key-value pairs, each with a corresponding partition ID, that serializes\n+ * its records upon insert and stores them as raw bytes.\n+ *\n+ * We use two data-structures to store the contents. The serialized records are stored in a\n+ * ChainedBuffer that can expand gracefully as records are added. This buffer is accompanied by a\n+ * metadata buffer that stores pointers into the data buffer as well as the partition ID of each\n+ * record. Each entry in the metadata buffer takes up a fixed amount of space.\n+ *\n+ * Sorting the collection means swapping entries in the metadata buffer - the record buffer need not\n+ * be modified at all. Storing the partition IDs in the metadata buffer means that comparisons can\n+ * happen without following any pointers, which should minimize cache misses.\n+ *\n+ * Currently, only sorting by partition is supported.\n+ *\n+ * @param metaInitialRecords The initial number of entries in the metadata buffer.\n+ * @param kvBlockSize The size of each byte buffer in the ChainedBuffer used to store the records.\n+ * @param serializerInstance the serializer used for serializing inserted records.\n+ */\n+private[spark] class PartitionedSerializedPairBuffer[K, V](\n+    metaInitialRecords: Int,\n+    kvBlockSize: Int,\n+    serializerInstance: SerializerInstance = SparkEnv.get.serializer.newInstance)\n+  extends WritablePartitionedPairCollection[K, V] {\n+\n+  if (serializerInstance.isInstanceOf[JavaSerializerInstance]) {\n+    throw new IllegalArgumentException(\"PartitionedSerializedPairBuffer does not support\" +\n+      \" Java-serialized objects.\")\n+  }\n+\n+  private var metaBuffer = IntBuffer.allocate(metaInitialRecords * NMETA)\n+\n+  private val kvBuffer: ChainedBuffer = new ChainedBuffer(kvBlockSize)\n+  private val kvOutputStream = new ChainedBufferOutputStream(kvBuffer)\n+  private val kvSerializationStream = serializerInstance.serializeStream(kvOutputStream)\n+\n+  def insert(partition: Int, key: K, value: V): Unit = {\n+    if (metaBuffer.position == metaBuffer.capacity) {\n+      growMetaBuffer()\n+    }\n+\n+    val keyStart = kvBuffer.size\n+    if (keyStart < 0) {\n+      throw new Exception(s\"Can't grow buffer beyond ${1 << 31} bytes\")\n+    }\n+    kvSerializationStream.writeObject[Any](key)\n+    kvSerializationStream.flush()\n+    val valueStart = kvBuffer.size\n+    kvSerializationStream.writeObject[Any](value)\n+    kvSerializationStream.flush()\n+    val valueEnd = kvBuffer.size\n+\n+    metaBuffer.put(keyStart)\n+    metaBuffer.put(valueStart)\n+    metaBuffer.put(valueEnd)\n+    metaBuffer.put(partition)\n+  }\n+\n+  /** Double the size of the array because we've reached capacity */\n+  private def growMetaBuffer(): Unit = {\n+    if (metaBuffer.capacity * 4 >= (1 << 30)) {"
  }],
  "prId": 4450
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "Maybe `Integer.MAX_VALUE` would be better here? I think it's fine to just say the number of bytes.\n",
    "commit": "8c70dd909c20af6dacaf4091e964a1239a221016",
    "createdAt": "2015-04-27T02:45:39Z",
    "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.collection\n+\n+import java.io.InputStream\n+import java.nio.IntBuffer\n+import java.util.Comparator\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.serializer.{JavaSerializerInstance, SerializerInstance}\n+import org.apache.spark.storage.BlockObjectWriter\n+import org.apache.spark.util.collection.PartitionedSerializedPairBuffer._\n+\n+/**\n+ * Append-only buffer of key-value pairs, each with a corresponding partition ID, that serializes\n+ * its records upon insert and stores them as raw bytes.\n+ *\n+ * We use two data-structures to store the contents. The serialized records are stored in a\n+ * ChainedBuffer that can expand gracefully as records are added. This buffer is accompanied by a\n+ * metadata buffer that stores pointers into the data buffer as well as the partition ID of each\n+ * record. Each entry in the metadata buffer takes up a fixed amount of space.\n+ *\n+ * Sorting the collection means swapping entries in the metadata buffer - the record buffer need not\n+ * be modified at all. Storing the partition IDs in the metadata buffer means that comparisons can\n+ * happen without following any pointers, which should minimize cache misses.\n+ *\n+ * Currently, only sorting by partition is supported.\n+ *\n+ * @param metaInitialRecords The initial number of entries in the metadata buffer.\n+ * @param kvBlockSize The size of each byte buffer in the ChainedBuffer used to store the records.\n+ * @param serializerInstance the serializer used for serializing inserted records.\n+ */\n+private[spark] class PartitionedSerializedPairBuffer[K, V](\n+    metaInitialRecords: Int,\n+    kvBlockSize: Int,\n+    serializerInstance: SerializerInstance = SparkEnv.get.serializer.newInstance)\n+  extends WritablePartitionedPairCollection[K, V] {\n+\n+  if (serializerInstance.isInstanceOf[JavaSerializerInstance]) {\n+    throw new IllegalArgumentException(\"PartitionedSerializedPairBuffer does not support\" +\n+      \" Java-serialized objects.\")\n+  }\n+\n+  private var metaBuffer = IntBuffer.allocate(metaInitialRecords * NMETA)\n+\n+  private val kvBuffer: ChainedBuffer = new ChainedBuffer(kvBlockSize)\n+  private val kvOutputStream = new ChainedBufferOutputStream(kvBuffer)\n+  private val kvSerializationStream = serializerInstance.serializeStream(kvOutputStream)\n+\n+  def insert(partition: Int, key: K, value: V): Unit = {\n+    if (metaBuffer.position == metaBuffer.capacity) {\n+      growMetaBuffer()\n+    }\n+\n+    val keyStart = kvBuffer.size\n+    if (keyStart < 0) {\n+      throw new Exception(s\"Can't grow buffer beyond ${1 << 31} bytes\")\n+    }\n+    kvSerializationStream.writeObject[Any](key)\n+    kvSerializationStream.flush()\n+    val valueStart = kvBuffer.size\n+    kvSerializationStream.writeObject[Any](value)\n+    kvSerializationStream.flush()\n+    val valueEnd = kvBuffer.size\n+\n+    metaBuffer.put(keyStart)\n+    metaBuffer.put(valueStart)\n+    metaBuffer.put(valueEnd)\n+    metaBuffer.put(partition)\n+  }\n+\n+  /** Double the size of the array because we've reached capacity */\n+  private def growMetaBuffer(): Unit = {\n+    if (metaBuffer.capacity * 4 >= (1 << 30)) {\n+      // Doubling the capacity would create an array bigger than Int.MaxValue, so don't\n+      throw new Exception(\n+        s\"Can't grow buffer beyond ${(1 << 30) / (NMETA * 4)} elements\")"
  }],
  "prId": 4450
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "Might be nicer to call this `RECORD_SIZE` or something more clear. Also for clarity maybe you can just do:\n\n```\nval RECORD_SIZE = Seq(KEY_START, VAL_START, VAL_END, PARTITION).size\n```\n",
    "commit": "8c70dd909c20af6dacaf4091e964a1239a221016",
    "createdAt": "2015-04-27T02:54:36Z",
    "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.collection\n+\n+import java.io.InputStream\n+import java.nio.IntBuffer\n+import java.util.Comparator\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.serializer.{JavaSerializerInstance, SerializerInstance}\n+import org.apache.spark.storage.BlockObjectWriter\n+import org.apache.spark.util.collection.PartitionedSerializedPairBuffer._\n+\n+/**\n+ * Append-only buffer of key-value pairs, each with a corresponding partition ID, that serializes\n+ * its records upon insert and stores them as raw bytes.\n+ *\n+ * We use two data-structures to store the contents. The serialized records are stored in a\n+ * ChainedBuffer that can expand gracefully as records are added. This buffer is accompanied by a\n+ * metadata buffer that stores pointers into the data buffer as well as the partition ID of each\n+ * record. Each entry in the metadata buffer takes up a fixed amount of space.\n+ *\n+ * Sorting the collection means swapping entries in the metadata buffer - the record buffer need not\n+ * be modified at all. Storing the partition IDs in the metadata buffer means that comparisons can\n+ * happen without following any pointers, which should minimize cache misses.\n+ *\n+ * Currently, only sorting by partition is supported.\n+ *\n+ * @param metaInitialRecords The initial number of entries in the metadata buffer.\n+ * @param kvBlockSize The size of each byte buffer in the ChainedBuffer used to store the records.\n+ * @param serializerInstance the serializer used for serializing inserted records.\n+ */\n+private[spark] class PartitionedSerializedPairBuffer[K, V](\n+    metaInitialRecords: Int,\n+    kvBlockSize: Int,\n+    serializerInstance: SerializerInstance = SparkEnv.get.serializer.newInstance)\n+  extends WritablePartitionedPairCollection[K, V] {\n+\n+  if (serializerInstance.isInstanceOf[JavaSerializerInstance]) {\n+    throw new IllegalArgumentException(\"PartitionedSerializedPairBuffer does not support\" +\n+      \" Java-serialized objects.\")\n+  }\n+\n+  private var metaBuffer = IntBuffer.allocate(metaInitialRecords * NMETA)\n+\n+  private val kvBuffer: ChainedBuffer = new ChainedBuffer(kvBlockSize)\n+  private val kvOutputStream = new ChainedBufferOutputStream(kvBuffer)\n+  private val kvSerializationStream = serializerInstance.serializeStream(kvOutputStream)\n+\n+  def insert(partition: Int, key: K, value: V): Unit = {\n+    if (metaBuffer.position == metaBuffer.capacity) {\n+      growMetaBuffer()\n+    }\n+\n+    val keyStart = kvBuffer.size\n+    if (keyStart < 0) {\n+      throw new Exception(s\"Can't grow buffer beyond ${1 << 31} bytes\")\n+    }\n+    kvSerializationStream.writeObject[Any](key)\n+    kvSerializationStream.flush()\n+    val valueStart = kvBuffer.size\n+    kvSerializationStream.writeObject[Any](value)\n+    kvSerializationStream.flush()\n+    val valueEnd = kvBuffer.size\n+\n+    metaBuffer.put(keyStart)\n+    metaBuffer.put(valueStart)\n+    metaBuffer.put(valueEnd)\n+    metaBuffer.put(partition)\n+  }\n+\n+  /** Double the size of the array because we've reached capacity */\n+  private def growMetaBuffer(): Unit = {\n+    if (metaBuffer.capacity * 4 >= (1 << 30)) {\n+      // Doubling the capacity would create an array bigger than Int.MaxValue, so don't\n+      throw new Exception(\n+        s\"Can't grow buffer beyond ${(1 << 30) / (NMETA * 4)} elements\")\n+    }\n+    val newMetaBuffer = IntBuffer.allocate(metaBuffer.capacity * 2)\n+    newMetaBuffer.put(metaBuffer.array)\n+    metaBuffer = newMetaBuffer\n+  }\n+\n+  /** Iterate through the data in a given order. For this class this is not really destructive. */\n+  override def partitionedDestructiveSortedIterator(keyComparator: Comparator[K])\n+    : Iterator[((Int, K), V)] = {\n+    sort(keyComparator)\n+    val is = orderedInputStream\n+    val deserStream = serializerInstance.deserializeStream(is)\n+    new Iterator[((Int, K), V)] {\n+      var metaBufferPos = 0\n+      def hasNext: Boolean = metaBufferPos < metaBuffer.position\n+      def next(): ((Int, K), V) = {\n+        val key = deserStream.readObject[Any]().asInstanceOf[K]\n+        val value = deserStream.readObject[Any]().asInstanceOf[V]\n+        val partition = metaBuffer.get(metaBufferPos + PARTITION)\n+        metaBufferPos += NMETA\n+        ((partition, key), value)\n+      }\n+    }\n+  }\n+\n+  override def estimateSize: Long = metaBuffer.capacity * 4 + kvBuffer.capacity\n+\n+  override def destructiveSortedWritablePartitionedIterator(keyComparator: Comparator[K])\n+    : WritablePartitionedIterator = {\n+    sort(keyComparator)\n+    writablePartitionedIterator\n+  }\n+\n+  override def writablePartitionedIterator(): WritablePartitionedIterator = {\n+    new WritablePartitionedIterator {\n+      // current position in the meta buffer in ints\n+      var pos = 0\n+\n+      def writeNext(writer: BlockObjectWriter): Unit = {\n+        val keyStart = metaBuffer.get(pos + KEY_START)\n+        val valueEnd = metaBuffer.get(pos + VAL_END)\n+        pos += NMETA\n+        kvBuffer.read(keyStart, writer, valueEnd - keyStart)\n+      }\n+      def nextPartition(): Int = metaBuffer.get(pos + PARTITION)\n+      def hasNext(): Boolean = pos < metaBuffer.position\n+    }\n+  }\n+\n+  // Visible for testing\n+  def orderedInputStream: OrderedInputStream = {\n+    new OrderedInputStream(metaBuffer, kvBuffer)\n+  }\n+\n+  private def sort(keyComparator: Comparator[K]): Unit = {\n+    val comparator = if (keyComparator == null) {\n+      new Comparator[Int]() {\n+        def compare(partition1: Int, partition2: Int): Int = {\n+          partition1 - partition2\n+        }\n+      }\n+    } else {\n+      throw new UnsupportedOperationException()\n+    }\n+\n+    val sorter = new Sorter(new SerializedSortDataFormat)\n+    sorter.sort(metaBuffer, 0, metaBuffer.position / NMETA, comparator)\n+  }\n+}\n+\n+private[spark] class OrderedInputStream(metaBuffer: IntBuffer, kvBuffer: ChainedBuffer)\n+    extends InputStream {\n+\n+  private var metaBufferPos = 0\n+  private var kvBufferPos =\n+    if (metaBuffer.position > 0) metaBuffer.get(metaBufferPos + KEY_START) else 0\n+\n+  override def read(bytes: Array[Byte]): Int = read(bytes, 0, bytes.length)\n+\n+  override def read(bytes: Array[Byte], offs: Int, len: Int): Int = {\n+    if (metaBufferPos >= metaBuffer.position) {\n+      return -1\n+    }\n+    val bytesRemainingInRecord = metaBuffer.get(metaBufferPos + VAL_END) - kvBufferPos\n+    val toRead = math.min(bytesRemainingInRecord, len)\n+    kvBuffer.read(kvBufferPos, bytes, offs, toRead)\n+    if (toRead == bytesRemainingInRecord) {\n+      metaBufferPos += NMETA\n+      if (metaBufferPos < metaBuffer.position) {\n+        kvBufferPos = metaBuffer.get(metaBufferPos + KEY_START)\n+      }\n+    } else {\n+      kvBufferPos += toRead\n+    }\n+    toRead\n+  }\n+\n+  override def read(): Int = {\n+    throw new UnsupportedOperationException()\n+  }\n+}\n+\n+private[spark] class SerializedSortDataFormat extends SortDataFormat[Int, IntBuffer] {\n+\n+  private val META_BUFFER_TMP = new Array[Int](NMETA)\n+\n+  /** Return the sort key for the element at the given index. */\n+  override protected def getKey(metaBuffer: IntBuffer, pos: Int): Int = {\n+    metaBuffer.get(pos * NMETA + PARTITION)\n+  }\n+\n+  /** Swap two elements. */\n+  override def swap(metaBuffer: IntBuffer, pos0: Int, pos1: Int): Unit = {\n+    val iOff = pos0 * NMETA\n+    val jOff = pos1 * NMETA\n+    System.arraycopy(metaBuffer.array, iOff, META_BUFFER_TMP, 0, NMETA)\n+    System.arraycopy(metaBuffer.array, jOff, metaBuffer.array, iOff, NMETA)\n+    System.arraycopy(META_BUFFER_TMP, 0, metaBuffer.array, jOff, NMETA)\n+  }\n+\n+  /** Copy a single element from src(srcPos) to dst(dstPos). */\n+  override def copyElement(\n+      src: IntBuffer,\n+      srcPos: Int,\n+      dst: IntBuffer,\n+      dstPos: Int): Unit = {\n+    val srcOff = srcPos * NMETA\n+    val dstOff = dstPos * NMETA\n+    System.arraycopy(src.array, srcOff, dst.array, dstOff, NMETA)\n+  }\n+\n+  /**\n+   * Copy a range of elements starting at src(srcPos) to dst, starting at dstPos.\n+   * Overlapping ranges are allowed.\n+   */\n+  override def copyRange(\n+      src: IntBuffer,\n+      srcPos: Int,\n+      dst: IntBuffer,\n+      dstPos: Int,\n+      length: Int): Unit = {\n+    val srcOff = srcPos * NMETA\n+    val dstOff = dstPos * NMETA\n+    System.arraycopy(src.array, srcOff, dst.array, dstOff, NMETA * length)\n+  }\n+\n+  /**\n+   * Allocates a Buffer that can hold up to 'length' elements.\n+   * All elements of the buffer should be considered invalid until data is explicitly copied in.\n+   */\n+  override def allocate(length: Int): IntBuffer = {\n+    IntBuffer.allocate(length * NMETA)\n+  }\n+}\n+\n+private[spark] object PartitionedSerializedPairBuffer {\n+  val KEY_START = 0\n+  val VAL_START = 1\n+  val VAL_END = 2\n+  val PARTITION = 3\n+  val NMETA = 4 // num meta ints"
  }],
  "prId": 4450
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "Should this use NMETA (or RECORD_SIZE if you change it)?\n",
    "commit": "8c70dd909c20af6dacaf4091e964a1239a221016",
    "createdAt": "2015-04-27T02:57:51Z",
    "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.collection\n+\n+import java.io.InputStream\n+import java.nio.IntBuffer\n+import java.util.Comparator\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.serializer.{JavaSerializerInstance, SerializerInstance}\n+import org.apache.spark.storage.BlockObjectWriter\n+import org.apache.spark.util.collection.PartitionedSerializedPairBuffer._\n+\n+/**\n+ * Append-only buffer of key-value pairs, each with a corresponding partition ID, that serializes\n+ * its records upon insert and stores them as raw bytes.\n+ *\n+ * We use two data-structures to store the contents. The serialized records are stored in a\n+ * ChainedBuffer that can expand gracefully as records are added. This buffer is accompanied by a\n+ * metadata buffer that stores pointers into the data buffer as well as the partition ID of each\n+ * record. Each entry in the metadata buffer takes up a fixed amount of space.\n+ *\n+ * Sorting the collection means swapping entries in the metadata buffer - the record buffer need not\n+ * be modified at all. Storing the partition IDs in the metadata buffer means that comparisons can\n+ * happen without following any pointers, which should minimize cache misses.\n+ *\n+ * Currently, only sorting by partition is supported.\n+ *\n+ * @param metaInitialRecords The initial number of entries in the metadata buffer.\n+ * @param kvBlockSize The size of each byte buffer in the ChainedBuffer used to store the records.\n+ * @param serializerInstance the serializer used for serializing inserted records.\n+ */\n+private[spark] class PartitionedSerializedPairBuffer[K, V](\n+    metaInitialRecords: Int,\n+    kvBlockSize: Int,\n+    serializerInstance: SerializerInstance = SparkEnv.get.serializer.newInstance)\n+  extends WritablePartitionedPairCollection[K, V] {\n+\n+  if (serializerInstance.isInstanceOf[JavaSerializerInstance]) {\n+    throw new IllegalArgumentException(\"PartitionedSerializedPairBuffer does not support\" +\n+      \" Java-serialized objects.\")\n+  }\n+\n+  private var metaBuffer = IntBuffer.allocate(metaInitialRecords * NMETA)\n+\n+  private val kvBuffer: ChainedBuffer = new ChainedBuffer(kvBlockSize)\n+  private val kvOutputStream = new ChainedBufferOutputStream(kvBuffer)\n+  private val kvSerializationStream = serializerInstance.serializeStream(kvOutputStream)\n+\n+  def insert(partition: Int, key: K, value: V): Unit = {\n+    if (metaBuffer.position == metaBuffer.capacity) {\n+      growMetaBuffer()\n+    }\n+\n+    val keyStart = kvBuffer.size\n+    if (keyStart < 0) {\n+      throw new Exception(s\"Can't grow buffer beyond ${1 << 31} bytes\")\n+    }\n+    kvSerializationStream.writeObject[Any](key)\n+    kvSerializationStream.flush()\n+    val valueStart = kvBuffer.size\n+    kvSerializationStream.writeObject[Any](value)\n+    kvSerializationStream.flush()\n+    val valueEnd = kvBuffer.size\n+\n+    metaBuffer.put(keyStart)\n+    metaBuffer.put(valueStart)\n+    metaBuffer.put(valueEnd)\n+    metaBuffer.put(partition)\n+  }\n+\n+  /** Double the size of the array because we've reached capacity */\n+  private def growMetaBuffer(): Unit = {\n+    if (metaBuffer.capacity * 4 >= (1 << 30)) {\n+      // Doubling the capacity would create an array bigger than Int.MaxValue, so don't\n+      throw new Exception(\n+        s\"Can't grow buffer beyond ${(1 << 30) / (NMETA * 4)} elements\")\n+    }\n+    val newMetaBuffer = IntBuffer.allocate(metaBuffer.capacity * 2)\n+    newMetaBuffer.put(metaBuffer.array)\n+    metaBuffer = newMetaBuffer\n+  }\n+\n+  /** Iterate through the data in a given order. For this class this is not really destructive. */\n+  override def partitionedDestructiveSortedIterator(keyComparator: Comparator[K])\n+    : Iterator[((Int, K), V)] = {\n+    sort(keyComparator)\n+    val is = orderedInputStream\n+    val deserStream = serializerInstance.deserializeStream(is)\n+    new Iterator[((Int, K), V)] {\n+      var metaBufferPos = 0\n+      def hasNext: Boolean = metaBufferPos < metaBuffer.position\n+      def next(): ((Int, K), V) = {\n+        val key = deserStream.readObject[Any]().asInstanceOf[K]\n+        val value = deserStream.readObject[Any]().asInstanceOf[V]\n+        val partition = metaBuffer.get(metaBufferPos + PARTITION)\n+        metaBufferPos += NMETA\n+        ((partition, key), value)\n+      }\n+    }\n+  }\n+\n+  override def estimateSize: Long = metaBuffer.capacity * 4 + kvBuffer.capacity",
    "line": 117
  }, {
    "author": {
      "login": "sryza"
    },
    "body": "metaBuffer is an IntBuffer, so 4 is there because it's the number of bytes in an integer.  It shouldn't change if NMETA changes.\n",
    "commit": "8c70dd909c20af6dacaf4091e964a1239a221016",
    "createdAt": "2015-04-27T21:11:09Z",
    "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.collection\n+\n+import java.io.InputStream\n+import java.nio.IntBuffer\n+import java.util.Comparator\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.serializer.{JavaSerializerInstance, SerializerInstance}\n+import org.apache.spark.storage.BlockObjectWriter\n+import org.apache.spark.util.collection.PartitionedSerializedPairBuffer._\n+\n+/**\n+ * Append-only buffer of key-value pairs, each with a corresponding partition ID, that serializes\n+ * its records upon insert and stores them as raw bytes.\n+ *\n+ * We use two data-structures to store the contents. The serialized records are stored in a\n+ * ChainedBuffer that can expand gracefully as records are added. This buffer is accompanied by a\n+ * metadata buffer that stores pointers into the data buffer as well as the partition ID of each\n+ * record. Each entry in the metadata buffer takes up a fixed amount of space.\n+ *\n+ * Sorting the collection means swapping entries in the metadata buffer - the record buffer need not\n+ * be modified at all. Storing the partition IDs in the metadata buffer means that comparisons can\n+ * happen without following any pointers, which should minimize cache misses.\n+ *\n+ * Currently, only sorting by partition is supported.\n+ *\n+ * @param metaInitialRecords The initial number of entries in the metadata buffer.\n+ * @param kvBlockSize The size of each byte buffer in the ChainedBuffer used to store the records.\n+ * @param serializerInstance the serializer used for serializing inserted records.\n+ */\n+private[spark] class PartitionedSerializedPairBuffer[K, V](\n+    metaInitialRecords: Int,\n+    kvBlockSize: Int,\n+    serializerInstance: SerializerInstance = SparkEnv.get.serializer.newInstance)\n+  extends WritablePartitionedPairCollection[K, V] {\n+\n+  if (serializerInstance.isInstanceOf[JavaSerializerInstance]) {\n+    throw new IllegalArgumentException(\"PartitionedSerializedPairBuffer does not support\" +\n+      \" Java-serialized objects.\")\n+  }\n+\n+  private var metaBuffer = IntBuffer.allocate(metaInitialRecords * NMETA)\n+\n+  private val kvBuffer: ChainedBuffer = new ChainedBuffer(kvBlockSize)\n+  private val kvOutputStream = new ChainedBufferOutputStream(kvBuffer)\n+  private val kvSerializationStream = serializerInstance.serializeStream(kvOutputStream)\n+\n+  def insert(partition: Int, key: K, value: V): Unit = {\n+    if (metaBuffer.position == metaBuffer.capacity) {\n+      growMetaBuffer()\n+    }\n+\n+    val keyStart = kvBuffer.size\n+    if (keyStart < 0) {\n+      throw new Exception(s\"Can't grow buffer beyond ${1 << 31} bytes\")\n+    }\n+    kvSerializationStream.writeObject[Any](key)\n+    kvSerializationStream.flush()\n+    val valueStart = kvBuffer.size\n+    kvSerializationStream.writeObject[Any](value)\n+    kvSerializationStream.flush()\n+    val valueEnd = kvBuffer.size\n+\n+    metaBuffer.put(keyStart)\n+    metaBuffer.put(valueStart)\n+    metaBuffer.put(valueEnd)\n+    metaBuffer.put(partition)\n+  }\n+\n+  /** Double the size of the array because we've reached capacity */\n+  private def growMetaBuffer(): Unit = {\n+    if (metaBuffer.capacity * 4 >= (1 << 30)) {\n+      // Doubling the capacity would create an array bigger than Int.MaxValue, so don't\n+      throw new Exception(\n+        s\"Can't grow buffer beyond ${(1 << 30) / (NMETA * 4)} elements\")\n+    }\n+    val newMetaBuffer = IntBuffer.allocate(metaBuffer.capacity * 2)\n+    newMetaBuffer.put(metaBuffer.array)\n+    metaBuffer = newMetaBuffer\n+  }\n+\n+  /** Iterate through the data in a given order. For this class this is not really destructive. */\n+  override def partitionedDestructiveSortedIterator(keyComparator: Comparator[K])\n+    : Iterator[((Int, K), V)] = {\n+    sort(keyComparator)\n+    val is = orderedInputStream\n+    val deserStream = serializerInstance.deserializeStream(is)\n+    new Iterator[((Int, K), V)] {\n+      var metaBufferPos = 0\n+      def hasNext: Boolean = metaBufferPos < metaBuffer.position\n+      def next(): ((Int, K), V) = {\n+        val key = deserStream.readObject[Any]().asInstanceOf[K]\n+        val value = deserStream.readObject[Any]().asInstanceOf[V]\n+        val partition = metaBuffer.get(metaBufferPos + PARTITION)\n+        metaBufferPos += NMETA\n+        ((partition, key), value)\n+      }\n+    }\n+  }\n+\n+  override def estimateSize: Long = metaBuffer.capacity * 4 + kvBuffer.capacity",
    "line": 117
  }, {
    "author": {
      "login": "pwendell"
    },
    "body": "Oh right - got it\n",
    "commit": "8c70dd909c20af6dacaf4091e964a1239a221016",
    "createdAt": "2015-04-28T00:56:13Z",
    "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.collection\n+\n+import java.io.InputStream\n+import java.nio.IntBuffer\n+import java.util.Comparator\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.serializer.{JavaSerializerInstance, SerializerInstance}\n+import org.apache.spark.storage.BlockObjectWriter\n+import org.apache.spark.util.collection.PartitionedSerializedPairBuffer._\n+\n+/**\n+ * Append-only buffer of key-value pairs, each with a corresponding partition ID, that serializes\n+ * its records upon insert and stores them as raw bytes.\n+ *\n+ * We use two data-structures to store the contents. The serialized records are stored in a\n+ * ChainedBuffer that can expand gracefully as records are added. This buffer is accompanied by a\n+ * metadata buffer that stores pointers into the data buffer as well as the partition ID of each\n+ * record. Each entry in the metadata buffer takes up a fixed amount of space.\n+ *\n+ * Sorting the collection means swapping entries in the metadata buffer - the record buffer need not\n+ * be modified at all. Storing the partition IDs in the metadata buffer means that comparisons can\n+ * happen without following any pointers, which should minimize cache misses.\n+ *\n+ * Currently, only sorting by partition is supported.\n+ *\n+ * @param metaInitialRecords The initial number of entries in the metadata buffer.\n+ * @param kvBlockSize The size of each byte buffer in the ChainedBuffer used to store the records.\n+ * @param serializerInstance the serializer used for serializing inserted records.\n+ */\n+private[spark] class PartitionedSerializedPairBuffer[K, V](\n+    metaInitialRecords: Int,\n+    kvBlockSize: Int,\n+    serializerInstance: SerializerInstance = SparkEnv.get.serializer.newInstance)\n+  extends WritablePartitionedPairCollection[K, V] {\n+\n+  if (serializerInstance.isInstanceOf[JavaSerializerInstance]) {\n+    throw new IllegalArgumentException(\"PartitionedSerializedPairBuffer does not support\" +\n+      \" Java-serialized objects.\")\n+  }\n+\n+  private var metaBuffer = IntBuffer.allocate(metaInitialRecords * NMETA)\n+\n+  private val kvBuffer: ChainedBuffer = new ChainedBuffer(kvBlockSize)\n+  private val kvOutputStream = new ChainedBufferOutputStream(kvBuffer)\n+  private val kvSerializationStream = serializerInstance.serializeStream(kvOutputStream)\n+\n+  def insert(partition: Int, key: K, value: V): Unit = {\n+    if (metaBuffer.position == metaBuffer.capacity) {\n+      growMetaBuffer()\n+    }\n+\n+    val keyStart = kvBuffer.size\n+    if (keyStart < 0) {\n+      throw new Exception(s\"Can't grow buffer beyond ${1 << 31} bytes\")\n+    }\n+    kvSerializationStream.writeObject[Any](key)\n+    kvSerializationStream.flush()\n+    val valueStart = kvBuffer.size\n+    kvSerializationStream.writeObject[Any](value)\n+    kvSerializationStream.flush()\n+    val valueEnd = kvBuffer.size\n+\n+    metaBuffer.put(keyStart)\n+    metaBuffer.put(valueStart)\n+    metaBuffer.put(valueEnd)\n+    metaBuffer.put(partition)\n+  }\n+\n+  /** Double the size of the array because we've reached capacity */\n+  private def growMetaBuffer(): Unit = {\n+    if (metaBuffer.capacity * 4 >= (1 << 30)) {\n+      // Doubling the capacity would create an array bigger than Int.MaxValue, so don't\n+      throw new Exception(\n+        s\"Can't grow buffer beyond ${(1 << 30) / (NMETA * 4)} elements\")\n+    }\n+    val newMetaBuffer = IntBuffer.allocate(metaBuffer.capacity * 2)\n+    newMetaBuffer.put(metaBuffer.array)\n+    metaBuffer = newMetaBuffer\n+  }\n+\n+  /** Iterate through the data in a given order. For this class this is not really destructive. */\n+  override def partitionedDestructiveSortedIterator(keyComparator: Comparator[K])\n+    : Iterator[((Int, K), V)] = {\n+    sort(keyComparator)\n+    val is = orderedInputStream\n+    val deserStream = serializerInstance.deserializeStream(is)\n+    new Iterator[((Int, K), V)] {\n+      var metaBufferPos = 0\n+      def hasNext: Boolean = metaBufferPos < metaBuffer.position\n+      def next(): ((Int, K), V) = {\n+        val key = deserStream.readObject[Any]().asInstanceOf[K]\n+        val value = deserStream.readObject[Any]().asInstanceOf[V]\n+        val partition = metaBuffer.get(metaBufferPos + PARTITION)\n+        metaBufferPos += NMETA\n+        ((partition, key), value)\n+      }\n+    }\n+  }\n+\n+  override def estimateSize: Long = metaBuffer.capacity * 4 + kvBuffer.capacity",
    "line": 117
  }],
  "prId": 4450
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "This patch introduced `writeKey` and `writeValue`, but it looks like this call and the one below it are still using `writeObject`.  Should we change this to use the new APIs instead?\n",
    "commit": "8c70dd909c20af6dacaf4091e964a1239a221016",
    "createdAt": "2015-05-03T20:33:46Z",
    "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.collection\n+\n+import java.io.InputStream\n+import java.nio.IntBuffer\n+import java.util.Comparator\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.serializer.{JavaSerializerInstance, SerializerInstance}\n+import org.apache.spark.storage.BlockObjectWriter\n+import org.apache.spark.util.collection.PartitionedSerializedPairBuffer._\n+\n+/**\n+ * Append-only buffer of key-value pairs, each with a corresponding partition ID, that serializes\n+ * its records upon insert and stores them as raw bytes.\n+ *\n+ * We use two data-structures to store the contents. The serialized records are stored in a\n+ * ChainedBuffer that can expand gracefully as records are added. This buffer is accompanied by a\n+ * metadata buffer that stores pointers into the data buffer as well as the partition ID of each\n+ * record. Each entry in the metadata buffer takes up a fixed amount of space.\n+ *\n+ * Sorting the collection means swapping entries in the metadata buffer - the record buffer need not\n+ * be modified at all. Storing the partition IDs in the metadata buffer means that comparisons can\n+ * happen without following any pointers, which should minimize cache misses.\n+ *\n+ * Currently, only sorting by partition is supported.\n+ *\n+ * @param metaInitialRecords The initial number of entries in the metadata buffer.\n+ * @param kvBlockSize The size of each byte buffer in the ChainedBuffer used to store the records.\n+ * @param serializerInstance the serializer used for serializing inserted records.\n+ */\n+private[spark] class PartitionedSerializedPairBuffer[K, V](\n+    metaInitialRecords: Int,\n+    kvBlockSize: Int,\n+    serializerInstance: SerializerInstance)\n+  extends WritablePartitionedPairCollection[K, V] with SizeTracker {\n+\n+  if (serializerInstance.isInstanceOf[JavaSerializerInstance]) {\n+    throw new IllegalArgumentException(\"PartitionedSerializedPairBuffer does not support\" +\n+      \" Java-serialized objects.\")\n+  }\n+\n+  private var metaBuffer = IntBuffer.allocate(metaInitialRecords * RECORD_SIZE)\n+\n+  private val kvBuffer: ChainedBuffer = new ChainedBuffer(kvBlockSize)\n+  private val kvOutputStream = new ChainedBufferOutputStream(kvBuffer)\n+  private val kvSerializationStream = serializerInstance.serializeStream(kvOutputStream)\n+\n+  def insert(partition: Int, key: K, value: V): Unit = {\n+    if (metaBuffer.position == metaBuffer.capacity) {\n+      growMetaBuffer()\n+    }\n+\n+    val keyStart = kvBuffer.size\n+    if (keyStart < 0) {\n+      throw new Exception(s\"Can't grow buffer beyond ${1 << 31} bytes\")\n+    }\n+    kvSerializationStream.writeObject[Any](key)",
    "line": 74
  }],
  "prId": 4450
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "This block of code was a bit difficult for me to understand because we only seem to pass in the keyComparator in order to verify that it's empty.  I think that it would be clearer to throw an `UnsupportedOperationException` or `IllegalArgumentException` higher up the call stack (e.g. in `partitionedDestructiveSortedIterator`).\n",
    "commit": "8c70dd909c20af6dacaf4091e964a1239a221016",
    "createdAt": "2015-05-25T06:13:12Z",
    "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.collection\n+\n+import java.io.InputStream\n+import java.nio.IntBuffer\n+import java.util.Comparator\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.serializer.{JavaSerializerInstance, SerializerInstance}\n+import org.apache.spark.storage.BlockObjectWriter\n+import org.apache.spark.util.collection.PartitionedSerializedPairBuffer._\n+\n+/**\n+ * Append-only buffer of key-value pairs, each with a corresponding partition ID, that serializes\n+ * its records upon insert and stores them as raw bytes.\n+ *\n+ * We use two data-structures to store the contents. The serialized records are stored in a\n+ * ChainedBuffer that can expand gracefully as records are added. This buffer is accompanied by a\n+ * metadata buffer that stores pointers into the data buffer as well as the partition ID of each\n+ * record. Each entry in the metadata buffer takes up a fixed amount of space.\n+ *\n+ * Sorting the collection means swapping entries in the metadata buffer - the record buffer need not\n+ * be modified at all. Storing the partition IDs in the metadata buffer means that comparisons can\n+ * happen without following any pointers, which should minimize cache misses.\n+ *\n+ * Currently, only sorting by partition is supported.\n+ *\n+ * @param metaInitialRecords The initial number of entries in the metadata buffer.\n+ * @param kvBlockSize The size of each byte buffer in the ChainedBuffer used to store the records.\n+ * @param serializerInstance the serializer used for serializing inserted records.\n+ */\n+private[spark] class PartitionedSerializedPairBuffer[K, V](\n+    metaInitialRecords: Int,\n+    kvBlockSize: Int,\n+    serializerInstance: SerializerInstance)\n+  extends WritablePartitionedPairCollection[K, V] with SizeTracker {\n+\n+  if (serializerInstance.isInstanceOf[JavaSerializerInstance]) {\n+    throw new IllegalArgumentException(\"PartitionedSerializedPairBuffer does not support\" +\n+      \" Java-serialized objects.\")\n+  }\n+\n+  private var metaBuffer = IntBuffer.allocate(metaInitialRecords * RECORD_SIZE)\n+\n+  private val kvBuffer: ChainedBuffer = new ChainedBuffer(kvBlockSize)\n+  private val kvOutputStream = new ChainedBufferOutputStream(kvBuffer)\n+  private val kvSerializationStream = serializerInstance.serializeStream(kvOutputStream)\n+\n+  def insert(partition: Int, key: K, value: V): Unit = {\n+    if (metaBuffer.position == metaBuffer.capacity) {\n+      growMetaBuffer()\n+    }\n+\n+    val keyStart = kvBuffer.size\n+    if (keyStart < 0) {\n+      throw new Exception(s\"Can't grow buffer beyond ${1 << 31} bytes\")\n+    }\n+    kvSerializationStream.writeObject[Any](key)\n+    kvSerializationStream.flush()\n+    val valueStart = kvBuffer.size\n+    kvSerializationStream.writeObject[Any](value)\n+    kvSerializationStream.flush()\n+    val valueEnd = kvBuffer.size\n+\n+    metaBuffer.put(keyStart)\n+    metaBuffer.put(valueStart)\n+    metaBuffer.put(valueEnd)\n+    metaBuffer.put(partition)\n+  }\n+\n+  /** Double the size of the array because we've reached capacity */\n+  private def growMetaBuffer(): Unit = {\n+    if (metaBuffer.capacity.toLong * 2 > Int.MaxValue) {\n+      // Doubling the capacity would create an array bigger than Int.MaxValue, so don't\n+      throw new Exception(s\"Can't grow buffer beyond ${Int.MaxValue} bytes\")\n+    }\n+    val newMetaBuffer = IntBuffer.allocate(metaBuffer.capacity * 2)\n+    newMetaBuffer.put(metaBuffer.array)\n+    metaBuffer = newMetaBuffer\n+  }\n+\n+  /** Iterate through the data in a given order. For this class this is not really destructive. */\n+  override def partitionedDestructiveSortedIterator(keyComparator: Option[Comparator[K]])\n+    : Iterator[((Int, K), V)] = {\n+    sort(keyComparator)\n+    val is = orderedInputStream\n+    val deserStream = serializerInstance.deserializeStream(is)\n+    new Iterator[((Int, K), V)] {\n+      var metaBufferPos = 0\n+      def hasNext: Boolean = metaBufferPos < metaBuffer.position\n+      def next(): ((Int, K), V) = {\n+        val key = deserStream.readKey[Any]().asInstanceOf[K]\n+        val value = deserStream.readValue[Any]().asInstanceOf[V]\n+        val partition = metaBuffer.get(metaBufferPos + PARTITION)\n+        metaBufferPos += RECORD_SIZE\n+        ((partition, key), value)\n+      }\n+    }\n+  }\n+\n+  override def estimateSize: Long = metaBuffer.capacity * 4 + kvBuffer.capacity\n+\n+  override def destructiveSortedWritablePartitionedIterator(keyComparator: Option[Comparator[K]])\n+    : WritablePartitionedIterator = {\n+    sort(keyComparator)\n+    writablePartitionedIterator\n+  }\n+\n+  override def writablePartitionedIterator(): WritablePartitionedIterator = {\n+    new WritablePartitionedIterator {\n+      // current position in the meta buffer in ints\n+      var pos = 0\n+\n+      def writeNext(writer: BlockObjectWriter): Unit = {\n+        val keyStart = metaBuffer.get(pos + KEY_START)\n+        val valueEnd = metaBuffer.get(pos + VAL_END)\n+        pos += RECORD_SIZE\n+        kvBuffer.read(keyStart, writer, valueEnd - keyStart)\n+        writer.recordWritten()\n+      }\n+      def nextPartition(): Int = metaBuffer.get(pos + PARTITION)\n+      def hasNext(): Boolean = pos < metaBuffer.position\n+    }\n+  }\n+\n+  // Visible for testing\n+  def orderedInputStream: OrderedInputStream = {\n+    new OrderedInputStream(metaBuffer, kvBuffer)\n+  }\n+\n+  private def sort(keyComparator: Option[Comparator[K]]): Unit = {\n+    val comparator = if (keyComparator.isEmpty) {",
    "line": 148
  }],
  "prId": 4450
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "Why is this sort call here and not at the beginning of `writablePartitionedIterator`?  Don't we also need to sort when calling `writablePartitionedIterator()` directly, since otherwise the data won't be partitioned?\n",
    "commit": "8c70dd909c20af6dacaf4091e964a1239a221016",
    "createdAt": "2015-05-25T06:26:41Z",
    "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.collection\n+\n+import java.io.InputStream\n+import java.nio.IntBuffer\n+import java.util.Comparator\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.serializer.{JavaSerializerInstance, SerializerInstance}\n+import org.apache.spark.storage.BlockObjectWriter\n+import org.apache.spark.util.collection.PartitionedSerializedPairBuffer._\n+\n+/**\n+ * Append-only buffer of key-value pairs, each with a corresponding partition ID, that serializes\n+ * its records upon insert and stores them as raw bytes.\n+ *\n+ * We use two data-structures to store the contents. The serialized records are stored in a\n+ * ChainedBuffer that can expand gracefully as records are added. This buffer is accompanied by a\n+ * metadata buffer that stores pointers into the data buffer as well as the partition ID of each\n+ * record. Each entry in the metadata buffer takes up a fixed amount of space.\n+ *\n+ * Sorting the collection means swapping entries in the metadata buffer - the record buffer need not\n+ * be modified at all. Storing the partition IDs in the metadata buffer means that comparisons can\n+ * happen without following any pointers, which should minimize cache misses.\n+ *\n+ * Currently, only sorting by partition is supported.\n+ *\n+ * @param metaInitialRecords The initial number of entries in the metadata buffer.\n+ * @param kvBlockSize The size of each byte buffer in the ChainedBuffer used to store the records.\n+ * @param serializerInstance the serializer used for serializing inserted records.\n+ */\n+private[spark] class PartitionedSerializedPairBuffer[K, V](\n+    metaInitialRecords: Int,\n+    kvBlockSize: Int,\n+    serializerInstance: SerializerInstance)\n+  extends WritablePartitionedPairCollection[K, V] with SizeTracker {\n+\n+  if (serializerInstance.isInstanceOf[JavaSerializerInstance]) {\n+    throw new IllegalArgumentException(\"PartitionedSerializedPairBuffer does not support\" +\n+      \" Java-serialized objects.\")\n+  }\n+\n+  private var metaBuffer = IntBuffer.allocate(metaInitialRecords * RECORD_SIZE)\n+\n+  private val kvBuffer: ChainedBuffer = new ChainedBuffer(kvBlockSize)\n+  private val kvOutputStream = new ChainedBufferOutputStream(kvBuffer)\n+  private val kvSerializationStream = serializerInstance.serializeStream(kvOutputStream)\n+\n+  def insert(partition: Int, key: K, value: V): Unit = {\n+    if (metaBuffer.position == metaBuffer.capacity) {\n+      growMetaBuffer()\n+    }\n+\n+    val keyStart = kvBuffer.size\n+    if (keyStart < 0) {\n+      throw new Exception(s\"Can't grow buffer beyond ${1 << 31} bytes\")\n+    }\n+    kvSerializationStream.writeObject[Any](key)\n+    kvSerializationStream.flush()\n+    val valueStart = kvBuffer.size\n+    kvSerializationStream.writeObject[Any](value)\n+    kvSerializationStream.flush()\n+    val valueEnd = kvBuffer.size\n+\n+    metaBuffer.put(keyStart)\n+    metaBuffer.put(valueStart)\n+    metaBuffer.put(valueEnd)\n+    metaBuffer.put(partition)\n+  }\n+\n+  /** Double the size of the array because we've reached capacity */\n+  private def growMetaBuffer(): Unit = {\n+    if (metaBuffer.capacity.toLong * 2 > Int.MaxValue) {\n+      // Doubling the capacity would create an array bigger than Int.MaxValue, so don't\n+      throw new Exception(s\"Can't grow buffer beyond ${Int.MaxValue} bytes\")\n+    }\n+    val newMetaBuffer = IntBuffer.allocate(metaBuffer.capacity * 2)\n+    newMetaBuffer.put(metaBuffer.array)\n+    metaBuffer = newMetaBuffer\n+  }\n+\n+  /** Iterate through the data in a given order. For this class this is not really destructive. */\n+  override def partitionedDestructiveSortedIterator(keyComparator: Option[Comparator[K]])\n+    : Iterator[((Int, K), V)] = {\n+    sort(keyComparator)\n+    val is = orderedInputStream\n+    val deserStream = serializerInstance.deserializeStream(is)\n+    new Iterator[((Int, K), V)] {\n+      var metaBufferPos = 0\n+      def hasNext: Boolean = metaBufferPos < metaBuffer.position\n+      def next(): ((Int, K), V) = {\n+        val key = deserStream.readKey[Any]().asInstanceOf[K]\n+        val value = deserStream.readValue[Any]().asInstanceOf[V]\n+        val partition = metaBuffer.get(metaBufferPos + PARTITION)\n+        metaBufferPos += RECORD_SIZE\n+        ((partition, key), value)\n+      }\n+    }\n+  }\n+\n+  override def estimateSize: Long = metaBuffer.capacity * 4 + kvBuffer.capacity\n+\n+  override def destructiveSortedWritablePartitionedIterator(keyComparator: Option[Comparator[K]])\n+    : WritablePartitionedIterator = {\n+    sort(keyComparator)",
    "line": 121
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "Ah, I see now: the iterator itself isn't _grouped_ by partition; the \"partitioned\" in its name only refers to the fact that each element of the iterator carries an associated partition id.  I wonder whether there's some alternative naming scheme that would avoid this confusion.\n",
    "commit": "8c70dd909c20af6dacaf4091e964a1239a221016",
    "createdAt": "2015-05-25T06:32:00Z",
    "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.collection\n+\n+import java.io.InputStream\n+import java.nio.IntBuffer\n+import java.util.Comparator\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.serializer.{JavaSerializerInstance, SerializerInstance}\n+import org.apache.spark.storage.BlockObjectWriter\n+import org.apache.spark.util.collection.PartitionedSerializedPairBuffer._\n+\n+/**\n+ * Append-only buffer of key-value pairs, each with a corresponding partition ID, that serializes\n+ * its records upon insert and stores them as raw bytes.\n+ *\n+ * We use two data-structures to store the contents. The serialized records are stored in a\n+ * ChainedBuffer that can expand gracefully as records are added. This buffer is accompanied by a\n+ * metadata buffer that stores pointers into the data buffer as well as the partition ID of each\n+ * record. Each entry in the metadata buffer takes up a fixed amount of space.\n+ *\n+ * Sorting the collection means swapping entries in the metadata buffer - the record buffer need not\n+ * be modified at all. Storing the partition IDs in the metadata buffer means that comparisons can\n+ * happen without following any pointers, which should minimize cache misses.\n+ *\n+ * Currently, only sorting by partition is supported.\n+ *\n+ * @param metaInitialRecords The initial number of entries in the metadata buffer.\n+ * @param kvBlockSize The size of each byte buffer in the ChainedBuffer used to store the records.\n+ * @param serializerInstance the serializer used for serializing inserted records.\n+ */\n+private[spark] class PartitionedSerializedPairBuffer[K, V](\n+    metaInitialRecords: Int,\n+    kvBlockSize: Int,\n+    serializerInstance: SerializerInstance)\n+  extends WritablePartitionedPairCollection[K, V] with SizeTracker {\n+\n+  if (serializerInstance.isInstanceOf[JavaSerializerInstance]) {\n+    throw new IllegalArgumentException(\"PartitionedSerializedPairBuffer does not support\" +\n+      \" Java-serialized objects.\")\n+  }\n+\n+  private var metaBuffer = IntBuffer.allocate(metaInitialRecords * RECORD_SIZE)\n+\n+  private val kvBuffer: ChainedBuffer = new ChainedBuffer(kvBlockSize)\n+  private val kvOutputStream = new ChainedBufferOutputStream(kvBuffer)\n+  private val kvSerializationStream = serializerInstance.serializeStream(kvOutputStream)\n+\n+  def insert(partition: Int, key: K, value: V): Unit = {\n+    if (metaBuffer.position == metaBuffer.capacity) {\n+      growMetaBuffer()\n+    }\n+\n+    val keyStart = kvBuffer.size\n+    if (keyStart < 0) {\n+      throw new Exception(s\"Can't grow buffer beyond ${1 << 31} bytes\")\n+    }\n+    kvSerializationStream.writeObject[Any](key)\n+    kvSerializationStream.flush()\n+    val valueStart = kvBuffer.size\n+    kvSerializationStream.writeObject[Any](value)\n+    kvSerializationStream.flush()\n+    val valueEnd = kvBuffer.size\n+\n+    metaBuffer.put(keyStart)\n+    metaBuffer.put(valueStart)\n+    metaBuffer.put(valueEnd)\n+    metaBuffer.put(partition)\n+  }\n+\n+  /** Double the size of the array because we've reached capacity */\n+  private def growMetaBuffer(): Unit = {\n+    if (metaBuffer.capacity.toLong * 2 > Int.MaxValue) {\n+      // Doubling the capacity would create an array bigger than Int.MaxValue, so don't\n+      throw new Exception(s\"Can't grow buffer beyond ${Int.MaxValue} bytes\")\n+    }\n+    val newMetaBuffer = IntBuffer.allocate(metaBuffer.capacity * 2)\n+    newMetaBuffer.put(metaBuffer.array)\n+    metaBuffer = newMetaBuffer\n+  }\n+\n+  /** Iterate through the data in a given order. For this class this is not really destructive. */\n+  override def partitionedDestructiveSortedIterator(keyComparator: Option[Comparator[K]])\n+    : Iterator[((Int, K), V)] = {\n+    sort(keyComparator)\n+    val is = orderedInputStream\n+    val deserStream = serializerInstance.deserializeStream(is)\n+    new Iterator[((Int, K), V)] {\n+      var metaBufferPos = 0\n+      def hasNext: Boolean = metaBufferPos < metaBuffer.position\n+      def next(): ((Int, K), V) = {\n+        val key = deserStream.readKey[Any]().asInstanceOf[K]\n+        val value = deserStream.readValue[Any]().asInstanceOf[V]\n+        val partition = metaBuffer.get(metaBufferPos + PARTITION)\n+        metaBufferPos += RECORD_SIZE\n+        ((partition, key), value)\n+      }\n+    }\n+  }\n+\n+  override def estimateSize: Long = metaBuffer.capacity * 4 + kvBuffer.capacity\n+\n+  override def destructiveSortedWritablePartitionedIterator(keyComparator: Option[Comparator[K]])\n+    : WritablePartitionedIterator = {\n+    sort(keyComparator)",
    "line": 121
  }],
  "prId": 4450
}]