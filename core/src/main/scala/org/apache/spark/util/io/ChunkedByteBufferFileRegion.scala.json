[{
  "comments": [{
    "author": {
      "login": "Ngone51"
    },
    "body": "Should this comment be moved above last line ?",
    "commit": "4664942f0509b8d34ff27ddc9427351ed836f663",
    "createdAt": "2018-05-28T03:32:44Z",
    "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.util.io\n+\n+import java.nio.channels.WritableByteChannel\n+\n+import io.netty.channel.FileRegion\n+import io.netty.util.AbstractReferenceCounted\n+\n+import org.apache.spark.internal.Logging\n+\n+\n+/**\n+ * This exposes a ChunkedByteBuffer as a netty FileRegion, just to allow sending > 2gb in one netty\n+ * message.   This is because netty cannot send a ByteBuf > 2g, but it can send a large FileRegion,\n+ * even though the data is not backed by a file.\n+ */\n+private[io] class ChunkedByteBufferFileRegion(\n+    val chunkedByteBuffer: ChunkedByteBuffer,\n+    val ioChunkSize: Int) extends AbstractReferenceCounted with FileRegion with Logging {\n+\n+  private var _transferred: Long = 0\n+  // this duplicates the original chunks, so we're free to modify the position, limit, etc.\n+  private val chunks = chunkedByteBuffer.getChunks()\n+  private val cumLength = chunks.scanLeft(0L) { _ + _.remaining()}\n+  private val size = cumLength.last\n+  // Chunk size in bytes"
  }],
  "prId": 21440
}, {
  "comments": [{
    "author": {
      "login": "Ngone51"
    },
    "body": "What's the difference between `size` and `count`? Should `count` indicates the rest data's size can be transfered ?",
    "commit": "4664942f0509b8d34ff27ddc9427351ed836f663",
    "createdAt": "2018-05-28T06:04:24Z",
    "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.util.io\n+\n+import java.nio.channels.WritableByteChannel\n+\n+import io.netty.channel.FileRegion\n+import io.netty.util.AbstractReferenceCounted\n+\n+import org.apache.spark.internal.Logging\n+\n+\n+/**\n+ * This exposes a ChunkedByteBuffer as a netty FileRegion, just to allow sending > 2gb in one netty\n+ * message.   This is because netty cannot send a ByteBuf > 2g, but it can send a large FileRegion,\n+ * even though the data is not backed by a file.\n+ */\n+private[io] class ChunkedByteBufferFileRegion(\n+    val chunkedByteBuffer: ChunkedByteBuffer,\n+    val ioChunkSize: Int) extends AbstractReferenceCounted with FileRegion with Logging {\n+\n+  private var _transferred: Long = 0\n+  // this duplicates the original chunks, so we're free to modify the position, limit, etc.\n+  private val chunks = chunkedByteBuffer.getChunks()\n+  private val cumLength = chunks.scanLeft(0L) { _ + _.remaining()}\n+  private val size = cumLength.last\n+  // Chunk size in bytes\n+\n+  protected def deallocate: Unit = {}\n+\n+  override def count(): Long = chunkedByteBuffer.size"
  }, {
    "author": {
      "login": "squito"
    },
    "body": "no difference, `count()` is just to satisfy an interface.  My mistake for having them look different, I'll make them the same",
    "commit": "4664942f0509b8d34ff27ddc9427351ed836f663",
    "createdAt": "2018-05-29T15:32:36Z",
    "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.util.io\n+\n+import java.nio.channels.WritableByteChannel\n+\n+import io.netty.channel.FileRegion\n+import io.netty.util.AbstractReferenceCounted\n+\n+import org.apache.spark.internal.Logging\n+\n+\n+/**\n+ * This exposes a ChunkedByteBuffer as a netty FileRegion, just to allow sending > 2gb in one netty\n+ * message.   This is because netty cannot send a ByteBuf > 2g, but it can send a large FileRegion,\n+ * even though the data is not backed by a file.\n+ */\n+private[io] class ChunkedByteBufferFileRegion(\n+    val chunkedByteBuffer: ChunkedByteBuffer,\n+    val ioChunkSize: Int) extends AbstractReferenceCounted with FileRegion with Logging {\n+\n+  private var _transferred: Long = 0\n+  // this duplicates the original chunks, so we're free to modify the position, limit, etc.\n+  private val chunks = chunkedByteBuffer.getChunks()\n+  private val cumLength = chunks.scanLeft(0L) { _ + _.remaining()}\n+  private val size = cumLength.last\n+  // Chunk size in bytes\n+\n+  protected def deallocate: Unit = {}\n+\n+  override def count(): Long = chunkedByteBuffer.size"
  }],
  "prId": 21440
}, {
  "comments": [{
    "author": {
      "login": "Ngone51"
    },
    "body": "Seems it is unused.",
    "commit": "4664942f0509b8d34ff27ddc9427351ed836f663",
    "createdAt": "2018-05-28T10:44:27Z",
    "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.util.io\n+\n+import java.nio.channels.WritableByteChannel\n+\n+import io.netty.channel.FileRegion\n+import io.netty.util.AbstractReferenceCounted\n+\n+import org.apache.spark.internal.Logging\n+\n+\n+/**\n+ * This exposes a ChunkedByteBuffer as a netty FileRegion, just to allow sending > 2gb in one netty\n+ * message.   This is because netty cannot send a ByteBuf > 2g, but it can send a large FileRegion,\n+ * even though the data is not backed by a file.\n+ */\n+private[io] class ChunkedByteBufferFileRegion(\n+    val chunkedByteBuffer: ChunkedByteBuffer,\n+    val ioChunkSize: Int) extends AbstractReferenceCounted with FileRegion with Logging {\n+\n+  private var _transferred: Long = 0\n+  // this duplicates the original chunks, so we're free to modify the position, limit, etc.\n+  private val chunks = chunkedByteBuffer.getChunks()\n+  private val cumLength = chunks.scanLeft(0L) { _ + _.remaining()}\n+  private val size = cumLength.last\n+  // Chunk size in bytes\n+\n+  protected def deallocate: Unit = {}\n+\n+  override def count(): Long = chunkedByteBuffer.size\n+\n+  // this is the \"start position\" of the overall Data in the backing file, not our current position\n+  override def position(): Long = 0\n+\n+  override def transferred(): Long = _transferred\n+\n+  override def transfered(): Long = _transferred\n+\n+  override def touch(): ChunkedByteBufferFileRegion = this\n+\n+  override def touch(hint: Object): ChunkedByteBufferFileRegion = this\n+\n+  override def retain(): FileRegion = {\n+    super.retain()\n+    this\n+  }\n+\n+  override def retain(increment: Int): FileRegion = {\n+    super.retain(increment)\n+    this\n+  }\n+\n+  private var currentChunkIdx = 0\n+\n+  def transferTo(target: WritableByteChannel, position: Long): Long = {\n+    assert(position == _transferred)\n+    if (position == size) return 0L\n+    var keepGoing = true\n+    var written = 0L\n+    var currentChunk = chunks(currentChunkIdx)\n+    var originalLimit = currentChunk.limit()"
  }],
  "prId": 21440
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Extend `AbstractFileRegion`?\r\n\r\nDo the fields need to be public?\r\n\r\nYou don't seem to need `Logging`.",
    "commit": "4664942f0509b8d34ff27ddc9427351ed836f663",
    "createdAt": "2018-06-27T20:35:51Z",
    "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.util.io\n+\n+import java.nio.channels.WritableByteChannel\n+\n+import io.netty.channel.FileRegion\n+import io.netty.util.AbstractReferenceCounted\n+\n+import org.apache.spark.internal.Logging\n+\n+\n+/**\n+ * This exposes a ChunkedByteBuffer as a netty FileRegion, just to allow sending > 2gb in one netty\n+ * message.   This is because netty cannot send a ByteBuf > 2g, but it can send a large FileRegion,\n+ * even though the data is not backed by a file.\n+ */\n+private[io] class ChunkedByteBufferFileRegion(\n+    val chunkedByteBuffer: ChunkedByteBuffer,\n+    val ioChunkSize: Int) extends AbstractReferenceCounted with FileRegion with Logging {"
  }],
  "prId": 21440
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Use `foldLeft(0) { blah }` + avoid the intermediate `val`?",
    "commit": "4664942f0509b8d34ff27ddc9427351ed836f663",
    "createdAt": "2018-06-27T20:49:45Z",
    "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.util.io\n+\n+import java.nio.channels.WritableByteChannel\n+\n+import io.netty.channel.FileRegion\n+import io.netty.util.AbstractReferenceCounted\n+\n+import org.apache.spark.internal.Logging\n+\n+\n+/**\n+ * This exposes a ChunkedByteBuffer as a netty FileRegion, just to allow sending > 2gb in one netty\n+ * message.   This is because netty cannot send a ByteBuf > 2g, but it can send a large FileRegion,\n+ * even though the data is not backed by a file.\n+ */\n+private[io] class ChunkedByteBufferFileRegion(\n+    val chunkedByteBuffer: ChunkedByteBuffer,\n+    val ioChunkSize: Int) extends AbstractReferenceCounted with FileRegion with Logging {\n+\n+  private var _transferred: Long = 0\n+  // this duplicates the original chunks, so we're free to modify the position, limit, etc.\n+  private val chunks = chunkedByteBuffer.getChunks()\n+  private val cumLength = chunks.scanLeft(0L) { _ + _.remaining()}"
  }],
  "prId": 21440
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Unused.",
    "commit": "4664942f0509b8d34ff27ddc9427351ed836f663",
    "createdAt": "2018-06-27T21:08:56Z",
    "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.util.io\n+\n+import java.nio.channels.WritableByteChannel\n+\n+import io.netty.channel.FileRegion\n+import io.netty.util.AbstractReferenceCounted\n+\n+import org.apache.spark.internal.Logging\n+\n+\n+/**\n+ * This exposes a ChunkedByteBuffer as a netty FileRegion, just to allow sending > 2gb in one netty\n+ * message.   This is because netty cannot send a ByteBuf > 2g, but it can send a large FileRegion,\n+ * even though the data is not backed by a file.\n+ */\n+private[io] class ChunkedByteBufferFileRegion(\n+    val chunkedByteBuffer: ChunkedByteBuffer,\n+    val ioChunkSize: Int) extends AbstractReferenceCounted with FileRegion with Logging {\n+\n+  private var _transferred: Long = 0\n+  // this duplicates the original chunks, so we're free to modify the position, limit, etc.\n+  private val chunks = chunkedByteBuffer.getChunks()\n+  private val cumLength = chunks.scanLeft(0L) { _ + _.remaining()}\n+  private val size = cumLength.last\n+\n+  protected def deallocate: Unit = {}\n+\n+  override def count(): Long = size\n+\n+  // this is the \"start position\" of the overall Data in the backing file, not our current position\n+  override def position(): Long = 0\n+\n+  override def transferred(): Long = _transferred\n+\n+  override def transfered(): Long = _transferred\n+\n+  override def touch(): ChunkedByteBufferFileRegion = this\n+\n+  override def touch(hint: Object): ChunkedByteBufferFileRegion = this\n+\n+  override def retain(): FileRegion = {\n+    super.retain()\n+    this\n+  }\n+\n+  override def retain(increment: Int): FileRegion = {\n+    super.retain(increment)\n+    this\n+  }\n+\n+  private var currentChunkIdx = 0\n+\n+  def transferTo(target: WritableByteChannel, position: Long): Long = {\n+    assert(position == _transferred)\n+    if (position == size) return 0L\n+    var keepGoing = true\n+    var written = 0L\n+    var currentChunk = chunks(currentChunkIdx)\n+    while (keepGoing) {\n+      while (currentChunk.hasRemaining && keepGoing) {\n+        val ioSize = Math.min(currentChunk.remaining(), ioChunkSize)\n+        val originalPos = currentChunk.position()"
  }, {
    "author": {
      "login": "squito"
    },
    "body": "sorry bunch of leftover bits from earlier debugging.  all cleaned up now",
    "commit": "4664942f0509b8d34ff27ddc9427351ed836f663",
    "createdAt": "2018-06-27T22:01:51Z",
    "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.util.io\n+\n+import java.nio.channels.WritableByteChannel\n+\n+import io.netty.channel.FileRegion\n+import io.netty.util.AbstractReferenceCounted\n+\n+import org.apache.spark.internal.Logging\n+\n+\n+/**\n+ * This exposes a ChunkedByteBuffer as a netty FileRegion, just to allow sending > 2gb in one netty\n+ * message.   This is because netty cannot send a ByteBuf > 2g, but it can send a large FileRegion,\n+ * even though the data is not backed by a file.\n+ */\n+private[io] class ChunkedByteBufferFileRegion(\n+    val chunkedByteBuffer: ChunkedByteBuffer,\n+    val ioChunkSize: Int) extends AbstractReferenceCounted with FileRegion with Logging {\n+\n+  private var _transferred: Long = 0\n+  // this duplicates the original chunks, so we're free to modify the position, limit, etc.\n+  private val chunks = chunkedByteBuffer.getChunks()\n+  private val cumLength = chunks.scanLeft(0L) { _ + _.remaining()}\n+  private val size = cumLength.last\n+\n+  protected def deallocate: Unit = {}\n+\n+  override def count(): Long = size\n+\n+  // this is the \"start position\" of the overall Data in the backing file, not our current position\n+  override def position(): Long = 0\n+\n+  override def transferred(): Long = _transferred\n+\n+  override def transfered(): Long = _transferred\n+\n+  override def touch(): ChunkedByteBufferFileRegion = this\n+\n+  override def touch(hint: Object): ChunkedByteBufferFileRegion = this\n+\n+  override def retain(): FileRegion = {\n+    super.retain()\n+    this\n+  }\n+\n+  override def retain(increment: Int): FileRegion = {\n+    super.retain(increment)\n+    this\n+  }\n+\n+  private var currentChunkIdx = 0\n+\n+  def transferTo(target: WritableByteChannel, position: Long): Long = {\n+    assert(position == _transferred)\n+    if (position == size) return 0L\n+    var keepGoing = true\n+    var written = 0L\n+    var currentChunk = chunks(currentChunkIdx)\n+    while (keepGoing) {\n+      while (currentChunk.hasRemaining && keepGoing) {\n+        val ioSize = Math.min(currentChunk.remaining(), ioChunkSize)\n+        val originalPos = currentChunk.position()"
  }],
  "prId": 21440
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "space before `}`",
    "commit": "4664942f0509b8d34ff27ddc9427351ed836f663",
    "createdAt": "2018-06-27T23:13:25Z",
    "diffHunk": "@@ -0,0 +1,86 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.util.io\n+\n+import java.nio.channels.WritableByteChannel\n+\n+import io.netty.channel.FileRegion\n+import io.netty.util.AbstractReferenceCounted\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.network.util.AbstractFileRegion\n+\n+\n+/**\n+ * This exposes a ChunkedByteBuffer as a netty FileRegion, just to allow sending > 2gb in one netty\n+ * message.   This is because netty cannot send a ByteBuf > 2g, but it can send a large FileRegion,\n+ * even though the data is not backed by a file.\n+ */\n+private[io] class ChunkedByteBufferFileRegion(\n+    private val chunkedByteBuffer: ChunkedByteBuffer,\n+    private val ioChunkSize: Int) extends AbstractFileRegion {\n+\n+  private var _transferred: Long = 0\n+  // this duplicates the original chunks, so we're free to modify the position, limit, etc.\n+  private val chunks = chunkedByteBuffer.getChunks()\n+  private val size = chunks.foldLeft(0) { _ + _.remaining()}"
  }],
  "prId": 21440
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "3 spaces",
    "commit": "4664942f0509b8d34ff27ddc9427351ed836f663",
    "createdAt": "2018-06-27T23:13:31Z",
    "diffHunk": "@@ -0,0 +1,86 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.util.io\n+\n+import java.nio.channels.WritableByteChannel\n+\n+import io.netty.channel.FileRegion\n+import io.netty.util.AbstractReferenceCounted\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.network.util.AbstractFileRegion\n+\n+\n+/**\n+ * This exposes a ChunkedByteBuffer as a netty FileRegion, just to allow sending > 2gb in one netty\n+ * message.   This is because netty cannot send a ByteBuf > 2g, but it can send a large FileRegion,"
  }],
  "prId": 21440
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`0L`? Otherwise this will overflow for > 2G right?",
    "commit": "4664942f0509b8d34ff27ddc9427351ed836f663",
    "createdAt": "2018-06-29T22:07:08Z",
    "diffHunk": "@@ -0,0 +1,86 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.util.io\n+\n+import java.nio.channels.WritableByteChannel\n+\n+import io.netty.channel.FileRegion\n+import io.netty.util.AbstractReferenceCounted\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.network.util.AbstractFileRegion\n+\n+\n+/**\n+ * This exposes a ChunkedByteBuffer as a netty FileRegion, just to allow sending > 2gb in one netty\n+ * message.  This is because netty cannot send a ByteBuf > 2g, but it can send a large FileRegion,\n+ * even though the data is not backed by a file.\n+ */\n+private[io] class ChunkedByteBufferFileRegion(\n+    private val chunkedByteBuffer: ChunkedByteBuffer,\n+    private val ioChunkSize: Int) extends AbstractFileRegion {\n+\n+  private var _transferred: Long = 0\n+  // this duplicates the original chunks, so we're free to modify the position, limit, etc.\n+  private val chunks = chunkedByteBuffer.getChunks()\n+  private val size = chunks.foldLeft(0) { _ + _.remaining() }"
  }],
  "prId": 21440
}, {
  "comments": [{
    "author": {
      "login": "jerryshao"
    },
    "body": "What will be happened if `thisWriteSize` is smaller than `ioSize`, will Spark throw an exception or something else?",
    "commit": "4664942f0509b8d34ff27ddc9427351ed836f663",
    "createdAt": "2018-07-18T02:44:58Z",
    "diffHunk": "@@ -0,0 +1,86 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.util.io\n+\n+import java.nio.channels.WritableByteChannel\n+\n+import io.netty.channel.FileRegion\n+import io.netty.util.AbstractReferenceCounted\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.network.util.AbstractFileRegion\n+\n+\n+/**\n+ * This exposes a ChunkedByteBuffer as a netty FileRegion, just to allow sending > 2gb in one netty\n+ * message.  This is because netty cannot send a ByteBuf > 2g, but it can send a large FileRegion,\n+ * even though the data is not backed by a file.\n+ */\n+private[io] class ChunkedByteBufferFileRegion(\n+    private val chunkedByteBuffer: ChunkedByteBuffer,\n+    private val ioChunkSize: Int) extends AbstractFileRegion {\n+\n+  private var _transferred: Long = 0\n+  // this duplicates the original chunks, so we're free to modify the position, limit, etc.\n+  private val chunks = chunkedByteBuffer.getChunks()\n+  private val size = chunks.foldLeft(0L) { _ + _.remaining() }\n+\n+  protected def deallocate: Unit = {}\n+\n+  override def count(): Long = size\n+\n+  // this is the \"start position\" of the overall Data in the backing file, not our current position\n+  override def position(): Long = 0\n+\n+  override def transferred(): Long = _transferred\n+\n+  private var currentChunkIdx = 0\n+\n+  def transferTo(target: WritableByteChannel, position: Long): Long = {\n+    assert(position == _transferred)\n+    if (position == size) return 0L\n+    var keepGoing = true\n+    var written = 0L\n+    var currentChunk = chunks(currentChunkIdx)\n+    while (keepGoing) {\n+      while (currentChunk.hasRemaining && keepGoing) {\n+        val ioSize = Math.min(currentChunk.remaining(), ioChunkSize)\n+        val originalLimit = currentChunk.limit()\n+        currentChunk.limit(currentChunk.position() + ioSize)\n+        val thisWriteSize = target.write(currentChunk)\n+        currentChunk.limit(originalLimit)\n+        written += thisWriteSize\n+        if (thisWriteSize < ioSize) {",
    "line": 67
  }, {
    "author": {
      "login": "squito"
    },
    "body": "actually this is a totally normal condition, it just means the channel is not currently ready to accept anymore data.  This is something netty expects, and it will make sure the rest of the data is put on the channel eventually (it'll get called the next time with the correct `position` argument indicating how far along it is).\r\n\r\nThe added unit tests cover this.",
    "commit": "4664942f0509b8d34ff27ddc9427351ed836f663",
    "createdAt": "2018-07-18T04:03:13Z",
    "diffHunk": "@@ -0,0 +1,86 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.util.io\n+\n+import java.nio.channels.WritableByteChannel\n+\n+import io.netty.channel.FileRegion\n+import io.netty.util.AbstractReferenceCounted\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.network.util.AbstractFileRegion\n+\n+\n+/**\n+ * This exposes a ChunkedByteBuffer as a netty FileRegion, just to allow sending > 2gb in one netty\n+ * message.  This is because netty cannot send a ByteBuf > 2g, but it can send a large FileRegion,\n+ * even though the data is not backed by a file.\n+ */\n+private[io] class ChunkedByteBufferFileRegion(\n+    private val chunkedByteBuffer: ChunkedByteBuffer,\n+    private val ioChunkSize: Int) extends AbstractFileRegion {\n+\n+  private var _transferred: Long = 0\n+  // this duplicates the original chunks, so we're free to modify the position, limit, etc.\n+  private val chunks = chunkedByteBuffer.getChunks()\n+  private val size = chunks.foldLeft(0L) { _ + _.remaining() }\n+\n+  protected def deallocate: Unit = {}\n+\n+  override def count(): Long = size\n+\n+  // this is the \"start position\" of the overall Data in the backing file, not our current position\n+  override def position(): Long = 0\n+\n+  override def transferred(): Long = _transferred\n+\n+  private var currentChunkIdx = 0\n+\n+  def transferTo(target: WritableByteChannel, position: Long): Long = {\n+    assert(position == _transferred)\n+    if (position == size) return 0L\n+    var keepGoing = true\n+    var written = 0L\n+    var currentChunk = chunks(currentChunkIdx)\n+    while (keepGoing) {\n+      while (currentChunk.hasRemaining && keepGoing) {\n+        val ioSize = Math.min(currentChunk.remaining(), ioChunkSize)\n+        val originalLimit = currentChunk.limit()\n+        currentChunk.limit(currentChunk.position() + ioSize)\n+        val thisWriteSize = target.write(currentChunk)\n+        currentChunk.limit(originalLimit)\n+        written += thisWriteSize\n+        if (thisWriteSize < ioSize) {",
    "line": 67
  }, {
    "author": {
      "login": "jerryshao"
    },
    "body": "I see, thanks for explain.",
    "commit": "4664942f0509b8d34ff27ddc9427351ed836f663",
    "createdAt": "2018-07-18T04:56:42Z",
    "diffHunk": "@@ -0,0 +1,86 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.util.io\n+\n+import java.nio.channels.WritableByteChannel\n+\n+import io.netty.channel.FileRegion\n+import io.netty.util.AbstractReferenceCounted\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.network.util.AbstractFileRegion\n+\n+\n+/**\n+ * This exposes a ChunkedByteBuffer as a netty FileRegion, just to allow sending > 2gb in one netty\n+ * message.  This is because netty cannot send a ByteBuf > 2g, but it can send a large FileRegion,\n+ * even though the data is not backed by a file.\n+ */\n+private[io] class ChunkedByteBufferFileRegion(\n+    private val chunkedByteBuffer: ChunkedByteBuffer,\n+    private val ioChunkSize: Int) extends AbstractFileRegion {\n+\n+  private var _transferred: Long = 0\n+  // this duplicates the original chunks, so we're free to modify the position, limit, etc.\n+  private val chunks = chunkedByteBuffer.getChunks()\n+  private val size = chunks.foldLeft(0L) { _ + _.remaining() }\n+\n+  protected def deallocate: Unit = {}\n+\n+  override def count(): Long = size\n+\n+  // this is the \"start position\" of the overall Data in the backing file, not our current position\n+  override def position(): Long = 0\n+\n+  override def transferred(): Long = _transferred\n+\n+  private var currentChunkIdx = 0\n+\n+  def transferTo(target: WritableByteChannel, position: Long): Long = {\n+    assert(position == _transferred)\n+    if (position == size) return 0L\n+    var keepGoing = true\n+    var written = 0L\n+    var currentChunk = chunks(currentChunkIdx)\n+    while (keepGoing) {\n+      while (currentChunk.hasRemaining && keepGoing) {\n+        val ioSize = Math.min(currentChunk.remaining(), ioChunkSize)\n+        val originalLimit = currentChunk.limit()\n+        currentChunk.limit(currentChunk.position() + ioSize)\n+        val thisWriteSize = target.write(currentChunk)\n+        currentChunk.limit(originalLimit)\n+        written += thisWriteSize\n+        if (thisWriteSize < ioSize) {",
    "line": 67
  }],
  "prId": 21440
}]