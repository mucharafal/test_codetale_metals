[{
  "comments": [{
    "author": {
      "login": "dorx"
    },
    "body": "\"[random]\" can be removed. it's only used in this class\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-03T23:31:38Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.{Map, mutable}\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.{Logging, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.random.{PoissonBounds => PB}\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * A version of {@link #aggregate()} that passes the TaskContext to the function that does\n+   * aggregation for each partition. This function avoids creating an extra depth in the RDD\n+   * lineage, as opposed to using mapPartitionsWithId, which results in slightly improved run time.\n+   */\n+  def aggregateWithContext[U: ClassTag, T: ClassTag](zeroValue: U)\n+      (rdd: RDD[T],\n+       seqOp: ((TaskContext, U), T) => U,\n+       combOp: (U, U) => U): U = {\n+    val sc: SparkContext = rdd.sparkContext\n+    // Clone the zero value since we will also be serializing it as part of tasks\n+    var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance())\n+    // pad seqOp and combOp with taskContext to conform to aggregate's signature in TraversableOnce\n+    val paddedSeqOp = (arg1: (TaskContext, U), item: T) => (arg1._1, seqOp(arg1, item))\n+    val paddedcombOp = (arg1: (TaskContext, U), arg2: (TaskContext, U)) =>\n+      (arg1._1, combOp(arg1._2, arg1._2))\n+    val cleanSeqOp = sc.clean(paddedSeqOp)\n+    val cleanCombOp = sc.clean(paddedcombOp)\n+    val aggregatePartition = (tc: TaskContext, it: Iterator[T]) =>\n+      (it.aggregate(tc, zeroValue)(cleanSeqOp, cleanCombOp))._2\n+    val mergeResult = (index: Int, taskResult: U) => jobResult = combOp(jobResult, taskResult)\n+    sc.runJob(rdd, aggregatePartition, mergeResult)\n+    jobResult\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractionByKey: (K => Double),\n+      counts: Option[Map[K, Long]]): ((TaskContext, Result[K]),(K, V)) => Result[K] = {\n+    val delta = 5e-5\n+    (U: (TaskContext, Result[K]), item: (K, V)) => {\n+      val result = U._2\n+      val tc = U._1\n+      val rng = result.getRand(tc.partitionId)\n+      val fraction = fractionByKey(item._1)\n+      val stratum = result.getEntry(item._1)\n+      if (withReplacement) {\n+        // compute q1 and q2 only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.q1.isEmpty || stratum.q2.isEmpty) {\n+          val n = counts.get(item._1)\n+          val s = math.ceil(n * fraction).toLong\n+          val lmbd1 = PB.getLowerBound(s)\n+          val minCount = PB.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) PB.getUpperBound(s) else PB.getUpperBound(s - minCount)\n+          val q1 = lmbd1 / n\n+          val q2 = lmbd2 / n\n+          stratum.q1 = Some(q1)\n+          stratum.q2 = Some(q2)\n+        }\n+        val x1 = if (stratum.q1.get == 0) 0L else rng.nextPoisson(stratum.q1.get)\n+        if (x1 > 0) {\n+          stratum.incrNumAccepted(x1)\n+        }\n+        val x2 = rng.nextPoisson(stratum.q2.get).toInt\n+        if (x2 > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(x2)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement.\n+        // Hence, q1 and q2 change on every iteration.\n+        val g1 = - math.log(delta) / stratum.numItems\n+        val g2 = (2.0 / 3.0) * g1\n+        val q1 = math.max(0, fraction + g2 - math.sqrt((g2 * g2 + 3 * g2 * fraction)))\n+        val q2 = math.min(1, fraction + g1 + math.sqrt(g1 * g1 + 2 * g1 * fraction))\n+\n+        val x = rng.nextUniform(0.0, 1.0)\n+        if (x < q1) {\n+          stratum.incrNumAccepted()\n+        } else if (x < q2) {\n+          stratum.addToWaitList(x)\n+        }\n+        stratum.q1 = Some(q1)\n+        stratum.q2 = Some(q2)\n+      }\n+      stratum.incrNumItems()\n+      result\n+    }\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to combine results from different partitions, as\n+   * returned by seqOp.\n+   */\n+  def getCombOp[K](): (Result[K], Result[K]) => Result[K] = {\n+    (r1: Result[K], r2: Result[K]) => {\n+      // take union of both key sets in case one partition doesn't contain all keys\n+      val keyUnion = r1.resultMap.keys.toSet.union(r2.resultMap.keys.toSet)\n+\n+      // Use r2 to keep the combined result since r1 is usual empty\n+      for (key <- keyUnion) {\n+        val entry1 = r1.resultMap.get(key)\n+        val entry2 = r2.resultMap.get(key)\n+        if (entry2.isEmpty && entry1.isDefined) {\n+          r2.resultMap += (key -> entry1.get)\n+        } else if (entry1.isDefined && entry2.isDefined) {\n+          entry2.get.addToWaitList(entry1.get.waitList)\n+          entry2.get.incrNumAccepted(entry1.get.numAccepted)\n+          entry2.get.incrNumItems(entry1.get.numItems)\n+        }\n+      }\n+      r2\n+    }\n+  }\n+\n+  /**\n+   * Given the result returned by the aggregate function, we need to determine the threshold used\n+   * to accept items to generate the exact sample size.\n+   */\n+  def computeThresholdByKey[K](finalResult: Map[K, Stratum], fractionByKey: (K => Double)):\n+    (K => Double) = {\n+    val thresholdByKey = new mutable.HashMap[K, Double]()\n+    for ((key, stratum) <- finalResult) {\n+      val fraction = fractionByKey(key)\n+      val s = math.ceil(stratum.numItems * fraction).toLong\n+      if (stratum.numAccepted > s) {\n+        logWarning(\"Pre-accepted too many\")\n+        thresholdByKey += (key -> stratum.q1.get)\n+      } else {\n+        val numWaitListAccepted = (s - stratum.numAccepted).toInt\n+        if (numWaitListAccepted >= stratum.waitList.size) {\n+          logWarning(\"WaitList too short\")\n+          thresholdByKey += (key -> stratum.q2.get)\n+        } else {\n+          thresholdByKey += (key -> stratum.waitList.sorted.apply(numWaitListAccepted))\n+        }\n+      }\n+    }\n+    thresholdByKey\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling without replacement.\n+   *\n+   * When exact sample size is required, we make an additional pass over the RDD to determine the\n+   * exact sampling rate that guarantees sample size with high confidence.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getBernoulliSamplingFunction[K, V](rdd: RDD[(K,  V)],\n+      fractionByKey: K => Double,\n+      exact: Boolean,\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    var samplingRateByKey = fractionByKey\n+    if (exact) {\n+      // determine threshold for each stratum and resample\n+      val seqOp = StratifiedSampler.getSeqOp[K,V](false, fractionByKey, None)\n+      val combOp = StratifiedSampler.getCombOp[K]()\n+      val zeroU = new Result[K](Map[K, Stratum](), seed = seed)\n+      val finalResult = aggregateWithContext(zeroU)(rdd, seqOp, combOp).resultMap\n+      samplingRateByKey = StratifiedSampler.computeThresholdByKey(finalResult, fractionByKey)\n+    }\n+    (idx: Int, iter: Iterator[(K, V)]) => {\n+      val random = new RandomDataGenerator\n+      random.reSeed(seed + idx)\n+      iter.filter(t => random.nextUniform(0.0, 1.0) < samplingRateByKey(t._1))\n+    }\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling with replacement.\n+   *\n+   * When exact sample size is required, we make two additional passed over the RDD to determine\n+   * the exact sampling rate that guarantees sample size with high confidence. The first pass\n+   * counts the number of items in each stratum (group of items with the same key) in the RDD, and\n+   * the second pass uses the counts to determine exact sampling rates.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getPoissonSamplingFunction[K, V](rdd:RDD[(K,  V)],\n+      fractionByKey: K => Double,\n+      exact: Boolean,\n+      counts: Option[Map[K, Long]],\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    // TODO implement the streaming version of sampling w/ replacement that doesn't require counts\n+    if (exact) {\n+      val seqOp = StratifiedSampler.getSeqOp[K,V](true, fractionByKey, counts)\n+      val combOp = StratifiedSampler.getCombOp[K]()\n+      val zeroU = new Result[K](Map[K, Stratum](), seed = seed)\n+      val finalResult = aggregateWithContext(zeroU)(rdd, seqOp, combOp).resultMap\n+      val thresholdByKey = StratifiedSampler.computeThresholdByKey(finalResult, fractionByKey)\n+      (idx: Int, iter: Iterator[(K, V)]) => {\n+        val random = new RandomDataGenerator()\n+        random.reSeed(seed + idx)\n+        iter.flatMap { t =>\n+          val q1 = finalResult.get(t._1).get.q1.getOrElse(0.0)\n+          val q2 = finalResult.get(t._1).get.q2.getOrElse(0.0)\n+          val x1 = if (q1 == 0) 0L else random.nextPoisson(q1)\n+          val x2 = random.nextPoisson(q2).toInt\n+          val x = x1 + (0 until x2).count(i => random.nextUniform(0.0, 1.0) < thresholdByKey(t._1))\n+          if (x > 0) {\n+            Iterator.fill(x.toInt)(t)\n+          } else {\n+            Iterator.empty\n+          }\n+        }\n+      }\n+    } else {\n+      (idx: Int, iter: Iterator[(K, V)]) => {\n+        val random = new RandomDataGenerator()\n+        random.reSeed(seed + idx)\n+        iter.flatMap { t =>\n+          val count = random.nextPoisson(fractionByKey(t._1)).toInt\n+          if (count > 0) {\n+            Iterator.fill(count)(t)\n+          } else {\n+            Iterator.empty\n+          }\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * Object used by seqOp to keep track of the number of items accepted and items waitlisted per\n+ * stratum, as well as the bounds for accepting and waitlisting items.\n+ */\n+private[random] class Stratum(var numItems: Long = 0L, var numAccepted: Long = 0L)"
  }],
  "prId": 1025
}, {
  "comments": [{
    "author": {
      "login": "falaki"
    },
    "body": "Do you mean `mapPartiionsWithIndex()`?\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-03T23:38:14Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.{Map, mutable}\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.{Logging, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.random.{PoissonBounds => PB}\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * A version of {@link #aggregate()} that passes the TaskContext to the function that does\n+   * aggregation for each partition. This function avoids creating an extra depth in the RDD\n+   * lineage, as opposed to using mapPartitionsWithId, which results in slightly improved run time."
  }],
  "prId": 1025
}, {
  "comments": [{
    "author": {
      "login": "falaki"
    },
    "body": "Nit: add a space after colon \n`rdd: Rdd[(K, V)]`\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-04T00:22:14Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.{Map, mutable}\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.{Logging, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.random.{PoissonBounds => PB}\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * A version of {@link #aggregate()} that passes the TaskContext to the function that does\n+   * aggregation for each partition. This function avoids creating an extra depth in the RDD\n+   * lineage, as opposed to using mapPartitionsWithId, which results in slightly improved run time.\n+   */\n+  def aggregateWithContext[U: ClassTag, T: ClassTag](zeroValue: U)\n+      (rdd: RDD[T],\n+       seqOp: ((TaskContext, U), T) => U,\n+       combOp: (U, U) => U): U = {\n+    val sc: SparkContext = rdd.sparkContext\n+    // Clone the zero value since we will also be serializing it as part of tasks\n+    var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance())\n+    // pad seqOp and combOp with taskContext to conform to aggregate's signature in TraversableOnce\n+    val paddedSeqOp = (arg1: (TaskContext, U), item: T) => (arg1._1, seqOp(arg1, item))\n+    val paddedcombOp = (arg1: (TaskContext, U), arg2: (TaskContext, U)) =>\n+      (arg1._1, combOp(arg1._2, arg1._2))\n+    val cleanSeqOp = sc.clean(paddedSeqOp)\n+    val cleanCombOp = sc.clean(paddedcombOp)\n+    val aggregatePartition = (tc: TaskContext, it: Iterator[T]) =>\n+      (it.aggregate(tc, zeroValue)(cleanSeqOp, cleanCombOp))._2\n+    val mergeResult = (index: Int, taskResult: U) => jobResult = combOp(jobResult, taskResult)\n+    sc.runJob(rdd, aggregatePartition, mergeResult)\n+    jobResult\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractionByKey: (K => Double),\n+      counts: Option[Map[K, Long]]): ((TaskContext, Result[K]),(K, V)) => Result[K] = {\n+    val delta = 5e-5\n+    (U: (TaskContext, Result[K]), item: (K, V)) => {\n+      val result = U._2\n+      val tc = U._1\n+      val rng = result.getRand(tc.partitionId)\n+      val fraction = fractionByKey(item._1)\n+      val stratum = result.getEntry(item._1)\n+      if (withReplacement) {\n+        // compute q1 and q2 only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.q1.isEmpty || stratum.q2.isEmpty) {\n+          val n = counts.get(item._1)\n+          val s = math.ceil(n * fraction).toLong\n+          val lmbd1 = PB.getLowerBound(s)\n+          val minCount = PB.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) PB.getUpperBound(s) else PB.getUpperBound(s - minCount)\n+          val q1 = lmbd1 / n\n+          val q2 = lmbd2 / n\n+          stratum.q1 = Some(q1)\n+          stratum.q2 = Some(q2)\n+        }\n+        val x1 = if (stratum.q1.get == 0) 0L else rng.nextPoisson(stratum.q1.get)\n+        if (x1 > 0) {\n+          stratum.incrNumAccepted(x1)\n+        }\n+        val x2 = rng.nextPoisson(stratum.q2.get).toInt\n+        if (x2 > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(x2)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement.\n+        // Hence, q1 and q2 change on every iteration.\n+        val g1 = - math.log(delta) / stratum.numItems\n+        val g2 = (2.0 / 3.0) * g1\n+        val q1 = math.max(0, fraction + g2 - math.sqrt((g2 * g2 + 3 * g2 * fraction)))\n+        val q2 = math.min(1, fraction + g1 + math.sqrt(g1 * g1 + 2 * g1 * fraction))\n+\n+        val x = rng.nextUniform(0.0, 1.0)\n+        if (x < q1) {\n+          stratum.incrNumAccepted()\n+        } else if (x < q2) {\n+          stratum.addToWaitList(x)\n+        }\n+        stratum.q1 = Some(q1)\n+        stratum.q2 = Some(q2)\n+      }\n+      stratum.incrNumItems()\n+      result\n+    }\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to combine results from different partitions, as\n+   * returned by seqOp.\n+   */\n+  def getCombOp[K](): (Result[K], Result[K]) => Result[K] = {\n+    (r1: Result[K], r2: Result[K]) => {\n+      // take union of both key sets in case one partition doesn't contain all keys\n+      val keyUnion = r1.resultMap.keys.toSet.union(r2.resultMap.keys.toSet)\n+\n+      // Use r2 to keep the combined result since r1 is usual empty\n+      for (key <- keyUnion) {\n+        val entry1 = r1.resultMap.get(key)\n+        val entry2 = r2.resultMap.get(key)\n+        if (entry2.isEmpty && entry1.isDefined) {\n+          r2.resultMap += (key -> entry1.get)\n+        } else if (entry1.isDefined && entry2.isDefined) {\n+          entry2.get.addToWaitList(entry1.get.waitList)\n+          entry2.get.incrNumAccepted(entry1.get.numAccepted)\n+          entry2.get.incrNumItems(entry1.get.numItems)\n+        }\n+      }\n+      r2\n+    }\n+  }\n+\n+  /**\n+   * Given the result returned by the aggregate function, we need to determine the threshold used\n+   * to accept items to generate the exact sample size.\n+   */\n+  def computeThresholdByKey[K](finalResult: Map[K, Stratum], fractionByKey: (K => Double)):\n+    (K => Double) = {\n+    val thresholdByKey = new mutable.HashMap[K, Double]()\n+    for ((key, stratum) <- finalResult) {\n+      val fraction = fractionByKey(key)\n+      val s = math.ceil(stratum.numItems * fraction).toLong\n+      if (stratum.numAccepted > s) {\n+        logWarning(\"Pre-accepted too many\")\n+        thresholdByKey += (key -> stratum.q1.get)\n+      } else {\n+        val numWaitListAccepted = (s - stratum.numAccepted).toInt\n+        if (numWaitListAccepted >= stratum.waitList.size) {\n+          logWarning(\"WaitList too short\")\n+          thresholdByKey += (key -> stratum.q2.get)\n+        } else {\n+          thresholdByKey += (key -> stratum.waitList.sorted.apply(numWaitListAccepted))\n+        }\n+      }\n+    }\n+    thresholdByKey\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling without replacement.\n+   *\n+   * When exact sample size is required, we make an additional pass over the RDD to determine the\n+   * exact sampling rate that guarantees sample size with high confidence.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getBernoulliSamplingFunction[K, V](rdd: RDD[(K,  V)],\n+      fractionByKey: K => Double,\n+      exact: Boolean,\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    var samplingRateByKey = fractionByKey\n+    if (exact) {\n+      // determine threshold for each stratum and resample\n+      val seqOp = StratifiedSampler.getSeqOp[K,V](false, fractionByKey, None)\n+      val combOp = StratifiedSampler.getCombOp[K]()\n+      val zeroU = new Result[K](Map[K, Stratum](), seed = seed)\n+      val finalResult = aggregateWithContext(zeroU)(rdd, seqOp, combOp).resultMap\n+      samplingRateByKey = StratifiedSampler.computeThresholdByKey(finalResult, fractionByKey)\n+    }\n+    (idx: Int, iter: Iterator[(K, V)]) => {\n+      val random = new RandomDataGenerator\n+      random.reSeed(seed + idx)\n+      iter.filter(t => random.nextUniform(0.0, 1.0) < samplingRateByKey(t._1))\n+    }\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling with replacement.\n+   *\n+   * When exact sample size is required, we make two additional passed over the RDD to determine\n+   * the exact sampling rate that guarantees sample size with high confidence. The first pass\n+   * counts the number of items in each stratum (group of items with the same key) in the RDD, and\n+   * the second pass uses the counts to determine exact sampling rates.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getPoissonSamplingFunction[K, V](rdd:RDD[(K,  V)],"
  }],
  "prId": 1025
}, {
  "comments": [{
    "author": {
      "login": "falaki"
    },
    "body": "Let's change the type of fractionByKey to Map[K, Double]\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-04T00:37:18Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.{Map, mutable}\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.{Logging, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.random.{PoissonBounds => PB}\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * A version of {@link #aggregate()} that passes the TaskContext to the function that does\n+   * aggregation for each partition. This function avoids creating an extra depth in the RDD\n+   * lineage, as opposed to using mapPartitionsWithId, which results in slightly improved run time.\n+   */\n+  def aggregateWithContext[U: ClassTag, T: ClassTag](zeroValue: U)\n+      (rdd: RDD[T],\n+       seqOp: ((TaskContext, U), T) => U,\n+       combOp: (U, U) => U): U = {\n+    val sc: SparkContext = rdd.sparkContext\n+    // Clone the zero value since we will also be serializing it as part of tasks\n+    var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance())\n+    // pad seqOp and combOp with taskContext to conform to aggregate's signature in TraversableOnce\n+    val paddedSeqOp = (arg1: (TaskContext, U), item: T) => (arg1._1, seqOp(arg1, item))\n+    val paddedcombOp = (arg1: (TaskContext, U), arg2: (TaskContext, U)) =>\n+      (arg1._1, combOp(arg1._2, arg1._2))\n+    val cleanSeqOp = sc.clean(paddedSeqOp)\n+    val cleanCombOp = sc.clean(paddedcombOp)\n+    val aggregatePartition = (tc: TaskContext, it: Iterator[T]) =>\n+      (it.aggregate(tc, zeroValue)(cleanSeqOp, cleanCombOp))._2\n+    val mergeResult = (index: Int, taskResult: U) => jobResult = combOp(jobResult, taskResult)\n+    sc.runJob(rdd, aggregatePartition, mergeResult)\n+    jobResult\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractionByKey: (K => Double),"
  }],
  "prId": 1025
}, {
  "comments": [{
    "author": {
      "login": "falaki"
    },
    "body": "Similarly, let's change the type of fractionByKey to Map[K, Double]\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-04T00:40:07Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.{Map, mutable}\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.{Logging, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.random.{PoissonBounds => PB}\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * A version of {@link #aggregate()} that passes the TaskContext to the function that does\n+   * aggregation for each partition. This function avoids creating an extra depth in the RDD\n+   * lineage, as opposed to using mapPartitionsWithId, which results in slightly improved run time.\n+   */\n+  def aggregateWithContext[U: ClassTag, T: ClassTag](zeroValue: U)\n+      (rdd: RDD[T],\n+       seqOp: ((TaskContext, U), T) => U,\n+       combOp: (U, U) => U): U = {\n+    val sc: SparkContext = rdd.sparkContext\n+    // Clone the zero value since we will also be serializing it as part of tasks\n+    var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance())\n+    // pad seqOp and combOp with taskContext to conform to aggregate's signature in TraversableOnce\n+    val paddedSeqOp = (arg1: (TaskContext, U), item: T) => (arg1._1, seqOp(arg1, item))\n+    val paddedcombOp = (arg1: (TaskContext, U), arg2: (TaskContext, U)) =>\n+      (arg1._1, combOp(arg1._2, arg1._2))\n+    val cleanSeqOp = sc.clean(paddedSeqOp)\n+    val cleanCombOp = sc.clean(paddedcombOp)\n+    val aggregatePartition = (tc: TaskContext, it: Iterator[T]) =>\n+      (it.aggregate(tc, zeroValue)(cleanSeqOp, cleanCombOp))._2\n+    val mergeResult = (index: Int, taskResult: U) => jobResult = combOp(jobResult, taskResult)\n+    sc.runJob(rdd, aggregatePartition, mergeResult)\n+    jobResult\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractionByKey: (K => Double),\n+      counts: Option[Map[K, Long]]): ((TaskContext, Result[K]),(K, V)) => Result[K] = {\n+    val delta = 5e-5\n+    (U: (TaskContext, Result[K]), item: (K, V)) => {\n+      val result = U._2\n+      val tc = U._1\n+      val rng = result.getRand(tc.partitionId)\n+      val fraction = fractionByKey(item._1)\n+      val stratum = result.getEntry(item._1)\n+      if (withReplacement) {\n+        // compute q1 and q2 only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.q1.isEmpty || stratum.q2.isEmpty) {\n+          val n = counts.get(item._1)\n+          val s = math.ceil(n * fraction).toLong\n+          val lmbd1 = PB.getLowerBound(s)\n+          val minCount = PB.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) PB.getUpperBound(s) else PB.getUpperBound(s - minCount)\n+          val q1 = lmbd1 / n\n+          val q2 = lmbd2 / n\n+          stratum.q1 = Some(q1)\n+          stratum.q2 = Some(q2)\n+        }\n+        val x1 = if (stratum.q1.get == 0) 0L else rng.nextPoisson(stratum.q1.get)\n+        if (x1 > 0) {\n+          stratum.incrNumAccepted(x1)\n+        }\n+        val x2 = rng.nextPoisson(stratum.q2.get).toInt\n+        if (x2 > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(x2)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement.\n+        // Hence, q1 and q2 change on every iteration.\n+        val g1 = - math.log(delta) / stratum.numItems\n+        val g2 = (2.0 / 3.0) * g1\n+        val q1 = math.max(0, fraction + g2 - math.sqrt((g2 * g2 + 3 * g2 * fraction)))\n+        val q2 = math.min(1, fraction + g1 + math.sqrt(g1 * g1 + 2 * g1 * fraction))\n+\n+        val x = rng.nextUniform(0.0, 1.0)\n+        if (x < q1) {\n+          stratum.incrNumAccepted()\n+        } else if (x < q2) {\n+          stratum.addToWaitList(x)\n+        }\n+        stratum.q1 = Some(q1)\n+        stratum.q2 = Some(q2)\n+      }\n+      stratum.incrNumItems()\n+      result\n+    }\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to combine results from different partitions, as\n+   * returned by seqOp.\n+   */\n+  def getCombOp[K](): (Result[K], Result[K]) => Result[K] = {\n+    (r1: Result[K], r2: Result[K]) => {\n+      // take union of both key sets in case one partition doesn't contain all keys\n+      val keyUnion = r1.resultMap.keys.toSet.union(r2.resultMap.keys.toSet)\n+\n+      // Use r2 to keep the combined result since r1 is usual empty\n+      for (key <- keyUnion) {\n+        val entry1 = r1.resultMap.get(key)\n+        val entry2 = r2.resultMap.get(key)\n+        if (entry2.isEmpty && entry1.isDefined) {\n+          r2.resultMap += (key -> entry1.get)\n+        } else if (entry1.isDefined && entry2.isDefined) {\n+          entry2.get.addToWaitList(entry1.get.waitList)\n+          entry2.get.incrNumAccepted(entry1.get.numAccepted)\n+          entry2.get.incrNumItems(entry1.get.numItems)\n+        }\n+      }\n+      r2\n+    }\n+  }\n+\n+  /**\n+   * Given the result returned by the aggregate function, we need to determine the threshold used\n+   * to accept items to generate the exact sample size.\n+   */\n+  def computeThresholdByKey[K](finalResult: Map[K, Stratum], fractionByKey: (K => Double)):"
  }, {
    "author": {
      "login": "falaki"
    },
    "body": "Style nit: Break the after the first argument, so the return type of the function is not on a new line by itself.\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-04T01:26:25Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.{Map, mutable}\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.{Logging, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.random.{PoissonBounds => PB}\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * A version of {@link #aggregate()} that passes the TaskContext to the function that does\n+   * aggregation for each partition. This function avoids creating an extra depth in the RDD\n+   * lineage, as opposed to using mapPartitionsWithId, which results in slightly improved run time.\n+   */\n+  def aggregateWithContext[U: ClassTag, T: ClassTag](zeroValue: U)\n+      (rdd: RDD[T],\n+       seqOp: ((TaskContext, U), T) => U,\n+       combOp: (U, U) => U): U = {\n+    val sc: SparkContext = rdd.sparkContext\n+    // Clone the zero value since we will also be serializing it as part of tasks\n+    var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance())\n+    // pad seqOp and combOp with taskContext to conform to aggregate's signature in TraversableOnce\n+    val paddedSeqOp = (arg1: (TaskContext, U), item: T) => (arg1._1, seqOp(arg1, item))\n+    val paddedcombOp = (arg1: (TaskContext, U), arg2: (TaskContext, U)) =>\n+      (arg1._1, combOp(arg1._2, arg1._2))\n+    val cleanSeqOp = sc.clean(paddedSeqOp)\n+    val cleanCombOp = sc.clean(paddedcombOp)\n+    val aggregatePartition = (tc: TaskContext, it: Iterator[T]) =>\n+      (it.aggregate(tc, zeroValue)(cleanSeqOp, cleanCombOp))._2\n+    val mergeResult = (index: Int, taskResult: U) => jobResult = combOp(jobResult, taskResult)\n+    sc.runJob(rdd, aggregatePartition, mergeResult)\n+    jobResult\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractionByKey: (K => Double),\n+      counts: Option[Map[K, Long]]): ((TaskContext, Result[K]),(K, V)) => Result[K] = {\n+    val delta = 5e-5\n+    (U: (TaskContext, Result[K]), item: (K, V)) => {\n+      val result = U._2\n+      val tc = U._1\n+      val rng = result.getRand(tc.partitionId)\n+      val fraction = fractionByKey(item._1)\n+      val stratum = result.getEntry(item._1)\n+      if (withReplacement) {\n+        // compute q1 and q2 only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.q1.isEmpty || stratum.q2.isEmpty) {\n+          val n = counts.get(item._1)\n+          val s = math.ceil(n * fraction).toLong\n+          val lmbd1 = PB.getLowerBound(s)\n+          val minCount = PB.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) PB.getUpperBound(s) else PB.getUpperBound(s - minCount)\n+          val q1 = lmbd1 / n\n+          val q2 = lmbd2 / n\n+          stratum.q1 = Some(q1)\n+          stratum.q2 = Some(q2)\n+        }\n+        val x1 = if (stratum.q1.get == 0) 0L else rng.nextPoisson(stratum.q1.get)\n+        if (x1 > 0) {\n+          stratum.incrNumAccepted(x1)\n+        }\n+        val x2 = rng.nextPoisson(stratum.q2.get).toInt\n+        if (x2 > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(x2)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement.\n+        // Hence, q1 and q2 change on every iteration.\n+        val g1 = - math.log(delta) / stratum.numItems\n+        val g2 = (2.0 / 3.0) * g1\n+        val q1 = math.max(0, fraction + g2 - math.sqrt((g2 * g2 + 3 * g2 * fraction)))\n+        val q2 = math.min(1, fraction + g1 + math.sqrt(g1 * g1 + 2 * g1 * fraction))\n+\n+        val x = rng.nextUniform(0.0, 1.0)\n+        if (x < q1) {\n+          stratum.incrNumAccepted()\n+        } else if (x < q2) {\n+          stratum.addToWaitList(x)\n+        }\n+        stratum.q1 = Some(q1)\n+        stratum.q2 = Some(q2)\n+      }\n+      stratum.incrNumItems()\n+      result\n+    }\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to combine results from different partitions, as\n+   * returned by seqOp.\n+   */\n+  def getCombOp[K](): (Result[K], Result[K]) => Result[K] = {\n+    (r1: Result[K], r2: Result[K]) => {\n+      // take union of both key sets in case one partition doesn't contain all keys\n+      val keyUnion = r1.resultMap.keys.toSet.union(r2.resultMap.keys.toSet)\n+\n+      // Use r2 to keep the combined result since r1 is usual empty\n+      for (key <- keyUnion) {\n+        val entry1 = r1.resultMap.get(key)\n+        val entry2 = r2.resultMap.get(key)\n+        if (entry2.isEmpty && entry1.isDefined) {\n+          r2.resultMap += (key -> entry1.get)\n+        } else if (entry1.isDefined && entry2.isDefined) {\n+          entry2.get.addToWaitList(entry1.get.waitList)\n+          entry2.get.incrNumAccepted(entry1.get.numAccepted)\n+          entry2.get.incrNumItems(entry1.get.numItems)\n+        }\n+      }\n+      r2\n+    }\n+  }\n+\n+  /**\n+   * Given the result returned by the aggregate function, we need to determine the threshold used\n+   * to accept items to generate the exact sample size.\n+   */\n+  def computeThresholdByKey[K](finalResult: Map[K, Stratum], fractionByKey: (K => Double)):"
  }],
  "prId": 1025
}, {
  "comments": [{
    "author": {
      "login": "falaki"
    },
    "body": "Similarly, let's change the type of fractionByKey to Map[K, Double]\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-04T00:40:23Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.{Map, mutable}\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.{Logging, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.random.{PoissonBounds => PB}\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * A version of {@link #aggregate()} that passes the TaskContext to the function that does\n+   * aggregation for each partition. This function avoids creating an extra depth in the RDD\n+   * lineage, as opposed to using mapPartitionsWithId, which results in slightly improved run time.\n+   */\n+  def aggregateWithContext[U: ClassTag, T: ClassTag](zeroValue: U)\n+      (rdd: RDD[T],\n+       seqOp: ((TaskContext, U), T) => U,\n+       combOp: (U, U) => U): U = {\n+    val sc: SparkContext = rdd.sparkContext\n+    // Clone the zero value since we will also be serializing it as part of tasks\n+    var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance())\n+    // pad seqOp and combOp with taskContext to conform to aggregate's signature in TraversableOnce\n+    val paddedSeqOp = (arg1: (TaskContext, U), item: T) => (arg1._1, seqOp(arg1, item))\n+    val paddedcombOp = (arg1: (TaskContext, U), arg2: (TaskContext, U)) =>\n+      (arg1._1, combOp(arg1._2, arg1._2))\n+    val cleanSeqOp = sc.clean(paddedSeqOp)\n+    val cleanCombOp = sc.clean(paddedcombOp)\n+    val aggregatePartition = (tc: TaskContext, it: Iterator[T]) =>\n+      (it.aggregate(tc, zeroValue)(cleanSeqOp, cleanCombOp))._2\n+    val mergeResult = (index: Int, taskResult: U) => jobResult = combOp(jobResult, taskResult)\n+    sc.runJob(rdd, aggregatePartition, mergeResult)\n+    jobResult\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractionByKey: (K => Double),\n+      counts: Option[Map[K, Long]]): ((TaskContext, Result[K]),(K, V)) => Result[K] = {\n+    val delta = 5e-5\n+    (U: (TaskContext, Result[K]), item: (K, V)) => {\n+      val result = U._2\n+      val tc = U._1\n+      val rng = result.getRand(tc.partitionId)\n+      val fraction = fractionByKey(item._1)\n+      val stratum = result.getEntry(item._1)\n+      if (withReplacement) {\n+        // compute q1 and q2 only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.q1.isEmpty || stratum.q2.isEmpty) {\n+          val n = counts.get(item._1)\n+          val s = math.ceil(n * fraction).toLong\n+          val lmbd1 = PB.getLowerBound(s)\n+          val minCount = PB.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) PB.getUpperBound(s) else PB.getUpperBound(s - minCount)\n+          val q1 = lmbd1 / n\n+          val q2 = lmbd2 / n\n+          stratum.q1 = Some(q1)\n+          stratum.q2 = Some(q2)\n+        }\n+        val x1 = if (stratum.q1.get == 0) 0L else rng.nextPoisson(stratum.q1.get)\n+        if (x1 > 0) {\n+          stratum.incrNumAccepted(x1)\n+        }\n+        val x2 = rng.nextPoisson(stratum.q2.get).toInt\n+        if (x2 > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(x2)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement.\n+        // Hence, q1 and q2 change on every iteration.\n+        val g1 = - math.log(delta) / stratum.numItems\n+        val g2 = (2.0 / 3.0) * g1\n+        val q1 = math.max(0, fraction + g2 - math.sqrt((g2 * g2 + 3 * g2 * fraction)))\n+        val q2 = math.min(1, fraction + g1 + math.sqrt(g1 * g1 + 2 * g1 * fraction))\n+\n+        val x = rng.nextUniform(0.0, 1.0)\n+        if (x < q1) {\n+          stratum.incrNumAccepted()\n+        } else if (x < q2) {\n+          stratum.addToWaitList(x)\n+        }\n+        stratum.q1 = Some(q1)\n+        stratum.q2 = Some(q2)\n+      }\n+      stratum.incrNumItems()\n+      result\n+    }\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to combine results from different partitions, as\n+   * returned by seqOp.\n+   */\n+  def getCombOp[K](): (Result[K], Result[K]) => Result[K] = {\n+    (r1: Result[K], r2: Result[K]) => {\n+      // take union of both key sets in case one partition doesn't contain all keys\n+      val keyUnion = r1.resultMap.keys.toSet.union(r2.resultMap.keys.toSet)\n+\n+      // Use r2 to keep the combined result since r1 is usual empty\n+      for (key <- keyUnion) {\n+        val entry1 = r1.resultMap.get(key)\n+        val entry2 = r2.resultMap.get(key)\n+        if (entry2.isEmpty && entry1.isDefined) {\n+          r2.resultMap += (key -> entry1.get)\n+        } else if (entry1.isDefined && entry2.isDefined) {\n+          entry2.get.addToWaitList(entry1.get.waitList)\n+          entry2.get.incrNumAccepted(entry1.get.numAccepted)\n+          entry2.get.incrNumItems(entry1.get.numItems)\n+        }\n+      }\n+      r2\n+    }\n+  }\n+\n+  /**\n+   * Given the result returned by the aggregate function, we need to determine the threshold used\n+   * to accept items to generate the exact sample size.\n+   */\n+  def computeThresholdByKey[K](finalResult: Map[K, Stratum], fractionByKey: (K => Double)):\n+    (K => Double) = {\n+    val thresholdByKey = new mutable.HashMap[K, Double]()\n+    for ((key, stratum) <- finalResult) {\n+      val fraction = fractionByKey(key)\n+      val s = math.ceil(stratum.numItems * fraction).toLong\n+      if (stratum.numAccepted > s) {\n+        logWarning(\"Pre-accepted too many\")\n+        thresholdByKey += (key -> stratum.q1.get)\n+      } else {\n+        val numWaitListAccepted = (s - stratum.numAccepted).toInt\n+        if (numWaitListAccepted >= stratum.waitList.size) {\n+          logWarning(\"WaitList too short\")\n+          thresholdByKey += (key -> stratum.q2.get)\n+        } else {\n+          thresholdByKey += (key -> stratum.waitList.sorted.apply(numWaitListAccepted))\n+        }\n+      }\n+    }\n+    thresholdByKey\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling without replacement.\n+   *\n+   * When exact sample size is required, we make an additional pass over the RDD to determine the\n+   * exact sampling rate that guarantees sample size with high confidence.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getBernoulliSamplingFunction[K, V](rdd: RDD[(K,  V)],\n+      fractionByKey: K => Double,"
  }],
  "prId": 1025
}, {
  "comments": [{
    "author": {
      "login": "falaki"
    },
    "body": "Space after comma\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-04T00:43:19Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.{Map, mutable}\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.{Logging, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.random.{PoissonBounds => PB}\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * A version of {@link #aggregate()} that passes the TaskContext to the function that does\n+   * aggregation for each partition. This function avoids creating an extra depth in the RDD\n+   * lineage, as opposed to using mapPartitionsWithId, which results in slightly improved run time.\n+   */\n+  def aggregateWithContext[U: ClassTag, T: ClassTag](zeroValue: U)\n+      (rdd: RDD[T],\n+       seqOp: ((TaskContext, U), T) => U,\n+       combOp: (U, U) => U): U = {\n+    val sc: SparkContext = rdd.sparkContext\n+    // Clone the zero value since we will also be serializing it as part of tasks\n+    var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance())\n+    // pad seqOp and combOp with taskContext to conform to aggregate's signature in TraversableOnce\n+    val paddedSeqOp = (arg1: (TaskContext, U), item: T) => (arg1._1, seqOp(arg1, item))\n+    val paddedcombOp = (arg1: (TaskContext, U), arg2: (TaskContext, U)) =>\n+      (arg1._1, combOp(arg1._2, arg1._2))\n+    val cleanSeqOp = sc.clean(paddedSeqOp)\n+    val cleanCombOp = sc.clean(paddedcombOp)\n+    val aggregatePartition = (tc: TaskContext, it: Iterator[T]) =>\n+      (it.aggregate(tc, zeroValue)(cleanSeqOp, cleanCombOp))._2\n+    val mergeResult = (index: Int, taskResult: U) => jobResult = combOp(jobResult, taskResult)\n+    sc.runJob(rdd, aggregatePartition, mergeResult)\n+    jobResult\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractionByKey: (K => Double),\n+      counts: Option[Map[K, Long]]): ((TaskContext, Result[K]),(K, V)) => Result[K] = {"
  }],
  "prId": 1025
}, {
  "comments": [{
    "author": {
      "login": "falaki"
    },
    "body": "We use uppercase letters for type parameters. Let's rename 'U' to a more descriptive name.\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-04T00:45:47Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.{Map, mutable}\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.{Logging, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.random.{PoissonBounds => PB}\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * A version of {@link #aggregate()} that passes the TaskContext to the function that does\n+   * aggregation for each partition. This function avoids creating an extra depth in the RDD\n+   * lineage, as opposed to using mapPartitionsWithId, which results in slightly improved run time.\n+   */\n+  def aggregateWithContext[U: ClassTag, T: ClassTag](zeroValue: U)\n+      (rdd: RDD[T],\n+       seqOp: ((TaskContext, U), T) => U,\n+       combOp: (U, U) => U): U = {\n+    val sc: SparkContext = rdd.sparkContext\n+    // Clone the zero value since we will also be serializing it as part of tasks\n+    var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance())\n+    // pad seqOp and combOp with taskContext to conform to aggregate's signature in TraversableOnce\n+    val paddedSeqOp = (arg1: (TaskContext, U), item: T) => (arg1._1, seqOp(arg1, item))\n+    val paddedcombOp = (arg1: (TaskContext, U), arg2: (TaskContext, U)) =>\n+      (arg1._1, combOp(arg1._2, arg1._2))\n+    val cleanSeqOp = sc.clean(paddedSeqOp)\n+    val cleanCombOp = sc.clean(paddedcombOp)\n+    val aggregatePartition = (tc: TaskContext, it: Iterator[T]) =>\n+      (it.aggregate(tc, zeroValue)(cleanSeqOp, cleanCombOp))._2\n+    val mergeResult = (index: Int, taskResult: U) => jobResult = combOp(jobResult, taskResult)\n+    sc.runJob(rdd, aggregatePartition, mergeResult)\n+    jobResult\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractionByKey: (K => Double),\n+      counts: Option[Map[K, Long]]): ((TaskContext, Result[K]),(K, V)) => Result[K] = {\n+    val delta = 5e-5\n+    (U: (TaskContext, Result[K]), item: (K, V)) => {"
  }],
  "prId": 1025
}, {
  "comments": [{
    "author": {
      "login": "falaki"
    },
    "body": "Similarly, \"[random]\" can be removed here.\nAlso please change the indent of arguments in next lines to two tabs (see the style guide at: https://cwiki.apache.org/confluence/display/SPARK/Spark+Code+Style+Guide#SparkCodeStyleGuide-Indentation)\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-04T00:53:02Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.{Map, mutable}\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.{Logging, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.random.{PoissonBounds => PB}\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * A version of {@link #aggregate()} that passes the TaskContext to the function that does\n+   * aggregation for each partition. This function avoids creating an extra depth in the RDD\n+   * lineage, as opposed to using mapPartitionsWithId, which results in slightly improved run time.\n+   */\n+  def aggregateWithContext[U: ClassTag, T: ClassTag](zeroValue: U)\n+      (rdd: RDD[T],\n+       seqOp: ((TaskContext, U), T) => U,\n+       combOp: (U, U) => U): U = {\n+    val sc: SparkContext = rdd.sparkContext\n+    // Clone the zero value since we will also be serializing it as part of tasks\n+    var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance())\n+    // pad seqOp and combOp with taskContext to conform to aggregate's signature in TraversableOnce\n+    val paddedSeqOp = (arg1: (TaskContext, U), item: T) => (arg1._1, seqOp(arg1, item))\n+    val paddedcombOp = (arg1: (TaskContext, U), arg2: (TaskContext, U)) =>\n+      (arg1._1, combOp(arg1._2, arg1._2))\n+    val cleanSeqOp = sc.clean(paddedSeqOp)\n+    val cleanCombOp = sc.clean(paddedcombOp)\n+    val aggregatePartition = (tc: TaskContext, it: Iterator[T]) =>\n+      (it.aggregate(tc, zeroValue)(cleanSeqOp, cleanCombOp))._2\n+    val mergeResult = (index: Int, taskResult: U) => jobResult = combOp(jobResult, taskResult)\n+    sc.runJob(rdd, aggregatePartition, mergeResult)\n+    jobResult\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractionByKey: (K => Double),\n+      counts: Option[Map[K, Long]]): ((TaskContext, Result[K]),(K, V)) => Result[K] = {\n+    val delta = 5e-5\n+    (U: (TaskContext, Result[K]), item: (K, V)) => {\n+      val result = U._2\n+      val tc = U._1\n+      val rng = result.getRand(tc.partitionId)\n+      val fraction = fractionByKey(item._1)\n+      val stratum = result.getEntry(item._1)\n+      if (withReplacement) {\n+        // compute q1 and q2 only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.q1.isEmpty || stratum.q2.isEmpty) {\n+          val n = counts.get(item._1)\n+          val s = math.ceil(n * fraction).toLong\n+          val lmbd1 = PB.getLowerBound(s)\n+          val minCount = PB.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) PB.getUpperBound(s) else PB.getUpperBound(s - minCount)\n+          val q1 = lmbd1 / n\n+          val q2 = lmbd2 / n\n+          stratum.q1 = Some(q1)\n+          stratum.q2 = Some(q2)\n+        }\n+        val x1 = if (stratum.q1.get == 0) 0L else rng.nextPoisson(stratum.q1.get)\n+        if (x1 > 0) {\n+          stratum.incrNumAccepted(x1)\n+        }\n+        val x2 = rng.nextPoisson(stratum.q2.get).toInt\n+        if (x2 > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(x2)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement.\n+        // Hence, q1 and q2 change on every iteration.\n+        val g1 = - math.log(delta) / stratum.numItems\n+        val g2 = (2.0 / 3.0) * g1\n+        val q1 = math.max(0, fraction + g2 - math.sqrt((g2 * g2 + 3 * g2 * fraction)))\n+        val q2 = math.min(1, fraction + g1 + math.sqrt(g1 * g1 + 2 * g1 * fraction))\n+\n+        val x = rng.nextUniform(0.0, 1.0)\n+        if (x < q1) {\n+          stratum.incrNumAccepted()\n+        } else if (x < q2) {\n+          stratum.addToWaitList(x)\n+        }\n+        stratum.q1 = Some(q1)\n+        stratum.q2 = Some(q2)\n+      }\n+      stratum.incrNumItems()\n+      result\n+    }\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to combine results from different partitions, as\n+   * returned by seqOp.\n+   */\n+  def getCombOp[K](): (Result[K], Result[K]) => Result[K] = {\n+    (r1: Result[K], r2: Result[K]) => {\n+      // take union of both key sets in case one partition doesn't contain all keys\n+      val keyUnion = r1.resultMap.keys.toSet.union(r2.resultMap.keys.toSet)\n+\n+      // Use r2 to keep the combined result since r1 is usual empty\n+      for (key <- keyUnion) {\n+        val entry1 = r1.resultMap.get(key)\n+        val entry2 = r2.resultMap.get(key)\n+        if (entry2.isEmpty && entry1.isDefined) {\n+          r2.resultMap += (key -> entry1.get)\n+        } else if (entry1.isDefined && entry2.isDefined) {\n+          entry2.get.addToWaitList(entry1.get.waitList)\n+          entry2.get.incrNumAccepted(entry1.get.numAccepted)\n+          entry2.get.incrNumItems(entry1.get.numItems)\n+        }\n+      }\n+      r2\n+    }\n+  }\n+\n+  /**\n+   * Given the result returned by the aggregate function, we need to determine the threshold used\n+   * to accept items to generate the exact sample size.\n+   */\n+  def computeThresholdByKey[K](finalResult: Map[K, Stratum], fractionByKey: (K => Double)):\n+    (K => Double) = {\n+    val thresholdByKey = new mutable.HashMap[K, Double]()\n+    for ((key, stratum) <- finalResult) {\n+      val fraction = fractionByKey(key)\n+      val s = math.ceil(stratum.numItems * fraction).toLong\n+      if (stratum.numAccepted > s) {\n+        logWarning(\"Pre-accepted too many\")\n+        thresholdByKey += (key -> stratum.q1.get)\n+      } else {\n+        val numWaitListAccepted = (s - stratum.numAccepted).toInt\n+        if (numWaitListAccepted >= stratum.waitList.size) {\n+          logWarning(\"WaitList too short\")\n+          thresholdByKey += (key -> stratum.q2.get)\n+        } else {\n+          thresholdByKey += (key -> stratum.waitList.sorted.apply(numWaitListAccepted))\n+        }\n+      }\n+    }\n+    thresholdByKey\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling without replacement.\n+   *\n+   * When exact sample size is required, we make an additional pass over the RDD to determine the\n+   * exact sampling rate that guarantees sample size with high confidence.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getBernoulliSamplingFunction[K, V](rdd: RDD[(K,  V)],\n+      fractionByKey: K => Double,\n+      exact: Boolean,\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    var samplingRateByKey = fractionByKey\n+    if (exact) {\n+      // determine threshold for each stratum and resample\n+      val seqOp = StratifiedSampler.getSeqOp[K,V](false, fractionByKey, None)\n+      val combOp = StratifiedSampler.getCombOp[K]()\n+      val zeroU = new Result[K](Map[K, Stratum](), seed = seed)\n+      val finalResult = aggregateWithContext(zeroU)(rdd, seqOp, combOp).resultMap\n+      samplingRateByKey = StratifiedSampler.computeThresholdByKey(finalResult, fractionByKey)\n+    }\n+    (idx: Int, iter: Iterator[(K, V)]) => {\n+      val random = new RandomDataGenerator\n+      random.reSeed(seed + idx)\n+      iter.filter(t => random.nextUniform(0.0, 1.0) < samplingRateByKey(t._1))\n+    }\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling with replacement.\n+   *\n+   * When exact sample size is required, we make two additional passed over the RDD to determine\n+   * the exact sampling rate that guarantees sample size with high confidence. The first pass\n+   * counts the number of items in each stratum (group of items with the same key) in the RDD, and\n+   * the second pass uses the counts to determine exact sampling rates.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getPoissonSamplingFunction[K, V](rdd:RDD[(K,  V)],\n+      fractionByKey: K => Double,\n+      exact: Boolean,\n+      counts: Option[Map[K, Long]],\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    // TODO implement the streaming version of sampling w/ replacement that doesn't require counts\n+    if (exact) {\n+      val seqOp = StratifiedSampler.getSeqOp[K,V](true, fractionByKey, counts)\n+      val combOp = StratifiedSampler.getCombOp[K]()\n+      val zeroU = new Result[K](Map[K, Stratum](), seed = seed)\n+      val finalResult = aggregateWithContext(zeroU)(rdd, seqOp, combOp).resultMap\n+      val thresholdByKey = StratifiedSampler.computeThresholdByKey(finalResult, fractionByKey)\n+      (idx: Int, iter: Iterator[(K, V)]) => {\n+        val random = new RandomDataGenerator()\n+        random.reSeed(seed + idx)\n+        iter.flatMap { t =>\n+          val q1 = finalResult.get(t._1).get.q1.getOrElse(0.0)\n+          val q2 = finalResult.get(t._1).get.q2.getOrElse(0.0)\n+          val x1 = if (q1 == 0) 0L else random.nextPoisson(q1)\n+          val x2 = random.nextPoisson(q2).toInt\n+          val x = x1 + (0 until x2).count(i => random.nextUniform(0.0, 1.0) < thresholdByKey(t._1))\n+          if (x > 0) {\n+            Iterator.fill(x.toInt)(t)\n+          } else {\n+            Iterator.empty\n+          }\n+        }\n+      }\n+    } else {\n+      (idx: Int, iter: Iterator[(K, V)]) => {\n+        val random = new RandomDataGenerator()\n+        random.reSeed(seed + idx)\n+        iter.flatMap { t =>\n+          val count = random.nextPoisson(fractionByKey(t._1)).toInt\n+          if (count > 0) {\n+            Iterator.fill(count)(t)\n+          } else {\n+            Iterator.empty\n+          }\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * Object used by seqOp to keep track of the number of items accepted and items waitlisted per\n+ * stratum, as well as the bounds for accepting and waitlisting items.\n+ */\n+private[random] class Stratum(var numItems: Long = 0L, var numAccepted: Long = 0L)\n+  extends Serializable {\n+\n+  var waitList: ArrayBuffer[Double] = new ArrayBuffer[Double]\n+  var q1: Option[Double] = None // upper bound for accepting item instantly\n+  var q2: Option[Double] = None // upper bound for adding item to waitlist\n+\n+  def incrNumItems(by: Long = 1L) = numItems += by\n+\n+  def incrNumAccepted(by: Long = 1L) = numAccepted += by\n+\n+  def addToWaitList(elem: Double) = waitList += elem\n+\n+  def addToWaitList(elems: ArrayBuffer[Double]) = waitList ++= elems\n+}\n+\n+/**\n+ * Object used by seqOp and combOp to keep track of the sampling statistics for all strata.\n+ *\n+ * When used by seqOp for each partition, we also keep track of the partition ID in this object\n+ * to make sure a single random number generator with a unique seed is used for each partition.\n+ */\n+private[random] class Result[K](var resultMap: Map[K, Stratum],"
  }, {
    "author": {
      "login": "falaki"
    },
    "body": "resultMap can be a value, of type mutable Map.\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-04T01:20:16Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.{Map, mutable}\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.{Logging, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.random.{PoissonBounds => PB}\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * A version of {@link #aggregate()} that passes the TaskContext to the function that does\n+   * aggregation for each partition. This function avoids creating an extra depth in the RDD\n+   * lineage, as opposed to using mapPartitionsWithId, which results in slightly improved run time.\n+   */\n+  def aggregateWithContext[U: ClassTag, T: ClassTag](zeroValue: U)\n+      (rdd: RDD[T],\n+       seqOp: ((TaskContext, U), T) => U,\n+       combOp: (U, U) => U): U = {\n+    val sc: SparkContext = rdd.sparkContext\n+    // Clone the zero value since we will also be serializing it as part of tasks\n+    var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance())\n+    // pad seqOp and combOp with taskContext to conform to aggregate's signature in TraversableOnce\n+    val paddedSeqOp = (arg1: (TaskContext, U), item: T) => (arg1._1, seqOp(arg1, item))\n+    val paddedcombOp = (arg1: (TaskContext, U), arg2: (TaskContext, U)) =>\n+      (arg1._1, combOp(arg1._2, arg1._2))\n+    val cleanSeqOp = sc.clean(paddedSeqOp)\n+    val cleanCombOp = sc.clean(paddedcombOp)\n+    val aggregatePartition = (tc: TaskContext, it: Iterator[T]) =>\n+      (it.aggregate(tc, zeroValue)(cleanSeqOp, cleanCombOp))._2\n+    val mergeResult = (index: Int, taskResult: U) => jobResult = combOp(jobResult, taskResult)\n+    sc.runJob(rdd, aggregatePartition, mergeResult)\n+    jobResult\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractionByKey: (K => Double),\n+      counts: Option[Map[K, Long]]): ((TaskContext, Result[K]),(K, V)) => Result[K] = {\n+    val delta = 5e-5\n+    (U: (TaskContext, Result[K]), item: (K, V)) => {\n+      val result = U._2\n+      val tc = U._1\n+      val rng = result.getRand(tc.partitionId)\n+      val fraction = fractionByKey(item._1)\n+      val stratum = result.getEntry(item._1)\n+      if (withReplacement) {\n+        // compute q1 and q2 only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.q1.isEmpty || stratum.q2.isEmpty) {\n+          val n = counts.get(item._1)\n+          val s = math.ceil(n * fraction).toLong\n+          val lmbd1 = PB.getLowerBound(s)\n+          val minCount = PB.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) PB.getUpperBound(s) else PB.getUpperBound(s - minCount)\n+          val q1 = lmbd1 / n\n+          val q2 = lmbd2 / n\n+          stratum.q1 = Some(q1)\n+          stratum.q2 = Some(q2)\n+        }\n+        val x1 = if (stratum.q1.get == 0) 0L else rng.nextPoisson(stratum.q1.get)\n+        if (x1 > 0) {\n+          stratum.incrNumAccepted(x1)\n+        }\n+        val x2 = rng.nextPoisson(stratum.q2.get).toInt\n+        if (x2 > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(x2)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement.\n+        // Hence, q1 and q2 change on every iteration.\n+        val g1 = - math.log(delta) / stratum.numItems\n+        val g2 = (2.0 / 3.0) * g1\n+        val q1 = math.max(0, fraction + g2 - math.sqrt((g2 * g2 + 3 * g2 * fraction)))\n+        val q2 = math.min(1, fraction + g1 + math.sqrt(g1 * g1 + 2 * g1 * fraction))\n+\n+        val x = rng.nextUniform(0.0, 1.0)\n+        if (x < q1) {\n+          stratum.incrNumAccepted()\n+        } else if (x < q2) {\n+          stratum.addToWaitList(x)\n+        }\n+        stratum.q1 = Some(q1)\n+        stratum.q2 = Some(q2)\n+      }\n+      stratum.incrNumItems()\n+      result\n+    }\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to combine results from different partitions, as\n+   * returned by seqOp.\n+   */\n+  def getCombOp[K](): (Result[K], Result[K]) => Result[K] = {\n+    (r1: Result[K], r2: Result[K]) => {\n+      // take union of both key sets in case one partition doesn't contain all keys\n+      val keyUnion = r1.resultMap.keys.toSet.union(r2.resultMap.keys.toSet)\n+\n+      // Use r2 to keep the combined result since r1 is usual empty\n+      for (key <- keyUnion) {\n+        val entry1 = r1.resultMap.get(key)\n+        val entry2 = r2.resultMap.get(key)\n+        if (entry2.isEmpty && entry1.isDefined) {\n+          r2.resultMap += (key -> entry1.get)\n+        } else if (entry1.isDefined && entry2.isDefined) {\n+          entry2.get.addToWaitList(entry1.get.waitList)\n+          entry2.get.incrNumAccepted(entry1.get.numAccepted)\n+          entry2.get.incrNumItems(entry1.get.numItems)\n+        }\n+      }\n+      r2\n+    }\n+  }\n+\n+  /**\n+   * Given the result returned by the aggregate function, we need to determine the threshold used\n+   * to accept items to generate the exact sample size.\n+   */\n+  def computeThresholdByKey[K](finalResult: Map[K, Stratum], fractionByKey: (K => Double)):\n+    (K => Double) = {\n+    val thresholdByKey = new mutable.HashMap[K, Double]()\n+    for ((key, stratum) <- finalResult) {\n+      val fraction = fractionByKey(key)\n+      val s = math.ceil(stratum.numItems * fraction).toLong\n+      if (stratum.numAccepted > s) {\n+        logWarning(\"Pre-accepted too many\")\n+        thresholdByKey += (key -> stratum.q1.get)\n+      } else {\n+        val numWaitListAccepted = (s - stratum.numAccepted).toInt\n+        if (numWaitListAccepted >= stratum.waitList.size) {\n+          logWarning(\"WaitList too short\")\n+          thresholdByKey += (key -> stratum.q2.get)\n+        } else {\n+          thresholdByKey += (key -> stratum.waitList.sorted.apply(numWaitListAccepted))\n+        }\n+      }\n+    }\n+    thresholdByKey\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling without replacement.\n+   *\n+   * When exact sample size is required, we make an additional pass over the RDD to determine the\n+   * exact sampling rate that guarantees sample size with high confidence.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getBernoulliSamplingFunction[K, V](rdd: RDD[(K,  V)],\n+      fractionByKey: K => Double,\n+      exact: Boolean,\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    var samplingRateByKey = fractionByKey\n+    if (exact) {\n+      // determine threshold for each stratum and resample\n+      val seqOp = StratifiedSampler.getSeqOp[K,V](false, fractionByKey, None)\n+      val combOp = StratifiedSampler.getCombOp[K]()\n+      val zeroU = new Result[K](Map[K, Stratum](), seed = seed)\n+      val finalResult = aggregateWithContext(zeroU)(rdd, seqOp, combOp).resultMap\n+      samplingRateByKey = StratifiedSampler.computeThresholdByKey(finalResult, fractionByKey)\n+    }\n+    (idx: Int, iter: Iterator[(K, V)]) => {\n+      val random = new RandomDataGenerator\n+      random.reSeed(seed + idx)\n+      iter.filter(t => random.nextUniform(0.0, 1.0) < samplingRateByKey(t._1))\n+    }\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling with replacement.\n+   *\n+   * When exact sample size is required, we make two additional passed over the RDD to determine\n+   * the exact sampling rate that guarantees sample size with high confidence. The first pass\n+   * counts the number of items in each stratum (group of items with the same key) in the RDD, and\n+   * the second pass uses the counts to determine exact sampling rates.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getPoissonSamplingFunction[K, V](rdd:RDD[(K,  V)],\n+      fractionByKey: K => Double,\n+      exact: Boolean,\n+      counts: Option[Map[K, Long]],\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    // TODO implement the streaming version of sampling w/ replacement that doesn't require counts\n+    if (exact) {\n+      val seqOp = StratifiedSampler.getSeqOp[K,V](true, fractionByKey, counts)\n+      val combOp = StratifiedSampler.getCombOp[K]()\n+      val zeroU = new Result[K](Map[K, Stratum](), seed = seed)\n+      val finalResult = aggregateWithContext(zeroU)(rdd, seqOp, combOp).resultMap\n+      val thresholdByKey = StratifiedSampler.computeThresholdByKey(finalResult, fractionByKey)\n+      (idx: Int, iter: Iterator[(K, V)]) => {\n+        val random = new RandomDataGenerator()\n+        random.reSeed(seed + idx)\n+        iter.flatMap { t =>\n+          val q1 = finalResult.get(t._1).get.q1.getOrElse(0.0)\n+          val q2 = finalResult.get(t._1).get.q2.getOrElse(0.0)\n+          val x1 = if (q1 == 0) 0L else random.nextPoisson(q1)\n+          val x2 = random.nextPoisson(q2).toInt\n+          val x = x1 + (0 until x2).count(i => random.nextUniform(0.0, 1.0) < thresholdByKey(t._1))\n+          if (x > 0) {\n+            Iterator.fill(x.toInt)(t)\n+          } else {\n+            Iterator.empty\n+          }\n+        }\n+      }\n+    } else {\n+      (idx: Int, iter: Iterator[(K, V)]) => {\n+        val random = new RandomDataGenerator()\n+        random.reSeed(seed + idx)\n+        iter.flatMap { t =>\n+          val count = random.nextPoisson(fractionByKey(t._1)).toInt\n+          if (count > 0) {\n+            Iterator.fill(count)(t)\n+          } else {\n+            Iterator.empty\n+          }\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * Object used by seqOp to keep track of the number of items accepted and items waitlisted per\n+ * stratum, as well as the bounds for accepting and waitlisting items.\n+ */\n+private[random] class Stratum(var numItems: Long = 0L, var numAccepted: Long = 0L)\n+  extends Serializable {\n+\n+  var waitList: ArrayBuffer[Double] = new ArrayBuffer[Double]\n+  var q1: Option[Double] = None // upper bound for accepting item instantly\n+  var q2: Option[Double] = None // upper bound for adding item to waitlist\n+\n+  def incrNumItems(by: Long = 1L) = numItems += by\n+\n+  def incrNumAccepted(by: Long = 1L) = numAccepted += by\n+\n+  def addToWaitList(elem: Double) = waitList += elem\n+\n+  def addToWaitList(elems: ArrayBuffer[Double]) = waitList ++= elems\n+}\n+\n+/**\n+ * Object used by seqOp and combOp to keep track of the sampling statistics for all strata.\n+ *\n+ * When used by seqOp for each partition, we also keep track of the partition ID in this object\n+ * to make sure a single random number generator with a unique seed is used for each partition.\n+ */\n+private[random] class Result[K](var resultMap: Map[K, Stratum],"
  }, {
    "author": {
      "login": "dorx"
    },
    "body": "Actually if I remove the \"[random]\" here I get an out-of-scope error: \"private class Result escapes its defining scope as part of type (org.apache.spark.util.random.Result[K], org.apache.spark.util.random.Result[K]) => org.apache.spark.util.random.Result[K]\"\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-06T23:00:45Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.{Map, mutable}\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.{Logging, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.random.{PoissonBounds => PB}\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * A version of {@link #aggregate()} that passes the TaskContext to the function that does\n+   * aggregation for each partition. This function avoids creating an extra depth in the RDD\n+   * lineage, as opposed to using mapPartitionsWithId, which results in slightly improved run time.\n+   */\n+  def aggregateWithContext[U: ClassTag, T: ClassTag](zeroValue: U)\n+      (rdd: RDD[T],\n+       seqOp: ((TaskContext, U), T) => U,\n+       combOp: (U, U) => U): U = {\n+    val sc: SparkContext = rdd.sparkContext\n+    // Clone the zero value since we will also be serializing it as part of tasks\n+    var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance())\n+    // pad seqOp and combOp with taskContext to conform to aggregate's signature in TraversableOnce\n+    val paddedSeqOp = (arg1: (TaskContext, U), item: T) => (arg1._1, seqOp(arg1, item))\n+    val paddedcombOp = (arg1: (TaskContext, U), arg2: (TaskContext, U)) =>\n+      (arg1._1, combOp(arg1._2, arg1._2))\n+    val cleanSeqOp = sc.clean(paddedSeqOp)\n+    val cleanCombOp = sc.clean(paddedcombOp)\n+    val aggregatePartition = (tc: TaskContext, it: Iterator[T]) =>\n+      (it.aggregate(tc, zeroValue)(cleanSeqOp, cleanCombOp))._2\n+    val mergeResult = (index: Int, taskResult: U) => jobResult = combOp(jobResult, taskResult)\n+    sc.runJob(rdd, aggregatePartition, mergeResult)\n+    jobResult\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractionByKey: (K => Double),\n+      counts: Option[Map[K, Long]]): ((TaskContext, Result[K]),(K, V)) => Result[K] = {\n+    val delta = 5e-5\n+    (U: (TaskContext, Result[K]), item: (K, V)) => {\n+      val result = U._2\n+      val tc = U._1\n+      val rng = result.getRand(tc.partitionId)\n+      val fraction = fractionByKey(item._1)\n+      val stratum = result.getEntry(item._1)\n+      if (withReplacement) {\n+        // compute q1 and q2 only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.q1.isEmpty || stratum.q2.isEmpty) {\n+          val n = counts.get(item._1)\n+          val s = math.ceil(n * fraction).toLong\n+          val lmbd1 = PB.getLowerBound(s)\n+          val minCount = PB.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) PB.getUpperBound(s) else PB.getUpperBound(s - minCount)\n+          val q1 = lmbd1 / n\n+          val q2 = lmbd2 / n\n+          stratum.q1 = Some(q1)\n+          stratum.q2 = Some(q2)\n+        }\n+        val x1 = if (stratum.q1.get == 0) 0L else rng.nextPoisson(stratum.q1.get)\n+        if (x1 > 0) {\n+          stratum.incrNumAccepted(x1)\n+        }\n+        val x2 = rng.nextPoisson(stratum.q2.get).toInt\n+        if (x2 > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(x2)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement.\n+        // Hence, q1 and q2 change on every iteration.\n+        val g1 = - math.log(delta) / stratum.numItems\n+        val g2 = (2.0 / 3.0) * g1\n+        val q1 = math.max(0, fraction + g2 - math.sqrt((g2 * g2 + 3 * g2 * fraction)))\n+        val q2 = math.min(1, fraction + g1 + math.sqrt(g1 * g1 + 2 * g1 * fraction))\n+\n+        val x = rng.nextUniform(0.0, 1.0)\n+        if (x < q1) {\n+          stratum.incrNumAccepted()\n+        } else if (x < q2) {\n+          stratum.addToWaitList(x)\n+        }\n+        stratum.q1 = Some(q1)\n+        stratum.q2 = Some(q2)\n+      }\n+      stratum.incrNumItems()\n+      result\n+    }\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to combine results from different partitions, as\n+   * returned by seqOp.\n+   */\n+  def getCombOp[K](): (Result[K], Result[K]) => Result[K] = {\n+    (r1: Result[K], r2: Result[K]) => {\n+      // take union of both key sets in case one partition doesn't contain all keys\n+      val keyUnion = r1.resultMap.keys.toSet.union(r2.resultMap.keys.toSet)\n+\n+      // Use r2 to keep the combined result since r1 is usual empty\n+      for (key <- keyUnion) {\n+        val entry1 = r1.resultMap.get(key)\n+        val entry2 = r2.resultMap.get(key)\n+        if (entry2.isEmpty && entry1.isDefined) {\n+          r2.resultMap += (key -> entry1.get)\n+        } else if (entry1.isDefined && entry2.isDefined) {\n+          entry2.get.addToWaitList(entry1.get.waitList)\n+          entry2.get.incrNumAccepted(entry1.get.numAccepted)\n+          entry2.get.incrNumItems(entry1.get.numItems)\n+        }\n+      }\n+      r2\n+    }\n+  }\n+\n+  /**\n+   * Given the result returned by the aggregate function, we need to determine the threshold used\n+   * to accept items to generate the exact sample size.\n+   */\n+  def computeThresholdByKey[K](finalResult: Map[K, Stratum], fractionByKey: (K => Double)):\n+    (K => Double) = {\n+    val thresholdByKey = new mutable.HashMap[K, Double]()\n+    for ((key, stratum) <- finalResult) {\n+      val fraction = fractionByKey(key)\n+      val s = math.ceil(stratum.numItems * fraction).toLong\n+      if (stratum.numAccepted > s) {\n+        logWarning(\"Pre-accepted too many\")\n+        thresholdByKey += (key -> stratum.q1.get)\n+      } else {\n+        val numWaitListAccepted = (s - stratum.numAccepted).toInt\n+        if (numWaitListAccepted >= stratum.waitList.size) {\n+          logWarning(\"WaitList too short\")\n+          thresholdByKey += (key -> stratum.q2.get)\n+        } else {\n+          thresholdByKey += (key -> stratum.waitList.sorted.apply(numWaitListAccepted))\n+        }\n+      }\n+    }\n+    thresholdByKey\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling without replacement.\n+   *\n+   * When exact sample size is required, we make an additional pass over the RDD to determine the\n+   * exact sampling rate that guarantees sample size with high confidence.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getBernoulliSamplingFunction[K, V](rdd: RDD[(K,  V)],\n+      fractionByKey: K => Double,\n+      exact: Boolean,\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    var samplingRateByKey = fractionByKey\n+    if (exact) {\n+      // determine threshold for each stratum and resample\n+      val seqOp = StratifiedSampler.getSeqOp[K,V](false, fractionByKey, None)\n+      val combOp = StratifiedSampler.getCombOp[K]()\n+      val zeroU = new Result[K](Map[K, Stratum](), seed = seed)\n+      val finalResult = aggregateWithContext(zeroU)(rdd, seqOp, combOp).resultMap\n+      samplingRateByKey = StratifiedSampler.computeThresholdByKey(finalResult, fractionByKey)\n+    }\n+    (idx: Int, iter: Iterator[(K, V)]) => {\n+      val random = new RandomDataGenerator\n+      random.reSeed(seed + idx)\n+      iter.filter(t => random.nextUniform(0.0, 1.0) < samplingRateByKey(t._1))\n+    }\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling with replacement.\n+   *\n+   * When exact sample size is required, we make two additional passed over the RDD to determine\n+   * the exact sampling rate that guarantees sample size with high confidence. The first pass\n+   * counts the number of items in each stratum (group of items with the same key) in the RDD, and\n+   * the second pass uses the counts to determine exact sampling rates.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getPoissonSamplingFunction[K, V](rdd:RDD[(K,  V)],\n+      fractionByKey: K => Double,\n+      exact: Boolean,\n+      counts: Option[Map[K, Long]],\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    // TODO implement the streaming version of sampling w/ replacement that doesn't require counts\n+    if (exact) {\n+      val seqOp = StratifiedSampler.getSeqOp[K,V](true, fractionByKey, counts)\n+      val combOp = StratifiedSampler.getCombOp[K]()\n+      val zeroU = new Result[K](Map[K, Stratum](), seed = seed)\n+      val finalResult = aggregateWithContext(zeroU)(rdd, seqOp, combOp).resultMap\n+      val thresholdByKey = StratifiedSampler.computeThresholdByKey(finalResult, fractionByKey)\n+      (idx: Int, iter: Iterator[(K, V)]) => {\n+        val random = new RandomDataGenerator()\n+        random.reSeed(seed + idx)\n+        iter.flatMap { t =>\n+          val q1 = finalResult.get(t._1).get.q1.getOrElse(0.0)\n+          val q2 = finalResult.get(t._1).get.q2.getOrElse(0.0)\n+          val x1 = if (q1 == 0) 0L else random.nextPoisson(q1)\n+          val x2 = random.nextPoisson(q2).toInt\n+          val x = x1 + (0 until x2).count(i => random.nextUniform(0.0, 1.0) < thresholdByKey(t._1))\n+          if (x > 0) {\n+            Iterator.fill(x.toInt)(t)\n+          } else {\n+            Iterator.empty\n+          }\n+        }\n+      }\n+    } else {\n+      (idx: Int, iter: Iterator[(K, V)]) => {\n+        val random = new RandomDataGenerator()\n+        random.reSeed(seed + idx)\n+        iter.flatMap { t =>\n+          val count = random.nextPoisson(fractionByKey(t._1)).toInt\n+          if (count > 0) {\n+            Iterator.fill(count)(t)\n+          } else {\n+            Iterator.empty\n+          }\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * Object used by seqOp to keep track of the number of items accepted and items waitlisted per\n+ * stratum, as well as the bounds for accepting and waitlisting items.\n+ */\n+private[random] class Stratum(var numItems: Long = 0L, var numAccepted: Long = 0L)\n+  extends Serializable {\n+\n+  var waitList: ArrayBuffer[Double] = new ArrayBuffer[Double]\n+  var q1: Option[Double] = None // upper bound for accepting item instantly\n+  var q2: Option[Double] = None // upper bound for adding item to waitlist\n+\n+  def incrNumItems(by: Long = 1L) = numItems += by\n+\n+  def incrNumAccepted(by: Long = 1L) = numAccepted += by\n+\n+  def addToWaitList(elem: Double) = waitList += elem\n+\n+  def addToWaitList(elems: ArrayBuffer[Double]) = waitList ++= elems\n+}\n+\n+/**\n+ * Object used by seqOp and combOp to keep track of the sampling statistics for all strata.\n+ *\n+ * When used by seqOp for each partition, we also keep track of the partition ID in this object\n+ * to make sure a single random number generator with a unique seed is used for each partition.\n+ */\n+private[random] class Result[K](var resultMap: Map[K, Stratum],"
  }],
  "prId": 1025
}, {
  "comments": [{
    "author": {
      "login": "falaki"
    },
    "body": "Let's not rename this. The original name is more descriptive.\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-04T01:12:45Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.{Map, mutable}\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.{Logging, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.random.{PoissonBounds => PB}"
  }],
  "prId": 1025
}, {
  "comments": [{
    "author": {
      "login": "falaki"
    },
    "body": "We are setting q1 and q2 from outside, let's add more methods to this class for setting and getting values of q1 and q2. Let's use better names than `updateq1()`. Even better would be if we rename q1 and q2 to more descriptive names (the comments you added help a lot by the way).\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-04T01:16:06Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.{Map, mutable}\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.{Logging, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.random.{PoissonBounds => PB}\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * A version of {@link #aggregate()} that passes the TaskContext to the function that does\n+   * aggregation for each partition. This function avoids creating an extra depth in the RDD\n+   * lineage, as opposed to using mapPartitionsWithId, which results in slightly improved run time.\n+   */\n+  def aggregateWithContext[U: ClassTag, T: ClassTag](zeroValue: U)\n+      (rdd: RDD[T],\n+       seqOp: ((TaskContext, U), T) => U,\n+       combOp: (U, U) => U): U = {\n+    val sc: SparkContext = rdd.sparkContext\n+    // Clone the zero value since we will also be serializing it as part of tasks\n+    var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance())\n+    // pad seqOp and combOp with taskContext to conform to aggregate's signature in TraversableOnce\n+    val paddedSeqOp = (arg1: (TaskContext, U), item: T) => (arg1._1, seqOp(arg1, item))\n+    val paddedcombOp = (arg1: (TaskContext, U), arg2: (TaskContext, U)) =>\n+      (arg1._1, combOp(arg1._2, arg1._2))\n+    val cleanSeqOp = sc.clean(paddedSeqOp)\n+    val cleanCombOp = sc.clean(paddedcombOp)\n+    val aggregatePartition = (tc: TaskContext, it: Iterator[T]) =>\n+      (it.aggregate(tc, zeroValue)(cleanSeqOp, cleanCombOp))._2\n+    val mergeResult = (index: Int, taskResult: U) => jobResult = combOp(jobResult, taskResult)\n+    sc.runJob(rdd, aggregatePartition, mergeResult)\n+    jobResult\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractionByKey: (K => Double),\n+      counts: Option[Map[K, Long]]): ((TaskContext, Result[K]),(K, V)) => Result[K] = {\n+    val delta = 5e-5\n+    (U: (TaskContext, Result[K]), item: (K, V)) => {\n+      val result = U._2\n+      val tc = U._1\n+      val rng = result.getRand(tc.partitionId)\n+      val fraction = fractionByKey(item._1)\n+      val stratum = result.getEntry(item._1)\n+      if (withReplacement) {\n+        // compute q1 and q2 only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.q1.isEmpty || stratum.q2.isEmpty) {\n+          val n = counts.get(item._1)\n+          val s = math.ceil(n * fraction).toLong\n+          val lmbd1 = PB.getLowerBound(s)\n+          val minCount = PB.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) PB.getUpperBound(s) else PB.getUpperBound(s - minCount)\n+          val q1 = lmbd1 / n\n+          val q2 = lmbd2 / n\n+          stratum.q1 = Some(q1)\n+          stratum.q2 = Some(q2)\n+        }\n+        val x1 = if (stratum.q1.get == 0) 0L else rng.nextPoisson(stratum.q1.get)\n+        if (x1 > 0) {\n+          stratum.incrNumAccepted(x1)\n+        }\n+        val x2 = rng.nextPoisson(stratum.q2.get).toInt\n+        if (x2 > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(x2)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement.\n+        // Hence, q1 and q2 change on every iteration.\n+        val g1 = - math.log(delta) / stratum.numItems\n+        val g2 = (2.0 / 3.0) * g1\n+        val q1 = math.max(0, fraction + g2 - math.sqrt((g2 * g2 + 3 * g2 * fraction)))\n+        val q2 = math.min(1, fraction + g1 + math.sqrt(g1 * g1 + 2 * g1 * fraction))\n+\n+        val x = rng.nextUniform(0.0, 1.0)\n+        if (x < q1) {\n+          stratum.incrNumAccepted()\n+        } else if (x < q2) {\n+          stratum.addToWaitList(x)\n+        }\n+        stratum.q1 = Some(q1)\n+        stratum.q2 = Some(q2)\n+      }\n+      stratum.incrNumItems()\n+      result\n+    }\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to combine results from different partitions, as\n+   * returned by seqOp.\n+   */\n+  def getCombOp[K](): (Result[K], Result[K]) => Result[K] = {\n+    (r1: Result[K], r2: Result[K]) => {\n+      // take union of both key sets in case one partition doesn't contain all keys\n+      val keyUnion = r1.resultMap.keys.toSet.union(r2.resultMap.keys.toSet)\n+\n+      // Use r2 to keep the combined result since r1 is usual empty\n+      for (key <- keyUnion) {\n+        val entry1 = r1.resultMap.get(key)\n+        val entry2 = r2.resultMap.get(key)\n+        if (entry2.isEmpty && entry1.isDefined) {\n+          r2.resultMap += (key -> entry1.get)\n+        } else if (entry1.isDefined && entry2.isDefined) {\n+          entry2.get.addToWaitList(entry1.get.waitList)\n+          entry2.get.incrNumAccepted(entry1.get.numAccepted)\n+          entry2.get.incrNumItems(entry1.get.numItems)\n+        }\n+      }\n+      r2\n+    }\n+  }\n+\n+  /**\n+   * Given the result returned by the aggregate function, we need to determine the threshold used\n+   * to accept items to generate the exact sample size.\n+   */\n+  def computeThresholdByKey[K](finalResult: Map[K, Stratum], fractionByKey: (K => Double)):\n+    (K => Double) = {\n+    val thresholdByKey = new mutable.HashMap[K, Double]()\n+    for ((key, stratum) <- finalResult) {\n+      val fraction = fractionByKey(key)\n+      val s = math.ceil(stratum.numItems * fraction).toLong\n+      if (stratum.numAccepted > s) {\n+        logWarning(\"Pre-accepted too many\")\n+        thresholdByKey += (key -> stratum.q1.get)\n+      } else {\n+        val numWaitListAccepted = (s - stratum.numAccepted).toInt\n+        if (numWaitListAccepted >= stratum.waitList.size) {\n+          logWarning(\"WaitList too short\")\n+          thresholdByKey += (key -> stratum.q2.get)\n+        } else {\n+          thresholdByKey += (key -> stratum.waitList.sorted.apply(numWaitListAccepted))\n+        }\n+      }\n+    }\n+    thresholdByKey\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling without replacement.\n+   *\n+   * When exact sample size is required, we make an additional pass over the RDD to determine the\n+   * exact sampling rate that guarantees sample size with high confidence.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getBernoulliSamplingFunction[K, V](rdd: RDD[(K,  V)],\n+      fractionByKey: K => Double,\n+      exact: Boolean,\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    var samplingRateByKey = fractionByKey\n+    if (exact) {\n+      // determine threshold for each stratum and resample\n+      val seqOp = StratifiedSampler.getSeqOp[K,V](false, fractionByKey, None)\n+      val combOp = StratifiedSampler.getCombOp[K]()\n+      val zeroU = new Result[K](Map[K, Stratum](), seed = seed)\n+      val finalResult = aggregateWithContext(zeroU)(rdd, seqOp, combOp).resultMap\n+      samplingRateByKey = StratifiedSampler.computeThresholdByKey(finalResult, fractionByKey)\n+    }\n+    (idx: Int, iter: Iterator[(K, V)]) => {\n+      val random = new RandomDataGenerator\n+      random.reSeed(seed + idx)\n+      iter.filter(t => random.nextUniform(0.0, 1.0) < samplingRateByKey(t._1))\n+    }\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling with replacement.\n+   *\n+   * When exact sample size is required, we make two additional passed over the RDD to determine\n+   * the exact sampling rate that guarantees sample size with high confidence. The first pass\n+   * counts the number of items in each stratum (group of items with the same key) in the RDD, and\n+   * the second pass uses the counts to determine exact sampling rates.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getPoissonSamplingFunction[K, V](rdd:RDD[(K,  V)],\n+      fractionByKey: K => Double,\n+      exact: Boolean,\n+      counts: Option[Map[K, Long]],\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    // TODO implement the streaming version of sampling w/ replacement that doesn't require counts\n+    if (exact) {\n+      val seqOp = StratifiedSampler.getSeqOp[K,V](true, fractionByKey, counts)\n+      val combOp = StratifiedSampler.getCombOp[K]()\n+      val zeroU = new Result[K](Map[K, Stratum](), seed = seed)\n+      val finalResult = aggregateWithContext(zeroU)(rdd, seqOp, combOp).resultMap\n+      val thresholdByKey = StratifiedSampler.computeThresholdByKey(finalResult, fractionByKey)\n+      (idx: Int, iter: Iterator[(K, V)]) => {\n+        val random = new RandomDataGenerator()\n+        random.reSeed(seed + idx)\n+        iter.flatMap { t =>\n+          val q1 = finalResult.get(t._1).get.q1.getOrElse(0.0)\n+          val q2 = finalResult.get(t._1).get.q2.getOrElse(0.0)\n+          val x1 = if (q1 == 0) 0L else random.nextPoisson(q1)\n+          val x2 = random.nextPoisson(q2).toInt\n+          val x = x1 + (0 until x2).count(i => random.nextUniform(0.0, 1.0) < thresholdByKey(t._1))\n+          if (x > 0) {\n+            Iterator.fill(x.toInt)(t)\n+          } else {\n+            Iterator.empty\n+          }\n+        }\n+      }\n+    } else {\n+      (idx: Int, iter: Iterator[(K, V)]) => {\n+        val random = new RandomDataGenerator()\n+        random.reSeed(seed + idx)\n+        iter.flatMap { t =>\n+          val count = random.nextPoisson(fractionByKey(t._1)).toInt\n+          if (count > 0) {\n+            Iterator.fill(count)(t)\n+          } else {\n+            Iterator.empty\n+          }\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * Object used by seqOp to keep track of the number of items accepted and items waitlisted per\n+ * stratum, as well as the bounds for accepting and waitlisting items.\n+ */\n+private[random] class Stratum(var numItems: Long = 0L, var numAccepted: Long = 0L)\n+  extends Serializable {\n+\n+  var waitList: ArrayBuffer[Double] = new ArrayBuffer[Double]\n+  var q1: Option[Double] = None // upper bound for accepting item instantly"
  }],
  "prId": 1025
}, {
  "comments": [{
    "author": {
      "login": "falaki"
    },
    "body": "This does not need to be a variable. ArrayBuffer is immutable and that seems enough.\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-04T01:16:30Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.{Map, mutable}\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.{Logging, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.random.{PoissonBounds => PB}\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * A version of {@link #aggregate()} that passes the TaskContext to the function that does\n+   * aggregation for each partition. This function avoids creating an extra depth in the RDD\n+   * lineage, as opposed to using mapPartitionsWithId, which results in slightly improved run time.\n+   */\n+  def aggregateWithContext[U: ClassTag, T: ClassTag](zeroValue: U)\n+      (rdd: RDD[T],\n+       seqOp: ((TaskContext, U), T) => U,\n+       combOp: (U, U) => U): U = {\n+    val sc: SparkContext = rdd.sparkContext\n+    // Clone the zero value since we will also be serializing it as part of tasks\n+    var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance())\n+    // pad seqOp and combOp with taskContext to conform to aggregate's signature in TraversableOnce\n+    val paddedSeqOp = (arg1: (TaskContext, U), item: T) => (arg1._1, seqOp(arg1, item))\n+    val paddedcombOp = (arg1: (TaskContext, U), arg2: (TaskContext, U)) =>\n+      (arg1._1, combOp(arg1._2, arg1._2))\n+    val cleanSeqOp = sc.clean(paddedSeqOp)\n+    val cleanCombOp = sc.clean(paddedcombOp)\n+    val aggregatePartition = (tc: TaskContext, it: Iterator[T]) =>\n+      (it.aggregate(tc, zeroValue)(cleanSeqOp, cleanCombOp))._2\n+    val mergeResult = (index: Int, taskResult: U) => jobResult = combOp(jobResult, taskResult)\n+    sc.runJob(rdd, aggregatePartition, mergeResult)\n+    jobResult\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractionByKey: (K => Double),\n+      counts: Option[Map[K, Long]]): ((TaskContext, Result[K]),(K, V)) => Result[K] = {\n+    val delta = 5e-5\n+    (U: (TaskContext, Result[K]), item: (K, V)) => {\n+      val result = U._2\n+      val tc = U._1\n+      val rng = result.getRand(tc.partitionId)\n+      val fraction = fractionByKey(item._1)\n+      val stratum = result.getEntry(item._1)\n+      if (withReplacement) {\n+        // compute q1 and q2 only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.q1.isEmpty || stratum.q2.isEmpty) {\n+          val n = counts.get(item._1)\n+          val s = math.ceil(n * fraction).toLong\n+          val lmbd1 = PB.getLowerBound(s)\n+          val minCount = PB.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) PB.getUpperBound(s) else PB.getUpperBound(s - minCount)\n+          val q1 = lmbd1 / n\n+          val q2 = lmbd2 / n\n+          stratum.q1 = Some(q1)\n+          stratum.q2 = Some(q2)\n+        }\n+        val x1 = if (stratum.q1.get == 0) 0L else rng.nextPoisson(stratum.q1.get)\n+        if (x1 > 0) {\n+          stratum.incrNumAccepted(x1)\n+        }\n+        val x2 = rng.nextPoisson(stratum.q2.get).toInt\n+        if (x2 > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(x2)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement.\n+        // Hence, q1 and q2 change on every iteration.\n+        val g1 = - math.log(delta) / stratum.numItems\n+        val g2 = (2.0 / 3.0) * g1\n+        val q1 = math.max(0, fraction + g2 - math.sqrt((g2 * g2 + 3 * g2 * fraction)))\n+        val q2 = math.min(1, fraction + g1 + math.sqrt(g1 * g1 + 2 * g1 * fraction))\n+\n+        val x = rng.nextUniform(0.0, 1.0)\n+        if (x < q1) {\n+          stratum.incrNumAccepted()\n+        } else if (x < q2) {\n+          stratum.addToWaitList(x)\n+        }\n+        stratum.q1 = Some(q1)\n+        stratum.q2 = Some(q2)\n+      }\n+      stratum.incrNumItems()\n+      result\n+    }\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to combine results from different partitions, as\n+   * returned by seqOp.\n+   */\n+  def getCombOp[K](): (Result[K], Result[K]) => Result[K] = {\n+    (r1: Result[K], r2: Result[K]) => {\n+      // take union of both key sets in case one partition doesn't contain all keys\n+      val keyUnion = r1.resultMap.keys.toSet.union(r2.resultMap.keys.toSet)\n+\n+      // Use r2 to keep the combined result since r1 is usual empty\n+      for (key <- keyUnion) {\n+        val entry1 = r1.resultMap.get(key)\n+        val entry2 = r2.resultMap.get(key)\n+        if (entry2.isEmpty && entry1.isDefined) {\n+          r2.resultMap += (key -> entry1.get)\n+        } else if (entry1.isDefined && entry2.isDefined) {\n+          entry2.get.addToWaitList(entry1.get.waitList)\n+          entry2.get.incrNumAccepted(entry1.get.numAccepted)\n+          entry2.get.incrNumItems(entry1.get.numItems)\n+        }\n+      }\n+      r2\n+    }\n+  }\n+\n+  /**\n+   * Given the result returned by the aggregate function, we need to determine the threshold used\n+   * to accept items to generate the exact sample size.\n+   */\n+  def computeThresholdByKey[K](finalResult: Map[K, Stratum], fractionByKey: (K => Double)):\n+    (K => Double) = {\n+    val thresholdByKey = new mutable.HashMap[K, Double]()\n+    for ((key, stratum) <- finalResult) {\n+      val fraction = fractionByKey(key)\n+      val s = math.ceil(stratum.numItems * fraction).toLong\n+      if (stratum.numAccepted > s) {\n+        logWarning(\"Pre-accepted too many\")\n+        thresholdByKey += (key -> stratum.q1.get)\n+      } else {\n+        val numWaitListAccepted = (s - stratum.numAccepted).toInt\n+        if (numWaitListAccepted >= stratum.waitList.size) {\n+          logWarning(\"WaitList too short\")\n+          thresholdByKey += (key -> stratum.q2.get)\n+        } else {\n+          thresholdByKey += (key -> stratum.waitList.sorted.apply(numWaitListAccepted))\n+        }\n+      }\n+    }\n+    thresholdByKey\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling without replacement.\n+   *\n+   * When exact sample size is required, we make an additional pass over the RDD to determine the\n+   * exact sampling rate that guarantees sample size with high confidence.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getBernoulliSamplingFunction[K, V](rdd: RDD[(K,  V)],\n+      fractionByKey: K => Double,\n+      exact: Boolean,\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    var samplingRateByKey = fractionByKey\n+    if (exact) {\n+      // determine threshold for each stratum and resample\n+      val seqOp = StratifiedSampler.getSeqOp[K,V](false, fractionByKey, None)\n+      val combOp = StratifiedSampler.getCombOp[K]()\n+      val zeroU = new Result[K](Map[K, Stratum](), seed = seed)\n+      val finalResult = aggregateWithContext(zeroU)(rdd, seqOp, combOp).resultMap\n+      samplingRateByKey = StratifiedSampler.computeThresholdByKey(finalResult, fractionByKey)\n+    }\n+    (idx: Int, iter: Iterator[(K, V)]) => {\n+      val random = new RandomDataGenerator\n+      random.reSeed(seed + idx)\n+      iter.filter(t => random.nextUniform(0.0, 1.0) < samplingRateByKey(t._1))\n+    }\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling with replacement.\n+   *\n+   * When exact sample size is required, we make two additional passed over the RDD to determine\n+   * the exact sampling rate that guarantees sample size with high confidence. The first pass\n+   * counts the number of items in each stratum (group of items with the same key) in the RDD, and\n+   * the second pass uses the counts to determine exact sampling rates.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getPoissonSamplingFunction[K, V](rdd:RDD[(K,  V)],\n+      fractionByKey: K => Double,\n+      exact: Boolean,\n+      counts: Option[Map[K, Long]],\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    // TODO implement the streaming version of sampling w/ replacement that doesn't require counts\n+    if (exact) {\n+      val seqOp = StratifiedSampler.getSeqOp[K,V](true, fractionByKey, counts)\n+      val combOp = StratifiedSampler.getCombOp[K]()\n+      val zeroU = new Result[K](Map[K, Stratum](), seed = seed)\n+      val finalResult = aggregateWithContext(zeroU)(rdd, seqOp, combOp).resultMap\n+      val thresholdByKey = StratifiedSampler.computeThresholdByKey(finalResult, fractionByKey)\n+      (idx: Int, iter: Iterator[(K, V)]) => {\n+        val random = new RandomDataGenerator()\n+        random.reSeed(seed + idx)\n+        iter.flatMap { t =>\n+          val q1 = finalResult.get(t._1).get.q1.getOrElse(0.0)\n+          val q2 = finalResult.get(t._1).get.q2.getOrElse(0.0)\n+          val x1 = if (q1 == 0) 0L else random.nextPoisson(q1)\n+          val x2 = random.nextPoisson(q2).toInt\n+          val x = x1 + (0 until x2).count(i => random.nextUniform(0.0, 1.0) < thresholdByKey(t._1))\n+          if (x > 0) {\n+            Iterator.fill(x.toInt)(t)\n+          } else {\n+            Iterator.empty\n+          }\n+        }\n+      }\n+    } else {\n+      (idx: Int, iter: Iterator[(K, V)]) => {\n+        val random = new RandomDataGenerator()\n+        random.reSeed(seed + idx)\n+        iter.flatMap { t =>\n+          val count = random.nextPoisson(fractionByKey(t._1)).toInt\n+          if (count > 0) {\n+            Iterator.fill(count)(t)\n+          } else {\n+            Iterator.empty\n+          }\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * Object used by seqOp to keep track of the number of items accepted and items waitlisted per\n+ * stratum, as well as the bounds for accepting and waitlisting items.\n+ */\n+private[random] class Stratum(var numItems: Long = 0L, var numAccepted: Long = 0L)\n+  extends Serializable {\n+\n+  var waitList: ArrayBuffer[Double] = new ArrayBuffer[Double]"
  }],
  "prId": 1025
}, {
  "comments": [{
    "author": {
      "login": "falaki"
    },
    "body": "We are implementing a non-trivial algorithm here. Lets use more descriptive names instead of g1, q2, q1, and q2. \n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-04T01:18:04Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.{Map, mutable}\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.{Logging, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.random.{PoissonBounds => PB}\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * A version of {@link #aggregate()} that passes the TaskContext to the function that does\n+   * aggregation for each partition. This function avoids creating an extra depth in the RDD\n+   * lineage, as opposed to using mapPartitionsWithId, which results in slightly improved run time.\n+   */\n+  def aggregateWithContext[U: ClassTag, T: ClassTag](zeroValue: U)\n+      (rdd: RDD[T],\n+       seqOp: ((TaskContext, U), T) => U,\n+       combOp: (U, U) => U): U = {\n+    val sc: SparkContext = rdd.sparkContext\n+    // Clone the zero value since we will also be serializing it as part of tasks\n+    var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance())\n+    // pad seqOp and combOp with taskContext to conform to aggregate's signature in TraversableOnce\n+    val paddedSeqOp = (arg1: (TaskContext, U), item: T) => (arg1._1, seqOp(arg1, item))\n+    val paddedcombOp = (arg1: (TaskContext, U), arg2: (TaskContext, U)) =>\n+      (arg1._1, combOp(arg1._2, arg1._2))\n+    val cleanSeqOp = sc.clean(paddedSeqOp)\n+    val cleanCombOp = sc.clean(paddedcombOp)\n+    val aggregatePartition = (tc: TaskContext, it: Iterator[T]) =>\n+      (it.aggregate(tc, zeroValue)(cleanSeqOp, cleanCombOp))._2\n+    val mergeResult = (index: Int, taskResult: U) => jobResult = combOp(jobResult, taskResult)\n+    sc.runJob(rdd, aggregatePartition, mergeResult)\n+    jobResult\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractionByKey: (K => Double),\n+      counts: Option[Map[K, Long]]): ((TaskContext, Result[K]),(K, V)) => Result[K] = {\n+    val delta = 5e-5\n+    (U: (TaskContext, Result[K]), item: (K, V)) => {\n+      val result = U._2\n+      val tc = U._1\n+      val rng = result.getRand(tc.partitionId)\n+      val fraction = fractionByKey(item._1)\n+      val stratum = result.getEntry(item._1)\n+      if (withReplacement) {\n+        // compute q1 and q2 only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.q1.isEmpty || stratum.q2.isEmpty) {\n+          val n = counts.get(item._1)\n+          val s = math.ceil(n * fraction).toLong\n+          val lmbd1 = PB.getLowerBound(s)\n+          val minCount = PB.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) PB.getUpperBound(s) else PB.getUpperBound(s - minCount)\n+          val q1 = lmbd1 / n\n+          val q2 = lmbd2 / n\n+          stratum.q1 = Some(q1)\n+          stratum.q2 = Some(q2)\n+        }\n+        val x1 = if (stratum.q1.get == 0) 0L else rng.nextPoisson(stratum.q1.get)\n+        if (x1 > 0) {\n+          stratum.incrNumAccepted(x1)\n+        }\n+        val x2 = rng.nextPoisson(stratum.q2.get).toInt\n+        if (x2 > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(x2)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement.\n+        // Hence, q1 and q2 change on every iteration."
  }, {
    "author": {
      "login": "dorx"
    },
    "body": "keeping g1 and g2 (since they're just constants) but changed q1 to q2 to more descriptive names. The expressions for q1  and q2 contains multiple references to g1 and g2. Longer names will make those two expressions really long.\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-06T22:03:44Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.{Map, mutable}\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.{Logging, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.random.{PoissonBounds => PB}\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * A version of {@link #aggregate()} that passes the TaskContext to the function that does\n+   * aggregation for each partition. This function avoids creating an extra depth in the RDD\n+   * lineage, as opposed to using mapPartitionsWithId, which results in slightly improved run time.\n+   */\n+  def aggregateWithContext[U: ClassTag, T: ClassTag](zeroValue: U)\n+      (rdd: RDD[T],\n+       seqOp: ((TaskContext, U), T) => U,\n+       combOp: (U, U) => U): U = {\n+    val sc: SparkContext = rdd.sparkContext\n+    // Clone the zero value since we will also be serializing it as part of tasks\n+    var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance())\n+    // pad seqOp and combOp with taskContext to conform to aggregate's signature in TraversableOnce\n+    val paddedSeqOp = (arg1: (TaskContext, U), item: T) => (arg1._1, seqOp(arg1, item))\n+    val paddedcombOp = (arg1: (TaskContext, U), arg2: (TaskContext, U)) =>\n+      (arg1._1, combOp(arg1._2, arg1._2))\n+    val cleanSeqOp = sc.clean(paddedSeqOp)\n+    val cleanCombOp = sc.clean(paddedcombOp)\n+    val aggregatePartition = (tc: TaskContext, it: Iterator[T]) =>\n+      (it.aggregate(tc, zeroValue)(cleanSeqOp, cleanCombOp))._2\n+    val mergeResult = (index: Int, taskResult: U) => jobResult = combOp(jobResult, taskResult)\n+    sc.runJob(rdd, aggregatePartition, mergeResult)\n+    jobResult\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractionByKey: (K => Double),\n+      counts: Option[Map[K, Long]]): ((TaskContext, Result[K]),(K, V)) => Result[K] = {\n+    val delta = 5e-5\n+    (U: (TaskContext, Result[K]), item: (K, V)) => {\n+      val result = U._2\n+      val tc = U._1\n+      val rng = result.getRand(tc.partitionId)\n+      val fraction = fractionByKey(item._1)\n+      val stratum = result.getEntry(item._1)\n+      if (withReplacement) {\n+        // compute q1 and q2 only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.q1.isEmpty || stratum.q2.isEmpty) {\n+          val n = counts.get(item._1)\n+          val s = math.ceil(n * fraction).toLong\n+          val lmbd1 = PB.getLowerBound(s)\n+          val minCount = PB.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) PB.getUpperBound(s) else PB.getUpperBound(s - minCount)\n+          val q1 = lmbd1 / n\n+          val q2 = lmbd2 / n\n+          stratum.q1 = Some(q1)\n+          stratum.q2 = Some(q2)\n+        }\n+        val x1 = if (stratum.q1.get == 0) 0L else rng.nextPoisson(stratum.q1.get)\n+        if (x1 > 0) {\n+          stratum.incrNumAccepted(x1)\n+        }\n+        val x2 = rng.nextPoisson(stratum.q2.get).toInt\n+        if (x2 > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(x2)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement.\n+        // Hence, q1 and q2 change on every iteration."
  }],
  "prId": 1025
}, {
  "comments": [{
    "author": {
      "login": "falaki"
    },
    "body": "Since we already have a helper class for Result, we can make these lines simpler by adding some helper methods to Result. For example this could become something like:\n\n``` scala\nr2.addMapping(key, entry1)\n```\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-04T01:24:21Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.{Map, mutable}\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.{Logging, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.random.{PoissonBounds => PB}\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * A version of {@link #aggregate()} that passes the TaskContext to the function that does\n+   * aggregation for each partition. This function avoids creating an extra depth in the RDD\n+   * lineage, as opposed to using mapPartitionsWithId, which results in slightly improved run time.\n+   */\n+  def aggregateWithContext[U: ClassTag, T: ClassTag](zeroValue: U)\n+      (rdd: RDD[T],\n+       seqOp: ((TaskContext, U), T) => U,\n+       combOp: (U, U) => U): U = {\n+    val sc: SparkContext = rdd.sparkContext\n+    // Clone the zero value since we will also be serializing it as part of tasks\n+    var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance())\n+    // pad seqOp and combOp with taskContext to conform to aggregate's signature in TraversableOnce\n+    val paddedSeqOp = (arg1: (TaskContext, U), item: T) => (arg1._1, seqOp(arg1, item))\n+    val paddedcombOp = (arg1: (TaskContext, U), arg2: (TaskContext, U)) =>\n+      (arg1._1, combOp(arg1._2, arg1._2))\n+    val cleanSeqOp = sc.clean(paddedSeqOp)\n+    val cleanCombOp = sc.clean(paddedcombOp)\n+    val aggregatePartition = (tc: TaskContext, it: Iterator[T]) =>\n+      (it.aggregate(tc, zeroValue)(cleanSeqOp, cleanCombOp))._2\n+    val mergeResult = (index: Int, taskResult: U) => jobResult = combOp(jobResult, taskResult)\n+    sc.runJob(rdd, aggregatePartition, mergeResult)\n+    jobResult\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractionByKey: (K => Double),\n+      counts: Option[Map[K, Long]]): ((TaskContext, Result[K]),(K, V)) => Result[K] = {\n+    val delta = 5e-5\n+    (U: (TaskContext, Result[K]), item: (K, V)) => {\n+      val result = U._2\n+      val tc = U._1\n+      val rng = result.getRand(tc.partitionId)\n+      val fraction = fractionByKey(item._1)\n+      val stratum = result.getEntry(item._1)\n+      if (withReplacement) {\n+        // compute q1 and q2 only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.q1.isEmpty || stratum.q2.isEmpty) {\n+          val n = counts.get(item._1)\n+          val s = math.ceil(n * fraction).toLong\n+          val lmbd1 = PB.getLowerBound(s)\n+          val minCount = PB.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) PB.getUpperBound(s) else PB.getUpperBound(s - minCount)\n+          val q1 = lmbd1 / n\n+          val q2 = lmbd2 / n\n+          stratum.q1 = Some(q1)\n+          stratum.q2 = Some(q2)\n+        }\n+        val x1 = if (stratum.q1.get == 0) 0L else rng.nextPoisson(stratum.q1.get)\n+        if (x1 > 0) {\n+          stratum.incrNumAccepted(x1)\n+        }\n+        val x2 = rng.nextPoisson(stratum.q2.get).toInt\n+        if (x2 > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(x2)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement.\n+        // Hence, q1 and q2 change on every iteration.\n+        val g1 = - math.log(delta) / stratum.numItems\n+        val g2 = (2.0 / 3.0) * g1\n+        val q1 = math.max(0, fraction + g2 - math.sqrt((g2 * g2 + 3 * g2 * fraction)))\n+        val q2 = math.min(1, fraction + g1 + math.sqrt(g1 * g1 + 2 * g1 * fraction))\n+\n+        val x = rng.nextUniform(0.0, 1.0)\n+        if (x < q1) {\n+          stratum.incrNumAccepted()\n+        } else if (x < q2) {\n+          stratum.addToWaitList(x)\n+        }\n+        stratum.q1 = Some(q1)\n+        stratum.q2 = Some(q2)\n+      }\n+      stratum.incrNumItems()\n+      result\n+    }\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to combine results from different partitions, as\n+   * returned by seqOp.\n+   */\n+  def getCombOp[K](): (Result[K], Result[K]) => Result[K] = {\n+    (r1: Result[K], r2: Result[K]) => {\n+      // take union of both key sets in case one partition doesn't contain all keys\n+      val keyUnion = r1.resultMap.keys.toSet.union(r2.resultMap.keys.toSet)\n+\n+      // Use r2 to keep the combined result since r1 is usual empty\n+      for (key <- keyUnion) {\n+        val entry1 = r1.resultMap.get(key)\n+        val entry2 = r2.resultMap.get(key)\n+        if (entry2.isEmpty && entry1.isDefined) {\n+          r2.resultMap += (key -> entry1.get)"
  }],
  "prId": 1025
}, {
  "comments": [{
    "author": {
      "login": "falaki"
    },
    "body": "get() is superfluous, this can just be `r1.resultMap(key)`\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-04T01:24:54Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.{Map, mutable}\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.{Logging, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.random.{PoissonBounds => PB}\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * A version of {@link #aggregate()} that passes the TaskContext to the function that does\n+   * aggregation for each partition. This function avoids creating an extra depth in the RDD\n+   * lineage, as opposed to using mapPartitionsWithId, which results in slightly improved run time.\n+   */\n+  def aggregateWithContext[U: ClassTag, T: ClassTag](zeroValue: U)\n+      (rdd: RDD[T],\n+       seqOp: ((TaskContext, U), T) => U,\n+       combOp: (U, U) => U): U = {\n+    val sc: SparkContext = rdd.sparkContext\n+    // Clone the zero value since we will also be serializing it as part of tasks\n+    var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance())\n+    // pad seqOp and combOp with taskContext to conform to aggregate's signature in TraversableOnce\n+    val paddedSeqOp = (arg1: (TaskContext, U), item: T) => (arg1._1, seqOp(arg1, item))\n+    val paddedcombOp = (arg1: (TaskContext, U), arg2: (TaskContext, U)) =>\n+      (arg1._1, combOp(arg1._2, arg1._2))\n+    val cleanSeqOp = sc.clean(paddedSeqOp)\n+    val cleanCombOp = sc.clean(paddedcombOp)\n+    val aggregatePartition = (tc: TaskContext, it: Iterator[T]) =>\n+      (it.aggregate(tc, zeroValue)(cleanSeqOp, cleanCombOp))._2\n+    val mergeResult = (index: Int, taskResult: U) => jobResult = combOp(jobResult, taskResult)\n+    sc.runJob(rdd, aggregatePartition, mergeResult)\n+    jobResult\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractionByKey: (K => Double),\n+      counts: Option[Map[K, Long]]): ((TaskContext, Result[K]),(K, V)) => Result[K] = {\n+    val delta = 5e-5\n+    (U: (TaskContext, Result[K]), item: (K, V)) => {\n+      val result = U._2\n+      val tc = U._1\n+      val rng = result.getRand(tc.partitionId)\n+      val fraction = fractionByKey(item._1)\n+      val stratum = result.getEntry(item._1)\n+      if (withReplacement) {\n+        // compute q1 and q2 only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.q1.isEmpty || stratum.q2.isEmpty) {\n+          val n = counts.get(item._1)\n+          val s = math.ceil(n * fraction).toLong\n+          val lmbd1 = PB.getLowerBound(s)\n+          val minCount = PB.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) PB.getUpperBound(s) else PB.getUpperBound(s - minCount)\n+          val q1 = lmbd1 / n\n+          val q2 = lmbd2 / n\n+          stratum.q1 = Some(q1)\n+          stratum.q2 = Some(q2)\n+        }\n+        val x1 = if (stratum.q1.get == 0) 0L else rng.nextPoisson(stratum.q1.get)\n+        if (x1 > 0) {\n+          stratum.incrNumAccepted(x1)\n+        }\n+        val x2 = rng.nextPoisson(stratum.q2.get).toInt\n+        if (x2 > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(x2)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement.\n+        // Hence, q1 and q2 change on every iteration.\n+        val g1 = - math.log(delta) / stratum.numItems\n+        val g2 = (2.0 / 3.0) * g1\n+        val q1 = math.max(0, fraction + g2 - math.sqrt((g2 * g2 + 3 * g2 * fraction)))\n+        val q2 = math.min(1, fraction + g1 + math.sqrt(g1 * g1 + 2 * g1 * fraction))\n+\n+        val x = rng.nextUniform(0.0, 1.0)\n+        if (x < q1) {\n+          stratum.incrNumAccepted()\n+        } else if (x < q2) {\n+          stratum.addToWaitList(x)\n+        }\n+        stratum.q1 = Some(q1)\n+        stratum.q2 = Some(q2)\n+      }\n+      stratum.incrNumItems()\n+      result\n+    }\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to combine results from different partitions, as\n+   * returned by seqOp.\n+   */\n+  def getCombOp[K](): (Result[K], Result[K]) => Result[K] = {\n+    (r1: Result[K], r2: Result[K]) => {\n+      // take union of both key sets in case one partition doesn't contain all keys\n+      val keyUnion = r1.resultMap.keys.toSet.union(r2.resultMap.keys.toSet)\n+\n+      // Use r2 to keep the combined result since r1 is usual empty\n+      for (key <- keyUnion) {\n+        val entry1 = r1.resultMap.get(key)"
  }, {
    "author": {
      "login": "dorx"
    },
    "body": "r1.resultMap(key) is equivalent to r1.resultMap.get(key).get, which removed the ability to check for null entries. So the gets here aren't superfluous. But I'll move some of the logic into helper functions in Result instead.\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-06T22:08:12Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.{Map, mutable}\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.{Logging, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.random.{PoissonBounds => PB}\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * A version of {@link #aggregate()} that passes the TaskContext to the function that does\n+   * aggregation for each partition. This function avoids creating an extra depth in the RDD\n+   * lineage, as opposed to using mapPartitionsWithId, which results in slightly improved run time.\n+   */\n+  def aggregateWithContext[U: ClassTag, T: ClassTag](zeroValue: U)\n+      (rdd: RDD[T],\n+       seqOp: ((TaskContext, U), T) => U,\n+       combOp: (U, U) => U): U = {\n+    val sc: SparkContext = rdd.sparkContext\n+    // Clone the zero value since we will also be serializing it as part of tasks\n+    var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance())\n+    // pad seqOp and combOp with taskContext to conform to aggregate's signature in TraversableOnce\n+    val paddedSeqOp = (arg1: (TaskContext, U), item: T) => (arg1._1, seqOp(arg1, item))\n+    val paddedcombOp = (arg1: (TaskContext, U), arg2: (TaskContext, U)) =>\n+      (arg1._1, combOp(arg1._2, arg1._2))\n+    val cleanSeqOp = sc.clean(paddedSeqOp)\n+    val cleanCombOp = sc.clean(paddedcombOp)\n+    val aggregatePartition = (tc: TaskContext, it: Iterator[T]) =>\n+      (it.aggregate(tc, zeroValue)(cleanSeqOp, cleanCombOp))._2\n+    val mergeResult = (index: Int, taskResult: U) => jobResult = combOp(jobResult, taskResult)\n+    sc.runJob(rdd, aggregatePartition, mergeResult)\n+    jobResult\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractionByKey: (K => Double),\n+      counts: Option[Map[K, Long]]): ((TaskContext, Result[K]),(K, V)) => Result[K] = {\n+    val delta = 5e-5\n+    (U: (TaskContext, Result[K]), item: (K, V)) => {\n+      val result = U._2\n+      val tc = U._1\n+      val rng = result.getRand(tc.partitionId)\n+      val fraction = fractionByKey(item._1)\n+      val stratum = result.getEntry(item._1)\n+      if (withReplacement) {\n+        // compute q1 and q2 only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.q1.isEmpty || stratum.q2.isEmpty) {\n+          val n = counts.get(item._1)\n+          val s = math.ceil(n * fraction).toLong\n+          val lmbd1 = PB.getLowerBound(s)\n+          val minCount = PB.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) PB.getUpperBound(s) else PB.getUpperBound(s - minCount)\n+          val q1 = lmbd1 / n\n+          val q2 = lmbd2 / n\n+          stratum.q1 = Some(q1)\n+          stratum.q2 = Some(q2)\n+        }\n+        val x1 = if (stratum.q1.get == 0) 0L else rng.nextPoisson(stratum.q1.get)\n+        if (x1 > 0) {\n+          stratum.incrNumAccepted(x1)\n+        }\n+        val x2 = rng.nextPoisson(stratum.q2.get).toInt\n+        if (x2 > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(x2)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement.\n+        // Hence, q1 and q2 change on every iteration.\n+        val g1 = - math.log(delta) / stratum.numItems\n+        val g2 = (2.0 / 3.0) * g1\n+        val q1 = math.max(0, fraction + g2 - math.sqrt((g2 * g2 + 3 * g2 * fraction)))\n+        val q2 = math.min(1, fraction + g1 + math.sqrt(g1 * g1 + 2 * g1 * fraction))\n+\n+        val x = rng.nextUniform(0.0, 1.0)\n+        if (x < q1) {\n+          stratum.incrNumAccepted()\n+        } else if (x < q2) {\n+          stratum.addToWaitList(x)\n+        }\n+        stratum.q1 = Some(q1)\n+        stratum.q2 = Some(q2)\n+      }\n+      stratum.incrNumItems()\n+      result\n+    }\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to combine results from different partitions, as\n+   * returned by seqOp.\n+   */\n+  def getCombOp[K](): (Result[K], Result[K]) => Result[K] = {\n+    (r1: Result[K], r2: Result[K]) => {\n+      // take union of both key sets in case one partition doesn't contain all keys\n+      val keyUnion = r1.resultMap.keys.toSet.union(r2.resultMap.keys.toSet)\n+\n+      // Use r2 to keep the combined result since r1 is usual empty\n+      for (key <- keyUnion) {\n+        val entry1 = r1.resultMap.get(key)"
  }],
  "prId": 1025
}, {
  "comments": [{
    "author": {
      "login": "falaki"
    },
    "body": "Some comments that explain the logic here would be great.\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-04T01:29:13Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.{Map, mutable}\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.{Logging, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.random.{PoissonBounds => PB}\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * A version of {@link #aggregate()} that passes the TaskContext to the function that does\n+   * aggregation for each partition. This function avoids creating an extra depth in the RDD\n+   * lineage, as opposed to using mapPartitionsWithId, which results in slightly improved run time.\n+   */\n+  def aggregateWithContext[U: ClassTag, T: ClassTag](zeroValue: U)\n+      (rdd: RDD[T],\n+       seqOp: ((TaskContext, U), T) => U,\n+       combOp: (U, U) => U): U = {\n+    val sc: SparkContext = rdd.sparkContext\n+    // Clone the zero value since we will also be serializing it as part of tasks\n+    var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance())\n+    // pad seqOp and combOp with taskContext to conform to aggregate's signature in TraversableOnce\n+    val paddedSeqOp = (arg1: (TaskContext, U), item: T) => (arg1._1, seqOp(arg1, item))\n+    val paddedcombOp = (arg1: (TaskContext, U), arg2: (TaskContext, U)) =>\n+      (arg1._1, combOp(arg1._2, arg1._2))\n+    val cleanSeqOp = sc.clean(paddedSeqOp)\n+    val cleanCombOp = sc.clean(paddedcombOp)\n+    val aggregatePartition = (tc: TaskContext, it: Iterator[T]) =>\n+      (it.aggregate(tc, zeroValue)(cleanSeqOp, cleanCombOp))._2\n+    val mergeResult = (index: Int, taskResult: U) => jobResult = combOp(jobResult, taskResult)\n+    sc.runJob(rdd, aggregatePartition, mergeResult)\n+    jobResult\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractionByKey: (K => Double),\n+      counts: Option[Map[K, Long]]): ((TaskContext, Result[K]),(K, V)) => Result[K] = {\n+    val delta = 5e-5\n+    (U: (TaskContext, Result[K]), item: (K, V)) => {\n+      val result = U._2\n+      val tc = U._1\n+      val rng = result.getRand(tc.partitionId)\n+      val fraction = fractionByKey(item._1)\n+      val stratum = result.getEntry(item._1)\n+      if (withReplacement) {\n+        // compute q1 and q2 only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.q1.isEmpty || stratum.q2.isEmpty) {\n+          val n = counts.get(item._1)\n+          val s = math.ceil(n * fraction).toLong\n+          val lmbd1 = PB.getLowerBound(s)\n+          val minCount = PB.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) PB.getUpperBound(s) else PB.getUpperBound(s - minCount)\n+          val q1 = lmbd1 / n\n+          val q2 = lmbd2 / n\n+          stratum.q1 = Some(q1)\n+          stratum.q2 = Some(q2)\n+        }\n+        val x1 = if (stratum.q1.get == 0) 0L else rng.nextPoisson(stratum.q1.get)\n+        if (x1 > 0) {\n+          stratum.incrNumAccepted(x1)\n+        }\n+        val x2 = rng.nextPoisson(stratum.q2.get).toInt\n+        if (x2 > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(x2)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement.\n+        // Hence, q1 and q2 change on every iteration.\n+        val g1 = - math.log(delta) / stratum.numItems\n+        val g2 = (2.0 / 3.0) * g1\n+        val q1 = math.max(0, fraction + g2 - math.sqrt((g2 * g2 + 3 * g2 * fraction)))\n+        val q2 = math.min(1, fraction + g1 + math.sqrt(g1 * g1 + 2 * g1 * fraction))\n+\n+        val x = rng.nextUniform(0.0, 1.0)\n+        if (x < q1) {\n+          stratum.incrNumAccepted()\n+        } else if (x < q2) {\n+          stratum.addToWaitList(x)\n+        }\n+        stratum.q1 = Some(q1)\n+        stratum.q2 = Some(q2)\n+      }\n+      stratum.incrNumItems()\n+      result\n+    }\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to combine results from different partitions, as\n+   * returned by seqOp.\n+   */\n+  def getCombOp[K](): (Result[K], Result[K]) => Result[K] = {\n+    (r1: Result[K], r2: Result[K]) => {\n+      // take union of both key sets in case one partition doesn't contain all keys\n+      val keyUnion = r1.resultMap.keys.toSet.union(r2.resultMap.keys.toSet)\n+\n+      // Use r2 to keep the combined result since r1 is usual empty\n+      for (key <- keyUnion) {\n+        val entry1 = r1.resultMap.get(key)\n+        val entry2 = r2.resultMap.get(key)\n+        if (entry2.isEmpty && entry1.isDefined) {\n+          r2.resultMap += (key -> entry1.get)\n+        } else if (entry1.isDefined && entry2.isDefined) {\n+          entry2.get.addToWaitList(entry1.get.waitList)\n+          entry2.get.incrNumAccepted(entry1.get.numAccepted)\n+          entry2.get.incrNumItems(entry1.get.numItems)\n+        }\n+      }\n+      r2\n+    }\n+  }\n+\n+  /**\n+   * Given the result returned by the aggregate function, we need to determine the threshold used\n+   * to accept items to generate the exact sample size.\n+   */\n+  def computeThresholdByKey[K](finalResult: Map[K, Stratum], fractionByKey: (K => Double)):\n+    (K => Double) = {\n+    val thresholdByKey = new mutable.HashMap[K, Double]()\n+    for ((key, stratum) <- finalResult) {"
  }],
  "prId": 1025
}, {
  "comments": [{
    "author": {
      "login": "falaki"
    },
    "body": "In these lines we can remove `StratifiedSampler`\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-04T01:31:27Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.{Map, mutable}\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.{Logging, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.random.{PoissonBounds => PB}\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * A version of {@link #aggregate()} that passes the TaskContext to the function that does\n+   * aggregation for each partition. This function avoids creating an extra depth in the RDD\n+   * lineage, as opposed to using mapPartitionsWithId, which results in slightly improved run time.\n+   */\n+  def aggregateWithContext[U: ClassTag, T: ClassTag](zeroValue: U)\n+      (rdd: RDD[T],\n+       seqOp: ((TaskContext, U), T) => U,\n+       combOp: (U, U) => U): U = {\n+    val sc: SparkContext = rdd.sparkContext\n+    // Clone the zero value since we will also be serializing it as part of tasks\n+    var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance())\n+    // pad seqOp and combOp with taskContext to conform to aggregate's signature in TraversableOnce\n+    val paddedSeqOp = (arg1: (TaskContext, U), item: T) => (arg1._1, seqOp(arg1, item))\n+    val paddedcombOp = (arg1: (TaskContext, U), arg2: (TaskContext, U)) =>\n+      (arg1._1, combOp(arg1._2, arg1._2))\n+    val cleanSeqOp = sc.clean(paddedSeqOp)\n+    val cleanCombOp = sc.clean(paddedcombOp)\n+    val aggregatePartition = (tc: TaskContext, it: Iterator[T]) =>\n+      (it.aggregate(tc, zeroValue)(cleanSeqOp, cleanCombOp))._2\n+    val mergeResult = (index: Int, taskResult: U) => jobResult = combOp(jobResult, taskResult)\n+    sc.runJob(rdd, aggregatePartition, mergeResult)\n+    jobResult\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractionByKey: (K => Double),\n+      counts: Option[Map[K, Long]]): ((TaskContext, Result[K]),(K, V)) => Result[K] = {\n+    val delta = 5e-5\n+    (U: (TaskContext, Result[K]), item: (K, V)) => {\n+      val result = U._2\n+      val tc = U._1\n+      val rng = result.getRand(tc.partitionId)\n+      val fraction = fractionByKey(item._1)\n+      val stratum = result.getEntry(item._1)\n+      if (withReplacement) {\n+        // compute q1 and q2 only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.q1.isEmpty || stratum.q2.isEmpty) {\n+          val n = counts.get(item._1)\n+          val s = math.ceil(n * fraction).toLong\n+          val lmbd1 = PB.getLowerBound(s)\n+          val minCount = PB.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) PB.getUpperBound(s) else PB.getUpperBound(s - minCount)\n+          val q1 = lmbd1 / n\n+          val q2 = lmbd2 / n\n+          stratum.q1 = Some(q1)\n+          stratum.q2 = Some(q2)\n+        }\n+        val x1 = if (stratum.q1.get == 0) 0L else rng.nextPoisson(stratum.q1.get)\n+        if (x1 > 0) {\n+          stratum.incrNumAccepted(x1)\n+        }\n+        val x2 = rng.nextPoisson(stratum.q2.get).toInt\n+        if (x2 > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(x2)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement.\n+        // Hence, q1 and q2 change on every iteration.\n+        val g1 = - math.log(delta) / stratum.numItems\n+        val g2 = (2.0 / 3.0) * g1\n+        val q1 = math.max(0, fraction + g2 - math.sqrt((g2 * g2 + 3 * g2 * fraction)))\n+        val q2 = math.min(1, fraction + g1 + math.sqrt(g1 * g1 + 2 * g1 * fraction))\n+\n+        val x = rng.nextUniform(0.0, 1.0)\n+        if (x < q1) {\n+          stratum.incrNumAccepted()\n+        } else if (x < q2) {\n+          stratum.addToWaitList(x)\n+        }\n+        stratum.q1 = Some(q1)\n+        stratum.q2 = Some(q2)\n+      }\n+      stratum.incrNumItems()\n+      result\n+    }\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to combine results from different partitions, as\n+   * returned by seqOp.\n+   */\n+  def getCombOp[K](): (Result[K], Result[K]) => Result[K] = {\n+    (r1: Result[K], r2: Result[K]) => {\n+      // take union of both key sets in case one partition doesn't contain all keys\n+      val keyUnion = r1.resultMap.keys.toSet.union(r2.resultMap.keys.toSet)\n+\n+      // Use r2 to keep the combined result since r1 is usual empty\n+      for (key <- keyUnion) {\n+        val entry1 = r1.resultMap.get(key)\n+        val entry2 = r2.resultMap.get(key)\n+        if (entry2.isEmpty && entry1.isDefined) {\n+          r2.resultMap += (key -> entry1.get)\n+        } else if (entry1.isDefined && entry2.isDefined) {\n+          entry2.get.addToWaitList(entry1.get.waitList)\n+          entry2.get.incrNumAccepted(entry1.get.numAccepted)\n+          entry2.get.incrNumItems(entry1.get.numItems)\n+        }\n+      }\n+      r2\n+    }\n+  }\n+\n+  /**\n+   * Given the result returned by the aggregate function, we need to determine the threshold used\n+   * to accept items to generate the exact sample size.\n+   */\n+  def computeThresholdByKey[K](finalResult: Map[K, Stratum], fractionByKey: (K => Double)):\n+    (K => Double) = {\n+    val thresholdByKey = new mutable.HashMap[K, Double]()\n+    for ((key, stratum) <- finalResult) {\n+      val fraction = fractionByKey(key)\n+      val s = math.ceil(stratum.numItems * fraction).toLong\n+      if (stratum.numAccepted > s) {\n+        logWarning(\"Pre-accepted too many\")\n+        thresholdByKey += (key -> stratum.q1.get)\n+      } else {\n+        val numWaitListAccepted = (s - stratum.numAccepted).toInt\n+        if (numWaitListAccepted >= stratum.waitList.size) {\n+          logWarning(\"WaitList too short\")\n+          thresholdByKey += (key -> stratum.q2.get)\n+        } else {\n+          thresholdByKey += (key -> stratum.waitList.sorted.apply(numWaitListAccepted))\n+        }\n+      }\n+    }\n+    thresholdByKey\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling without replacement.\n+   *\n+   * When exact sample size is required, we make an additional pass over the RDD to determine the\n+   * exact sampling rate that guarantees sample size with high confidence.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getBernoulliSamplingFunction[K, V](rdd: RDD[(K,  V)],\n+      fractionByKey: K => Double,\n+      exact: Boolean,\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    var samplingRateByKey = fractionByKey\n+    if (exact) {\n+      // determine threshold for each stratum and resample\n+      val seqOp = StratifiedSampler.getSeqOp[K,V](false, fractionByKey, None)"
  }],
  "prId": 1025
}, {
  "comments": [{
    "author": {
      "login": "falaki"
    },
    "body": "Similarly remove `StratifiedSampler` object name from these lines\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-04T01:32:26Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.{Map, mutable}\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.{Logging, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.random.{PoissonBounds => PB}\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * A version of {@link #aggregate()} that passes the TaskContext to the function that does\n+   * aggregation for each partition. This function avoids creating an extra depth in the RDD\n+   * lineage, as opposed to using mapPartitionsWithId, which results in slightly improved run time.\n+   */\n+  def aggregateWithContext[U: ClassTag, T: ClassTag](zeroValue: U)\n+      (rdd: RDD[T],\n+       seqOp: ((TaskContext, U), T) => U,\n+       combOp: (U, U) => U): U = {\n+    val sc: SparkContext = rdd.sparkContext\n+    // Clone the zero value since we will also be serializing it as part of tasks\n+    var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance())\n+    // pad seqOp and combOp with taskContext to conform to aggregate's signature in TraversableOnce\n+    val paddedSeqOp = (arg1: (TaskContext, U), item: T) => (arg1._1, seqOp(arg1, item))\n+    val paddedcombOp = (arg1: (TaskContext, U), arg2: (TaskContext, U)) =>\n+      (arg1._1, combOp(arg1._2, arg1._2))\n+    val cleanSeqOp = sc.clean(paddedSeqOp)\n+    val cleanCombOp = sc.clean(paddedcombOp)\n+    val aggregatePartition = (tc: TaskContext, it: Iterator[T]) =>\n+      (it.aggregate(tc, zeroValue)(cleanSeqOp, cleanCombOp))._2\n+    val mergeResult = (index: Int, taskResult: U) => jobResult = combOp(jobResult, taskResult)\n+    sc.runJob(rdd, aggregatePartition, mergeResult)\n+    jobResult\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractionByKey: (K => Double),\n+      counts: Option[Map[K, Long]]): ((TaskContext, Result[K]),(K, V)) => Result[K] = {\n+    val delta = 5e-5\n+    (U: (TaskContext, Result[K]), item: (K, V)) => {\n+      val result = U._2\n+      val tc = U._1\n+      val rng = result.getRand(tc.partitionId)\n+      val fraction = fractionByKey(item._1)\n+      val stratum = result.getEntry(item._1)\n+      if (withReplacement) {\n+        // compute q1 and q2 only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.q1.isEmpty || stratum.q2.isEmpty) {\n+          val n = counts.get(item._1)\n+          val s = math.ceil(n * fraction).toLong\n+          val lmbd1 = PB.getLowerBound(s)\n+          val minCount = PB.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) PB.getUpperBound(s) else PB.getUpperBound(s - minCount)\n+          val q1 = lmbd1 / n\n+          val q2 = lmbd2 / n\n+          stratum.q1 = Some(q1)\n+          stratum.q2 = Some(q2)\n+        }\n+        val x1 = if (stratum.q1.get == 0) 0L else rng.nextPoisson(stratum.q1.get)\n+        if (x1 > 0) {\n+          stratum.incrNumAccepted(x1)\n+        }\n+        val x2 = rng.nextPoisson(stratum.q2.get).toInt\n+        if (x2 > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(x2)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement.\n+        // Hence, q1 and q2 change on every iteration.\n+        val g1 = - math.log(delta) / stratum.numItems\n+        val g2 = (2.0 / 3.0) * g1\n+        val q1 = math.max(0, fraction + g2 - math.sqrt((g2 * g2 + 3 * g2 * fraction)))\n+        val q2 = math.min(1, fraction + g1 + math.sqrt(g1 * g1 + 2 * g1 * fraction))\n+\n+        val x = rng.nextUniform(0.0, 1.0)\n+        if (x < q1) {\n+          stratum.incrNumAccepted()\n+        } else if (x < q2) {\n+          stratum.addToWaitList(x)\n+        }\n+        stratum.q1 = Some(q1)\n+        stratum.q2 = Some(q2)\n+      }\n+      stratum.incrNumItems()\n+      result\n+    }\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to combine results from different partitions, as\n+   * returned by seqOp.\n+   */\n+  def getCombOp[K](): (Result[K], Result[K]) => Result[K] = {\n+    (r1: Result[K], r2: Result[K]) => {\n+      // take union of both key sets in case one partition doesn't contain all keys\n+      val keyUnion = r1.resultMap.keys.toSet.union(r2.resultMap.keys.toSet)\n+\n+      // Use r2 to keep the combined result since r1 is usual empty\n+      for (key <- keyUnion) {\n+        val entry1 = r1.resultMap.get(key)\n+        val entry2 = r2.resultMap.get(key)\n+        if (entry2.isEmpty && entry1.isDefined) {\n+          r2.resultMap += (key -> entry1.get)\n+        } else if (entry1.isDefined && entry2.isDefined) {\n+          entry2.get.addToWaitList(entry1.get.waitList)\n+          entry2.get.incrNumAccepted(entry1.get.numAccepted)\n+          entry2.get.incrNumItems(entry1.get.numItems)\n+        }\n+      }\n+      r2\n+    }\n+  }\n+\n+  /**\n+   * Given the result returned by the aggregate function, we need to determine the threshold used\n+   * to accept items to generate the exact sample size.\n+   */\n+  def computeThresholdByKey[K](finalResult: Map[K, Stratum], fractionByKey: (K => Double)):\n+    (K => Double) = {\n+    val thresholdByKey = new mutable.HashMap[K, Double]()\n+    for ((key, stratum) <- finalResult) {\n+      val fraction = fractionByKey(key)\n+      val s = math.ceil(stratum.numItems * fraction).toLong\n+      if (stratum.numAccepted > s) {\n+        logWarning(\"Pre-accepted too many\")\n+        thresholdByKey += (key -> stratum.q1.get)\n+      } else {\n+        val numWaitListAccepted = (s - stratum.numAccepted).toInt\n+        if (numWaitListAccepted >= stratum.waitList.size) {\n+          logWarning(\"WaitList too short\")\n+          thresholdByKey += (key -> stratum.q2.get)\n+        } else {\n+          thresholdByKey += (key -> stratum.waitList.sorted.apply(numWaitListAccepted))\n+        }\n+      }\n+    }\n+    thresholdByKey\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling without replacement.\n+   *\n+   * When exact sample size is required, we make an additional pass over the RDD to determine the\n+   * exact sampling rate that guarantees sample size with high confidence.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getBernoulliSamplingFunction[K, V](rdd: RDD[(K,  V)],\n+      fractionByKey: K => Double,\n+      exact: Boolean,\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    var samplingRateByKey = fractionByKey\n+    if (exact) {\n+      // determine threshold for each stratum and resample\n+      val seqOp = StratifiedSampler.getSeqOp[K,V](false, fractionByKey, None)\n+      val combOp = StratifiedSampler.getCombOp[K]()\n+      val zeroU = new Result[K](Map[K, Stratum](), seed = seed)\n+      val finalResult = aggregateWithContext(zeroU)(rdd, seqOp, combOp).resultMap\n+      samplingRateByKey = StratifiedSampler.computeThresholdByKey(finalResult, fractionByKey)\n+    }\n+    (idx: Int, iter: Iterator[(K, V)]) => {\n+      val random = new RandomDataGenerator\n+      random.reSeed(seed + idx)\n+      iter.filter(t => random.nextUniform(0.0, 1.0) < samplingRateByKey(t._1))\n+    }\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling with replacement.\n+   *\n+   * When exact sample size is required, we make two additional passed over the RDD to determine\n+   * the exact sampling rate that guarantees sample size with high confidence. The first pass\n+   * counts the number of items in each stratum (group of items with the same key) in the RDD, and\n+   * the second pass uses the counts to determine exact sampling rates.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getPoissonSamplingFunction[K, V](rdd:RDD[(K,  V)],\n+      fractionByKey: K => Double,\n+      exact: Boolean,\n+      counts: Option[Map[K, Long]],\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    // TODO implement the streaming version of sampling w/ replacement that doesn't require counts\n+    if (exact) {\n+      val seqOp = StratifiedSampler.getSeqOp[K,V](true, fractionByKey, counts)"
  }],
  "prId": 1025
}, {
  "comments": [{
    "author": {
      "login": "falaki"
    },
    "body": "Similarly, better names instead of q1, q2, g1, and g2.\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-04T01:33:09Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.{Map, mutable}\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.{Logging, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.random.{PoissonBounds => PB}\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * A version of {@link #aggregate()} that passes the TaskContext to the function that does\n+   * aggregation for each partition. This function avoids creating an extra depth in the RDD\n+   * lineage, as opposed to using mapPartitionsWithId, which results in slightly improved run time.\n+   */\n+  def aggregateWithContext[U: ClassTag, T: ClassTag](zeroValue: U)\n+      (rdd: RDD[T],\n+       seqOp: ((TaskContext, U), T) => U,\n+       combOp: (U, U) => U): U = {\n+    val sc: SparkContext = rdd.sparkContext\n+    // Clone the zero value since we will also be serializing it as part of tasks\n+    var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance())\n+    // pad seqOp and combOp with taskContext to conform to aggregate's signature in TraversableOnce\n+    val paddedSeqOp = (arg1: (TaskContext, U), item: T) => (arg1._1, seqOp(arg1, item))\n+    val paddedcombOp = (arg1: (TaskContext, U), arg2: (TaskContext, U)) =>\n+      (arg1._1, combOp(arg1._2, arg1._2))\n+    val cleanSeqOp = sc.clean(paddedSeqOp)\n+    val cleanCombOp = sc.clean(paddedcombOp)\n+    val aggregatePartition = (tc: TaskContext, it: Iterator[T]) =>\n+      (it.aggregate(tc, zeroValue)(cleanSeqOp, cleanCombOp))._2\n+    val mergeResult = (index: Int, taskResult: U) => jobResult = combOp(jobResult, taskResult)\n+    sc.runJob(rdd, aggregatePartition, mergeResult)\n+    jobResult\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractionByKey: (K => Double),\n+      counts: Option[Map[K, Long]]): ((TaskContext, Result[K]),(K, V)) => Result[K] = {\n+    val delta = 5e-5\n+    (U: (TaskContext, Result[K]), item: (K, V)) => {\n+      val result = U._2\n+      val tc = U._1\n+      val rng = result.getRand(tc.partitionId)\n+      val fraction = fractionByKey(item._1)\n+      val stratum = result.getEntry(item._1)\n+      if (withReplacement) {\n+        // compute q1 and q2 only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.q1.isEmpty || stratum.q2.isEmpty) {\n+          val n = counts.get(item._1)\n+          val s = math.ceil(n * fraction).toLong\n+          val lmbd1 = PB.getLowerBound(s)\n+          val minCount = PB.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) PB.getUpperBound(s) else PB.getUpperBound(s - minCount)\n+          val q1 = lmbd1 / n\n+          val q2 = lmbd2 / n\n+          stratum.q1 = Some(q1)\n+          stratum.q2 = Some(q2)\n+        }\n+        val x1 = if (stratum.q1.get == 0) 0L else rng.nextPoisson(stratum.q1.get)\n+        if (x1 > 0) {\n+          stratum.incrNumAccepted(x1)\n+        }\n+        val x2 = rng.nextPoisson(stratum.q2.get).toInt\n+        if (x2 > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(x2)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement.\n+        // Hence, q1 and q2 change on every iteration.\n+        val g1 = - math.log(delta) / stratum.numItems\n+        val g2 = (2.0 / 3.0) * g1\n+        val q1 = math.max(0, fraction + g2 - math.sqrt((g2 * g2 + 3 * g2 * fraction)))\n+        val q2 = math.min(1, fraction + g1 + math.sqrt(g1 * g1 + 2 * g1 * fraction))\n+\n+        val x = rng.nextUniform(0.0, 1.0)\n+        if (x < q1) {\n+          stratum.incrNumAccepted()\n+        } else if (x < q2) {\n+          stratum.addToWaitList(x)\n+        }\n+        stratum.q1 = Some(q1)\n+        stratum.q2 = Some(q2)\n+      }\n+      stratum.incrNumItems()\n+      result\n+    }\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to combine results from different partitions, as\n+   * returned by seqOp.\n+   */\n+  def getCombOp[K](): (Result[K], Result[K]) => Result[K] = {\n+    (r1: Result[K], r2: Result[K]) => {\n+      // take union of both key sets in case one partition doesn't contain all keys\n+      val keyUnion = r1.resultMap.keys.toSet.union(r2.resultMap.keys.toSet)\n+\n+      // Use r2 to keep the combined result since r1 is usual empty\n+      for (key <- keyUnion) {\n+        val entry1 = r1.resultMap.get(key)\n+        val entry2 = r2.resultMap.get(key)\n+        if (entry2.isEmpty && entry1.isDefined) {\n+          r2.resultMap += (key -> entry1.get)\n+        } else if (entry1.isDefined && entry2.isDefined) {\n+          entry2.get.addToWaitList(entry1.get.waitList)\n+          entry2.get.incrNumAccepted(entry1.get.numAccepted)\n+          entry2.get.incrNumItems(entry1.get.numItems)\n+        }\n+      }\n+      r2\n+    }\n+  }\n+\n+  /**\n+   * Given the result returned by the aggregate function, we need to determine the threshold used\n+   * to accept items to generate the exact sample size.\n+   */\n+  def computeThresholdByKey[K](finalResult: Map[K, Stratum], fractionByKey: (K => Double)):\n+    (K => Double) = {\n+    val thresholdByKey = new mutable.HashMap[K, Double]()\n+    for ((key, stratum) <- finalResult) {\n+      val fraction = fractionByKey(key)\n+      val s = math.ceil(stratum.numItems * fraction).toLong\n+      if (stratum.numAccepted > s) {\n+        logWarning(\"Pre-accepted too many\")\n+        thresholdByKey += (key -> stratum.q1.get)\n+      } else {\n+        val numWaitListAccepted = (s - stratum.numAccepted).toInt\n+        if (numWaitListAccepted >= stratum.waitList.size) {\n+          logWarning(\"WaitList too short\")\n+          thresholdByKey += (key -> stratum.q2.get)\n+        } else {\n+          thresholdByKey += (key -> stratum.waitList.sorted.apply(numWaitListAccepted))\n+        }\n+      }\n+    }\n+    thresholdByKey\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling without replacement.\n+   *\n+   * When exact sample size is required, we make an additional pass over the RDD to determine the\n+   * exact sampling rate that guarantees sample size with high confidence.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getBernoulliSamplingFunction[K, V](rdd: RDD[(K,  V)],\n+      fractionByKey: K => Double,\n+      exact: Boolean,\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    var samplingRateByKey = fractionByKey\n+    if (exact) {\n+      // determine threshold for each stratum and resample\n+      val seqOp = StratifiedSampler.getSeqOp[K,V](false, fractionByKey, None)\n+      val combOp = StratifiedSampler.getCombOp[K]()\n+      val zeroU = new Result[K](Map[K, Stratum](), seed = seed)\n+      val finalResult = aggregateWithContext(zeroU)(rdd, seqOp, combOp).resultMap\n+      samplingRateByKey = StratifiedSampler.computeThresholdByKey(finalResult, fractionByKey)\n+    }\n+    (idx: Int, iter: Iterator[(K, V)]) => {\n+      val random = new RandomDataGenerator\n+      random.reSeed(seed + idx)\n+      iter.filter(t => random.nextUniform(0.0, 1.0) < samplingRateByKey(t._1))\n+    }\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling with replacement.\n+   *\n+   * When exact sample size is required, we make two additional passed over the RDD to determine\n+   * the exact sampling rate that guarantees sample size with high confidence. The first pass\n+   * counts the number of items in each stratum (group of items with the same key) in the RDD, and\n+   * the second pass uses the counts to determine exact sampling rates.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getPoissonSamplingFunction[K, V](rdd:RDD[(K,  V)],\n+      fractionByKey: K => Double,\n+      exact: Boolean,\n+      counts: Option[Map[K, Long]],\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    // TODO implement the streaming version of sampling w/ replacement that doesn't require counts\n+    if (exact) {\n+      val seqOp = StratifiedSampler.getSeqOp[K,V](true, fractionByKey, counts)\n+      val combOp = StratifiedSampler.getCombOp[K]()\n+      val zeroU = new Result[K](Map[K, Stratum](), seed = seed)\n+      val finalResult = aggregateWithContext(zeroU)(rdd, seqOp, combOp).resultMap\n+      val thresholdByKey = StratifiedSampler.computeThresholdByKey(finalResult, fractionByKey)\n+      (idx: Int, iter: Iterator[(K, V)]) => {\n+        val random = new RandomDataGenerator()\n+        random.reSeed(seed + idx)\n+        iter.flatMap { t =>\n+          val q1 = finalResult.get(t._1).get.q1.getOrElse(0.0)"
  }],
  "prId": 1025
}, {
  "comments": [{
    "author": {
      "login": "falaki"
    },
    "body": "No need to call get on a Map.\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-04T01:33:30Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.{Map, mutable}\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.{Logging, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.random.{PoissonBounds => PB}\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * A version of {@link #aggregate()} that passes the TaskContext to the function that does\n+   * aggregation for each partition. This function avoids creating an extra depth in the RDD\n+   * lineage, as opposed to using mapPartitionsWithId, which results in slightly improved run time.\n+   */\n+  def aggregateWithContext[U: ClassTag, T: ClassTag](zeroValue: U)\n+      (rdd: RDD[T],\n+       seqOp: ((TaskContext, U), T) => U,\n+       combOp: (U, U) => U): U = {\n+    val sc: SparkContext = rdd.sparkContext\n+    // Clone the zero value since we will also be serializing it as part of tasks\n+    var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance())\n+    // pad seqOp and combOp with taskContext to conform to aggregate's signature in TraversableOnce\n+    val paddedSeqOp = (arg1: (TaskContext, U), item: T) => (arg1._1, seqOp(arg1, item))\n+    val paddedcombOp = (arg1: (TaskContext, U), arg2: (TaskContext, U)) =>\n+      (arg1._1, combOp(arg1._2, arg1._2))\n+    val cleanSeqOp = sc.clean(paddedSeqOp)\n+    val cleanCombOp = sc.clean(paddedcombOp)\n+    val aggregatePartition = (tc: TaskContext, it: Iterator[T]) =>\n+      (it.aggregate(tc, zeroValue)(cleanSeqOp, cleanCombOp))._2\n+    val mergeResult = (index: Int, taskResult: U) => jobResult = combOp(jobResult, taskResult)\n+    sc.runJob(rdd, aggregatePartition, mergeResult)\n+    jobResult\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractionByKey: (K => Double),\n+      counts: Option[Map[K, Long]]): ((TaskContext, Result[K]),(K, V)) => Result[K] = {\n+    val delta = 5e-5\n+    (U: (TaskContext, Result[K]), item: (K, V)) => {\n+      val result = U._2\n+      val tc = U._1\n+      val rng = result.getRand(tc.partitionId)\n+      val fraction = fractionByKey(item._1)\n+      val stratum = result.getEntry(item._1)\n+      if (withReplacement) {\n+        // compute q1 and q2 only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.q1.isEmpty || stratum.q2.isEmpty) {\n+          val n = counts.get(item._1)\n+          val s = math.ceil(n * fraction).toLong\n+          val lmbd1 = PB.getLowerBound(s)\n+          val minCount = PB.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) PB.getUpperBound(s) else PB.getUpperBound(s - minCount)\n+          val q1 = lmbd1 / n\n+          val q2 = lmbd2 / n\n+          stratum.q1 = Some(q1)\n+          stratum.q2 = Some(q2)\n+        }\n+        val x1 = if (stratum.q1.get == 0) 0L else rng.nextPoisson(stratum.q1.get)\n+        if (x1 > 0) {\n+          stratum.incrNumAccepted(x1)\n+        }\n+        val x2 = rng.nextPoisson(stratum.q2.get).toInt\n+        if (x2 > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(x2)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement.\n+        // Hence, q1 and q2 change on every iteration.\n+        val g1 = - math.log(delta) / stratum.numItems\n+        val g2 = (2.0 / 3.0) * g1\n+        val q1 = math.max(0, fraction + g2 - math.sqrt((g2 * g2 + 3 * g2 * fraction)))\n+        val q2 = math.min(1, fraction + g1 + math.sqrt(g1 * g1 + 2 * g1 * fraction))\n+\n+        val x = rng.nextUniform(0.0, 1.0)\n+        if (x < q1) {\n+          stratum.incrNumAccepted()\n+        } else if (x < q2) {\n+          stratum.addToWaitList(x)\n+        }\n+        stratum.q1 = Some(q1)\n+        stratum.q2 = Some(q2)\n+      }\n+      stratum.incrNumItems()\n+      result\n+    }\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to combine results from different partitions, as\n+   * returned by seqOp.\n+   */\n+  def getCombOp[K](): (Result[K], Result[K]) => Result[K] = {\n+    (r1: Result[K], r2: Result[K]) => {\n+      // take union of both key sets in case one partition doesn't contain all keys\n+      val keyUnion = r1.resultMap.keys.toSet.union(r2.resultMap.keys.toSet)\n+\n+      // Use r2 to keep the combined result since r1 is usual empty\n+      for (key <- keyUnion) {\n+        val entry1 = r1.resultMap.get(key)\n+        val entry2 = r2.resultMap.get(key)\n+        if (entry2.isEmpty && entry1.isDefined) {\n+          r2.resultMap += (key -> entry1.get)\n+        } else if (entry1.isDefined && entry2.isDefined) {\n+          entry2.get.addToWaitList(entry1.get.waitList)\n+          entry2.get.incrNumAccepted(entry1.get.numAccepted)\n+          entry2.get.incrNumItems(entry1.get.numItems)\n+        }\n+      }\n+      r2\n+    }\n+  }\n+\n+  /**\n+   * Given the result returned by the aggregate function, we need to determine the threshold used\n+   * to accept items to generate the exact sample size.\n+   */\n+  def computeThresholdByKey[K](finalResult: Map[K, Stratum], fractionByKey: (K => Double)):\n+    (K => Double) = {\n+    val thresholdByKey = new mutable.HashMap[K, Double]()\n+    for ((key, stratum) <- finalResult) {\n+      val fraction = fractionByKey(key)\n+      val s = math.ceil(stratum.numItems * fraction).toLong\n+      if (stratum.numAccepted > s) {\n+        logWarning(\"Pre-accepted too many\")\n+        thresholdByKey += (key -> stratum.q1.get)\n+      } else {\n+        val numWaitListAccepted = (s - stratum.numAccepted).toInt\n+        if (numWaitListAccepted >= stratum.waitList.size) {\n+          logWarning(\"WaitList too short\")\n+          thresholdByKey += (key -> stratum.q2.get)\n+        } else {\n+          thresholdByKey += (key -> stratum.waitList.sorted.apply(numWaitListAccepted))\n+        }\n+      }\n+    }\n+    thresholdByKey\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling without replacement.\n+   *\n+   * When exact sample size is required, we make an additional pass over the RDD to determine the\n+   * exact sampling rate that guarantees sample size with high confidence.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getBernoulliSamplingFunction[K, V](rdd: RDD[(K,  V)],\n+      fractionByKey: K => Double,\n+      exact: Boolean,\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    var samplingRateByKey = fractionByKey\n+    if (exact) {\n+      // determine threshold for each stratum and resample\n+      val seqOp = StratifiedSampler.getSeqOp[K,V](false, fractionByKey, None)\n+      val combOp = StratifiedSampler.getCombOp[K]()\n+      val zeroU = new Result[K](Map[K, Stratum](), seed = seed)\n+      val finalResult = aggregateWithContext(zeroU)(rdd, seqOp, combOp).resultMap\n+      samplingRateByKey = StratifiedSampler.computeThresholdByKey(finalResult, fractionByKey)\n+    }\n+    (idx: Int, iter: Iterator[(K, V)]) => {\n+      val random = new RandomDataGenerator\n+      random.reSeed(seed + idx)\n+      iter.filter(t => random.nextUniform(0.0, 1.0) < samplingRateByKey(t._1))\n+    }\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling with replacement.\n+   *\n+   * When exact sample size is required, we make two additional passed over the RDD to determine\n+   * the exact sampling rate that guarantees sample size with high confidence. The first pass\n+   * counts the number of items in each stratum (group of items with the same key) in the RDD, and\n+   * the second pass uses the counts to determine exact sampling rates.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getPoissonSamplingFunction[K, V](rdd:RDD[(K,  V)],\n+      fractionByKey: K => Double,\n+      exact: Boolean,\n+      counts: Option[Map[K, Long]],\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    // TODO implement the streaming version of sampling w/ replacement that doesn't require counts\n+    if (exact) {\n+      val seqOp = StratifiedSampler.getSeqOp[K,V](true, fractionByKey, counts)\n+      val combOp = StratifiedSampler.getCombOp[K]()\n+      val zeroU = new Result[K](Map[K, Stratum](), seed = seed)\n+      val finalResult = aggregateWithContext(zeroU)(rdd, seqOp, combOp).resultMap\n+      val thresholdByKey = StratifiedSampler.computeThresholdByKey(finalResult, fractionByKey)\n+      (idx: Int, iter: Iterator[(K, V)]) => {\n+        val random = new RandomDataGenerator()\n+        random.reSeed(seed + idx)\n+        iter.flatMap { t =>\n+          val q1 = finalResult.get(t._1).get.q1.getOrElse(0.0)\n+          val q2 = finalResult.get(t._1).get.q2.getOrElse(0.0)"
  }],
  "prId": 1025
}, {
  "comments": [{
    "author": {
      "login": "falaki"
    },
    "body": "Here you don't need to explicitly declare the type (please apply this comment to other similar places where you explicitly declare type of values or variables).\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-04T01:36:06Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.{Map, mutable}\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.{Logging, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.Utils\n+import org.apache.spark.util.random.{PoissonBounds => PB}\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * A version of {@link #aggregate()} that passes the TaskContext to the function that does\n+   * aggregation for each partition. This function avoids creating an extra depth in the RDD\n+   * lineage, as opposed to using mapPartitionsWithId, which results in slightly improved run time.\n+   */\n+  def aggregateWithContext[U: ClassTag, T: ClassTag](zeroValue: U)\n+      (rdd: RDD[T],\n+       seqOp: ((TaskContext, U), T) => U,\n+       combOp: (U, U) => U): U = {\n+    val sc: SparkContext = rdd.sparkContext\n+    // Clone the zero value since we will also be serializing it as part of tasks\n+    var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance())\n+    // pad seqOp and combOp with taskContext to conform to aggregate's signature in TraversableOnce\n+    val paddedSeqOp = (arg1: (TaskContext, U), item: T) => (arg1._1, seqOp(arg1, item))\n+    val paddedcombOp = (arg1: (TaskContext, U), arg2: (TaskContext, U)) =>\n+      (arg1._1, combOp(arg1._2, arg1._2))\n+    val cleanSeqOp = sc.clean(paddedSeqOp)\n+    val cleanCombOp = sc.clean(paddedcombOp)\n+    val aggregatePartition = (tc: TaskContext, it: Iterator[T]) =>\n+      (it.aggregate(tc, zeroValue)(cleanSeqOp, cleanCombOp))._2\n+    val mergeResult = (index: Int, taskResult: U) => jobResult = combOp(jobResult, taskResult)\n+    sc.runJob(rdd, aggregatePartition, mergeResult)\n+    jobResult\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractionByKey: (K => Double),\n+      counts: Option[Map[K, Long]]): ((TaskContext, Result[K]),(K, V)) => Result[K] = {\n+    val delta = 5e-5\n+    (U: (TaskContext, Result[K]), item: (K, V)) => {\n+      val result = U._2\n+      val tc = U._1\n+      val rng = result.getRand(tc.partitionId)\n+      val fraction = fractionByKey(item._1)\n+      val stratum = result.getEntry(item._1)\n+      if (withReplacement) {\n+        // compute q1 and q2 only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.q1.isEmpty || stratum.q2.isEmpty) {\n+          val n = counts.get(item._1)\n+          val s = math.ceil(n * fraction).toLong\n+          val lmbd1 = PB.getLowerBound(s)\n+          val minCount = PB.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) PB.getUpperBound(s) else PB.getUpperBound(s - minCount)\n+          val q1 = lmbd1 / n\n+          val q2 = lmbd2 / n\n+          stratum.q1 = Some(q1)\n+          stratum.q2 = Some(q2)\n+        }\n+        val x1 = if (stratum.q1.get == 0) 0L else rng.nextPoisson(stratum.q1.get)\n+        if (x1 > 0) {\n+          stratum.incrNumAccepted(x1)\n+        }\n+        val x2 = rng.nextPoisson(stratum.q2.get).toInt\n+        if (x2 > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(x2)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement.\n+        // Hence, q1 and q2 change on every iteration.\n+        val g1 = - math.log(delta) / stratum.numItems\n+        val g2 = (2.0 / 3.0) * g1\n+        val q1 = math.max(0, fraction + g2 - math.sqrt((g2 * g2 + 3 * g2 * fraction)))\n+        val q2 = math.min(1, fraction + g1 + math.sqrt(g1 * g1 + 2 * g1 * fraction))\n+\n+        val x = rng.nextUniform(0.0, 1.0)\n+        if (x < q1) {\n+          stratum.incrNumAccepted()\n+        } else if (x < q2) {\n+          stratum.addToWaitList(x)\n+        }\n+        stratum.q1 = Some(q1)\n+        stratum.q2 = Some(q2)\n+      }\n+      stratum.incrNumItems()\n+      result\n+    }\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to combine results from different partitions, as\n+   * returned by seqOp.\n+   */\n+  def getCombOp[K](): (Result[K], Result[K]) => Result[K] = {\n+    (r1: Result[K], r2: Result[K]) => {\n+      // take union of both key sets in case one partition doesn't contain all keys\n+      val keyUnion = r1.resultMap.keys.toSet.union(r2.resultMap.keys.toSet)\n+\n+      // Use r2 to keep the combined result since r1 is usual empty\n+      for (key <- keyUnion) {\n+        val entry1 = r1.resultMap.get(key)\n+        val entry2 = r2.resultMap.get(key)\n+        if (entry2.isEmpty && entry1.isDefined) {\n+          r2.resultMap += (key -> entry1.get)\n+        } else if (entry1.isDefined && entry2.isDefined) {\n+          entry2.get.addToWaitList(entry1.get.waitList)\n+          entry2.get.incrNumAccepted(entry1.get.numAccepted)\n+          entry2.get.incrNumItems(entry1.get.numItems)\n+        }\n+      }\n+      r2\n+    }\n+  }\n+\n+  /**\n+   * Given the result returned by the aggregate function, we need to determine the threshold used\n+   * to accept items to generate the exact sample size.\n+   */\n+  def computeThresholdByKey[K](finalResult: Map[K, Stratum], fractionByKey: (K => Double)):\n+    (K => Double) = {\n+    val thresholdByKey = new mutable.HashMap[K, Double]()\n+    for ((key, stratum) <- finalResult) {\n+      val fraction = fractionByKey(key)\n+      val s = math.ceil(stratum.numItems * fraction).toLong\n+      if (stratum.numAccepted > s) {\n+        logWarning(\"Pre-accepted too many\")\n+        thresholdByKey += (key -> stratum.q1.get)\n+      } else {\n+        val numWaitListAccepted = (s - stratum.numAccepted).toInt\n+        if (numWaitListAccepted >= stratum.waitList.size) {\n+          logWarning(\"WaitList too short\")\n+          thresholdByKey += (key -> stratum.q2.get)\n+        } else {\n+          thresholdByKey += (key -> stratum.waitList.sorted.apply(numWaitListAccepted))\n+        }\n+      }\n+    }\n+    thresholdByKey\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling without replacement.\n+   *\n+   * When exact sample size is required, we make an additional pass over the RDD to determine the\n+   * exact sampling rate that guarantees sample size with high confidence.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getBernoulliSamplingFunction[K, V](rdd: RDD[(K,  V)],\n+      fractionByKey: K => Double,\n+      exact: Boolean,\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    var samplingRateByKey = fractionByKey\n+    if (exact) {\n+      // determine threshold for each stratum and resample\n+      val seqOp = StratifiedSampler.getSeqOp[K,V](false, fractionByKey, None)\n+      val combOp = StratifiedSampler.getCombOp[K]()\n+      val zeroU = new Result[K](Map[K, Stratum](), seed = seed)\n+      val finalResult = aggregateWithContext(zeroU)(rdd, seqOp, combOp).resultMap\n+      samplingRateByKey = StratifiedSampler.computeThresholdByKey(finalResult, fractionByKey)\n+    }\n+    (idx: Int, iter: Iterator[(K, V)]) => {\n+      val random = new RandomDataGenerator\n+      random.reSeed(seed + idx)\n+      iter.filter(t => random.nextUniform(0.0, 1.0) < samplingRateByKey(t._1))\n+    }\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling with replacement.\n+   *\n+   * When exact sample size is required, we make two additional passed over the RDD to determine\n+   * the exact sampling rate that guarantees sample size with high confidence. The first pass\n+   * counts the number of items in each stratum (group of items with the same key) in the RDD, and\n+   * the second pass uses the counts to determine exact sampling rates.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getPoissonSamplingFunction[K, V](rdd:RDD[(K,  V)],\n+      fractionByKey: K => Double,\n+      exact: Boolean,\n+      counts: Option[Map[K, Long]],\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    // TODO implement the streaming version of sampling w/ replacement that doesn't require counts\n+    if (exact) {\n+      val seqOp = StratifiedSampler.getSeqOp[K,V](true, fractionByKey, counts)\n+      val combOp = StratifiedSampler.getCombOp[K]()\n+      val zeroU = new Result[K](Map[K, Stratum](), seed = seed)\n+      val finalResult = aggregateWithContext(zeroU)(rdd, seqOp, combOp).resultMap\n+      val thresholdByKey = StratifiedSampler.computeThresholdByKey(finalResult, fractionByKey)\n+      (idx: Int, iter: Iterator[(K, V)]) => {\n+        val random = new RandomDataGenerator()\n+        random.reSeed(seed + idx)\n+        iter.flatMap { t =>\n+          val q1 = finalResult.get(t._1).get.q1.getOrElse(0.0)\n+          val q2 = finalResult.get(t._1).get.q2.getOrElse(0.0)\n+          val x1 = if (q1 == 0) 0L else random.nextPoisson(q1)\n+          val x2 = random.nextPoisson(q2).toInt\n+          val x = x1 + (0 until x2).count(i => random.nextUniform(0.0, 1.0) < thresholdByKey(t._1))\n+          if (x > 0) {\n+            Iterator.fill(x.toInt)(t)\n+          } else {\n+            Iterator.empty\n+          }\n+        }\n+      }\n+    } else {\n+      (idx: Int, iter: Iterator[(K, V)]) => {\n+        val random = new RandomDataGenerator()\n+        random.reSeed(seed + idx)\n+        iter.flatMap { t =>\n+          val count = random.nextPoisson(fractionByKey(t._1)).toInt\n+          if (count > 0) {\n+            Iterator.fill(count)(t)\n+          } else {\n+            Iterator.empty\n+          }\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * Object used by seqOp to keep track of the number of items accepted and items waitlisted per\n+ * stratum, as well as the bounds for accepting and waitlisting items.\n+ */\n+private[random] class Stratum(var numItems: Long = 0L, var numAccepted: Long = 0L)\n+  extends Serializable {\n+\n+  var waitList: ArrayBuffer[Double] = new ArrayBuffer[Double]\n+  var q1: Option[Double] = None // upper bound for accepting item instantly\n+  var q2: Option[Double] = None // upper bound for adding item to waitlist\n+\n+  def incrNumItems(by: Long = 1L) = numItems += by\n+\n+  def incrNumAccepted(by: Long = 1L) = numAccepted += by\n+\n+  def addToWaitList(elem: Double) = waitList += elem\n+\n+  def addToWaitList(elems: ArrayBuffer[Double]) = waitList ++= elems\n+}\n+\n+/**\n+ * Object used by seqOp and combOp to keep track of the sampling statistics for all strata.\n+ *\n+ * When used by seqOp for each partition, we also keep track of the partition ID in this object\n+ * to make sure a single random number generator with a unique seed is used for each partition.\n+ */\n+private[random] class Result[K](var resultMap: Map[K, Stratum],\n+                var cachedPartitionId: Option[Int] = None,\n+                val seed: Long)\n+  extends Serializable {\n+\n+  var rand: RandomDataGenerator = new RandomDataGenerator"
  }],
  "prId": 1025
}, {
  "comments": [{
    "author": {
      "login": "falaki"
    },
    "body": "Style nit:  Return type of the function is in a new line. We can join these two lines.\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-07T21:11:09Z",
    "diffHunk": "@@ -0,0 +1,335 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.Map\n+import scala.collection.mutable.{ArrayBuffer, HashMap, Map => MMap}\n+import scala.reflect.ClassTag\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.{Logging, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Auxiliary functions and data structures for the sampleByKey method in PairRDDFunctions.\n+ *\n+ * For more theoretical background on the sampling technqiues used here, please refer to\n+ * http://jmlr.org/proceedings/papers/v28/meng13a.html\n+ */\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * A version of {@link #aggregate()} that passes the TaskContext to the function that does\n+   * aggregation for each partition. This function avoids creating an extra depth in the RDD\n+   * lineage, as opposed to using mapPartitionsWithIndex, which results in slightly improved\n+   * run time.\n+   */\n+  def aggregateWithContext[U: ClassTag, T: ClassTag](zeroValue: U)\n+      (rdd: RDD[T],\n+       seqOp: ((TaskContext, U), T) => U,\n+       combOp: (U, U) => U): U = {\n+    val sc: SparkContext = rdd.sparkContext\n+    // Clone the zero value since we will also be serializing it as part of tasks\n+    var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance())\n+    // pad seqOp and combOp with taskContext to conform to aggregate's signature in TraversableOnce\n+    val paddedSeqOp = (arg1: (TaskContext, U), item: T) => (arg1._1, seqOp(arg1, item))\n+    val paddedCombOp = (arg1: (TaskContext, U), arg2: (TaskContext, U)) =>\n+      (arg1._1, combOp(arg1._2, arg1._2))\n+    val cleanSeqOp = sc.clean(paddedSeqOp)\n+    val cleanCombOp = sc.clean(paddedCombOp)\n+    val aggregatePartition = (tc: TaskContext, it: Iterator[T]) =>\n+      (it.aggregate(tc, zeroValue)(cleanSeqOp, cleanCombOp))._2\n+    val mergeResult = (index: Int, taskResult: U) => jobResult = combOp(jobResult, taskResult)\n+    sc.runJob(rdd, aggregatePartition, mergeResult)\n+    jobResult\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      counts: Option[Map[K, Long]]): ((TaskContext, Result[K]), (K, V)) => Result[K] = {\n+    val delta = 5e-5\n+    (output: (TaskContext, Result[K]), item: (K, V)) => {\n+      val result = output._2\n+      val tc = output._1\n+      val rng = result.getRand(tc.partitionId)\n+      val fraction = fractions(item._1)\n+      val stratum = result.getEntry(item._1)\n+      if (withReplacement) {\n+        // compute acceptBound and waitListBound only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.areBoundsEmpty) {\n+          val n = counts.get(item._1)\n+          val sampleSize = math.ceil(n * fraction).toLong\n+          val lmbd1 = PoissonBounds.getLowerBound(sampleSize)\n+          val minCount = PoissonBounds.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) {\n+            PoissonBounds.getUpperBound(sampleSize)\n+          } else {\n+            PoissonBounds.getUpperBound(sampleSize - minCount)\n+          }\n+          stratum.acceptBound = lmbd1 / n\n+          stratum.waitListBound = lmbd2 / n\n+        }\n+        val x1 = if (stratum.acceptBound == 0.0) 0L else rng.nextPoisson(stratum.acceptBound)\n+        if (x1 > 0) {\n+          stratum.incrNumAccepted(x1)\n+        }\n+        val x2 = rng.nextPoisson(stratum.waitListBound).toInt\n+        if (x2 > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(x2)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement to avoid\n+        // using an extra pass over the RDD for computing the count.\n+        // Hence, acceptBound and waitListBound change on every iteration.\n+        val g1 = - math.log(delta) / stratum.numItems // gamma1\n+        val g2 = (2.0 / 3.0) * g1 // gamma 2\n+        stratum.acceptBound = math.max(0, fraction + g2 - math.sqrt((g2 * g2 + 3 * g2 * fraction)))\n+        stratum.waitListBound = math.min(1, fraction + g1 + math.sqrt(g1 * g1 + 2 * g1 * fraction))\n+\n+        val x = rng.nextUniform(0.0, 1.0)\n+        if (x < stratum.acceptBound) {\n+          stratum.incrNumAccepted()\n+        } else if (x < stratum.waitListBound) {\n+          stratum.addToWaitList(x)\n+        }\n+      }\n+      stratum.incrNumItems()\n+      result\n+    }\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to combine results from different partitions, as\n+   * returned by seqOp.\n+   */\n+  def getCombOp[K](): (Result[K], Result[K]) => Result[K] = {\n+    (r1: Result[K], r2: Result[K]) => {\n+      // take union of both key sets in case one partition doesn't contain all keys\n+      val keyUnion = r1.resultMap.keySet.union(r2.resultMap.keySet)\n+\n+      // Use r2 to keep the combined result since r1 is usual empty\n+      for (key <- keyUnion) {\n+        val entry1 = r1.resultMap.get(key)\n+        if (r2.resultMap.contains(key)) {\n+          r2.resultMap(key).merge(entry1)\n+        } else {\n+          r2.addEntry(key, entry1)\n+        }\n+      }\n+      r2\n+    }\n+  }\n+\n+  /**\n+   * Given the result returned by the aggregate function, determine the threshold for accepting\n+   * items to generate exact sample size.\n+   *\n+   * To do so, we compute sampleSize = math.ceil(size * samplingRate) for each stratum and compare\n+   * it to the number of items that were accepted instantly and the number of items in the waitlist\n+   * for that stratum. Most of the time, numAccepted <= sampleSize <= (numAccepted + numWaitlisted),\n+   * which means we need to sort the elements in the waitlist by their associated values in order\n+   * to find the value T s.t. |{elements in the stratum whose associated values <= T}| = sampleSize.\n+   * Note that all elements in the waitlist have values >= bound for instant accept, so a T value\n+   * in the waitlist range would allow all elements that were instantly accepted on the first pass\n+   * to be included in the sample.\n+   */\n+  def computeThresholdByKey[K](finalResult: Map[K, Stratum],\n+      fractions: Map[K, Double]):"
  }, {
    "author": {
      "login": "dorx"
    },
    "body": "Actually what's the rule of thumb here? I was under the impression that if you put your arguments on different lines the return type needs its own line, no?\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-08T18:44:44Z",
    "diffHunk": "@@ -0,0 +1,335 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.Map\n+import scala.collection.mutable.{ArrayBuffer, HashMap, Map => MMap}\n+import scala.reflect.ClassTag\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.{Logging, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Auxiliary functions and data structures for the sampleByKey method in PairRDDFunctions.\n+ *\n+ * For more theoretical background on the sampling technqiues used here, please refer to\n+ * http://jmlr.org/proceedings/papers/v28/meng13a.html\n+ */\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * A version of {@link #aggregate()} that passes the TaskContext to the function that does\n+   * aggregation for each partition. This function avoids creating an extra depth in the RDD\n+   * lineage, as opposed to using mapPartitionsWithIndex, which results in slightly improved\n+   * run time.\n+   */\n+  def aggregateWithContext[U: ClassTag, T: ClassTag](zeroValue: U)\n+      (rdd: RDD[T],\n+       seqOp: ((TaskContext, U), T) => U,\n+       combOp: (U, U) => U): U = {\n+    val sc: SparkContext = rdd.sparkContext\n+    // Clone the zero value since we will also be serializing it as part of tasks\n+    var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance())\n+    // pad seqOp and combOp with taskContext to conform to aggregate's signature in TraversableOnce\n+    val paddedSeqOp = (arg1: (TaskContext, U), item: T) => (arg1._1, seqOp(arg1, item))\n+    val paddedCombOp = (arg1: (TaskContext, U), arg2: (TaskContext, U)) =>\n+      (arg1._1, combOp(arg1._2, arg1._2))\n+    val cleanSeqOp = sc.clean(paddedSeqOp)\n+    val cleanCombOp = sc.clean(paddedCombOp)\n+    val aggregatePartition = (tc: TaskContext, it: Iterator[T]) =>\n+      (it.aggregate(tc, zeroValue)(cleanSeqOp, cleanCombOp))._2\n+    val mergeResult = (index: Int, taskResult: U) => jobResult = combOp(jobResult, taskResult)\n+    sc.runJob(rdd, aggregatePartition, mergeResult)\n+    jobResult\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      counts: Option[Map[K, Long]]): ((TaskContext, Result[K]), (K, V)) => Result[K] = {\n+    val delta = 5e-5\n+    (output: (TaskContext, Result[K]), item: (K, V)) => {\n+      val result = output._2\n+      val tc = output._1\n+      val rng = result.getRand(tc.partitionId)\n+      val fraction = fractions(item._1)\n+      val stratum = result.getEntry(item._1)\n+      if (withReplacement) {\n+        // compute acceptBound and waitListBound only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.areBoundsEmpty) {\n+          val n = counts.get(item._1)\n+          val sampleSize = math.ceil(n * fraction).toLong\n+          val lmbd1 = PoissonBounds.getLowerBound(sampleSize)\n+          val minCount = PoissonBounds.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) {\n+            PoissonBounds.getUpperBound(sampleSize)\n+          } else {\n+            PoissonBounds.getUpperBound(sampleSize - minCount)\n+          }\n+          stratum.acceptBound = lmbd1 / n\n+          stratum.waitListBound = lmbd2 / n\n+        }\n+        val x1 = if (stratum.acceptBound == 0.0) 0L else rng.nextPoisson(stratum.acceptBound)\n+        if (x1 > 0) {\n+          stratum.incrNumAccepted(x1)\n+        }\n+        val x2 = rng.nextPoisson(stratum.waitListBound).toInt\n+        if (x2 > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(x2)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement to avoid\n+        // using an extra pass over the RDD for computing the count.\n+        // Hence, acceptBound and waitListBound change on every iteration.\n+        val g1 = - math.log(delta) / stratum.numItems // gamma1\n+        val g2 = (2.0 / 3.0) * g1 // gamma 2\n+        stratum.acceptBound = math.max(0, fraction + g2 - math.sqrt((g2 * g2 + 3 * g2 * fraction)))\n+        stratum.waitListBound = math.min(1, fraction + g1 + math.sqrt(g1 * g1 + 2 * g1 * fraction))\n+\n+        val x = rng.nextUniform(0.0, 1.0)\n+        if (x < stratum.acceptBound) {\n+          stratum.incrNumAccepted()\n+        } else if (x < stratum.waitListBound) {\n+          stratum.addToWaitList(x)\n+        }\n+      }\n+      stratum.incrNumItems()\n+      result\n+    }\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to combine results from different partitions, as\n+   * returned by seqOp.\n+   */\n+  def getCombOp[K](): (Result[K], Result[K]) => Result[K] = {\n+    (r1: Result[K], r2: Result[K]) => {\n+      // take union of both key sets in case one partition doesn't contain all keys\n+      val keyUnion = r1.resultMap.keySet.union(r2.resultMap.keySet)\n+\n+      // Use r2 to keep the combined result since r1 is usual empty\n+      for (key <- keyUnion) {\n+        val entry1 = r1.resultMap.get(key)\n+        if (r2.resultMap.contains(key)) {\n+          r2.resultMap(key).merge(entry1)\n+        } else {\n+          r2.addEntry(key, entry1)\n+        }\n+      }\n+      r2\n+    }\n+  }\n+\n+  /**\n+   * Given the result returned by the aggregate function, determine the threshold for accepting\n+   * items to generate exact sample size.\n+   *\n+   * To do so, we compute sampleSize = math.ceil(size * samplingRate) for each stratum and compare\n+   * it to the number of items that were accepted instantly and the number of items in the waitlist\n+   * for that stratum. Most of the time, numAccepted <= sampleSize <= (numAccepted + numWaitlisted),\n+   * which means we need to sort the elements in the waitlist by their associated values in order\n+   * to find the value T s.t. |{elements in the stratum whose associated values <= T}| = sampleSize.\n+   * Note that all elements in the waitlist have values >= bound for instant accept, so a T value\n+   * in the waitlist range would allow all elements that were instantly accepted on the first pass\n+   * to be included in the sample.\n+   */\n+  def computeThresholdByKey[K](finalResult: Map[K, Stratum],\n+      fractions: Map[K, Double]):"
  }],
  "prId": 1025
}, {
  "comments": [{
    "author": {
      "login": "falaki"
    },
    "body": "Minor: I still think it is nicer to just rename `g1` to `gamma1`\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-09T00:25:44Z",
    "diffHunk": "@@ -0,0 +1,310 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.Map\n+import scala.collection.mutable.{ArrayBuffer, HashMap, Map => MMap}\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Auxiliary functions and data structures for the sampleByKey method in PairRDDFunctions.\n+ *\n+ * Essentially, when exact sample size is necessary, we make additional passes over the RDD to\n+ * compute the exact threshold value to use for each stratum to guarantee exact sample size with\n+ * high probability. This is achieved by maintaining a waitlist of size O(log(s)), where s is the\n+ * desired sample size for each stratum.\n+ *\n+ * Like in simple random sampling, we generate a random value for each item from the\n+ * uniform  distribution [0.0, 1.0]. All items with values <= min(values of items in the waitlist)\n+ * are accepted into the sample instantly. The threshold for instant accept is designed so that\n+ * s - numAccepted = O(log(s)), where s is again the desired sample size. Thus, by maintaining a\n+ * waitlist size = O(log(s)), we will be able to create a sample of the exact size s by adding\n+ * a portion of the waitlist to the set of items that are instantly accepted. The exact threshold\n+ * is computed by sorting the values in the waitlist and picking the value at (s - numAccepted).\n+ *\n+ * Note that since we use the same seed for the RNG when computing the thresholds and the actual\n+ * sample, our computed thresholds are guaranteed to produce the desired sample size.\n+ *\n+ * For more theoretical background on the sampling techniques used here, please refer to\n+ * http://jmlr.org/proceedings/papers/v28/meng13a.html\n+ */\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * Count the number of items instantly accepted and generate the waitlist for each stratum.\n+   *\n+   * This is only invoked when exact sample size is required.\n+   */\n+  def getCounts[K, V](rdd: RDD[(K, V)],\n+      withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      counts: Option[Map[K, Long]],\n+      seed: Long): MMap[K, Stratum] = {\n+    val combOp = getCombOp[K]\n+    val mappedPartitionRDD = rdd.mapPartitionsWithIndex({ case (partition, iter) =>\n+      val zeroU: MMap[K, Stratum] = new HashMap[K, Stratum]()\n+      val rng = new RandomDataGenerator()\n+      rng.reSeed(seed + partition)\n+      val seqOp = getSeqOp(withReplacement, fractions, rng, counts)\n+      Iterator(iter.aggregate(zeroU)(seqOp, combOp))\n+    }, preservesPartitioning=true)\n+    mappedPartitionRDD.reduce(combOp)\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      rng: RandomDataGenerator,\n+      counts: Option[Map[K, Long]]): (MMap[K, Stratum], (K, V)) => MMap[K, Stratum] = {\n+    val delta = 5e-5\n+    (result: MMap[K, Stratum], item: (K, V)) => {\n+      val key = item._1\n+      val fraction = fractions(key)\n+      if (!result.contains(key)) {\n+        result += (key -> new Stratum())\n+      }\n+      val stratum = result(key)\n+\n+      if (withReplacement) {\n+        // compute acceptBound and waitListBound only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.areBoundsEmpty) {\n+          val n = counts.get(key)\n+          val sampleSize = math.ceil(n * fraction).toLong\n+          val lmbd1 = PoissonBounds.getLowerBound(sampleSize)\n+          val minCount = PoissonBounds.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) {\n+            PoissonBounds.getUpperBound(sampleSize)\n+          } else {\n+            PoissonBounds.getUpperBound(sampleSize - minCount)\n+          }\n+          stratum.acceptBound = lmbd1 / n\n+          stratum.waitListBound = lmbd2 / n\n+        }\n+        val acceptBound = stratum.acceptBound\n+        val copiesAccepted = if (acceptBound == 0.0) 0L else rng.nextPoisson(acceptBound)\n+        if (copiesAccepted > 0) {\n+          stratum.incrNumAccepted(copiesAccepted)\n+        }\n+        val copiesWaitlisted = rng.nextPoisson(stratum.waitListBound).toInt\n+        if (copiesWaitlisted > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(copiesWaitlisted)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement to avoid\n+        // using an extra pass over the RDD for computing the count.\n+        // Hence, acceptBound and waitListBound change on every iteration.\n+        val g1 = - math.log(delta) / stratum.numItems // gamma1\n+        val g2 = (2.0 / 3.0) * g1                     // gamma 2"
  }, {
    "author": {
      "login": "dorx"
    },
    "body": "I was trying to make the next two lines fit on the same lines, but it doesn't actually look too bad breaking them into two lines each.\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-09T00:35:25Z",
    "diffHunk": "@@ -0,0 +1,310 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.Map\n+import scala.collection.mutable.{ArrayBuffer, HashMap, Map => MMap}\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Auxiliary functions and data structures for the sampleByKey method in PairRDDFunctions.\n+ *\n+ * Essentially, when exact sample size is necessary, we make additional passes over the RDD to\n+ * compute the exact threshold value to use for each stratum to guarantee exact sample size with\n+ * high probability. This is achieved by maintaining a waitlist of size O(log(s)), where s is the\n+ * desired sample size for each stratum.\n+ *\n+ * Like in simple random sampling, we generate a random value for each item from the\n+ * uniform  distribution [0.0, 1.0]. All items with values <= min(values of items in the waitlist)\n+ * are accepted into the sample instantly. The threshold for instant accept is designed so that\n+ * s - numAccepted = O(log(s)), where s is again the desired sample size. Thus, by maintaining a\n+ * waitlist size = O(log(s)), we will be able to create a sample of the exact size s by adding\n+ * a portion of the waitlist to the set of items that are instantly accepted. The exact threshold\n+ * is computed by sorting the values in the waitlist and picking the value at (s - numAccepted).\n+ *\n+ * Note that since we use the same seed for the RNG when computing the thresholds and the actual\n+ * sample, our computed thresholds are guaranteed to produce the desired sample size.\n+ *\n+ * For more theoretical background on the sampling techniques used here, please refer to\n+ * http://jmlr.org/proceedings/papers/v28/meng13a.html\n+ */\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * Count the number of items instantly accepted and generate the waitlist for each stratum.\n+   *\n+   * This is only invoked when exact sample size is required.\n+   */\n+  def getCounts[K, V](rdd: RDD[(K, V)],\n+      withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      counts: Option[Map[K, Long]],\n+      seed: Long): MMap[K, Stratum] = {\n+    val combOp = getCombOp[K]\n+    val mappedPartitionRDD = rdd.mapPartitionsWithIndex({ case (partition, iter) =>\n+      val zeroU: MMap[K, Stratum] = new HashMap[K, Stratum]()\n+      val rng = new RandomDataGenerator()\n+      rng.reSeed(seed + partition)\n+      val seqOp = getSeqOp(withReplacement, fractions, rng, counts)\n+      Iterator(iter.aggregate(zeroU)(seqOp, combOp))\n+    }, preservesPartitioning=true)\n+    mappedPartitionRDD.reduce(combOp)\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      rng: RandomDataGenerator,\n+      counts: Option[Map[K, Long]]): (MMap[K, Stratum], (K, V)) => MMap[K, Stratum] = {\n+    val delta = 5e-5\n+    (result: MMap[K, Stratum], item: (K, V)) => {\n+      val key = item._1\n+      val fraction = fractions(key)\n+      if (!result.contains(key)) {\n+        result += (key -> new Stratum())\n+      }\n+      val stratum = result(key)\n+\n+      if (withReplacement) {\n+        // compute acceptBound and waitListBound only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.areBoundsEmpty) {\n+          val n = counts.get(key)\n+          val sampleSize = math.ceil(n * fraction).toLong\n+          val lmbd1 = PoissonBounds.getLowerBound(sampleSize)\n+          val minCount = PoissonBounds.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) {\n+            PoissonBounds.getUpperBound(sampleSize)\n+          } else {\n+            PoissonBounds.getUpperBound(sampleSize - minCount)\n+          }\n+          stratum.acceptBound = lmbd1 / n\n+          stratum.waitListBound = lmbd2 / n\n+        }\n+        val acceptBound = stratum.acceptBound\n+        val copiesAccepted = if (acceptBound == 0.0) 0L else rng.nextPoisson(acceptBound)\n+        if (copiesAccepted > 0) {\n+          stratum.incrNumAccepted(copiesAccepted)\n+        }\n+        val copiesWaitlisted = rng.nextPoisson(stratum.waitListBound).toInt\n+        if (copiesWaitlisted > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(copiesWaitlisted)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement to avoid\n+        // using an extra pass over the RDD for computing the count.\n+        // Hence, acceptBound and waitListBound change on every iteration.\n+        val g1 = - math.log(delta) / stratum.numItems // gamma1\n+        val g2 = (2.0 / 3.0) * g1                     // gamma 2"
  }],
  "prId": 1025
}, {
  "comments": [{
    "author": {
      "login": "falaki"
    },
    "body": "Minor: Similarly for r1 and r2\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-09T00:26:29Z",
    "diffHunk": "@@ -0,0 +1,310 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.Map\n+import scala.collection.mutable.{ArrayBuffer, HashMap, Map => MMap}\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Auxiliary functions and data structures for the sampleByKey method in PairRDDFunctions.\n+ *\n+ * Essentially, when exact sample size is necessary, we make additional passes over the RDD to\n+ * compute the exact threshold value to use for each stratum to guarantee exact sample size with\n+ * high probability. This is achieved by maintaining a waitlist of size O(log(s)), where s is the\n+ * desired sample size for each stratum.\n+ *\n+ * Like in simple random sampling, we generate a random value for each item from the\n+ * uniform  distribution [0.0, 1.0]. All items with values <= min(values of items in the waitlist)\n+ * are accepted into the sample instantly. The threshold for instant accept is designed so that\n+ * s - numAccepted = O(log(s)), where s is again the desired sample size. Thus, by maintaining a\n+ * waitlist size = O(log(s)), we will be able to create a sample of the exact size s by adding\n+ * a portion of the waitlist to the set of items that are instantly accepted. The exact threshold\n+ * is computed by sorting the values in the waitlist and picking the value at (s - numAccepted).\n+ *\n+ * Note that since we use the same seed for the RNG when computing the thresholds and the actual\n+ * sample, our computed thresholds are guaranteed to produce the desired sample size.\n+ *\n+ * For more theoretical background on the sampling techniques used here, please refer to\n+ * http://jmlr.org/proceedings/papers/v28/meng13a.html\n+ */\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * Count the number of items instantly accepted and generate the waitlist for each stratum.\n+   *\n+   * This is only invoked when exact sample size is required.\n+   */\n+  def getCounts[K, V](rdd: RDD[(K, V)],\n+      withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      counts: Option[Map[K, Long]],\n+      seed: Long): MMap[K, Stratum] = {\n+    val combOp = getCombOp[K]\n+    val mappedPartitionRDD = rdd.mapPartitionsWithIndex({ case (partition, iter) =>\n+      val zeroU: MMap[K, Stratum] = new HashMap[K, Stratum]()\n+      val rng = new RandomDataGenerator()\n+      rng.reSeed(seed + partition)\n+      val seqOp = getSeqOp(withReplacement, fractions, rng, counts)\n+      Iterator(iter.aggregate(zeroU)(seqOp, combOp))\n+    }, preservesPartitioning=true)\n+    mappedPartitionRDD.reduce(combOp)\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      rng: RandomDataGenerator,\n+      counts: Option[Map[K, Long]]): (MMap[K, Stratum], (K, V)) => MMap[K, Stratum] = {\n+    val delta = 5e-5\n+    (result: MMap[K, Stratum], item: (K, V)) => {\n+      val key = item._1\n+      val fraction = fractions(key)\n+      if (!result.contains(key)) {\n+        result += (key -> new Stratum())\n+      }\n+      val stratum = result(key)\n+\n+      if (withReplacement) {\n+        // compute acceptBound and waitListBound only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.areBoundsEmpty) {\n+          val n = counts.get(key)\n+          val sampleSize = math.ceil(n * fraction).toLong\n+          val lmbd1 = PoissonBounds.getLowerBound(sampleSize)\n+          val minCount = PoissonBounds.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) {\n+            PoissonBounds.getUpperBound(sampleSize)\n+          } else {\n+            PoissonBounds.getUpperBound(sampleSize - minCount)\n+          }\n+          stratum.acceptBound = lmbd1 / n\n+          stratum.waitListBound = lmbd2 / n\n+        }\n+        val acceptBound = stratum.acceptBound\n+        val copiesAccepted = if (acceptBound == 0.0) 0L else rng.nextPoisson(acceptBound)\n+        if (copiesAccepted > 0) {\n+          stratum.incrNumAccepted(copiesAccepted)\n+        }\n+        val copiesWaitlisted = rng.nextPoisson(stratum.waitListBound).toInt\n+        if (copiesWaitlisted > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(copiesWaitlisted)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement to avoid\n+        // using an extra pass over the RDD for computing the count.\n+        // Hence, acceptBound and waitListBound change on every iteration.\n+        val g1 = - math.log(delta) / stratum.numItems // gamma1\n+        val g2 = (2.0 / 3.0) * g1                     // gamma 2\n+        stratum.acceptBound = math.max(0, fraction + g2 - math.sqrt(g2 * g2 + 3 * g2 * fraction))\n+        stratum.waitListBound = math.min(1, fraction + g1 + math.sqrt(g1 * g1 + 2 * g1 * fraction))\n+\n+        val x = rng.nextUniform(0.0, 1.0)\n+        if (x < stratum.acceptBound) {\n+          stratum.incrNumAccepted()\n+        } else if (x < stratum.waitListBound) {\n+          stratum.addToWaitList(x)\n+        }\n+      }\n+      stratum.incrNumItems()\n+      result\n+    }\n+  }\n+\n+  /**\n+   * Returns the function used combine results returned by seqOp from different partitions.\n+   */\n+  def getCombOp[K]: (MMap[K, Stratum], MMap[K, Stratum]) => MMap[K, Stratum] = {\n+    (r1: MMap[K, Stratum], r2: MMap[K, Stratum]) => {"
  }],
  "prId": 1025
}, {
  "comments": [{
    "author": {
      "login": "falaki"
    },
    "body": "Similarly for q1 and q2, more descriptive names can help.\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-09T00:27:45Z",
    "diffHunk": "@@ -0,0 +1,310 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.Map\n+import scala.collection.mutable.{ArrayBuffer, HashMap, Map => MMap}\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Auxiliary functions and data structures for the sampleByKey method in PairRDDFunctions.\n+ *\n+ * Essentially, when exact sample size is necessary, we make additional passes over the RDD to\n+ * compute the exact threshold value to use for each stratum to guarantee exact sample size with\n+ * high probability. This is achieved by maintaining a waitlist of size O(log(s)), where s is the\n+ * desired sample size for each stratum.\n+ *\n+ * Like in simple random sampling, we generate a random value for each item from the\n+ * uniform  distribution [0.0, 1.0]. All items with values <= min(values of items in the waitlist)\n+ * are accepted into the sample instantly. The threshold for instant accept is designed so that\n+ * s - numAccepted = O(log(s)), where s is again the desired sample size. Thus, by maintaining a\n+ * waitlist size = O(log(s)), we will be able to create a sample of the exact size s by adding\n+ * a portion of the waitlist to the set of items that are instantly accepted. The exact threshold\n+ * is computed by sorting the values in the waitlist and picking the value at (s - numAccepted).\n+ *\n+ * Note that since we use the same seed for the RNG when computing the thresholds and the actual\n+ * sample, our computed thresholds are guaranteed to produce the desired sample size.\n+ *\n+ * For more theoretical background on the sampling techniques used here, please refer to\n+ * http://jmlr.org/proceedings/papers/v28/meng13a.html\n+ */\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * Count the number of items instantly accepted and generate the waitlist for each stratum.\n+   *\n+   * This is only invoked when exact sample size is required.\n+   */\n+  def getCounts[K, V](rdd: RDD[(K, V)],\n+      withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      counts: Option[Map[K, Long]],\n+      seed: Long): MMap[K, Stratum] = {\n+    val combOp = getCombOp[K]\n+    val mappedPartitionRDD = rdd.mapPartitionsWithIndex({ case (partition, iter) =>\n+      val zeroU: MMap[K, Stratum] = new HashMap[K, Stratum]()\n+      val rng = new RandomDataGenerator()\n+      rng.reSeed(seed + partition)\n+      val seqOp = getSeqOp(withReplacement, fractions, rng, counts)\n+      Iterator(iter.aggregate(zeroU)(seqOp, combOp))\n+    }, preservesPartitioning=true)\n+    mappedPartitionRDD.reduce(combOp)\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      rng: RandomDataGenerator,\n+      counts: Option[Map[K, Long]]): (MMap[K, Stratum], (K, V)) => MMap[K, Stratum] = {\n+    val delta = 5e-5\n+    (result: MMap[K, Stratum], item: (K, V)) => {\n+      val key = item._1\n+      val fraction = fractions(key)\n+      if (!result.contains(key)) {\n+        result += (key -> new Stratum())\n+      }\n+      val stratum = result(key)\n+\n+      if (withReplacement) {\n+        // compute acceptBound and waitListBound only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.areBoundsEmpty) {\n+          val n = counts.get(key)\n+          val sampleSize = math.ceil(n * fraction).toLong\n+          val lmbd1 = PoissonBounds.getLowerBound(sampleSize)\n+          val minCount = PoissonBounds.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) {\n+            PoissonBounds.getUpperBound(sampleSize)\n+          } else {\n+            PoissonBounds.getUpperBound(sampleSize - minCount)\n+          }\n+          stratum.acceptBound = lmbd1 / n\n+          stratum.waitListBound = lmbd2 / n\n+        }\n+        val acceptBound = stratum.acceptBound\n+        val copiesAccepted = if (acceptBound == 0.0) 0L else rng.nextPoisson(acceptBound)\n+        if (copiesAccepted > 0) {\n+          stratum.incrNumAccepted(copiesAccepted)\n+        }\n+        val copiesWaitlisted = rng.nextPoisson(stratum.waitListBound).toInt\n+        if (copiesWaitlisted > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(copiesWaitlisted)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement to avoid\n+        // using an extra pass over the RDD for computing the count.\n+        // Hence, acceptBound and waitListBound change on every iteration.\n+        val g1 = - math.log(delta) / stratum.numItems // gamma1\n+        val g2 = (2.0 / 3.0) * g1                     // gamma 2\n+        stratum.acceptBound = math.max(0, fraction + g2 - math.sqrt(g2 * g2 + 3 * g2 * fraction))\n+        stratum.waitListBound = math.min(1, fraction + g1 + math.sqrt(g1 * g1 + 2 * g1 * fraction))\n+\n+        val x = rng.nextUniform(0.0, 1.0)\n+        if (x < stratum.acceptBound) {\n+          stratum.incrNumAccepted()\n+        } else if (x < stratum.waitListBound) {\n+          stratum.addToWaitList(x)\n+        }\n+      }\n+      stratum.incrNumItems()\n+      result\n+    }\n+  }\n+\n+  /**\n+   * Returns the function used combine results returned by seqOp from different partitions.\n+   */\n+  def getCombOp[K]: (MMap[K, Stratum], MMap[K, Stratum]) => MMap[K, Stratum] = {\n+    (r1: MMap[K, Stratum], r2: MMap[K, Stratum]) => {\n+      // take union of both key sets in case one partition doesn't contain all keys\n+      for (key <- r1.keySet.union(r2.keySet)) {\n+        // Use r2 to keep the combined result since r1 is usual empty\n+        val entry1 = r1.get(key)\n+        if (r2.contains(key)) {\n+          r2(key).merge(entry1)\n+        } else {\n+          if (entry1.isDefined) {\n+            r2 += (key -> entry1.get)\n+          }\n+        }\n+      }\n+      r2\n+    }\n+  }\n+\n+  /**\n+   * Given the result returned by getCounts, determine the threshold for accepting items to\n+   * generate exact sample size.\n+   *\n+   * To do so, we compute sampleSize = math.ceil(size * samplingRate) for each stratum and compare\n+   * it to the number of items that were accepted instantly and the number of items in the waitlist\n+   * for that stratum. Most of the time, numAccepted <= sampleSize <= (numAccepted + numWaitlisted),\n+   * which means we need to sort the elements in the waitlist by their associated values in order\n+   * to find the value T s.t. |{elements in the stratum whose associated values <= T}| = sampleSize.\n+   * Note that all elements in the waitlist have values >= bound for instant accept, so a T value\n+   * in the waitlist range would allow all elements that were instantly accepted on the first pass\n+   * to be included in the sample.\n+   */\n+  def computeThresholdByKey[K](finalResult: Map[K, Stratum],\n+      fractions: Map[K, Double]): Map[K, Double] = {\n+    val thresholdByKey = new HashMap[K, Double]()\n+    for ((key, stratum) <- finalResult) {\n+      val sampleSize = math.ceil(stratum.numItems * fractions(key)).toLong\n+      if (stratum.numAccepted > sampleSize) {\n+        logWarning(\"Pre-accepted too many\")\n+        thresholdByKey += (key -> stratum.acceptBound)\n+      } else {\n+        val numWaitListAccepted = (sampleSize - stratum.numAccepted).toInt\n+        if (numWaitListAccepted >= stratum.waitList.size) {\n+          logWarning(\"WaitList too short\")\n+          thresholdByKey += (key -> stratum.waitListBound)\n+        } else {\n+          thresholdByKey += (key -> stratum.waitList.sorted.apply(numWaitListAccepted))\n+        }\n+      }\n+    }\n+    thresholdByKey\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling without replacement.\n+   *\n+   * When exact sample size is required, we make an additional pass over the RDD to determine the\n+   * exact sampling rate that guarantees sample size with high confidence.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getBernoulliSamplingFunction[K, V](rdd: RDD[(K,  V)],\n+      fractions: Map[K, Double],\n+      exact: Boolean,\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    var samplingRateByKey = fractions\n+    if (exact) {\n+      // determine threshold for each stratum and resample\n+      val finalResult = getCounts(rdd, false, fractions, None, seed)\n+      samplingRateByKey = computeThresholdByKey(finalResult, fractions)\n+    }\n+    (idx: Int, iter: Iterator[(K, V)]) => {\n+      val rng = new RandomDataGenerator\n+      rng.reSeed(seed + idx)\n+      iter.filter(t => rng.nextUniform(0.0, 1.0) < samplingRateByKey(t._1))\n+    }\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling with replacement.\n+   *\n+   * When exact sample size is required, we make two additional passed over the RDD to determine\n+   * the exact sampling rate that guarantees sample size with high confidence. The first pass\n+   * counts the number of items in each stratum (group of items with the same key) in the RDD, and\n+   * the second pass uses the counts to determine exact sampling rates.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getPoissonSamplingFunction[K, V](rdd: RDD[(K, V)],\n+      fractions: Map[K, Double],\n+      exact: Boolean,\n+      counts: Option[Map[K, Long]],\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    // TODO implement the streaming version of sampling w/ replacement that doesn't require counts\n+    if (exact) {\n+      val finalResult = getCounts(rdd, true, fractions, counts, seed)\n+      val thresholdByKey = computeThresholdByKey(finalResult, fractions)\n+      (idx: Int, iter: Iterator[(K, V)]) => {\n+        val rng = new RandomDataGenerator()\n+        rng.reSeed(seed + idx)\n+        iter.flatMap { item =>\n+          val key = item._1\n+          val q1 = finalResult(key).acceptBound\n+          val q2 = finalResult(key).waitListBound"
  }],
  "prId": 1025
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Do not import a class under `mutable` if there is one with the same name under `immutable`. Instead, import `scala.collection.mutable` and reference `Map` and `HashMap` as `mutable.Map` and `mutable.HashMap` in the code. Importing `mutable.ArrayBuffer` is fine.\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-09T05:41:04Z",
    "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.Map\n+import scala.collection.mutable.{ArrayBuffer, HashMap, Map => MMap}"
  }, {
    "author": {
      "login": "dorx"
    },
    "body": "There isn't a immutable equivalent of `HashMap`. Also, using `mutable.Map` instead of `MMap` (short for mutable.Map) might push a couple lines over the 100 char limit. \n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-14T21:50:38Z",
    "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.Map\n+import scala.collection.mutable.{ArrayBuffer, HashMap, Map => MMap}"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "`immutable.HashMap`: http://www.scala-lang.org/api/2.11.1/index.html#scala.collection.immutable.HashMap\n\n`MMap` may be confusing because of `MultiMap`: http://www.scala-lang.org/api/2.11.1/index.html#scala.collection.immutable.MultiMap\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-14T22:05:01Z",
    "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.Map\n+import scala.collection.mutable.{ArrayBuffer, HashMap, Map => MMap}"
  }],
  "prId": 1025
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Separate 3rd party imports from spark imports.\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-09T05:41:06Z",
    "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.Map\n+import scala.collection.mutable.{ArrayBuffer, HashMap, Map => MMap}\n+\n+import org.apache.commons.math3.random.RandomDataGenerator"
  }],
  "prId": 1025
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Should be `O(sqrt(s))` for sampling without replacement.\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-09T05:41:14Z",
    "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.Map\n+import scala.collection.mutable.{ArrayBuffer, HashMap, Map => MMap}\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Auxiliary functions and data structures for the sampleByKey method in PairRDDFunctions.\n+ *\n+ * Essentially, when exact sample size is necessary, we make additional passes over the RDD to\n+ * compute the exact threshold value to use for each stratum to guarantee exact sample size with\n+ * high probability. This is achieved by maintaining a waitlist of size O(log(s)), where s is the"
  }],
  "prId": 1025
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "As mentioned in the doc, this is not a sampler but something like `StratifiedSamplingUtils`. I think it is okay to merge the functions into `SamplingUtils`.\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-09T05:41:16Z",
    "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.Map\n+import scala.collection.mutable.{ArrayBuffer, HashMap, Map => MMap}\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Auxiliary functions and data structures for the sampleByKey method in PairRDDFunctions.\n+ *\n+ * Essentially, when exact sample size is necessary, we make additional passes over the RDD to\n+ * compute the exact threshold value to use for each stratum to guarantee exact sample size with\n+ * high probability. This is achieved by maintaining a waitlist of size O(log(s)), where s is the\n+ * desired sample size for each stratum.\n+ *\n+ * Like in simple random sampling, we generate a random value for each item from the\n+ * uniform  distribution [0.0, 1.0]. All items with values <= min(values of items in the waitlist)\n+ * are accepted into the sample instantly. The threshold for instant accept is designed so that\n+ * s - numAccepted = O(log(s)), where s is again the desired sample size. Thus, by maintaining a\n+ * waitlist size = O(log(s)), we will be able to create a sample of the exact size s by adding\n+ * a portion of the waitlist to the set of items that are instantly accepted. The exact threshold\n+ * is computed by sorting the values in the waitlist and picking the value at (s - numAccepted).\n+ *\n+ * Note that since we use the same seed for the RNG when computing the thresholds and the actual\n+ * sample, our computed thresholds are guaranteed to produce the desired sample size.\n+ *\n+ * For more theoretical background on the sampling techniques used here, please refer to\n+ * http://jmlr.org/proceedings/papers/v28/meng13a.html\n+ */\n+\n+private[spark] object StratifiedSampler extends Logging {"
  }, {
    "author": {
      "login": "dorx"
    },
    "body": "I'd prefer renaming the class to `StratifiedSamplingUtils` instead of adding too many things into `SamplingUtils`\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-14T21:52:05Z",
    "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.Map\n+import scala.collection.mutable.{ArrayBuffer, HashMap, Map => MMap}\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Auxiliary functions and data structures for the sampleByKey method in PairRDDFunctions.\n+ *\n+ * Essentially, when exact sample size is necessary, we make additional passes over the RDD to\n+ * compute the exact threshold value to use for each stratum to guarantee exact sample size with\n+ * high probability. This is achieved by maintaining a waitlist of size O(log(s)), where s is the\n+ * desired sample size for each stratum.\n+ *\n+ * Like in simple random sampling, we generate a random value for each item from the\n+ * uniform  distribution [0.0, 1.0]. All items with values <= min(values of items in the waitlist)\n+ * are accepted into the sample instantly. The threshold for instant accept is designed so that\n+ * s - numAccepted = O(log(s)), where s is again the desired sample size. Thus, by maintaining a\n+ * waitlist size = O(log(s)), we will be able to create a sample of the exact size s by adding\n+ * a portion of the waitlist to the set of items that are instantly accepted. The exact threshold\n+ * is computed by sorting the values in the waitlist and picking the value at (s - numAccepted).\n+ *\n+ * Note that since we use the same seed for the RNG when computing the thresholds and the actual\n+ * sample, our computed thresholds are guaranteed to produce the desired sample size.\n+ *\n+ * For more theoretical background on the sampling techniques used here, please refer to\n+ * http://jmlr.org/proceedings/papers/v28/meng13a.html\n+ */\n+\n+private[spark] object StratifiedSampler extends Logging {"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "Sounds good.\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-14T22:10:21Z",
    "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.Map\n+import scala.collection.mutable.{ArrayBuffer, HashMap, Map => MMap}\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Auxiliary functions and data structures for the sampleByKey method in PairRDDFunctions.\n+ *\n+ * Essentially, when exact sample size is necessary, we make additional passes over the RDD to\n+ * compute the exact threshold value to use for each stratum to guarantee exact sample size with\n+ * high probability. This is achieved by maintaining a waitlist of size O(log(s)), where s is the\n+ * desired sample size for each stratum.\n+ *\n+ * Like in simple random sampling, we generate a random value for each item from the\n+ * uniform  distribution [0.0, 1.0]. All items with values <= min(values of items in the waitlist)\n+ * are accepted into the sample instantly. The threshold for instant accept is designed so that\n+ * s - numAccepted = O(log(s)), where s is again the desired sample size. Thus, by maintaining a\n+ * waitlist size = O(log(s)), we will be able to create a sample of the exact size s by adding\n+ * a portion of the waitlist to the set of items that are instantly accepted. The exact threshold\n+ * is computed by sorting the values in the waitlist and picking the value at (s - numAccepted).\n+ *\n+ * Note that since we use the same seed for the RNG when computing the thresholds and the actual\n+ * sample, our computed thresholds are guaranteed to produce the desired sample size.\n+ *\n+ * For more theoretical background on the sampling techniques used here, please refer to\n+ * http://jmlr.org/proceedings/papers/v28/meng13a.html\n+ */\n+\n+private[spark] object StratifiedSampler extends Logging {"
  }],
  "prId": 1025
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "The name is a little confusing, especially there is an argument named `counts`. See the comments below on the name of `Stratum`. If we change `Stratum` to `AcceptanceResult`, this could be changed to `getAcceptanceResults`.\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-09T05:41:33Z",
    "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.Map\n+import scala.collection.mutable.{ArrayBuffer, HashMap, Map => MMap}\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Auxiliary functions and data structures for the sampleByKey method in PairRDDFunctions.\n+ *\n+ * Essentially, when exact sample size is necessary, we make additional passes over the RDD to\n+ * compute the exact threshold value to use for each stratum to guarantee exact sample size with\n+ * high probability. This is achieved by maintaining a waitlist of size O(log(s)), where s is the\n+ * desired sample size for each stratum.\n+ *\n+ * Like in simple random sampling, we generate a random value for each item from the\n+ * uniform  distribution [0.0, 1.0]. All items with values <= min(values of items in the waitlist)\n+ * are accepted into the sample instantly. The threshold for instant accept is designed so that\n+ * s - numAccepted = O(log(s)), where s is again the desired sample size. Thus, by maintaining a\n+ * waitlist size = O(log(s)), we will be able to create a sample of the exact size s by adding\n+ * a portion of the waitlist to the set of items that are instantly accepted. The exact threshold\n+ * is computed by sorting the values in the waitlist and picking the value at (s - numAccepted).\n+ *\n+ * Note that since we use the same seed for the RNG when computing the thresholds and the actual\n+ * sample, our computed thresholds are guaranteed to produce the desired sample size.\n+ *\n+ * For more theoretical background on the sampling techniques used here, please refer to\n+ * http://jmlr.org/proceedings/papers/v28/meng13a.html\n+ */\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * Count the number of items instantly accepted and generate the waitlist for each stratum.\n+   *\n+   * This is only invoked when exact sample size is required.\n+   */\n+  def getCounts[K, V](rdd: RDD[(K, V)],"
  }],
  "prId": 1025
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "We already have the exact count for the stratum. Is it necessary?\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-09T05:41:36Z",
    "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.Map\n+import scala.collection.mutable.{ArrayBuffer, HashMap, Map => MMap}\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Auxiliary functions and data structures for the sampleByKey method in PairRDDFunctions.\n+ *\n+ * Essentially, when exact sample size is necessary, we make additional passes over the RDD to\n+ * compute the exact threshold value to use for each stratum to guarantee exact sample size with\n+ * high probability. This is achieved by maintaining a waitlist of size O(log(s)), where s is the\n+ * desired sample size for each stratum.\n+ *\n+ * Like in simple random sampling, we generate a random value for each item from the\n+ * uniform  distribution [0.0, 1.0]. All items with values <= min(values of items in the waitlist)\n+ * are accepted into the sample instantly. The threshold for instant accept is designed so that\n+ * s - numAccepted = O(log(s)), where s is again the desired sample size. Thus, by maintaining a\n+ * waitlist size = O(log(s)), we will be able to create a sample of the exact size s by adding\n+ * a portion of the waitlist to the set of items that are instantly accepted. The exact threshold\n+ * is computed by sorting the values in the waitlist and picking the value at (s - numAccepted).\n+ *\n+ * Note that since we use the same seed for the RNG when computing the thresholds and the actual\n+ * sample, our computed thresholds are guaranteed to produce the desired sample size.\n+ *\n+ * For more theoretical background on the sampling techniques used here, please refer to\n+ * http://jmlr.org/proceedings/papers/v28/meng13a.html\n+ */\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * Count the number of items instantly accepted and generate the waitlist for each stratum.\n+   *\n+   * This is only invoked when exact sample size is required.\n+   */\n+  def getCounts[K, V](rdd: RDD[(K, V)],\n+      withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      counts: Option[Map[K, Long]],\n+      seed: Long): MMap[K, Stratum] = {\n+    val combOp = getCombOp[K]\n+    val mappedPartitionRDD = rdd.mapPartitionsWithIndex({ case (partition, iter) =>\n+      val zeroU: MMap[K, Stratum] = new HashMap[K, Stratum]()\n+      val rng = new RandomDataGenerator()\n+      rng.reSeed(seed + partition)\n+      val seqOp = getSeqOp(withReplacement, fractions, rng, counts)\n+      Iterator(iter.aggregate(zeroU)(seqOp, combOp))\n+    }, preservesPartitioning=true)\n+    mappedPartitionRDD.reduce(combOp)\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      rng: RandomDataGenerator,\n+      counts: Option[Map[K, Long]]): (MMap[K, Stratum], (K, V)) => MMap[K, Stratum] = {\n+    val delta = 5e-5\n+    (result: MMap[K, Stratum], item: (K, V)) => {\n+      val key = item._1\n+      val fraction = fractions(key)\n+      if (!result.contains(key)) {\n+        result += (key -> new Stratum())\n+      }\n+      val stratum = result(key)\n+\n+      if (withReplacement) {\n+        // compute acceptBound and waitListBound only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.areBoundsEmpty) {\n+          val n = counts.get(key)\n+          val sampleSize = math.ceil(n * fraction).toLong\n+          val lmbd1 = PoissonBounds.getLowerBound(sampleSize)\n+          val minCount = PoissonBounds.getMinCount(lmbd1)"
  }, {
    "author": {
      "login": "dorx"
    },
    "body": "I'm not clear what \"it\" refers to.\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-14T21:53:34Z",
    "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.Map\n+import scala.collection.mutable.{ArrayBuffer, HashMap, Map => MMap}\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Auxiliary functions and data structures for the sampleByKey method in PairRDDFunctions.\n+ *\n+ * Essentially, when exact sample size is necessary, we make additional passes over the RDD to\n+ * compute the exact threshold value to use for each stratum to guarantee exact sample size with\n+ * high probability. This is achieved by maintaining a waitlist of size O(log(s)), where s is the\n+ * desired sample size for each stratum.\n+ *\n+ * Like in simple random sampling, we generate a random value for each item from the\n+ * uniform  distribution [0.0, 1.0]. All items with values <= min(values of items in the waitlist)\n+ * are accepted into the sample instantly. The threshold for instant accept is designed so that\n+ * s - numAccepted = O(log(s)), where s is again the desired sample size. Thus, by maintaining a\n+ * waitlist size = O(log(s)), we will be able to create a sample of the exact size s by adding\n+ * a portion of the waitlist to the set of items that are instantly accepted. The exact threshold\n+ * is computed by sorting the values in the waitlist and picking the value at (s - numAccepted).\n+ *\n+ * Note that since we use the same seed for the RNG when computing the thresholds and the actual\n+ * sample, our computed thresholds are guaranteed to produce the desired sample size.\n+ *\n+ * For more theoretical background on the sampling techniques used here, please refer to\n+ * http://jmlr.org/proceedings/papers/v28/meng13a.html\n+ */\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * Count the number of items instantly accepted and generate the waitlist for each stratum.\n+   *\n+   * This is only invoked when exact sample size is required.\n+   */\n+  def getCounts[K, V](rdd: RDD[(K, V)],\n+      withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      counts: Option[Map[K, Long]],\n+      seed: Long): MMap[K, Stratum] = {\n+    val combOp = getCombOp[K]\n+    val mappedPartitionRDD = rdd.mapPartitionsWithIndex({ case (partition, iter) =>\n+      val zeroU: MMap[K, Stratum] = new HashMap[K, Stratum]()\n+      val rng = new RandomDataGenerator()\n+      rng.reSeed(seed + partition)\n+      val seqOp = getSeqOp(withReplacement, fractions, rng, counts)\n+      Iterator(iter.aggregate(zeroU)(seqOp, combOp))\n+    }, preservesPartitioning=true)\n+    mappedPartitionRDD.reduce(combOp)\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      rng: RandomDataGenerator,\n+      counts: Option[Map[K, Long]]): (MMap[K, Stratum], (K, V)) => MMap[K, Stratum] = {\n+    val delta = 5e-5\n+    (result: MMap[K, Stratum], item: (K, V)) => {\n+      val key = item._1\n+      val fraction = fractions(key)\n+      if (!result.contains(key)) {\n+        result += (key -> new Stratum())\n+      }\n+      val stratum = result(key)\n+\n+      if (withReplacement) {\n+        // compute acceptBound and waitListBound only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.areBoundsEmpty) {\n+          val n = counts.get(key)\n+          val sampleSize = math.ceil(n * fraction).toLong\n+          val lmbd1 = PoissonBounds.getLowerBound(sampleSize)\n+          val minCount = PoissonBounds.getMinCount(lmbd1)"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "Having `minCount` here. I think it is only useful in streaming environments but we didn't implement it in that way.\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-14T22:11:27Z",
    "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.Map\n+import scala.collection.mutable.{ArrayBuffer, HashMap, Map => MMap}\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Auxiliary functions and data structures for the sampleByKey method in PairRDDFunctions.\n+ *\n+ * Essentially, when exact sample size is necessary, we make additional passes over the RDD to\n+ * compute the exact threshold value to use for each stratum to guarantee exact sample size with\n+ * high probability. This is achieved by maintaining a waitlist of size O(log(s)), where s is the\n+ * desired sample size for each stratum.\n+ *\n+ * Like in simple random sampling, we generate a random value for each item from the\n+ * uniform  distribution [0.0, 1.0]. All items with values <= min(values of items in the waitlist)\n+ * are accepted into the sample instantly. The threshold for instant accept is designed so that\n+ * s - numAccepted = O(log(s)), where s is again the desired sample size. Thus, by maintaining a\n+ * waitlist size = O(log(s)), we will be able to create a sample of the exact size s by adding\n+ * a portion of the waitlist to the set of items that are instantly accepted. The exact threshold\n+ * is computed by sorting the values in the waitlist and picking the value at (s - numAccepted).\n+ *\n+ * Note that since we use the same seed for the RNG when computing the thresholds and the actual\n+ * sample, our computed thresholds are guaranteed to produce the desired sample size.\n+ *\n+ * For more theoretical background on the sampling techniques used here, please refer to\n+ * http://jmlr.org/proceedings/papers/v28/meng13a.html\n+ */\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * Count the number of items instantly accepted and generate the waitlist for each stratum.\n+   *\n+   * This is only invoked when exact sample size is required.\n+   */\n+  def getCounts[K, V](rdd: RDD[(K, V)],\n+      withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      counts: Option[Map[K, Long]],\n+      seed: Long): MMap[K, Stratum] = {\n+    val combOp = getCombOp[K]\n+    val mappedPartitionRDD = rdd.mapPartitionsWithIndex({ case (partition, iter) =>\n+      val zeroU: MMap[K, Stratum] = new HashMap[K, Stratum]()\n+      val rng = new RandomDataGenerator()\n+      rng.reSeed(seed + partition)\n+      val seqOp = getSeqOp(withReplacement, fractions, rng, counts)\n+      Iterator(iter.aggregate(zeroU)(seqOp, combOp))\n+    }, preservesPartitioning=true)\n+    mappedPartitionRDD.reduce(combOp)\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      rng: RandomDataGenerator,\n+      counts: Option[Map[K, Long]]): (MMap[K, Stratum], (K, V)) => MMap[K, Stratum] = {\n+    val delta = 5e-5\n+    (result: MMap[K, Stratum], item: (K, V)) => {\n+      val key = item._1\n+      val fraction = fractions(key)\n+      if (!result.contains(key)) {\n+        result += (key -> new Stratum())\n+      }\n+      val stratum = result(key)\n+\n+      if (withReplacement) {\n+        // compute acceptBound and waitListBound only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.areBoundsEmpty) {\n+          val n = counts.get(key)\n+          val sampleSize = math.ceil(n * fraction).toLong\n+          val lmbd1 = PoissonBounds.getLowerBound(sampleSize)\n+          val minCount = PoissonBounds.getMinCount(lmbd1)"
  }],
  "prId": 1025
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Should be `(lmbd2 - lmbd1) / n`. Otherwise, we waitlist too many. Please check the waitlist size for some large n.\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-09T05:41:37Z",
    "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.Map\n+import scala.collection.mutable.{ArrayBuffer, HashMap, Map => MMap}\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Auxiliary functions and data structures for the sampleByKey method in PairRDDFunctions.\n+ *\n+ * Essentially, when exact sample size is necessary, we make additional passes over the RDD to\n+ * compute the exact threshold value to use for each stratum to guarantee exact sample size with\n+ * high probability. This is achieved by maintaining a waitlist of size O(log(s)), where s is the\n+ * desired sample size for each stratum.\n+ *\n+ * Like in simple random sampling, we generate a random value for each item from the\n+ * uniform  distribution [0.0, 1.0]. All items with values <= min(values of items in the waitlist)\n+ * are accepted into the sample instantly. The threshold for instant accept is designed so that\n+ * s - numAccepted = O(log(s)), where s is again the desired sample size. Thus, by maintaining a\n+ * waitlist size = O(log(s)), we will be able to create a sample of the exact size s by adding\n+ * a portion of the waitlist to the set of items that are instantly accepted. The exact threshold\n+ * is computed by sorting the values in the waitlist and picking the value at (s - numAccepted).\n+ *\n+ * Note that since we use the same seed for the RNG when computing the thresholds and the actual\n+ * sample, our computed thresholds are guaranteed to produce the desired sample size.\n+ *\n+ * For more theoretical background on the sampling techniques used here, please refer to\n+ * http://jmlr.org/proceedings/papers/v28/meng13a.html\n+ */\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * Count the number of items instantly accepted and generate the waitlist for each stratum.\n+   *\n+   * This is only invoked when exact sample size is required.\n+   */\n+  def getCounts[K, V](rdd: RDD[(K, V)],\n+      withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      counts: Option[Map[K, Long]],\n+      seed: Long): MMap[K, Stratum] = {\n+    val combOp = getCombOp[K]\n+    val mappedPartitionRDD = rdd.mapPartitionsWithIndex({ case (partition, iter) =>\n+      val zeroU: MMap[K, Stratum] = new HashMap[K, Stratum]()\n+      val rng = new RandomDataGenerator()\n+      rng.reSeed(seed + partition)\n+      val seqOp = getSeqOp(withReplacement, fractions, rng, counts)\n+      Iterator(iter.aggregate(zeroU)(seqOp, combOp))\n+    }, preservesPartitioning=true)\n+    mappedPartitionRDD.reduce(combOp)\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      rng: RandomDataGenerator,\n+      counts: Option[Map[K, Long]]): (MMap[K, Stratum], (K, V)) => MMap[K, Stratum] = {\n+    val delta = 5e-5\n+    (result: MMap[K, Stratum], item: (K, V)) => {\n+      val key = item._1\n+      val fraction = fractions(key)\n+      if (!result.contains(key)) {\n+        result += (key -> new Stratum())\n+      }\n+      val stratum = result(key)\n+\n+      if (withReplacement) {\n+        // compute acceptBound and waitListBound only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.areBoundsEmpty) {\n+          val n = counts.get(key)\n+          val sampleSize = math.ceil(n * fraction).toLong\n+          val lmbd1 = PoissonBounds.getLowerBound(sampleSize)\n+          val minCount = PoissonBounds.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) {\n+            PoissonBounds.getUpperBound(sampleSize)\n+          } else {\n+            PoissonBounds.getUpperBound(sampleSize - minCount)\n+          }\n+          stratum.acceptBound = lmbd1 / n\n+          stratum.waitListBound = lmbd2 / n"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "Sorry, this bug was introduced by me.\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-09T05:48:00Z",
    "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.Map\n+import scala.collection.mutable.{ArrayBuffer, HashMap, Map => MMap}\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Auxiliary functions and data structures for the sampleByKey method in PairRDDFunctions.\n+ *\n+ * Essentially, when exact sample size is necessary, we make additional passes over the RDD to\n+ * compute the exact threshold value to use for each stratum to guarantee exact sample size with\n+ * high probability. This is achieved by maintaining a waitlist of size O(log(s)), where s is the\n+ * desired sample size for each stratum.\n+ *\n+ * Like in simple random sampling, we generate a random value for each item from the\n+ * uniform  distribution [0.0, 1.0]. All items with values <= min(values of items in the waitlist)\n+ * are accepted into the sample instantly. The threshold for instant accept is designed so that\n+ * s - numAccepted = O(log(s)), where s is again the desired sample size. Thus, by maintaining a\n+ * waitlist size = O(log(s)), we will be able to create a sample of the exact size s by adding\n+ * a portion of the waitlist to the set of items that are instantly accepted. The exact threshold\n+ * is computed by sorting the values in the waitlist and picking the value at (s - numAccepted).\n+ *\n+ * Note that since we use the same seed for the RNG when computing the thresholds and the actual\n+ * sample, our computed thresholds are guaranteed to produce the desired sample size.\n+ *\n+ * For more theoretical background on the sampling techniques used here, please refer to\n+ * http://jmlr.org/proceedings/papers/v28/meng13a.html\n+ */\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * Count the number of items instantly accepted and generate the waitlist for each stratum.\n+   *\n+   * This is only invoked when exact sample size is required.\n+   */\n+  def getCounts[K, V](rdd: RDD[(K, V)],\n+      withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      counts: Option[Map[K, Long]],\n+      seed: Long): MMap[K, Stratum] = {\n+    val combOp = getCombOp[K]\n+    val mappedPartitionRDD = rdd.mapPartitionsWithIndex({ case (partition, iter) =>\n+      val zeroU: MMap[K, Stratum] = new HashMap[K, Stratum]()\n+      val rng = new RandomDataGenerator()\n+      rng.reSeed(seed + partition)\n+      val seqOp = getSeqOp(withReplacement, fractions, rng, counts)\n+      Iterator(iter.aggregate(zeroU)(seqOp, combOp))\n+    }, preservesPartitioning=true)\n+    mappedPartitionRDD.reduce(combOp)\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      rng: RandomDataGenerator,\n+      counts: Option[Map[K, Long]]): (MMap[K, Stratum], (K, V)) => MMap[K, Stratum] = {\n+    val delta = 5e-5\n+    (result: MMap[K, Stratum], item: (K, V)) => {\n+      val key = item._1\n+      val fraction = fractions(key)\n+      if (!result.contains(key)) {\n+        result += (key -> new Stratum())\n+      }\n+      val stratum = result(key)\n+\n+      if (withReplacement) {\n+        // compute acceptBound and waitListBound only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.areBoundsEmpty) {\n+          val n = counts.get(key)\n+          val sampleSize = math.ceil(n * fraction).toLong\n+          val lmbd1 = PoissonBounds.getLowerBound(sampleSize)\n+          val minCount = PoissonBounds.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) {\n+            PoissonBounds.getUpperBound(sampleSize)\n+          } else {\n+            PoissonBounds.getUpperBound(sampleSize - minCount)\n+          }\n+          stratum.acceptBound = lmbd1 / n\n+          stratum.waitListBound = lmbd2 / n"
  }],
  "prId": 1025
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Those bounds are also used in `takeSample`. Should we have BernoulliBounds in SamplingUtils, similar to PoissonBounds?\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-09T05:41:39Z",
    "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.Map\n+import scala.collection.mutable.{ArrayBuffer, HashMap, Map => MMap}\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Auxiliary functions and data structures for the sampleByKey method in PairRDDFunctions.\n+ *\n+ * Essentially, when exact sample size is necessary, we make additional passes over the RDD to\n+ * compute the exact threshold value to use for each stratum to guarantee exact sample size with\n+ * high probability. This is achieved by maintaining a waitlist of size O(log(s)), where s is the\n+ * desired sample size for each stratum.\n+ *\n+ * Like in simple random sampling, we generate a random value for each item from the\n+ * uniform  distribution [0.0, 1.0]. All items with values <= min(values of items in the waitlist)\n+ * are accepted into the sample instantly. The threshold for instant accept is designed so that\n+ * s - numAccepted = O(log(s)), where s is again the desired sample size. Thus, by maintaining a\n+ * waitlist size = O(log(s)), we will be able to create a sample of the exact size s by adding\n+ * a portion of the waitlist to the set of items that are instantly accepted. The exact threshold\n+ * is computed by sorting the values in the waitlist and picking the value at (s - numAccepted).\n+ *\n+ * Note that since we use the same seed for the RNG when computing the thresholds and the actual\n+ * sample, our computed thresholds are guaranteed to produce the desired sample size.\n+ *\n+ * For more theoretical background on the sampling techniques used here, please refer to\n+ * http://jmlr.org/proceedings/papers/v28/meng13a.html\n+ */\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * Count the number of items instantly accepted and generate the waitlist for each stratum.\n+   *\n+   * This is only invoked when exact sample size is required.\n+   */\n+  def getCounts[K, V](rdd: RDD[(K, V)],\n+      withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      counts: Option[Map[K, Long]],\n+      seed: Long): MMap[K, Stratum] = {\n+    val combOp = getCombOp[K]\n+    val mappedPartitionRDD = rdd.mapPartitionsWithIndex({ case (partition, iter) =>\n+      val zeroU: MMap[K, Stratum] = new HashMap[K, Stratum]()\n+      val rng = new RandomDataGenerator()\n+      rng.reSeed(seed + partition)\n+      val seqOp = getSeqOp(withReplacement, fractions, rng, counts)\n+      Iterator(iter.aggregate(zeroU)(seqOp, combOp))\n+    }, preservesPartitioning=true)\n+    mappedPartitionRDD.reduce(combOp)\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      rng: RandomDataGenerator,\n+      counts: Option[Map[K, Long]]): (MMap[K, Stratum], (K, V)) => MMap[K, Stratum] = {\n+    val delta = 5e-5\n+    (result: MMap[K, Stratum], item: (K, V)) => {\n+      val key = item._1\n+      val fraction = fractions(key)\n+      if (!result.contains(key)) {\n+        result += (key -> new Stratum())\n+      }\n+      val stratum = result(key)\n+\n+      if (withReplacement) {\n+        // compute acceptBound and waitListBound only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.areBoundsEmpty) {\n+          val n = counts.get(key)\n+          val sampleSize = math.ceil(n * fraction).toLong\n+          val lmbd1 = PoissonBounds.getLowerBound(sampleSize)\n+          val minCount = PoissonBounds.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) {\n+            PoissonBounds.getUpperBound(sampleSize)\n+          } else {\n+            PoissonBounds.getUpperBound(sampleSize - minCount)\n+          }\n+          stratum.acceptBound = lmbd1 / n\n+          stratum.waitListBound = lmbd2 / n\n+        }\n+        val acceptBound = stratum.acceptBound\n+        val copiesAccepted = if (acceptBound == 0.0) 0L else rng.nextPoisson(acceptBound)\n+        if (copiesAccepted > 0) {\n+          stratum.incrNumAccepted(copiesAccepted)\n+        }\n+        val copiesWaitlisted = rng.nextPoisson(stratum.waitListBound).toInt\n+        if (copiesWaitlisted > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(copiesWaitlisted)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement to avoid\n+        // using an extra pass over the RDD for computing the count.\n+        // Hence, acceptBound and waitListBound change on every iteration.\n+        val gamma1 = - math.log(delta) / stratum.numItems\n+        val gamma2 = (2.0 / 3.0) * gamma1\n+        stratum.acceptBound = math.max(0,"
  }, {
    "author": {
      "login": "dorx"
    },
    "body": "Sure. Always nice to avoid duplicate logic in different spots.\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-14T21:55:40Z",
    "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.Map\n+import scala.collection.mutable.{ArrayBuffer, HashMap, Map => MMap}\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Auxiliary functions and data structures for the sampleByKey method in PairRDDFunctions.\n+ *\n+ * Essentially, when exact sample size is necessary, we make additional passes over the RDD to\n+ * compute the exact threshold value to use for each stratum to guarantee exact sample size with\n+ * high probability. This is achieved by maintaining a waitlist of size O(log(s)), where s is the\n+ * desired sample size for each stratum.\n+ *\n+ * Like in simple random sampling, we generate a random value for each item from the\n+ * uniform  distribution [0.0, 1.0]. All items with values <= min(values of items in the waitlist)\n+ * are accepted into the sample instantly. The threshold for instant accept is designed so that\n+ * s - numAccepted = O(log(s)), where s is again the desired sample size. Thus, by maintaining a\n+ * waitlist size = O(log(s)), we will be able to create a sample of the exact size s by adding\n+ * a portion of the waitlist to the set of items that are instantly accepted. The exact threshold\n+ * is computed by sorting the values in the waitlist and picking the value at (s - numAccepted).\n+ *\n+ * Note that since we use the same seed for the RNG when computing the thresholds and the actual\n+ * sample, our computed thresholds are guaranteed to produce the desired sample size.\n+ *\n+ * For more theoretical background on the sampling techniques used here, please refer to\n+ * http://jmlr.org/proceedings/papers/v28/meng13a.html\n+ */\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * Count the number of items instantly accepted and generate the waitlist for each stratum.\n+   *\n+   * This is only invoked when exact sample size is required.\n+   */\n+  def getCounts[K, V](rdd: RDD[(K, V)],\n+      withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      counts: Option[Map[K, Long]],\n+      seed: Long): MMap[K, Stratum] = {\n+    val combOp = getCombOp[K]\n+    val mappedPartitionRDD = rdd.mapPartitionsWithIndex({ case (partition, iter) =>\n+      val zeroU: MMap[K, Stratum] = new HashMap[K, Stratum]()\n+      val rng = new RandomDataGenerator()\n+      rng.reSeed(seed + partition)\n+      val seqOp = getSeqOp(withReplacement, fractions, rng, counts)\n+      Iterator(iter.aggregate(zeroU)(seqOp, combOp))\n+    }, preservesPartitioning=true)\n+    mappedPartitionRDD.reduce(combOp)\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      rng: RandomDataGenerator,\n+      counts: Option[Map[K, Long]]): (MMap[K, Stratum], (K, V)) => MMap[K, Stratum] = {\n+    val delta = 5e-5\n+    (result: MMap[K, Stratum], item: (K, V)) => {\n+      val key = item._1\n+      val fraction = fractions(key)\n+      if (!result.contains(key)) {\n+        result += (key -> new Stratum())\n+      }\n+      val stratum = result(key)\n+\n+      if (withReplacement) {\n+        // compute acceptBound and waitListBound only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.areBoundsEmpty) {\n+          val n = counts.get(key)\n+          val sampleSize = math.ceil(n * fraction).toLong\n+          val lmbd1 = PoissonBounds.getLowerBound(sampleSize)\n+          val minCount = PoissonBounds.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) {\n+            PoissonBounds.getUpperBound(sampleSize)\n+          } else {\n+            PoissonBounds.getUpperBound(sampleSize - minCount)\n+          }\n+          stratum.acceptBound = lmbd1 / n\n+          stratum.waitListBound = lmbd2 / n\n+        }\n+        val acceptBound = stratum.acceptBound\n+        val copiesAccepted = if (acceptBound == 0.0) 0L else rng.nextPoisson(acceptBound)\n+        if (copiesAccepted > 0) {\n+          stratum.incrNumAccepted(copiesAccepted)\n+        }\n+        val copiesWaitlisted = rng.nextPoisson(stratum.waitListBound).toInt\n+        if (copiesWaitlisted > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(copiesWaitlisted)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement to avoid\n+        // using an extra pass over the RDD for computing the count.\n+        // Hence, acceptBound and waitListBound change on every iteration.\n+        val gamma1 = - math.log(delta) / stratum.numItems\n+        val gamma2 = (2.0 / 3.0) * gamma1\n+        stratum.acceptBound = math.max(0,"
  }],
  "prId": 1025
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "foreach is more efficient than for:\n\n`result1.keySet.union(result2.keySet).foreach { key =>`\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-09T05:41:41Z",
    "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.Map\n+import scala.collection.mutable.{ArrayBuffer, HashMap, Map => MMap}\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Auxiliary functions and data structures for the sampleByKey method in PairRDDFunctions.\n+ *\n+ * Essentially, when exact sample size is necessary, we make additional passes over the RDD to\n+ * compute the exact threshold value to use for each stratum to guarantee exact sample size with\n+ * high probability. This is achieved by maintaining a waitlist of size O(log(s)), where s is the\n+ * desired sample size for each stratum.\n+ *\n+ * Like in simple random sampling, we generate a random value for each item from the\n+ * uniform  distribution [0.0, 1.0]. All items with values <= min(values of items in the waitlist)\n+ * are accepted into the sample instantly. The threshold for instant accept is designed so that\n+ * s - numAccepted = O(log(s)), where s is again the desired sample size. Thus, by maintaining a\n+ * waitlist size = O(log(s)), we will be able to create a sample of the exact size s by adding\n+ * a portion of the waitlist to the set of items that are instantly accepted. The exact threshold\n+ * is computed by sorting the values in the waitlist and picking the value at (s - numAccepted).\n+ *\n+ * Note that since we use the same seed for the RNG when computing the thresholds and the actual\n+ * sample, our computed thresholds are guaranteed to produce the desired sample size.\n+ *\n+ * For more theoretical background on the sampling techniques used here, please refer to\n+ * http://jmlr.org/proceedings/papers/v28/meng13a.html\n+ */\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * Count the number of items instantly accepted and generate the waitlist for each stratum.\n+   *\n+   * This is only invoked when exact sample size is required.\n+   */\n+  def getCounts[K, V](rdd: RDD[(K, V)],\n+      withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      counts: Option[Map[K, Long]],\n+      seed: Long): MMap[K, Stratum] = {\n+    val combOp = getCombOp[K]\n+    val mappedPartitionRDD = rdd.mapPartitionsWithIndex({ case (partition, iter) =>\n+      val zeroU: MMap[K, Stratum] = new HashMap[K, Stratum]()\n+      val rng = new RandomDataGenerator()\n+      rng.reSeed(seed + partition)\n+      val seqOp = getSeqOp(withReplacement, fractions, rng, counts)\n+      Iterator(iter.aggregate(zeroU)(seqOp, combOp))\n+    }, preservesPartitioning=true)\n+    mappedPartitionRDD.reduce(combOp)\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      rng: RandomDataGenerator,\n+      counts: Option[Map[K, Long]]): (MMap[K, Stratum], (K, V)) => MMap[K, Stratum] = {\n+    val delta = 5e-5\n+    (result: MMap[K, Stratum], item: (K, V)) => {\n+      val key = item._1\n+      val fraction = fractions(key)\n+      if (!result.contains(key)) {\n+        result += (key -> new Stratum())\n+      }\n+      val stratum = result(key)\n+\n+      if (withReplacement) {\n+        // compute acceptBound and waitListBound only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.areBoundsEmpty) {\n+          val n = counts.get(key)\n+          val sampleSize = math.ceil(n * fraction).toLong\n+          val lmbd1 = PoissonBounds.getLowerBound(sampleSize)\n+          val minCount = PoissonBounds.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) {\n+            PoissonBounds.getUpperBound(sampleSize)\n+          } else {\n+            PoissonBounds.getUpperBound(sampleSize - minCount)\n+          }\n+          stratum.acceptBound = lmbd1 / n\n+          stratum.waitListBound = lmbd2 / n\n+        }\n+        val acceptBound = stratum.acceptBound\n+        val copiesAccepted = if (acceptBound == 0.0) 0L else rng.nextPoisson(acceptBound)\n+        if (copiesAccepted > 0) {\n+          stratum.incrNumAccepted(copiesAccepted)\n+        }\n+        val copiesWaitlisted = rng.nextPoisson(stratum.waitListBound).toInt\n+        if (copiesWaitlisted > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(copiesWaitlisted)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement to avoid\n+        // using an extra pass over the RDD for computing the count.\n+        // Hence, acceptBound and waitListBound change on every iteration.\n+        val gamma1 = - math.log(delta) / stratum.numItems\n+        val gamma2 = (2.0 / 3.0) * gamma1\n+        stratum.acceptBound = math.max(0,\n+          fraction + gamma2 - math.sqrt(gamma2 * gamma2 + 3 * gamma2 * fraction))\n+        stratum.waitListBound = math.min(1,\n+          fraction + gamma1 + math.sqrt(gamma1 * gamma1 + 2 * gamma1 * fraction))\n+\n+        val x = rng.nextUniform(0.0, 1.0)\n+        if (x < stratum.acceptBound) {\n+          stratum.incrNumAccepted()\n+        } else if (x < stratum.waitListBound) {\n+          stratum.addToWaitList(x)\n+        }\n+      }\n+      stratum.incrNumItems()\n+      result\n+    }\n+  }\n+\n+  /**\n+   * Returns the function used combine results returned by seqOp from different partitions.\n+   */\n+  def getCombOp[K]: (MMap[K, Stratum], MMap[K, Stratum]) => MMap[K, Stratum] = {\n+    (result1: MMap[K, Stratum], result2: MMap[K, Stratum]) => {\n+      // take union of both key sets in case one partition doesn't contain all keys\n+      for (key <- result1.keySet.union(result2.keySet)) {"
  }],
  "prId": 1025
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "The contract of aggregate only allows modification to the first argument.\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-09T05:41:43Z",
    "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.Map\n+import scala.collection.mutable.{ArrayBuffer, HashMap, Map => MMap}\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Auxiliary functions and data structures for the sampleByKey method in PairRDDFunctions.\n+ *\n+ * Essentially, when exact sample size is necessary, we make additional passes over the RDD to\n+ * compute the exact threshold value to use for each stratum to guarantee exact sample size with\n+ * high probability. This is achieved by maintaining a waitlist of size O(log(s)), where s is the\n+ * desired sample size for each stratum.\n+ *\n+ * Like in simple random sampling, we generate a random value for each item from the\n+ * uniform  distribution [0.0, 1.0]. All items with values <= min(values of items in the waitlist)\n+ * are accepted into the sample instantly. The threshold for instant accept is designed so that\n+ * s - numAccepted = O(log(s)), where s is again the desired sample size. Thus, by maintaining a\n+ * waitlist size = O(log(s)), we will be able to create a sample of the exact size s by adding\n+ * a portion of the waitlist to the set of items that are instantly accepted. The exact threshold\n+ * is computed by sorting the values in the waitlist and picking the value at (s - numAccepted).\n+ *\n+ * Note that since we use the same seed for the RNG when computing the thresholds and the actual\n+ * sample, our computed thresholds are guaranteed to produce the desired sample size.\n+ *\n+ * For more theoretical background on the sampling techniques used here, please refer to\n+ * http://jmlr.org/proceedings/papers/v28/meng13a.html\n+ */\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * Count the number of items instantly accepted and generate the waitlist for each stratum.\n+   *\n+   * This is only invoked when exact sample size is required.\n+   */\n+  def getCounts[K, V](rdd: RDD[(K, V)],\n+      withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      counts: Option[Map[K, Long]],\n+      seed: Long): MMap[K, Stratum] = {\n+    val combOp = getCombOp[K]\n+    val mappedPartitionRDD = rdd.mapPartitionsWithIndex({ case (partition, iter) =>\n+      val zeroU: MMap[K, Stratum] = new HashMap[K, Stratum]()\n+      val rng = new RandomDataGenerator()\n+      rng.reSeed(seed + partition)\n+      val seqOp = getSeqOp(withReplacement, fractions, rng, counts)\n+      Iterator(iter.aggregate(zeroU)(seqOp, combOp))\n+    }, preservesPartitioning=true)\n+    mappedPartitionRDD.reduce(combOp)\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      rng: RandomDataGenerator,\n+      counts: Option[Map[K, Long]]): (MMap[K, Stratum], (K, V)) => MMap[K, Stratum] = {\n+    val delta = 5e-5\n+    (result: MMap[K, Stratum], item: (K, V)) => {\n+      val key = item._1\n+      val fraction = fractions(key)\n+      if (!result.contains(key)) {\n+        result += (key -> new Stratum())\n+      }\n+      val stratum = result(key)\n+\n+      if (withReplacement) {\n+        // compute acceptBound and waitListBound only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.areBoundsEmpty) {\n+          val n = counts.get(key)\n+          val sampleSize = math.ceil(n * fraction).toLong\n+          val lmbd1 = PoissonBounds.getLowerBound(sampleSize)\n+          val minCount = PoissonBounds.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) {\n+            PoissonBounds.getUpperBound(sampleSize)\n+          } else {\n+            PoissonBounds.getUpperBound(sampleSize - minCount)\n+          }\n+          stratum.acceptBound = lmbd1 / n\n+          stratum.waitListBound = lmbd2 / n\n+        }\n+        val acceptBound = stratum.acceptBound\n+        val copiesAccepted = if (acceptBound == 0.0) 0L else rng.nextPoisson(acceptBound)\n+        if (copiesAccepted > 0) {\n+          stratum.incrNumAccepted(copiesAccepted)\n+        }\n+        val copiesWaitlisted = rng.nextPoisson(stratum.waitListBound).toInt\n+        if (copiesWaitlisted > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(copiesWaitlisted)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement to avoid\n+        // using an extra pass over the RDD for computing the count.\n+        // Hence, acceptBound and waitListBound change on every iteration.\n+        val gamma1 = - math.log(delta) / stratum.numItems\n+        val gamma2 = (2.0 / 3.0) * gamma1\n+        stratum.acceptBound = math.max(0,\n+          fraction + gamma2 - math.sqrt(gamma2 * gamma2 + 3 * gamma2 * fraction))\n+        stratum.waitListBound = math.min(1,\n+          fraction + gamma1 + math.sqrt(gamma1 * gamma1 + 2 * gamma1 * fraction))\n+\n+        val x = rng.nextUniform(0.0, 1.0)\n+        if (x < stratum.acceptBound) {\n+          stratum.incrNumAccepted()\n+        } else if (x < stratum.waitListBound) {\n+          stratum.addToWaitList(x)\n+        }\n+      }\n+      stratum.incrNumItems()\n+      result\n+    }\n+  }\n+\n+  /**\n+   * Returns the function used combine results returned by seqOp from different partitions.\n+   */\n+  def getCombOp[K]: (MMap[K, Stratum], MMap[K, Stratum]) => MMap[K, Stratum] = {\n+    (result1: MMap[K, Stratum], result2: MMap[K, Stratum]) => {\n+      // take union of both key sets in case one partition doesn't contain all keys\n+      for (key <- result1.keySet.union(result2.keySet)) {\n+        // Use result2 to keep the combined result since r1 is usual empty"
  }, {
    "author": {
      "login": "dorx"
    },
    "body": "contract per...? I'm not seeing it in the Scala docs: http://www.scala-lang.org/api/2.10.3/index.html#scala.collection.immutable.List\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-14T22:00:44Z",
    "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.Map\n+import scala.collection.mutable.{ArrayBuffer, HashMap, Map => MMap}\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Auxiliary functions and data structures for the sampleByKey method in PairRDDFunctions.\n+ *\n+ * Essentially, when exact sample size is necessary, we make additional passes over the RDD to\n+ * compute the exact threshold value to use for each stratum to guarantee exact sample size with\n+ * high probability. This is achieved by maintaining a waitlist of size O(log(s)), where s is the\n+ * desired sample size for each stratum.\n+ *\n+ * Like in simple random sampling, we generate a random value for each item from the\n+ * uniform  distribution [0.0, 1.0]. All items with values <= min(values of items in the waitlist)\n+ * are accepted into the sample instantly. The threshold for instant accept is designed so that\n+ * s - numAccepted = O(log(s)), where s is again the desired sample size. Thus, by maintaining a\n+ * waitlist size = O(log(s)), we will be able to create a sample of the exact size s by adding\n+ * a portion of the waitlist to the set of items that are instantly accepted. The exact threshold\n+ * is computed by sorting the values in the waitlist and picking the value at (s - numAccepted).\n+ *\n+ * Note that since we use the same seed for the RNG when computing the thresholds and the actual\n+ * sample, our computed thresholds are guaranteed to produce the desired sample size.\n+ *\n+ * For more theoretical background on the sampling techniques used here, please refer to\n+ * http://jmlr.org/proceedings/papers/v28/meng13a.html\n+ */\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * Count the number of items instantly accepted and generate the waitlist for each stratum.\n+   *\n+   * This is only invoked when exact sample size is required.\n+   */\n+  def getCounts[K, V](rdd: RDD[(K, V)],\n+      withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      counts: Option[Map[K, Long]],\n+      seed: Long): MMap[K, Stratum] = {\n+    val combOp = getCombOp[K]\n+    val mappedPartitionRDD = rdd.mapPartitionsWithIndex({ case (partition, iter) =>\n+      val zeroU: MMap[K, Stratum] = new HashMap[K, Stratum]()\n+      val rng = new RandomDataGenerator()\n+      rng.reSeed(seed + partition)\n+      val seqOp = getSeqOp(withReplacement, fractions, rng, counts)\n+      Iterator(iter.aggregate(zeroU)(seqOp, combOp))\n+    }, preservesPartitioning=true)\n+    mappedPartitionRDD.reduce(combOp)\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      rng: RandomDataGenerator,\n+      counts: Option[Map[K, Long]]): (MMap[K, Stratum], (K, V)) => MMap[K, Stratum] = {\n+    val delta = 5e-5\n+    (result: MMap[K, Stratum], item: (K, V)) => {\n+      val key = item._1\n+      val fraction = fractions(key)\n+      if (!result.contains(key)) {\n+        result += (key -> new Stratum())\n+      }\n+      val stratum = result(key)\n+\n+      if (withReplacement) {\n+        // compute acceptBound and waitListBound only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.areBoundsEmpty) {\n+          val n = counts.get(key)\n+          val sampleSize = math.ceil(n * fraction).toLong\n+          val lmbd1 = PoissonBounds.getLowerBound(sampleSize)\n+          val minCount = PoissonBounds.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) {\n+            PoissonBounds.getUpperBound(sampleSize)\n+          } else {\n+            PoissonBounds.getUpperBound(sampleSize - minCount)\n+          }\n+          stratum.acceptBound = lmbd1 / n\n+          stratum.waitListBound = lmbd2 / n\n+        }\n+        val acceptBound = stratum.acceptBound\n+        val copiesAccepted = if (acceptBound == 0.0) 0L else rng.nextPoisson(acceptBound)\n+        if (copiesAccepted > 0) {\n+          stratum.incrNumAccepted(copiesAccepted)\n+        }\n+        val copiesWaitlisted = rng.nextPoisson(stratum.waitListBound).toInt\n+        if (copiesWaitlisted > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(copiesWaitlisted)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement to avoid\n+        // using an extra pass over the RDD for computing the count.\n+        // Hence, acceptBound and waitListBound change on every iteration.\n+        val gamma1 = - math.log(delta) / stratum.numItems\n+        val gamma2 = (2.0 / 3.0) * gamma1\n+        stratum.acceptBound = math.max(0,\n+          fraction + gamma2 - math.sqrt(gamma2 * gamma2 + 3 * gamma2 * fraction))\n+        stratum.waitListBound = math.min(1,\n+          fraction + gamma1 + math.sqrt(gamma1 * gamma1 + 2 * gamma1 * fraction))\n+\n+        val x = rng.nextUniform(0.0, 1.0)\n+        if (x < stratum.acceptBound) {\n+          stratum.incrNumAccepted()\n+        } else if (x < stratum.waitListBound) {\n+          stratum.addToWaitList(x)\n+        }\n+      }\n+      stratum.incrNumItems()\n+      result\n+    }\n+  }\n+\n+  /**\n+   * Returns the function used combine results returned by seqOp from different partitions.\n+   */\n+  def getCombOp[K]: (MMap[K, Stratum], MMap[K, Stratum]) => MMap[K, Stratum] = {\n+    (result1: MMap[K, Stratum], result2: MMap[K, Stratum]) => {\n+      // take union of both key sets in case one partition doesn't contain all keys\n+      for (key <- result1.keySet.union(result2.keySet)) {\n+        // Use result2 to keep the combined result since r1 is usual empty"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "Check Spark's RDD.aggregate doc.\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-14T22:12:08Z",
    "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.Map\n+import scala.collection.mutable.{ArrayBuffer, HashMap, Map => MMap}\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Auxiliary functions and data structures for the sampleByKey method in PairRDDFunctions.\n+ *\n+ * Essentially, when exact sample size is necessary, we make additional passes over the RDD to\n+ * compute the exact threshold value to use for each stratum to guarantee exact sample size with\n+ * high probability. This is achieved by maintaining a waitlist of size O(log(s)), where s is the\n+ * desired sample size for each stratum.\n+ *\n+ * Like in simple random sampling, we generate a random value for each item from the\n+ * uniform  distribution [0.0, 1.0]. All items with values <= min(values of items in the waitlist)\n+ * are accepted into the sample instantly. The threshold for instant accept is designed so that\n+ * s - numAccepted = O(log(s)), where s is again the desired sample size. Thus, by maintaining a\n+ * waitlist size = O(log(s)), we will be able to create a sample of the exact size s by adding\n+ * a portion of the waitlist to the set of items that are instantly accepted. The exact threshold\n+ * is computed by sorting the values in the waitlist and picking the value at (s - numAccepted).\n+ *\n+ * Note that since we use the same seed for the RNG when computing the thresholds and the actual\n+ * sample, our computed thresholds are guaranteed to produce the desired sample size.\n+ *\n+ * For more theoretical background on the sampling techniques used here, please refer to\n+ * http://jmlr.org/proceedings/papers/v28/meng13a.html\n+ */\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * Count the number of items instantly accepted and generate the waitlist for each stratum.\n+   *\n+   * This is only invoked when exact sample size is required.\n+   */\n+  def getCounts[K, V](rdd: RDD[(K, V)],\n+      withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      counts: Option[Map[K, Long]],\n+      seed: Long): MMap[K, Stratum] = {\n+    val combOp = getCombOp[K]\n+    val mappedPartitionRDD = rdd.mapPartitionsWithIndex({ case (partition, iter) =>\n+      val zeroU: MMap[K, Stratum] = new HashMap[K, Stratum]()\n+      val rng = new RandomDataGenerator()\n+      rng.reSeed(seed + partition)\n+      val seqOp = getSeqOp(withReplacement, fractions, rng, counts)\n+      Iterator(iter.aggregate(zeroU)(seqOp, combOp))\n+    }, preservesPartitioning=true)\n+    mappedPartitionRDD.reduce(combOp)\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      rng: RandomDataGenerator,\n+      counts: Option[Map[K, Long]]): (MMap[K, Stratum], (K, V)) => MMap[K, Stratum] = {\n+    val delta = 5e-5\n+    (result: MMap[K, Stratum], item: (K, V)) => {\n+      val key = item._1\n+      val fraction = fractions(key)\n+      if (!result.contains(key)) {\n+        result += (key -> new Stratum())\n+      }\n+      val stratum = result(key)\n+\n+      if (withReplacement) {\n+        // compute acceptBound and waitListBound only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.areBoundsEmpty) {\n+          val n = counts.get(key)\n+          val sampleSize = math.ceil(n * fraction).toLong\n+          val lmbd1 = PoissonBounds.getLowerBound(sampleSize)\n+          val minCount = PoissonBounds.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) {\n+            PoissonBounds.getUpperBound(sampleSize)\n+          } else {\n+            PoissonBounds.getUpperBound(sampleSize - minCount)\n+          }\n+          stratum.acceptBound = lmbd1 / n\n+          stratum.waitListBound = lmbd2 / n\n+        }\n+        val acceptBound = stratum.acceptBound\n+        val copiesAccepted = if (acceptBound == 0.0) 0L else rng.nextPoisson(acceptBound)\n+        if (copiesAccepted > 0) {\n+          stratum.incrNumAccepted(copiesAccepted)\n+        }\n+        val copiesWaitlisted = rng.nextPoisson(stratum.waitListBound).toInt\n+        if (copiesWaitlisted > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(copiesWaitlisted)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement to avoid\n+        // using an extra pass over the RDD for computing the count.\n+        // Hence, acceptBound and waitListBound change on every iteration.\n+        val gamma1 = - math.log(delta) / stratum.numItems\n+        val gamma2 = (2.0 / 3.0) * gamma1\n+        stratum.acceptBound = math.max(0,\n+          fraction + gamma2 - math.sqrt(gamma2 * gamma2 + 3 * gamma2 * fraction))\n+        stratum.waitListBound = math.min(1,\n+          fraction + gamma1 + math.sqrt(gamma1 * gamma1 + 2 * gamma1 * fraction))\n+\n+        val x = rng.nextUniform(0.0, 1.0)\n+        if (x < stratum.acceptBound) {\n+          stratum.incrNumAccepted()\n+        } else if (x < stratum.waitListBound) {\n+          stratum.addToWaitList(x)\n+        }\n+      }\n+      stratum.incrNumItems()\n+      result\n+    }\n+  }\n+\n+  /**\n+   * Returns the function used combine results returned by seqOp from different partitions.\n+   */\n+  def getCombOp[K]: (MMap[K, Stratum], MMap[K, Stratum]) => MMap[K, Stratum] = {\n+    (result1: MMap[K, Stratum], result2: MMap[K, Stratum]) => {\n+      // take union of both key sets in case one partition doesn't contain all keys\n+      for (key <- result1.keySet.union(result2.keySet)) {\n+        // Use result2 to keep the combined result since r1 is usual empty"
  }, {
    "author": {
      "login": "dorx"
    },
    "body": "Yeah but we're not using RDD.aggregate here. We're using Scala's TraversableOnce.aggregate\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-14T22:15:00Z",
    "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.Map\n+import scala.collection.mutable.{ArrayBuffer, HashMap, Map => MMap}\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Auxiliary functions and data structures for the sampleByKey method in PairRDDFunctions.\n+ *\n+ * Essentially, when exact sample size is necessary, we make additional passes over the RDD to\n+ * compute the exact threshold value to use for each stratum to guarantee exact sample size with\n+ * high probability. This is achieved by maintaining a waitlist of size O(log(s)), where s is the\n+ * desired sample size for each stratum.\n+ *\n+ * Like in simple random sampling, we generate a random value for each item from the\n+ * uniform  distribution [0.0, 1.0]. All items with values <= min(values of items in the waitlist)\n+ * are accepted into the sample instantly. The threshold for instant accept is designed so that\n+ * s - numAccepted = O(log(s)), where s is again the desired sample size. Thus, by maintaining a\n+ * waitlist size = O(log(s)), we will be able to create a sample of the exact size s by adding\n+ * a portion of the waitlist to the set of items that are instantly accepted. The exact threshold\n+ * is computed by sorting the values in the waitlist and picking the value at (s - numAccepted).\n+ *\n+ * Note that since we use the same seed for the RNG when computing the thresholds and the actual\n+ * sample, our computed thresholds are guaranteed to produce the desired sample size.\n+ *\n+ * For more theoretical background on the sampling techniques used here, please refer to\n+ * http://jmlr.org/proceedings/papers/v28/meng13a.html\n+ */\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * Count the number of items instantly accepted and generate the waitlist for each stratum.\n+   *\n+   * This is only invoked when exact sample size is required.\n+   */\n+  def getCounts[K, V](rdd: RDD[(K, V)],\n+      withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      counts: Option[Map[K, Long]],\n+      seed: Long): MMap[K, Stratum] = {\n+    val combOp = getCombOp[K]\n+    val mappedPartitionRDD = rdd.mapPartitionsWithIndex({ case (partition, iter) =>\n+      val zeroU: MMap[K, Stratum] = new HashMap[K, Stratum]()\n+      val rng = new RandomDataGenerator()\n+      rng.reSeed(seed + partition)\n+      val seqOp = getSeqOp(withReplacement, fractions, rng, counts)\n+      Iterator(iter.aggregate(zeroU)(seqOp, combOp))\n+    }, preservesPartitioning=true)\n+    mappedPartitionRDD.reduce(combOp)\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      rng: RandomDataGenerator,\n+      counts: Option[Map[K, Long]]): (MMap[K, Stratum], (K, V)) => MMap[K, Stratum] = {\n+    val delta = 5e-5\n+    (result: MMap[K, Stratum], item: (K, V)) => {\n+      val key = item._1\n+      val fraction = fractions(key)\n+      if (!result.contains(key)) {\n+        result += (key -> new Stratum())\n+      }\n+      val stratum = result(key)\n+\n+      if (withReplacement) {\n+        // compute acceptBound and waitListBound only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.areBoundsEmpty) {\n+          val n = counts.get(key)\n+          val sampleSize = math.ceil(n * fraction).toLong\n+          val lmbd1 = PoissonBounds.getLowerBound(sampleSize)\n+          val minCount = PoissonBounds.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) {\n+            PoissonBounds.getUpperBound(sampleSize)\n+          } else {\n+            PoissonBounds.getUpperBound(sampleSize - minCount)\n+          }\n+          stratum.acceptBound = lmbd1 / n\n+          stratum.waitListBound = lmbd2 / n\n+        }\n+        val acceptBound = stratum.acceptBound\n+        val copiesAccepted = if (acceptBound == 0.0) 0L else rng.nextPoisson(acceptBound)\n+        if (copiesAccepted > 0) {\n+          stratum.incrNumAccepted(copiesAccepted)\n+        }\n+        val copiesWaitlisted = rng.nextPoisson(stratum.waitListBound).toInt\n+        if (copiesWaitlisted > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(copiesWaitlisted)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement to avoid\n+        // using an extra pass over the RDD for computing the count.\n+        // Hence, acceptBound and waitListBound change on every iteration.\n+        val gamma1 = - math.log(delta) / stratum.numItems\n+        val gamma2 = (2.0 / 3.0) * gamma1\n+        stratum.acceptBound = math.max(0,\n+          fraction + gamma2 - math.sqrt(gamma2 * gamma2 + 3 * gamma2 * fraction))\n+        stratum.waitListBound = math.min(1,\n+          fraction + gamma1 + math.sqrt(gamma1 * gamma1 + 2 * gamma1 * fraction))\n+\n+        val x = rng.nextUniform(0.0, 1.0)\n+        if (x < stratum.acceptBound) {\n+          stratum.incrNumAccepted()\n+        } else if (x < stratum.waitListBound) {\n+          stratum.addToWaitList(x)\n+        }\n+      }\n+      stratum.incrNumItems()\n+      result\n+    }\n+  }\n+\n+  /**\n+   * Returns the function used combine results returned by seqOp from different partitions.\n+   */\n+  def getCombOp[K]: (MMap[K, Stratum], MMap[K, Stratum]) => MMap[K, Stratum] = {\n+    (result1: MMap[K, Stratum], result2: MMap[K, Stratum]) => {\n+      // take union of both key sets in case one partition doesn't contain all keys\n+      for (key <- result1.keySet.union(result2.keySet)) {\n+        // Use result2 to keep the combined result since r1 is usual empty"
  }],
  "prId": 1025
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Please add a comment here saying the way we call rng must be the same as in `getCounts`.\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-09T05:41:47Z",
    "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.Map\n+import scala.collection.mutable.{ArrayBuffer, HashMap, Map => MMap}\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Auxiliary functions and data structures for the sampleByKey method in PairRDDFunctions.\n+ *\n+ * Essentially, when exact sample size is necessary, we make additional passes over the RDD to\n+ * compute the exact threshold value to use for each stratum to guarantee exact sample size with\n+ * high probability. This is achieved by maintaining a waitlist of size O(log(s)), where s is the\n+ * desired sample size for each stratum.\n+ *\n+ * Like in simple random sampling, we generate a random value for each item from the\n+ * uniform  distribution [0.0, 1.0]. All items with values <= min(values of items in the waitlist)\n+ * are accepted into the sample instantly. The threshold for instant accept is designed so that\n+ * s - numAccepted = O(log(s)), where s is again the desired sample size. Thus, by maintaining a\n+ * waitlist size = O(log(s)), we will be able to create a sample of the exact size s by adding\n+ * a portion of the waitlist to the set of items that are instantly accepted. The exact threshold\n+ * is computed by sorting the values in the waitlist and picking the value at (s - numAccepted).\n+ *\n+ * Note that since we use the same seed for the RNG when computing the thresholds and the actual\n+ * sample, our computed thresholds are guaranteed to produce the desired sample size.\n+ *\n+ * For more theoretical background on the sampling techniques used here, please refer to\n+ * http://jmlr.org/proceedings/papers/v28/meng13a.html\n+ */\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * Count the number of items instantly accepted and generate the waitlist for each stratum.\n+   *\n+   * This is only invoked when exact sample size is required.\n+   */\n+  def getCounts[K, V](rdd: RDD[(K, V)],\n+      withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      counts: Option[Map[K, Long]],\n+      seed: Long): MMap[K, Stratum] = {\n+    val combOp = getCombOp[K]\n+    val mappedPartitionRDD = rdd.mapPartitionsWithIndex({ case (partition, iter) =>\n+      val zeroU: MMap[K, Stratum] = new HashMap[K, Stratum]()\n+      val rng = new RandomDataGenerator()\n+      rng.reSeed(seed + partition)\n+      val seqOp = getSeqOp(withReplacement, fractions, rng, counts)\n+      Iterator(iter.aggregate(zeroU)(seqOp, combOp))\n+    }, preservesPartitioning=true)\n+    mappedPartitionRDD.reduce(combOp)\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      rng: RandomDataGenerator,\n+      counts: Option[Map[K, Long]]): (MMap[K, Stratum], (K, V)) => MMap[K, Stratum] = {\n+    val delta = 5e-5\n+    (result: MMap[K, Stratum], item: (K, V)) => {\n+      val key = item._1\n+      val fraction = fractions(key)\n+      if (!result.contains(key)) {\n+        result += (key -> new Stratum())\n+      }\n+      val stratum = result(key)\n+\n+      if (withReplacement) {\n+        // compute acceptBound and waitListBound only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.areBoundsEmpty) {\n+          val n = counts.get(key)\n+          val sampleSize = math.ceil(n * fraction).toLong\n+          val lmbd1 = PoissonBounds.getLowerBound(sampleSize)\n+          val minCount = PoissonBounds.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) {\n+            PoissonBounds.getUpperBound(sampleSize)\n+          } else {\n+            PoissonBounds.getUpperBound(sampleSize - minCount)\n+          }\n+          stratum.acceptBound = lmbd1 / n\n+          stratum.waitListBound = lmbd2 / n\n+        }\n+        val acceptBound = stratum.acceptBound\n+        val copiesAccepted = if (acceptBound == 0.0) 0L else rng.nextPoisson(acceptBound)\n+        if (copiesAccepted > 0) {\n+          stratum.incrNumAccepted(copiesAccepted)\n+        }\n+        val copiesWaitlisted = rng.nextPoisson(stratum.waitListBound).toInt\n+        if (copiesWaitlisted > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(copiesWaitlisted)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement to avoid\n+        // using an extra pass over the RDD for computing the count.\n+        // Hence, acceptBound and waitListBound change on every iteration.\n+        val gamma1 = - math.log(delta) / stratum.numItems\n+        val gamma2 = (2.0 / 3.0) * gamma1\n+        stratum.acceptBound = math.max(0,\n+          fraction + gamma2 - math.sqrt(gamma2 * gamma2 + 3 * gamma2 * fraction))\n+        stratum.waitListBound = math.min(1,\n+          fraction + gamma1 + math.sqrt(gamma1 * gamma1 + 2 * gamma1 * fraction))\n+\n+        val x = rng.nextUniform(0.0, 1.0)\n+        if (x < stratum.acceptBound) {\n+          stratum.incrNumAccepted()\n+        } else if (x < stratum.waitListBound) {\n+          stratum.addToWaitList(x)\n+        }\n+      }\n+      stratum.incrNumItems()\n+      result\n+    }\n+  }\n+\n+  /**\n+   * Returns the function used combine results returned by seqOp from different partitions.\n+   */\n+  def getCombOp[K]: (MMap[K, Stratum], MMap[K, Stratum]) => MMap[K, Stratum] = {\n+    (result1: MMap[K, Stratum], result2: MMap[K, Stratum]) => {\n+      // take union of both key sets in case one partition doesn't contain all keys\n+      for (key <- result1.keySet.union(result2.keySet)) {\n+        // Use result2 to keep the combined result since r1 is usual empty\n+        val entry1 = result1.get(key)\n+        if (result2.contains(key)) {\n+          result2(key).merge(entry1)\n+        } else {\n+          if (entry1.isDefined) {\n+            result2 += (key -> entry1.get)\n+          }\n+        }\n+      }\n+      result2\n+    }\n+  }\n+\n+  /**\n+   * Given the result returned by getCounts, determine the threshold for accepting items to\n+   * generate exact sample size.\n+   *\n+   * To do so, we compute sampleSize = math.ceil(size * samplingRate) for each stratum and compare\n+   * it to the number of items that were accepted instantly and the number of items in the waitlist\n+   * for that stratum. Most of the time, numAccepted <= sampleSize <= (numAccepted + numWaitlisted),\n+   * which means we need to sort the elements in the waitlist by their associated values in order\n+   * to find the value T s.t. |{elements in the stratum whose associated values <= T}| = sampleSize.\n+   * Note that all elements in the waitlist have values >= bound for instant accept, so a T value\n+   * in the waitlist range would allow all elements that were instantly accepted on the first pass\n+   * to be included in the sample.\n+   */\n+  def computeThresholdByKey[K](finalResult: Map[K, Stratum],\n+      fractions: Map[K, Double]): Map[K, Double] = {\n+    val thresholdByKey = new HashMap[K, Double]()\n+    for ((key, stratum) <- finalResult) {\n+      val sampleSize = math.ceil(stratum.numItems * fractions(key)).toLong\n+      if (stratum.numAccepted > sampleSize) {\n+        logWarning(\"Pre-accepted too many\")\n+        thresholdByKey += (key -> stratum.acceptBound)\n+      } else {\n+        val numWaitListAccepted = (sampleSize - stratum.numAccepted).toInt\n+        if (numWaitListAccepted >= stratum.waitList.size) {\n+          logWarning(\"WaitList too short\")\n+          thresholdByKey += (key -> stratum.waitListBound)\n+        } else {\n+          thresholdByKey += (key -> stratum.waitList.sorted.apply(numWaitListAccepted))\n+        }\n+      }\n+    }\n+    thresholdByKey\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling without replacement.\n+   *\n+   * When exact sample size is required, we make an additional pass over the RDD to determine the\n+   * exact sampling rate that guarantees sample size with high confidence.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getBernoulliSamplingFunction[K, V](rdd: RDD[(K,  V)],\n+      fractions: Map[K, Double],\n+      exact: Boolean,\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    var samplingRateByKey = fractions\n+    if (exact) {\n+      // determine threshold for each stratum and resample\n+      val finalResult = getCounts(rdd, false, fractions, None, seed)\n+      samplingRateByKey = computeThresholdByKey(finalResult, fractions)\n+    }\n+    (idx: Int, iter: Iterator[(K, V)]) => {\n+      val rng = new RandomDataGenerator\n+      rng.reSeed(seed + idx)\n+      iter.filter(t => rng.nextUniform(0.0, 1.0) < samplingRateByKey(t._1))"
  }],
  "prId": 1025
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Remove counts and compute it inside the function. Since this is only needed for Poisson and when `exact == true`.\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-09T05:41:49Z",
    "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.Map\n+import scala.collection.mutable.{ArrayBuffer, HashMap, Map => MMap}\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Auxiliary functions and data structures for the sampleByKey method in PairRDDFunctions.\n+ *\n+ * Essentially, when exact sample size is necessary, we make additional passes over the RDD to\n+ * compute the exact threshold value to use for each stratum to guarantee exact sample size with\n+ * high probability. This is achieved by maintaining a waitlist of size O(log(s)), where s is the\n+ * desired sample size for each stratum.\n+ *\n+ * Like in simple random sampling, we generate a random value for each item from the\n+ * uniform  distribution [0.0, 1.0]. All items with values <= min(values of items in the waitlist)\n+ * are accepted into the sample instantly. The threshold for instant accept is designed so that\n+ * s - numAccepted = O(log(s)), where s is again the desired sample size. Thus, by maintaining a\n+ * waitlist size = O(log(s)), we will be able to create a sample of the exact size s by adding\n+ * a portion of the waitlist to the set of items that are instantly accepted. The exact threshold\n+ * is computed by sorting the values in the waitlist and picking the value at (s - numAccepted).\n+ *\n+ * Note that since we use the same seed for the RNG when computing the thresholds and the actual\n+ * sample, our computed thresholds are guaranteed to produce the desired sample size.\n+ *\n+ * For more theoretical background on the sampling techniques used here, please refer to\n+ * http://jmlr.org/proceedings/papers/v28/meng13a.html\n+ */\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * Count the number of items instantly accepted and generate the waitlist for each stratum.\n+   *\n+   * This is only invoked when exact sample size is required.\n+   */\n+  def getCounts[K, V](rdd: RDD[(K, V)],\n+      withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      counts: Option[Map[K, Long]],\n+      seed: Long): MMap[K, Stratum] = {\n+    val combOp = getCombOp[K]\n+    val mappedPartitionRDD = rdd.mapPartitionsWithIndex({ case (partition, iter) =>\n+      val zeroU: MMap[K, Stratum] = new HashMap[K, Stratum]()\n+      val rng = new RandomDataGenerator()\n+      rng.reSeed(seed + partition)\n+      val seqOp = getSeqOp(withReplacement, fractions, rng, counts)\n+      Iterator(iter.aggregate(zeroU)(seqOp, combOp))\n+    }, preservesPartitioning=true)\n+    mappedPartitionRDD.reduce(combOp)\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      rng: RandomDataGenerator,\n+      counts: Option[Map[K, Long]]): (MMap[K, Stratum], (K, V)) => MMap[K, Stratum] = {\n+    val delta = 5e-5\n+    (result: MMap[K, Stratum], item: (K, V)) => {\n+      val key = item._1\n+      val fraction = fractions(key)\n+      if (!result.contains(key)) {\n+        result += (key -> new Stratum())\n+      }\n+      val stratum = result(key)\n+\n+      if (withReplacement) {\n+        // compute acceptBound and waitListBound only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.areBoundsEmpty) {\n+          val n = counts.get(key)\n+          val sampleSize = math.ceil(n * fraction).toLong\n+          val lmbd1 = PoissonBounds.getLowerBound(sampleSize)\n+          val minCount = PoissonBounds.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) {\n+            PoissonBounds.getUpperBound(sampleSize)\n+          } else {\n+            PoissonBounds.getUpperBound(sampleSize - minCount)\n+          }\n+          stratum.acceptBound = lmbd1 / n\n+          stratum.waitListBound = lmbd2 / n\n+        }\n+        val acceptBound = stratum.acceptBound\n+        val copiesAccepted = if (acceptBound == 0.0) 0L else rng.nextPoisson(acceptBound)\n+        if (copiesAccepted > 0) {\n+          stratum.incrNumAccepted(copiesAccepted)\n+        }\n+        val copiesWaitlisted = rng.nextPoisson(stratum.waitListBound).toInt\n+        if (copiesWaitlisted > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(copiesWaitlisted)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement to avoid\n+        // using an extra pass over the RDD for computing the count.\n+        // Hence, acceptBound and waitListBound change on every iteration.\n+        val gamma1 = - math.log(delta) / stratum.numItems\n+        val gamma2 = (2.0 / 3.0) * gamma1\n+        stratum.acceptBound = math.max(0,\n+          fraction + gamma2 - math.sqrt(gamma2 * gamma2 + 3 * gamma2 * fraction))\n+        stratum.waitListBound = math.min(1,\n+          fraction + gamma1 + math.sqrt(gamma1 * gamma1 + 2 * gamma1 * fraction))\n+\n+        val x = rng.nextUniform(0.0, 1.0)\n+        if (x < stratum.acceptBound) {\n+          stratum.incrNumAccepted()\n+        } else if (x < stratum.waitListBound) {\n+          stratum.addToWaitList(x)\n+        }\n+      }\n+      stratum.incrNumItems()\n+      result\n+    }\n+  }\n+\n+  /**\n+   * Returns the function used combine results returned by seqOp from different partitions.\n+   */\n+  def getCombOp[K]: (MMap[K, Stratum], MMap[K, Stratum]) => MMap[K, Stratum] = {\n+    (result1: MMap[K, Stratum], result2: MMap[K, Stratum]) => {\n+      // take union of both key sets in case one partition doesn't contain all keys\n+      for (key <- result1.keySet.union(result2.keySet)) {\n+        // Use result2 to keep the combined result since r1 is usual empty\n+        val entry1 = result1.get(key)\n+        if (result2.contains(key)) {\n+          result2(key).merge(entry1)\n+        } else {\n+          if (entry1.isDefined) {\n+            result2 += (key -> entry1.get)\n+          }\n+        }\n+      }\n+      result2\n+    }\n+  }\n+\n+  /**\n+   * Given the result returned by getCounts, determine the threshold for accepting items to\n+   * generate exact sample size.\n+   *\n+   * To do so, we compute sampleSize = math.ceil(size * samplingRate) for each stratum and compare\n+   * it to the number of items that were accepted instantly and the number of items in the waitlist\n+   * for that stratum. Most of the time, numAccepted <= sampleSize <= (numAccepted + numWaitlisted),\n+   * which means we need to sort the elements in the waitlist by their associated values in order\n+   * to find the value T s.t. |{elements in the stratum whose associated values <= T}| = sampleSize.\n+   * Note that all elements in the waitlist have values >= bound for instant accept, so a T value\n+   * in the waitlist range would allow all elements that were instantly accepted on the first pass\n+   * to be included in the sample.\n+   */\n+  def computeThresholdByKey[K](finalResult: Map[K, Stratum],\n+      fractions: Map[K, Double]): Map[K, Double] = {\n+    val thresholdByKey = new HashMap[K, Double]()\n+    for ((key, stratum) <- finalResult) {\n+      val sampleSize = math.ceil(stratum.numItems * fractions(key)).toLong\n+      if (stratum.numAccepted > sampleSize) {\n+        logWarning(\"Pre-accepted too many\")\n+        thresholdByKey += (key -> stratum.acceptBound)\n+      } else {\n+        val numWaitListAccepted = (sampleSize - stratum.numAccepted).toInt\n+        if (numWaitListAccepted >= stratum.waitList.size) {\n+          logWarning(\"WaitList too short\")\n+          thresholdByKey += (key -> stratum.waitListBound)\n+        } else {\n+          thresholdByKey += (key -> stratum.waitList.sorted.apply(numWaitListAccepted))\n+        }\n+      }\n+    }\n+    thresholdByKey\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling without replacement.\n+   *\n+   * When exact sample size is required, we make an additional pass over the RDD to determine the\n+   * exact sampling rate that guarantees sample size with high confidence.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getBernoulliSamplingFunction[K, V](rdd: RDD[(K,  V)],\n+      fractions: Map[K, Double],\n+      exact: Boolean,\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    var samplingRateByKey = fractions\n+    if (exact) {\n+      // determine threshold for each stratum and resample\n+      val finalResult = getCounts(rdd, false, fractions, None, seed)\n+      samplingRateByKey = computeThresholdByKey(finalResult, fractions)\n+    }\n+    (idx: Int, iter: Iterator[(K, V)]) => {\n+      val rng = new RandomDataGenerator\n+      rng.reSeed(seed + idx)\n+      iter.filter(t => rng.nextUniform(0.0, 1.0) < samplingRateByKey(t._1))\n+    }\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling with replacement.\n+   *\n+   * When exact sample size is required, we make two additional passed over the RDD to determine\n+   * the exact sampling rate that guarantees sample size with high confidence. The first pass\n+   * counts the number of items in each stratum (group of items with the same key) in the RDD, and\n+   * the second pass uses the counts to determine exact sampling rates.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getPoissonSamplingFunction[K, V](rdd: RDD[(K, V)],\n+      fractions: Map[K, Double],\n+      exact: Boolean,\n+      counts: Option[Map[K, Long]],"
  }],
  "prId": 1025
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Same as above. Need a comment saying rng calls must be the same as in `getCounts`.\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-09T05:41:50Z",
    "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.Map\n+import scala.collection.mutable.{ArrayBuffer, HashMap, Map => MMap}\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Auxiliary functions and data structures for the sampleByKey method in PairRDDFunctions.\n+ *\n+ * Essentially, when exact sample size is necessary, we make additional passes over the RDD to\n+ * compute the exact threshold value to use for each stratum to guarantee exact sample size with\n+ * high probability. This is achieved by maintaining a waitlist of size O(log(s)), where s is the\n+ * desired sample size for each stratum.\n+ *\n+ * Like in simple random sampling, we generate a random value for each item from the\n+ * uniform  distribution [0.0, 1.0]. All items with values <= min(values of items in the waitlist)\n+ * are accepted into the sample instantly. The threshold for instant accept is designed so that\n+ * s - numAccepted = O(log(s)), where s is again the desired sample size. Thus, by maintaining a\n+ * waitlist size = O(log(s)), we will be able to create a sample of the exact size s by adding\n+ * a portion of the waitlist to the set of items that are instantly accepted. The exact threshold\n+ * is computed by sorting the values in the waitlist and picking the value at (s - numAccepted).\n+ *\n+ * Note that since we use the same seed for the RNG when computing the thresholds and the actual\n+ * sample, our computed thresholds are guaranteed to produce the desired sample size.\n+ *\n+ * For more theoretical background on the sampling techniques used here, please refer to\n+ * http://jmlr.org/proceedings/papers/v28/meng13a.html\n+ */\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * Count the number of items instantly accepted and generate the waitlist for each stratum.\n+   *\n+   * This is only invoked when exact sample size is required.\n+   */\n+  def getCounts[K, V](rdd: RDD[(K, V)],\n+      withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      counts: Option[Map[K, Long]],\n+      seed: Long): MMap[K, Stratum] = {\n+    val combOp = getCombOp[K]\n+    val mappedPartitionRDD = rdd.mapPartitionsWithIndex({ case (partition, iter) =>\n+      val zeroU: MMap[K, Stratum] = new HashMap[K, Stratum]()\n+      val rng = new RandomDataGenerator()\n+      rng.reSeed(seed + partition)\n+      val seqOp = getSeqOp(withReplacement, fractions, rng, counts)\n+      Iterator(iter.aggregate(zeroU)(seqOp, combOp))\n+    }, preservesPartitioning=true)\n+    mappedPartitionRDD.reduce(combOp)\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      rng: RandomDataGenerator,\n+      counts: Option[Map[K, Long]]): (MMap[K, Stratum], (K, V)) => MMap[K, Stratum] = {\n+    val delta = 5e-5\n+    (result: MMap[K, Stratum], item: (K, V)) => {\n+      val key = item._1\n+      val fraction = fractions(key)\n+      if (!result.contains(key)) {\n+        result += (key -> new Stratum())\n+      }\n+      val stratum = result(key)\n+\n+      if (withReplacement) {\n+        // compute acceptBound and waitListBound only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.areBoundsEmpty) {\n+          val n = counts.get(key)\n+          val sampleSize = math.ceil(n * fraction).toLong\n+          val lmbd1 = PoissonBounds.getLowerBound(sampleSize)\n+          val minCount = PoissonBounds.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) {\n+            PoissonBounds.getUpperBound(sampleSize)\n+          } else {\n+            PoissonBounds.getUpperBound(sampleSize - minCount)\n+          }\n+          stratum.acceptBound = lmbd1 / n\n+          stratum.waitListBound = lmbd2 / n\n+        }\n+        val acceptBound = stratum.acceptBound\n+        val copiesAccepted = if (acceptBound == 0.0) 0L else rng.nextPoisson(acceptBound)\n+        if (copiesAccepted > 0) {\n+          stratum.incrNumAccepted(copiesAccepted)\n+        }\n+        val copiesWaitlisted = rng.nextPoisson(stratum.waitListBound).toInt\n+        if (copiesWaitlisted > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(copiesWaitlisted)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement to avoid\n+        // using an extra pass over the RDD for computing the count.\n+        // Hence, acceptBound and waitListBound change on every iteration.\n+        val gamma1 = - math.log(delta) / stratum.numItems\n+        val gamma2 = (2.0 / 3.0) * gamma1\n+        stratum.acceptBound = math.max(0,\n+          fraction + gamma2 - math.sqrt(gamma2 * gamma2 + 3 * gamma2 * fraction))\n+        stratum.waitListBound = math.min(1,\n+          fraction + gamma1 + math.sqrt(gamma1 * gamma1 + 2 * gamma1 * fraction))\n+\n+        val x = rng.nextUniform(0.0, 1.0)\n+        if (x < stratum.acceptBound) {\n+          stratum.incrNumAccepted()\n+        } else if (x < stratum.waitListBound) {\n+          stratum.addToWaitList(x)\n+        }\n+      }\n+      stratum.incrNumItems()\n+      result\n+    }\n+  }\n+\n+  /**\n+   * Returns the function used combine results returned by seqOp from different partitions.\n+   */\n+  def getCombOp[K]: (MMap[K, Stratum], MMap[K, Stratum]) => MMap[K, Stratum] = {\n+    (result1: MMap[K, Stratum], result2: MMap[K, Stratum]) => {\n+      // take union of both key sets in case one partition doesn't contain all keys\n+      for (key <- result1.keySet.union(result2.keySet)) {\n+        // Use result2 to keep the combined result since r1 is usual empty\n+        val entry1 = result1.get(key)\n+        if (result2.contains(key)) {\n+          result2(key).merge(entry1)\n+        } else {\n+          if (entry1.isDefined) {\n+            result2 += (key -> entry1.get)\n+          }\n+        }\n+      }\n+      result2\n+    }\n+  }\n+\n+  /**\n+   * Given the result returned by getCounts, determine the threshold for accepting items to\n+   * generate exact sample size.\n+   *\n+   * To do so, we compute sampleSize = math.ceil(size * samplingRate) for each stratum and compare\n+   * it to the number of items that were accepted instantly and the number of items in the waitlist\n+   * for that stratum. Most of the time, numAccepted <= sampleSize <= (numAccepted + numWaitlisted),\n+   * which means we need to sort the elements in the waitlist by their associated values in order\n+   * to find the value T s.t. |{elements in the stratum whose associated values <= T}| = sampleSize.\n+   * Note that all elements in the waitlist have values >= bound for instant accept, so a T value\n+   * in the waitlist range would allow all elements that were instantly accepted on the first pass\n+   * to be included in the sample.\n+   */\n+  def computeThresholdByKey[K](finalResult: Map[K, Stratum],\n+      fractions: Map[K, Double]): Map[K, Double] = {\n+    val thresholdByKey = new HashMap[K, Double]()\n+    for ((key, stratum) <- finalResult) {\n+      val sampleSize = math.ceil(stratum.numItems * fractions(key)).toLong\n+      if (stratum.numAccepted > sampleSize) {\n+        logWarning(\"Pre-accepted too many\")\n+        thresholdByKey += (key -> stratum.acceptBound)\n+      } else {\n+        val numWaitListAccepted = (sampleSize - stratum.numAccepted).toInt\n+        if (numWaitListAccepted >= stratum.waitList.size) {\n+          logWarning(\"WaitList too short\")\n+          thresholdByKey += (key -> stratum.waitListBound)\n+        } else {\n+          thresholdByKey += (key -> stratum.waitList.sorted.apply(numWaitListAccepted))\n+        }\n+      }\n+    }\n+    thresholdByKey\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling without replacement.\n+   *\n+   * When exact sample size is required, we make an additional pass over the RDD to determine the\n+   * exact sampling rate that guarantees sample size with high confidence.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getBernoulliSamplingFunction[K, V](rdd: RDD[(K,  V)],\n+      fractions: Map[K, Double],\n+      exact: Boolean,\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    var samplingRateByKey = fractions\n+    if (exact) {\n+      // determine threshold for each stratum and resample\n+      val finalResult = getCounts(rdd, false, fractions, None, seed)\n+      samplingRateByKey = computeThresholdByKey(finalResult, fractions)\n+    }\n+    (idx: Int, iter: Iterator[(K, V)]) => {\n+      val rng = new RandomDataGenerator\n+      rng.reSeed(seed + idx)\n+      iter.filter(t => rng.nextUniform(0.0, 1.0) < samplingRateByKey(t._1))\n+    }\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling with replacement.\n+   *\n+   * When exact sample size is required, we make two additional passed over the RDD to determine\n+   * the exact sampling rate that guarantees sample size with high confidence. The first pass\n+   * counts the number of items in each stratum (group of items with the same key) in the RDD, and\n+   * the second pass uses the counts to determine exact sampling rates.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getPoissonSamplingFunction[K, V](rdd: RDD[(K, V)],\n+      fractions: Map[K, Double],\n+      exact: Boolean,\n+      counts: Option[Map[K, Long]],\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    // TODO implement the streaming version of sampling w/ replacement that doesn't require counts\n+    if (exact) {\n+      val finalResult = getCounts(rdd, true, fractions, counts, seed)\n+      val thresholdByKey = computeThresholdByKey(finalResult, fractions)\n+      (idx: Int, iter: Iterator[(K, V)]) => {\n+        val rng = new RandomDataGenerator()\n+        rng.reSeed(seed + idx)\n+        iter.flatMap { item =>\n+          val key = item._1\n+          val acceptBound = finalResult(key).acceptBound\n+          val copiesAccepted = if (acceptBound == 0) 0L else rng.nextPoisson(acceptBound)"
  }],
  "prId": 1025
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "key doesn't change here. So we don't need to recompute it.\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-09T05:41:52Z",
    "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.Map\n+import scala.collection.mutable.{ArrayBuffer, HashMap, Map => MMap}\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Auxiliary functions and data structures for the sampleByKey method in PairRDDFunctions.\n+ *\n+ * Essentially, when exact sample size is necessary, we make additional passes over the RDD to\n+ * compute the exact threshold value to use for each stratum to guarantee exact sample size with\n+ * high probability. This is achieved by maintaining a waitlist of size O(log(s)), where s is the\n+ * desired sample size for each stratum.\n+ *\n+ * Like in simple random sampling, we generate a random value for each item from the\n+ * uniform  distribution [0.0, 1.0]. All items with values <= min(values of items in the waitlist)\n+ * are accepted into the sample instantly. The threshold for instant accept is designed so that\n+ * s - numAccepted = O(log(s)), where s is again the desired sample size. Thus, by maintaining a\n+ * waitlist size = O(log(s)), we will be able to create a sample of the exact size s by adding\n+ * a portion of the waitlist to the set of items that are instantly accepted. The exact threshold\n+ * is computed by sorting the values in the waitlist and picking the value at (s - numAccepted).\n+ *\n+ * Note that since we use the same seed for the RNG when computing the thresholds and the actual\n+ * sample, our computed thresholds are guaranteed to produce the desired sample size.\n+ *\n+ * For more theoretical background on the sampling techniques used here, please refer to\n+ * http://jmlr.org/proceedings/papers/v28/meng13a.html\n+ */\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * Count the number of items instantly accepted and generate the waitlist for each stratum.\n+   *\n+   * This is only invoked when exact sample size is required.\n+   */\n+  def getCounts[K, V](rdd: RDD[(K, V)],\n+      withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      counts: Option[Map[K, Long]],\n+      seed: Long): MMap[K, Stratum] = {\n+    val combOp = getCombOp[K]\n+    val mappedPartitionRDD = rdd.mapPartitionsWithIndex({ case (partition, iter) =>\n+      val zeroU: MMap[K, Stratum] = new HashMap[K, Stratum]()\n+      val rng = new RandomDataGenerator()\n+      rng.reSeed(seed + partition)\n+      val seqOp = getSeqOp(withReplacement, fractions, rng, counts)\n+      Iterator(iter.aggregate(zeroU)(seqOp, combOp))\n+    }, preservesPartitioning=true)\n+    mappedPartitionRDD.reduce(combOp)\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      rng: RandomDataGenerator,\n+      counts: Option[Map[K, Long]]): (MMap[K, Stratum], (K, V)) => MMap[K, Stratum] = {\n+    val delta = 5e-5\n+    (result: MMap[K, Stratum], item: (K, V)) => {\n+      val key = item._1\n+      val fraction = fractions(key)\n+      if (!result.contains(key)) {\n+        result += (key -> new Stratum())\n+      }\n+      val stratum = result(key)\n+\n+      if (withReplacement) {\n+        // compute acceptBound and waitListBound only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.areBoundsEmpty) {\n+          val n = counts.get(key)\n+          val sampleSize = math.ceil(n * fraction).toLong\n+          val lmbd1 = PoissonBounds.getLowerBound(sampleSize)\n+          val minCount = PoissonBounds.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) {\n+            PoissonBounds.getUpperBound(sampleSize)\n+          } else {\n+            PoissonBounds.getUpperBound(sampleSize - minCount)\n+          }\n+          stratum.acceptBound = lmbd1 / n\n+          stratum.waitListBound = lmbd2 / n\n+        }\n+        val acceptBound = stratum.acceptBound\n+        val copiesAccepted = if (acceptBound == 0.0) 0L else rng.nextPoisson(acceptBound)\n+        if (copiesAccepted > 0) {\n+          stratum.incrNumAccepted(copiesAccepted)\n+        }\n+        val copiesWaitlisted = rng.nextPoisson(stratum.waitListBound).toInt\n+        if (copiesWaitlisted > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(copiesWaitlisted)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement to avoid\n+        // using an extra pass over the RDD for computing the count.\n+        // Hence, acceptBound and waitListBound change on every iteration.\n+        val gamma1 = - math.log(delta) / stratum.numItems\n+        val gamma2 = (2.0 / 3.0) * gamma1\n+        stratum.acceptBound = math.max(0,\n+          fraction + gamma2 - math.sqrt(gamma2 * gamma2 + 3 * gamma2 * fraction))\n+        stratum.waitListBound = math.min(1,\n+          fraction + gamma1 + math.sqrt(gamma1 * gamma1 + 2 * gamma1 * fraction))\n+\n+        val x = rng.nextUniform(0.0, 1.0)\n+        if (x < stratum.acceptBound) {\n+          stratum.incrNumAccepted()\n+        } else if (x < stratum.waitListBound) {\n+          stratum.addToWaitList(x)\n+        }\n+      }\n+      stratum.incrNumItems()\n+      result\n+    }\n+  }\n+\n+  /**\n+   * Returns the function used combine results returned by seqOp from different partitions.\n+   */\n+  def getCombOp[K]: (MMap[K, Stratum], MMap[K, Stratum]) => MMap[K, Stratum] = {\n+    (result1: MMap[K, Stratum], result2: MMap[K, Stratum]) => {\n+      // take union of both key sets in case one partition doesn't contain all keys\n+      for (key <- result1.keySet.union(result2.keySet)) {\n+        // Use result2 to keep the combined result since r1 is usual empty\n+        val entry1 = result1.get(key)\n+        if (result2.contains(key)) {\n+          result2(key).merge(entry1)\n+        } else {\n+          if (entry1.isDefined) {\n+            result2 += (key -> entry1.get)\n+          }\n+        }\n+      }\n+      result2\n+    }\n+  }\n+\n+  /**\n+   * Given the result returned by getCounts, determine the threshold for accepting items to\n+   * generate exact sample size.\n+   *\n+   * To do so, we compute sampleSize = math.ceil(size * samplingRate) for each stratum and compare\n+   * it to the number of items that were accepted instantly and the number of items in the waitlist\n+   * for that stratum. Most of the time, numAccepted <= sampleSize <= (numAccepted + numWaitlisted),\n+   * which means we need to sort the elements in the waitlist by their associated values in order\n+   * to find the value T s.t. |{elements in the stratum whose associated values <= T}| = sampleSize.\n+   * Note that all elements in the waitlist have values >= bound for instant accept, so a T value\n+   * in the waitlist range would allow all elements that were instantly accepted on the first pass\n+   * to be included in the sample.\n+   */\n+  def computeThresholdByKey[K](finalResult: Map[K, Stratum],\n+      fractions: Map[K, Double]): Map[K, Double] = {\n+    val thresholdByKey = new HashMap[K, Double]()\n+    for ((key, stratum) <- finalResult) {\n+      val sampleSize = math.ceil(stratum.numItems * fractions(key)).toLong\n+      if (stratum.numAccepted > sampleSize) {\n+        logWarning(\"Pre-accepted too many\")\n+        thresholdByKey += (key -> stratum.acceptBound)\n+      } else {\n+        val numWaitListAccepted = (sampleSize - stratum.numAccepted).toInt\n+        if (numWaitListAccepted >= stratum.waitList.size) {\n+          logWarning(\"WaitList too short\")\n+          thresholdByKey += (key -> stratum.waitListBound)\n+        } else {\n+          thresholdByKey += (key -> stratum.waitList.sorted.apply(numWaitListAccepted))\n+        }\n+      }\n+    }\n+    thresholdByKey\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling without replacement.\n+   *\n+   * When exact sample size is required, we make an additional pass over the RDD to determine the\n+   * exact sampling rate that guarantees sample size with high confidence.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getBernoulliSamplingFunction[K, V](rdd: RDD[(K,  V)],\n+      fractions: Map[K, Double],\n+      exact: Boolean,\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    var samplingRateByKey = fractions\n+    if (exact) {\n+      // determine threshold for each stratum and resample\n+      val finalResult = getCounts(rdd, false, fractions, None, seed)\n+      samplingRateByKey = computeThresholdByKey(finalResult, fractions)\n+    }\n+    (idx: Int, iter: Iterator[(K, V)]) => {\n+      val rng = new RandomDataGenerator\n+      rng.reSeed(seed + idx)\n+      iter.filter(t => rng.nextUniform(0.0, 1.0) < samplingRateByKey(t._1))\n+    }\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling with replacement.\n+   *\n+   * When exact sample size is required, we make two additional passed over the RDD to determine\n+   * the exact sampling rate that guarantees sample size with high confidence. The first pass\n+   * counts the number of items in each stratum (group of items with the same key) in the RDD, and\n+   * the second pass uses the counts to determine exact sampling rates.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getPoissonSamplingFunction[K, V](rdd: RDD[(K, V)],\n+      fractions: Map[K, Double],\n+      exact: Boolean,\n+      counts: Option[Map[K, Long]],\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    // TODO implement the streaming version of sampling w/ replacement that doesn't require counts\n+    if (exact) {\n+      val finalResult = getCounts(rdd, true, fractions, counts, seed)\n+      val thresholdByKey = computeThresholdByKey(finalResult, fractions)\n+      (idx: Int, iter: Iterator[(K, V)]) => {\n+        val rng = new RandomDataGenerator()\n+        rng.reSeed(seed + idx)\n+        iter.flatMap { item =>\n+          val key = item._1\n+          val acceptBound = finalResult(key).acceptBound\n+          val copiesAccepted = if (acceptBound == 0) 0L else rng.nextPoisson(acceptBound)\n+          val copiesWailisted = rng.nextPoisson(finalResult(key).waitListBound).toInt\n+          val copiesInSample = copiesAccepted +\n+            (0 until copiesWailisted).count(i => rng.nextUniform(0.0, 1.0) < thresholdByKey(key))"
  }, {
    "author": {
      "login": "dorx"
    },
    "body": "Where is the key recomputed?\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-14T22:12:15Z",
    "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.Map\n+import scala.collection.mutable.{ArrayBuffer, HashMap, Map => MMap}\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Auxiliary functions and data structures for the sampleByKey method in PairRDDFunctions.\n+ *\n+ * Essentially, when exact sample size is necessary, we make additional passes over the RDD to\n+ * compute the exact threshold value to use for each stratum to guarantee exact sample size with\n+ * high probability. This is achieved by maintaining a waitlist of size O(log(s)), where s is the\n+ * desired sample size for each stratum.\n+ *\n+ * Like in simple random sampling, we generate a random value for each item from the\n+ * uniform  distribution [0.0, 1.0]. All items with values <= min(values of items in the waitlist)\n+ * are accepted into the sample instantly. The threshold for instant accept is designed so that\n+ * s - numAccepted = O(log(s)), where s is again the desired sample size. Thus, by maintaining a\n+ * waitlist size = O(log(s)), we will be able to create a sample of the exact size s by adding\n+ * a portion of the waitlist to the set of items that are instantly accepted. The exact threshold\n+ * is computed by sorting the values in the waitlist and picking the value at (s - numAccepted).\n+ *\n+ * Note that since we use the same seed for the RNG when computing the thresholds and the actual\n+ * sample, our computed thresholds are guaranteed to produce the desired sample size.\n+ *\n+ * For more theoretical background on the sampling techniques used here, please refer to\n+ * http://jmlr.org/proceedings/papers/v28/meng13a.html\n+ */\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * Count the number of items instantly accepted and generate the waitlist for each stratum.\n+   *\n+   * This is only invoked when exact sample size is required.\n+   */\n+  def getCounts[K, V](rdd: RDD[(K, V)],\n+      withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      counts: Option[Map[K, Long]],\n+      seed: Long): MMap[K, Stratum] = {\n+    val combOp = getCombOp[K]\n+    val mappedPartitionRDD = rdd.mapPartitionsWithIndex({ case (partition, iter) =>\n+      val zeroU: MMap[K, Stratum] = new HashMap[K, Stratum]()\n+      val rng = new RandomDataGenerator()\n+      rng.reSeed(seed + partition)\n+      val seqOp = getSeqOp(withReplacement, fractions, rng, counts)\n+      Iterator(iter.aggregate(zeroU)(seqOp, combOp))\n+    }, preservesPartitioning=true)\n+    mappedPartitionRDD.reduce(combOp)\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      rng: RandomDataGenerator,\n+      counts: Option[Map[K, Long]]): (MMap[K, Stratum], (K, V)) => MMap[K, Stratum] = {\n+    val delta = 5e-5\n+    (result: MMap[K, Stratum], item: (K, V)) => {\n+      val key = item._1\n+      val fraction = fractions(key)\n+      if (!result.contains(key)) {\n+        result += (key -> new Stratum())\n+      }\n+      val stratum = result(key)\n+\n+      if (withReplacement) {\n+        // compute acceptBound and waitListBound only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.areBoundsEmpty) {\n+          val n = counts.get(key)\n+          val sampleSize = math.ceil(n * fraction).toLong\n+          val lmbd1 = PoissonBounds.getLowerBound(sampleSize)\n+          val minCount = PoissonBounds.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) {\n+            PoissonBounds.getUpperBound(sampleSize)\n+          } else {\n+            PoissonBounds.getUpperBound(sampleSize - minCount)\n+          }\n+          stratum.acceptBound = lmbd1 / n\n+          stratum.waitListBound = lmbd2 / n\n+        }\n+        val acceptBound = stratum.acceptBound\n+        val copiesAccepted = if (acceptBound == 0.0) 0L else rng.nextPoisson(acceptBound)\n+        if (copiesAccepted > 0) {\n+          stratum.incrNumAccepted(copiesAccepted)\n+        }\n+        val copiesWaitlisted = rng.nextPoisson(stratum.waitListBound).toInt\n+        if (copiesWaitlisted > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(copiesWaitlisted)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement to avoid\n+        // using an extra pass over the RDD for computing the count.\n+        // Hence, acceptBound and waitListBound change on every iteration.\n+        val gamma1 = - math.log(delta) / stratum.numItems\n+        val gamma2 = (2.0 / 3.0) * gamma1\n+        stratum.acceptBound = math.max(0,\n+          fraction + gamma2 - math.sqrt(gamma2 * gamma2 + 3 * gamma2 * fraction))\n+        stratum.waitListBound = math.min(1,\n+          fraction + gamma1 + math.sqrt(gamma1 * gamma1 + 2 * gamma1 * fraction))\n+\n+        val x = rng.nextUniform(0.0, 1.0)\n+        if (x < stratum.acceptBound) {\n+          stratum.incrNumAccepted()\n+        } else if (x < stratum.waitListBound) {\n+          stratum.addToWaitList(x)\n+        }\n+      }\n+      stratum.incrNumItems()\n+      result\n+    }\n+  }\n+\n+  /**\n+   * Returns the function used combine results returned by seqOp from different partitions.\n+   */\n+  def getCombOp[K]: (MMap[K, Stratum], MMap[K, Stratum]) => MMap[K, Stratum] = {\n+    (result1: MMap[K, Stratum], result2: MMap[K, Stratum]) => {\n+      // take union of both key sets in case one partition doesn't contain all keys\n+      for (key <- result1.keySet.union(result2.keySet)) {\n+        // Use result2 to keep the combined result since r1 is usual empty\n+        val entry1 = result1.get(key)\n+        if (result2.contains(key)) {\n+          result2(key).merge(entry1)\n+        } else {\n+          if (entry1.isDefined) {\n+            result2 += (key -> entry1.get)\n+          }\n+        }\n+      }\n+      result2\n+    }\n+  }\n+\n+  /**\n+   * Given the result returned by getCounts, determine the threshold for accepting items to\n+   * generate exact sample size.\n+   *\n+   * To do so, we compute sampleSize = math.ceil(size * samplingRate) for each stratum and compare\n+   * it to the number of items that were accepted instantly and the number of items in the waitlist\n+   * for that stratum. Most of the time, numAccepted <= sampleSize <= (numAccepted + numWaitlisted),\n+   * which means we need to sort the elements in the waitlist by their associated values in order\n+   * to find the value T s.t. |{elements in the stratum whose associated values <= T}| = sampleSize.\n+   * Note that all elements in the waitlist have values >= bound for instant accept, so a T value\n+   * in the waitlist range would allow all elements that were instantly accepted on the first pass\n+   * to be included in the sample.\n+   */\n+  def computeThresholdByKey[K](finalResult: Map[K, Stratum],\n+      fractions: Map[K, Double]): Map[K, Double] = {\n+    val thresholdByKey = new HashMap[K, Double]()\n+    for ((key, stratum) <- finalResult) {\n+      val sampleSize = math.ceil(stratum.numItems * fractions(key)).toLong\n+      if (stratum.numAccepted > sampleSize) {\n+        logWarning(\"Pre-accepted too many\")\n+        thresholdByKey += (key -> stratum.acceptBound)\n+      } else {\n+        val numWaitListAccepted = (sampleSize - stratum.numAccepted).toInt\n+        if (numWaitListAccepted >= stratum.waitList.size) {\n+          logWarning(\"WaitList too short\")\n+          thresholdByKey += (key -> stratum.waitListBound)\n+        } else {\n+          thresholdByKey += (key -> stratum.waitList.sorted.apply(numWaitListAccepted))\n+        }\n+      }\n+    }\n+    thresholdByKey\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling without replacement.\n+   *\n+   * When exact sample size is required, we make an additional pass over the RDD to determine the\n+   * exact sampling rate that guarantees sample size with high confidence.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getBernoulliSamplingFunction[K, V](rdd: RDD[(K,  V)],\n+      fractions: Map[K, Double],\n+      exact: Boolean,\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    var samplingRateByKey = fractions\n+    if (exact) {\n+      // determine threshold for each stratum and resample\n+      val finalResult = getCounts(rdd, false, fractions, None, seed)\n+      samplingRateByKey = computeThresholdByKey(finalResult, fractions)\n+    }\n+    (idx: Int, iter: Iterator[(K, V)]) => {\n+      val rng = new RandomDataGenerator\n+      rng.reSeed(seed + idx)\n+      iter.filter(t => rng.nextUniform(0.0, 1.0) < samplingRateByKey(t._1))\n+    }\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling with replacement.\n+   *\n+   * When exact sample size is required, we make two additional passed over the RDD to determine\n+   * the exact sampling rate that guarantees sample size with high confidence. The first pass\n+   * counts the number of items in each stratum (group of items with the same key) in the RDD, and\n+   * the second pass uses the counts to determine exact sampling rates.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getPoissonSamplingFunction[K, V](rdd: RDD[(K, V)],\n+      fractions: Map[K, Double],\n+      exact: Boolean,\n+      counts: Option[Map[K, Long]],\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    // TODO implement the streaming version of sampling w/ replacement that doesn't require counts\n+    if (exact) {\n+      val finalResult = getCounts(rdd, true, fractions, counts, seed)\n+      val thresholdByKey = computeThresholdByKey(finalResult, fractions)\n+      (idx: Int, iter: Iterator[(K, V)]) => {\n+        val rng = new RandomDataGenerator()\n+        rng.reSeed(seed + idx)\n+        iter.flatMap { item =>\n+          val key = item._1\n+          val acceptBound = finalResult(key).acceptBound\n+          val copiesAccepted = if (acceptBound == 0) 0L else rng.nextPoisson(acceptBound)\n+          val copiesWailisted = rng.nextPoisson(finalResult(key).waitListBound).toInt\n+          val copiesInSample = copiesAccepted +\n+            (0 until copiesWailisted).count(i => rng.nextUniform(0.0, 1.0) < thresholdByKey(key))"
  }],
  "prId": 1025
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Not necessary.\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-09T05:41:54Z",
    "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.Map\n+import scala.collection.mutable.{ArrayBuffer, HashMap, Map => MMap}\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Auxiliary functions and data structures for the sampleByKey method in PairRDDFunctions.\n+ *\n+ * Essentially, when exact sample size is necessary, we make additional passes over the RDD to\n+ * compute the exact threshold value to use for each stratum to guarantee exact sample size with\n+ * high probability. This is achieved by maintaining a waitlist of size O(log(s)), where s is the\n+ * desired sample size for each stratum.\n+ *\n+ * Like in simple random sampling, we generate a random value for each item from the\n+ * uniform  distribution [0.0, 1.0]. All items with values <= min(values of items in the waitlist)\n+ * are accepted into the sample instantly. The threshold for instant accept is designed so that\n+ * s - numAccepted = O(log(s)), where s is again the desired sample size. Thus, by maintaining a\n+ * waitlist size = O(log(s)), we will be able to create a sample of the exact size s by adding\n+ * a portion of the waitlist to the set of items that are instantly accepted. The exact threshold\n+ * is computed by sorting the values in the waitlist and picking the value at (s - numAccepted).\n+ *\n+ * Note that since we use the same seed for the RNG when computing the thresholds and the actual\n+ * sample, our computed thresholds are guaranteed to produce the desired sample size.\n+ *\n+ * For more theoretical background on the sampling techniques used here, please refer to\n+ * http://jmlr.org/proceedings/papers/v28/meng13a.html\n+ */\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * Count the number of items instantly accepted and generate the waitlist for each stratum.\n+   *\n+   * This is only invoked when exact sample size is required.\n+   */\n+  def getCounts[K, V](rdd: RDD[(K, V)],\n+      withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      counts: Option[Map[K, Long]],\n+      seed: Long): MMap[K, Stratum] = {\n+    val combOp = getCombOp[K]\n+    val mappedPartitionRDD = rdd.mapPartitionsWithIndex({ case (partition, iter) =>\n+      val zeroU: MMap[K, Stratum] = new HashMap[K, Stratum]()\n+      val rng = new RandomDataGenerator()\n+      rng.reSeed(seed + partition)\n+      val seqOp = getSeqOp(withReplacement, fractions, rng, counts)\n+      Iterator(iter.aggregate(zeroU)(seqOp, combOp))\n+    }, preservesPartitioning=true)\n+    mappedPartitionRDD.reduce(combOp)\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      rng: RandomDataGenerator,\n+      counts: Option[Map[K, Long]]): (MMap[K, Stratum], (K, V)) => MMap[K, Stratum] = {\n+    val delta = 5e-5\n+    (result: MMap[K, Stratum], item: (K, V)) => {\n+      val key = item._1\n+      val fraction = fractions(key)\n+      if (!result.contains(key)) {\n+        result += (key -> new Stratum())\n+      }\n+      val stratum = result(key)\n+\n+      if (withReplacement) {\n+        // compute acceptBound and waitListBound only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.areBoundsEmpty) {\n+          val n = counts.get(key)\n+          val sampleSize = math.ceil(n * fraction).toLong\n+          val lmbd1 = PoissonBounds.getLowerBound(sampleSize)\n+          val minCount = PoissonBounds.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) {\n+            PoissonBounds.getUpperBound(sampleSize)\n+          } else {\n+            PoissonBounds.getUpperBound(sampleSize - minCount)\n+          }\n+          stratum.acceptBound = lmbd1 / n\n+          stratum.waitListBound = lmbd2 / n\n+        }\n+        val acceptBound = stratum.acceptBound\n+        val copiesAccepted = if (acceptBound == 0.0) 0L else rng.nextPoisson(acceptBound)\n+        if (copiesAccepted > 0) {\n+          stratum.incrNumAccepted(copiesAccepted)\n+        }\n+        val copiesWaitlisted = rng.nextPoisson(stratum.waitListBound).toInt\n+        if (copiesWaitlisted > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(copiesWaitlisted)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement to avoid\n+        // using an extra pass over the RDD for computing the count.\n+        // Hence, acceptBound and waitListBound change on every iteration.\n+        val gamma1 = - math.log(delta) / stratum.numItems\n+        val gamma2 = (2.0 / 3.0) * gamma1\n+        stratum.acceptBound = math.max(0,\n+          fraction + gamma2 - math.sqrt(gamma2 * gamma2 + 3 * gamma2 * fraction))\n+        stratum.waitListBound = math.min(1,\n+          fraction + gamma1 + math.sqrt(gamma1 * gamma1 + 2 * gamma1 * fraction))\n+\n+        val x = rng.nextUniform(0.0, 1.0)\n+        if (x < stratum.acceptBound) {\n+          stratum.incrNumAccepted()\n+        } else if (x < stratum.waitListBound) {\n+          stratum.addToWaitList(x)\n+        }\n+      }\n+      stratum.incrNumItems()\n+      result\n+    }\n+  }\n+\n+  /**\n+   * Returns the function used combine results returned by seqOp from different partitions.\n+   */\n+  def getCombOp[K]: (MMap[K, Stratum], MMap[K, Stratum]) => MMap[K, Stratum] = {\n+    (result1: MMap[K, Stratum], result2: MMap[K, Stratum]) => {\n+      // take union of both key sets in case one partition doesn't contain all keys\n+      for (key <- result1.keySet.union(result2.keySet)) {\n+        // Use result2 to keep the combined result since r1 is usual empty\n+        val entry1 = result1.get(key)\n+        if (result2.contains(key)) {\n+          result2(key).merge(entry1)\n+        } else {\n+          if (entry1.isDefined) {\n+            result2 += (key -> entry1.get)\n+          }\n+        }\n+      }\n+      result2\n+    }\n+  }\n+\n+  /**\n+   * Given the result returned by getCounts, determine the threshold for accepting items to\n+   * generate exact sample size.\n+   *\n+   * To do so, we compute sampleSize = math.ceil(size * samplingRate) for each stratum and compare\n+   * it to the number of items that were accepted instantly and the number of items in the waitlist\n+   * for that stratum. Most of the time, numAccepted <= sampleSize <= (numAccepted + numWaitlisted),\n+   * which means we need to sort the elements in the waitlist by their associated values in order\n+   * to find the value T s.t. |{elements in the stratum whose associated values <= T}| = sampleSize.\n+   * Note that all elements in the waitlist have values >= bound for instant accept, so a T value\n+   * in the waitlist range would allow all elements that were instantly accepted on the first pass\n+   * to be included in the sample.\n+   */\n+  def computeThresholdByKey[K](finalResult: Map[K, Stratum],\n+      fractions: Map[K, Double]): Map[K, Double] = {\n+    val thresholdByKey = new HashMap[K, Double]()\n+    for ((key, stratum) <- finalResult) {\n+      val sampleSize = math.ceil(stratum.numItems * fractions(key)).toLong\n+      if (stratum.numAccepted > sampleSize) {\n+        logWarning(\"Pre-accepted too many\")\n+        thresholdByKey += (key -> stratum.acceptBound)\n+      } else {\n+        val numWaitListAccepted = (sampleSize - stratum.numAccepted).toInt\n+        if (numWaitListAccepted >= stratum.waitList.size) {\n+          logWarning(\"WaitList too short\")\n+          thresholdByKey += (key -> stratum.waitListBound)\n+        } else {\n+          thresholdByKey += (key -> stratum.waitList.sorted.apply(numWaitListAccepted))\n+        }\n+      }\n+    }\n+    thresholdByKey\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling without replacement.\n+   *\n+   * When exact sample size is required, we make an additional pass over the RDD to determine the\n+   * exact sampling rate that guarantees sample size with high confidence.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getBernoulliSamplingFunction[K, V](rdd: RDD[(K,  V)],\n+      fractions: Map[K, Double],\n+      exact: Boolean,\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    var samplingRateByKey = fractions\n+    if (exact) {\n+      // determine threshold for each stratum and resample\n+      val finalResult = getCounts(rdd, false, fractions, None, seed)\n+      samplingRateByKey = computeThresholdByKey(finalResult, fractions)\n+    }\n+    (idx: Int, iter: Iterator[(K, V)]) => {\n+      val rng = new RandomDataGenerator\n+      rng.reSeed(seed + idx)\n+      iter.filter(t => rng.nextUniform(0.0, 1.0) < samplingRateByKey(t._1))\n+    }\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling with replacement.\n+   *\n+   * When exact sample size is required, we make two additional passed over the RDD to determine\n+   * the exact sampling rate that guarantees sample size with high confidence. The first pass\n+   * counts the number of items in each stratum (group of items with the same key) in the RDD, and\n+   * the second pass uses the counts to determine exact sampling rates.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getPoissonSamplingFunction[K, V](rdd: RDD[(K, V)],\n+      fractions: Map[K, Double],\n+      exact: Boolean,\n+      counts: Option[Map[K, Long]],\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    // TODO implement the streaming version of sampling w/ replacement that doesn't require counts\n+    if (exact) {\n+      val finalResult = getCounts(rdd, true, fractions, counts, seed)\n+      val thresholdByKey = computeThresholdByKey(finalResult, fractions)\n+      (idx: Int, iter: Iterator[(K, V)]) => {\n+        val rng = new RandomDataGenerator()\n+        rng.reSeed(seed + idx)\n+        iter.flatMap { item =>\n+          val key = item._1\n+          val acceptBound = finalResult(key).acceptBound\n+          val copiesAccepted = if (acceptBound == 0) 0L else rng.nextPoisson(acceptBound)\n+          val copiesWailisted = rng.nextPoisson(finalResult(key).waitListBound).toInt\n+          val copiesInSample = copiesAccepted +\n+            (0 until copiesWailisted).count(i => rng.nextUniform(0.0, 1.0) < thresholdByKey(key))\n+          if (copiesInSample > 0) {\n+            Iterator.fill(copiesInSample.toInt)(item)\n+          } else {\n+            Iterator.empty\n+          }\n+        }\n+      }\n+    } else {\n+      (idx: Int, iter: Iterator[(K, V)]) => {\n+        val rng = new RandomDataGenerator()\n+        rng.reSeed(seed + idx)\n+        iter.flatMap { item =>\n+          val count = rng.nextPoisson(fractions(item._1)).toInt\n+          if (count > 0) {\n+            Iterator.fill(count)(item)\n+          } else {\n+            Iterator.empty\n+          }\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * Object used by seqOp to keep track of the number of items accepted and items waitlisted per\n+ * stratum, as well as the bounds for accepting and waitlisting items.\n+ *\n+ * `[random]` here is necessary since it's in the return type signature of seqOp defined above"
  }, {
    "author": {
      "login": "dorx"
    },
    "body": "Want to try to compile it for me without `[random]`?\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-14T22:01:33Z",
    "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.Map\n+import scala.collection.mutable.{ArrayBuffer, HashMap, Map => MMap}\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Auxiliary functions and data structures for the sampleByKey method in PairRDDFunctions.\n+ *\n+ * Essentially, when exact sample size is necessary, we make additional passes over the RDD to\n+ * compute the exact threshold value to use for each stratum to guarantee exact sample size with\n+ * high probability. This is achieved by maintaining a waitlist of size O(log(s)), where s is the\n+ * desired sample size for each stratum.\n+ *\n+ * Like in simple random sampling, we generate a random value for each item from the\n+ * uniform  distribution [0.0, 1.0]. All items with values <= min(values of items in the waitlist)\n+ * are accepted into the sample instantly. The threshold for instant accept is designed so that\n+ * s - numAccepted = O(log(s)), where s is again the desired sample size. Thus, by maintaining a\n+ * waitlist size = O(log(s)), we will be able to create a sample of the exact size s by adding\n+ * a portion of the waitlist to the set of items that are instantly accepted. The exact threshold\n+ * is computed by sorting the values in the waitlist and picking the value at (s - numAccepted).\n+ *\n+ * Note that since we use the same seed for the RNG when computing the thresholds and the actual\n+ * sample, our computed thresholds are guaranteed to produce the desired sample size.\n+ *\n+ * For more theoretical background on the sampling techniques used here, please refer to\n+ * http://jmlr.org/proceedings/papers/v28/meng13a.html\n+ */\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * Count the number of items instantly accepted and generate the waitlist for each stratum.\n+   *\n+   * This is only invoked when exact sample size is required.\n+   */\n+  def getCounts[K, V](rdd: RDD[(K, V)],\n+      withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      counts: Option[Map[K, Long]],\n+      seed: Long): MMap[K, Stratum] = {\n+    val combOp = getCombOp[K]\n+    val mappedPartitionRDD = rdd.mapPartitionsWithIndex({ case (partition, iter) =>\n+      val zeroU: MMap[K, Stratum] = new HashMap[K, Stratum]()\n+      val rng = new RandomDataGenerator()\n+      rng.reSeed(seed + partition)\n+      val seqOp = getSeqOp(withReplacement, fractions, rng, counts)\n+      Iterator(iter.aggregate(zeroU)(seqOp, combOp))\n+    }, preservesPartitioning=true)\n+    mappedPartitionRDD.reduce(combOp)\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      rng: RandomDataGenerator,\n+      counts: Option[Map[K, Long]]): (MMap[K, Stratum], (K, V)) => MMap[K, Stratum] = {\n+    val delta = 5e-5\n+    (result: MMap[K, Stratum], item: (K, V)) => {\n+      val key = item._1\n+      val fraction = fractions(key)\n+      if (!result.contains(key)) {\n+        result += (key -> new Stratum())\n+      }\n+      val stratum = result(key)\n+\n+      if (withReplacement) {\n+        // compute acceptBound and waitListBound only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.areBoundsEmpty) {\n+          val n = counts.get(key)\n+          val sampleSize = math.ceil(n * fraction).toLong\n+          val lmbd1 = PoissonBounds.getLowerBound(sampleSize)\n+          val minCount = PoissonBounds.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) {\n+            PoissonBounds.getUpperBound(sampleSize)\n+          } else {\n+            PoissonBounds.getUpperBound(sampleSize - minCount)\n+          }\n+          stratum.acceptBound = lmbd1 / n\n+          stratum.waitListBound = lmbd2 / n\n+        }\n+        val acceptBound = stratum.acceptBound\n+        val copiesAccepted = if (acceptBound == 0.0) 0L else rng.nextPoisson(acceptBound)\n+        if (copiesAccepted > 0) {\n+          stratum.incrNumAccepted(copiesAccepted)\n+        }\n+        val copiesWaitlisted = rng.nextPoisson(stratum.waitListBound).toInt\n+        if (copiesWaitlisted > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(copiesWaitlisted)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement to avoid\n+        // using an extra pass over the RDD for computing the count.\n+        // Hence, acceptBound and waitListBound change on every iteration.\n+        val gamma1 = - math.log(delta) / stratum.numItems\n+        val gamma2 = (2.0 / 3.0) * gamma1\n+        stratum.acceptBound = math.max(0,\n+          fraction + gamma2 - math.sqrt(gamma2 * gamma2 + 3 * gamma2 * fraction))\n+        stratum.waitListBound = math.min(1,\n+          fraction + gamma1 + math.sqrt(gamma1 * gamma1 + 2 * gamma1 * fraction))\n+\n+        val x = rng.nextUniform(0.0, 1.0)\n+        if (x < stratum.acceptBound) {\n+          stratum.incrNumAccepted()\n+        } else if (x < stratum.waitListBound) {\n+          stratum.addToWaitList(x)\n+        }\n+      }\n+      stratum.incrNumItems()\n+      result\n+    }\n+  }\n+\n+  /**\n+   * Returns the function used combine results returned by seqOp from different partitions.\n+   */\n+  def getCombOp[K]: (MMap[K, Stratum], MMap[K, Stratum]) => MMap[K, Stratum] = {\n+    (result1: MMap[K, Stratum], result2: MMap[K, Stratum]) => {\n+      // take union of both key sets in case one partition doesn't contain all keys\n+      for (key <- result1.keySet.union(result2.keySet)) {\n+        // Use result2 to keep the combined result since r1 is usual empty\n+        val entry1 = result1.get(key)\n+        if (result2.contains(key)) {\n+          result2(key).merge(entry1)\n+        } else {\n+          if (entry1.isDefined) {\n+            result2 += (key -> entry1.get)\n+          }\n+        }\n+      }\n+      result2\n+    }\n+  }\n+\n+  /**\n+   * Given the result returned by getCounts, determine the threshold for accepting items to\n+   * generate exact sample size.\n+   *\n+   * To do so, we compute sampleSize = math.ceil(size * samplingRate) for each stratum and compare\n+   * it to the number of items that were accepted instantly and the number of items in the waitlist\n+   * for that stratum. Most of the time, numAccepted <= sampleSize <= (numAccepted + numWaitlisted),\n+   * which means we need to sort the elements in the waitlist by their associated values in order\n+   * to find the value T s.t. |{elements in the stratum whose associated values <= T}| = sampleSize.\n+   * Note that all elements in the waitlist have values >= bound for instant accept, so a T value\n+   * in the waitlist range would allow all elements that were instantly accepted on the first pass\n+   * to be included in the sample.\n+   */\n+  def computeThresholdByKey[K](finalResult: Map[K, Stratum],\n+      fractions: Map[K, Double]): Map[K, Double] = {\n+    val thresholdByKey = new HashMap[K, Double]()\n+    for ((key, stratum) <- finalResult) {\n+      val sampleSize = math.ceil(stratum.numItems * fractions(key)).toLong\n+      if (stratum.numAccepted > sampleSize) {\n+        logWarning(\"Pre-accepted too many\")\n+        thresholdByKey += (key -> stratum.acceptBound)\n+      } else {\n+        val numWaitListAccepted = (sampleSize - stratum.numAccepted).toInt\n+        if (numWaitListAccepted >= stratum.waitList.size) {\n+          logWarning(\"WaitList too short\")\n+          thresholdByKey += (key -> stratum.waitListBound)\n+        } else {\n+          thresholdByKey += (key -> stratum.waitList.sorted.apply(numWaitListAccepted))\n+        }\n+      }\n+    }\n+    thresholdByKey\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling without replacement.\n+   *\n+   * When exact sample size is required, we make an additional pass over the RDD to determine the\n+   * exact sampling rate that guarantees sample size with high confidence.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getBernoulliSamplingFunction[K, V](rdd: RDD[(K,  V)],\n+      fractions: Map[K, Double],\n+      exact: Boolean,\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    var samplingRateByKey = fractions\n+    if (exact) {\n+      // determine threshold for each stratum and resample\n+      val finalResult = getCounts(rdd, false, fractions, None, seed)\n+      samplingRateByKey = computeThresholdByKey(finalResult, fractions)\n+    }\n+    (idx: Int, iter: Iterator[(K, V)]) => {\n+      val rng = new RandomDataGenerator\n+      rng.reSeed(seed + idx)\n+      iter.filter(t => rng.nextUniform(0.0, 1.0) < samplingRateByKey(t._1))\n+    }\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling with replacement.\n+   *\n+   * When exact sample size is required, we make two additional passed over the RDD to determine\n+   * the exact sampling rate that guarantees sample size with high confidence. The first pass\n+   * counts the number of items in each stratum (group of items with the same key) in the RDD, and\n+   * the second pass uses the counts to determine exact sampling rates.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getPoissonSamplingFunction[K, V](rdd: RDD[(K, V)],\n+      fractions: Map[K, Double],\n+      exact: Boolean,\n+      counts: Option[Map[K, Long]],\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    // TODO implement the streaming version of sampling w/ replacement that doesn't require counts\n+    if (exact) {\n+      val finalResult = getCounts(rdd, true, fractions, counts, seed)\n+      val thresholdByKey = computeThresholdByKey(finalResult, fractions)\n+      (idx: Int, iter: Iterator[(K, V)]) => {\n+        val rng = new RandomDataGenerator()\n+        rng.reSeed(seed + idx)\n+        iter.flatMap { item =>\n+          val key = item._1\n+          val acceptBound = finalResult(key).acceptBound\n+          val copiesAccepted = if (acceptBound == 0) 0L else rng.nextPoisson(acceptBound)\n+          val copiesWailisted = rng.nextPoisson(finalResult(key).waitListBound).toInt\n+          val copiesInSample = copiesAccepted +\n+            (0 until copiesWailisted).count(i => rng.nextUniform(0.0, 1.0) < thresholdByKey(key))\n+          if (copiesInSample > 0) {\n+            Iterator.fill(copiesInSample.toInt)(item)\n+          } else {\n+            Iterator.empty\n+          }\n+        }\n+      }\n+    } else {\n+      (idx: Int, iter: Iterator[(K, V)]) => {\n+        val rng = new RandomDataGenerator()\n+        rng.reSeed(seed + idx)\n+        iter.flatMap { item =>\n+          val count = rng.nextPoisson(fractions(item._1)).toInt\n+          if (count > 0) {\n+            Iterator.fill(count)(item)\n+          } else {\n+            Iterator.empty\n+          }\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * Object used by seqOp to keep track of the number of items accepted and items waitlisted per\n+ * stratum, as well as the bounds for accepting and waitlisting items.\n+ *\n+ * `[random]` here is necessary since it's in the return type signature of seqOp defined above"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "I meant the comment is not necessary.\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-14T22:12:36Z",
    "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.Map\n+import scala.collection.mutable.{ArrayBuffer, HashMap, Map => MMap}\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Auxiliary functions and data structures for the sampleByKey method in PairRDDFunctions.\n+ *\n+ * Essentially, when exact sample size is necessary, we make additional passes over the RDD to\n+ * compute the exact threshold value to use for each stratum to guarantee exact sample size with\n+ * high probability. This is achieved by maintaining a waitlist of size O(log(s)), where s is the\n+ * desired sample size for each stratum.\n+ *\n+ * Like in simple random sampling, we generate a random value for each item from the\n+ * uniform  distribution [0.0, 1.0]. All items with values <= min(values of items in the waitlist)\n+ * are accepted into the sample instantly. The threshold for instant accept is designed so that\n+ * s - numAccepted = O(log(s)), where s is again the desired sample size. Thus, by maintaining a\n+ * waitlist size = O(log(s)), we will be able to create a sample of the exact size s by adding\n+ * a portion of the waitlist to the set of items that are instantly accepted. The exact threshold\n+ * is computed by sorting the values in the waitlist and picking the value at (s - numAccepted).\n+ *\n+ * Note that since we use the same seed for the RNG when computing the thresholds and the actual\n+ * sample, our computed thresholds are guaranteed to produce the desired sample size.\n+ *\n+ * For more theoretical background on the sampling techniques used here, please refer to\n+ * http://jmlr.org/proceedings/papers/v28/meng13a.html\n+ */\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * Count the number of items instantly accepted and generate the waitlist for each stratum.\n+   *\n+   * This is only invoked when exact sample size is required.\n+   */\n+  def getCounts[K, V](rdd: RDD[(K, V)],\n+      withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      counts: Option[Map[K, Long]],\n+      seed: Long): MMap[K, Stratum] = {\n+    val combOp = getCombOp[K]\n+    val mappedPartitionRDD = rdd.mapPartitionsWithIndex({ case (partition, iter) =>\n+      val zeroU: MMap[K, Stratum] = new HashMap[K, Stratum]()\n+      val rng = new RandomDataGenerator()\n+      rng.reSeed(seed + partition)\n+      val seqOp = getSeqOp(withReplacement, fractions, rng, counts)\n+      Iterator(iter.aggregate(zeroU)(seqOp, combOp))\n+    }, preservesPartitioning=true)\n+    mappedPartitionRDD.reduce(combOp)\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      rng: RandomDataGenerator,\n+      counts: Option[Map[K, Long]]): (MMap[K, Stratum], (K, V)) => MMap[K, Stratum] = {\n+    val delta = 5e-5\n+    (result: MMap[K, Stratum], item: (K, V)) => {\n+      val key = item._1\n+      val fraction = fractions(key)\n+      if (!result.contains(key)) {\n+        result += (key -> new Stratum())\n+      }\n+      val stratum = result(key)\n+\n+      if (withReplacement) {\n+        // compute acceptBound and waitListBound only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.areBoundsEmpty) {\n+          val n = counts.get(key)\n+          val sampleSize = math.ceil(n * fraction).toLong\n+          val lmbd1 = PoissonBounds.getLowerBound(sampleSize)\n+          val minCount = PoissonBounds.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) {\n+            PoissonBounds.getUpperBound(sampleSize)\n+          } else {\n+            PoissonBounds.getUpperBound(sampleSize - minCount)\n+          }\n+          stratum.acceptBound = lmbd1 / n\n+          stratum.waitListBound = lmbd2 / n\n+        }\n+        val acceptBound = stratum.acceptBound\n+        val copiesAccepted = if (acceptBound == 0.0) 0L else rng.nextPoisson(acceptBound)\n+        if (copiesAccepted > 0) {\n+          stratum.incrNumAccepted(copiesAccepted)\n+        }\n+        val copiesWaitlisted = rng.nextPoisson(stratum.waitListBound).toInt\n+        if (copiesWaitlisted > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(copiesWaitlisted)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement to avoid\n+        // using an extra pass over the RDD for computing the count.\n+        // Hence, acceptBound and waitListBound change on every iteration.\n+        val gamma1 = - math.log(delta) / stratum.numItems\n+        val gamma2 = (2.0 / 3.0) * gamma1\n+        stratum.acceptBound = math.max(0,\n+          fraction + gamma2 - math.sqrt(gamma2 * gamma2 + 3 * gamma2 * fraction))\n+        stratum.waitListBound = math.min(1,\n+          fraction + gamma1 + math.sqrt(gamma1 * gamma1 + 2 * gamma1 * fraction))\n+\n+        val x = rng.nextUniform(0.0, 1.0)\n+        if (x < stratum.acceptBound) {\n+          stratum.incrNumAccepted()\n+        } else if (x < stratum.waitListBound) {\n+          stratum.addToWaitList(x)\n+        }\n+      }\n+      stratum.incrNumItems()\n+      result\n+    }\n+  }\n+\n+  /**\n+   * Returns the function used combine results returned by seqOp from different partitions.\n+   */\n+  def getCombOp[K]: (MMap[K, Stratum], MMap[K, Stratum]) => MMap[K, Stratum] = {\n+    (result1: MMap[K, Stratum], result2: MMap[K, Stratum]) => {\n+      // take union of both key sets in case one partition doesn't contain all keys\n+      for (key <- result1.keySet.union(result2.keySet)) {\n+        // Use result2 to keep the combined result since r1 is usual empty\n+        val entry1 = result1.get(key)\n+        if (result2.contains(key)) {\n+          result2(key).merge(entry1)\n+        } else {\n+          if (entry1.isDefined) {\n+            result2 += (key -> entry1.get)\n+          }\n+        }\n+      }\n+      result2\n+    }\n+  }\n+\n+  /**\n+   * Given the result returned by getCounts, determine the threshold for accepting items to\n+   * generate exact sample size.\n+   *\n+   * To do so, we compute sampleSize = math.ceil(size * samplingRate) for each stratum and compare\n+   * it to the number of items that were accepted instantly and the number of items in the waitlist\n+   * for that stratum. Most of the time, numAccepted <= sampleSize <= (numAccepted + numWaitlisted),\n+   * which means we need to sort the elements in the waitlist by their associated values in order\n+   * to find the value T s.t. |{elements in the stratum whose associated values <= T}| = sampleSize.\n+   * Note that all elements in the waitlist have values >= bound for instant accept, so a T value\n+   * in the waitlist range would allow all elements that were instantly accepted on the first pass\n+   * to be included in the sample.\n+   */\n+  def computeThresholdByKey[K](finalResult: Map[K, Stratum],\n+      fractions: Map[K, Double]): Map[K, Double] = {\n+    val thresholdByKey = new HashMap[K, Double]()\n+    for ((key, stratum) <- finalResult) {\n+      val sampleSize = math.ceil(stratum.numItems * fractions(key)).toLong\n+      if (stratum.numAccepted > sampleSize) {\n+        logWarning(\"Pre-accepted too many\")\n+        thresholdByKey += (key -> stratum.acceptBound)\n+      } else {\n+        val numWaitListAccepted = (sampleSize - stratum.numAccepted).toInt\n+        if (numWaitListAccepted >= stratum.waitList.size) {\n+          logWarning(\"WaitList too short\")\n+          thresholdByKey += (key -> stratum.waitListBound)\n+        } else {\n+          thresholdByKey += (key -> stratum.waitList.sorted.apply(numWaitListAccepted))\n+        }\n+      }\n+    }\n+    thresholdByKey\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling without replacement.\n+   *\n+   * When exact sample size is required, we make an additional pass over the RDD to determine the\n+   * exact sampling rate that guarantees sample size with high confidence.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getBernoulliSamplingFunction[K, V](rdd: RDD[(K,  V)],\n+      fractions: Map[K, Double],\n+      exact: Boolean,\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    var samplingRateByKey = fractions\n+    if (exact) {\n+      // determine threshold for each stratum and resample\n+      val finalResult = getCounts(rdd, false, fractions, None, seed)\n+      samplingRateByKey = computeThresholdByKey(finalResult, fractions)\n+    }\n+    (idx: Int, iter: Iterator[(K, V)]) => {\n+      val rng = new RandomDataGenerator\n+      rng.reSeed(seed + idx)\n+      iter.filter(t => rng.nextUniform(0.0, 1.0) < samplingRateByKey(t._1))\n+    }\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling with replacement.\n+   *\n+   * When exact sample size is required, we make two additional passed over the RDD to determine\n+   * the exact sampling rate that guarantees sample size with high confidence. The first pass\n+   * counts the number of items in each stratum (group of items with the same key) in the RDD, and\n+   * the second pass uses the counts to determine exact sampling rates.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getPoissonSamplingFunction[K, V](rdd: RDD[(K, V)],\n+      fractions: Map[K, Double],\n+      exact: Boolean,\n+      counts: Option[Map[K, Long]],\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    // TODO implement the streaming version of sampling w/ replacement that doesn't require counts\n+    if (exact) {\n+      val finalResult = getCounts(rdd, true, fractions, counts, seed)\n+      val thresholdByKey = computeThresholdByKey(finalResult, fractions)\n+      (idx: Int, iter: Iterator[(K, V)]) => {\n+        val rng = new RandomDataGenerator()\n+        rng.reSeed(seed + idx)\n+        iter.flatMap { item =>\n+          val key = item._1\n+          val acceptBound = finalResult(key).acceptBound\n+          val copiesAccepted = if (acceptBound == 0) 0L else rng.nextPoisson(acceptBound)\n+          val copiesWailisted = rng.nextPoisson(finalResult(key).waitListBound).toInt\n+          val copiesInSample = copiesAccepted +\n+            (0 until copiesWailisted).count(i => rng.nextUniform(0.0, 1.0) < thresholdByKey(key))\n+          if (copiesInSample > 0) {\n+            Iterator.fill(copiesInSample.toInt)(item)\n+          } else {\n+            Iterator.empty\n+          }\n+        }\n+      }\n+    } else {\n+      (idx: Int, iter: Iterator[(K, V)]) => {\n+        val rng = new RandomDataGenerator()\n+        rng.reSeed(seed + idx)\n+        iter.flatMap { item =>\n+          val count = rng.nextPoisson(fractions(item._1)).toInt\n+          if (count > 0) {\n+            Iterator.fill(count)(item)\n+          } else {\n+            Iterator.empty\n+          }\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * Object used by seqOp to keep track of the number of items accepted and items waitlisted per\n+ * stratum, as well as the bounds for accepting and waitlisting items.\n+ *\n+ * `[random]` here is necessary since it's in the return type signature of seqOp defined above"
  }, {
    "author": {
      "login": "dorx"
    },
    "body": "I don't know. I can see people down the road wanting to remove it thinking it's superfluous and break compile by accident.\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-14T22:17:30Z",
    "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.Map\n+import scala.collection.mutable.{ArrayBuffer, HashMap, Map => MMap}\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Auxiliary functions and data structures for the sampleByKey method in PairRDDFunctions.\n+ *\n+ * Essentially, when exact sample size is necessary, we make additional passes over the RDD to\n+ * compute the exact threshold value to use for each stratum to guarantee exact sample size with\n+ * high probability. This is achieved by maintaining a waitlist of size O(log(s)), where s is the\n+ * desired sample size for each stratum.\n+ *\n+ * Like in simple random sampling, we generate a random value for each item from the\n+ * uniform  distribution [0.0, 1.0]. All items with values <= min(values of items in the waitlist)\n+ * are accepted into the sample instantly. The threshold for instant accept is designed so that\n+ * s - numAccepted = O(log(s)), where s is again the desired sample size. Thus, by maintaining a\n+ * waitlist size = O(log(s)), we will be able to create a sample of the exact size s by adding\n+ * a portion of the waitlist to the set of items that are instantly accepted. The exact threshold\n+ * is computed by sorting the values in the waitlist and picking the value at (s - numAccepted).\n+ *\n+ * Note that since we use the same seed for the RNG when computing the thresholds and the actual\n+ * sample, our computed thresholds are guaranteed to produce the desired sample size.\n+ *\n+ * For more theoretical background on the sampling techniques used here, please refer to\n+ * http://jmlr.org/proceedings/papers/v28/meng13a.html\n+ */\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * Count the number of items instantly accepted and generate the waitlist for each stratum.\n+   *\n+   * This is only invoked when exact sample size is required.\n+   */\n+  def getCounts[K, V](rdd: RDD[(K, V)],\n+      withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      counts: Option[Map[K, Long]],\n+      seed: Long): MMap[K, Stratum] = {\n+    val combOp = getCombOp[K]\n+    val mappedPartitionRDD = rdd.mapPartitionsWithIndex({ case (partition, iter) =>\n+      val zeroU: MMap[K, Stratum] = new HashMap[K, Stratum]()\n+      val rng = new RandomDataGenerator()\n+      rng.reSeed(seed + partition)\n+      val seqOp = getSeqOp(withReplacement, fractions, rng, counts)\n+      Iterator(iter.aggregate(zeroU)(seqOp, combOp))\n+    }, preservesPartitioning=true)\n+    mappedPartitionRDD.reduce(combOp)\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      rng: RandomDataGenerator,\n+      counts: Option[Map[K, Long]]): (MMap[K, Stratum], (K, V)) => MMap[K, Stratum] = {\n+    val delta = 5e-5\n+    (result: MMap[K, Stratum], item: (K, V)) => {\n+      val key = item._1\n+      val fraction = fractions(key)\n+      if (!result.contains(key)) {\n+        result += (key -> new Stratum())\n+      }\n+      val stratum = result(key)\n+\n+      if (withReplacement) {\n+        // compute acceptBound and waitListBound only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.areBoundsEmpty) {\n+          val n = counts.get(key)\n+          val sampleSize = math.ceil(n * fraction).toLong\n+          val lmbd1 = PoissonBounds.getLowerBound(sampleSize)\n+          val minCount = PoissonBounds.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) {\n+            PoissonBounds.getUpperBound(sampleSize)\n+          } else {\n+            PoissonBounds.getUpperBound(sampleSize - minCount)\n+          }\n+          stratum.acceptBound = lmbd1 / n\n+          stratum.waitListBound = lmbd2 / n\n+        }\n+        val acceptBound = stratum.acceptBound\n+        val copiesAccepted = if (acceptBound == 0.0) 0L else rng.nextPoisson(acceptBound)\n+        if (copiesAccepted > 0) {\n+          stratum.incrNumAccepted(copiesAccepted)\n+        }\n+        val copiesWaitlisted = rng.nextPoisson(stratum.waitListBound).toInt\n+        if (copiesWaitlisted > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(copiesWaitlisted)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement to avoid\n+        // using an extra pass over the RDD for computing the count.\n+        // Hence, acceptBound and waitListBound change on every iteration.\n+        val gamma1 = - math.log(delta) / stratum.numItems\n+        val gamma2 = (2.0 / 3.0) * gamma1\n+        stratum.acceptBound = math.max(0,\n+          fraction + gamma2 - math.sqrt(gamma2 * gamma2 + 3 * gamma2 * fraction))\n+        stratum.waitListBound = math.min(1,\n+          fraction + gamma1 + math.sqrt(gamma1 * gamma1 + 2 * gamma1 * fraction))\n+\n+        val x = rng.nextUniform(0.0, 1.0)\n+        if (x < stratum.acceptBound) {\n+          stratum.incrNumAccepted()\n+        } else if (x < stratum.waitListBound) {\n+          stratum.addToWaitList(x)\n+        }\n+      }\n+      stratum.incrNumItems()\n+      result\n+    }\n+  }\n+\n+  /**\n+   * Returns the function used combine results returned by seqOp from different partitions.\n+   */\n+  def getCombOp[K]: (MMap[K, Stratum], MMap[K, Stratum]) => MMap[K, Stratum] = {\n+    (result1: MMap[K, Stratum], result2: MMap[K, Stratum]) => {\n+      // take union of both key sets in case one partition doesn't contain all keys\n+      for (key <- result1.keySet.union(result2.keySet)) {\n+        // Use result2 to keep the combined result since r1 is usual empty\n+        val entry1 = result1.get(key)\n+        if (result2.contains(key)) {\n+          result2(key).merge(entry1)\n+        } else {\n+          if (entry1.isDefined) {\n+            result2 += (key -> entry1.get)\n+          }\n+        }\n+      }\n+      result2\n+    }\n+  }\n+\n+  /**\n+   * Given the result returned by getCounts, determine the threshold for accepting items to\n+   * generate exact sample size.\n+   *\n+   * To do so, we compute sampleSize = math.ceil(size * samplingRate) for each stratum and compare\n+   * it to the number of items that were accepted instantly and the number of items in the waitlist\n+   * for that stratum. Most of the time, numAccepted <= sampleSize <= (numAccepted + numWaitlisted),\n+   * which means we need to sort the elements in the waitlist by their associated values in order\n+   * to find the value T s.t. |{elements in the stratum whose associated values <= T}| = sampleSize.\n+   * Note that all elements in the waitlist have values >= bound for instant accept, so a T value\n+   * in the waitlist range would allow all elements that were instantly accepted on the first pass\n+   * to be included in the sample.\n+   */\n+  def computeThresholdByKey[K](finalResult: Map[K, Stratum],\n+      fractions: Map[K, Double]): Map[K, Double] = {\n+    val thresholdByKey = new HashMap[K, Double]()\n+    for ((key, stratum) <- finalResult) {\n+      val sampleSize = math.ceil(stratum.numItems * fractions(key)).toLong\n+      if (stratum.numAccepted > sampleSize) {\n+        logWarning(\"Pre-accepted too many\")\n+        thresholdByKey += (key -> stratum.acceptBound)\n+      } else {\n+        val numWaitListAccepted = (sampleSize - stratum.numAccepted).toInt\n+        if (numWaitListAccepted >= stratum.waitList.size) {\n+          logWarning(\"WaitList too short\")\n+          thresholdByKey += (key -> stratum.waitListBound)\n+        } else {\n+          thresholdByKey += (key -> stratum.waitList.sorted.apply(numWaitListAccepted))\n+        }\n+      }\n+    }\n+    thresholdByKey\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling without replacement.\n+   *\n+   * When exact sample size is required, we make an additional pass over the RDD to determine the\n+   * exact sampling rate that guarantees sample size with high confidence.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getBernoulliSamplingFunction[K, V](rdd: RDD[(K,  V)],\n+      fractions: Map[K, Double],\n+      exact: Boolean,\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    var samplingRateByKey = fractions\n+    if (exact) {\n+      // determine threshold for each stratum and resample\n+      val finalResult = getCounts(rdd, false, fractions, None, seed)\n+      samplingRateByKey = computeThresholdByKey(finalResult, fractions)\n+    }\n+    (idx: Int, iter: Iterator[(K, V)]) => {\n+      val rng = new RandomDataGenerator\n+      rng.reSeed(seed + idx)\n+      iter.filter(t => rng.nextUniform(0.0, 1.0) < samplingRateByKey(t._1))\n+    }\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling with replacement.\n+   *\n+   * When exact sample size is required, we make two additional passed over the RDD to determine\n+   * the exact sampling rate that guarantees sample size with high confidence. The first pass\n+   * counts the number of items in each stratum (group of items with the same key) in the RDD, and\n+   * the second pass uses the counts to determine exact sampling rates.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getPoissonSamplingFunction[K, V](rdd: RDD[(K, V)],\n+      fractions: Map[K, Double],\n+      exact: Boolean,\n+      counts: Option[Map[K, Long]],\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    // TODO implement the streaming version of sampling w/ replacement that doesn't require counts\n+    if (exact) {\n+      val finalResult = getCounts(rdd, true, fractions, counts, seed)\n+      val thresholdByKey = computeThresholdByKey(finalResult, fractions)\n+      (idx: Int, iter: Iterator[(K, V)]) => {\n+        val rng = new RandomDataGenerator()\n+        rng.reSeed(seed + idx)\n+        iter.flatMap { item =>\n+          val key = item._1\n+          val acceptBound = finalResult(key).acceptBound\n+          val copiesAccepted = if (acceptBound == 0) 0L else rng.nextPoisson(acceptBound)\n+          val copiesWailisted = rng.nextPoisson(finalResult(key).waitListBound).toInt\n+          val copiesInSample = copiesAccepted +\n+            (0 until copiesWailisted).count(i => rng.nextUniform(0.0, 1.0) < thresholdByKey(key))\n+          if (copiesInSample > 0) {\n+            Iterator.fill(copiesInSample.toInt)(item)\n+          } else {\n+            Iterator.empty\n+          }\n+        }\n+      }\n+    } else {\n+      (idx: Int, iter: Iterator[(K, V)]) => {\n+        val rng = new RandomDataGenerator()\n+        rng.reSeed(seed + idx)\n+        iter.flatMap { item =>\n+          val count = rng.nextPoisson(fractions(item._1)).toInt\n+          if (count > 0) {\n+            Iterator.fill(count)(item)\n+          } else {\n+            Iterator.empty\n+          }\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * Object used by seqOp to keep track of the number of items accepted and items waitlisted per\n+ * stratum, as well as the bounds for accepting and waitlisting items.\n+ *\n+ * `[random]` here is necessary since it's in the return type signature of seqOp defined above"
  }],
  "prId": 1025
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "This is not a stratum but something like an `AcceptanceSummary` or `AcceptanceResult`.\n\nThe implementation could be simplified:\n\n1) replace `private val _waitlist` and `def waitlist` by `val waitlist`\n2) define `var acceptBound` and initialize it to `Double.NaN` to avoid possibly boxing/unboxing. Same for `waitlistBound`.\n3) `this.incrNumItems(num)` can be replaced by `this.numItems += num`, which should be concise and sufficient. Same for `incrNumAccepted` and `addToWaitList`.\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-09T05:41:56Z",
    "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.Map\n+import scala.collection.mutable.{ArrayBuffer, HashMap, Map => MMap}\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Auxiliary functions and data structures for the sampleByKey method in PairRDDFunctions.\n+ *\n+ * Essentially, when exact sample size is necessary, we make additional passes over the RDD to\n+ * compute the exact threshold value to use for each stratum to guarantee exact sample size with\n+ * high probability. This is achieved by maintaining a waitlist of size O(log(s)), where s is the\n+ * desired sample size for each stratum.\n+ *\n+ * Like in simple random sampling, we generate a random value for each item from the\n+ * uniform  distribution [0.0, 1.0]. All items with values <= min(values of items in the waitlist)\n+ * are accepted into the sample instantly. The threshold for instant accept is designed so that\n+ * s - numAccepted = O(log(s)), where s is again the desired sample size. Thus, by maintaining a\n+ * waitlist size = O(log(s)), we will be able to create a sample of the exact size s by adding\n+ * a portion of the waitlist to the set of items that are instantly accepted. The exact threshold\n+ * is computed by sorting the values in the waitlist and picking the value at (s - numAccepted).\n+ *\n+ * Note that since we use the same seed for the RNG when computing the thresholds and the actual\n+ * sample, our computed thresholds are guaranteed to produce the desired sample size.\n+ *\n+ * For more theoretical background on the sampling techniques used here, please refer to\n+ * http://jmlr.org/proceedings/papers/v28/meng13a.html\n+ */\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * Count the number of items instantly accepted and generate the waitlist for each stratum.\n+   *\n+   * This is only invoked when exact sample size is required.\n+   */\n+  def getCounts[K, V](rdd: RDD[(K, V)],\n+      withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      counts: Option[Map[K, Long]],\n+      seed: Long): MMap[K, Stratum] = {\n+    val combOp = getCombOp[K]\n+    val mappedPartitionRDD = rdd.mapPartitionsWithIndex({ case (partition, iter) =>\n+      val zeroU: MMap[K, Stratum] = new HashMap[K, Stratum]()\n+      val rng = new RandomDataGenerator()\n+      rng.reSeed(seed + partition)\n+      val seqOp = getSeqOp(withReplacement, fractions, rng, counts)\n+      Iterator(iter.aggregate(zeroU)(seqOp, combOp))\n+    }, preservesPartitioning=true)\n+    mappedPartitionRDD.reduce(combOp)\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      rng: RandomDataGenerator,\n+      counts: Option[Map[K, Long]]): (MMap[K, Stratum], (K, V)) => MMap[K, Stratum] = {\n+    val delta = 5e-5\n+    (result: MMap[K, Stratum], item: (K, V)) => {\n+      val key = item._1\n+      val fraction = fractions(key)\n+      if (!result.contains(key)) {\n+        result += (key -> new Stratum())\n+      }\n+      val stratum = result(key)\n+\n+      if (withReplacement) {\n+        // compute acceptBound and waitListBound only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.areBoundsEmpty) {\n+          val n = counts.get(key)\n+          val sampleSize = math.ceil(n * fraction).toLong\n+          val lmbd1 = PoissonBounds.getLowerBound(sampleSize)\n+          val minCount = PoissonBounds.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) {\n+            PoissonBounds.getUpperBound(sampleSize)\n+          } else {\n+            PoissonBounds.getUpperBound(sampleSize - minCount)\n+          }\n+          stratum.acceptBound = lmbd1 / n\n+          stratum.waitListBound = lmbd2 / n\n+        }\n+        val acceptBound = stratum.acceptBound\n+        val copiesAccepted = if (acceptBound == 0.0) 0L else rng.nextPoisson(acceptBound)\n+        if (copiesAccepted > 0) {\n+          stratum.incrNumAccepted(copiesAccepted)\n+        }\n+        val copiesWaitlisted = rng.nextPoisson(stratum.waitListBound).toInt\n+        if (copiesWaitlisted > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(copiesWaitlisted)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement to avoid\n+        // using an extra pass over the RDD for computing the count.\n+        // Hence, acceptBound and waitListBound change on every iteration.\n+        val gamma1 = - math.log(delta) / stratum.numItems\n+        val gamma2 = (2.0 / 3.0) * gamma1\n+        stratum.acceptBound = math.max(0,\n+          fraction + gamma2 - math.sqrt(gamma2 * gamma2 + 3 * gamma2 * fraction))\n+        stratum.waitListBound = math.min(1,\n+          fraction + gamma1 + math.sqrt(gamma1 * gamma1 + 2 * gamma1 * fraction))\n+\n+        val x = rng.nextUniform(0.0, 1.0)\n+        if (x < stratum.acceptBound) {\n+          stratum.incrNumAccepted()\n+        } else if (x < stratum.waitListBound) {\n+          stratum.addToWaitList(x)\n+        }\n+      }\n+      stratum.incrNumItems()\n+      result\n+    }\n+  }\n+\n+  /**\n+   * Returns the function used combine results returned by seqOp from different partitions.\n+   */\n+  def getCombOp[K]: (MMap[K, Stratum], MMap[K, Stratum]) => MMap[K, Stratum] = {\n+    (result1: MMap[K, Stratum], result2: MMap[K, Stratum]) => {\n+      // take union of both key sets in case one partition doesn't contain all keys\n+      for (key <- result1.keySet.union(result2.keySet)) {\n+        // Use result2 to keep the combined result since r1 is usual empty\n+        val entry1 = result1.get(key)\n+        if (result2.contains(key)) {\n+          result2(key).merge(entry1)\n+        } else {\n+          if (entry1.isDefined) {\n+            result2 += (key -> entry1.get)\n+          }\n+        }\n+      }\n+      result2\n+    }\n+  }\n+\n+  /**\n+   * Given the result returned by getCounts, determine the threshold for accepting items to\n+   * generate exact sample size.\n+   *\n+   * To do so, we compute sampleSize = math.ceil(size * samplingRate) for each stratum and compare\n+   * it to the number of items that were accepted instantly and the number of items in the waitlist\n+   * for that stratum. Most of the time, numAccepted <= sampleSize <= (numAccepted + numWaitlisted),\n+   * which means we need to sort the elements in the waitlist by their associated values in order\n+   * to find the value T s.t. |{elements in the stratum whose associated values <= T}| = sampleSize.\n+   * Note that all elements in the waitlist have values >= bound for instant accept, so a T value\n+   * in the waitlist range would allow all elements that were instantly accepted on the first pass\n+   * to be included in the sample.\n+   */\n+  def computeThresholdByKey[K](finalResult: Map[K, Stratum],\n+      fractions: Map[K, Double]): Map[K, Double] = {\n+    val thresholdByKey = new HashMap[K, Double]()\n+    for ((key, stratum) <- finalResult) {\n+      val sampleSize = math.ceil(stratum.numItems * fractions(key)).toLong\n+      if (stratum.numAccepted > sampleSize) {\n+        logWarning(\"Pre-accepted too many\")\n+        thresholdByKey += (key -> stratum.acceptBound)\n+      } else {\n+        val numWaitListAccepted = (sampleSize - stratum.numAccepted).toInt\n+        if (numWaitListAccepted >= stratum.waitList.size) {\n+          logWarning(\"WaitList too short\")\n+          thresholdByKey += (key -> stratum.waitListBound)\n+        } else {\n+          thresholdByKey += (key -> stratum.waitList.sorted.apply(numWaitListAccepted))\n+        }\n+      }\n+    }\n+    thresholdByKey\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling without replacement.\n+   *\n+   * When exact sample size is required, we make an additional pass over the RDD to determine the\n+   * exact sampling rate that guarantees sample size with high confidence.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getBernoulliSamplingFunction[K, V](rdd: RDD[(K,  V)],\n+      fractions: Map[K, Double],\n+      exact: Boolean,\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    var samplingRateByKey = fractions\n+    if (exact) {\n+      // determine threshold for each stratum and resample\n+      val finalResult = getCounts(rdd, false, fractions, None, seed)\n+      samplingRateByKey = computeThresholdByKey(finalResult, fractions)\n+    }\n+    (idx: Int, iter: Iterator[(K, V)]) => {\n+      val rng = new RandomDataGenerator\n+      rng.reSeed(seed + idx)\n+      iter.filter(t => rng.nextUniform(0.0, 1.0) < samplingRateByKey(t._1))\n+    }\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling with replacement.\n+   *\n+   * When exact sample size is required, we make two additional passed over the RDD to determine\n+   * the exact sampling rate that guarantees sample size with high confidence. The first pass\n+   * counts the number of items in each stratum (group of items with the same key) in the RDD, and\n+   * the second pass uses the counts to determine exact sampling rates.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getPoissonSamplingFunction[K, V](rdd: RDD[(K, V)],\n+      fractions: Map[K, Double],\n+      exact: Boolean,\n+      counts: Option[Map[K, Long]],\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    // TODO implement the streaming version of sampling w/ replacement that doesn't require counts\n+    if (exact) {\n+      val finalResult = getCounts(rdd, true, fractions, counts, seed)\n+      val thresholdByKey = computeThresholdByKey(finalResult, fractions)\n+      (idx: Int, iter: Iterator[(K, V)]) => {\n+        val rng = new RandomDataGenerator()\n+        rng.reSeed(seed + idx)\n+        iter.flatMap { item =>\n+          val key = item._1\n+          val acceptBound = finalResult(key).acceptBound\n+          val copiesAccepted = if (acceptBound == 0) 0L else rng.nextPoisson(acceptBound)\n+          val copiesWailisted = rng.nextPoisson(finalResult(key).waitListBound).toInt\n+          val copiesInSample = copiesAccepted +\n+            (0 until copiesWailisted).count(i => rng.nextUniform(0.0, 1.0) < thresholdByKey(key))\n+          if (copiesInSample > 0) {\n+            Iterator.fill(copiesInSample.toInt)(item)\n+          } else {\n+            Iterator.empty\n+          }\n+        }\n+      }\n+    } else {\n+      (idx: Int, iter: Iterator[(K, V)]) => {\n+        val rng = new RandomDataGenerator()\n+        rng.reSeed(seed + idx)\n+        iter.flatMap { item =>\n+          val count = rng.nextPoisson(fractions(item._1)).toInt\n+          if (count > 0) {\n+            Iterator.fill(count)(item)\n+          } else {\n+            Iterator.empty\n+          }\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * Object used by seqOp to keep track of the number of items accepted and items waitlisted per\n+ * stratum, as well as the bounds for accepting and waitlisting items.\n+ *\n+ * `[random]` here is necessary since it's in the return type signature of seqOp defined above\n+ */\n+private[random] class Stratum(var numItems: Long = 0L, var numAccepted: Long = 0L)"
  }],
  "prId": 1025
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Any chance of merging a `None`?\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-09T05:41:59Z",
    "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.Map\n+import scala.collection.mutable.{ArrayBuffer, HashMap, Map => MMap}\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Auxiliary functions and data structures for the sampleByKey method in PairRDDFunctions.\n+ *\n+ * Essentially, when exact sample size is necessary, we make additional passes over the RDD to\n+ * compute the exact threshold value to use for each stratum to guarantee exact sample size with\n+ * high probability. This is achieved by maintaining a waitlist of size O(log(s)), where s is the\n+ * desired sample size for each stratum.\n+ *\n+ * Like in simple random sampling, we generate a random value for each item from the\n+ * uniform  distribution [0.0, 1.0]. All items with values <= min(values of items in the waitlist)\n+ * are accepted into the sample instantly. The threshold for instant accept is designed so that\n+ * s - numAccepted = O(log(s)), where s is again the desired sample size. Thus, by maintaining a\n+ * waitlist size = O(log(s)), we will be able to create a sample of the exact size s by adding\n+ * a portion of the waitlist to the set of items that are instantly accepted. The exact threshold\n+ * is computed by sorting the values in the waitlist and picking the value at (s - numAccepted).\n+ *\n+ * Note that since we use the same seed for the RNG when computing the thresholds and the actual\n+ * sample, our computed thresholds are guaranteed to produce the desired sample size.\n+ *\n+ * For more theoretical background on the sampling techniques used here, please refer to\n+ * http://jmlr.org/proceedings/papers/v28/meng13a.html\n+ */\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * Count the number of items instantly accepted and generate the waitlist for each stratum.\n+   *\n+   * This is only invoked when exact sample size is required.\n+   */\n+  def getCounts[K, V](rdd: RDD[(K, V)],\n+      withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      counts: Option[Map[K, Long]],\n+      seed: Long): MMap[K, Stratum] = {\n+    val combOp = getCombOp[K]\n+    val mappedPartitionRDD = rdd.mapPartitionsWithIndex({ case (partition, iter) =>\n+      val zeroU: MMap[K, Stratum] = new HashMap[K, Stratum]()\n+      val rng = new RandomDataGenerator()\n+      rng.reSeed(seed + partition)\n+      val seqOp = getSeqOp(withReplacement, fractions, rng, counts)\n+      Iterator(iter.aggregate(zeroU)(seqOp, combOp))\n+    }, preservesPartitioning=true)\n+    mappedPartitionRDD.reduce(combOp)\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      rng: RandomDataGenerator,\n+      counts: Option[Map[K, Long]]): (MMap[K, Stratum], (K, V)) => MMap[K, Stratum] = {\n+    val delta = 5e-5\n+    (result: MMap[K, Stratum], item: (K, V)) => {\n+      val key = item._1\n+      val fraction = fractions(key)\n+      if (!result.contains(key)) {\n+        result += (key -> new Stratum())\n+      }\n+      val stratum = result(key)\n+\n+      if (withReplacement) {\n+        // compute acceptBound and waitListBound only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.areBoundsEmpty) {\n+          val n = counts.get(key)\n+          val sampleSize = math.ceil(n * fraction).toLong\n+          val lmbd1 = PoissonBounds.getLowerBound(sampleSize)\n+          val minCount = PoissonBounds.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) {\n+            PoissonBounds.getUpperBound(sampleSize)\n+          } else {\n+            PoissonBounds.getUpperBound(sampleSize - minCount)\n+          }\n+          stratum.acceptBound = lmbd1 / n\n+          stratum.waitListBound = lmbd2 / n\n+        }\n+        val acceptBound = stratum.acceptBound\n+        val copiesAccepted = if (acceptBound == 0.0) 0L else rng.nextPoisson(acceptBound)\n+        if (copiesAccepted > 0) {\n+          stratum.incrNumAccepted(copiesAccepted)\n+        }\n+        val copiesWaitlisted = rng.nextPoisson(stratum.waitListBound).toInt\n+        if (copiesWaitlisted > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(copiesWaitlisted)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement to avoid\n+        // using an extra pass over the RDD for computing the count.\n+        // Hence, acceptBound and waitListBound change on every iteration.\n+        val gamma1 = - math.log(delta) / stratum.numItems\n+        val gamma2 = (2.0 / 3.0) * gamma1\n+        stratum.acceptBound = math.max(0,\n+          fraction + gamma2 - math.sqrt(gamma2 * gamma2 + 3 * gamma2 * fraction))\n+        stratum.waitListBound = math.min(1,\n+          fraction + gamma1 + math.sqrt(gamma1 * gamma1 + 2 * gamma1 * fraction))\n+\n+        val x = rng.nextUniform(0.0, 1.0)\n+        if (x < stratum.acceptBound) {\n+          stratum.incrNumAccepted()\n+        } else if (x < stratum.waitListBound) {\n+          stratum.addToWaitList(x)\n+        }\n+      }\n+      stratum.incrNumItems()\n+      result\n+    }\n+  }\n+\n+  /**\n+   * Returns the function used combine results returned by seqOp from different partitions.\n+   */\n+  def getCombOp[K]: (MMap[K, Stratum], MMap[K, Stratum]) => MMap[K, Stratum] = {\n+    (result1: MMap[K, Stratum], result2: MMap[K, Stratum]) => {\n+      // take union of both key sets in case one partition doesn't contain all keys\n+      for (key <- result1.keySet.union(result2.keySet)) {\n+        // Use result2 to keep the combined result since r1 is usual empty\n+        val entry1 = result1.get(key)\n+        if (result2.contains(key)) {\n+          result2(key).merge(entry1)\n+        } else {\n+          if (entry1.isDefined) {\n+            result2 += (key -> entry1.get)\n+          }\n+        }\n+      }\n+      result2\n+    }\n+  }\n+\n+  /**\n+   * Given the result returned by getCounts, determine the threshold for accepting items to\n+   * generate exact sample size.\n+   *\n+   * To do so, we compute sampleSize = math.ceil(size * samplingRate) for each stratum and compare\n+   * it to the number of items that were accepted instantly and the number of items in the waitlist\n+   * for that stratum. Most of the time, numAccepted <= sampleSize <= (numAccepted + numWaitlisted),\n+   * which means we need to sort the elements in the waitlist by their associated values in order\n+   * to find the value T s.t. |{elements in the stratum whose associated values <= T}| = sampleSize.\n+   * Note that all elements in the waitlist have values >= bound for instant accept, so a T value\n+   * in the waitlist range would allow all elements that were instantly accepted on the first pass\n+   * to be included in the sample.\n+   */\n+  def computeThresholdByKey[K](finalResult: Map[K, Stratum],\n+      fractions: Map[K, Double]): Map[K, Double] = {\n+    val thresholdByKey = new HashMap[K, Double]()\n+    for ((key, stratum) <- finalResult) {\n+      val sampleSize = math.ceil(stratum.numItems * fractions(key)).toLong\n+      if (stratum.numAccepted > sampleSize) {\n+        logWarning(\"Pre-accepted too many\")\n+        thresholdByKey += (key -> stratum.acceptBound)\n+      } else {\n+        val numWaitListAccepted = (sampleSize - stratum.numAccepted).toInt\n+        if (numWaitListAccepted >= stratum.waitList.size) {\n+          logWarning(\"WaitList too short\")\n+          thresholdByKey += (key -> stratum.waitListBound)\n+        } else {\n+          thresholdByKey += (key -> stratum.waitList.sorted.apply(numWaitListAccepted))\n+        }\n+      }\n+    }\n+    thresholdByKey\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling without replacement.\n+   *\n+   * When exact sample size is required, we make an additional pass over the RDD to determine the\n+   * exact sampling rate that guarantees sample size with high confidence.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getBernoulliSamplingFunction[K, V](rdd: RDD[(K,  V)],\n+      fractions: Map[K, Double],\n+      exact: Boolean,\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    var samplingRateByKey = fractions\n+    if (exact) {\n+      // determine threshold for each stratum and resample\n+      val finalResult = getCounts(rdd, false, fractions, None, seed)\n+      samplingRateByKey = computeThresholdByKey(finalResult, fractions)\n+    }\n+    (idx: Int, iter: Iterator[(K, V)]) => {\n+      val rng = new RandomDataGenerator\n+      rng.reSeed(seed + idx)\n+      iter.filter(t => rng.nextUniform(0.0, 1.0) < samplingRateByKey(t._1))\n+    }\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling with replacement.\n+   *\n+   * When exact sample size is required, we make two additional passed over the RDD to determine\n+   * the exact sampling rate that guarantees sample size with high confidence. The first pass\n+   * counts the number of items in each stratum (group of items with the same key) in the RDD, and\n+   * the second pass uses the counts to determine exact sampling rates.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getPoissonSamplingFunction[K, V](rdd: RDD[(K, V)],\n+      fractions: Map[K, Double],\n+      exact: Boolean,\n+      counts: Option[Map[K, Long]],\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    // TODO implement the streaming version of sampling w/ replacement that doesn't require counts\n+    if (exact) {\n+      val finalResult = getCounts(rdd, true, fractions, counts, seed)\n+      val thresholdByKey = computeThresholdByKey(finalResult, fractions)\n+      (idx: Int, iter: Iterator[(K, V)]) => {\n+        val rng = new RandomDataGenerator()\n+        rng.reSeed(seed + idx)\n+        iter.flatMap { item =>\n+          val key = item._1\n+          val acceptBound = finalResult(key).acceptBound\n+          val copiesAccepted = if (acceptBound == 0) 0L else rng.nextPoisson(acceptBound)\n+          val copiesWailisted = rng.nextPoisson(finalResult(key).waitListBound).toInt\n+          val copiesInSample = copiesAccepted +\n+            (0 until copiesWailisted).count(i => rng.nextUniform(0.0, 1.0) < thresholdByKey(key))\n+          if (copiesInSample > 0) {\n+            Iterator.fill(copiesInSample.toInt)(item)\n+          } else {\n+            Iterator.empty\n+          }\n+        }\n+      }\n+    } else {\n+      (idx: Int, iter: Iterator[(K, V)]) => {\n+        val rng = new RandomDataGenerator()\n+        rng.reSeed(seed + idx)\n+        iter.flatMap { item =>\n+          val count = rng.nextPoisson(fractions(item._1)).toInt\n+          if (count > 0) {\n+            Iterator.fill(count)(item)\n+          } else {\n+            Iterator.empty\n+          }\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * Object used by seqOp to keep track of the number of items accepted and items waitlisted per\n+ * stratum, as well as the bounds for accepting and waitlisting items.\n+ *\n+ * `[random]` here is necessary since it's in the return type signature of seqOp defined above\n+ */\n+private[random] class Stratum(var numItems: Long = 0L, var numAccepted: Long = 0L)\n+  extends Serializable {\n+\n+  private val _waitList = new ArrayBuffer[Double]\n+  private var _acceptBound: Option[Double] = None // upper bound for accepting item instantly\n+  private var _waitListBound: Option[Double] = None // upper bound for adding item to waitlist\n+\n+  def incrNumItems(by: Long = 1L) = numItems += by\n+\n+  def incrNumAccepted(by: Long = 1L) = numAccepted += by\n+\n+  def addToWaitList(elem: Double) = _waitList += elem\n+\n+  def addToWaitList(elems: ArrayBuffer[Double]) = _waitList ++= elems\n+\n+  def waitList = _waitList\n+\n+  def acceptBound = _acceptBound.getOrElse(0.0)\n+\n+  def acceptBound_= (value: Double) = _acceptBound = Some(value)\n+\n+  def waitListBound = _waitListBound.getOrElse(0.0)\n+\n+  def waitListBound_= (value: Double) = _waitListBound = Some(value)\n+\n+  def areBoundsEmpty = _acceptBound.isEmpty || _waitListBound.isEmpty\n+\n+  def merge(other: Option[Stratum]): Unit = {"
  }, {
    "author": {
      "login": "dorx"
    },
    "body": "Nope.\n",
    "commit": "245439ef18393d814821f4c1d445aee8cc774f09",
    "createdAt": "2014-07-14T22:18:22Z",
    "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.random\n+\n+import scala.collection.Map\n+import scala.collection.mutable.{ArrayBuffer, HashMap, Map => MMap}\n+\n+import org.apache.commons.math3.random.RandomDataGenerator\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Auxiliary functions and data structures for the sampleByKey method in PairRDDFunctions.\n+ *\n+ * Essentially, when exact sample size is necessary, we make additional passes over the RDD to\n+ * compute the exact threshold value to use for each stratum to guarantee exact sample size with\n+ * high probability. This is achieved by maintaining a waitlist of size O(log(s)), where s is the\n+ * desired sample size for each stratum.\n+ *\n+ * Like in simple random sampling, we generate a random value for each item from the\n+ * uniform  distribution [0.0, 1.0]. All items with values <= min(values of items in the waitlist)\n+ * are accepted into the sample instantly. The threshold for instant accept is designed so that\n+ * s - numAccepted = O(log(s)), where s is again the desired sample size. Thus, by maintaining a\n+ * waitlist size = O(log(s)), we will be able to create a sample of the exact size s by adding\n+ * a portion of the waitlist to the set of items that are instantly accepted. The exact threshold\n+ * is computed by sorting the values in the waitlist and picking the value at (s - numAccepted).\n+ *\n+ * Note that since we use the same seed for the RNG when computing the thresholds and the actual\n+ * sample, our computed thresholds are guaranteed to produce the desired sample size.\n+ *\n+ * For more theoretical background on the sampling techniques used here, please refer to\n+ * http://jmlr.org/proceedings/papers/v28/meng13a.html\n+ */\n+\n+private[spark] object StratifiedSampler extends Logging {\n+\n+  /**\n+   * Count the number of items instantly accepted and generate the waitlist for each stratum.\n+   *\n+   * This is only invoked when exact sample size is required.\n+   */\n+  def getCounts[K, V](rdd: RDD[(K, V)],\n+      withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      counts: Option[Map[K, Long]],\n+      seed: Long): MMap[K, Stratum] = {\n+    val combOp = getCombOp[K]\n+    val mappedPartitionRDD = rdd.mapPartitionsWithIndex({ case (partition, iter) =>\n+      val zeroU: MMap[K, Stratum] = new HashMap[K, Stratum]()\n+      val rng = new RandomDataGenerator()\n+      rng.reSeed(seed + partition)\n+      val seqOp = getSeqOp(withReplacement, fractions, rng, counts)\n+      Iterator(iter.aggregate(zeroU)(seqOp, combOp))\n+    }, preservesPartitioning=true)\n+    mappedPartitionRDD.reduce(combOp)\n+  }\n+\n+  /**\n+   * Returns the function used by aggregate to collect sampling statistics for each partition.\n+   */\n+  def getSeqOp[K, V](withReplacement: Boolean,\n+      fractions: Map[K, Double],\n+      rng: RandomDataGenerator,\n+      counts: Option[Map[K, Long]]): (MMap[K, Stratum], (K, V)) => MMap[K, Stratum] = {\n+    val delta = 5e-5\n+    (result: MMap[K, Stratum], item: (K, V)) => {\n+      val key = item._1\n+      val fraction = fractions(key)\n+      if (!result.contains(key)) {\n+        result += (key -> new Stratum())\n+      }\n+      val stratum = result(key)\n+\n+      if (withReplacement) {\n+        // compute acceptBound and waitListBound only if they haven't been computed already\n+        // since they don't change from iteration to iteration.\n+        // TODO change this to the streaming version\n+        if (stratum.areBoundsEmpty) {\n+          val n = counts.get(key)\n+          val sampleSize = math.ceil(n * fraction).toLong\n+          val lmbd1 = PoissonBounds.getLowerBound(sampleSize)\n+          val minCount = PoissonBounds.getMinCount(lmbd1)\n+          val lmbd2 = if (lmbd1 == 0) {\n+            PoissonBounds.getUpperBound(sampleSize)\n+          } else {\n+            PoissonBounds.getUpperBound(sampleSize - minCount)\n+          }\n+          stratum.acceptBound = lmbd1 / n\n+          stratum.waitListBound = lmbd2 / n\n+        }\n+        val acceptBound = stratum.acceptBound\n+        val copiesAccepted = if (acceptBound == 0.0) 0L else rng.nextPoisson(acceptBound)\n+        if (copiesAccepted > 0) {\n+          stratum.incrNumAccepted(copiesAccepted)\n+        }\n+        val copiesWaitlisted = rng.nextPoisson(stratum.waitListBound).toInt\n+        if (copiesWaitlisted > 0) {\n+          stratum.addToWaitList(ArrayBuffer.fill(copiesWaitlisted)(rng.nextUniform(0.0, 1.0)))\n+        }\n+      } else {\n+        // We use the streaming version of the algorithm for sampling without replacement to avoid\n+        // using an extra pass over the RDD for computing the count.\n+        // Hence, acceptBound and waitListBound change on every iteration.\n+        val gamma1 = - math.log(delta) / stratum.numItems\n+        val gamma2 = (2.0 / 3.0) * gamma1\n+        stratum.acceptBound = math.max(0,\n+          fraction + gamma2 - math.sqrt(gamma2 * gamma2 + 3 * gamma2 * fraction))\n+        stratum.waitListBound = math.min(1,\n+          fraction + gamma1 + math.sqrt(gamma1 * gamma1 + 2 * gamma1 * fraction))\n+\n+        val x = rng.nextUniform(0.0, 1.0)\n+        if (x < stratum.acceptBound) {\n+          stratum.incrNumAccepted()\n+        } else if (x < stratum.waitListBound) {\n+          stratum.addToWaitList(x)\n+        }\n+      }\n+      stratum.incrNumItems()\n+      result\n+    }\n+  }\n+\n+  /**\n+   * Returns the function used combine results returned by seqOp from different partitions.\n+   */\n+  def getCombOp[K]: (MMap[K, Stratum], MMap[K, Stratum]) => MMap[K, Stratum] = {\n+    (result1: MMap[K, Stratum], result2: MMap[K, Stratum]) => {\n+      // take union of both key sets in case one partition doesn't contain all keys\n+      for (key <- result1.keySet.union(result2.keySet)) {\n+        // Use result2 to keep the combined result since r1 is usual empty\n+        val entry1 = result1.get(key)\n+        if (result2.contains(key)) {\n+          result2(key).merge(entry1)\n+        } else {\n+          if (entry1.isDefined) {\n+            result2 += (key -> entry1.get)\n+          }\n+        }\n+      }\n+      result2\n+    }\n+  }\n+\n+  /**\n+   * Given the result returned by getCounts, determine the threshold for accepting items to\n+   * generate exact sample size.\n+   *\n+   * To do so, we compute sampleSize = math.ceil(size * samplingRate) for each stratum and compare\n+   * it to the number of items that were accepted instantly and the number of items in the waitlist\n+   * for that stratum. Most of the time, numAccepted <= sampleSize <= (numAccepted + numWaitlisted),\n+   * which means we need to sort the elements in the waitlist by their associated values in order\n+   * to find the value T s.t. |{elements in the stratum whose associated values <= T}| = sampleSize.\n+   * Note that all elements in the waitlist have values >= bound for instant accept, so a T value\n+   * in the waitlist range would allow all elements that were instantly accepted on the first pass\n+   * to be included in the sample.\n+   */\n+  def computeThresholdByKey[K](finalResult: Map[K, Stratum],\n+      fractions: Map[K, Double]): Map[K, Double] = {\n+    val thresholdByKey = new HashMap[K, Double]()\n+    for ((key, stratum) <- finalResult) {\n+      val sampleSize = math.ceil(stratum.numItems * fractions(key)).toLong\n+      if (stratum.numAccepted > sampleSize) {\n+        logWarning(\"Pre-accepted too many\")\n+        thresholdByKey += (key -> stratum.acceptBound)\n+      } else {\n+        val numWaitListAccepted = (sampleSize - stratum.numAccepted).toInt\n+        if (numWaitListAccepted >= stratum.waitList.size) {\n+          logWarning(\"WaitList too short\")\n+          thresholdByKey += (key -> stratum.waitListBound)\n+        } else {\n+          thresholdByKey += (key -> stratum.waitList.sorted.apply(numWaitListAccepted))\n+        }\n+      }\n+    }\n+    thresholdByKey\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling without replacement.\n+   *\n+   * When exact sample size is required, we make an additional pass over the RDD to determine the\n+   * exact sampling rate that guarantees sample size with high confidence.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getBernoulliSamplingFunction[K, V](rdd: RDD[(K,  V)],\n+      fractions: Map[K, Double],\n+      exact: Boolean,\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    var samplingRateByKey = fractions\n+    if (exact) {\n+      // determine threshold for each stratum and resample\n+      val finalResult = getCounts(rdd, false, fractions, None, seed)\n+      samplingRateByKey = computeThresholdByKey(finalResult, fractions)\n+    }\n+    (idx: Int, iter: Iterator[(K, V)]) => {\n+      val rng = new RandomDataGenerator\n+      rng.reSeed(seed + idx)\n+      iter.filter(t => rng.nextUniform(0.0, 1.0) < samplingRateByKey(t._1))\n+    }\n+  }\n+\n+  /**\n+   * Return the per partition sampling function used for sampling with replacement.\n+   *\n+   * When exact sample size is required, we make two additional passed over the RDD to determine\n+   * the exact sampling rate that guarantees sample size with high confidence. The first pass\n+   * counts the number of items in each stratum (group of items with the same key) in the RDD, and\n+   * the second pass uses the counts to determine exact sampling rates.\n+   *\n+   * The sampling function has a unique seed per partition.\n+   */\n+  def getPoissonSamplingFunction[K, V](rdd: RDD[(K, V)],\n+      fractions: Map[K, Double],\n+      exact: Boolean,\n+      counts: Option[Map[K, Long]],\n+      seed: Long): (Int, Iterator[(K, V)]) => Iterator[(K, V)] = {\n+    // TODO implement the streaming version of sampling w/ replacement that doesn't require counts\n+    if (exact) {\n+      val finalResult = getCounts(rdd, true, fractions, counts, seed)\n+      val thresholdByKey = computeThresholdByKey(finalResult, fractions)\n+      (idx: Int, iter: Iterator[(K, V)]) => {\n+        val rng = new RandomDataGenerator()\n+        rng.reSeed(seed + idx)\n+        iter.flatMap { item =>\n+          val key = item._1\n+          val acceptBound = finalResult(key).acceptBound\n+          val copiesAccepted = if (acceptBound == 0) 0L else rng.nextPoisson(acceptBound)\n+          val copiesWailisted = rng.nextPoisson(finalResult(key).waitListBound).toInt\n+          val copiesInSample = copiesAccepted +\n+            (0 until copiesWailisted).count(i => rng.nextUniform(0.0, 1.0) < thresholdByKey(key))\n+          if (copiesInSample > 0) {\n+            Iterator.fill(copiesInSample.toInt)(item)\n+          } else {\n+            Iterator.empty\n+          }\n+        }\n+      }\n+    } else {\n+      (idx: Int, iter: Iterator[(K, V)]) => {\n+        val rng = new RandomDataGenerator()\n+        rng.reSeed(seed + idx)\n+        iter.flatMap { item =>\n+          val count = rng.nextPoisson(fractions(item._1)).toInt\n+          if (count > 0) {\n+            Iterator.fill(count)(item)\n+          } else {\n+            Iterator.empty\n+          }\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * Object used by seqOp to keep track of the number of items accepted and items waitlisted per\n+ * stratum, as well as the bounds for accepting and waitlisting items.\n+ *\n+ * `[random]` here is necessary since it's in the return type signature of seqOp defined above\n+ */\n+private[random] class Stratum(var numItems: Long = 0L, var numAccepted: Long = 0L)\n+  extends Serializable {\n+\n+  private val _waitList = new ArrayBuffer[Double]\n+  private var _acceptBound: Option[Double] = None // upper bound for accepting item instantly\n+  private var _waitListBound: Option[Double] = None // upper bound for adding item to waitlist\n+\n+  def incrNumItems(by: Long = 1L) = numItems += by\n+\n+  def incrNumAccepted(by: Long = 1L) = numAccepted += by\n+\n+  def addToWaitList(elem: Double) = _waitList += elem\n+\n+  def addToWaitList(elems: ArrayBuffer[Double]) = _waitList ++= elems\n+\n+  def waitList = _waitList\n+\n+  def acceptBound = _acceptBound.getOrElse(0.0)\n+\n+  def acceptBound_= (value: Double) = _acceptBound = Some(value)\n+\n+  def waitListBound = _waitListBound.getOrElse(0.0)\n+\n+  def waitListBound_= (value: Double) = _waitListBound = Some(value)\n+\n+  def areBoundsEmpty = _acceptBound.isEmpty || _waitListBound.isEmpty\n+\n+  def merge(other: Option[Stratum]): Unit = {"
  }],
  "prId": 1025
}]