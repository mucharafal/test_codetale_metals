[{
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "we should document what dispose means. does it mean releasing the buffer upon close? why would we ever want to do that?\n",
    "commit": "2970932bf11ab9cfc8cac33c05a13912ddb345d4",
    "createdAt": "2016-03-16T05:34:20Z",
    "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.io\n+\n+import java.io.InputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.WritableByteChannel\n+\n+import com.google.common.primitives.UnsignedBytes\n+import io.netty.buffer.{ByteBuf, Unpooled}\n+\n+import org.apache.spark.network.util.ByteArrayWritableChannel\n+import org.apache.spark.storage.BlockManager\n+\n+private[spark] class ChunkedByteBuffer(var chunks: Array[ByteBuffer]) {\n+  require(chunks != null, \"chunks must not be null\")\n+  require(chunks.nonEmpty, \"Cannot create a ChunkedByteBuffer with no chunks\")\n+  require(chunks.forall(_.limit() > 0), \"chunks must be non-empty\")\n+  require(chunks.forall(_.position() == 0), \"chunks' positions must be 0\")\n+\n+  val limit: Long = chunks.map(_.limit().asInstanceOf[Long]).sum\n+\n+  def this(byteBuffer: ByteBuffer) = {\n+    this(Array(byteBuffer))\n+  }\n+\n+  def writeFully(channel: WritableByteChannel): Unit = {\n+    for (bytes <- getChunks()) {\n+      while (bytes.remaining > 0) {\n+        channel.write(bytes)\n+      }\n+    }\n+  }\n+\n+  def toNetty: ByteBuf = {\n+    Unpooled.wrappedBuffer(getChunks(): _*)\n+  }\n+\n+  def toArray: Array[Byte] = {\n+    if (limit >= Integer.MAX_VALUE) {\n+      throw new UnsupportedOperationException(\n+        s\"cannot call toArray because buffer size ($limit bytes) exceeds maximum array size\")\n+    }\n+    val byteChannel = new ByteArrayWritableChannel(limit.toInt)\n+    writeFully(byteChannel)\n+    byteChannel.close()\n+    byteChannel.getData\n+  }\n+\n+  def toByteBuffer: ByteBuffer = {\n+    if (chunks.length == 1) {\n+      chunks.head\n+    } else {\n+      ByteBuffer.wrap(toArray)\n+    }\n+  }\n+\n+  def toInputStream(dispose: Boolean = false): InputStream = {\n+    new ChunkedByteBufferInputStream(this, dispose)\n+  }\n+\n+  def getChunks(): Array[ByteBuffer] = {\n+    chunks.map(_.duplicate())\n+  }\n+\n+  def copy(): ChunkedByteBuffer = {\n+    val copiedChunks = getChunks().map { chunk =>\n+      // TODO: accept an allocator in this copy method to integrate with mem. accounting systems\n+      val newChunk = ByteBuffer.allocate(chunk.limit())\n+      newChunk.put(chunk)\n+      newChunk.flip()\n+      newChunk\n+    }\n+    new ChunkedByteBuffer(copiedChunks)\n+  }\n+\n+  /**\n+   * Attempt to clean up a ByteBuffer if it is memory-mapped. This uses an *unsafe* Sun API that\n+   * might cause errors if one attempts to read from the unmapped buffer, but it's better than\n+   * waiting for the GC to find it because that could lead to huge numbers of open files. There's\n+   * unfortunately no standard API to do this.\n+   */\n+  def dispose(): Unit = {\n+    chunks.foreach(BlockManager.dispose)\n+  }\n+}\n+\n+/**\n+ * Reads data from a ChunkedByteBuffer, and optionally cleans it up using BlockManager.dispose()\n+ * at the end of the stream (e.g. to close a memory-mapped file).\n+ */\n+private class ChunkedByteBufferInputStream(\n+    var chunkedByteBuffer: ChunkedByteBuffer,\n+    dispose: Boolean)"
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "never mind the close part makes sense. just document dispose.\n",
    "commit": "2970932bf11ab9cfc8cac33c05a13912ddb345d4",
    "createdAt": "2016-03-16T05:35:56Z",
    "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.io\n+\n+import java.io.InputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.WritableByteChannel\n+\n+import com.google.common.primitives.UnsignedBytes\n+import io.netty.buffer.{ByteBuf, Unpooled}\n+\n+import org.apache.spark.network.util.ByteArrayWritableChannel\n+import org.apache.spark.storage.BlockManager\n+\n+private[spark] class ChunkedByteBuffer(var chunks: Array[ByteBuffer]) {\n+  require(chunks != null, \"chunks must not be null\")\n+  require(chunks.nonEmpty, \"Cannot create a ChunkedByteBuffer with no chunks\")\n+  require(chunks.forall(_.limit() > 0), \"chunks must be non-empty\")\n+  require(chunks.forall(_.position() == 0), \"chunks' positions must be 0\")\n+\n+  val limit: Long = chunks.map(_.limit().asInstanceOf[Long]).sum\n+\n+  def this(byteBuffer: ByteBuffer) = {\n+    this(Array(byteBuffer))\n+  }\n+\n+  def writeFully(channel: WritableByteChannel): Unit = {\n+    for (bytes <- getChunks()) {\n+      while (bytes.remaining > 0) {\n+        channel.write(bytes)\n+      }\n+    }\n+  }\n+\n+  def toNetty: ByteBuf = {\n+    Unpooled.wrappedBuffer(getChunks(): _*)\n+  }\n+\n+  def toArray: Array[Byte] = {\n+    if (limit >= Integer.MAX_VALUE) {\n+      throw new UnsupportedOperationException(\n+        s\"cannot call toArray because buffer size ($limit bytes) exceeds maximum array size\")\n+    }\n+    val byteChannel = new ByteArrayWritableChannel(limit.toInt)\n+    writeFully(byteChannel)\n+    byteChannel.close()\n+    byteChannel.getData\n+  }\n+\n+  def toByteBuffer: ByteBuffer = {\n+    if (chunks.length == 1) {\n+      chunks.head\n+    } else {\n+      ByteBuffer.wrap(toArray)\n+    }\n+  }\n+\n+  def toInputStream(dispose: Boolean = false): InputStream = {\n+    new ChunkedByteBufferInputStream(this, dispose)\n+  }\n+\n+  def getChunks(): Array[ByteBuffer] = {\n+    chunks.map(_.duplicate())\n+  }\n+\n+  def copy(): ChunkedByteBuffer = {\n+    val copiedChunks = getChunks().map { chunk =>\n+      // TODO: accept an allocator in this copy method to integrate with mem. accounting systems\n+      val newChunk = ByteBuffer.allocate(chunk.limit())\n+      newChunk.put(chunk)\n+      newChunk.flip()\n+      newChunk\n+    }\n+    new ChunkedByteBuffer(copiedChunks)\n+  }\n+\n+  /**\n+   * Attempt to clean up a ByteBuffer if it is memory-mapped. This uses an *unsafe* Sun API that\n+   * might cause errors if one attempts to read from the unmapped buffer, but it's better than\n+   * waiting for the GC to find it because that could lead to huge numbers of open files. There's\n+   * unfortunately no standard API to do this.\n+   */\n+  def dispose(): Unit = {\n+    chunks.foreach(BlockManager.dispose)\n+  }\n+}\n+\n+/**\n+ * Reads data from a ChunkedByteBuffer, and optionally cleans it up using BlockManager.dispose()\n+ * at the end of the stream (e.g. to close a memory-mapped file).\n+ */\n+private class ChunkedByteBufferInputStream(\n+    var chunkedByteBuffer: ChunkedByteBuffer,\n+    dispose: Boolean)"
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "Yep, will do. The semantics of `dispose()` are a bit confusing given some other weird semantics surrounding its use elsewhere in the BlockManager.\n",
    "commit": "2970932bf11ab9cfc8cac33c05a13912ddb345d4",
    "createdAt": "2016-03-16T05:42:59Z",
    "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.io\n+\n+import java.io.InputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.WritableByteChannel\n+\n+import com.google.common.primitives.UnsignedBytes\n+import io.netty.buffer.{ByteBuf, Unpooled}\n+\n+import org.apache.spark.network.util.ByteArrayWritableChannel\n+import org.apache.spark.storage.BlockManager\n+\n+private[spark] class ChunkedByteBuffer(var chunks: Array[ByteBuffer]) {\n+  require(chunks != null, \"chunks must not be null\")\n+  require(chunks.nonEmpty, \"Cannot create a ChunkedByteBuffer with no chunks\")\n+  require(chunks.forall(_.limit() > 0), \"chunks must be non-empty\")\n+  require(chunks.forall(_.position() == 0), \"chunks' positions must be 0\")\n+\n+  val limit: Long = chunks.map(_.limit().asInstanceOf[Long]).sum\n+\n+  def this(byteBuffer: ByteBuffer) = {\n+    this(Array(byteBuffer))\n+  }\n+\n+  def writeFully(channel: WritableByteChannel): Unit = {\n+    for (bytes <- getChunks()) {\n+      while (bytes.remaining > 0) {\n+        channel.write(bytes)\n+      }\n+    }\n+  }\n+\n+  def toNetty: ByteBuf = {\n+    Unpooled.wrappedBuffer(getChunks(): _*)\n+  }\n+\n+  def toArray: Array[Byte] = {\n+    if (limit >= Integer.MAX_VALUE) {\n+      throw new UnsupportedOperationException(\n+        s\"cannot call toArray because buffer size ($limit bytes) exceeds maximum array size\")\n+    }\n+    val byteChannel = new ByteArrayWritableChannel(limit.toInt)\n+    writeFully(byteChannel)\n+    byteChannel.close()\n+    byteChannel.getData\n+  }\n+\n+  def toByteBuffer: ByteBuffer = {\n+    if (chunks.length == 1) {\n+      chunks.head\n+    } else {\n+      ByteBuffer.wrap(toArray)\n+    }\n+  }\n+\n+  def toInputStream(dispose: Boolean = false): InputStream = {\n+    new ChunkedByteBufferInputStream(this, dispose)\n+  }\n+\n+  def getChunks(): Array[ByteBuffer] = {\n+    chunks.map(_.duplicate())\n+  }\n+\n+  def copy(): ChunkedByteBuffer = {\n+    val copiedChunks = getChunks().map { chunk =>\n+      // TODO: accept an allocator in this copy method to integrate with mem. accounting systems\n+      val newChunk = ByteBuffer.allocate(chunk.limit())\n+      newChunk.put(chunk)\n+      newChunk.flip()\n+      newChunk\n+    }\n+    new ChunkedByteBuffer(copiedChunks)\n+  }\n+\n+  /**\n+   * Attempt to clean up a ByteBuffer if it is memory-mapped. This uses an *unsafe* Sun API that\n+   * might cause errors if one attempts to read from the unmapped buffer, but it's better than\n+   * waiting for the GC to find it because that could lead to huge numbers of open files. There's\n+   * unfortunately no standard API to do this.\n+   */\n+  def dispose(): Unit = {\n+    chunks.foreach(BlockManager.dispose)\n+  }\n+}\n+\n+/**\n+ * Reads data from a ChunkedByteBuffer, and optionally cleans it up using BlockManager.dispose()\n+ * at the end of the stream (e.g. to close a memory-mapped file).\n+ */\n+private class ChunkedByteBufferInputStream(\n+    var chunkedByteBuffer: ChunkedByteBuffer,\n+    dispose: Boolean)"
  }],
  "prId": 11748
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "should document this throws exceptions if size doesn't fit. \n",
    "commit": "2970932bf11ab9cfc8cac33c05a13912ddb345d4",
    "createdAt": "2016-03-16T05:35:24Z",
    "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.io\n+\n+import java.io.InputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.WritableByteChannel\n+\n+import com.google.common.primitives.UnsignedBytes\n+import io.netty.buffer.{ByteBuf, Unpooled}\n+\n+import org.apache.spark.network.util.ByteArrayWritableChannel\n+import org.apache.spark.storage.BlockManager\n+\n+private[spark] class ChunkedByteBuffer(var chunks: Array[ByteBuffer]) {\n+  require(chunks != null, \"chunks must not be null\")\n+  require(chunks.nonEmpty, \"Cannot create a ChunkedByteBuffer with no chunks\")\n+  require(chunks.forall(_.limit() > 0), \"chunks must be non-empty\")\n+  require(chunks.forall(_.position() == 0), \"chunks' positions must be 0\")\n+\n+  val limit: Long = chunks.map(_.limit().asInstanceOf[Long]).sum\n+\n+  def this(byteBuffer: ByteBuffer) = {\n+    this(Array(byteBuffer))\n+  }\n+\n+  def writeFully(channel: WritableByteChannel): Unit = {\n+    for (bytes <- getChunks()) {\n+      while (bytes.remaining > 0) {\n+        channel.write(bytes)\n+      }\n+    }\n+  }\n+\n+  def toNetty: ByteBuf = {\n+    Unpooled.wrappedBuffer(getChunks(): _*)\n+  }\n+\n+  def toArray: Array[Byte] = {"
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "same for toByteBuffer\n",
    "commit": "2970932bf11ab9cfc8cac33c05a13912ddb345d4",
    "createdAt": "2016-03-16T05:35:32Z",
    "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.io\n+\n+import java.io.InputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.WritableByteChannel\n+\n+import com.google.common.primitives.UnsignedBytes\n+import io.netty.buffer.{ByteBuf, Unpooled}\n+\n+import org.apache.spark.network.util.ByteArrayWritableChannel\n+import org.apache.spark.storage.BlockManager\n+\n+private[spark] class ChunkedByteBuffer(var chunks: Array[ByteBuffer]) {\n+  require(chunks != null, \"chunks must not be null\")\n+  require(chunks.nonEmpty, \"Cannot create a ChunkedByteBuffer with no chunks\")\n+  require(chunks.forall(_.limit() > 0), \"chunks must be non-empty\")\n+  require(chunks.forall(_.position() == 0), \"chunks' positions must be 0\")\n+\n+  val limit: Long = chunks.map(_.limit().asInstanceOf[Long]).sum\n+\n+  def this(byteBuffer: ByteBuffer) = {\n+    this(Array(byteBuffer))\n+  }\n+\n+  def writeFully(channel: WritableByteChannel): Unit = {\n+    for (bytes <- getChunks()) {\n+      while (bytes.remaining > 0) {\n+        channel.write(bytes)\n+      }\n+    }\n+  }\n+\n+  def toNetty: ByteBuf = {\n+    Unpooled.wrappedBuffer(getChunks(): _*)\n+  }\n+\n+  def toArray: Array[Byte] = {"
  }],
  "prId": 11748
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "maybe move BlockManager.dispose into an util and call it there? we reduces some coupling this way.\n",
    "commit": "2970932bf11ab9cfc8cac33c05a13912ddb345d4",
    "createdAt": "2016-03-16T05:37:10Z",
    "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.io\n+\n+import java.io.InputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.WritableByteChannel\n+\n+import com.google.common.primitives.UnsignedBytes\n+import io.netty.buffer.{ByteBuf, Unpooled}\n+\n+import org.apache.spark.network.util.ByteArrayWritableChannel\n+import org.apache.spark.storage.BlockManager\n+\n+private[spark] class ChunkedByteBuffer(var chunks: Array[ByteBuffer]) {\n+  require(chunks != null, \"chunks must not be null\")\n+  require(chunks.nonEmpty, \"Cannot create a ChunkedByteBuffer with no chunks\")\n+  require(chunks.forall(_.limit() > 0), \"chunks must be non-empty\")\n+  require(chunks.forall(_.position() == 0), \"chunks' positions must be 0\")\n+\n+  val limit: Long = chunks.map(_.limit().asInstanceOf[Long]).sum\n+\n+  def this(byteBuffer: ByteBuffer) = {\n+    this(Array(byteBuffer))\n+  }\n+\n+  def writeFully(channel: WritableByteChannel): Unit = {\n+    for (bytes <- getChunks()) {\n+      while (bytes.remaining > 0) {\n+        channel.write(bytes)\n+      }\n+    }\n+  }\n+\n+  def toNetty: ByteBuf = {\n+    Unpooled.wrappedBuffer(getChunks(): _*)\n+  }\n+\n+  def toArray: Array[Byte] = {\n+    if (limit >= Integer.MAX_VALUE) {\n+      throw new UnsupportedOperationException(\n+        s\"cannot call toArray because buffer size ($limit bytes) exceeds maximum array size\")\n+    }\n+    val byteChannel = new ByteArrayWritableChannel(limit.toInt)\n+    writeFully(byteChannel)\n+    byteChannel.close()\n+    byteChannel.getData\n+  }\n+\n+  def toByteBuffer: ByteBuffer = {\n+    if (chunks.length == 1) {\n+      chunks.head\n+    } else {\n+      ByteBuffer.wrap(toArray)\n+    }\n+  }\n+\n+  def toInputStream(dispose: Boolean = false): InputStream = {\n+    new ChunkedByteBufferInputStream(this, dispose)\n+  }\n+\n+  def getChunks(): Array[ByteBuffer] = {\n+    chunks.map(_.duplicate())\n+  }\n+\n+  def copy(): ChunkedByteBuffer = {\n+    val copiedChunks = getChunks().map { chunk =>\n+      // TODO: accept an allocator in this copy method to integrate with mem. accounting systems\n+      val newChunk = ByteBuffer.allocate(chunk.limit())\n+      newChunk.put(chunk)\n+      newChunk.flip()\n+      newChunk\n+    }\n+    new ChunkedByteBuffer(copiedChunks)\n+  }\n+\n+  /**\n+   * Attempt to clean up a ByteBuffer if it is memory-mapped. This uses an *unsafe* Sun API that\n+   * might cause errors if one attempts to read from the unmapped buffer, but it's better than\n+   * waiting for the GC to find it because that could lead to huge numbers of open files. There's\n+   * unfortunately no standard API to do this.\n+   */\n+  def dispose(): Unit = {\n+    chunks.foreach(BlockManager.dispose)"
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "Yeah, we should definitely move it into a different companion object or utilities class.\n",
    "commit": "2970932bf11ab9cfc8cac33c05a13912ddb345d4",
    "createdAt": "2016-03-16T05:41:47Z",
    "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.io\n+\n+import java.io.InputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.WritableByteChannel\n+\n+import com.google.common.primitives.UnsignedBytes\n+import io.netty.buffer.{ByteBuf, Unpooled}\n+\n+import org.apache.spark.network.util.ByteArrayWritableChannel\n+import org.apache.spark.storage.BlockManager\n+\n+private[spark] class ChunkedByteBuffer(var chunks: Array[ByteBuffer]) {\n+  require(chunks != null, \"chunks must not be null\")\n+  require(chunks.nonEmpty, \"Cannot create a ChunkedByteBuffer with no chunks\")\n+  require(chunks.forall(_.limit() > 0), \"chunks must be non-empty\")\n+  require(chunks.forall(_.position() == 0), \"chunks' positions must be 0\")\n+\n+  val limit: Long = chunks.map(_.limit().asInstanceOf[Long]).sum\n+\n+  def this(byteBuffer: ByteBuffer) = {\n+    this(Array(byteBuffer))\n+  }\n+\n+  def writeFully(channel: WritableByteChannel): Unit = {\n+    for (bytes <- getChunks()) {\n+      while (bytes.remaining > 0) {\n+        channel.write(bytes)\n+      }\n+    }\n+  }\n+\n+  def toNetty: ByteBuf = {\n+    Unpooled.wrappedBuffer(getChunks(): _*)\n+  }\n+\n+  def toArray: Array[Byte] = {\n+    if (limit >= Integer.MAX_VALUE) {\n+      throw new UnsupportedOperationException(\n+        s\"cannot call toArray because buffer size ($limit bytes) exceeds maximum array size\")\n+    }\n+    val byteChannel = new ByteArrayWritableChannel(limit.toInt)\n+    writeFully(byteChannel)\n+    byteChannel.close()\n+    byteChannel.getData\n+  }\n+\n+  def toByteBuffer: ByteBuffer = {\n+    if (chunks.length == 1) {\n+      chunks.head\n+    } else {\n+      ByteBuffer.wrap(toArray)\n+    }\n+  }\n+\n+  def toInputStream(dispose: Boolean = false): InputStream = {\n+    new ChunkedByteBufferInputStream(this, dispose)\n+  }\n+\n+  def getChunks(): Array[ByteBuffer] = {\n+    chunks.map(_.duplicate())\n+  }\n+\n+  def copy(): ChunkedByteBuffer = {\n+    val copiedChunks = getChunks().map { chunk =>\n+      // TODO: accept an allocator in this copy method to integrate with mem. accounting systems\n+      val newChunk = ByteBuffer.allocate(chunk.limit())\n+      newChunk.put(chunk)\n+      newChunk.flip()\n+      newChunk\n+    }\n+    new ChunkedByteBuffer(copiedChunks)\n+  }\n+\n+  /**\n+   * Attempt to clean up a ByteBuffer if it is memory-mapped. This uses an *unsafe* Sun API that\n+   * might cause errors if one attempts to read from the unmapped buffer, but it's better than\n+   * waiting for the GC to find it because that could lead to huge numbers of open files. There's\n+   * unfortunately no standard API to do this.\n+   */\n+  def dispose(): Unit = {\n+    chunks.foreach(BlockManager.dispose)"
  }],
  "prId": 11748
}, {
  "comments": [{
    "author": {
      "login": "nongli"
    },
    "body": "Does this need to call dispose()?\n",
    "commit": "2970932bf11ab9cfc8cac33c05a13912ddb345d4",
    "createdAt": "2016-03-17T20:54:12Z",
    "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.io\n+\n+import java.io.InputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.WritableByteChannel\n+\n+import com.google.common.primitives.UnsignedBytes\n+import io.netty.buffer.{ByteBuf, Unpooled}\n+\n+import org.apache.spark.network.util.ByteArrayWritableChannel\n+import org.apache.spark.storage.StorageUtils\n+\n+/**\n+ * Read-only byte buffer which is physically stored as multiple chunks rather than a single\n+ * contiguous array.\n+ *\n+ * @param chunks an array of [[ByteBuffer]]s. Each buffer in this array must be non-empty and have\n+ *               position == 0. Ownership of these buffers is transferred to the ChunkedByteBuffer,\n+ *               so if these buffers may also be used elsewhere then the caller is responsible for\n+ *               copying them as needed.\n+ */\n+private[spark] class ChunkedByteBuffer(var chunks: Array[ByteBuffer]) {\n+  require(chunks != null, \"chunks must not be null\")\n+  require(chunks.forall(_.limit() > 0), \"chunks must be non-empty\")\n+  require(chunks.forall(_.position() == 0), \"chunks' positions must be 0\")\n+\n+  /**\n+   * This size of this buffer, in bytes.\n+   */\n+  val size: Long = chunks.map(_.limit().asInstanceOf[Long]).sum\n+\n+  def this(byteBuffer: ByteBuffer) = {\n+    this(Array(byteBuffer))\n+  }\n+\n+  /**\n+   * Write this buffer to a channel.\n+   */\n+  def writeFully(channel: WritableByteChannel): Unit = {\n+    for (bytes <- getChunks()) {\n+      while (bytes.remaining > 0) {\n+        channel.write(bytes)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Wrap this buffer to view it as a Netty ByteBuf.\n+   */\n+  def toNetty: ByteBuf = {\n+    Unpooled.wrappedBuffer(getChunks(): _*)\n+  }\n+\n+  /**\n+   * Copy this buffer into a new byte array.\n+   *\n+   * @throws UnsupportedOperationException if this buffer's size exceeds the maximum array size.\n+   */\n+  def toArray: Array[Byte] = {\n+    if (size >= Integer.MAX_VALUE) {\n+      throw new UnsupportedOperationException(\n+        s\"cannot call toArray because buffer size ($size bytes) exceeds maximum array size\")\n+    }\n+    val byteChannel = new ByteArrayWritableChannel(size.toInt)\n+    writeFully(byteChannel)\n+    byteChannel.close()\n+    byteChannel.getData\n+  }\n+\n+  /**\n+   * Copy this buffer into a new ByteBuffer.\n+   *\n+   * @throws UnsupportedOperationException if this buffer's size exceeds the max ByteBuffer size.\n+   */\n+  def toByteBuffer: ByteBuffer = {\n+    if (chunks.length == 1) {\n+      chunks.head.duplicate()\n+    } else {\n+      ByteBuffer.wrap(toArray)\n+    }\n+  }\n+\n+  /**\n+   * Creates an input stream to read data from this ChunkedByteBuffer.\n+   *\n+   * @param dispose if true, [[dispose()]] will be called at the end of the stream\n+   *                in order to close any memory-mapped files which back this buffer.\n+   */\n+  def toInputStream(dispose: Boolean = false): InputStream = {\n+    new ChunkedByteBufferInputStream(this, dispose)\n+  }\n+\n+  /**\n+   * Get duplicates of the ByteBuffers backing this ChunkedByteBuffer.\n+   */\n+  def getChunks(): Array[ByteBuffer] = {\n+    chunks.map(_.duplicate())\n+  }\n+\n+  /**\n+   * Make a copy of this ChunkedByteBuffer, copying all of the backing data into new buffers.\n+   * The new buffer will share no resources with the original buffer.\n+   */\n+  def copy(): ChunkedByteBuffer = {\n+    val copiedChunks = getChunks().map { chunk =>\n+      // TODO: accept an allocator in this copy method to integrate with mem. accounting systems\n+      val newChunk = ByteBuffer.allocate(chunk.limit())\n+      newChunk.put(chunk)\n+      newChunk.flip()\n+      newChunk\n+    }\n+    new ChunkedByteBuffer(copiedChunks)\n+  }\n+\n+  /**\n+   * Attempt to clean up a ByteBuffer if it is memory-mapped. This uses an *unsafe* Sun API that\n+   * might cause errors if one attempts to read from the unmapped buffer, but it's better than\n+   * waiting for the GC to find it because that could lead to huge numbers of open files. There's\n+   * unfortunately no standard API to do this.\n+   */\n+  def dispose(): Unit = {\n+    chunks.foreach(StorageUtils.dispose)\n+  }\n+}\n+\n+/**\n+ * Reads data from a ChunkedByteBuffer.\n+ *\n+ * @param dispose if true, [[ChunkedByteBuffer.dispose()]] will be called at the end of the stream\n+ *                in order to close any memory-mapped files which back the buffer.\n+ */\n+private class ChunkedByteBufferInputStream(\n+    var chunkedByteBuffer: ChunkedByteBuffer,\n+    dispose: Boolean)\n+  extends InputStream {\n+\n+  private[this] var chunks = chunkedByteBuffer.getChunks().iterator\n+  private[this] var currentChunk: ByteBuffer = {\n+    if (chunks.hasNext) {\n+      chunks.next()\n+    } else {\n+      null\n+    }\n+  }\n+\n+  override def read(): Int = {\n+    if (currentChunk != null && !currentChunk.hasRemaining && chunks.hasNext) {\n+      StorageUtils.dispose(currentChunk)\n+      currentChunk = chunks.next()\n+    }\n+    if (currentChunk != null && currentChunk.hasRemaining) {\n+      UnsignedBytes.toInt(currentChunk.get())\n+    } else {\n+      close()\n+      -1\n+    }\n+  }\n+\n+  override def read(dest: Array[Byte], offset: Int, length: Int): Int = {\n+    if (currentChunk != null && !currentChunk.hasRemaining && chunks.hasNext) {\n+      StorageUtils.dispose(currentChunk)\n+      currentChunk = chunks.next()\n+    }\n+    if (currentChunk != null && currentChunk.hasRemaining) {\n+      val amountToGet = math.min(currentChunk.remaining(), length)\n+      currentChunk.get(dest, offset, amountToGet)\n+      amountToGet\n+    } else {\n+      close()\n+      -1\n+    }\n+  }\n+\n+  override def skip(bytes: Long): Long = {\n+    if (currentChunk != null) {\n+      val amountToSkip = math.min(bytes, currentChunk.remaining).toInt\n+      currentChunk.position(currentChunk.position + amountToSkip)\n+      if (currentChunk.remaining() == 0) {\n+        if (chunks.hasNext) {\n+          currentChunk = chunks.next()",
    "line": 195
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "Currently we dispose of all chunks at the same time (in the `close()` call).\n",
    "commit": "2970932bf11ab9cfc8cac33c05a13912ddb345d4",
    "createdAt": "2016-03-17T20:55:43Z",
    "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.io\n+\n+import java.io.InputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.WritableByteChannel\n+\n+import com.google.common.primitives.UnsignedBytes\n+import io.netty.buffer.{ByteBuf, Unpooled}\n+\n+import org.apache.spark.network.util.ByteArrayWritableChannel\n+import org.apache.spark.storage.StorageUtils\n+\n+/**\n+ * Read-only byte buffer which is physically stored as multiple chunks rather than a single\n+ * contiguous array.\n+ *\n+ * @param chunks an array of [[ByteBuffer]]s. Each buffer in this array must be non-empty and have\n+ *               position == 0. Ownership of these buffers is transferred to the ChunkedByteBuffer,\n+ *               so if these buffers may also be used elsewhere then the caller is responsible for\n+ *               copying them as needed.\n+ */\n+private[spark] class ChunkedByteBuffer(var chunks: Array[ByteBuffer]) {\n+  require(chunks != null, \"chunks must not be null\")\n+  require(chunks.forall(_.limit() > 0), \"chunks must be non-empty\")\n+  require(chunks.forall(_.position() == 0), \"chunks' positions must be 0\")\n+\n+  /**\n+   * This size of this buffer, in bytes.\n+   */\n+  val size: Long = chunks.map(_.limit().asInstanceOf[Long]).sum\n+\n+  def this(byteBuffer: ByteBuffer) = {\n+    this(Array(byteBuffer))\n+  }\n+\n+  /**\n+   * Write this buffer to a channel.\n+   */\n+  def writeFully(channel: WritableByteChannel): Unit = {\n+    for (bytes <- getChunks()) {\n+      while (bytes.remaining > 0) {\n+        channel.write(bytes)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Wrap this buffer to view it as a Netty ByteBuf.\n+   */\n+  def toNetty: ByteBuf = {\n+    Unpooled.wrappedBuffer(getChunks(): _*)\n+  }\n+\n+  /**\n+   * Copy this buffer into a new byte array.\n+   *\n+   * @throws UnsupportedOperationException if this buffer's size exceeds the maximum array size.\n+   */\n+  def toArray: Array[Byte] = {\n+    if (size >= Integer.MAX_VALUE) {\n+      throw new UnsupportedOperationException(\n+        s\"cannot call toArray because buffer size ($size bytes) exceeds maximum array size\")\n+    }\n+    val byteChannel = new ByteArrayWritableChannel(size.toInt)\n+    writeFully(byteChannel)\n+    byteChannel.close()\n+    byteChannel.getData\n+  }\n+\n+  /**\n+   * Copy this buffer into a new ByteBuffer.\n+   *\n+   * @throws UnsupportedOperationException if this buffer's size exceeds the max ByteBuffer size.\n+   */\n+  def toByteBuffer: ByteBuffer = {\n+    if (chunks.length == 1) {\n+      chunks.head.duplicate()\n+    } else {\n+      ByteBuffer.wrap(toArray)\n+    }\n+  }\n+\n+  /**\n+   * Creates an input stream to read data from this ChunkedByteBuffer.\n+   *\n+   * @param dispose if true, [[dispose()]] will be called at the end of the stream\n+   *                in order to close any memory-mapped files which back this buffer.\n+   */\n+  def toInputStream(dispose: Boolean = false): InputStream = {\n+    new ChunkedByteBufferInputStream(this, dispose)\n+  }\n+\n+  /**\n+   * Get duplicates of the ByteBuffers backing this ChunkedByteBuffer.\n+   */\n+  def getChunks(): Array[ByteBuffer] = {\n+    chunks.map(_.duplicate())\n+  }\n+\n+  /**\n+   * Make a copy of this ChunkedByteBuffer, copying all of the backing data into new buffers.\n+   * The new buffer will share no resources with the original buffer.\n+   */\n+  def copy(): ChunkedByteBuffer = {\n+    val copiedChunks = getChunks().map { chunk =>\n+      // TODO: accept an allocator in this copy method to integrate with mem. accounting systems\n+      val newChunk = ByteBuffer.allocate(chunk.limit())\n+      newChunk.put(chunk)\n+      newChunk.flip()\n+      newChunk\n+    }\n+    new ChunkedByteBuffer(copiedChunks)\n+  }\n+\n+  /**\n+   * Attempt to clean up a ByteBuffer if it is memory-mapped. This uses an *unsafe* Sun API that\n+   * might cause errors if one attempts to read from the unmapped buffer, but it's better than\n+   * waiting for the GC to find it because that could lead to huge numbers of open files. There's\n+   * unfortunately no standard API to do this.\n+   */\n+  def dispose(): Unit = {\n+    chunks.foreach(StorageUtils.dispose)\n+  }\n+}\n+\n+/**\n+ * Reads data from a ChunkedByteBuffer.\n+ *\n+ * @param dispose if true, [[ChunkedByteBuffer.dispose()]] will be called at the end of the stream\n+ *                in order to close any memory-mapped files which back the buffer.\n+ */\n+private class ChunkedByteBufferInputStream(\n+    var chunkedByteBuffer: ChunkedByteBuffer,\n+    dispose: Boolean)\n+  extends InputStream {\n+\n+  private[this] var chunks = chunkedByteBuffer.getChunks().iterator\n+  private[this] var currentChunk: ByteBuffer = {\n+    if (chunks.hasNext) {\n+      chunks.next()\n+    } else {\n+      null\n+    }\n+  }\n+\n+  override def read(): Int = {\n+    if (currentChunk != null && !currentChunk.hasRemaining && chunks.hasNext) {\n+      StorageUtils.dispose(currentChunk)\n+      currentChunk = chunks.next()\n+    }\n+    if (currentChunk != null && currentChunk.hasRemaining) {\n+      UnsignedBytes.toInt(currentChunk.get())\n+    } else {\n+      close()\n+      -1\n+    }\n+  }\n+\n+  override def read(dest: Array[Byte], offset: Int, length: Int): Int = {\n+    if (currentChunk != null && !currentChunk.hasRemaining && chunks.hasNext) {\n+      StorageUtils.dispose(currentChunk)\n+      currentChunk = chunks.next()\n+    }\n+    if (currentChunk != null && currentChunk.hasRemaining) {\n+      val amountToGet = math.min(currentChunk.remaining(), length)\n+      currentChunk.get(dest, offset, amountToGet)\n+      amountToGet\n+    } else {\n+      close()\n+      -1\n+    }\n+  }\n+\n+  override def skip(bytes: Long): Long = {\n+    if (currentChunk != null) {\n+      val amountToSkip = math.min(bytes, currentChunk.remaining).toInt\n+      currentChunk.position(currentChunk.position + amountToSkip)\n+      if (currentChunk.remaining() == 0) {\n+        if (chunks.hasNext) {\n+          currentChunk = chunks.next()",
    "line": 195
  }, {
    "author": {
      "login": "nongli"
    },
    "body": "Any reason this behaves differently that way than read()\n",
    "commit": "2970932bf11ab9cfc8cac33c05a13912ddb345d4",
    "createdAt": "2016-03-17T20:58:51Z",
    "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.io\n+\n+import java.io.InputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.WritableByteChannel\n+\n+import com.google.common.primitives.UnsignedBytes\n+import io.netty.buffer.{ByteBuf, Unpooled}\n+\n+import org.apache.spark.network.util.ByteArrayWritableChannel\n+import org.apache.spark.storage.StorageUtils\n+\n+/**\n+ * Read-only byte buffer which is physically stored as multiple chunks rather than a single\n+ * contiguous array.\n+ *\n+ * @param chunks an array of [[ByteBuffer]]s. Each buffer in this array must be non-empty and have\n+ *               position == 0. Ownership of these buffers is transferred to the ChunkedByteBuffer,\n+ *               so if these buffers may also be used elsewhere then the caller is responsible for\n+ *               copying them as needed.\n+ */\n+private[spark] class ChunkedByteBuffer(var chunks: Array[ByteBuffer]) {\n+  require(chunks != null, \"chunks must not be null\")\n+  require(chunks.forall(_.limit() > 0), \"chunks must be non-empty\")\n+  require(chunks.forall(_.position() == 0), \"chunks' positions must be 0\")\n+\n+  /**\n+   * This size of this buffer, in bytes.\n+   */\n+  val size: Long = chunks.map(_.limit().asInstanceOf[Long]).sum\n+\n+  def this(byteBuffer: ByteBuffer) = {\n+    this(Array(byteBuffer))\n+  }\n+\n+  /**\n+   * Write this buffer to a channel.\n+   */\n+  def writeFully(channel: WritableByteChannel): Unit = {\n+    for (bytes <- getChunks()) {\n+      while (bytes.remaining > 0) {\n+        channel.write(bytes)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Wrap this buffer to view it as a Netty ByteBuf.\n+   */\n+  def toNetty: ByteBuf = {\n+    Unpooled.wrappedBuffer(getChunks(): _*)\n+  }\n+\n+  /**\n+   * Copy this buffer into a new byte array.\n+   *\n+   * @throws UnsupportedOperationException if this buffer's size exceeds the maximum array size.\n+   */\n+  def toArray: Array[Byte] = {\n+    if (size >= Integer.MAX_VALUE) {\n+      throw new UnsupportedOperationException(\n+        s\"cannot call toArray because buffer size ($size bytes) exceeds maximum array size\")\n+    }\n+    val byteChannel = new ByteArrayWritableChannel(size.toInt)\n+    writeFully(byteChannel)\n+    byteChannel.close()\n+    byteChannel.getData\n+  }\n+\n+  /**\n+   * Copy this buffer into a new ByteBuffer.\n+   *\n+   * @throws UnsupportedOperationException if this buffer's size exceeds the max ByteBuffer size.\n+   */\n+  def toByteBuffer: ByteBuffer = {\n+    if (chunks.length == 1) {\n+      chunks.head.duplicate()\n+    } else {\n+      ByteBuffer.wrap(toArray)\n+    }\n+  }\n+\n+  /**\n+   * Creates an input stream to read data from this ChunkedByteBuffer.\n+   *\n+   * @param dispose if true, [[dispose()]] will be called at the end of the stream\n+   *                in order to close any memory-mapped files which back this buffer.\n+   */\n+  def toInputStream(dispose: Boolean = false): InputStream = {\n+    new ChunkedByteBufferInputStream(this, dispose)\n+  }\n+\n+  /**\n+   * Get duplicates of the ByteBuffers backing this ChunkedByteBuffer.\n+   */\n+  def getChunks(): Array[ByteBuffer] = {\n+    chunks.map(_.duplicate())\n+  }\n+\n+  /**\n+   * Make a copy of this ChunkedByteBuffer, copying all of the backing data into new buffers.\n+   * The new buffer will share no resources with the original buffer.\n+   */\n+  def copy(): ChunkedByteBuffer = {\n+    val copiedChunks = getChunks().map { chunk =>\n+      // TODO: accept an allocator in this copy method to integrate with mem. accounting systems\n+      val newChunk = ByteBuffer.allocate(chunk.limit())\n+      newChunk.put(chunk)\n+      newChunk.flip()\n+      newChunk\n+    }\n+    new ChunkedByteBuffer(copiedChunks)\n+  }\n+\n+  /**\n+   * Attempt to clean up a ByteBuffer if it is memory-mapped. This uses an *unsafe* Sun API that\n+   * might cause errors if one attempts to read from the unmapped buffer, but it's better than\n+   * waiting for the GC to find it because that could lead to huge numbers of open files. There's\n+   * unfortunately no standard API to do this.\n+   */\n+  def dispose(): Unit = {\n+    chunks.foreach(StorageUtils.dispose)\n+  }\n+}\n+\n+/**\n+ * Reads data from a ChunkedByteBuffer.\n+ *\n+ * @param dispose if true, [[ChunkedByteBuffer.dispose()]] will be called at the end of the stream\n+ *                in order to close any memory-mapped files which back the buffer.\n+ */\n+private class ChunkedByteBufferInputStream(\n+    var chunkedByteBuffer: ChunkedByteBuffer,\n+    dispose: Boolean)\n+  extends InputStream {\n+\n+  private[this] var chunks = chunkedByteBuffer.getChunks().iterator\n+  private[this] var currentChunk: ByteBuffer = {\n+    if (chunks.hasNext) {\n+      chunks.next()\n+    } else {\n+      null\n+    }\n+  }\n+\n+  override def read(): Int = {\n+    if (currentChunk != null && !currentChunk.hasRemaining && chunks.hasNext) {\n+      StorageUtils.dispose(currentChunk)\n+      currentChunk = chunks.next()\n+    }\n+    if (currentChunk != null && currentChunk.hasRemaining) {\n+      UnsignedBytes.toInt(currentChunk.get())\n+    } else {\n+      close()\n+      -1\n+    }\n+  }\n+\n+  override def read(dest: Array[Byte], offset: Int, length: Int): Int = {\n+    if (currentChunk != null && !currentChunk.hasRemaining && chunks.hasNext) {\n+      StorageUtils.dispose(currentChunk)\n+      currentChunk = chunks.next()\n+    }\n+    if (currentChunk != null && currentChunk.hasRemaining) {\n+      val amountToGet = math.min(currentChunk.remaining(), length)\n+      currentChunk.get(dest, offset, amountToGet)\n+      amountToGet\n+    } else {\n+      close()\n+      -1\n+    }\n+  }\n+\n+  override def skip(bytes: Long): Long = {\n+    if (currentChunk != null) {\n+      val amountToSkip = math.min(bytes, currentChunk.remaining).toInt\n+      currentChunk.position(currentChunk.position + amountToSkip)\n+      if (currentChunk.remaining() == 0) {\n+        if (chunks.hasNext) {\n+          currentChunk = chunks.next()",
    "line": 195
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "Oh, whoops. Stupid mistake.\n",
    "commit": "2970932bf11ab9cfc8cac33c05a13912ddb345d4",
    "createdAt": "2016-03-17T21:01:20Z",
    "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.io\n+\n+import java.io.InputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.WritableByteChannel\n+\n+import com.google.common.primitives.UnsignedBytes\n+import io.netty.buffer.{ByteBuf, Unpooled}\n+\n+import org.apache.spark.network.util.ByteArrayWritableChannel\n+import org.apache.spark.storage.StorageUtils\n+\n+/**\n+ * Read-only byte buffer which is physically stored as multiple chunks rather than a single\n+ * contiguous array.\n+ *\n+ * @param chunks an array of [[ByteBuffer]]s. Each buffer in this array must be non-empty and have\n+ *               position == 0. Ownership of these buffers is transferred to the ChunkedByteBuffer,\n+ *               so if these buffers may also be used elsewhere then the caller is responsible for\n+ *               copying them as needed.\n+ */\n+private[spark] class ChunkedByteBuffer(var chunks: Array[ByteBuffer]) {\n+  require(chunks != null, \"chunks must not be null\")\n+  require(chunks.forall(_.limit() > 0), \"chunks must be non-empty\")\n+  require(chunks.forall(_.position() == 0), \"chunks' positions must be 0\")\n+\n+  /**\n+   * This size of this buffer, in bytes.\n+   */\n+  val size: Long = chunks.map(_.limit().asInstanceOf[Long]).sum\n+\n+  def this(byteBuffer: ByteBuffer) = {\n+    this(Array(byteBuffer))\n+  }\n+\n+  /**\n+   * Write this buffer to a channel.\n+   */\n+  def writeFully(channel: WritableByteChannel): Unit = {\n+    for (bytes <- getChunks()) {\n+      while (bytes.remaining > 0) {\n+        channel.write(bytes)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Wrap this buffer to view it as a Netty ByteBuf.\n+   */\n+  def toNetty: ByteBuf = {\n+    Unpooled.wrappedBuffer(getChunks(): _*)\n+  }\n+\n+  /**\n+   * Copy this buffer into a new byte array.\n+   *\n+   * @throws UnsupportedOperationException if this buffer's size exceeds the maximum array size.\n+   */\n+  def toArray: Array[Byte] = {\n+    if (size >= Integer.MAX_VALUE) {\n+      throw new UnsupportedOperationException(\n+        s\"cannot call toArray because buffer size ($size bytes) exceeds maximum array size\")\n+    }\n+    val byteChannel = new ByteArrayWritableChannel(size.toInt)\n+    writeFully(byteChannel)\n+    byteChannel.close()\n+    byteChannel.getData\n+  }\n+\n+  /**\n+   * Copy this buffer into a new ByteBuffer.\n+   *\n+   * @throws UnsupportedOperationException if this buffer's size exceeds the max ByteBuffer size.\n+   */\n+  def toByteBuffer: ByteBuffer = {\n+    if (chunks.length == 1) {\n+      chunks.head.duplicate()\n+    } else {\n+      ByteBuffer.wrap(toArray)\n+    }\n+  }\n+\n+  /**\n+   * Creates an input stream to read data from this ChunkedByteBuffer.\n+   *\n+   * @param dispose if true, [[dispose()]] will be called at the end of the stream\n+   *                in order to close any memory-mapped files which back this buffer.\n+   */\n+  def toInputStream(dispose: Boolean = false): InputStream = {\n+    new ChunkedByteBufferInputStream(this, dispose)\n+  }\n+\n+  /**\n+   * Get duplicates of the ByteBuffers backing this ChunkedByteBuffer.\n+   */\n+  def getChunks(): Array[ByteBuffer] = {\n+    chunks.map(_.duplicate())\n+  }\n+\n+  /**\n+   * Make a copy of this ChunkedByteBuffer, copying all of the backing data into new buffers.\n+   * The new buffer will share no resources with the original buffer.\n+   */\n+  def copy(): ChunkedByteBuffer = {\n+    val copiedChunks = getChunks().map { chunk =>\n+      // TODO: accept an allocator in this copy method to integrate with mem. accounting systems\n+      val newChunk = ByteBuffer.allocate(chunk.limit())\n+      newChunk.put(chunk)\n+      newChunk.flip()\n+      newChunk\n+    }\n+    new ChunkedByteBuffer(copiedChunks)\n+  }\n+\n+  /**\n+   * Attempt to clean up a ByteBuffer if it is memory-mapped. This uses an *unsafe* Sun API that\n+   * might cause errors if one attempts to read from the unmapped buffer, but it's better than\n+   * waiting for the GC to find it because that could lead to huge numbers of open files. There's\n+   * unfortunately no standard API to do this.\n+   */\n+  def dispose(): Unit = {\n+    chunks.foreach(StorageUtils.dispose)\n+  }\n+}\n+\n+/**\n+ * Reads data from a ChunkedByteBuffer.\n+ *\n+ * @param dispose if true, [[ChunkedByteBuffer.dispose()]] will be called at the end of the stream\n+ *                in order to close any memory-mapped files which back the buffer.\n+ */\n+private class ChunkedByteBufferInputStream(\n+    var chunkedByteBuffer: ChunkedByteBuffer,\n+    dispose: Boolean)\n+  extends InputStream {\n+\n+  private[this] var chunks = chunkedByteBuffer.getChunks().iterator\n+  private[this] var currentChunk: ByteBuffer = {\n+    if (chunks.hasNext) {\n+      chunks.next()\n+    } else {\n+      null\n+    }\n+  }\n+\n+  override def read(): Int = {\n+    if (currentChunk != null && !currentChunk.hasRemaining && chunks.hasNext) {\n+      StorageUtils.dispose(currentChunk)\n+      currentChunk = chunks.next()\n+    }\n+    if (currentChunk != null && currentChunk.hasRemaining) {\n+      UnsignedBytes.toInt(currentChunk.get())\n+    } else {\n+      close()\n+      -1\n+    }\n+  }\n+\n+  override def read(dest: Array[Byte], offset: Int, length: Int): Int = {\n+    if (currentChunk != null && !currentChunk.hasRemaining && chunks.hasNext) {\n+      StorageUtils.dispose(currentChunk)\n+      currentChunk = chunks.next()\n+    }\n+    if (currentChunk != null && currentChunk.hasRemaining) {\n+      val amountToGet = math.min(currentChunk.remaining(), length)\n+      currentChunk.get(dest, offset, amountToGet)\n+      amountToGet\n+    } else {\n+      close()\n+      -1\n+    }\n+  }\n+\n+  override def skip(bytes: Long): Long = {\n+    if (currentChunk != null) {\n+      val amountToSkip = math.min(bytes, currentChunk.remaining).toInt\n+      currentChunk.position(currentChunk.position + amountToSkip)\n+      if (currentChunk.remaining() == 0) {\n+        if (chunks.hasNext) {\n+          currentChunk = chunks.next()",
    "line": 195
  }],
  "prId": 11748
}, {
  "comments": [{
    "author": {
      "login": "nongli"
    },
    "body": "This seems to be weird. When is it only checked on close() but not read?\n",
    "commit": "2970932bf11ab9cfc8cac33c05a13912ddb345d4",
    "createdAt": "2016-03-17T20:55:55Z",
    "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.io\n+\n+import java.io.InputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.WritableByteChannel\n+\n+import com.google.common.primitives.UnsignedBytes\n+import io.netty.buffer.{ByteBuf, Unpooled}\n+\n+import org.apache.spark.network.util.ByteArrayWritableChannel\n+import org.apache.spark.storage.StorageUtils\n+\n+/**\n+ * Read-only byte buffer which is physically stored as multiple chunks rather than a single\n+ * contiguous array.\n+ *\n+ * @param chunks an array of [[ByteBuffer]]s. Each buffer in this array must be non-empty and have\n+ *               position == 0. Ownership of these buffers is transferred to the ChunkedByteBuffer,\n+ *               so if these buffers may also be used elsewhere then the caller is responsible for\n+ *               copying them as needed.\n+ */\n+private[spark] class ChunkedByteBuffer(var chunks: Array[ByteBuffer]) {\n+  require(chunks != null, \"chunks must not be null\")\n+  require(chunks.forall(_.limit() > 0), \"chunks must be non-empty\")\n+  require(chunks.forall(_.position() == 0), \"chunks' positions must be 0\")\n+\n+  /**\n+   * This size of this buffer, in bytes.\n+   */\n+  val size: Long = chunks.map(_.limit().asInstanceOf[Long]).sum\n+\n+  def this(byteBuffer: ByteBuffer) = {\n+    this(Array(byteBuffer))\n+  }\n+\n+  /**\n+   * Write this buffer to a channel.\n+   */\n+  def writeFully(channel: WritableByteChannel): Unit = {\n+    for (bytes <- getChunks()) {\n+      while (bytes.remaining > 0) {\n+        channel.write(bytes)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Wrap this buffer to view it as a Netty ByteBuf.\n+   */\n+  def toNetty: ByteBuf = {\n+    Unpooled.wrappedBuffer(getChunks(): _*)\n+  }\n+\n+  /**\n+   * Copy this buffer into a new byte array.\n+   *\n+   * @throws UnsupportedOperationException if this buffer's size exceeds the maximum array size.\n+   */\n+  def toArray: Array[Byte] = {\n+    if (size >= Integer.MAX_VALUE) {\n+      throw new UnsupportedOperationException(\n+        s\"cannot call toArray because buffer size ($size bytes) exceeds maximum array size\")\n+    }\n+    val byteChannel = new ByteArrayWritableChannel(size.toInt)\n+    writeFully(byteChannel)\n+    byteChannel.close()\n+    byteChannel.getData\n+  }\n+\n+  /**\n+   * Copy this buffer into a new ByteBuffer.\n+   *\n+   * @throws UnsupportedOperationException if this buffer's size exceeds the max ByteBuffer size.\n+   */\n+  def toByteBuffer: ByteBuffer = {\n+    if (chunks.length == 1) {\n+      chunks.head.duplicate()\n+    } else {\n+      ByteBuffer.wrap(toArray)\n+    }\n+  }\n+\n+  /**\n+   * Creates an input stream to read data from this ChunkedByteBuffer.\n+   *\n+   * @param dispose if true, [[dispose()]] will be called at the end of the stream\n+   *                in order to close any memory-mapped files which back this buffer.\n+   */\n+  def toInputStream(dispose: Boolean = false): InputStream = {\n+    new ChunkedByteBufferInputStream(this, dispose)\n+  }\n+\n+  /**\n+   * Get duplicates of the ByteBuffers backing this ChunkedByteBuffer.\n+   */\n+  def getChunks(): Array[ByteBuffer] = {\n+    chunks.map(_.duplicate())\n+  }\n+\n+  /**\n+   * Make a copy of this ChunkedByteBuffer, copying all of the backing data into new buffers.\n+   * The new buffer will share no resources with the original buffer.\n+   */\n+  def copy(): ChunkedByteBuffer = {\n+    val copiedChunks = getChunks().map { chunk =>\n+      // TODO: accept an allocator in this copy method to integrate with mem. accounting systems\n+      val newChunk = ByteBuffer.allocate(chunk.limit())\n+      newChunk.put(chunk)\n+      newChunk.flip()\n+      newChunk\n+    }\n+    new ChunkedByteBuffer(copiedChunks)\n+  }\n+\n+  /**\n+   * Attempt to clean up a ByteBuffer if it is memory-mapped. This uses an *unsafe* Sun API that\n+   * might cause errors if one attempts to read from the unmapped buffer, but it's better than\n+   * waiting for the GC to find it because that could lead to huge numbers of open files. There's\n+   * unfortunately no standard API to do this.\n+   */\n+  def dispose(): Unit = {\n+    chunks.foreach(StorageUtils.dispose)\n+  }\n+}\n+\n+/**\n+ * Reads data from a ChunkedByteBuffer.\n+ *\n+ * @param dispose if true, [[ChunkedByteBuffer.dispose()]] will be called at the end of the stream",
    "line": 146
  }],
  "prId": 11748
}, {
  "comments": [{
    "author": {
      "login": "nongli"
    },
    "body": "I'd more naturally expect this to be written as:\nif (chunkedByteBuffer != null) {\n ...\n}\n\nwhy'd you check with currentChunk\n",
    "commit": "2970932bf11ab9cfc8cac33c05a13912ddb345d4",
    "createdAt": "2016-03-17T20:57:30Z",
    "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.io\n+\n+import java.io.InputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.WritableByteChannel\n+\n+import com.google.common.primitives.UnsignedBytes\n+import io.netty.buffer.{ByteBuf, Unpooled}\n+\n+import org.apache.spark.network.util.ByteArrayWritableChannel\n+import org.apache.spark.storage.StorageUtils\n+\n+/**\n+ * Read-only byte buffer which is physically stored as multiple chunks rather than a single\n+ * contiguous array.\n+ *\n+ * @param chunks an array of [[ByteBuffer]]s. Each buffer in this array must be non-empty and have\n+ *               position == 0. Ownership of these buffers is transferred to the ChunkedByteBuffer,\n+ *               so if these buffers may also be used elsewhere then the caller is responsible for\n+ *               copying them as needed.\n+ */\n+private[spark] class ChunkedByteBuffer(var chunks: Array[ByteBuffer]) {\n+  require(chunks != null, \"chunks must not be null\")\n+  require(chunks.forall(_.limit() > 0), \"chunks must be non-empty\")\n+  require(chunks.forall(_.position() == 0), \"chunks' positions must be 0\")\n+\n+  /**\n+   * This size of this buffer, in bytes.\n+   */\n+  val size: Long = chunks.map(_.limit().asInstanceOf[Long]).sum\n+\n+  def this(byteBuffer: ByteBuffer) = {\n+    this(Array(byteBuffer))\n+  }\n+\n+  /**\n+   * Write this buffer to a channel.\n+   */\n+  def writeFully(channel: WritableByteChannel): Unit = {\n+    for (bytes <- getChunks()) {\n+      while (bytes.remaining > 0) {\n+        channel.write(bytes)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Wrap this buffer to view it as a Netty ByteBuf.\n+   */\n+  def toNetty: ByteBuf = {\n+    Unpooled.wrappedBuffer(getChunks(): _*)\n+  }\n+\n+  /**\n+   * Copy this buffer into a new byte array.\n+   *\n+   * @throws UnsupportedOperationException if this buffer's size exceeds the maximum array size.\n+   */\n+  def toArray: Array[Byte] = {\n+    if (size >= Integer.MAX_VALUE) {\n+      throw new UnsupportedOperationException(\n+        s\"cannot call toArray because buffer size ($size bytes) exceeds maximum array size\")\n+    }\n+    val byteChannel = new ByteArrayWritableChannel(size.toInt)\n+    writeFully(byteChannel)\n+    byteChannel.close()\n+    byteChannel.getData\n+  }\n+\n+  /**\n+   * Copy this buffer into a new ByteBuffer.\n+   *\n+   * @throws UnsupportedOperationException if this buffer's size exceeds the max ByteBuffer size.\n+   */\n+  def toByteBuffer: ByteBuffer = {\n+    if (chunks.length == 1) {\n+      chunks.head.duplicate()\n+    } else {\n+      ByteBuffer.wrap(toArray)\n+    }\n+  }\n+\n+  /**\n+   * Creates an input stream to read data from this ChunkedByteBuffer.\n+   *\n+   * @param dispose if true, [[dispose()]] will be called at the end of the stream\n+   *                in order to close any memory-mapped files which back this buffer.\n+   */\n+  def toInputStream(dispose: Boolean = false): InputStream = {\n+    new ChunkedByteBufferInputStream(this, dispose)\n+  }\n+\n+  /**\n+   * Get duplicates of the ByteBuffers backing this ChunkedByteBuffer.\n+   */\n+  def getChunks(): Array[ByteBuffer] = {\n+    chunks.map(_.duplicate())\n+  }\n+\n+  /**\n+   * Make a copy of this ChunkedByteBuffer, copying all of the backing data into new buffers.\n+   * The new buffer will share no resources with the original buffer.\n+   */\n+  def copy(): ChunkedByteBuffer = {\n+    val copiedChunks = getChunks().map { chunk =>\n+      // TODO: accept an allocator in this copy method to integrate with mem. accounting systems\n+      val newChunk = ByteBuffer.allocate(chunk.limit())\n+      newChunk.put(chunk)\n+      newChunk.flip()\n+      newChunk\n+    }\n+    new ChunkedByteBuffer(copiedChunks)\n+  }\n+\n+  /**\n+   * Attempt to clean up a ByteBuffer if it is memory-mapped. This uses an *unsafe* Sun API that\n+   * might cause errors if one attempts to read from the unmapped buffer, but it's better than\n+   * waiting for the GC to find it because that could lead to huge numbers of open files. There's\n+   * unfortunately no standard API to do this.\n+   */\n+  def dispose(): Unit = {\n+    chunks.foreach(StorageUtils.dispose)\n+  }\n+}\n+\n+/**\n+ * Reads data from a ChunkedByteBuffer.\n+ *\n+ * @param dispose if true, [[ChunkedByteBuffer.dispose()]] will be called at the end of the stream\n+ *                in order to close any memory-mapped files which back the buffer.\n+ */\n+private class ChunkedByteBufferInputStream(\n+    var chunkedByteBuffer: ChunkedByteBuffer,\n+    dispose: Boolean)\n+  extends InputStream {\n+\n+  private[this] var chunks = chunkedByteBuffer.getChunks().iterator\n+  private[this] var currentChunk: ByteBuffer = {\n+    if (chunks.hasNext) {\n+      chunks.next()\n+    } else {\n+      null\n+    }\n+  }\n+\n+  override def read(): Int = {\n+    if (currentChunk != null && !currentChunk.hasRemaining && chunks.hasNext) {\n+      StorageUtils.dispose(currentChunk)\n+      currentChunk = chunks.next()\n+    }\n+    if (currentChunk != null && currentChunk.hasRemaining) {\n+      UnsignedBytes.toInt(currentChunk.get())\n+    } else {\n+      close()\n+      -1\n+    }\n+  }\n+\n+  override def read(dest: Array[Byte], offset: Int, length: Int): Int = {\n+    if (currentChunk != null && !currentChunk.hasRemaining && chunks.hasNext) {\n+      StorageUtils.dispose(currentChunk)\n+      currentChunk = chunks.next()\n+    }\n+    if (currentChunk != null && currentChunk.hasRemaining) {\n+      val amountToGet = math.min(currentChunk.remaining(), length)\n+      currentChunk.get(dest, offset, amountToGet)\n+      amountToGet\n+    } else {\n+      close()\n+      -1\n+    }\n+  }\n+\n+  override def skip(bytes: Long): Long = {\n+    if (currentChunk != null) {\n+      val amountToSkip = math.min(bytes, currentChunk.remaining).toInt\n+      currentChunk.position(currentChunk.position + amountToSkip)\n+      if (currentChunk.remaining() == 0) {\n+        if (chunks.hasNext) {\n+          currentChunk = chunks.next()\n+        } else {\n+          close()\n+        }\n+      }\n+      amountToSkip\n+    } else {\n+      0L\n+    }\n+  }\n+\n+  override def close(): Unit = {\n+    if (currentChunk != null) {"
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "Carryover from older version of the code.\n",
    "commit": "2970932bf11ab9cfc8cac33c05a13912ddb345d4",
    "createdAt": "2016-03-17T20:58:18Z",
    "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.io\n+\n+import java.io.InputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.WritableByteChannel\n+\n+import com.google.common.primitives.UnsignedBytes\n+import io.netty.buffer.{ByteBuf, Unpooled}\n+\n+import org.apache.spark.network.util.ByteArrayWritableChannel\n+import org.apache.spark.storage.StorageUtils\n+\n+/**\n+ * Read-only byte buffer which is physically stored as multiple chunks rather than a single\n+ * contiguous array.\n+ *\n+ * @param chunks an array of [[ByteBuffer]]s. Each buffer in this array must be non-empty and have\n+ *               position == 0. Ownership of these buffers is transferred to the ChunkedByteBuffer,\n+ *               so if these buffers may also be used elsewhere then the caller is responsible for\n+ *               copying them as needed.\n+ */\n+private[spark] class ChunkedByteBuffer(var chunks: Array[ByteBuffer]) {\n+  require(chunks != null, \"chunks must not be null\")\n+  require(chunks.forall(_.limit() > 0), \"chunks must be non-empty\")\n+  require(chunks.forall(_.position() == 0), \"chunks' positions must be 0\")\n+\n+  /**\n+   * This size of this buffer, in bytes.\n+   */\n+  val size: Long = chunks.map(_.limit().asInstanceOf[Long]).sum\n+\n+  def this(byteBuffer: ByteBuffer) = {\n+    this(Array(byteBuffer))\n+  }\n+\n+  /**\n+   * Write this buffer to a channel.\n+   */\n+  def writeFully(channel: WritableByteChannel): Unit = {\n+    for (bytes <- getChunks()) {\n+      while (bytes.remaining > 0) {\n+        channel.write(bytes)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Wrap this buffer to view it as a Netty ByteBuf.\n+   */\n+  def toNetty: ByteBuf = {\n+    Unpooled.wrappedBuffer(getChunks(): _*)\n+  }\n+\n+  /**\n+   * Copy this buffer into a new byte array.\n+   *\n+   * @throws UnsupportedOperationException if this buffer's size exceeds the maximum array size.\n+   */\n+  def toArray: Array[Byte] = {\n+    if (size >= Integer.MAX_VALUE) {\n+      throw new UnsupportedOperationException(\n+        s\"cannot call toArray because buffer size ($size bytes) exceeds maximum array size\")\n+    }\n+    val byteChannel = new ByteArrayWritableChannel(size.toInt)\n+    writeFully(byteChannel)\n+    byteChannel.close()\n+    byteChannel.getData\n+  }\n+\n+  /**\n+   * Copy this buffer into a new ByteBuffer.\n+   *\n+   * @throws UnsupportedOperationException if this buffer's size exceeds the max ByteBuffer size.\n+   */\n+  def toByteBuffer: ByteBuffer = {\n+    if (chunks.length == 1) {\n+      chunks.head.duplicate()\n+    } else {\n+      ByteBuffer.wrap(toArray)\n+    }\n+  }\n+\n+  /**\n+   * Creates an input stream to read data from this ChunkedByteBuffer.\n+   *\n+   * @param dispose if true, [[dispose()]] will be called at the end of the stream\n+   *                in order to close any memory-mapped files which back this buffer.\n+   */\n+  def toInputStream(dispose: Boolean = false): InputStream = {\n+    new ChunkedByteBufferInputStream(this, dispose)\n+  }\n+\n+  /**\n+   * Get duplicates of the ByteBuffers backing this ChunkedByteBuffer.\n+   */\n+  def getChunks(): Array[ByteBuffer] = {\n+    chunks.map(_.duplicate())\n+  }\n+\n+  /**\n+   * Make a copy of this ChunkedByteBuffer, copying all of the backing data into new buffers.\n+   * The new buffer will share no resources with the original buffer.\n+   */\n+  def copy(): ChunkedByteBuffer = {\n+    val copiedChunks = getChunks().map { chunk =>\n+      // TODO: accept an allocator in this copy method to integrate with mem. accounting systems\n+      val newChunk = ByteBuffer.allocate(chunk.limit())\n+      newChunk.put(chunk)\n+      newChunk.flip()\n+      newChunk\n+    }\n+    new ChunkedByteBuffer(copiedChunks)\n+  }\n+\n+  /**\n+   * Attempt to clean up a ByteBuffer if it is memory-mapped. This uses an *unsafe* Sun API that\n+   * might cause errors if one attempts to read from the unmapped buffer, but it's better than\n+   * waiting for the GC to find it because that could lead to huge numbers of open files. There's\n+   * unfortunately no standard API to do this.\n+   */\n+  def dispose(): Unit = {\n+    chunks.foreach(StorageUtils.dispose)\n+  }\n+}\n+\n+/**\n+ * Reads data from a ChunkedByteBuffer.\n+ *\n+ * @param dispose if true, [[ChunkedByteBuffer.dispose()]] will be called at the end of the stream\n+ *                in order to close any memory-mapped files which back the buffer.\n+ */\n+private class ChunkedByteBufferInputStream(\n+    var chunkedByteBuffer: ChunkedByteBuffer,\n+    dispose: Boolean)\n+  extends InputStream {\n+\n+  private[this] var chunks = chunkedByteBuffer.getChunks().iterator\n+  private[this] var currentChunk: ByteBuffer = {\n+    if (chunks.hasNext) {\n+      chunks.next()\n+    } else {\n+      null\n+    }\n+  }\n+\n+  override def read(): Int = {\n+    if (currentChunk != null && !currentChunk.hasRemaining && chunks.hasNext) {\n+      StorageUtils.dispose(currentChunk)\n+      currentChunk = chunks.next()\n+    }\n+    if (currentChunk != null && currentChunk.hasRemaining) {\n+      UnsignedBytes.toInt(currentChunk.get())\n+    } else {\n+      close()\n+      -1\n+    }\n+  }\n+\n+  override def read(dest: Array[Byte], offset: Int, length: Int): Int = {\n+    if (currentChunk != null && !currentChunk.hasRemaining && chunks.hasNext) {\n+      StorageUtils.dispose(currentChunk)\n+      currentChunk = chunks.next()\n+    }\n+    if (currentChunk != null && currentChunk.hasRemaining) {\n+      val amountToGet = math.min(currentChunk.remaining(), length)\n+      currentChunk.get(dest, offset, amountToGet)\n+      amountToGet\n+    } else {\n+      close()\n+      -1\n+    }\n+  }\n+\n+  override def skip(bytes: Long): Long = {\n+    if (currentChunk != null) {\n+      val amountToSkip = math.min(bytes, currentChunk.remaining).toInt\n+      currentChunk.position(currentChunk.position + amountToSkip)\n+      if (currentChunk.remaining() == 0) {\n+        if (chunks.hasNext) {\n+          currentChunk = chunks.next()\n+        } else {\n+          close()\n+        }\n+      }\n+      amountToSkip\n+    } else {\n+      0L\n+    }\n+  }\n+\n+  override def close(): Unit = {\n+    if (currentChunk != null) {"
  }],
  "prId": 11748
}]