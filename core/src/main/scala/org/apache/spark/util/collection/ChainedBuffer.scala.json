[{
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "could you instead re-use `ByteArrayChunkOutputStream`?  I guess that only gives you an output stream, but most likely there can at least be some sharing?  I suppose the implementation is different, since this one has specially sized chunks, but maybe that isn't necessary?\n",
    "commit": "8c70dd909c20af6dacaf4091e964a1239a221016",
    "createdAt": "2015-03-02T19:07:09Z",
    "diffHunk": "@@ -0,0 +1,134 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.collection\n+\n+import java.io.OutputStream\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.storage.BlockObjectWriter\n+\n+/**\n+ * A logical byte buffer that wraps a list of byte arrays. All the byte arrays have equal size. The\n+ * advantage of this over a standard ArrayBuffer is that it can grow without claiming large amounts\n+ * of memory and needing to copy the full contents.\n+ */\n+private[spark] class ChainedBuffer(chunkSize: Int) {"
  }, {
    "author": {
      "login": "sryza"
    },
    "body": "Ah, wasn't aware of that.  Yeah, there might be a way to consolidate them somehow.  The current advantages of `ChainedBuffer` over `ByteArrayChunkOutputStream` are that it provides the ability to read from an arbitrary position and allows writes to cross chunk boundaries.\n",
    "commit": "8c70dd909c20af6dacaf4091e964a1239a221016",
    "createdAt": "2015-03-09T06:56:34Z",
    "diffHunk": "@@ -0,0 +1,134 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.collection\n+\n+import java.io.OutputStream\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.storage.BlockObjectWriter\n+\n+/**\n+ * A logical byte buffer that wraps a list of byte arrays. All the byte arrays have equal size. The\n+ * advantage of this over a standard ArrayBuffer is that it can grow without claiming large amounts\n+ * of memory and needing to copy the full contents.\n+ */\n+private[spark] class ChainedBuffer(chunkSize: Int) {"
  }, {
    "author": {
      "login": "pwendell"
    },
    "body": "Do we ever use the fact that we can do arbitrary offset reads? If we only read sequentially from this then I think merging them would be a good idea.\n",
    "commit": "8c70dd909c20af6dacaf4091e964a1239a221016",
    "createdAt": "2015-03-30T19:33:55Z",
    "diffHunk": "@@ -0,0 +1,134 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.collection\n+\n+import java.io.OutputStream\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.storage.BlockObjectWriter\n+\n+/**\n+ * A logical byte buffer that wraps a list of byte arrays. All the byte arrays have equal size. The\n+ * advantage of this over a standard ArrayBuffer is that it can grow without claiming large amounts\n+ * of memory and needing to copy the full contents.\n+ */\n+private[spark] class ChainedBuffer(chunkSize: Int) {"
  }, {
    "author": {
      "login": "sryza"
    },
    "body": "We do (always) read at arbitrary offsets.  There still might be a way to merge them though.\n",
    "commit": "8c70dd909c20af6dacaf4091e964a1239a221016",
    "createdAt": "2015-04-26T18:34:52Z",
    "diffHunk": "@@ -0,0 +1,134 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.collection\n+\n+import java.io.OutputStream\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.storage.BlockObjectWriter\n+\n+/**\n+ * A logical byte buffer that wraps a list of byte arrays. All the byte arrays have equal size. The\n+ * advantage of this over a standard ArrayBuffer is that it can grow without claiming large amounts\n+ * of memory and needing to copy the full contents.\n+ */\n+private[spark] class ChainedBuffer(chunkSize: Int) {"
  }, {
    "author": {
      "login": "pwendell"
    },
    "body": "After looking at this closer, this is somewhat similar to that, except that you support the random lookups. Their both simple enough I'm not sure they need to be merged - so it might be fine to just add a TODO saying we should look into merging them later.\n",
    "commit": "8c70dd909c20af6dacaf4091e964a1239a221016",
    "createdAt": "2015-04-27T02:09:07Z",
    "diffHunk": "@@ -0,0 +1,134 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.collection\n+\n+import java.io.OutputStream\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.storage.BlockObjectWriter\n+\n+/**\n+ * A logical byte buffer that wraps a list of byte arrays. All the byte arrays have equal size. The\n+ * advantage of this over a standard ArrayBuffer is that it can grow without claiming large amounts\n+ * of memory and needing to copy the full contents.\n+ */\n+private[spark] class ChainedBuffer(chunkSize: Int) {"
  }],
  "prId": 4450
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "I would maybe say explicitly that the disadvantage is we can't view/return it as a single large array. Otherwise it's not clear why ArrayBuffer isn't just implemented this way.\n",
    "commit": "8c70dd909c20af6dacaf4091e964a1239a221016",
    "createdAt": "2015-03-30T19:36:24Z",
    "diffHunk": "@@ -0,0 +1,134 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.collection\n+\n+import java.io.OutputStream\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.storage.BlockObjectWriter\n+\n+/**\n+ * A logical byte buffer that wraps a list of byte arrays. All the byte arrays have equal size. The\n+ * advantage of this over a standard ArrayBuffer is that it can grow without claiming large amounts"
  }],
  "prId": 4450
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "It seems weird to me that this ChainedBuffer abstraction should be aware of a BlockObjectWriter - no concrete suggestion at this point (just getting started with this patch), but perhaps there is a clear way to separate the concerns here.\n",
    "commit": "8c70dd909c20af6dacaf4091e964a1239a221016",
    "createdAt": "2015-03-30T19:45:46Z",
    "diffHunk": "@@ -0,0 +1,134 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.collection\n+\n+import java.io.OutputStream\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.storage.BlockObjectWriter\n+\n+/**\n+ * A logical byte buffer that wraps a list of byte arrays. All the byte arrays have equal size. The\n+ * advantage of this over a standard ArrayBuffer is that it can grow without claiming large amounts\n+ * of memory and needing to copy the full contents.\n+ */\n+private[spark] class ChainedBuffer(chunkSize: Int) {\n+  private val chunkSizeLog2 = (math.log(chunkSize) / math.log(2)).toInt\n+  assert(math.pow(2, chunkSizeLog2).toInt == chunkSize)\n+  private val chunks: ArrayBuffer[Array[Byte]] = new ArrayBuffer[Array[Byte]]()\n+  private var _size: Int = _\n+\n+  /**\n+   * Feed bytes from this buffer into a BlockObjectWriter.\n+   *\n+   * @param pos Offset in the buffer to read from.\n+   * @param writer BlockObjectWriter to read into.\n+   * @param len Number of bytes to read.\n+   */\n+  def read(pos: Int, writer: BlockObjectWriter, len: Int): Unit = {"
  }, {
    "author": {
      "login": "pwendell"
    },
    "body": "one small way to improve it would be to have this be called:\n\n```\nwriteSegmentToStream(pos: Int, len: Int, stream: OutputStream)\n```\n\nThen you could have a simple output stream that wraps the block object writer and just calls writeBytes.\n",
    "commit": "8c70dd909c20af6dacaf4091e964a1239a221016",
    "createdAt": "2015-04-26T20:23:20Z",
    "diffHunk": "@@ -0,0 +1,134 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.collection\n+\n+import java.io.OutputStream\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.storage.BlockObjectWriter\n+\n+/**\n+ * A logical byte buffer that wraps a list of byte arrays. All the byte arrays have equal size. The\n+ * advantage of this over a standard ArrayBuffer is that it can grow without claiming large amounts\n+ * of memory and needing to copy the full contents.\n+ */\n+private[spark] class ChainedBuffer(chunkSize: Int) {\n+  private val chunkSizeLog2 = (math.log(chunkSize) / math.log(2)).toInt\n+  assert(math.pow(2, chunkSizeLog2).toInt == chunkSize)\n+  private val chunks: ArrayBuffer[Array[Byte]] = new ArrayBuffer[Array[Byte]]()\n+  private var _size: Int = _\n+\n+  /**\n+   * Feed bytes from this buffer into a BlockObjectWriter.\n+   *\n+   * @param pos Offset in the buffer to read from.\n+   * @param writer BlockObjectWriter to read into.\n+   * @param len Number of bytes to read.\n+   */\n+  def read(pos: Int, writer: BlockObjectWriter, len: Int): Unit = {"
  }],
  "prId": 4450
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "Do these asserts happen always at runtime (or only when JVM assertions are enabled)? It would be good to give a clear error message if this assertion triggers, just print out the chunkSize and say that it needs to be a power of 2. Right now the chunksize can be set by the user, so having invalid chunk sizes could happen pretty easily.\n",
    "commit": "8c70dd909c20af6dacaf4091e964a1239a221016",
    "createdAt": "2015-04-26T20:26:32Z",
    "diffHunk": "@@ -0,0 +1,134 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.collection\n+\n+import java.io.OutputStream\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.storage.BlockObjectWriter\n+\n+/**\n+ * A logical byte buffer that wraps a list of byte arrays. All the byte arrays have equal size. The\n+ * advantage of this over a standard ArrayBuffer is that it can grow without claiming large amounts\n+ * of memory and needing to copy the full contents.\n+ */\n+private[spark] class ChainedBuffer(chunkSize: Int) {\n+  private val chunkSizeLog2 = (math.log(chunkSize) / math.log(2)).toInt\n+  assert(math.pow(2, chunkSizeLog2).toInt == chunkSize)"
  }],
  "prId": 4450
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "For some reason in my head this is clearer saying \"written\" rather than \"read\". Either is correct though since you are both reading and writing.\n\n```\nvar written = 0\n    while (written < len) {\n      val toWrite = math.min(len - written, chunkSize - posInChunk)\n      writer.writeBytes(chunks(chunkIndex), posInChunk, toWrite)\n      written += toWrite\n      chunkIndex += 1\n      posInChunk = 0\n    }\n```\n",
    "commit": "8c70dd909c20af6dacaf4091e964a1239a221016",
    "createdAt": "2015-04-26T20:38:29Z",
    "diffHunk": "@@ -0,0 +1,134 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.collection\n+\n+import java.io.OutputStream\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.storage.BlockObjectWriter\n+\n+/**\n+ * A logical byte buffer that wraps a list of byte arrays. All the byte arrays have equal size. The\n+ * advantage of this over a standard ArrayBuffer is that it can grow without claiming large amounts\n+ * of memory and needing to copy the full contents.\n+ */\n+private[spark] class ChainedBuffer(chunkSize: Int) {\n+  private val chunkSizeLog2 = (math.log(chunkSize) / math.log(2)).toInt\n+  assert(math.pow(2, chunkSizeLog2).toInt == chunkSize)\n+  private val chunks: ArrayBuffer[Array[Byte]] = new ArrayBuffer[Array[Byte]]()\n+  private var _size: Int = _\n+\n+  /**\n+   * Feed bytes from this buffer into a BlockObjectWriter.\n+   *\n+   * @param pos Offset in the buffer to read from.\n+   * @param writer BlockObjectWriter to read into.\n+   * @param len Number of bytes to read.\n+   */\n+  def read(pos: Int, writer: BlockObjectWriter, len: Int): Unit = {\n+    var chunkIndex = pos >> chunkSizeLog2\n+    var posInChunk = pos - (chunkIndex << chunkSizeLog2)\n+    var moved = 0\n+    while (moved < len) {\n+      val toRead = math.min(len - moved, chunkSize - posInChunk)"
  }],
  "prId": 4450
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "It would be good to do bounds checking on the input here. Right now if someone asks for a bad position, it will just barf when indexing into `chunks`.\n",
    "commit": "8c70dd909c20af6dacaf4091e964a1239a221016",
    "createdAt": "2015-04-26T20:39:55Z",
    "diffHunk": "@@ -0,0 +1,134 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.collection\n+\n+import java.io.OutputStream\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.storage.BlockObjectWriter\n+\n+/**\n+ * A logical byte buffer that wraps a list of byte arrays. All the byte arrays have equal size. The\n+ * advantage of this over a standard ArrayBuffer is that it can grow without claiming large amounts\n+ * of memory and needing to copy the full contents.\n+ */\n+private[spark] class ChainedBuffer(chunkSize: Int) {\n+  private val chunkSizeLog2 = (math.log(chunkSize) / math.log(2)).toInt\n+  assert(math.pow(2, chunkSizeLog2).toInt == chunkSize)\n+  private val chunks: ArrayBuffer[Array[Byte]] = new ArrayBuffer[Array[Byte]]()\n+  private var _size: Int = _\n+\n+  /**\n+   * Feed bytes from this buffer into a BlockObjectWriter.\n+   *\n+   * @param pos Offset in the buffer to read from.\n+   * @param writer BlockObjectWriter to read into.\n+   * @param len Number of bytes to read.\n+   */\n+  def read(pos: Int, writer: BlockObjectWriter, len: Int): Unit = {\n+    var chunkIndex = pos >> chunkSizeLog2\n+    var posInChunk = pos - (chunkIndex << chunkSizeLog2)"
  }],
  "prId": 4450
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "also feel like this should be `written` (minor)\n",
    "commit": "8c70dd909c20af6dacaf4091e964a1239a221016",
    "createdAt": "2015-04-26T20:43:02Z",
    "diffHunk": "@@ -0,0 +1,134 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.collection\n+\n+import java.io.OutputStream\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.storage.BlockObjectWriter\n+\n+/**\n+ * A logical byte buffer that wraps a list of byte arrays. All the byte arrays have equal size. The\n+ * advantage of this over a standard ArrayBuffer is that it can grow without claiming large amounts\n+ * of memory and needing to copy the full contents.\n+ */\n+private[spark] class ChainedBuffer(chunkSize: Int) {\n+  private val chunkSizeLog2 = (math.log(chunkSize) / math.log(2)).toInt\n+  assert(math.pow(2, chunkSizeLog2).toInt == chunkSize)\n+  private val chunks: ArrayBuffer[Array[Byte]] = new ArrayBuffer[Array[Byte]]()\n+  private var _size: Int = _\n+\n+  /**\n+   * Feed bytes from this buffer into a BlockObjectWriter.\n+   *\n+   * @param pos Offset in the buffer to read from.\n+   * @param writer BlockObjectWriter to read into.\n+   * @param len Number of bytes to read.\n+   */\n+  def read(pos: Int, writer: BlockObjectWriter, len: Int): Unit = {\n+    var chunkIndex = pos >> chunkSizeLog2\n+    var posInChunk = pos - (chunkIndex << chunkSizeLog2)\n+    var moved = 0"
  }],
  "prId": 4450
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "also feel like this should be `written` (minor)\n",
    "commit": "8c70dd909c20af6dacaf4091e964a1239a221016",
    "createdAt": "2015-04-26T20:43:29Z",
    "diffHunk": "@@ -0,0 +1,134 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.collection\n+\n+import java.io.OutputStream\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.storage.BlockObjectWriter\n+\n+/**\n+ * A logical byte buffer that wraps a list of byte arrays. All the byte arrays have equal size. The\n+ * advantage of this over a standard ArrayBuffer is that it can grow without claiming large amounts\n+ * of memory and needing to copy the full contents.\n+ */\n+private[spark] class ChainedBuffer(chunkSize: Int) {\n+  private val chunkSizeLog2 = (math.log(chunkSize) / math.log(2)).toInt\n+  assert(math.pow(2, chunkSizeLog2).toInt == chunkSize)\n+  private val chunks: ArrayBuffer[Array[Byte]] = new ArrayBuffer[Array[Byte]]()\n+  private var _size: Int = _\n+\n+  /**\n+   * Feed bytes from this buffer into a BlockObjectWriter.\n+   *\n+   * @param pos Offset in the buffer to read from.\n+   * @param writer BlockObjectWriter to read into.\n+   * @param len Number of bytes to read.\n+   */\n+  def read(pos: Int, writer: BlockObjectWriter, len: Int): Unit = {\n+    var chunkIndex = pos >> chunkSizeLog2\n+    var posInChunk = pos - (chunkIndex << chunkSizeLog2)\n+    var moved = 0\n+    while (moved < len) {\n+      val toRead = math.min(len - moved, chunkSize - posInChunk)\n+      writer.writeBytes(chunks(chunkIndex), posInChunk, toRead)\n+      moved += toRead\n+      chunkIndex += 1\n+      posInChunk = 0\n+    }\n+  }\n+\n+  /**\n+   * Read bytes from this buffer into a byte array.\n+   *\n+   * @param pos Offset in the buffer to read from.\n+   * @param bytes Byte array to read into.\n+   * @param offs Offset in the byte array to read to.\n+   * @param len Number of bytes to read.\n+   */\n+  def read(pos: Int, bytes: Array[Byte], offs: Int, len: Int): Unit = {\n+    var chunkIndex = pos >> chunkSizeLog2\n+    var posInChunk = pos - (chunkIndex << chunkSizeLog2)\n+    var moved = 0\n+    while (moved < len) {\n+      val toRead = math.min(len - moved, chunkSize - posInChunk)\n+      System.arraycopy(chunks(chunkIndex), posInChunk, bytes, offs + moved, toRead)\n+      moved += toRead\n+      chunkIndex += 1\n+      posInChunk = 0\n+    }\n+  }\n+\n+  /**\n+   * Write bytes from a byte array into this buffer.\n+   *\n+   * @param pos Offset in the buffer to write to.\n+   * @param bytes Byte array to write from.\n+   * @param offs Offset in the byte array to write from.\n+   * @param len Number of bytes to write.\n+   */\n+  def write(pos: Int, bytes: Array[Byte], offs: Int, len: Int): Unit = {\n+    // Grow if needed\n+    val endChunkIndex = (pos + len - 1) >> chunkSizeLog2\n+    while (endChunkIndex >= chunks.length) {\n+      chunks += new Array[Byte](chunkSize)\n+    }\n+\n+    var chunkIndex = pos >> chunkSizeLog2\n+    var posInChunk = pos - (chunkIndex << chunkSizeLog2)\n+    var moved = 0"
  }],
  "prId": 4450
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "is this used anywhere?\n",
    "commit": "8c70dd909c20af6dacaf4091e964a1239a221016",
    "createdAt": "2015-04-26T20:45:23Z",
    "diffHunk": "@@ -0,0 +1,134 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.collection\n+\n+import java.io.OutputStream\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.storage.BlockObjectWriter\n+\n+/**\n+ * A logical byte buffer that wraps a list of byte arrays. All the byte arrays have equal size. The\n+ * advantage of this over a standard ArrayBuffer is that it can grow without claiming large amounts\n+ * of memory and needing to copy the full contents.\n+ */\n+private[spark] class ChainedBuffer(chunkSize: Int) {\n+  private val chunkSizeLog2 = (math.log(chunkSize) / math.log(2)).toInt\n+  assert(math.pow(2, chunkSizeLog2).toInt == chunkSize)\n+  private val chunks: ArrayBuffer[Array[Byte]] = new ArrayBuffer[Array[Byte]]()\n+  private var _size: Int = _\n+\n+  /**\n+   * Feed bytes from this buffer into a BlockObjectWriter.\n+   *\n+   * @param pos Offset in the buffer to read from.\n+   * @param writer BlockObjectWriter to read into.\n+   * @param len Number of bytes to read.\n+   */\n+  def read(pos: Int, writer: BlockObjectWriter, len: Int): Unit = {\n+    var chunkIndex = pos >> chunkSizeLog2\n+    var posInChunk = pos - (chunkIndex << chunkSizeLog2)\n+    var moved = 0\n+    while (moved < len) {\n+      val toRead = math.min(len - moved, chunkSize - posInChunk)\n+      writer.writeBytes(chunks(chunkIndex), posInChunk, toRead)\n+      moved += toRead\n+      chunkIndex += 1\n+      posInChunk = 0\n+    }\n+  }\n+\n+  /**\n+   * Read bytes from this buffer into a byte array.\n+   *\n+   * @param pos Offset in the buffer to read from.\n+   * @param bytes Byte array to read into.\n+   * @param offs Offset in the byte array to read to.\n+   * @param len Number of bytes to read.\n+   */\n+  def read(pos: Int, bytes: Array[Byte], offs: Int, len: Int): Unit = {\n+    var chunkIndex = pos >> chunkSizeLog2\n+    var posInChunk = pos - (chunkIndex << chunkSizeLog2)\n+    var moved = 0\n+    while (moved < len) {\n+      val toRead = math.min(len - moved, chunkSize - posInChunk)\n+      System.arraycopy(chunks(chunkIndex), posInChunk, bytes, offs + moved, toRead)\n+      moved += toRead\n+      chunkIndex += 1\n+      posInChunk = 0\n+    }\n+  }\n+\n+  /**\n+   * Write bytes from a byte array into this buffer.\n+   *\n+   * @param pos Offset in the buffer to write to.\n+   * @param bytes Byte array to write from.\n+   * @param offs Offset in the byte array to write from.\n+   * @param len Number of bytes to write.\n+   */\n+  def write(pos: Int, bytes: Array[Byte], offs: Int, len: Int): Unit = {\n+    // Grow if needed\n+    val endChunkIndex = (pos + len - 1) >> chunkSizeLog2\n+    while (endChunkIndex >= chunks.length) {\n+      chunks += new Array[Byte](chunkSize)\n+    }\n+\n+    var chunkIndex = pos >> chunkSizeLog2\n+    var posInChunk = pos - (chunkIndex << chunkSizeLog2)\n+    var moved = 0\n+    while (moved < len) {\n+      val toWrite = math.min(len - moved, chunkSize - posInChunk)\n+      System.arraycopy(bytes, offs + moved, chunks(chunkIndex), posInChunk, toWrite)\n+      moved += toWrite\n+      chunkIndex += 1\n+      posInChunk = 0\n+    }\n+\n+    _size = math.max(_size, pos + len)\n+  }\n+\n+  /**\n+   * Total size of buffer that can be written to without allocating additional memory.\n+   */\n+  def capacity: Int = chunks.size * chunkSize\n+\n+  /**\n+   * Size of the logical buffer.\n+   */\n+  def size: Int = _size\n+}\n+\n+/**\n+ * Output stream that writes to a ChainedBuffer.\n+ */\n+private[spark] class ChainedBufferOutputStream(chainedBuffer: ChainedBuffer) extends OutputStream {\n+  private var _pos = 0\n+\n+  override def write(b: Int): Unit = {\n+    throw new UnsupportedOperationException()\n+  }\n+\n+  override def write(bytes: Array[Byte], offs: Int, len: Int): Unit = {\n+    chainedBuffer.write(_pos, bytes, offs, len)\n+    _pos += len\n+  }\n+  \n+  def pos: Int = _pos"
  }, {
    "author": {
      "login": "sryza"
    },
    "body": "Oops, nope, must be vestigial.\n",
    "commit": "8c70dd909c20af6dacaf4091e964a1239a221016",
    "createdAt": "2015-04-27T21:03:35Z",
    "diffHunk": "@@ -0,0 +1,134 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.collection\n+\n+import java.io.OutputStream\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.storage.BlockObjectWriter\n+\n+/**\n+ * A logical byte buffer that wraps a list of byte arrays. All the byte arrays have equal size. The\n+ * advantage of this over a standard ArrayBuffer is that it can grow without claiming large amounts\n+ * of memory and needing to copy the full contents.\n+ */\n+private[spark] class ChainedBuffer(chunkSize: Int) {\n+  private val chunkSizeLog2 = (math.log(chunkSize) / math.log(2)).toInt\n+  assert(math.pow(2, chunkSizeLog2).toInt == chunkSize)\n+  private val chunks: ArrayBuffer[Array[Byte]] = new ArrayBuffer[Array[Byte]]()\n+  private var _size: Int = _\n+\n+  /**\n+   * Feed bytes from this buffer into a BlockObjectWriter.\n+   *\n+   * @param pos Offset in the buffer to read from.\n+   * @param writer BlockObjectWriter to read into.\n+   * @param len Number of bytes to read.\n+   */\n+  def read(pos: Int, writer: BlockObjectWriter, len: Int): Unit = {\n+    var chunkIndex = pos >> chunkSizeLog2\n+    var posInChunk = pos - (chunkIndex << chunkSizeLog2)\n+    var moved = 0\n+    while (moved < len) {\n+      val toRead = math.min(len - moved, chunkSize - posInChunk)\n+      writer.writeBytes(chunks(chunkIndex), posInChunk, toRead)\n+      moved += toRead\n+      chunkIndex += 1\n+      posInChunk = 0\n+    }\n+  }\n+\n+  /**\n+   * Read bytes from this buffer into a byte array.\n+   *\n+   * @param pos Offset in the buffer to read from.\n+   * @param bytes Byte array to read into.\n+   * @param offs Offset in the byte array to read to.\n+   * @param len Number of bytes to read.\n+   */\n+  def read(pos: Int, bytes: Array[Byte], offs: Int, len: Int): Unit = {\n+    var chunkIndex = pos >> chunkSizeLog2\n+    var posInChunk = pos - (chunkIndex << chunkSizeLog2)\n+    var moved = 0\n+    while (moved < len) {\n+      val toRead = math.min(len - moved, chunkSize - posInChunk)\n+      System.arraycopy(chunks(chunkIndex), posInChunk, bytes, offs + moved, toRead)\n+      moved += toRead\n+      chunkIndex += 1\n+      posInChunk = 0\n+    }\n+  }\n+\n+  /**\n+   * Write bytes from a byte array into this buffer.\n+   *\n+   * @param pos Offset in the buffer to write to.\n+   * @param bytes Byte array to write from.\n+   * @param offs Offset in the byte array to write from.\n+   * @param len Number of bytes to write.\n+   */\n+  def write(pos: Int, bytes: Array[Byte], offs: Int, len: Int): Unit = {\n+    // Grow if needed\n+    val endChunkIndex = (pos + len - 1) >> chunkSizeLog2\n+    while (endChunkIndex >= chunks.length) {\n+      chunks += new Array[Byte](chunkSize)\n+    }\n+\n+    var chunkIndex = pos >> chunkSizeLog2\n+    var posInChunk = pos - (chunkIndex << chunkSizeLog2)\n+    var moved = 0\n+    while (moved < len) {\n+      val toWrite = math.min(len - moved, chunkSize - posInChunk)\n+      System.arraycopy(bytes, offs + moved, chunks(chunkIndex), posInChunk, toWrite)\n+      moved += toWrite\n+      chunkIndex += 1\n+      posInChunk = 0\n+    }\n+\n+    _size = math.max(_size, pos + len)\n+  }\n+\n+  /**\n+   * Total size of buffer that can be written to without allocating additional memory.\n+   */\n+  def capacity: Int = chunks.size * chunkSize\n+\n+  /**\n+   * Size of the logical buffer.\n+   */\n+  def size: Int = _size\n+}\n+\n+/**\n+ * Output stream that writes to a ChainedBuffer.\n+ */\n+private[spark] class ChainedBufferOutputStream(chainedBuffer: ChainedBuffer) extends OutputStream {\n+  private var _pos = 0\n+\n+  override def write(b: Int): Unit = {\n+    throw new UnsupportedOperationException()\n+  }\n+\n+  override def write(bytes: Array[Byte], offs: Int, len: Int): Unit = {\n+    chainedBuffer.write(_pos, bytes, offs, len)\n+    _pos += len\n+  }\n+  \n+  def pos: Int = _pos"
  }],
  "prId": 4450
}]