[{
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Don't you need to update `_pos`?\n",
    "commit": "3447bb995b53c4d93154328c7c7c06e08a5ec9b9",
    "createdAt": "2015-04-07T21:37:25Z",
    "diffHunk": "@@ -0,0 +1,70 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util\n+\n+import java.io.OutputStream\n+import java.nio.ByteBuffer\n+\n+import org.apache.spark.network.buffer.{LargeByteBufferHelper, WrappedLargeByteBuffer, LargeByteBuffer}\n+import org.apache.spark.util.io.ByteArrayChunkOutputStream\n+\n+private[spark]\n+class LargeByteBufferOutputStream(chunkSize: Int = 65536)\n+  extends OutputStream {\n+\n+  private[util] val output = new ByteArrayChunkOutputStream(chunkSize)\n+\n+  private var _pos = 0\n+\n+  override def write(b: Int): Unit = {\n+    output.write(b)"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "In fact, it seems like `_pos` is not really used.\n",
    "commit": "3447bb995b53c4d93154328c7c7c06e08a5ec9b9",
    "createdAt": "2015-04-07T21:38:12Z",
    "diffHunk": "@@ -0,0 +1,70 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util\n+\n+import java.io.OutputStream\n+import java.nio.ByteBuffer\n+\n+import org.apache.spark.network.buffer.{LargeByteBufferHelper, WrappedLargeByteBuffer, LargeByteBuffer}\n+import org.apache.spark.util.io.ByteArrayChunkOutputStream\n+\n+private[spark]\n+class LargeByteBufferOutputStream(chunkSize: Int = 65536)\n+  extends OutputStream {\n+\n+  private[util] val output = new ByteArrayChunkOutputStream(chunkSize)\n+\n+  private var _pos = 0\n+\n+  override def write(b: Int): Unit = {\n+    output.write(b)"
  }],
  "prId": 5400
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: space before `-`\n",
    "commit": "3447bb995b53c4d93154328c7c7c06e08a5ec9b9",
    "createdAt": "2015-04-07T21:40:02Z",
    "diffHunk": "@@ -0,0 +1,70 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util\n+\n+import java.io.OutputStream\n+import java.nio.ByteBuffer\n+\n+import org.apache.spark.network.buffer.{LargeByteBufferHelper, WrappedLargeByteBuffer, LargeByteBuffer}\n+import org.apache.spark.util.io.ByteArrayChunkOutputStream\n+\n+private[spark]\n+class LargeByteBufferOutputStream(chunkSize: Int = 65536)\n+  extends OutputStream {\n+\n+  private[util] val output = new ByteArrayChunkOutputStream(chunkSize)\n+\n+  private var _pos = 0\n+\n+  override def write(b: Int): Unit = {\n+    output.write(b)\n+  }\n+\n+  override def write(bytes: Array[Byte], offs: Int, len: Int): Unit = {\n+    output.write(bytes, offs, len)\n+    _pos += len\n+  }\n+\n+  def largeBuffer: LargeByteBuffer = {\n+    largeBuffer(LargeByteBufferHelper.MAX_CHUNK)\n+  }\n+\n+  // exposed for testing\n+  private[util] def largeBuffer(maxChunk: Int): LargeByteBuffer = {\n+    // LargeByteBuffer is supposed to make a \"best effort\" to get all the data\n+    // in one nio.ByteBuffer, so we want to try to merge the smaller chunks together\n+    // as much as possible.  This is necessary b/c there are a number of parts of spark that\n+    // can only deal w/ one nio.ByteBuffer, and can't use a LargeByteBuffer yet.\n+    val totalSize = output.size\n+    val chunksNeeded = ((totalSize + maxChunk -1) / maxChunk).toInt"
  }],
  "prId": 5400
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: spaces around `{`\n",
    "commit": "3447bb995b53c4d93154328c7c7c06e08a5ec9b9",
    "createdAt": "2015-04-07T21:40:19Z",
    "diffHunk": "@@ -0,0 +1,70 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util\n+\n+import java.io.OutputStream\n+import java.nio.ByteBuffer\n+\n+import org.apache.spark.network.buffer.{LargeByteBufferHelper, WrappedLargeByteBuffer, LargeByteBuffer}\n+import org.apache.spark.util.io.ByteArrayChunkOutputStream\n+\n+private[spark]\n+class LargeByteBufferOutputStream(chunkSize: Int = 65536)\n+  extends OutputStream {\n+\n+  private[util] val output = new ByteArrayChunkOutputStream(chunkSize)\n+\n+  private var _pos = 0\n+\n+  override def write(b: Int): Unit = {\n+    output.write(b)\n+  }\n+\n+  override def write(bytes: Array[Byte], offs: Int, len: Int): Unit = {\n+    output.write(bytes, offs, len)\n+    _pos += len\n+  }\n+\n+  def largeBuffer: LargeByteBuffer = {\n+    largeBuffer(LargeByteBufferHelper.MAX_CHUNK)\n+  }\n+\n+  // exposed for testing\n+  private[util] def largeBuffer(maxChunk: Int): LargeByteBuffer = {\n+    // LargeByteBuffer is supposed to make a \"best effort\" to get all the data\n+    // in one nio.ByteBuffer, so we want to try to merge the smaller chunks together\n+    // as much as possible.  This is necessary b/c there are a number of parts of spark that\n+    // can only deal w/ one nio.ByteBuffer, and can't use a LargeByteBuffer yet.\n+    val totalSize = output.size\n+    val chunksNeeded = ((totalSize + maxChunk -1) / maxChunk).toInt\n+    val chunks = new Array[Array[Byte]](chunksNeeded)\n+    var remaining = totalSize\n+    var pos = 0\n+    (0 until chunksNeeded).foreach{idx =>"
  }],
  "prId": 5400
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: `.map(ByteBuffer.wrap)`\n",
    "commit": "3447bb995b53c4d93154328c7c7c06e08a5ec9b9",
    "createdAt": "2015-04-07T21:40:45Z",
    "diffHunk": "@@ -0,0 +1,70 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util\n+\n+import java.io.OutputStream\n+import java.nio.ByteBuffer\n+\n+import org.apache.spark.network.buffer.{LargeByteBufferHelper, WrappedLargeByteBuffer, LargeByteBuffer}\n+import org.apache.spark.util.io.ByteArrayChunkOutputStream\n+\n+private[spark]\n+class LargeByteBufferOutputStream(chunkSize: Int = 65536)\n+  extends OutputStream {\n+\n+  private[util] val output = new ByteArrayChunkOutputStream(chunkSize)\n+\n+  private var _pos = 0\n+\n+  override def write(b: Int): Unit = {\n+    output.write(b)\n+  }\n+\n+  override def write(bytes: Array[Byte], offs: Int, len: Int): Unit = {\n+    output.write(bytes, offs, len)\n+    _pos += len\n+  }\n+\n+  def largeBuffer: LargeByteBuffer = {\n+    largeBuffer(LargeByteBufferHelper.MAX_CHUNK)\n+  }\n+\n+  // exposed for testing\n+  private[util] def largeBuffer(maxChunk: Int): LargeByteBuffer = {\n+    // LargeByteBuffer is supposed to make a \"best effort\" to get all the data\n+    // in one nio.ByteBuffer, so we want to try to merge the smaller chunks together\n+    // as much as possible.  This is necessary b/c there are a number of parts of spark that\n+    // can only deal w/ one nio.ByteBuffer, and can't use a LargeByteBuffer yet.\n+    val totalSize = output.size\n+    val chunksNeeded = ((totalSize + maxChunk -1) / maxChunk).toInt\n+    val chunks = new Array[Array[Byte]](chunksNeeded)\n+    var remaining = totalSize\n+    var pos = 0\n+    (0 until chunksNeeded).foreach{idx =>\n+      val nextSize = math.min(maxChunk, remaining).toInt\n+      chunks(idx) = output.slice(pos, pos + nextSize)\n+      pos += nextSize\n+      remaining -= nextSize\n+    }\n+    new WrappedLargeByteBuffer(chunks.map{ByteBuffer.wrap})"
  }],
  "prId": 5400
}]