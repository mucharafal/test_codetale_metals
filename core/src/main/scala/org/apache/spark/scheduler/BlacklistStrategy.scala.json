[{
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "you mean, implementations are free to ignore the taskID -- but the caller _must_ provide the taskID, right?  so it should not be an `Option[Long]` in the signature, it should just be a `Long` and the implementations can ignore.\n",
    "commit": "ecad5ffe2bcc02f9b181a209172f22294e6af4ef",
    "createdAt": "2015-09-22T02:31:17Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.util.SystemClock\n+\n+/**\n+ * The interface to determine executor blacklist and node blacklist.\n+ */\n+trait BlacklistStrategy {\n+  val expireTimeInMillisecond: Long\n+\n+  // TaskId is optional here because the BlacklistStrategy could be unrelated to TaskId"
  }, {
    "author": {
      "login": "mwws"
    },
    "body": "Not exactly. If TaskID unrelated strategy is applied, the caller should be valid to not provide taskID, in this case, I think input \"None\" is better than \"null\"? The logic is:\n1. Caller provide TaskID + ID related Strategy => works fine\n2. Caller provide TaskID + ID unrelated Strategy => print warn log to announce that provided ID will be ignored, and works as Strategy defined.\n3. Caller NOT provide TaskID + ID related Strategy => Strategy can decide the behavior, eg: return blacklisted executor of all IDs or return empty, or even throw exception\n4. Caller NOT provide TaskID + ID unrelated Strategy => works fine\n",
    "commit": "ecad5ffe2bcc02f9b181a209172f22294e6af4ef",
    "createdAt": "2015-09-22T02:58:55Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.util.SystemClock\n+\n+/**\n+ * The interface to determine executor blacklist and node blacklist.\n+ */\n+trait BlacklistStrategy {\n+  val expireTimeInMillisecond: Long\n+\n+  // TaskId is optional here because the BlacklistStrategy could be unrelated to TaskId"
  }, {
    "author": {
      "login": "squito"
    },
    "body": "I think I'm still missing something.  Whats the point of cases 3 & 4?  Is there a situation in which the caller wouldn't have access to a task id, and so couldn't provide it?  The only place I see it called, it will always get called with `Some(taskId)`.  And if there is, would it actually make more sense as a different method?\n\nOtherwise this seems like unusual interface design.  The caller shouldn't care what the actual implementation does -- it just passes in a the task id, b/c the implementation _might_ need it.  Implementations are free to ignore it.  In fact, this even means that in situation (2), you shouldn't be logging a warning either, its expected that the caller will give you a task id even if you don't need it.\n\nI do agree that we should `Option` rather than `null` if we do need that flexibility, I'm just not seeing why its necessary.\n",
    "commit": "ecad5ffe2bcc02f9b181a209172f22294e6af4ef",
    "createdAt": "2015-09-22T13:40:09Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.util.SystemClock\n+\n+/**\n+ * The interface to determine executor blacklist and node blacklist.\n+ */\n+trait BlacklistStrategy {\n+  val expireTimeInMillisecond: Long\n+\n+  // TaskId is optional here because the BlacklistStrategy could be unrelated to TaskId"
  }, {
    "author": {
      "login": "mwws"
    },
    "body": "You are right, it's not used now. But it might be necessary, to avoid allocating new YARN container on blacklisted node, \"YarnSchedulerBackend\" will call \"nodeBlacklist\" (nodeBlacklist could be depend on executorBlacklist, eg: if more than 3 blacked executor is on the same node, we think the node is also blacked), and YarnSchedulerBackend is not able to provide taskId.\n\nAnd I agree that define two method instead is clearer and better interface design. I will remove Option here.\n",
    "commit": "ecad5ffe2bcc02f9b181a209172f22294e6af4ef",
    "createdAt": "2015-09-23T02:23:57Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.util.SystemClock\n+\n+/**\n+ * The interface to determine executor blacklist and node blacklist.\n+ */\n+trait BlacklistStrategy {\n+  val expireTimeInMillisecond: Long\n+\n+  // TaskId is optional here because the BlacklistStrategy could be unrelated to TaskId"
  }, {
    "author": {
      "login": "squito"
    },
    "body": "ah, I see now.  I was originally referring to both `getExecutorBlacklist` and `getNodeBlacklist` -- thanks for pointing out that `getNodeBlacklist` actually is used w/out a `taskId`, I had missed that.\n\nI'd like to avoid over-engineering this api -- it'll be more clear to future readers & easier to test if we keep it minimal, so I am thinking that maybe we should just have two methods:\n\n``` scala\ndef getExecutorBlacklist(\n    failedExecutorMap: mutable.HashMap[String, FailureStatus],\n    taskId: Long): Set[String]\n)\n\ndef getNodeBlacklist(failedExecutorMap: mutable.HashMap[String, FailureStatus]): Set[String]\n```\n\nthat is, `getExecutorBlacklist` always takes a `taskId`, and `getNodeBlacklist` never does.  Does that support the required use case?  (Docs would also really help explain the purpose of these methods.)  Feel free to push back if I'm still missing something.\n",
    "commit": "ecad5ffe2bcc02f9b181a209172f22294e6af4ef",
    "createdAt": "2015-09-23T15:11:27Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.util.SystemClock\n+\n+/**\n+ * The interface to determine executor blacklist and node blacklist.\n+ */\n+trait BlacklistStrategy {\n+  val expireTimeInMillisecond: Long\n+\n+  // TaskId is optional here because the BlacklistStrategy could be unrelated to TaskId"
  }, {
    "author": {
      "login": "mwws"
    },
    "body": "My original thinking was becasue (1) nodeBlacklist could depend on the result of executorBlacklist (2) executorBlacklist might be taskId related ==> then nodeBlacklist could also be taskId related. Your suggestion extinguish the possibility to make nodeBlacklist taskId related. And In \"speculate\" mode, when calling `getExecutorBlacklist` related to some taskId, we would like to `getNodeBlacklist` related to the same taskId.\n\nMaybe we just keep both method with/without taskId to make the API more extendable and provide additional layer of trait to simplify the implementation?\n",
    "commit": "ecad5ffe2bcc02f9b181a209172f22294e6af4ef",
    "createdAt": "2015-09-24T02:58:27Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.util.SystemClock\n+\n+/**\n+ * The interface to determine executor blacklist and node blacklist.\n+ */\n+trait BlacklistStrategy {\n+  val expireTimeInMillisecond: Long\n+\n+  // TaskId is optional here because the BlacklistStrategy could be unrelated to TaskId"
  }, {
    "author": {
      "login": "mwws"
    },
    "body": "Rethink about it, for \"speculate\" mode, we can also call taskId unrelated `getNodeBlacklist`, it should be fine. And I have simplified the API as you suggested.\n",
    "commit": "ecad5ffe2bcc02f9b181a209172f22294e6af4ef",
    "createdAt": "2015-09-24T09:11:01Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.util.SystemClock\n+\n+/**\n+ * The interface to determine executor blacklist and node blacklist.\n+ */\n+trait BlacklistStrategy {\n+  val expireTimeInMillisecond: Long\n+\n+  // TaskId is optional here because the BlacklistStrategy could be unrelated to TaskId"
  }],
  "prId": 8760
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "these functions get called a _lot_ (at least once per scheduled task).  They will almost always return the same answer in successive calls, but they are doing non-trivial work in each call (several operations over large collections, eg. filtering, summing values, etc.).  We should probably cache those values and just update when there are more failures (probably need to be careful on thread-safety).\n",
    "commit": "ecad5ffe2bcc02f9b181a209172f22294e6af4ef",
    "createdAt": "2015-09-22T14:03:44Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.util.SystemClock\n+\n+/**\n+ * The interface to determine executor blacklist and node blacklist.\n+ */\n+trait BlacklistStrategy {\n+  val expireTimeInMillisecond: Long\n+\n+  // TaskId is optional here because the BlacklistStrategy could be unrelated to TaskId\n+  def getExecutorBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus],\n+      taskId: Option[Long]): Set[String]\n+\n+  def getNodeBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus],\n+      taskId: Option[Long]): Set[String]\n+\n+  // Default implementation to remove failure executors from HashMap based on given time period.\n+  def expireExecutorsInBlackList(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus]): Unit = {\n+    val now = new SystemClock().getTimeMillis()\n+    failedExecutorMap.retain((executorid, failureStatus) => {\n+      (now - failureStatus.updatedTime) < expireTimeInMillisecond\n+    })\n+  }\n+}\n+\n+/**\n+ * The interface for taskId unrelated strategy. In this case, provided taskId will be ignored. The\n+ * benefit for taskId unrelated strategy is different taskSets can learn experience from other\n+ * taskSet to avoid allocating tasks to problematic executors.\n+ */\n+trait TaskUnrelatedBlacklistStrategy extends BlacklistStrategy with Logging{\n+  def getExecutorBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus],\n+      taskId: Option[Long]): Set[String] = {\n+    if (taskId.isDefined){\n+      logWarning(\"TaskId unrelated strategy is applied, given taskId will be ignored\")\n+    }\n+    getExecutorBlacklist(failedExecutorMap)\n+  }\n+\n+  def getNodeBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus],\n+      taskId: Option[Long]): Set[String] = {\n+    if (taskId.isDefined){\n+      logWarning(\"TaskId unrelated strategy is applied, given taskId will be ignored\")\n+    }\n+    getNodeBlacklist(failedExecutorMap)\n+  }\n+\n+  // define new interface which remove taskId from parameter.\n+  def getExecutorBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus]): Set[String]\n+  def getNodeBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus]): Set[String]\n+}\n+\n+/**\n+ * This strategy is simply based on given threshold and is taskId unrelated. An executor will be\n+ * in blacklist, if it failed more than \"maxFailureTaskNumber\" times. A node will be in blacklist,\n+ * if there are more than \"maxBlackExecutorNumber\" executors on it in executor blacklist.\n+ */\n+class SimpleStrategy(\n+    maxFailureTaskNumber: Int,\n+    maxBlackExecutorNumber: Int,\n+    val expireTimeInMillisecond: Long\n+  )extends TaskUnrelatedBlacklistStrategy {\n+\n+  private def getSelectedExecutorMap(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus]) = {\n+    failedExecutorMap.filter{\n+      case (id, failureStatus) => failureStatus.failureTimes > maxFailureTaskNumber\n+    }\n+  }\n+\n+  def getExecutorBlacklist(\n+      failureExecutors: mutable.HashMap[String, FailureStatus]): Set[String] = {\n+    getSelectedExecutorMap(failureExecutors).keys.toSet\n+  }\n+\n+  def getNodeBlacklist(\n+      failureExecutors: mutable.HashMap[String, FailureStatus]): Set[String] = {"
  }, {
    "author": {
      "login": "mwws"
    },
    "body": "If the failureExecutors become large, it would be an issue. For the cache, do you mean maintain a local HashMap in this class as cache?\n",
    "commit": "ecad5ffe2bcc02f9b181a209172f22294e6af4ef",
    "createdAt": "2015-09-23T03:13:29Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.util.SystemClock\n+\n+/**\n+ * The interface to determine executor blacklist and node blacklist.\n+ */\n+trait BlacklistStrategy {\n+  val expireTimeInMillisecond: Long\n+\n+  // TaskId is optional here because the BlacklistStrategy could be unrelated to TaskId\n+  def getExecutorBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus],\n+      taskId: Option[Long]): Set[String]\n+\n+  def getNodeBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus],\n+      taskId: Option[Long]): Set[String]\n+\n+  // Default implementation to remove failure executors from HashMap based on given time period.\n+  def expireExecutorsInBlackList(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus]): Unit = {\n+    val now = new SystemClock().getTimeMillis()\n+    failedExecutorMap.retain((executorid, failureStatus) => {\n+      (now - failureStatus.updatedTime) < expireTimeInMillisecond\n+    })\n+  }\n+}\n+\n+/**\n+ * The interface for taskId unrelated strategy. In this case, provided taskId will be ignored. The\n+ * benefit for taskId unrelated strategy is different taskSets can learn experience from other\n+ * taskSet to avoid allocating tasks to problematic executors.\n+ */\n+trait TaskUnrelatedBlacklistStrategy extends BlacklistStrategy with Logging{\n+  def getExecutorBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus],\n+      taskId: Option[Long]): Set[String] = {\n+    if (taskId.isDefined){\n+      logWarning(\"TaskId unrelated strategy is applied, given taskId will be ignored\")\n+    }\n+    getExecutorBlacklist(failedExecutorMap)\n+  }\n+\n+  def getNodeBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus],\n+      taskId: Option[Long]): Set[String] = {\n+    if (taskId.isDefined){\n+      logWarning(\"TaskId unrelated strategy is applied, given taskId will be ignored\")\n+    }\n+    getNodeBlacklist(failedExecutorMap)\n+  }\n+\n+  // define new interface which remove taskId from parameter.\n+  def getExecutorBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus]): Set[String]\n+  def getNodeBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus]): Set[String]\n+}\n+\n+/**\n+ * This strategy is simply based on given threshold and is taskId unrelated. An executor will be\n+ * in blacklist, if it failed more than \"maxFailureTaskNumber\" times. A node will be in blacklist,\n+ * if there are more than \"maxBlackExecutorNumber\" executors on it in executor blacklist.\n+ */\n+class SimpleStrategy(\n+    maxFailureTaskNumber: Int,\n+    maxBlackExecutorNumber: Int,\n+    val expireTimeInMillisecond: Long\n+  )extends TaskUnrelatedBlacklistStrategy {\n+\n+  private def getSelectedExecutorMap(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus]) = {\n+    failedExecutorMap.filter{\n+      case (id, failureStatus) => failureStatus.failureTimes > maxFailureTaskNumber\n+    }\n+  }\n+\n+  def getExecutorBlacklist(\n+      failureExecutors: mutable.HashMap[String, FailureStatus]): Set[String] = {\n+    getSelectedExecutorMap(failureExecutors).keys.toSet\n+  }\n+\n+  def getNodeBlacklist(\n+      failureExecutors: mutable.HashMap[String, FailureStatus]): Set[String] = {"
  }, {
    "author": {
      "login": "mwws"
    },
    "body": "Current thinking is maintain extra \"blacklistExecutors\" and \"blacklistNodes\" map as cache and add a AtomicBoolean mark \"isChanged\" to decide if we need to refresh cache. \n\n\"isChanged\" will be set to true when failureExecutors updated. And when we call getExecutorBlacklist, it will check \"isChanged\" if true, refresh cache, set \"isChanged\" to false and return, If false, direct return the value in cache.\n\nI am now implementing it, any comments, let me know. Thanks\n",
    "commit": "ecad5ffe2bcc02f9b181a209172f22294e6af4ef",
    "createdAt": "2015-09-23T09:34:48Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.util.SystemClock\n+\n+/**\n+ * The interface to determine executor blacklist and node blacklist.\n+ */\n+trait BlacklistStrategy {\n+  val expireTimeInMillisecond: Long\n+\n+  // TaskId is optional here because the BlacklistStrategy could be unrelated to TaskId\n+  def getExecutorBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus],\n+      taskId: Option[Long]): Set[String]\n+\n+  def getNodeBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus],\n+      taskId: Option[Long]): Set[String]\n+\n+  // Default implementation to remove failure executors from HashMap based on given time period.\n+  def expireExecutorsInBlackList(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus]): Unit = {\n+    val now = new SystemClock().getTimeMillis()\n+    failedExecutorMap.retain((executorid, failureStatus) => {\n+      (now - failureStatus.updatedTime) < expireTimeInMillisecond\n+    })\n+  }\n+}\n+\n+/**\n+ * The interface for taskId unrelated strategy. In this case, provided taskId will be ignored. The\n+ * benefit for taskId unrelated strategy is different taskSets can learn experience from other\n+ * taskSet to avoid allocating tasks to problematic executors.\n+ */\n+trait TaskUnrelatedBlacklistStrategy extends BlacklistStrategy with Logging{\n+  def getExecutorBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus],\n+      taskId: Option[Long]): Set[String] = {\n+    if (taskId.isDefined){\n+      logWarning(\"TaskId unrelated strategy is applied, given taskId will be ignored\")\n+    }\n+    getExecutorBlacklist(failedExecutorMap)\n+  }\n+\n+  def getNodeBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus],\n+      taskId: Option[Long]): Set[String] = {\n+    if (taskId.isDefined){\n+      logWarning(\"TaskId unrelated strategy is applied, given taskId will be ignored\")\n+    }\n+    getNodeBlacklist(failedExecutorMap)\n+  }\n+\n+  // define new interface which remove taskId from parameter.\n+  def getExecutorBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus]): Set[String]\n+  def getNodeBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus]): Set[String]\n+}\n+\n+/**\n+ * This strategy is simply based on given threshold and is taskId unrelated. An executor will be\n+ * in blacklist, if it failed more than \"maxFailureTaskNumber\" times. A node will be in blacklist,\n+ * if there are more than \"maxBlackExecutorNumber\" executors on it in executor blacklist.\n+ */\n+class SimpleStrategy(\n+    maxFailureTaskNumber: Int,\n+    maxBlackExecutorNumber: Int,\n+    val expireTimeInMillisecond: Long\n+  )extends TaskUnrelatedBlacklistStrategy {\n+\n+  private def getSelectedExecutorMap(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus]) = {\n+    failedExecutorMap.filter{\n+      case (id, failureStatus) => failureStatus.failureTimes > maxFailureTaskNumber\n+    }\n+  }\n+\n+  def getExecutorBlacklist(\n+      failureExecutors: mutable.HashMap[String, FailureStatus]): Set[String] = {\n+    getSelectedExecutorMap(failureExecutors).keys.toSet\n+  }\n+\n+  def getNodeBlacklist(\n+      failureExecutors: mutable.HashMap[String, FailureStatus]): Set[String] = {"
  }, {
    "author": {
      "login": "squito"
    },
    "body": "ah, I was thinking before that the runtime would be proportional to the number of executors, but its really just dependent on the executors w/ failed tasks.  Still I think it is probably good to minimize the work done here if we can and its not too hard.  For instance, one easy thing is just avoiding querying the sparkConf inside this function ( currently there is `sparkConf.getBoolean(\"spark.scheduler.blacklist.speculate\", false)`).\n\nI see now that it is a little hard to do the caching, given the separation between `BlacklistTracker` and `BlacklistStrategy`, since `BlacklistStrategy` is responsible for updating `failureExecutors`, and there is also a time-expiration as well.  I dont' see how a `isChanged` boolean is enough to manage this, but maybe I'm wrong -- happy to look at the code.  I was thinking you would need a `failedExecutorVersion: Long`, which gets incremented by the tracker if its ever updated, but maybe there is something simpler.\n\nI will think about this some more ...\n",
    "commit": "ecad5ffe2bcc02f9b181a209172f22294e6af4ef",
    "createdAt": "2015-09-23T16:02:04Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.util.SystemClock\n+\n+/**\n+ * The interface to determine executor blacklist and node blacklist.\n+ */\n+trait BlacklistStrategy {\n+  val expireTimeInMillisecond: Long\n+\n+  // TaskId is optional here because the BlacklistStrategy could be unrelated to TaskId\n+  def getExecutorBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus],\n+      taskId: Option[Long]): Set[String]\n+\n+  def getNodeBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus],\n+      taskId: Option[Long]): Set[String]\n+\n+  // Default implementation to remove failure executors from HashMap based on given time period.\n+  def expireExecutorsInBlackList(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus]): Unit = {\n+    val now = new SystemClock().getTimeMillis()\n+    failedExecutorMap.retain((executorid, failureStatus) => {\n+      (now - failureStatus.updatedTime) < expireTimeInMillisecond\n+    })\n+  }\n+}\n+\n+/**\n+ * The interface for taskId unrelated strategy. In this case, provided taskId will be ignored. The\n+ * benefit for taskId unrelated strategy is different taskSets can learn experience from other\n+ * taskSet to avoid allocating tasks to problematic executors.\n+ */\n+trait TaskUnrelatedBlacklistStrategy extends BlacklistStrategy with Logging{\n+  def getExecutorBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus],\n+      taskId: Option[Long]): Set[String] = {\n+    if (taskId.isDefined){\n+      logWarning(\"TaskId unrelated strategy is applied, given taskId will be ignored\")\n+    }\n+    getExecutorBlacklist(failedExecutorMap)\n+  }\n+\n+  def getNodeBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus],\n+      taskId: Option[Long]): Set[String] = {\n+    if (taskId.isDefined){\n+      logWarning(\"TaskId unrelated strategy is applied, given taskId will be ignored\")\n+    }\n+    getNodeBlacklist(failedExecutorMap)\n+  }\n+\n+  // define new interface which remove taskId from parameter.\n+  def getExecutorBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus]): Set[String]\n+  def getNodeBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus]): Set[String]\n+}\n+\n+/**\n+ * This strategy is simply based on given threshold and is taskId unrelated. An executor will be\n+ * in blacklist, if it failed more than \"maxFailureTaskNumber\" times. A node will be in blacklist,\n+ * if there are more than \"maxBlackExecutorNumber\" executors on it in executor blacklist.\n+ */\n+class SimpleStrategy(\n+    maxFailureTaskNumber: Int,\n+    maxBlackExecutorNumber: Int,\n+    val expireTimeInMillisecond: Long\n+  )extends TaskUnrelatedBlacklistStrategy {\n+\n+  private def getSelectedExecutorMap(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus]) = {\n+    failedExecutorMap.filter{\n+      case (id, failureStatus) => failureStatus.failureTimes > maxFailureTaskNumber\n+    }\n+  }\n+\n+  def getExecutorBlacklist(\n+      failureExecutors: mutable.HashMap[String, FailureStatus]): Set[String] = {\n+    getSelectedExecutorMap(failureExecutors).keys.toSet\n+  }\n+\n+  def getNodeBlacklist(\n+      failureExecutors: mutable.HashMap[String, FailureStatus]): Set[String] = {"
  }],
  "prId": 8760
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "This comment is helpful for me reviewing now, but I think for future readers you should describe the actual behavior :)  You can add that it was the standard behavior before spark 1.6\n",
    "commit": "ecad5ffe2bcc02f9b181a209172f22294e6af4ef",
    "createdAt": "2015-09-22T14:12:02Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.util.SystemClock\n+\n+/**\n+ * The interface to determine executor blacklist and node blacklist.\n+ */\n+trait BlacklistStrategy {\n+  val expireTimeInMillisecond: Long\n+\n+  // TaskId is optional here because the BlacklistStrategy could be unrelated to TaskId\n+  def getExecutorBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus],\n+      taskId: Option[Long]): Set[String]\n+\n+  def getNodeBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus],\n+      taskId: Option[Long]): Set[String]\n+\n+  // Default implementation to remove failure executors from HashMap based on given time period.\n+  def expireExecutorsInBlackList(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus]): Unit = {\n+    val now = new SystemClock().getTimeMillis()\n+    failedExecutorMap.retain((executorid, failureStatus) => {\n+      (now - failureStatus.updatedTime) < expireTimeInMillisecond\n+    })\n+  }\n+}\n+\n+/**\n+ * The interface for taskId unrelated strategy. In this case, provided taskId will be ignored. The\n+ * benefit for taskId unrelated strategy is different taskSets can learn experience from other\n+ * taskSet to avoid allocating tasks to problematic executors.\n+ */\n+trait TaskUnrelatedBlacklistStrategy extends BlacklistStrategy with Logging{\n+  def getExecutorBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus],\n+      taskId: Option[Long]): Set[String] = {\n+    if (taskId.isDefined){\n+      logWarning(\"TaskId unrelated strategy is applied, given taskId will be ignored\")\n+    }\n+    getExecutorBlacklist(failedExecutorMap)\n+  }\n+\n+  def getNodeBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus],\n+      taskId: Option[Long]): Set[String] = {\n+    if (taskId.isDefined){\n+      logWarning(\"TaskId unrelated strategy is applied, given taskId will be ignored\")\n+    }\n+    getNodeBlacklist(failedExecutorMap)\n+  }\n+\n+  // define new interface which remove taskId from parameter.\n+  def getExecutorBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus]): Set[String]\n+  def getNodeBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus]): Set[String]\n+}\n+\n+/**\n+ * This strategy is simply based on given threshold and is taskId unrelated. An executor will be\n+ * in blacklist, if it failed more than \"maxFailureTaskNumber\" times. A node will be in blacklist,\n+ * if there are more than \"maxBlackExecutorNumber\" executors on it in executor blacklist.\n+ */\n+class SimpleStrategy(\n+    maxFailureTaskNumber: Int,\n+    maxBlackExecutorNumber: Int,\n+    val expireTimeInMillisecond: Long\n+  )extends TaskUnrelatedBlacklistStrategy {\n+\n+  private def getSelectedExecutorMap(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus]) = {\n+    failedExecutorMap.filter{\n+      case (id, failureStatus) => failureStatus.failureTimes > maxFailureTaskNumber\n+    }\n+  }\n+\n+  def getExecutorBlacklist(\n+      failureExecutors: mutable.HashMap[String, FailureStatus]): Set[String] = {\n+    getSelectedExecutorMap(failureExecutors).keys.toSet\n+  }\n+\n+  def getNodeBlacklist(\n+      failureExecutors: mutable.HashMap[String, FailureStatus]): Set[String] = {\n+    getSelectedExecutorMap(failureExecutors)\n+      .groupBy{case (id, failureStatus) => failureStatus.host}\n+      .filter {case (host, failureExecutors) => failureExecutors.size > maxBlackExecutorNumber}\n+      .keys.toSet\n+  }\n+}\n+\n+/**\n+ * This strategy is applied as default to keep the same semantics as original."
  }, {
    "author": {
      "login": "mwws"
    },
    "body": "Sure, I will enhance the comment\n",
    "commit": "ecad5ffe2bcc02f9b181a209172f22294e6af4ef",
    "createdAt": "2015-09-23T01:42:23Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.util.SystemClock\n+\n+/**\n+ * The interface to determine executor blacklist and node blacklist.\n+ */\n+trait BlacklistStrategy {\n+  val expireTimeInMillisecond: Long\n+\n+  // TaskId is optional here because the BlacklistStrategy could be unrelated to TaskId\n+  def getExecutorBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus],\n+      taskId: Option[Long]): Set[String]\n+\n+  def getNodeBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus],\n+      taskId: Option[Long]): Set[String]\n+\n+  // Default implementation to remove failure executors from HashMap based on given time period.\n+  def expireExecutorsInBlackList(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus]): Unit = {\n+    val now = new SystemClock().getTimeMillis()\n+    failedExecutorMap.retain((executorid, failureStatus) => {\n+      (now - failureStatus.updatedTime) < expireTimeInMillisecond\n+    })\n+  }\n+}\n+\n+/**\n+ * The interface for taskId unrelated strategy. In this case, provided taskId will be ignored. The\n+ * benefit for taskId unrelated strategy is different taskSets can learn experience from other\n+ * taskSet to avoid allocating tasks to problematic executors.\n+ */\n+trait TaskUnrelatedBlacklistStrategy extends BlacklistStrategy with Logging{\n+  def getExecutorBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus],\n+      taskId: Option[Long]): Set[String] = {\n+    if (taskId.isDefined){\n+      logWarning(\"TaskId unrelated strategy is applied, given taskId will be ignored\")\n+    }\n+    getExecutorBlacklist(failedExecutorMap)\n+  }\n+\n+  def getNodeBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus],\n+      taskId: Option[Long]): Set[String] = {\n+    if (taskId.isDefined){\n+      logWarning(\"TaskId unrelated strategy is applied, given taskId will be ignored\")\n+    }\n+    getNodeBlacklist(failedExecutorMap)\n+  }\n+\n+  // define new interface which remove taskId from parameter.\n+  def getExecutorBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus]): Set[String]\n+  def getNodeBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus]): Set[String]\n+}\n+\n+/**\n+ * This strategy is simply based on given threshold and is taskId unrelated. An executor will be\n+ * in blacklist, if it failed more than \"maxFailureTaskNumber\" times. A node will be in blacklist,\n+ * if there are more than \"maxBlackExecutorNumber\" executors on it in executor blacklist.\n+ */\n+class SimpleStrategy(\n+    maxFailureTaskNumber: Int,\n+    maxBlackExecutorNumber: Int,\n+    val expireTimeInMillisecond: Long\n+  )extends TaskUnrelatedBlacklistStrategy {\n+\n+  private def getSelectedExecutorMap(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus]) = {\n+    failedExecutorMap.filter{\n+      case (id, failureStatus) => failureStatus.failureTimes > maxFailureTaskNumber\n+    }\n+  }\n+\n+  def getExecutorBlacklist(\n+      failureExecutors: mutable.HashMap[String, FailureStatus]): Set[String] = {\n+    getSelectedExecutorMap(failureExecutors).keys.toSet\n+  }\n+\n+  def getNodeBlacklist(\n+      failureExecutors: mutable.HashMap[String, FailureStatus]): Set[String] = {\n+    getSelectedExecutorMap(failureExecutors)\n+      .groupBy{case (id, failureStatus) => failureStatus.host}\n+      .filter {case (host, failureExecutors) => failureExecutors.size > maxBlackExecutorNumber}\n+      .keys.toSet\n+  }\n+}\n+\n+/**\n+ * This strategy is applied as default to keep the same semantics as original."
  }],
  "prId": 8760
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "given my comments above on the interface, I think this trait is not necessary.  Its easy enough to have the strategies just ignore `taskId` in `getExecutorBlacklist` w/out adding this extra level.\n",
    "commit": "ecad5ffe2bcc02f9b181a209172f22294e6af4ef",
    "createdAt": "2015-09-23T15:39:15Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.util.SystemClock\n+\n+/**\n+ * The interface to determine executor blacklist and node blacklist.\n+ */\n+trait BlacklistStrategy {\n+  val expireTimeInMillisecond: Long\n+\n+  // TaskId is optional here because the BlacklistStrategy could be unrelated to TaskId\n+  def getExecutorBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus],\n+      taskId: Option[Long]): Set[String]\n+\n+  def getNodeBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus],\n+      taskId: Option[Long]): Set[String]\n+\n+  // Default implementation to remove failure executors from HashMap based on given time period.\n+  def expireExecutorsInBlackList(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus]): Unit = {\n+    val now = new SystemClock().getTimeMillis()\n+    failedExecutorMap.retain((executorid, failureStatus) => {\n+      (now - failureStatus.updatedTime) < expireTimeInMillisecond\n+    })\n+  }\n+}\n+\n+/**\n+ * The interface for taskId unrelated strategy. In this case, provided taskId will be ignored. The\n+ * benefit for taskId unrelated strategy is different taskSets can learn experience from other\n+ * taskSet to avoid allocating tasks to problematic executors.\n+ */\n+trait TaskUnrelatedBlacklistStrategy extends BlacklistStrategy with Logging{"
  }],
  "prId": 8760
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "(a) this is modifying `failedExecutorMap` in the blacklist-expire-timer thread, while everything else is happening in the scheduler main event loop, so we need to synchronize on all access to it.\n\n(b) I think there is a change in behavior here.  `failureStatus.updatedTime` is getting updated even on successful task completion.  (`taskSetManager.handleSuccessfulTask` -> `tracker.updateFailureExecutors` -> `tracker.removeFailureExecutors`).  So you'll never expire an executor if it has just one failure, but then a series of successes of other tasks.  I think the right thing to do is probably to not update `failureStatus.updatedTime` on task success, and maybe rename it to `lastFailureTime`?\n",
    "commit": "ecad5ffe2bcc02f9b181a209172f22294e6af4ef",
    "createdAt": "2015-09-23T16:27:06Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.util.SystemClock\n+\n+/**\n+ * The interface to determine executor blacklist and node blacklist.\n+ */\n+trait BlacklistStrategy {\n+  val expireTimeInMillisecond: Long\n+\n+  // TaskId is optional here because the BlacklistStrategy could be unrelated to TaskId\n+  def getExecutorBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus],\n+      taskId: Option[Long]): Set[String]\n+\n+  def getNodeBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus],\n+      taskId: Option[Long]): Set[String]\n+\n+  // Default implementation to remove failure executors from HashMap based on given time period.\n+  def expireExecutorsInBlackList(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus]): Unit = {\n+    val now = new SystemClock().getTimeMillis()\n+    failedExecutorMap.retain((executorid, failureStatus) => {\n+      (now - failureStatus.updatedTime) < expireTimeInMillisecond\n+    })"
  }, {
    "author": {
      "login": "squito"
    },
    "body": "oops, ignore (a) -- sorry I had overlooked the `synchronized`s in `BlackListTracker`\n",
    "commit": "ecad5ffe2bcc02f9b181a209172f22294e6af4ef",
    "createdAt": "2015-09-23T16:39:30Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.util.SystemClock\n+\n+/**\n+ * The interface to determine executor blacklist and node blacklist.\n+ */\n+trait BlacklistStrategy {\n+  val expireTimeInMillisecond: Long\n+\n+  // TaskId is optional here because the BlacklistStrategy could be unrelated to TaskId\n+  def getExecutorBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus],\n+      taskId: Option[Long]): Set[String]\n+\n+  def getNodeBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus],\n+      taskId: Option[Long]): Set[String]\n+\n+  // Default implementation to remove failure executors from HashMap based on given time period.\n+  def expireExecutorsInBlackList(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus]): Unit = {\n+    val now = new SystemClock().getTimeMillis()\n+    failedExecutorMap.retain((executorid, failureStatus) => {\n+      (now - failureStatus.updatedTime) < expireTimeInMillisecond\n+    })"
  }, {
    "author": {
      "login": "mwws"
    },
    "body": "For (b),  Yes, there is logical flaw, and I agree with your suggestion. Thanks for pointing it out.\n",
    "commit": "ecad5ffe2bcc02f9b181a209172f22294e6af4ef",
    "createdAt": "2015-09-24T01:31:06Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.util.SystemClock\n+\n+/**\n+ * The interface to determine executor blacklist and node blacklist.\n+ */\n+trait BlacklistStrategy {\n+  val expireTimeInMillisecond: Long\n+\n+  // TaskId is optional here because the BlacklistStrategy could be unrelated to TaskId\n+  def getExecutorBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus],\n+      taskId: Option[Long]): Set[String]\n+\n+  def getNodeBlacklist(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus],\n+      taskId: Option[Long]): Set[String]\n+\n+  // Default implementation to remove failure executors from HashMap based on given time period.\n+  def expireExecutorsInBlackList(\n+      failedExecutorMap: mutable.HashMap[String, FailureStatus]): Unit = {\n+    val now = new SystemClock().getTimeMillis()\n+    failedExecutorMap.retain((executorid, failureStatus) => {\n+      (now - failureStatus.updatedTime) < expireTimeInMillisecond\n+    })"
  }],
  "prId": 8760
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "`private[scheduler]` for all classes in this file\n",
    "commit": "ecad5ffe2bcc02f9b181a209172f22294e6af4ef",
    "createdAt": "2015-10-14T14:57:53Z",
    "diffHunk": "@@ -0,0 +1,134 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.util.SystemClock\n+\n+/**\n+ * The interface to determine executor blacklist and node blacklist.\n+ */\n+trait BlacklistStrategy {"
  }],
  "prId": 8760
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "can you change all of these to doc comments?\n",
    "commit": "ecad5ffe2bcc02f9b181a209172f22294e6af4ef",
    "createdAt": "2015-10-14T14:58:49Z",
    "diffHunk": "@@ -0,0 +1,134 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.util.SystemClock\n+\n+/**\n+ * The interface to determine executor blacklist and node blacklist.\n+ */\n+trait BlacklistStrategy {\n+  val expireTimeInMillisecond: Long\n+\n+  // Return executors in blacklist which are related to given TaskId\n+  def getExecutorBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus],\n+      taskId: Long): Set[String]\n+\n+  // Return all nodes in blacklist\n+  def getNodeBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus]): Set[String]\n+\n+  // Default implementation to remove failure executors from HashMap based on given time period."
  }],
  "prId": 8760
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "can you rename to `expireTimeInMilliseconds` ('s' at the end) and add a doc?\n",
    "commit": "ecad5ffe2bcc02f9b181a209172f22294e6af4ef",
    "createdAt": "2015-10-14T14:59:16Z",
    "diffHunk": "@@ -0,0 +1,134 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.util.SystemClock\n+\n+/**\n+ * The interface to determine executor blacklist and node blacklist.\n+ */\n+trait BlacklistStrategy {\n+  val expireTimeInMillisecond: Long"
  }],
  "prId": 8760
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "I don't really see the point of adding \"strict\" -- do you think it will be a very common use case?  Otherwise its just another thing to document and might confuse users a bit.\n\nI'd also like to think of better names for the other strategies as well.  It seems the key difference is whether or not they are task-specific, which isn't very clear from \"default\" and \"threshold\".  I'm terrible at naming myself ... something like \"singleTask\" and \"completeExecutor\"?\n",
    "commit": "ecad5ffe2bcc02f9b181a209172f22294e6af4ef",
    "createdAt": "2015-10-14T15:24:30Z",
    "diffHunk": "@@ -0,0 +1,134 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.util.SystemClock\n+\n+/**\n+ * The interface to determine executor blacklist and node blacklist.\n+ */\n+trait BlacklistStrategy {\n+  val expireTimeInMillisecond: Long\n+\n+  // Return executors in blacklist which are related to given TaskId\n+  def getExecutorBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus],\n+      taskId: Long): Set[String]\n+\n+  // Return all nodes in blacklist\n+  def getNodeBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus]): Set[String]\n+\n+  // Default implementation to remove failure executors from HashMap based on given time period.\n+  def expireExecutorsInBlackList(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus]): Unit = {\n+    val now = new SystemClock().getTimeMillis()\n+    executorIdToFailureStatus.retain((executorid, failureStatus) => {\n+      (now - failureStatus.updatedTime) < expireTimeInMillisecond\n+    })\n+  }\n+}\n+\n+/**\n+ * This strategy is simply based on given threshold and is taskId unrelated. An executor will be\n+ * in blacklist, if it failed more than \"maxFailureTaskNumber\" times. A node will be in blacklist,\n+ * if there are more than \"maxBlackExecutorNumber\" executors on it in executor blacklist.\n+ *\n+ * In this case, provided taskId will be ignored. The benefit for taskId unrelated strategy is that\n+ * different taskSets can learn experience from other taskSet to avoid allocating tasks on\n+ * problematic executors.\n+ */\n+class SimpleStrategy(\n+    maxFailureTaskNumber: Int,\n+    maxBlackExecutorNumber: Int,\n+    val expireTimeInMillisecond: Long\n+  )extends BlacklistStrategy {\n+\n+  private def getSelectedExecutorMap(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus]) = {\n+    executorIdToFailureStatus.filter{\n+      case (id, failureStatus) => failureStatus.totalNumFailures > maxFailureTaskNumber\n+    }\n+  }\n+\n+  // As this is a taskId unrelated strategy, the input taskId will be ignored\n+  def getExecutorBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus],\n+      taskId: Long): Set[String] = {\n+    getSelectedExecutorMap(executorIdToFailureStatus).keys.toSet\n+  }\n+\n+  def getNodeBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus]): Set[String] = {\n+    getSelectedExecutorMap(executorIdToFailureStatus)\n+      .groupBy{case (id, failureStatus) => failureStatus.host}\n+      .filter {case (host, executorIdToFailureStatus) =>\n+        executorIdToFailureStatus.size > maxBlackExecutorNumber}\n+      .keys.toSet\n+  }\n+}\n+\n+/**\n+ * This strategy is applied as default to keep the same semantics as original. It's an taskId\n+ * related strategy. If an executor failed running \"task A\", then we think this executor is\n+ * blacked for \"task A\". And we think the executor is still healthy for other task. node blacklist\n+ * is always empty.\n+ *\n+ * It was the standard behavior before spark 1.6\n+ */\n+class DefaultStrategy(val expireTimeInMillisecond: Long) extends BlacklistStrategy {\n+  def getExecutorBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus],\n+      taskId: Long): Set[String] = {\n+    executorIdToFailureStatus.filter{\n+      case (_, failureStatus) => failureStatus.numFailuresPerTask.keySet.contains(taskId)\n+    }.keys.toSet\n+  }\n+\n+  def getNodeBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus]): Set[String] =\n+        Set.empty[String]\n+}\n+\n+/**\n+ * Create BlacklistStrategy instance according to SparkConf\n+ */\n+object BlacklistStrategy {\n+  def apply(sparkConf: SparkConf): BlacklistStrategy = {\n+    val timeout = sparkConf.getLong(\"spark.scheduler.executorTaskBlacklistTime\", 0L)\n+    sparkConf.get(\"spark.scheduler.blacklist.strategy\", \"default\") match {\n+      case \"default\" =>\n+        new DefaultStrategy(timeout)\n+      case \"threshold\" =>\n+        new SimpleStrategy(\n+            sparkConf.getInt(\"spark.scheduler.blacklist.threshold.maxFailureTaskNumber\", 3),\n+            sparkConf.getInt(\"spark.scheduler.blacklist.threshold.maxBlackExecutorNumber\", 3),\n+            timeout)\n+      case \"strict\" =>\n+        // A special case of SimpleStrategy: Once task failed  at executor,\n+        // put the executor and its node into blacklist.\n+        new SimpleStrategy(0, 0, timeout)"
  }, {
    "author": {
      "login": "mwws"
    },
    "body": "No strong reason to keep \"strict\" strategy, I will delete it.\n",
    "commit": "ecad5ffe2bcc02f9b181a209172f22294e6af4ef",
    "createdAt": "2015-10-16T00:53:00Z",
    "diffHunk": "@@ -0,0 +1,134 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.util.SystemClock\n+\n+/**\n+ * The interface to determine executor blacklist and node blacklist.\n+ */\n+trait BlacklistStrategy {\n+  val expireTimeInMillisecond: Long\n+\n+  // Return executors in blacklist which are related to given TaskId\n+  def getExecutorBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus],\n+      taskId: Long): Set[String]\n+\n+  // Return all nodes in blacklist\n+  def getNodeBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus]): Set[String]\n+\n+  // Default implementation to remove failure executors from HashMap based on given time period.\n+  def expireExecutorsInBlackList(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus]): Unit = {\n+    val now = new SystemClock().getTimeMillis()\n+    executorIdToFailureStatus.retain((executorid, failureStatus) => {\n+      (now - failureStatus.updatedTime) < expireTimeInMillisecond\n+    })\n+  }\n+}\n+\n+/**\n+ * This strategy is simply based on given threshold and is taskId unrelated. An executor will be\n+ * in blacklist, if it failed more than \"maxFailureTaskNumber\" times. A node will be in blacklist,\n+ * if there are more than \"maxBlackExecutorNumber\" executors on it in executor blacklist.\n+ *\n+ * In this case, provided taskId will be ignored. The benefit for taskId unrelated strategy is that\n+ * different taskSets can learn experience from other taskSet to avoid allocating tasks on\n+ * problematic executors.\n+ */\n+class SimpleStrategy(\n+    maxFailureTaskNumber: Int,\n+    maxBlackExecutorNumber: Int,\n+    val expireTimeInMillisecond: Long\n+  )extends BlacklistStrategy {\n+\n+  private def getSelectedExecutorMap(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus]) = {\n+    executorIdToFailureStatus.filter{\n+      case (id, failureStatus) => failureStatus.totalNumFailures > maxFailureTaskNumber\n+    }\n+  }\n+\n+  // As this is a taskId unrelated strategy, the input taskId will be ignored\n+  def getExecutorBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus],\n+      taskId: Long): Set[String] = {\n+    getSelectedExecutorMap(executorIdToFailureStatus).keys.toSet\n+  }\n+\n+  def getNodeBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus]): Set[String] = {\n+    getSelectedExecutorMap(executorIdToFailureStatus)\n+      .groupBy{case (id, failureStatus) => failureStatus.host}\n+      .filter {case (host, executorIdToFailureStatus) =>\n+        executorIdToFailureStatus.size > maxBlackExecutorNumber}\n+      .keys.toSet\n+  }\n+}\n+\n+/**\n+ * This strategy is applied as default to keep the same semantics as original. It's an taskId\n+ * related strategy. If an executor failed running \"task A\", then we think this executor is\n+ * blacked for \"task A\". And we think the executor is still healthy for other task. node blacklist\n+ * is always empty.\n+ *\n+ * It was the standard behavior before spark 1.6\n+ */\n+class DefaultStrategy(val expireTimeInMillisecond: Long) extends BlacklistStrategy {\n+  def getExecutorBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus],\n+      taskId: Long): Set[String] = {\n+    executorIdToFailureStatus.filter{\n+      case (_, failureStatus) => failureStatus.numFailuresPerTask.keySet.contains(taskId)\n+    }.keys.toSet\n+  }\n+\n+  def getNodeBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus]): Set[String] =\n+        Set.empty[String]\n+}\n+\n+/**\n+ * Create BlacklistStrategy instance according to SparkConf\n+ */\n+object BlacklistStrategy {\n+  def apply(sparkConf: SparkConf): BlacklistStrategy = {\n+    val timeout = sparkConf.getLong(\"spark.scheduler.executorTaskBlacklistTime\", 0L)\n+    sparkConf.get(\"spark.scheduler.blacklist.strategy\", \"default\") match {\n+      case \"default\" =>\n+        new DefaultStrategy(timeout)\n+      case \"threshold\" =>\n+        new SimpleStrategy(\n+            sparkConf.getInt(\"spark.scheduler.blacklist.threshold.maxFailureTaskNumber\", 3),\n+            sparkConf.getInt(\"spark.scheduler.blacklist.threshold.maxBlackExecutorNumber\", 3),\n+            timeout)\n+      case \"strict\" =>\n+        // A special case of SimpleStrategy: Once task failed  at executor,\n+        // put the executor and its node into blacklist.\n+        new SimpleStrategy(0, 0, timeout)"
  }],
  "prId": 8760
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "this should take a `Clock` for testing\n",
    "commit": "ecad5ffe2bcc02f9b181a209172f22294e6af4ef",
    "createdAt": "2015-10-14T19:49:31Z",
    "diffHunk": "@@ -0,0 +1,134 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.util.SystemClock\n+\n+/**\n+ * The interface to determine executor blacklist and node blacklist.\n+ */\n+trait BlacklistStrategy {\n+  val expireTimeInMillisecond: Long\n+\n+  // Return executors in blacklist which are related to given TaskId\n+  def getExecutorBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus],\n+      taskId: Long): Set[String]\n+\n+  // Return all nodes in blacklist\n+  def getNodeBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus]): Set[String]\n+\n+  // Default implementation to remove failure executors from HashMap based on given time period.\n+  def expireExecutorsInBlackList(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus]): Unit = {\n+    val now = new SystemClock().getTimeMillis()"
  }],
  "prId": 8760
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "given my comments on the docs, change this to \n\n``` scala\nsparkConf.getTimeAsSeconds(\"spark.scheduler.blacklist.timeout\", sparkConf.getLong(\"spark.scheduler.executorTaskBlacklistTime\", 0L) / 1000)\n```\n",
    "commit": "ecad5ffe2bcc02f9b181a209172f22294e6af4ef",
    "createdAt": "2015-11-03T19:59:54Z",
    "diffHunk": "@@ -0,0 +1,145 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.util.SystemClock\n+import org.apache.spark.util.Clock\n+\n+/**\n+ * The interface to determine executor blacklist and node blacklist.\n+ */\n+private [scheduler] trait BlacklistStrategy {\n+  /** Defined a time interval to expire failure information of executors */\n+  val expireTimeInMilliseconds: Long\n+\n+  /** Return executors in blacklist which are related to given taskIndex */\n+  def getExecutorBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus],\n+      taskIndex: Int, clock: Clock): Set[String]\n+\n+  /** Return all nodes in blacklist */\n+  def getNodeBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus]): Set[String]\n+\n+  /**\n+   * Default implementation to remove failure executors from HashMap based on given time period.\n+   * The return value identity whether or not it updated anything\n+   */\n+  def expireExecutorsInBlackList(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus], clock: Clock): Boolean = {\n+    val now = clock.getTimeMillis()\n+    val expiredKey = executorIdToFailureStatus.filter {\n+      case (executorid, failureStatus) => {\n+        (now - failureStatus.updatedTime) >= expireTimeInMilliseconds\n+      }\n+    }.keySet\n+\n+    if (expiredKey.isEmpty) {\n+      false\n+    } else {\n+      executorIdToFailureStatus --= expiredKey\n+      true\n+    }\n+  }\n+}\n+\n+/**\n+ * This strategy is simply based on given threshold and is taskId unrelated. An executor will be\n+ * in blacklist, if it failed more than \"maxFailureTaskNumber\" times. A node will be in blacklist,\n+ * if there are more than \"maxBlackExecutorNumber\" executors on it in executor blacklist.\n+ *\n+ * In this case, provided taskId will be ignored. The benefit for taskId unrelated strategy is that\n+ * different taskSets can learn experience from other taskSet to avoid allocating tasks on\n+ * problematic executors.\n+ */\n+private[scheduler] class SimpleStrategy(\n+    maxFailureTaskNumber: Int,\n+    maxBlackExecutorNumber: Int,\n+    val expireTimeInMilliseconds: Long\n+  )extends BlacklistStrategy {\n+\n+  private def getSelectedExecutorMap(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus]) = {\n+    executorIdToFailureStatus.filter{\n+      case (id, failureStatus) => failureStatus.totalNumFailures > maxFailureTaskNumber\n+    }\n+  }\n+\n+  // As this is a taskId unrelated strategy, the input taskId will be ignored\n+  def getExecutorBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus],\n+      taskIndex: Int, clock: Clock): Set[String] = {\n+    getSelectedExecutorMap(executorIdToFailureStatus).keys.toSet\n+  }\n+\n+  def getNodeBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus]): Set[String] = {\n+    getSelectedExecutorMap(executorIdToFailureStatus)\n+      .groupBy{case (id, failureStatus) => failureStatus.host}\n+      .filter {case (host, executorIdToFailureStatus) =>\n+        executorIdToFailureStatus.size > maxBlackExecutorNumber}\n+      .keys.toSet\n+  }\n+}\n+\n+/**\n+ * This strategy is applied as default to keep the same semantics as original. It's an taskId\n+ * related strategy. If an executor failed running \"task A\", then we think this executor is\n+ * blacked for \"task A\". And we think the executor is still healthy for other task. node blacklist\n+ * is always empty.\n+ *\n+ * It was the standard behavior before spark 1.6\n+ */\n+private[scheduler] class DefaultStrategy(\n+    val expireTimeInMilliseconds: Long) extends BlacklistStrategy {\n+  def getExecutorBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus],\n+      taskIndex: Int, clock: Clock): Set[String] = {\n+    executorIdToFailureStatus.filter{\n+      case (_, failureStatus) => failureStatus.numFailuresPerTask.keySet.contains(taskIndex) &&\n+        clock.getTimeMillis() - failureStatus.updatedTime < expireTimeInMilliseconds\n+    }.keys.toSet\n+  }\n+\n+  def getNodeBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus]): Set[String] =\n+        Set.empty[String]\n+}\n+\n+/**\n+ * Create BlacklistStrategy instance according to SparkConf\n+ */\n+private[scheduler] object BlacklistStrategy {\n+  def apply(sparkConf: SparkConf): BlacklistStrategy = {\n+    val timeout = sparkConf.getLong(\"spark.scheduler.executorTaskBlacklistTime\", 0L)"
  }, {
    "author": {
      "login": "mwws"
    },
    "body": "use _getTimeAsMs_ instead and I think time unit could be automatically converted \"s <--> ms <--> us \" by this method, so it's not necessary to divide 1000 here.\n",
    "commit": "ecad5ffe2bcc02f9b181a209172f22294e6af4ef",
    "createdAt": "2015-11-05T02:38:12Z",
    "diffHunk": "@@ -0,0 +1,145 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.util.SystemClock\n+import org.apache.spark.util.Clock\n+\n+/**\n+ * The interface to determine executor blacklist and node blacklist.\n+ */\n+private [scheduler] trait BlacklistStrategy {\n+  /** Defined a time interval to expire failure information of executors */\n+  val expireTimeInMilliseconds: Long\n+\n+  /** Return executors in blacklist which are related to given taskIndex */\n+  def getExecutorBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus],\n+      taskIndex: Int, clock: Clock): Set[String]\n+\n+  /** Return all nodes in blacklist */\n+  def getNodeBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus]): Set[String]\n+\n+  /**\n+   * Default implementation to remove failure executors from HashMap based on given time period.\n+   * The return value identity whether or not it updated anything\n+   */\n+  def expireExecutorsInBlackList(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus], clock: Clock): Boolean = {\n+    val now = clock.getTimeMillis()\n+    val expiredKey = executorIdToFailureStatus.filter {\n+      case (executorid, failureStatus) => {\n+        (now - failureStatus.updatedTime) >= expireTimeInMilliseconds\n+      }\n+    }.keySet\n+\n+    if (expiredKey.isEmpty) {\n+      false\n+    } else {\n+      executorIdToFailureStatus --= expiredKey\n+      true\n+    }\n+  }\n+}\n+\n+/**\n+ * This strategy is simply based on given threshold and is taskId unrelated. An executor will be\n+ * in blacklist, if it failed more than \"maxFailureTaskNumber\" times. A node will be in blacklist,\n+ * if there are more than \"maxBlackExecutorNumber\" executors on it in executor blacklist.\n+ *\n+ * In this case, provided taskId will be ignored. The benefit for taskId unrelated strategy is that\n+ * different taskSets can learn experience from other taskSet to avoid allocating tasks on\n+ * problematic executors.\n+ */\n+private[scheduler] class SimpleStrategy(\n+    maxFailureTaskNumber: Int,\n+    maxBlackExecutorNumber: Int,\n+    val expireTimeInMilliseconds: Long\n+  )extends BlacklistStrategy {\n+\n+  private def getSelectedExecutorMap(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus]) = {\n+    executorIdToFailureStatus.filter{\n+      case (id, failureStatus) => failureStatus.totalNumFailures > maxFailureTaskNumber\n+    }\n+  }\n+\n+  // As this is a taskId unrelated strategy, the input taskId will be ignored\n+  def getExecutorBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus],\n+      taskIndex: Int, clock: Clock): Set[String] = {\n+    getSelectedExecutorMap(executorIdToFailureStatus).keys.toSet\n+  }\n+\n+  def getNodeBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus]): Set[String] = {\n+    getSelectedExecutorMap(executorIdToFailureStatus)\n+      .groupBy{case (id, failureStatus) => failureStatus.host}\n+      .filter {case (host, executorIdToFailureStatus) =>\n+        executorIdToFailureStatus.size > maxBlackExecutorNumber}\n+      .keys.toSet\n+  }\n+}\n+\n+/**\n+ * This strategy is applied as default to keep the same semantics as original. It's an taskId\n+ * related strategy. If an executor failed running \"task A\", then we think this executor is\n+ * blacked for \"task A\". And we think the executor is still healthy for other task. node blacklist\n+ * is always empty.\n+ *\n+ * It was the standard behavior before spark 1.6\n+ */\n+private[scheduler] class DefaultStrategy(\n+    val expireTimeInMilliseconds: Long) extends BlacklistStrategy {\n+  def getExecutorBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus],\n+      taskIndex: Int, clock: Clock): Set[String] = {\n+    executorIdToFailureStatus.filter{\n+      case (_, failureStatus) => failureStatus.numFailuresPerTask.keySet.contains(taskIndex) &&\n+        clock.getTimeMillis() - failureStatus.updatedTime < expireTimeInMilliseconds\n+    }.keys.toSet\n+  }\n+\n+  def getNodeBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus]): Set[String] =\n+        Set.empty[String]\n+}\n+\n+/**\n+ * Create BlacklistStrategy instance according to SparkConf\n+ */\n+private[scheduler] object BlacklistStrategy {\n+  def apply(sparkConf: SparkConf): BlacklistStrategy = {\n+    val timeout = sparkConf.getLong(\"spark.scheduler.executorTaskBlacklistTime\", 0L)"
  }],
  "prId": 8760
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "nit: blank line between the `scala` and the `org.apache.spark` imports\n",
    "commit": "ecad5ffe2bcc02f9b181a209172f22294e6af4ef",
    "createdAt": "2015-11-24T15:45:36Z",
    "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+import org.apache.spark.Logging"
  }],
  "prId": 8760
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "nit: each arg on its own line\n",
    "commit": "ecad5ffe2bcc02f9b181a209172f22294e6af4ef",
    "createdAt": "2015-11-24T15:46:41Z",
    "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.util.SystemClock\n+import org.apache.spark.util.Clock\n+\n+/**\n+ * The interface to determine executor blacklist and node blacklist.\n+ */\n+private [scheduler] trait BlacklistStrategy {\n+  /** Defined a time interval to expire failure information of executors */\n+  val expireTimeInMilliseconds: Long\n+\n+  /** Return executors in blacklist which are related to given taskIndex */\n+  def getExecutorBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus],\n+      atomTask: BlacklistAtomTask, clock: Clock): Set[String]"
  }],
  "prId": 8760
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "each arg on its own line\n",
    "commit": "ecad5ffe2bcc02f9b181a209172f22294e6af4ef",
    "createdAt": "2015-11-24T15:47:06Z",
    "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.util.SystemClock\n+import org.apache.spark.util.Clock\n+\n+/**\n+ * The interface to determine executor blacklist and node blacklist.\n+ */\n+private [scheduler] trait BlacklistStrategy {\n+  /** Defined a time interval to expire failure information of executors */\n+  val expireTimeInMilliseconds: Long\n+\n+  /** Return executors in blacklist which are related to given taskIndex */\n+  def getExecutorBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus],\n+      atomTask: BlacklistAtomTask, clock: Clock): Set[String]\n+\n+  /** Return all nodes in blacklist */\n+  def getNodeBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus]): Set[String]\n+\n+  /**\n+   * Default implementation to remove failure executors from HashMap based on given time period.\n+   * The return value identity whether or not it updated anything\n+   */\n+  def expireExecutorsInBlackList(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus], clock: Clock): Boolean = {"
  }],
  "prId": 8760
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "Reword to:\nChoose which executors (if any) should be removed from the blacklist.  Return true if any executors are removed from the blacklist, false otherwise.   The default implementation removes exectors from the blacklist after [[expireTimeInMillis]].\n",
    "commit": "ecad5ffe2bcc02f9b181a209172f22294e6af4ef",
    "createdAt": "2015-11-24T15:52:34Z",
    "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.util.SystemClock\n+import org.apache.spark.util.Clock\n+\n+/**\n+ * The interface to determine executor blacklist and node blacklist.\n+ */\n+private [scheduler] trait BlacklistStrategy {\n+  /** Defined a time interval to expire failure information of executors */\n+  val expireTimeInMilliseconds: Long\n+\n+  /** Return executors in blacklist which are related to given taskIndex */\n+  def getExecutorBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus],\n+      atomTask: BlacklistAtomTask, clock: Clock): Set[String]\n+\n+  /** Return all nodes in blacklist */\n+  def getNodeBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus]): Set[String]\n+\n+  /**\n+   * Default implementation to remove failure executors from HashMap based on given time period.\n+   * The return value identity whether or not it updated anything"
  }],
  "prId": 8760
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "nit: space after `)`\n",
    "commit": "ecad5ffe2bcc02f9b181a209172f22294e6af4ef",
    "createdAt": "2015-11-24T15:53:21Z",
    "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.util.SystemClock\n+import org.apache.spark.util.Clock\n+\n+/**\n+ * The interface to determine executor blacklist and node blacklist.\n+ */\n+private [scheduler] trait BlacklistStrategy {\n+  /** Defined a time interval to expire failure information of executors */\n+  val expireTimeInMilliseconds: Long\n+\n+  /** Return executors in blacklist which are related to given taskIndex */\n+  def getExecutorBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus],\n+      atomTask: BlacklistAtomTask, clock: Clock): Set[String]\n+\n+  /** Return all nodes in blacklist */\n+  def getNodeBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus]): Set[String]\n+\n+  /**\n+   * Default implementation to remove failure executors from HashMap based on given time period.\n+   * The return value identity whether or not it updated anything\n+   */\n+  def expireExecutorsInBlackList(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus], clock: Clock): Boolean = {\n+    val now = clock.getTimeMillis()\n+    val expiredKey = executorIdToFailureStatus.filter {\n+      case (executorid, failureStatus) => {\n+        (now - failureStatus.updatedTime) >= expireTimeInMilliseconds\n+      }\n+    }.keySet\n+\n+    if (expiredKey.isEmpty) {\n+      false\n+    } else {\n+      executorIdToFailureStatus --= expiredKey\n+      true\n+    }\n+  }\n+}\n+\n+/**\n+ * This strategy is simply based on given threshold and is taskId unrelated. An executor will be\n+ * in blacklist, if it failed more than \"maxFailureTaskNumber\" times. A node will be in blacklist,\n+ * if there are more than \"maxBlackExecutorNumber\" executors on it in executor blacklist.\n+ *\n+ * In this case, provided taskId will be ignored. The benefit for taskId unrelated strategy is that\n+ * different taskSets can learn experience from other taskSet to avoid allocating tasks on\n+ * problematic executors.\n+ */\n+private[scheduler] class SimpleStrategy(\n+    maxFailureTaskNumber: Int,\n+    maxBlackExecutorNumber: Int,\n+    val expireTimeInMilliseconds: Long\n+  )extends BlacklistStrategy {"
  }],
  "prId": 8760
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "each arg on its own line\n",
    "commit": "ecad5ffe2bcc02f9b181a209172f22294e6af4ef",
    "createdAt": "2015-11-24T15:55:03Z",
    "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.util.SystemClock\n+import org.apache.spark.util.Clock\n+\n+/**\n+ * The interface to determine executor blacklist and node blacklist.\n+ */\n+private [scheduler] trait BlacklistStrategy {\n+  /** Defined a time interval to expire failure information of executors */\n+  val expireTimeInMilliseconds: Long\n+\n+  /** Return executors in blacklist which are related to given taskIndex */\n+  def getExecutorBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus],\n+      atomTask: BlacklistAtomTask, clock: Clock): Set[String]\n+\n+  /** Return all nodes in blacklist */\n+  def getNodeBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus]): Set[String]\n+\n+  /**\n+   * Default implementation to remove failure executors from HashMap based on given time period.\n+   * The return value identity whether or not it updated anything\n+   */\n+  def expireExecutorsInBlackList(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus], clock: Clock): Boolean = {\n+    val now = clock.getTimeMillis()\n+    val expiredKey = executorIdToFailureStatus.filter {\n+      case (executorid, failureStatus) => {\n+        (now - failureStatus.updatedTime) >= expireTimeInMilliseconds\n+      }\n+    }.keySet\n+\n+    if (expiredKey.isEmpty) {\n+      false\n+    } else {\n+      executorIdToFailureStatus --= expiredKey\n+      true\n+    }\n+  }\n+}\n+\n+/**\n+ * This strategy is simply based on given threshold and is taskId unrelated. An executor will be\n+ * in blacklist, if it failed more than \"maxFailureTaskNumber\" times. A node will be in blacklist,\n+ * if there are more than \"maxBlackExecutorNumber\" executors on it in executor blacklist.\n+ *\n+ * In this case, provided taskId will be ignored. The benefit for taskId unrelated strategy is that\n+ * different taskSets can learn experience from other taskSet to avoid allocating tasks on\n+ * problematic executors.\n+ */\n+private[scheduler] class SimpleStrategy(\n+    maxFailureTaskNumber: Int,\n+    maxBlackExecutorNumber: Int,\n+    val expireTimeInMilliseconds: Long\n+  )extends BlacklistStrategy {\n+\n+  private def getSelectedExecutorMap(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus]) = {\n+    executorIdToFailureStatus.filter{\n+      case (id, failureStatus) => failureStatus.totalNumFailures > maxFailureTaskNumber\n+    }\n+  }\n+\n+  // As this is a taskId unrelated strategy, the input taskId will be ignored\n+  def getExecutorBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus],\n+      atomTask: BlacklistAtomTask, clock: Clock): Set[String] = {"
  }],
  "prId": 8760
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "IllegalArgumentException\n",
    "commit": "ecad5ffe2bcc02f9b181a209172f22294e6af4ef",
    "createdAt": "2015-11-24T18:07:50Z",
    "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.util.SystemClock\n+import org.apache.spark.util.Clock\n+\n+/**\n+ * The interface to determine executor blacklist and node blacklist.\n+ */\n+private [scheduler] trait BlacklistStrategy {\n+  /** Defined a time interval to expire failure information of executors */\n+  val expireTimeInMilliseconds: Long\n+\n+  /** Return executors in blacklist which are related to given taskIndex */\n+  def getExecutorBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus],\n+      atomTask: BlacklistAtomTask, clock: Clock): Set[String]\n+\n+  /** Return all nodes in blacklist */\n+  def getNodeBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus]): Set[String]\n+\n+  /**\n+   * Default implementation to remove failure executors from HashMap based on given time period.\n+   * The return value identity whether or not it updated anything\n+   */\n+  def expireExecutorsInBlackList(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus], clock: Clock): Boolean = {\n+    val now = clock.getTimeMillis()\n+    val expiredKey = executorIdToFailureStatus.filter {\n+      case (executorid, failureStatus) => {\n+        (now - failureStatus.updatedTime) >= expireTimeInMilliseconds\n+      }\n+    }.keySet\n+\n+    if (expiredKey.isEmpty) {\n+      false\n+    } else {\n+      executorIdToFailureStatus --= expiredKey\n+      true\n+    }\n+  }\n+}\n+\n+/**\n+ * This strategy is simply based on given threshold and is taskId unrelated. An executor will be\n+ * in blacklist, if it failed more than \"maxFailureTaskNumber\" times. A node will be in blacklist,\n+ * if there are more than \"maxBlackExecutorNumber\" executors on it in executor blacklist.\n+ *\n+ * In this case, provided taskId will be ignored. The benefit for taskId unrelated strategy is that\n+ * different taskSets can learn experience from other taskSet to avoid allocating tasks on\n+ * problematic executors.\n+ */\n+private[scheduler] class SimpleStrategy(\n+    maxFailureTaskNumber: Int,\n+    maxBlackExecutorNumber: Int,\n+    val expireTimeInMilliseconds: Long\n+  )extends BlacklistStrategy {\n+\n+  private def getSelectedExecutorMap(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus]) = {\n+    executorIdToFailureStatus.filter{\n+      case (id, failureStatus) => failureStatus.totalNumFailures > maxFailureTaskNumber\n+    }\n+  }\n+\n+  // As this is a taskId unrelated strategy, the input taskId will be ignored\n+  def getExecutorBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus],\n+      atomTask: BlacklistAtomTask, clock: Clock): Set[String] = {\n+    getSelectedExecutorMap(executorIdToFailureStatus).keys.toSet\n+  }\n+\n+  def getNodeBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus]): Set[String] = {\n+    getSelectedExecutorMap(executorIdToFailureStatus)\n+      .groupBy{case (id, failureStatus) => failureStatus.host}\n+      .filter {case (host, executorIdToFailureStatus) =>\n+        executorIdToFailureStatus.size > maxBlackExecutorNumber}\n+      .keys.toSet\n+  }\n+}\n+\n+/**\n+ * This strategy is applied as default to keep the same semantics as original. It's an taskId\n+ * related strategy. If an executor failed running \"task A\", then we think this executor is\n+ * blacked for \"task A\". And we think the executor is still healthy for other task. node blacklist\n+ * is always empty.\n+ *\n+ * It was the standard behavior before spark 1.6\n+ */\n+private[scheduler] class DefaultStrategy(\n+    val expireTimeInMilliseconds: Long) extends BlacklistStrategy {\n+  def getExecutorBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus],\n+      atomTask: BlacklistAtomTask, clock: Clock): Set[String] = {\n+    executorIdToFailureStatus.filter{\n+      case (_, failureStatus) => failureStatus.numFailuresPerTask.keySet.contains(atomTask) &&\n+        clock.getTimeMillis() - failureStatus.updatedTime < expireTimeInMilliseconds\n+    }.keys.toSet\n+  }\n+\n+  def getNodeBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus]): Set[String] =\n+        Set.empty[String]\n+}\n+\n+/**\n+ * Create BlacklistStrategy instance according to SparkConf\n+ */\n+private[scheduler] object BlacklistStrategy {\n+  def apply(sparkConf: SparkConf): BlacklistStrategy = {\n+    val timeout = sparkConf.getTimeAsMs(\"spark.scheduler.blacklist.timeout\",\n+        sparkConf.getLong(\"spark.scheduler.executorTaskBlacklistTime\", 0L).toString() + \"ms\")\n+    sparkConf.get(\"spark.scheduler.blacklist.strategy\", \"default\") match {\n+      case \"default\" =>\n+        new DefaultStrategy(timeout)\n+      case \"threshold\" =>\n+        new SimpleStrategy(\n+            sparkConf.getInt(\"spark.scheduler.blacklist.threshold.maxFailureTaskNumber\", 3),\n+            sparkConf.getInt(\"spark.scheduler.blacklist.threshold.maxBlackExecutorNumber\", 3),\n+            timeout)\n+      case unsupported =>\n+        throw new Exception(s\"No matching blacklist strategy for: $unsupported\")"
  }],
  "prId": 8760
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "can you rename \n\"default\" -> \"SingleTask\"\n\"threshold\" -> \"ExecutorAndNode\"\n, both the conf names and the class names?\nI don't love those names either, but I think \"default\" and \"threshold\" are not clear at all, so am just trying to choose something better.\n",
    "commit": "ecad5ffe2bcc02f9b181a209172f22294e6af4ef",
    "createdAt": "2015-11-24T20:16:27Z",
    "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.util.SystemClock\n+import org.apache.spark.util.Clock\n+\n+/**\n+ * The interface to determine executor blacklist and node blacklist.\n+ */\n+private [scheduler] trait BlacklistStrategy {\n+  /** Defined a time interval to expire failure information of executors */\n+  val expireTimeInMilliseconds: Long\n+\n+  /** Return executors in blacklist which are related to given taskIndex */\n+  def getExecutorBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus],\n+      atomTask: BlacklistAtomTask, clock: Clock): Set[String]\n+\n+  /** Return all nodes in blacklist */\n+  def getNodeBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus]): Set[String]\n+\n+  /**\n+   * Default implementation to remove failure executors from HashMap based on given time period.\n+   * The return value identity whether or not it updated anything\n+   */\n+  def expireExecutorsInBlackList(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus], clock: Clock): Boolean = {\n+    val now = clock.getTimeMillis()\n+    val expiredKey = executorIdToFailureStatus.filter {\n+      case (executorid, failureStatus) => {\n+        (now - failureStatus.updatedTime) >= expireTimeInMilliseconds\n+      }\n+    }.keySet\n+\n+    if (expiredKey.isEmpty) {\n+      false\n+    } else {\n+      executorIdToFailureStatus --= expiredKey\n+      true\n+    }\n+  }\n+}\n+\n+/**\n+ * This strategy is simply based on given threshold and is taskId unrelated. An executor will be\n+ * in blacklist, if it failed more than \"maxFailureTaskNumber\" times. A node will be in blacklist,\n+ * if there are more than \"maxBlackExecutorNumber\" executors on it in executor blacklist.\n+ *\n+ * In this case, provided taskId will be ignored. The benefit for taskId unrelated strategy is that\n+ * different taskSets can learn experience from other taskSet to avoid allocating tasks on\n+ * problematic executors.\n+ */\n+private[scheduler] class SimpleStrategy(\n+    maxFailureTaskNumber: Int,\n+    maxBlackExecutorNumber: Int,\n+    val expireTimeInMilliseconds: Long\n+  )extends BlacklistStrategy {\n+\n+  private def getSelectedExecutorMap(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus]) = {\n+    executorIdToFailureStatus.filter{\n+      case (id, failureStatus) => failureStatus.totalNumFailures > maxFailureTaskNumber\n+    }\n+  }\n+\n+  // As this is a taskId unrelated strategy, the input taskId will be ignored\n+  def getExecutorBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus],\n+      atomTask: BlacklistAtomTask, clock: Clock): Set[String] = {\n+    getSelectedExecutorMap(executorIdToFailureStatus).keys.toSet\n+  }\n+\n+  def getNodeBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus]): Set[String] = {\n+    getSelectedExecutorMap(executorIdToFailureStatus)\n+      .groupBy{case (id, failureStatus) => failureStatus.host}\n+      .filter {case (host, executorIdToFailureStatus) =>\n+        executorIdToFailureStatus.size > maxBlackExecutorNumber}\n+      .keys.toSet\n+  }\n+}\n+\n+/**\n+ * This strategy is applied as default to keep the same semantics as original. It's an taskId\n+ * related strategy. If an executor failed running \"task A\", then we think this executor is\n+ * blacked for \"task A\". And we think the executor is still healthy for other task. node blacklist\n+ * is always empty.\n+ *\n+ * It was the standard behavior before spark 1.6\n+ */\n+private[scheduler] class DefaultStrategy(\n+    val expireTimeInMilliseconds: Long) extends BlacklistStrategy {\n+  def getExecutorBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus],\n+      atomTask: BlacklistAtomTask, clock: Clock): Set[String] = {\n+    executorIdToFailureStatus.filter{\n+      case (_, failureStatus) => failureStatus.numFailuresPerTask.keySet.contains(atomTask) &&\n+        clock.getTimeMillis() - failureStatus.updatedTime < expireTimeInMilliseconds\n+    }.keys.toSet\n+  }\n+\n+  def getNodeBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus]): Set[String] =\n+        Set.empty[String]\n+}\n+\n+/**\n+ * Create BlacklistStrategy instance according to SparkConf\n+ */\n+private[scheduler] object BlacklistStrategy {\n+  def apply(sparkConf: SparkConf): BlacklistStrategy = {\n+    val timeout = sparkConf.getTimeAsMs(\"spark.scheduler.blacklist.timeout\",\n+        sparkConf.getLong(\"spark.scheduler.executorTaskBlacklistTime\", 0L).toString() + \"ms\")\n+    sparkConf.get(\"spark.scheduler.blacklist.strategy\", \"default\") match {\n+      case \"default\" =>\n+        new DefaultStrategy(timeout)\n+      case \"threshold\" =>\n+        new SimpleStrategy("
  }],
  "prId": 8760
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "can you change these params to `maxFailedTasks` and `maxBlacklistedExecutors`, and change the comment to:\n\nThis strategy adds an executor to the blacklist for _all_ tasks when the executor has too many task failures.  An executor is placed in the blacklist when there are more than [[maxFailedTasks]] failed tasks.  Furthermore, all executors in one node are put into the blacklist if there are more than [[maxBlacklistedExecutors]] blacklisted executors on one node.  The benefit of this strategy is that different taskSets can learn experience from other taskSet to avoid allocating tasks on problematic executors.\n",
    "commit": "ecad5ffe2bcc02f9b181a209172f22294e6af4ef",
    "createdAt": "2015-11-24T20:40:05Z",
    "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.util.SystemClock\n+import org.apache.spark.util.Clock\n+\n+/**\n+ * The interface to determine executor blacklist and node blacklist.\n+ */\n+private [scheduler] trait BlacklistStrategy {\n+  /** Defined a time interval to expire failure information of executors */\n+  val expireTimeInMilliseconds: Long\n+\n+  /** Return executors in blacklist which are related to given taskIndex */\n+  def getExecutorBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus],\n+      atomTask: BlacklistAtomTask, clock: Clock): Set[String]\n+\n+  /** Return all nodes in blacklist */\n+  def getNodeBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus]): Set[String]\n+\n+  /**\n+   * Default implementation to remove failure executors from HashMap based on given time period.\n+   * The return value identity whether or not it updated anything\n+   */\n+  def expireExecutorsInBlackList(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus], clock: Clock): Boolean = {\n+    val now = clock.getTimeMillis()\n+    val expiredKey = executorIdToFailureStatus.filter {\n+      case (executorid, failureStatus) => {\n+        (now - failureStatus.updatedTime) >= expireTimeInMilliseconds\n+      }\n+    }.keySet\n+\n+    if (expiredKey.isEmpty) {\n+      false\n+    } else {\n+      executorIdToFailureStatus --= expiredKey\n+      true\n+    }\n+  }\n+}\n+\n+/**\n+ * This strategy is simply based on given threshold and is taskId unrelated. An executor will be\n+ * in blacklist, if it failed more than \"maxFailureTaskNumber\" times. A node will be in blacklist,\n+ * if there are more than \"maxBlackExecutorNumber\" executors on it in executor blacklist.\n+ *\n+ * In this case, provided taskId will be ignored. The benefit for taskId unrelated strategy is that\n+ * different taskSets can learn experience from other taskSet to avoid allocating tasks on\n+ * problematic executors.\n+ */\n+private[scheduler] class SimpleStrategy(\n+    maxFailureTaskNumber: Int,\n+    maxBlackExecutorNumber: Int,"
  }, {
    "author": {
      "login": "squito"
    },
    "body": "can you change these params to `maxFailedTasks` and `maxBlacklistedExecutors`, and change the comment to:\n\nThis strategy adds an executor to the blacklist for _all_ tasks when the executor has too many task failures.  An executor is placed in the blacklist when there are more than [[maxFailedTasks]] failed tasks.  Furthermore, all executors in one node are put into the blacklist if there are more than [[maxBlacklistedExecutors]] blacklisted executors on one node.  The benefit of this strategy is that different taskSets can learn experience from other taskSet to avoid allocating tasks on problematic executors.\n",
    "commit": "ecad5ffe2bcc02f9b181a209172f22294e6af4ef",
    "createdAt": "2015-11-24T20:41:38Z",
    "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.util.SystemClock\n+import org.apache.spark.util.Clock\n+\n+/**\n+ * The interface to determine executor blacklist and node blacklist.\n+ */\n+private [scheduler] trait BlacklistStrategy {\n+  /** Defined a time interval to expire failure information of executors */\n+  val expireTimeInMilliseconds: Long\n+\n+  /** Return executors in blacklist which are related to given taskIndex */\n+  def getExecutorBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus],\n+      atomTask: BlacklistAtomTask, clock: Clock): Set[String]\n+\n+  /** Return all nodes in blacklist */\n+  def getNodeBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus]): Set[String]\n+\n+  /**\n+   * Default implementation to remove failure executors from HashMap based on given time period.\n+   * The return value identity whether or not it updated anything\n+   */\n+  def expireExecutorsInBlackList(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus], clock: Clock): Boolean = {\n+    val now = clock.getTimeMillis()\n+    val expiredKey = executorIdToFailureStatus.filter {\n+      case (executorid, failureStatus) => {\n+        (now - failureStatus.updatedTime) >= expireTimeInMilliseconds\n+      }\n+    }.keySet\n+\n+    if (expiredKey.isEmpty) {\n+      false\n+    } else {\n+      executorIdToFailureStatus --= expiredKey\n+      true\n+    }\n+  }\n+}\n+\n+/**\n+ * This strategy is simply based on given threshold and is taskId unrelated. An executor will be\n+ * in blacklist, if it failed more than \"maxFailureTaskNumber\" times. A node will be in blacklist,\n+ * if there are more than \"maxBlackExecutorNumber\" executors on it in executor blacklist.\n+ *\n+ * In this case, provided taskId will be ignored. The benefit for taskId unrelated strategy is that\n+ * different taskSets can learn experience from other taskSet to avoid allocating tasks on\n+ * problematic executors.\n+ */\n+private[scheduler] class SimpleStrategy(\n+    maxFailureTaskNumber: Int,\n+    maxBlackExecutorNumber: Int,"
  }],
  "prId": 8760
}, {
  "comments": [{
    "author": {
      "login": "mridulm"
    },
    "body": "This needs to be renamed to something more appropriate - filterFailedExecutors or some such - so that it is clear what it does. Selected Executor does not really convey what it is supposed to do.\n",
    "commit": "ecad5ffe2bcc02f9b181a209172f22294e6af4ef",
    "createdAt": "2015-12-01T07:32:46Z",
    "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.util.SystemClock\n+import org.apache.spark.util.Clock\n+\n+/**\n+ * The interface to determine executor blacklist and node blacklist.\n+ */\n+private [scheduler] trait BlacklistStrategy {\n+  /** Define a time interval to expire failure information of executors */\n+  val expireTimeInMilliseconds: Long\n+\n+  /** Return executors in blacklist which are related to given stage and partition */\n+  def getExecutorBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus],\n+      atomTask: StageAndPartition,\n+      clock: Clock): Set[String]\n+\n+  /** Return all nodes in blacklist */\n+  def getNodeBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus]): Set[String]\n+\n+  /**\n+   * Choose which executors should be removed from blacklist. Return true if any executors are\n+   * removed from the blacklist, false otherwise. The default implementation removes executors from\n+   * the blacklist after [[expireTimeInMilliseconds]]\n+   */\n+  def expireExecutorsInBlackList(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus],\n+      clock: Clock): Boolean = {\n+    val now = clock.getTimeMillis()\n+    val expiredKey = executorIdToFailureStatus.filter {\n+      case (executorid, failureStatus) => {\n+        (now - failureStatus.updatedTime) >= expireTimeInMilliseconds\n+      }\n+    }.keySet\n+\n+    if (expiredKey.isEmpty) {\n+      false\n+    } else {\n+      executorIdToFailureStatus --= expiredKey\n+      true\n+    }\n+  }\n+}\n+\n+/**\n+ * This strategy adds an executor to the blacklist for all tasks when the executor has too many\n+ * task failures. An executor is placed in the blacklist when there are more than\n+ * [[maxFailedTasks]] failed tasks. Furthermore, all executors in one node are put into the\n+ * blacklist if there are more than [[maxBlacklistedExecutors]] blacklisted executors on one node.\n+ * The benefit of this strategy is that different taskSets can learn experience from other taskSet\n+ * to avoid allocating tasks on problematic executors.\n+ */\n+private[scheduler] class ExecutorAndNodeStrategy(\n+    maxFailedTasks: Int,\n+    maxBlacklistedExecutors: Int,\n+    val expireTimeInMilliseconds: Long\n+  ) extends BlacklistStrategy {\n+\n+  private def getSelectedExecutorMap("
  }],
  "prId": 8760
}, {
  "comments": [{
    "author": {
      "login": "chenghao-intel"
    },
    "body": "As we talked offline, I am wondering if we can simplify the configuration by just enabling the experimental feature or not? \n",
    "commit": "ecad5ffe2bcc02f9b181a209172f22294e6af4ef",
    "createdAt": "2016-02-17T13:51:23Z",
    "diffHunk": "@@ -0,0 +1,182 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.util.Clock\n+\n+/**\n+ * The interface to determine executor blacklist and node blacklist.\n+ */\n+private [scheduler] trait BlacklistStrategy {\n+  /** Define a time interval to expire failure information of executors */\n+  val expireTimeInMilliseconds: Long\n+\n+  /** Return executors in blacklist which are related to given stage and partition */\n+  def getExecutorBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus],\n+      atomTask: StageAndPartition,\n+      clock: Clock): Set[String]\n+\n+  /** Return all nodes in blacklist */\n+  def getNodeBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus],\n+      clock: Clock): Set[String]\n+\n+  /** Return all nodes in blacklist for specified stage. By default it returns the same result as\n+   *  getNodeBlacklist. It could be override in strategy implementation.\n+   */\n+  def getNodeBlacklistForStage(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus],\n+      stageId: Int,\n+      clock: Clock): Set[String] = getNodeBlacklist(executorIdToFailureStatus, clock)\n+\n+  /**\n+   * Choose which executors should be removed from blacklist. Return true if any executors are\n+   * removed from the blacklist, false otherwise. The default implementation removes executors from\n+   * the blacklist after [[expireTimeInMilliseconds]]\n+   */\n+  def expireExecutorsInBlackList(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus],\n+      clock: Clock): Boolean = {\n+    val now = clock.getTimeMillis()\n+    val expiredKey = executorIdToFailureStatus.filter {\n+      case (executorid, failureStatus) => {\n+        (now - failureStatus.updatedTime) >= expireTimeInMilliseconds\n+      }\n+    }.keySet\n+\n+    if (expiredKey.isEmpty) {\n+      false\n+    } else {\n+      executorIdToFailureStatus --= expiredKey\n+      true\n+    }\n+  }\n+}\n+\n+/**\n+ * This strategy adds an executor to the blacklist for all tasks when the executor has too many\n+ * task failures. An executor is placed in the blacklist when there are more than\n+ * [[maxFailedTasks]] failed tasks. Furthermore, all executors in one node are put into the\n+ * blacklist if there are more than [[maxBlacklistedExecutors]] blacklisted executors on one node.\n+ * The benefit of this strategy is that different taskSets can learn experience from other taskSet\n+ * to avoid allocating tasks on problematic executors.\n+ */\n+private[scheduler] class ExecutorAndNodeStrategy(\n+    maxFailedTasks: Int,\n+    maxBlacklistedExecutors: Int,\n+    val expireTimeInMilliseconds: Long\n+  ) extends BlacklistStrategy {\n+\n+  private def getExecutorBlacklistInfo(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus]) = {\n+    executorIdToFailureStatus.filter{\n+      case (id, failureStatus) => failureStatus.totalNumFailures > maxFailedTasks\n+    }\n+  }\n+\n+  // As this is a task unrelated strategy, the input StageAndPartition info will be ignored\n+  def getExecutorBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus],\n+      atomTask: StageAndPartition,\n+      clock: Clock): Set[String] = {\n+    getExecutorBlacklistInfo(executorIdToFailureStatus).keys.toSet\n+  }\n+\n+  def getNodeBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus],\n+      clock: Clock): Set[String] = {\n+    getExecutorBlacklistInfo(executorIdToFailureStatus)\n+      .groupBy{case (id, failureStatus) => failureStatus.host}\n+      .filter {case (host, executorIdToFailureStatus) =>\n+        executorIdToFailureStatus.size > maxBlacklistedExecutors}\n+      .keys.toSet\n+  }\n+}\n+\n+/**\n+ * This strategy is applied as default to keep the same semantics as original. It's an task\n+ * related strategy. If an executor failed running \"task A\", then we think this executor is\n+ * blacked for \"task A\". And we think the executor is still healthy for other task. node blacklist\n+ * is always empty.\n+ *\n+ * It was the standard behavior before spark 1.6\n+ */\n+private[scheduler] class SingleTaskStrategy(\n+    val expireTimeInMilliseconds: Long) extends BlacklistStrategy {\n+  def getExecutorBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus],\n+      atomTask: StageAndPartition,\n+      clock: Clock): Set[String] = {\n+    executorIdToFailureStatus.filter{\n+      case (_, failureStatus) => failureStatus.numFailuresPerTask.keySet.contains(atomTask) &&\n+        clock.getTimeMillis() - failureStatus.updatedTime < expireTimeInMilliseconds\n+    }.keys.toSet\n+  }\n+\n+  def getNodeBlacklist(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus],\n+      clock: Clock): Set[String] = Set.empty[String]\n+}\n+\n+/**\n+ * Support getNodeBlacklistForStage. With this strategy, once executor failed running a task, we\n+ * put all executors on the same node into blacklist, so all tasks on the same stage will not be\n+ * allocated to that node.\n+ */\n+private[scheduler] class AdvancedSingleTaskStrategy(\n+    expireTimeInMilliseconds: Long) extends SingleTaskStrategy(expireTimeInMilliseconds) {\n+\n+  override def getNodeBlacklistForStage(\n+      executorIdToFailureStatus: mutable.HashMap[String, FailureStatus],\n+      stageId: Int,\n+      clock: Clock): Set[String] = {\n+    executorIdToFailureStatus.filter{\n+      case (_, failureStatus) =>\n+        failureStatus.numFailuresPerTask.keySet.map(_.stageId).contains(stageId) &&\n+        clock.getTimeMillis() - failureStatus.updatedTime < expireTimeInMilliseconds\n+    }.values.map(_.host).toSet\n+  }\n+}\n+\n+/**\n+ * Create BlacklistStrategy instance according to SparkConf\n+ */\n+private[scheduler] object BlacklistStrategy {\n+  def apply(sparkConf: SparkConf): BlacklistStrategy = {\n+    val timeout = sparkConf.getTimeAsMs(\"spark.scheduler.blacklist.timeout\",\n+        sparkConf.getLong(\"spark.scheduler.executorTaskBlacklistTime\", 0L).toString() + \"ms\")\n+    sparkConf.get(\"spark.scheduler.blacklist.strategy\", \"singleTask\") match {"
  }],
  "prId": 8760
}]