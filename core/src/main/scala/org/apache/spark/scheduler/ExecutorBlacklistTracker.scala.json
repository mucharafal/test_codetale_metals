[{
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "~~I think we should also generate a SparkListener event when an executor gets blacklisted.  I imagine that any tool that is monitoring cluster / app health would want to know about it~~\n\non second thought, maybe better to leave for a future pr ...\n",
    "commit": "fb3821f58de7ec95556de5632cb3686177303f34",
    "createdAt": "2015-07-01T21:02:53Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.util.concurrent.TimeUnit\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark._\n+import org.apache.spark.util.{Clock, SystemClock, ThreadUtils, Utils}\n+\n+/**\n+ * ExecutorBlacklistTracker blacklists the executors by tracking the status of running tasks with\n+ * heuristic algorithm.\n+ *\n+ * A executor will be considered bad enough only when:\n+ * 1. The failure task number on this executor is more than\n+ *    spark.scheduler.blacklist.executorFaultThreshold.\n+ * 2. The failure task number on this executor is\n+ *    spark.scheduler.blacklist.averageBlacklistThreshold more than average failure task number\n+ *    of this cluster.\n+ *\n+ * Also max number of blacklisted executors will not exceed the\n+ * spark.scheduler.blacklist.maxBlacklistFraction of whole cluster, and blacklisted executors\n+ * will be forgiven when there is no failure tasks in the\n+ * spark.scheduler.blacklist.executorFaultTimeoutWindowInMinutes.\n+ */\n+private[spark] class ExecutorBlacklistTracker(conf: SparkConf) extends SparkListener {\n+  import ExecutorBlacklistTracker._\n+\n+  private val maxBlacklistFraction = conf.getDouble(\n+    \"spark.scheduler.blacklist.maxBlacklistFraction\", MAX_BLACKLIST_FRACTION)\n+  private val avgBlacklistThreshold = conf.getDouble(\n+    \"spark.scheduler.blacklist.averageBlacklistThreshold\", AVERAGE_BLACKLIST_THRESHOLD)\n+  private val executorFaultThreshold = conf.getInt(\n+    \"spark.scheduler.blacklist.executorFaultThreshold\", EXECUTOR_FAULT_THRESHOLD)\n+  private val executorFaultTimeoutWindowInMinutes = conf.getInt(\n+    \"spark.scheduler.blacklist.executorFaultTimeoutWindowInMinutes\", EXECUTOR_FAULT_TIMEOUT_WINDOW)\n+\n+  // Count the number of executors registered\n+  var numExecutorsRegistered: Int = 0\n+\n+  // Track the number of failure tasks and time of latest failure to executor id\n+  val executorIdToTaskFailures = new mutable.HashMap[String, ExecutorFailureStatus]()\n+\n+  // Clock used to update and exclude the executors which are out of time window.\n+  private var clock: Clock = new SystemClock()\n+\n+  // Executor that handles the scheduling task\n+  private val executor = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+    \"spark-scheduler-blacklist-expire-timer\")\n+\n+  def start(): Unit = {\n+    val scheduleTask = new Runnable() {\n+      override def run(): Unit = {\n+        Utils.logUncaughtExceptions(expireTimeoutExecutorBlacklist())\n+      }\n+    }\n+    executor.scheduleAtFixedRate(scheduleTask, 0L, 60, TimeUnit.SECONDS)\n+  }\n+\n+  def stop(): Unit = {\n+    executor.shutdown()\n+    executor.awaitTermination(10, TimeUnit.SECONDS)\n+  }\n+\n+  def setClock(newClock: Clock): Unit = {\n+    clock = newClock\n+  }\n+\n+  def getExecutorBlacklist: Set[String] = synchronized {\n+    executorIdToTaskFailures.filter(_._2.isBlackListed).keys.toSet\n+  }\n+\n+  override def onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit = synchronized {\n+    taskEnd.reason match {\n+      case _: FetchFailed | _: ExceptionFailure | TaskResultLost |\n+          _: ExecutorLostFailure | UnknownReason =>\n+        val failureStatus = executorIdToTaskFailures.getOrElseUpdate(taskEnd.taskInfo.executorId,\n+          new ExecutorFailureStatus)\n+        failureStatus.numFailures += 1\n+        failureStatus.updatedTime = clock.getTimeMillis()\n+\n+        // Update the executor blacklist\n+        updateExecutorBlacklist()\n+      case _ => Unit\n+    }\n+  }\n+\n+  override def onExecutorAdded(executorAdded: SparkListenerExecutorAdded): Unit = {\n+    numExecutorsRegistered += 1\n+  }\n+\n+  override def onExecutorRemoved(executorRemoved: SparkListenerExecutorRemoved\n+      ): Unit = synchronized {\n+    numExecutorsRegistered -= 1\n+    executorIdToTaskFailures -= executorRemoved.executorId\n+  }\n+\n+  private def updateExecutorBlacklist(): Unit = {\n+    // Filter out the executor Ids where task failure number is larger than\n+    // executorFaultThreshold and not blacklisted\n+    val failedExecutors = executorIdToTaskFailures.filter { case(_, e) =>\n+      e.numFailures >= executorFaultThreshold && !e.isBlackListed\n+    }\n+\n+    val blacklistedExecutorNum = executorIdToTaskFailures.filter(_._2.isBlackListed).size\n+\n+    if (failedExecutors.nonEmpty) {\n+      val avgNumFailed = executorIdToTaskFailures.values.map(_.numFailures).sum.toDouble /\n+        numExecutorsRegistered\n+      for ((executorId, failureStatus) <- failedExecutors) {\n+        // If the number of failure task is more than average blacklist threshold of average\n+        // failed number and current executor blacklist is less than the max fraction of number\n+        // executors\n+        if ((failureStatus.numFailures.toDouble > avgNumFailed * (1 + avgBlacklistThreshold)) &&\n+          (blacklistedExecutorNum.toDouble < numExecutorsRegistered * maxBlacklistFraction)) {\n+          failureStatus.isBlackListed = true",
    "line": 133
  }],
  "prId": 6870
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "if you don't reset `numFailures = 0`, on the next task end event, won't you immediately put this executor back into the blacklist?\n",
    "commit": "fb3821f58de7ec95556de5632cb3686177303f34",
    "createdAt": "2015-07-01T21:53:10Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.util.concurrent.TimeUnit\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark._\n+import org.apache.spark.util.{Clock, SystemClock, ThreadUtils, Utils}\n+\n+/**\n+ * ExecutorBlacklistTracker blacklists the executors by tracking the status of running tasks with\n+ * heuristic algorithm.\n+ *\n+ * A executor will be considered bad enough only when:\n+ * 1. The failure task number on this executor is more than\n+ *    spark.scheduler.blacklist.executorFaultThreshold.\n+ * 2. The failure task number on this executor is\n+ *    spark.scheduler.blacklist.averageBlacklistThreshold more than average failure task number\n+ *    of this cluster.\n+ *\n+ * Also max number of blacklisted executors will not exceed the\n+ * spark.scheduler.blacklist.maxBlacklistFraction of whole cluster, and blacklisted executors\n+ * will be forgiven when there is no failure tasks in the\n+ * spark.scheduler.blacklist.executorFaultTimeoutWindowInMinutes.\n+ */\n+private[spark] class ExecutorBlacklistTracker(conf: SparkConf) extends SparkListener {\n+  import ExecutorBlacklistTracker._\n+\n+  private val maxBlacklistFraction = conf.getDouble(\n+    \"spark.scheduler.blacklist.maxBlacklistFraction\", MAX_BLACKLIST_FRACTION)\n+  private val avgBlacklistThreshold = conf.getDouble(\n+    \"spark.scheduler.blacklist.averageBlacklistThreshold\", AVERAGE_BLACKLIST_THRESHOLD)\n+  private val executorFaultThreshold = conf.getInt(\n+    \"spark.scheduler.blacklist.executorFaultThreshold\", EXECUTOR_FAULT_THRESHOLD)\n+  private val executorFaultTimeoutWindowInMinutes = conf.getInt(\n+    \"spark.scheduler.blacklist.executorFaultTimeoutWindowInMinutes\", EXECUTOR_FAULT_TIMEOUT_WINDOW)\n+\n+  // Count the number of executors registered\n+  var numExecutorsRegistered: Int = 0\n+\n+  // Track the number of failure tasks and time of latest failure to executor id\n+  val executorIdToTaskFailures = new mutable.HashMap[String, ExecutorFailureStatus]()\n+\n+  // Clock used to update and exclude the executors which are out of time window.\n+  private var clock: Clock = new SystemClock()\n+\n+  // Executor that handles the scheduling task\n+  private val executor = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+    \"spark-scheduler-blacklist-expire-timer\")\n+\n+  def start(): Unit = {\n+    val scheduleTask = new Runnable() {\n+      override def run(): Unit = {\n+        Utils.logUncaughtExceptions(expireTimeoutExecutorBlacklist())\n+      }\n+    }\n+    executor.scheduleAtFixedRate(scheduleTask, 0L, 60, TimeUnit.SECONDS)\n+  }\n+\n+  def stop(): Unit = {\n+    executor.shutdown()\n+    executor.awaitTermination(10, TimeUnit.SECONDS)\n+  }\n+\n+  def setClock(newClock: Clock): Unit = {\n+    clock = newClock\n+  }\n+\n+  def getExecutorBlacklist: Set[String] = synchronized {\n+    executorIdToTaskFailures.filter(_._2.isBlackListed).keys.toSet\n+  }\n+\n+  override def onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit = synchronized {\n+    taskEnd.reason match {\n+      case _: FetchFailed | _: ExceptionFailure | TaskResultLost |\n+          _: ExecutorLostFailure | UnknownReason =>\n+        val failureStatus = executorIdToTaskFailures.getOrElseUpdate(taskEnd.taskInfo.executorId,\n+          new ExecutorFailureStatus)\n+        failureStatus.numFailures += 1\n+        failureStatus.updatedTime = clock.getTimeMillis()\n+\n+        // Update the executor blacklist\n+        updateExecutorBlacklist()\n+      case _ => Unit\n+    }\n+  }\n+\n+  override def onExecutorAdded(executorAdded: SparkListenerExecutorAdded): Unit = {\n+    numExecutorsRegistered += 1\n+  }\n+\n+  override def onExecutorRemoved(executorRemoved: SparkListenerExecutorRemoved\n+      ): Unit = synchronized {\n+    numExecutorsRegistered -= 1\n+    executorIdToTaskFailures -= executorRemoved.executorId\n+  }\n+\n+  private def updateExecutorBlacklist(): Unit = {\n+    // Filter out the executor Ids where task failure number is larger than\n+    // executorFaultThreshold and not blacklisted\n+    val failedExecutors = executorIdToTaskFailures.filter { case(_, e) =>\n+      e.numFailures >= executorFaultThreshold && !e.isBlackListed\n+    }\n+\n+    val blacklistedExecutorNum = executorIdToTaskFailures.filter(_._2.isBlackListed).size\n+\n+    if (failedExecutors.nonEmpty) {\n+      val avgNumFailed = executorIdToTaskFailures.values.map(_.numFailures).sum.toDouble /\n+        numExecutorsRegistered\n+      for ((executorId, failureStatus) <- failedExecutors) {\n+        // If the number of failure task is more than average blacklist threshold of average\n+        // failed number and current executor blacklist is less than the max fraction of number\n+        // executors\n+        if ((failureStatus.numFailures.toDouble > avgNumFailed * (1 + avgBlacklistThreshold)) &&\n+          (blacklistedExecutorNum.toDouble < numExecutorsRegistered * maxBlacklistFraction)) {\n+          failureStatus.isBlackListed = true\n+        }\n+      }\n+    }\n+  }\n+\n+  private def expireTimeoutExecutorBlacklist(): Unit = synchronized {\n+    val now = clock.getTimeMillis()\n+\n+    executorIdToTaskFailures.foreach { case (id, failureStatus) =>\n+      if ((now - failureStatus.updatedTime) > executorFaultTimeoutWindowInMinutes * 60 * 1000\n+        && failureStatus.isBlackListed) {\n+        failureStatus.isBlackListed = false",
    "line": 145
  }, {
    "author": {
      "login": "jerryshao"
    },
    "body": "Yes, that's what I thought about, I'm not sure which way is a better, I checked the MR code, where they don't clean the `numFailures` as I know. So maybe this should be carefully rethink about, thanks for your pointing out.\n",
    "commit": "fb3821f58de7ec95556de5632cb3686177303f34",
    "createdAt": "2015-07-02T01:18:07Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.util.concurrent.TimeUnit\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark._\n+import org.apache.spark.util.{Clock, SystemClock, ThreadUtils, Utils}\n+\n+/**\n+ * ExecutorBlacklistTracker blacklists the executors by tracking the status of running tasks with\n+ * heuristic algorithm.\n+ *\n+ * A executor will be considered bad enough only when:\n+ * 1. The failure task number on this executor is more than\n+ *    spark.scheduler.blacklist.executorFaultThreshold.\n+ * 2. The failure task number on this executor is\n+ *    spark.scheduler.blacklist.averageBlacklistThreshold more than average failure task number\n+ *    of this cluster.\n+ *\n+ * Also max number of blacklisted executors will not exceed the\n+ * spark.scheduler.blacklist.maxBlacklistFraction of whole cluster, and blacklisted executors\n+ * will be forgiven when there is no failure tasks in the\n+ * spark.scheduler.blacklist.executorFaultTimeoutWindowInMinutes.\n+ */\n+private[spark] class ExecutorBlacklistTracker(conf: SparkConf) extends SparkListener {\n+  import ExecutorBlacklistTracker._\n+\n+  private val maxBlacklistFraction = conf.getDouble(\n+    \"spark.scheduler.blacklist.maxBlacklistFraction\", MAX_BLACKLIST_FRACTION)\n+  private val avgBlacklistThreshold = conf.getDouble(\n+    \"spark.scheduler.blacklist.averageBlacklistThreshold\", AVERAGE_BLACKLIST_THRESHOLD)\n+  private val executorFaultThreshold = conf.getInt(\n+    \"spark.scheduler.blacklist.executorFaultThreshold\", EXECUTOR_FAULT_THRESHOLD)\n+  private val executorFaultTimeoutWindowInMinutes = conf.getInt(\n+    \"spark.scheduler.blacklist.executorFaultTimeoutWindowInMinutes\", EXECUTOR_FAULT_TIMEOUT_WINDOW)\n+\n+  // Count the number of executors registered\n+  var numExecutorsRegistered: Int = 0\n+\n+  // Track the number of failure tasks and time of latest failure to executor id\n+  val executorIdToTaskFailures = new mutable.HashMap[String, ExecutorFailureStatus]()\n+\n+  // Clock used to update and exclude the executors which are out of time window.\n+  private var clock: Clock = new SystemClock()\n+\n+  // Executor that handles the scheduling task\n+  private val executor = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+    \"spark-scheduler-blacklist-expire-timer\")\n+\n+  def start(): Unit = {\n+    val scheduleTask = new Runnable() {\n+      override def run(): Unit = {\n+        Utils.logUncaughtExceptions(expireTimeoutExecutorBlacklist())\n+      }\n+    }\n+    executor.scheduleAtFixedRate(scheduleTask, 0L, 60, TimeUnit.SECONDS)\n+  }\n+\n+  def stop(): Unit = {\n+    executor.shutdown()\n+    executor.awaitTermination(10, TimeUnit.SECONDS)\n+  }\n+\n+  def setClock(newClock: Clock): Unit = {\n+    clock = newClock\n+  }\n+\n+  def getExecutorBlacklist: Set[String] = synchronized {\n+    executorIdToTaskFailures.filter(_._2.isBlackListed).keys.toSet\n+  }\n+\n+  override def onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit = synchronized {\n+    taskEnd.reason match {\n+      case _: FetchFailed | _: ExceptionFailure | TaskResultLost |\n+          _: ExecutorLostFailure | UnknownReason =>\n+        val failureStatus = executorIdToTaskFailures.getOrElseUpdate(taskEnd.taskInfo.executorId,\n+          new ExecutorFailureStatus)\n+        failureStatus.numFailures += 1\n+        failureStatus.updatedTime = clock.getTimeMillis()\n+\n+        // Update the executor blacklist\n+        updateExecutorBlacklist()\n+      case _ => Unit\n+    }\n+  }\n+\n+  override def onExecutorAdded(executorAdded: SparkListenerExecutorAdded): Unit = {\n+    numExecutorsRegistered += 1\n+  }\n+\n+  override def onExecutorRemoved(executorRemoved: SparkListenerExecutorRemoved\n+      ): Unit = synchronized {\n+    numExecutorsRegistered -= 1\n+    executorIdToTaskFailures -= executorRemoved.executorId\n+  }\n+\n+  private def updateExecutorBlacklist(): Unit = {\n+    // Filter out the executor Ids where task failure number is larger than\n+    // executorFaultThreshold and not blacklisted\n+    val failedExecutors = executorIdToTaskFailures.filter { case(_, e) =>\n+      e.numFailures >= executorFaultThreshold && !e.isBlackListed\n+    }\n+\n+    val blacklistedExecutorNum = executorIdToTaskFailures.filter(_._2.isBlackListed).size\n+\n+    if (failedExecutors.nonEmpty) {\n+      val avgNumFailed = executorIdToTaskFailures.values.map(_.numFailures).sum.toDouble /\n+        numExecutorsRegistered\n+      for ((executorId, failureStatus) <- failedExecutors) {\n+        // If the number of failure task is more than average blacklist threshold of average\n+        // failed number and current executor blacklist is less than the max fraction of number\n+        // executors\n+        if ((failureStatus.numFailures.toDouble > avgNumFailed * (1 + avgBlacklistThreshold)) &&\n+          (blacklistedExecutorNum.toDouble < numExecutorsRegistered * maxBlacklistFraction)) {\n+          failureStatus.isBlackListed = true\n+        }\n+      }\n+    }\n+  }\n+\n+  private def expireTimeoutExecutorBlacklist(): Unit = synchronized {\n+    val now = clock.getTimeMillis()\n+\n+    executorIdToTaskFailures.foreach { case (id, failureStatus) =>\n+      if ((now - failureStatus.updatedTime) > executorFaultTimeoutWindowInMinutes * 60 * 1000\n+        && failureStatus.isBlackListed) {\n+        failureStatus.isBlackListed = false",
    "line": 145
  }, {
    "author": {
      "login": "squito"
    },
    "body": "I realized its not any task end event -- its only on a task failure that you'll re-blacklist the executor.  \n\nBut it still doesn't make much sense to me.  I don't understand why MR has this behavior (I'm not familiar w/ the MR code).  In any case, we don't need to follow MR's model.  In particular, I'm guessing that with MR the blacklist is cleared at every job step, while with spark this blacklist is for one app, which will likely have many jobs and may last a very long time.\n",
    "commit": "fb3821f58de7ec95556de5632cb3686177303f34",
    "createdAt": "2015-07-02T15:55:56Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.util.concurrent.TimeUnit\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark._\n+import org.apache.spark.util.{Clock, SystemClock, ThreadUtils, Utils}\n+\n+/**\n+ * ExecutorBlacklistTracker blacklists the executors by tracking the status of running tasks with\n+ * heuristic algorithm.\n+ *\n+ * A executor will be considered bad enough only when:\n+ * 1. The failure task number on this executor is more than\n+ *    spark.scheduler.blacklist.executorFaultThreshold.\n+ * 2. The failure task number on this executor is\n+ *    spark.scheduler.blacklist.averageBlacklistThreshold more than average failure task number\n+ *    of this cluster.\n+ *\n+ * Also max number of blacklisted executors will not exceed the\n+ * spark.scheduler.blacklist.maxBlacklistFraction of whole cluster, and blacklisted executors\n+ * will be forgiven when there is no failure tasks in the\n+ * spark.scheduler.blacklist.executorFaultTimeoutWindowInMinutes.\n+ */\n+private[spark] class ExecutorBlacklistTracker(conf: SparkConf) extends SparkListener {\n+  import ExecutorBlacklistTracker._\n+\n+  private val maxBlacklistFraction = conf.getDouble(\n+    \"spark.scheduler.blacklist.maxBlacklistFraction\", MAX_BLACKLIST_FRACTION)\n+  private val avgBlacklistThreshold = conf.getDouble(\n+    \"spark.scheduler.blacklist.averageBlacklistThreshold\", AVERAGE_BLACKLIST_THRESHOLD)\n+  private val executorFaultThreshold = conf.getInt(\n+    \"spark.scheduler.blacklist.executorFaultThreshold\", EXECUTOR_FAULT_THRESHOLD)\n+  private val executorFaultTimeoutWindowInMinutes = conf.getInt(\n+    \"spark.scheduler.blacklist.executorFaultTimeoutWindowInMinutes\", EXECUTOR_FAULT_TIMEOUT_WINDOW)\n+\n+  // Count the number of executors registered\n+  var numExecutorsRegistered: Int = 0\n+\n+  // Track the number of failure tasks and time of latest failure to executor id\n+  val executorIdToTaskFailures = new mutable.HashMap[String, ExecutorFailureStatus]()\n+\n+  // Clock used to update and exclude the executors which are out of time window.\n+  private var clock: Clock = new SystemClock()\n+\n+  // Executor that handles the scheduling task\n+  private val executor = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+    \"spark-scheduler-blacklist-expire-timer\")\n+\n+  def start(): Unit = {\n+    val scheduleTask = new Runnable() {\n+      override def run(): Unit = {\n+        Utils.logUncaughtExceptions(expireTimeoutExecutorBlacklist())\n+      }\n+    }\n+    executor.scheduleAtFixedRate(scheduleTask, 0L, 60, TimeUnit.SECONDS)\n+  }\n+\n+  def stop(): Unit = {\n+    executor.shutdown()\n+    executor.awaitTermination(10, TimeUnit.SECONDS)\n+  }\n+\n+  def setClock(newClock: Clock): Unit = {\n+    clock = newClock\n+  }\n+\n+  def getExecutorBlacklist: Set[String] = synchronized {\n+    executorIdToTaskFailures.filter(_._2.isBlackListed).keys.toSet\n+  }\n+\n+  override def onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit = synchronized {\n+    taskEnd.reason match {\n+      case _: FetchFailed | _: ExceptionFailure | TaskResultLost |\n+          _: ExecutorLostFailure | UnknownReason =>\n+        val failureStatus = executorIdToTaskFailures.getOrElseUpdate(taskEnd.taskInfo.executorId,\n+          new ExecutorFailureStatus)\n+        failureStatus.numFailures += 1\n+        failureStatus.updatedTime = clock.getTimeMillis()\n+\n+        // Update the executor blacklist\n+        updateExecutorBlacklist()\n+      case _ => Unit\n+    }\n+  }\n+\n+  override def onExecutorAdded(executorAdded: SparkListenerExecutorAdded): Unit = {\n+    numExecutorsRegistered += 1\n+  }\n+\n+  override def onExecutorRemoved(executorRemoved: SparkListenerExecutorRemoved\n+      ): Unit = synchronized {\n+    numExecutorsRegistered -= 1\n+    executorIdToTaskFailures -= executorRemoved.executorId\n+  }\n+\n+  private def updateExecutorBlacklist(): Unit = {\n+    // Filter out the executor Ids where task failure number is larger than\n+    // executorFaultThreshold and not blacklisted\n+    val failedExecutors = executorIdToTaskFailures.filter { case(_, e) =>\n+      e.numFailures >= executorFaultThreshold && !e.isBlackListed\n+    }\n+\n+    val blacklistedExecutorNum = executorIdToTaskFailures.filter(_._2.isBlackListed).size\n+\n+    if (failedExecutors.nonEmpty) {\n+      val avgNumFailed = executorIdToTaskFailures.values.map(_.numFailures).sum.toDouble /\n+        numExecutorsRegistered\n+      for ((executorId, failureStatus) <- failedExecutors) {\n+        // If the number of failure task is more than average blacklist threshold of average\n+        // failed number and current executor blacklist is less than the max fraction of number\n+        // executors\n+        if ((failureStatus.numFailures.toDouble > avgNumFailed * (1 + avgBlacklistThreshold)) &&\n+          (blacklistedExecutorNum.toDouble < numExecutorsRegistered * maxBlacklistFraction)) {\n+          failureStatus.isBlackListed = true\n+        }\n+      }\n+    }\n+  }\n+\n+  private def expireTimeoutExecutorBlacklist(): Unit = synchronized {\n+    val now = clock.getTimeMillis()\n+\n+    executorIdToTaskFailures.foreach { case (id, failureStatus) =>\n+      if ((now - failureStatus.updatedTime) > executorFaultTimeoutWindowInMinutes * 60 * 1000\n+        && failureStatus.isBlackListed) {\n+        failureStatus.isBlackListed = false",
    "line": 145
  }],
  "prId": 6870
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "these new confs need to be documented (along with \"spark.scheduler.blacklist.enabled\")\n",
    "commit": "fb3821f58de7ec95556de5632cb3686177303f34",
    "createdAt": "2015-07-01T21:56:20Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.util.concurrent.TimeUnit\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark._\n+import org.apache.spark.util.{Clock, SystemClock, ThreadUtils, Utils}\n+\n+/**\n+ * ExecutorBlacklistTracker blacklists the executors by tracking the status of running tasks with\n+ * heuristic algorithm.\n+ *\n+ * A executor will be considered bad enough only when:\n+ * 1. The failure task number on this executor is more than\n+ *    spark.scheduler.blacklist.executorFaultThreshold.\n+ * 2. The failure task number on this executor is\n+ *    spark.scheduler.blacklist.averageBlacklistThreshold more than average failure task number\n+ *    of this cluster.\n+ *\n+ * Also max number of blacklisted executors will not exceed the\n+ * spark.scheduler.blacklist.maxBlacklistFraction of whole cluster, and blacklisted executors\n+ * will be forgiven when there is no failure tasks in the\n+ * spark.scheduler.blacklist.executorFaultTimeoutWindowInMinutes.\n+ */\n+private[spark] class ExecutorBlacklistTracker(conf: SparkConf) extends SparkListener {\n+  import ExecutorBlacklistTracker._\n+\n+  private val maxBlacklistFraction = conf.getDouble(\n+    \"spark.scheduler.blacklist.maxBlacklistFraction\", MAX_BLACKLIST_FRACTION)\n+  private val avgBlacklistThreshold = conf.getDouble(\n+    \"spark.scheduler.blacklist.averageBlacklistThreshold\", AVERAGE_BLACKLIST_THRESHOLD)\n+  private val executorFaultThreshold = conf.getInt(\n+    \"spark.scheduler.blacklist.executorFaultThreshold\", EXECUTOR_FAULT_THRESHOLD)\n+  private val executorFaultTimeoutWindowInMinutes = conf.getInt(\n+    \"spark.scheduler.blacklist.executorFaultTimeoutWindowInMinutes\", EXECUTOR_FAULT_TIMEOUT_WINDOW)",
    "line": 53
  }],
  "prId": 6870
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "this gets called a lot, and (hopefully) is updated only rarely.  It should probably be computed only when it changes, and then stored.  (Also I think the stored value could probably just be `@volatile`, you wouldn't need to synchronize ... but I'm not 100% sure ...)\n",
    "commit": "fb3821f58de7ec95556de5632cb3686177303f34",
    "createdAt": "2015-07-01T22:01:55Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.util.concurrent.TimeUnit\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark._\n+import org.apache.spark.util.{Clock, SystemClock, ThreadUtils, Utils}\n+\n+/**\n+ * ExecutorBlacklistTracker blacklists the executors by tracking the status of running tasks with\n+ * heuristic algorithm.\n+ *\n+ * A executor will be considered bad enough only when:\n+ * 1. The failure task number on this executor is more than\n+ *    spark.scheduler.blacklist.executorFaultThreshold.\n+ * 2. The failure task number on this executor is\n+ *    spark.scheduler.blacklist.averageBlacklistThreshold more than average failure task number\n+ *    of this cluster.\n+ *\n+ * Also max number of blacklisted executors will not exceed the\n+ * spark.scheduler.blacklist.maxBlacklistFraction of whole cluster, and blacklisted executors\n+ * will be forgiven when there is no failure tasks in the\n+ * spark.scheduler.blacklist.executorFaultTimeoutWindowInMinutes.\n+ */\n+private[spark] class ExecutorBlacklistTracker(conf: SparkConf) extends SparkListener {\n+  import ExecutorBlacklistTracker._\n+\n+  private val maxBlacklistFraction = conf.getDouble(\n+    \"spark.scheduler.blacklist.maxBlacklistFraction\", MAX_BLACKLIST_FRACTION)\n+  private val avgBlacklistThreshold = conf.getDouble(\n+    \"spark.scheduler.blacklist.averageBlacklistThreshold\", AVERAGE_BLACKLIST_THRESHOLD)\n+  private val executorFaultThreshold = conf.getInt(\n+    \"spark.scheduler.blacklist.executorFaultThreshold\", EXECUTOR_FAULT_THRESHOLD)\n+  private val executorFaultTimeoutWindowInMinutes = conf.getInt(\n+    \"spark.scheduler.blacklist.executorFaultTimeoutWindowInMinutes\", EXECUTOR_FAULT_TIMEOUT_WINDOW)\n+\n+  // Count the number of executors registered\n+  var numExecutorsRegistered: Int = 0\n+\n+  // Track the number of failure tasks and time of latest failure to executor id\n+  val executorIdToTaskFailures = new mutable.HashMap[String, ExecutorFailureStatus]()\n+\n+  // Clock used to update and exclude the executors which are out of time window.\n+  private var clock: Clock = new SystemClock()\n+\n+  // Executor that handles the scheduling task\n+  private val executor = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+    \"spark-scheduler-blacklist-expire-timer\")\n+\n+  def start(): Unit = {\n+    val scheduleTask = new Runnable() {\n+      override def run(): Unit = {\n+        Utils.logUncaughtExceptions(expireTimeoutExecutorBlacklist())\n+      }\n+    }\n+    executor.scheduleAtFixedRate(scheduleTask, 0L, 60, TimeUnit.SECONDS)\n+  }\n+\n+  def stop(): Unit = {\n+    executor.shutdown()\n+    executor.awaitTermination(10, TimeUnit.SECONDS)\n+  }\n+\n+  def setClock(newClock: Clock): Unit = {\n+    clock = newClock\n+  }\n+\n+  def getExecutorBlacklist: Set[String] = synchronized {\n+    executorIdToTaskFailures.filter(_._2.isBlackListed).keys.toSet",
    "line": 87
  }],
  "prId": 6870
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "I'm not totally sure if this style is OK or not ... but it does look weird to me.  I think probably best to just put the arg on its own line, as with normal multi-line function defs\n\n``` scala\noverride def onExecutorRemoved(\n    executorRemoved: SparkListenerExecutorRemoved): Unit = synchronized {\n```\n",
    "commit": "fb3821f58de7ec95556de5632cb3686177303f34",
    "createdAt": "2015-07-01T22:10:35Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.util.concurrent.TimeUnit\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark._\n+import org.apache.spark.util.{Clock, SystemClock, ThreadUtils, Utils}\n+\n+/**\n+ * ExecutorBlacklistTracker blacklists the executors by tracking the status of running tasks with\n+ * heuristic algorithm.\n+ *\n+ * A executor will be considered bad enough only when:\n+ * 1. The failure task number on this executor is more than\n+ *    spark.scheduler.blacklist.executorFaultThreshold.\n+ * 2. The failure task number on this executor is\n+ *    spark.scheduler.blacklist.averageBlacklistThreshold more than average failure task number\n+ *    of this cluster.\n+ *\n+ * Also max number of blacklisted executors will not exceed the\n+ * spark.scheduler.blacklist.maxBlacklistFraction of whole cluster, and blacklisted executors\n+ * will be forgiven when there is no failure tasks in the\n+ * spark.scheduler.blacklist.executorFaultTimeoutWindowInMinutes.\n+ */\n+private[spark] class ExecutorBlacklistTracker(conf: SparkConf) extends SparkListener {\n+  import ExecutorBlacklistTracker._\n+\n+  private val maxBlacklistFraction = conf.getDouble(\n+    \"spark.scheduler.blacklist.maxBlacklistFraction\", MAX_BLACKLIST_FRACTION)\n+  private val avgBlacklistThreshold = conf.getDouble(\n+    \"spark.scheduler.blacklist.averageBlacklistThreshold\", AVERAGE_BLACKLIST_THRESHOLD)\n+  private val executorFaultThreshold = conf.getInt(\n+    \"spark.scheduler.blacklist.executorFaultThreshold\", EXECUTOR_FAULT_THRESHOLD)\n+  private val executorFaultTimeoutWindowInMinutes = conf.getInt(\n+    \"spark.scheduler.blacklist.executorFaultTimeoutWindowInMinutes\", EXECUTOR_FAULT_TIMEOUT_WINDOW)\n+\n+  // Count the number of executors registered\n+  var numExecutorsRegistered: Int = 0\n+\n+  // Track the number of failure tasks and time of latest failure to executor id\n+  val executorIdToTaskFailures = new mutable.HashMap[String, ExecutorFailureStatus]()\n+\n+  // Clock used to update and exclude the executors which are out of time window.\n+  private var clock: Clock = new SystemClock()\n+\n+  // Executor that handles the scheduling task\n+  private val executor = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+    \"spark-scheduler-blacklist-expire-timer\")\n+\n+  def start(): Unit = {\n+    val scheduleTask = new Runnable() {\n+      override def run(): Unit = {\n+        Utils.logUncaughtExceptions(expireTimeoutExecutorBlacklist())\n+      }\n+    }\n+    executor.scheduleAtFixedRate(scheduleTask, 0L, 60, TimeUnit.SECONDS)\n+  }\n+\n+  def stop(): Unit = {\n+    executor.shutdown()\n+    executor.awaitTermination(10, TimeUnit.SECONDS)\n+  }\n+\n+  def setClock(newClock: Clock): Unit = {\n+    clock = newClock\n+  }\n+\n+  def getExecutorBlacklist: Set[String] = synchronized {\n+    executorIdToTaskFailures.filter(_._2.isBlackListed).keys.toSet\n+  }\n+\n+  override def onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit = synchronized {\n+    taskEnd.reason match {\n+      case _: FetchFailed | _: ExceptionFailure | TaskResultLost |\n+          _: ExecutorLostFailure | UnknownReason =>\n+        val failureStatus = executorIdToTaskFailures.getOrElseUpdate(taskEnd.taskInfo.executorId,\n+          new ExecutorFailureStatus)\n+        failureStatus.numFailures += 1\n+        failureStatus.updatedTime = clock.getTimeMillis()\n+\n+        // Update the executor blacklist\n+        updateExecutorBlacklist()\n+      case _ => Unit\n+    }\n+  }\n+\n+  override def onExecutorAdded(executorAdded: SparkListenerExecutorAdded): Unit = {\n+    numExecutorsRegistered += 1\n+  }\n+\n+  override def onExecutorRemoved(executorRemoved: SparkListenerExecutorRemoved\n+      ): Unit = synchronized {",
    "line": 110
  }],
  "prId": 6870
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "more random thoughts from me -- I have this nagging worry about building functionality on top of `SparkListener` since the listener bus is free to just drop events if it gets backlogged.  Even if this listener is fast, as we start adding more of these listeners, plus maybe a user-listener which is a little slow, and then lots of small tasks ... even if each listener individually is fine, you could drop events and things will start behaving erratically.\n\nBut maybe the right answer to that is to fix the listener bus to include back pressure or something in the future, and for now we keep using it to build components on top of.\n",
    "commit": "fb3821f58de7ec95556de5632cb3686177303f34",
    "createdAt": "2015-07-01T22:29:40Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.util.concurrent.TimeUnit\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark._\n+import org.apache.spark.util.{Clock, SystemClock, ThreadUtils, Utils}\n+\n+/**\n+ * ExecutorBlacklistTracker blacklists the executors by tracking the status of running tasks with\n+ * heuristic algorithm.\n+ *\n+ * A executor will be considered bad enough only when:\n+ * 1. The failure task number on this executor is more than\n+ *    spark.scheduler.blacklist.executorFaultThreshold.\n+ * 2. The failure task number on this executor is\n+ *    spark.scheduler.blacklist.averageBlacklistThreshold more than average failure task number\n+ *    of this cluster.\n+ *\n+ * Also max number of blacklisted executors will not exceed the\n+ * spark.scheduler.blacklist.maxBlacklistFraction of whole cluster, and blacklisted executors\n+ * will be forgiven when there is no failure tasks in the\n+ * spark.scheduler.blacklist.executorFaultTimeoutWindowInMinutes.\n+ */\n+private[spark] class ExecutorBlacklistTracker(conf: SparkConf) extends SparkListener {\n+  import ExecutorBlacklistTracker._\n+\n+  private val maxBlacklistFraction = conf.getDouble(\n+    \"spark.scheduler.blacklist.maxBlacklistFraction\", MAX_BLACKLIST_FRACTION)\n+  private val avgBlacklistThreshold = conf.getDouble(\n+    \"spark.scheduler.blacklist.averageBlacklistThreshold\", AVERAGE_BLACKLIST_THRESHOLD)\n+  private val executorFaultThreshold = conf.getInt(\n+    \"spark.scheduler.blacklist.executorFaultThreshold\", EXECUTOR_FAULT_THRESHOLD)\n+  private val executorFaultTimeoutWindowInMinutes = conf.getInt(\n+    \"spark.scheduler.blacklist.executorFaultTimeoutWindowInMinutes\", EXECUTOR_FAULT_TIMEOUT_WINDOW)\n+\n+  // Count the number of executors registered\n+  var numExecutorsRegistered: Int = 0\n+\n+  // Track the number of failure tasks and time of latest failure to executor id\n+  val executorIdToTaskFailures = new mutable.HashMap[String, ExecutorFailureStatus]()\n+\n+  // Clock used to update and exclude the executors which are out of time window.\n+  private var clock: Clock = new SystemClock()\n+\n+  // Executor that handles the scheduling task\n+  private val executor = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+    \"spark-scheduler-blacklist-expire-timer\")\n+\n+  def start(): Unit = {\n+    val scheduleTask = new Runnable() {\n+      override def run(): Unit = {\n+        Utils.logUncaughtExceptions(expireTimeoutExecutorBlacklist())\n+      }\n+    }\n+    executor.scheduleAtFixedRate(scheduleTask, 0L, 60, TimeUnit.SECONDS)\n+  }\n+\n+  def stop(): Unit = {\n+    executor.shutdown()\n+    executor.awaitTermination(10, TimeUnit.SECONDS)\n+  }\n+\n+  def setClock(newClock: Clock): Unit = {\n+    clock = newClock\n+  }\n+\n+  def getExecutorBlacklist: Set[String] = synchronized {\n+    executorIdToTaskFailures.filter(_._2.isBlackListed).keys.toSet\n+  }\n+\n+  override def onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit = synchronized {\n+    taskEnd.reason match {\n+      case _: FetchFailed | _: ExceptionFailure | TaskResultLost |",
    "line": 92
  }],
  "prId": 6870
}]