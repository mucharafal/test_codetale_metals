[{
  "comments": [{
    "author": {
      "login": "liancheng"
    },
    "body": "I don't think `sparkHome` works for the `uri != null` branch. Since executors inherit their `spark.home` property from the driver, but in case of Mesos, each executor has it's own Spark home in its own sandbox directory. That's why `basename` and the `cd %s*` trick is used here.\n",
    "commit": "2be86ae573edcc0134602c51e622cf7cc2e4421b",
    "createdAt": "2014-08-26T02:20:29Z",
    "diffHunk": "@@ -111,48 +113,39 @@ private[spark] class CoarseMesosSchedulerBackend(\n \n   def createCommand(offer: Offer, numCores: Int): CommandInfo = {\n     val environment = Environment.newBuilder()\n-    val extraClassPath = conf.getOption(\"spark.executor.extraClassPath\")\n-    extraClassPath.foreach { cp =>\n-      environment.addVariables(\n-        Environment.Variable.newBuilder().setName(\"SPARK_CLASSPATH\").setValue(cp).build())\n-    }\n+    val mesosCommand = CommandInfo.newBuilder()\n+      .setEnvironment(environment)\n+      \n+    val driverUrl = \"akka.tcp://spark@%s:%s/user/%s\".format(\n+      conf.get(\"spark.driver.host\"), conf.get(\"spark.driver.port\"),\n+      CoarseGrainedSchedulerBackend.ACTOR_NAME)\n+    val args = Seq(driverUrl, offer.getSlaveId.getValue, offer.getHostname, numCores.toString)\n     val extraJavaOpts = conf.getOption(\"spark.executor.extraJavaOptions\")\n+      .map(Utils.splitCommandString).getOrElse(Seq.empty)\n \n-    val libraryPathOption = \"spark.executor.extraLibraryPath\"\n-    val extraLibraryPath = conf.getOption(libraryPathOption).map(p => s\"-Djava.library.path=$p\")\n-    val extraOpts = Seq(extraJavaOpts, extraLibraryPath).flatten.mkString(\" \")\n+    // Start executors with a few necessary configs for registering with the scheduler\n+    val sparkJavaOpts = Utils.sparkJavaOpts(conf, SparkConf.isExecutorStartupConf)\n+    val javaOpts = sparkJavaOpts ++ extraJavaOpts\n \n-    sc.executorEnvs.foreach { case (key, value) =>\n-      environment.addVariables(Environment.Variable.newBuilder()\n-        .setName(key)\n-        .setValue(value)\n-        .build())\n+    val classPathEntries = conf.getOption(\"spark.executor.extraClassPath\").toSeq.flatMap { cp =>\n+      cp.split(java.io.File.pathSeparator)\n     }\n-    val command = CommandInfo.newBuilder()\n-      .setEnvironment(environment)\n-    val driverUrl = \"akka.tcp://spark@%s:%s/user/%s\".format(\n-      conf.get(\"spark.driver.host\"),\n-      conf.get(\"spark.driver.port\"),\n-      CoarseGrainedSchedulerBackend.ACTOR_NAME)\n+    val libraryPathEntries =\n+      conf.getOption(\"spark.executor.extraLibraryPath\").toSeq.flatMap { cp =>\n+        cp.split(java.io.File.pathSeparator)\n+      }\n+\n+    val command = Command(\n+      \"org.apache.spark.executor.CoarseGrainedExecutorBackend\", args, sc.executorEnvs,\n+      classPathEntries, libraryPathEntries, javaOpts)\n \n     val uri = conf.get(\"spark.executor.uri\", null)\n-    if (uri == null) {\n-      val runScript = new File(sparkHome, \"./bin/spark-class\").getCanonicalPath\n-      command.setValue(\n-        \"\\\"%s\\\" org.apache.spark.executor.CoarseGrainedExecutorBackend %s %s %s %s %d\".format(\n-          runScript, extraOpts, driverUrl, offer.getSlaveId.getValue, offer.getHostname, numCores))\n-    } else {\n-      // Grab everything to the first '.'. We'll use that and '*' to\n-      // glob the directory \"correctly\".\n-      val basename = uri.split('/').last.split('.').head\n-      command.setValue(\n-        (\"cd %s*; \" +\n-          \"./bin/spark-class org.apache.spark.executor.CoarseGrainedExecutorBackend %s %s %s %s %d\")\n-          .format(basename, extraOpts, driverUrl, offer.getSlaveId.getValue,\n-            offer.getHostname, numCores))\n-      command.addUris(CommandInfo.URI.newBuilder().setValue(uri))\n+    mesosCommand.setValue(CommandUtils.buildCommandSeq(command, sc.executorMemory,\n+      sparkHome).mkString(\"\\\"\", \"\\\" \\\"\", \"\\\"\"))",
    "line": 88
  }, {
    "author": {
      "login": "ScrapCodes"
    },
    "body": "I see, looks like I get the intention. I was wondering what are other changes about ?\n",
    "commit": "2be86ae573edcc0134602c51e622cf7cc2e4421b",
    "createdAt": "2014-08-26T05:58:50Z",
    "diffHunk": "@@ -111,48 +113,39 @@ private[spark] class CoarseMesosSchedulerBackend(\n \n   def createCommand(offer: Offer, numCores: Int): CommandInfo = {\n     val environment = Environment.newBuilder()\n-    val extraClassPath = conf.getOption(\"spark.executor.extraClassPath\")\n-    extraClassPath.foreach { cp =>\n-      environment.addVariables(\n-        Environment.Variable.newBuilder().setName(\"SPARK_CLASSPATH\").setValue(cp).build())\n-    }\n+    val mesosCommand = CommandInfo.newBuilder()\n+      .setEnvironment(environment)\n+      \n+    val driverUrl = \"akka.tcp://spark@%s:%s/user/%s\".format(\n+      conf.get(\"spark.driver.host\"), conf.get(\"spark.driver.port\"),\n+      CoarseGrainedSchedulerBackend.ACTOR_NAME)\n+    val args = Seq(driverUrl, offer.getSlaveId.getValue, offer.getHostname, numCores.toString)\n     val extraJavaOpts = conf.getOption(\"spark.executor.extraJavaOptions\")\n+      .map(Utils.splitCommandString).getOrElse(Seq.empty)\n \n-    val libraryPathOption = \"spark.executor.extraLibraryPath\"\n-    val extraLibraryPath = conf.getOption(libraryPathOption).map(p => s\"-Djava.library.path=$p\")\n-    val extraOpts = Seq(extraJavaOpts, extraLibraryPath).flatten.mkString(\" \")\n+    // Start executors with a few necessary configs for registering with the scheduler\n+    val sparkJavaOpts = Utils.sparkJavaOpts(conf, SparkConf.isExecutorStartupConf)\n+    val javaOpts = sparkJavaOpts ++ extraJavaOpts\n \n-    sc.executorEnvs.foreach { case (key, value) =>\n-      environment.addVariables(Environment.Variable.newBuilder()\n-        .setName(key)\n-        .setValue(value)\n-        .build())\n+    val classPathEntries = conf.getOption(\"spark.executor.extraClassPath\").toSeq.flatMap { cp =>\n+      cp.split(java.io.File.pathSeparator)\n     }\n-    val command = CommandInfo.newBuilder()\n-      .setEnvironment(environment)\n-    val driverUrl = \"akka.tcp://spark@%s:%s/user/%s\".format(\n-      conf.get(\"spark.driver.host\"),\n-      conf.get(\"spark.driver.port\"),\n-      CoarseGrainedSchedulerBackend.ACTOR_NAME)\n+    val libraryPathEntries =\n+      conf.getOption(\"spark.executor.extraLibraryPath\").toSeq.flatMap { cp =>\n+        cp.split(java.io.File.pathSeparator)\n+      }\n+\n+    val command = Command(\n+      \"org.apache.spark.executor.CoarseGrainedExecutorBackend\", args, sc.executorEnvs,\n+      classPathEntries, libraryPathEntries, javaOpts)\n \n     val uri = conf.get(\"spark.executor.uri\", null)\n-    if (uri == null) {\n-      val runScript = new File(sparkHome, \"./bin/spark-class\").getCanonicalPath\n-      command.setValue(\n-        \"\\\"%s\\\" org.apache.spark.executor.CoarseGrainedExecutorBackend %s %s %s %s %d\".format(\n-          runScript, extraOpts, driverUrl, offer.getSlaveId.getValue, offer.getHostname, numCores))\n-    } else {\n-      // Grab everything to the first '.'. We'll use that and '*' to\n-      // glob the directory \"correctly\".\n-      val basename = uri.split('/').last.split('.').head\n-      command.setValue(\n-        (\"cd %s*; \" +\n-          \"./bin/spark-class org.apache.spark.executor.CoarseGrainedExecutorBackend %s %s %s %s %d\")\n-          .format(basename, extraOpts, driverUrl, offer.getSlaveId.getValue,\n-            offer.getHostname, numCores))\n-      command.addUris(CommandInfo.URI.newBuilder().setValue(uri))\n+    mesosCommand.setValue(CommandUtils.buildCommandSeq(command, sc.executorMemory,\n+      sparkHome).mkString(\"\\\"\", \"\\\" \\\"\", \"\\\"\"))",
    "line": 88
  }],
  "prId": 2103
}]