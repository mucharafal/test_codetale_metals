[{
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "overridden. But this is kinda obvious since the methods don't have implementation...",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-17T20:11:34Z",
    "diffHunk": "@@ -0,0 +1,245 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io.{BufferedInputStream, InputStream}\n+import java.util.zip.{ZipEntry, ZipOutputStream}\n+\n+import scala.collection.mutable.Map\n+\n+import com.google.common.io.ByteStreams\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.apache.hadoop.hdfs.DFSInputStream\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.scheduler.EventLogFileWriter.codecName\n+import org.apache.spark.util.Utils\n+\n+/** The base class of reader which will read the information of event log file(s). */\n+abstract class EventLogFileReader(\n+    protected val fileSystem: FileSystem,\n+    val rootPath: Path) {\n+\n+  protected def fileSizeForDFS(path: Path): Option[Long] = {\n+    Utils.tryWithResource(fileSystem.open(path)) { in =>\n+      in.getWrappedStream match {\n+        case dfsIn: DFSInputStream => Some(dfsIn.getFileLength)\n+        case _ => None\n+      }\n+    }\n+  }\n+\n+  protected def addFileAsZipEntry(\n+      zipStream: ZipOutputStream,\n+      path: Path,\n+      entryName: String): Unit = {\n+    Utils.tryWithResource(fileSystem.open(path, 1 * 1024 * 1024)) { inputStream =>\n+      zipStream.putNextEntry(new ZipEntry(entryName))\n+      ByteStreams.copy(inputStream, zipStream)\n+      zipStream.closeEntry()\n+    }\n+  }\n+\n+  // ================ methods to be override ================"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Just removed.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-17T23:22:30Z",
    "diffHunk": "@@ -0,0 +1,245 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io.{BufferedInputStream, InputStream}\n+import java.util.zip.{ZipEntry, ZipOutputStream}\n+\n+import scala.collection.mutable.Map\n+\n+import com.google.common.io.ByteStreams\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.apache.hadoop.hdfs.DFSInputStream\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.scheduler.EventLogFileWriter.codecName\n+import org.apache.spark.util.Utils\n+\n+/** The base class of reader which will read the information of event log file(s). */\n+abstract class EventLogFileReader(\n+    protected val fileSystem: FileSystem,\n+    val rootPath: Path) {\n+\n+  protected def fileSizeForDFS(path: Path): Option[Long] = {\n+    Utils.tryWithResource(fileSystem.open(path)) { in =>\n+      in.getWrappedStream match {\n+        case dfsIn: DFSInputStream => Some(dfsIn.getFileLength)\n+        case _ => None\n+      }\n+    }\n+  }\n+\n+  protected def addFileAsZipEntry(\n+      zipStream: ZipOutputStream,\n+      path: Path,\n+      entryName: String): Unit = {\n+    Utils.tryWithResource(fileSystem.open(path, 1 * 1024 * 1024)) { inputStream =>\n+      zipStream.putNextEntry(new ZipEntry(entryName))\n+      ByteStreams.copy(inputStream, zipStream)\n+      zipStream.closeEntry()\n+    }\n+  }\n+\n+  // ================ methods to be override ================"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Same re: calling this \"sequenceNum\" or \"index\", here and in other methods.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-17T20:11:58Z",
    "diffHunk": "@@ -0,0 +1,245 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io.{BufferedInputStream, InputStream}\n+import java.util.zip.{ZipEntry, ZipOutputStream}\n+\n+import scala.collection.mutable.Map\n+\n+import com.google.common.io.ByteStreams\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.apache.hadoop.hdfs.DFSInputStream\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.scheduler.EventLogFileWriter.codecName\n+import org.apache.spark.util.Utils\n+\n+/** The base class of reader which will read the information of event log file(s). */\n+abstract class EventLogFileReader(\n+    protected val fileSystem: FileSystem,\n+    val rootPath: Path) {\n+\n+  protected def fileSizeForDFS(path: Path): Option[Long] = {\n+    Utils.tryWithResource(fileSystem.open(path)) { in =>\n+      in.getWrappedStream match {\n+        case dfsIn: DFSInputStream => Some(dfsIn.getFileLength)\n+        case _ => None\n+      }\n+    }\n+  }\n+\n+  protected def addFileAsZipEntry(\n+      zipStream: ZipOutputStream,\n+      path: Path,\n+      entryName: String): Unit = {\n+    Utils.tryWithResource(fileSystem.open(path, 1 * 1024 * 1024)) { inputStream =>\n+      zipStream.putNextEntry(new ZipEntry(entryName))\n+      ByteStreams.copy(inputStream, zipStream)\n+      zipStream.closeEntry()\n+    }\n+  }\n+\n+  // ================ methods to be override ================\n+\n+  /** Returns the last sequence of event log files. None for single event log file. */"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`compressionCodec`",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-17T20:13:43Z",
    "diffHunk": "@@ -0,0 +1,245 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io.{BufferedInputStream, InputStream}\n+import java.util.zip.{ZipEntry, ZipOutputStream}\n+\n+import scala.collection.mutable.Map\n+\n+import com.google.common.io.ByteStreams\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.apache.hadoop.hdfs.DFSInputStream\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.scheduler.EventLogFileWriter.codecName\n+import org.apache.spark.util.Utils\n+\n+/** The base class of reader which will read the information of event log file(s). */\n+abstract class EventLogFileReader(\n+    protected val fileSystem: FileSystem,\n+    val rootPath: Path) {\n+\n+  protected def fileSizeForDFS(path: Path): Option[Long] = {\n+    Utils.tryWithResource(fileSystem.open(path)) { in =>\n+      in.getWrappedStream match {\n+        case dfsIn: DFSInputStream => Some(dfsIn.getFileLength)\n+        case _ => None\n+      }\n+    }\n+  }\n+\n+  protected def addFileAsZipEntry(\n+      zipStream: ZipOutputStream,\n+      path: Path,\n+      entryName: String): Unit = {\n+    Utils.tryWithResource(fileSystem.open(path, 1 * 1024 * 1024)) { inputStream =>\n+      zipStream.putNextEntry(new ZipEntry(entryName))\n+      ByteStreams.copy(inputStream, zipStream)\n+      zipStream.closeEntry()\n+    }\n+  }\n+\n+  // ================ methods to be override ================\n+\n+  /** Returns the last sequence of event log files. None for single event log file. */\n+  def lastSequence: Option[Long]\n+\n+  /**\n+   * Returns the size of file for the last sequence of event log files. Returns its size for\n+   * single event log file.\n+   */\n+  def fileSizeForLastSequence: Long\n+\n+  /** Returns whether the application is completed. */\n+  def completed: Boolean\n+\n+  /**\n+   * Returns the size of file for the last sequence of event log files, only when\n+   * underlying input stream is DFSInputStream. Otherwise returns None.\n+   */\n+  def fileSizeForLastSequenceForDFS: Option[Long]\n+\n+  /** Returns the modification time for the last sequence of event log files. */\n+  def modificationTime: Long\n+\n+  /**\n+   * This method compresses the files passed in, and writes the compressed data out into the\n+   * ZipOutputStream passed in. Each file is written as a new ZipEntry with its name being\n+   * the name of the file being compressed.\n+   */\n+  def zipEventLogFiles(zipStream: ZipOutputStream): Unit\n+\n+  /** Returns all available event log files. */\n+  def listEventLogFiles: Seq[FileStatus]\n+\n+  /** Returns the short compression name if being used. None if it's uncompressed. */\n+  def compression: Option[String]"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`totalSize`",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-17T20:13:56Z",
    "diffHunk": "@@ -0,0 +1,245 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io.{BufferedInputStream, InputStream}\n+import java.util.zip.{ZipEntry, ZipOutputStream}\n+\n+import scala.collection.mutable.Map\n+\n+import com.google.common.io.ByteStreams\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.apache.hadoop.hdfs.DFSInputStream\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.scheduler.EventLogFileWriter.codecName\n+import org.apache.spark.util.Utils\n+\n+/** The base class of reader which will read the information of event log file(s). */\n+abstract class EventLogFileReader(\n+    protected val fileSystem: FileSystem,\n+    val rootPath: Path) {\n+\n+  protected def fileSizeForDFS(path: Path): Option[Long] = {\n+    Utils.tryWithResource(fileSystem.open(path)) { in =>\n+      in.getWrappedStream match {\n+        case dfsIn: DFSInputStream => Some(dfsIn.getFileLength)\n+        case _ => None\n+      }\n+    }\n+  }\n+\n+  protected def addFileAsZipEntry(\n+      zipStream: ZipOutputStream,\n+      path: Path,\n+      entryName: String): Unit = {\n+    Utils.tryWithResource(fileSystem.open(path, 1 * 1024 * 1024)) { inputStream =>\n+      zipStream.putNextEntry(new ZipEntry(entryName))\n+      ByteStreams.copy(inputStream, zipStream)\n+      zipStream.closeEntry()\n+    }\n+  }\n+\n+  // ================ methods to be override ================\n+\n+  /** Returns the last sequence of event log files. None for single event log file. */\n+  def lastSequence: Option[Long]\n+\n+  /**\n+   * Returns the size of file for the last sequence of event log files. Returns its size for\n+   * single event log file.\n+   */\n+  def fileSizeForLastSequence: Long\n+\n+  /** Returns whether the application is completed. */\n+  def completed: Boolean\n+\n+  /**\n+   * Returns the size of file for the last sequence of event log files, only when\n+   * underlying input stream is DFSInputStream. Otherwise returns None.\n+   */\n+  def fileSizeForLastSequenceForDFS: Option[Long]\n+\n+  /** Returns the modification time for the last sequence of event log files. */\n+  def modificationTime: Long\n+\n+  /**\n+   * This method compresses the files passed in, and writes the compressed data out into the\n+   * ZipOutputStream passed in. Each file is written as a new ZipEntry with its name being\n+   * the name of the file being compressed.\n+   */\n+  def zipEventLogFiles(zipStream: ZipOutputStream): Unit\n+\n+  /** Returns all available event log files. */\n+  def listEventLogFiles: Seq[FileStatus]\n+\n+  /** Returns the short compression name if being used. None if it's uncompressed. */\n+  def compression: Option[String]\n+\n+  /** Returns the size of all event log files. */\n+  def allSize: Long"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "The usual way for factory methods in Scala is to use `apply(...)`. Then you can just use `EventLogFileReader(blah)` instead of `EventLogFileReader.getEventLogReader(blah)`.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-17T20:15:18Z",
    "diffHunk": "@@ -0,0 +1,245 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io.{BufferedInputStream, InputStream}\n+import java.util.zip.{ZipEntry, ZipOutputStream}\n+\n+import scala.collection.mutable.Map\n+\n+import com.google.common.io.ByteStreams\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.apache.hadoop.hdfs.DFSInputStream\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.scheduler.EventLogFileWriter.codecName\n+import org.apache.spark.util.Utils\n+\n+/** The base class of reader which will read the information of event log file(s). */\n+abstract class EventLogFileReader(\n+    protected val fileSystem: FileSystem,\n+    val rootPath: Path) {\n+\n+  protected def fileSizeForDFS(path: Path): Option[Long] = {\n+    Utils.tryWithResource(fileSystem.open(path)) { in =>\n+      in.getWrappedStream match {\n+        case dfsIn: DFSInputStream => Some(dfsIn.getFileLength)\n+        case _ => None\n+      }\n+    }\n+  }\n+\n+  protected def addFileAsZipEntry(\n+      zipStream: ZipOutputStream,\n+      path: Path,\n+      entryName: String): Unit = {\n+    Utils.tryWithResource(fileSystem.open(path, 1 * 1024 * 1024)) { inputStream =>\n+      zipStream.putNextEntry(new ZipEntry(entryName))\n+      ByteStreams.copy(inputStream, zipStream)\n+      zipStream.closeEntry()\n+    }\n+  }\n+\n+  // ================ methods to be override ================\n+\n+  /** Returns the last sequence of event log files. None for single event log file. */\n+  def lastSequence: Option[Long]\n+\n+  /**\n+   * Returns the size of file for the last sequence of event log files. Returns its size for\n+   * single event log file.\n+   */\n+  def fileSizeForLastSequence: Long\n+\n+  /** Returns whether the application is completed. */\n+  def completed: Boolean\n+\n+  /**\n+   * Returns the size of file for the last sequence of event log files, only when\n+   * underlying input stream is DFSInputStream. Otherwise returns None.\n+   */\n+  def fileSizeForLastSequenceForDFS: Option[Long]\n+\n+  /** Returns the modification time for the last sequence of event log files. */\n+  def modificationTime: Long\n+\n+  /**\n+   * This method compresses the files passed in, and writes the compressed data out into the\n+   * ZipOutputStream passed in. Each file is written as a new ZipEntry with its name being\n+   * the name of the file being compressed.\n+   */\n+  def zipEventLogFiles(zipStream: ZipOutputStream): Unit\n+\n+  /** Returns all available event log files. */\n+  def listEventLogFiles: Seq[FileStatus]\n+\n+  /** Returns the short compression name if being used. None if it's uncompressed. */\n+  def compression: Option[String]\n+\n+  /** Returns the size of all event log files. */\n+  def allSize: Long\n+}\n+\n+object EventLogFileReader {\n+  // A cache for compression codecs to avoid creating the same codec many times\n+  private val codecMap = Map.empty[String, CompressionCodec]\n+\n+  def getEventLogReader("
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "This should call the overridden method or vice-versa, since they're basically doing the same thing.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-17T20:16:32Z",
    "diffHunk": "@@ -0,0 +1,245 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io.{BufferedInputStream, InputStream}\n+import java.util.zip.{ZipEntry, ZipOutputStream}\n+\n+import scala.collection.mutable.Map\n+\n+import com.google.common.io.ByteStreams\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.apache.hadoop.hdfs.DFSInputStream\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.scheduler.EventLogFileWriter.codecName\n+import org.apache.spark.util.Utils\n+\n+/** The base class of reader which will read the information of event log file(s). */\n+abstract class EventLogFileReader(\n+    protected val fileSystem: FileSystem,\n+    val rootPath: Path) {\n+\n+  protected def fileSizeForDFS(path: Path): Option[Long] = {\n+    Utils.tryWithResource(fileSystem.open(path)) { in =>\n+      in.getWrappedStream match {\n+        case dfsIn: DFSInputStream => Some(dfsIn.getFileLength)\n+        case _ => None\n+      }\n+    }\n+  }\n+\n+  protected def addFileAsZipEntry(\n+      zipStream: ZipOutputStream,\n+      path: Path,\n+      entryName: String): Unit = {\n+    Utils.tryWithResource(fileSystem.open(path, 1 * 1024 * 1024)) { inputStream =>\n+      zipStream.putNextEntry(new ZipEntry(entryName))\n+      ByteStreams.copy(inputStream, zipStream)\n+      zipStream.closeEntry()\n+    }\n+  }\n+\n+  // ================ methods to be override ================\n+\n+  /** Returns the last sequence of event log files. None for single event log file. */\n+  def lastSequence: Option[Long]\n+\n+  /**\n+   * Returns the size of file for the last sequence of event log files. Returns its size for\n+   * single event log file.\n+   */\n+  def fileSizeForLastSequence: Long\n+\n+  /** Returns whether the application is completed. */\n+  def completed: Boolean\n+\n+  /**\n+   * Returns the size of file for the last sequence of event log files, only when\n+   * underlying input stream is DFSInputStream. Otherwise returns None.\n+   */\n+  def fileSizeForLastSequenceForDFS: Option[Long]\n+\n+  /** Returns the modification time for the last sequence of event log files. */\n+  def modificationTime: Long\n+\n+  /**\n+   * This method compresses the files passed in, and writes the compressed data out into the\n+   * ZipOutputStream passed in. Each file is written as a new ZipEntry with its name being\n+   * the name of the file being compressed.\n+   */\n+  def zipEventLogFiles(zipStream: ZipOutputStream): Unit\n+\n+  /** Returns all available event log files. */\n+  def listEventLogFiles: Seq[FileStatus]\n+\n+  /** Returns the short compression name if being used. None if it's uncompressed. */\n+  def compression: Option[String]\n+\n+  /** Returns the size of all event log files. */\n+  def allSize: Long\n+}\n+\n+object EventLogFileReader {\n+  // A cache for compression codecs to avoid creating the same codec many times\n+  private val codecMap = Map.empty[String, CompressionCodec]\n+\n+  def getEventLogReader(\n+      fs: FileSystem,\n+      path: Path,\n+      lastSequence: Option[Long]): EventLogFileReader = {\n+    lastSequence match {\n+      case Some(_) => new RollingEventLogFilesFileReader(fs, path)\n+      case None => new SingleFileEventLogFileReader(fs, path)\n+    }\n+  }\n+\n+  def getEventLogReader(fs: FileSystem, path: Path): Option[EventLogFileReader] = {"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "I know the existing listener is in this package, but I don't think it makes sense to add all this new code into this package. Makes more sense in `deploy.history` or even a new package `deploy.eventLog`.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-17T20:21:26Z",
    "diffHunk": "@@ -0,0 +1,245 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "I’ll pick existing one for now, and change altogether if the new package looks better. Thanks!",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-18T00:53:40Z",
    "diffHunk": "@@ -0,0 +1,245 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "\"stats\" generally means \"statistics\". Don't you mean \"status\"?",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-17T20:24:56Z",
    "diffHunk": "@@ -0,0 +1,245 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io.{BufferedInputStream, InputStream}\n+import java.util.zip.{ZipEntry, ZipOutputStream}\n+\n+import scala.collection.mutable.Map\n+\n+import com.google.common.io.ByteStreams\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.apache.hadoop.hdfs.DFSInputStream\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.scheduler.EventLogFileWriter.codecName\n+import org.apache.spark.util.Utils\n+\n+/** The base class of reader which will read the information of event log file(s). */\n+abstract class EventLogFileReader(\n+    protected val fileSystem: FileSystem,\n+    val rootPath: Path) {\n+\n+  protected def fileSizeForDFS(path: Path): Option[Long] = {\n+    Utils.tryWithResource(fileSystem.open(path)) { in =>\n+      in.getWrappedStream match {\n+        case dfsIn: DFSInputStream => Some(dfsIn.getFileLength)\n+        case _ => None\n+      }\n+    }\n+  }\n+\n+  protected def addFileAsZipEntry(\n+      zipStream: ZipOutputStream,\n+      path: Path,\n+      entryName: String): Unit = {\n+    Utils.tryWithResource(fileSystem.open(path, 1 * 1024 * 1024)) { inputStream =>\n+      zipStream.putNextEntry(new ZipEntry(entryName))\n+      ByteStreams.copy(inputStream, zipStream)\n+      zipStream.closeEntry()\n+    }\n+  }\n+\n+  // ================ methods to be override ================\n+\n+  /** Returns the last sequence of event log files. None for single event log file. */\n+  def lastSequence: Option[Long]\n+\n+  /**\n+   * Returns the size of file for the last sequence of event log files. Returns its size for\n+   * single event log file.\n+   */\n+  def fileSizeForLastSequence: Long\n+\n+  /** Returns whether the application is completed. */\n+  def completed: Boolean\n+\n+  /**\n+   * Returns the size of file for the last sequence of event log files, only when\n+   * underlying input stream is DFSInputStream. Otherwise returns None.\n+   */\n+  def fileSizeForLastSequenceForDFS: Option[Long]\n+\n+  /** Returns the modification time for the last sequence of event log files. */\n+  def modificationTime: Long\n+\n+  /**\n+   * This method compresses the files passed in, and writes the compressed data out into the\n+   * ZipOutputStream passed in. Each file is written as a new ZipEntry with its name being\n+   * the name of the file being compressed.\n+   */\n+  def zipEventLogFiles(zipStream: ZipOutputStream): Unit\n+\n+  /** Returns all available event log files. */\n+  def listEventLogFiles: Seq[FileStatus]\n+\n+  /** Returns the short compression name if being used. None if it's uncompressed. */\n+  def compression: Option[String]\n+\n+  /** Returns the size of all event log files. */\n+  def allSize: Long\n+}\n+\n+object EventLogFileReader {\n+  // A cache for compression codecs to avoid creating the same codec many times\n+  private val codecMap = Map.empty[String, CompressionCodec]\n+\n+  def getEventLogReader(\n+      fs: FileSystem,\n+      path: Path,\n+      lastSequence: Option[Long]): EventLogFileReader = {\n+    lastSequence match {\n+      case Some(_) => new RollingEventLogFilesFileReader(fs, path)\n+      case None => new SingleFileEventLogFileReader(fs, path)\n+    }\n+  }\n+\n+  def getEventLogReader(fs: FileSystem, path: Path): Option[EventLogFileReader] = {\n+    val status = fs.getFileStatus(path)\n+    if (isSingleEventLog(status)) {\n+      Some(new SingleFileEventLogFileReader(fs, path))\n+    } else if (isRollingEventLogs(status)) {\n+      Some(new RollingEventLogFilesFileReader(fs, path))\n+    } else {\n+      None\n+    }\n+  }\n+\n+  def getEventLogReader(fs: FileSystem, status: FileStatus): Option[EventLogFileReader] = {\n+    if (isSingleEventLog(status)) {\n+      Some(new SingleFileEventLogFileReader(fs, status.getPath))\n+    } else if (isRollingEventLogs(status)) {\n+      Some(new RollingEventLogFilesFileReader(fs, status.getPath))\n+    } else {\n+      None\n+    }\n+  }\n+\n+  /**\n+   * Opens an event log file and returns an input stream that contains the event data.\n+   *\n+   * @return input stream that holds one JSON record per line.\n+   */\n+  def openEventLog(log: Path, fs: FileSystem): InputStream = {\n+    val in = new BufferedInputStream(fs.open(log))\n+    try {\n+      val codec = codecName(log).map { c =>\n+        codecMap.getOrElseUpdate(c, CompressionCodec.createCodec(new SparkConf, c))\n+      }\n+      codec.map(_.compressedContinuousInputStream(in)).getOrElse(in)\n+    } catch {\n+      case e: Throwable =>\n+        in.close()\n+        throw e\n+    }\n+  }\n+\n+  private def isSingleEventLog(status: FileStatus): Boolean = {\n+    !status.isDirectory &&\n+      // FsHistoryProvider used to generate a hidden file which can't be read.  Accidentally\n+      // reading a garbage file is safe, but we would log an error which can be scary to\n+      // the end-user.\n+      !status.getPath.getName.startsWith(\".\")\n+  }\n+\n+  private def isRollingEventLogs(status: FileStatus): Boolean = {\n+    status.isDirectory && RollingEventLogFilesWriter.isEventLogDir(status)\n+  }\n+}\n+\n+/** The reader which will read the information of single event log file. */\n+class SingleFileEventLogFileReader(\n+    fs: FileSystem,\n+    path: Path) extends EventLogFileReader(fs, path) {\n+  // TODO: get stats with constructor and only call if it's needed?\n+  private lazy val stats = fileSystem.getFileStatus(rootPath)"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Hmm... I guess the term is tend to be used for both cases but I agree it could bring confusion. I’ll rename to status.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-18T00:57:52Z",
    "diffHunk": "@@ -0,0 +1,245 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io.{BufferedInputStream, InputStream}\n+import java.util.zip.{ZipEntry, ZipOutputStream}\n+\n+import scala.collection.mutable.Map\n+\n+import com.google.common.io.ByteStreams\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.apache.hadoop.hdfs.DFSInputStream\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.scheduler.EventLogFileWriter.codecName\n+import org.apache.spark.util.Utils\n+\n+/** The base class of reader which will read the information of event log file(s). */\n+abstract class EventLogFileReader(\n+    protected val fileSystem: FileSystem,\n+    val rootPath: Path) {\n+\n+  protected def fileSizeForDFS(path: Path): Option[Long] = {\n+    Utils.tryWithResource(fileSystem.open(path)) { in =>\n+      in.getWrappedStream match {\n+        case dfsIn: DFSInputStream => Some(dfsIn.getFileLength)\n+        case _ => None\n+      }\n+    }\n+  }\n+\n+  protected def addFileAsZipEntry(\n+      zipStream: ZipOutputStream,\n+      path: Path,\n+      entryName: String): Unit = {\n+    Utils.tryWithResource(fileSystem.open(path, 1 * 1024 * 1024)) { inputStream =>\n+      zipStream.putNextEntry(new ZipEntry(entryName))\n+      ByteStreams.copy(inputStream, zipStream)\n+      zipStream.closeEntry()\n+    }\n+  }\n+\n+  // ================ methods to be override ================\n+\n+  /** Returns the last sequence of event log files. None for single event log file. */\n+  def lastSequence: Option[Long]\n+\n+  /**\n+   * Returns the size of file for the last sequence of event log files. Returns its size for\n+   * single event log file.\n+   */\n+  def fileSizeForLastSequence: Long\n+\n+  /** Returns whether the application is completed. */\n+  def completed: Boolean\n+\n+  /**\n+   * Returns the size of file for the last sequence of event log files, only when\n+   * underlying input stream is DFSInputStream. Otherwise returns None.\n+   */\n+  def fileSizeForLastSequenceForDFS: Option[Long]\n+\n+  /** Returns the modification time for the last sequence of event log files. */\n+  def modificationTime: Long\n+\n+  /**\n+   * This method compresses the files passed in, and writes the compressed data out into the\n+   * ZipOutputStream passed in. Each file is written as a new ZipEntry with its name being\n+   * the name of the file being compressed.\n+   */\n+  def zipEventLogFiles(zipStream: ZipOutputStream): Unit\n+\n+  /** Returns all available event log files. */\n+  def listEventLogFiles: Seq[FileStatus]\n+\n+  /** Returns the short compression name if being used. None if it's uncompressed. */\n+  def compression: Option[String]\n+\n+  /** Returns the size of all event log files. */\n+  def allSize: Long\n+}\n+\n+object EventLogFileReader {\n+  // A cache for compression codecs to avoid creating the same codec many times\n+  private val codecMap = Map.empty[String, CompressionCodec]\n+\n+  def getEventLogReader(\n+      fs: FileSystem,\n+      path: Path,\n+      lastSequence: Option[Long]): EventLogFileReader = {\n+    lastSequence match {\n+      case Some(_) => new RollingEventLogFilesFileReader(fs, path)\n+      case None => new SingleFileEventLogFileReader(fs, path)\n+    }\n+  }\n+\n+  def getEventLogReader(fs: FileSystem, path: Path): Option[EventLogFileReader] = {\n+    val status = fs.getFileStatus(path)\n+    if (isSingleEventLog(status)) {\n+      Some(new SingleFileEventLogFileReader(fs, path))\n+    } else if (isRollingEventLogs(status)) {\n+      Some(new RollingEventLogFilesFileReader(fs, path))\n+    } else {\n+      None\n+    }\n+  }\n+\n+  def getEventLogReader(fs: FileSystem, status: FileStatus): Option[EventLogFileReader] = {\n+    if (isSingleEventLog(status)) {\n+      Some(new SingleFileEventLogFileReader(fs, status.getPath))\n+    } else if (isRollingEventLogs(status)) {\n+      Some(new RollingEventLogFilesFileReader(fs, status.getPath))\n+    } else {\n+      None\n+    }\n+  }\n+\n+  /**\n+   * Opens an event log file and returns an input stream that contains the event data.\n+   *\n+   * @return input stream that holds one JSON record per line.\n+   */\n+  def openEventLog(log: Path, fs: FileSystem): InputStream = {\n+    val in = new BufferedInputStream(fs.open(log))\n+    try {\n+      val codec = codecName(log).map { c =>\n+        codecMap.getOrElseUpdate(c, CompressionCodec.createCodec(new SparkConf, c))\n+      }\n+      codec.map(_.compressedContinuousInputStream(in)).getOrElse(in)\n+    } catch {\n+      case e: Throwable =>\n+        in.close()\n+        throw e\n+    }\n+  }\n+\n+  private def isSingleEventLog(status: FileStatus): Boolean = {\n+    !status.isDirectory &&\n+      // FsHistoryProvider used to generate a hidden file which can't be read.  Accidentally\n+      // reading a garbage file is safe, but we would log an error which can be scary to\n+      // the end-user.\n+      !status.getPath.getName.startsWith(\".\")\n+  }\n+\n+  private def isRollingEventLogs(status: FileStatus): Boolean = {\n+    status.isDirectory && RollingEventLogFilesWriter.isEventLogDir(status)\n+  }\n+}\n+\n+/** The reader which will read the information of single event log file. */\n+class SingleFileEventLogFileReader(\n+    fs: FileSystem,\n+    path: Path) extends EventLogFileReader(fs, path) {\n+  // TODO: get stats with constructor and only call if it's needed?\n+  private lazy val stats = fileSystem.getFileStatus(rootPath)"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`fileSizeForDFS` should only be called if the app is still in progress.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-17T20:25:41Z",
    "diffHunk": "@@ -0,0 +1,245 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io.{BufferedInputStream, InputStream}\n+import java.util.zip.{ZipEntry, ZipOutputStream}\n+\n+import scala.collection.mutable.Map\n+\n+import com.google.common.io.ByteStreams\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.apache.hadoop.hdfs.DFSInputStream\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.scheduler.EventLogFileWriter.codecName\n+import org.apache.spark.util.Utils\n+\n+/** The base class of reader which will read the information of event log file(s). */\n+abstract class EventLogFileReader(\n+    protected val fileSystem: FileSystem,\n+    val rootPath: Path) {\n+\n+  protected def fileSizeForDFS(path: Path): Option[Long] = {\n+    Utils.tryWithResource(fileSystem.open(path)) { in =>\n+      in.getWrappedStream match {\n+        case dfsIn: DFSInputStream => Some(dfsIn.getFileLength)\n+        case _ => None\n+      }\n+    }\n+  }\n+\n+  protected def addFileAsZipEntry(\n+      zipStream: ZipOutputStream,\n+      path: Path,\n+      entryName: String): Unit = {\n+    Utils.tryWithResource(fileSystem.open(path, 1 * 1024 * 1024)) { inputStream =>\n+      zipStream.putNextEntry(new ZipEntry(entryName))\n+      ByteStreams.copy(inputStream, zipStream)\n+      zipStream.closeEntry()\n+    }\n+  }\n+\n+  // ================ methods to be override ================\n+\n+  /** Returns the last sequence of event log files. None for single event log file. */\n+  def lastSequence: Option[Long]\n+\n+  /**\n+   * Returns the size of file for the last sequence of event log files. Returns its size for\n+   * single event log file.\n+   */\n+  def fileSizeForLastSequence: Long\n+\n+  /** Returns whether the application is completed. */\n+  def completed: Boolean\n+\n+  /**\n+   * Returns the size of file for the last sequence of event log files, only when\n+   * underlying input stream is DFSInputStream. Otherwise returns None.\n+   */\n+  def fileSizeForLastSequenceForDFS: Option[Long]\n+\n+  /** Returns the modification time for the last sequence of event log files. */\n+  def modificationTime: Long\n+\n+  /**\n+   * This method compresses the files passed in, and writes the compressed data out into the\n+   * ZipOutputStream passed in. Each file is written as a new ZipEntry with its name being\n+   * the name of the file being compressed.\n+   */\n+  def zipEventLogFiles(zipStream: ZipOutputStream): Unit\n+\n+  /** Returns all available event log files. */\n+  def listEventLogFiles: Seq[FileStatus]\n+\n+  /** Returns the short compression name if being used. None if it's uncompressed. */\n+  def compression: Option[String]\n+\n+  /** Returns the size of all event log files. */\n+  def allSize: Long\n+}\n+\n+object EventLogFileReader {\n+  // A cache for compression codecs to avoid creating the same codec many times\n+  private val codecMap = Map.empty[String, CompressionCodec]\n+\n+  def getEventLogReader(\n+      fs: FileSystem,\n+      path: Path,\n+      lastSequence: Option[Long]): EventLogFileReader = {\n+    lastSequence match {\n+      case Some(_) => new RollingEventLogFilesFileReader(fs, path)\n+      case None => new SingleFileEventLogFileReader(fs, path)\n+    }\n+  }\n+\n+  def getEventLogReader(fs: FileSystem, path: Path): Option[EventLogFileReader] = {\n+    val status = fs.getFileStatus(path)\n+    if (isSingleEventLog(status)) {\n+      Some(new SingleFileEventLogFileReader(fs, path))\n+    } else if (isRollingEventLogs(status)) {\n+      Some(new RollingEventLogFilesFileReader(fs, path))\n+    } else {\n+      None\n+    }\n+  }\n+\n+  def getEventLogReader(fs: FileSystem, status: FileStatus): Option[EventLogFileReader] = {\n+    if (isSingleEventLog(status)) {\n+      Some(new SingleFileEventLogFileReader(fs, status.getPath))\n+    } else if (isRollingEventLogs(status)) {\n+      Some(new RollingEventLogFilesFileReader(fs, status.getPath))\n+    } else {\n+      None\n+    }\n+  }\n+\n+  /**\n+   * Opens an event log file and returns an input stream that contains the event data.\n+   *\n+   * @return input stream that holds one JSON record per line.\n+   */\n+  def openEventLog(log: Path, fs: FileSystem): InputStream = {\n+    val in = new BufferedInputStream(fs.open(log))\n+    try {\n+      val codec = codecName(log).map { c =>\n+        codecMap.getOrElseUpdate(c, CompressionCodec.createCodec(new SparkConf, c))\n+      }\n+      codec.map(_.compressedContinuousInputStream(in)).getOrElse(in)\n+    } catch {\n+      case e: Throwable =>\n+        in.close()\n+        throw e\n+    }\n+  }\n+\n+  private def isSingleEventLog(status: FileStatus): Boolean = {\n+    !status.isDirectory &&\n+      // FsHistoryProvider used to generate a hidden file which can't be read.  Accidentally\n+      // reading a garbage file is safe, but we would log an error which can be scary to\n+      // the end-user.\n+      !status.getPath.getName.startsWith(\".\")\n+  }\n+\n+  private def isRollingEventLogs(status: FileStatus): Boolean = {\n+    status.isDirectory && RollingEventLogFilesWriter.isEventLogDir(status)\n+  }\n+}\n+\n+/** The reader which will read the information of single event log file. */\n+class SingleFileEventLogFileReader(\n+    fs: FileSystem,\n+    path: Path) extends EventLogFileReader(fs, path) {\n+  // TODO: get stats with constructor and only call if it's needed?\n+  private lazy val stats = fileSystem.getFileStatus(rootPath)\n+\n+  override def lastSequence: Option[Long] = None\n+\n+  override def fileSizeForLastSequence: Long = stats.getLen\n+\n+  override def completed: Boolean = !rootPath.getName.endsWith(EventLogFileWriter.IN_PROGRESS)\n+\n+  override def fileSizeForLastSequenceForDFS: Option[Long] = fileSizeForDFS(rootPath)"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`.map { status => ... }`",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-17T20:26:53Z",
    "diffHunk": "@@ -0,0 +1,245 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io.{BufferedInputStream, InputStream}\n+import java.util.zip.{ZipEntry, ZipOutputStream}\n+\n+import scala.collection.mutable.Map\n+\n+import com.google.common.io.ByteStreams\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.apache.hadoop.hdfs.DFSInputStream\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.scheduler.EventLogFileWriter.codecName\n+import org.apache.spark.util.Utils\n+\n+/** The base class of reader which will read the information of event log file(s). */\n+abstract class EventLogFileReader(\n+    protected val fileSystem: FileSystem,\n+    val rootPath: Path) {\n+\n+  protected def fileSizeForDFS(path: Path): Option[Long] = {\n+    Utils.tryWithResource(fileSystem.open(path)) { in =>\n+      in.getWrappedStream match {\n+        case dfsIn: DFSInputStream => Some(dfsIn.getFileLength)\n+        case _ => None\n+      }\n+    }\n+  }\n+\n+  protected def addFileAsZipEntry(\n+      zipStream: ZipOutputStream,\n+      path: Path,\n+      entryName: String): Unit = {\n+    Utils.tryWithResource(fileSystem.open(path, 1 * 1024 * 1024)) { inputStream =>\n+      zipStream.putNextEntry(new ZipEntry(entryName))\n+      ByteStreams.copy(inputStream, zipStream)\n+      zipStream.closeEntry()\n+    }\n+  }\n+\n+  // ================ methods to be override ================\n+\n+  /** Returns the last sequence of event log files. None for single event log file. */\n+  def lastSequence: Option[Long]\n+\n+  /**\n+   * Returns the size of file for the last sequence of event log files. Returns its size for\n+   * single event log file.\n+   */\n+  def fileSizeForLastSequence: Long\n+\n+  /** Returns whether the application is completed. */\n+  def completed: Boolean\n+\n+  /**\n+   * Returns the size of file for the last sequence of event log files, only when\n+   * underlying input stream is DFSInputStream. Otherwise returns None.\n+   */\n+  def fileSizeForLastSequenceForDFS: Option[Long]\n+\n+  /** Returns the modification time for the last sequence of event log files. */\n+  def modificationTime: Long\n+\n+  /**\n+   * This method compresses the files passed in, and writes the compressed data out into the\n+   * ZipOutputStream passed in. Each file is written as a new ZipEntry with its name being\n+   * the name of the file being compressed.\n+   */\n+  def zipEventLogFiles(zipStream: ZipOutputStream): Unit\n+\n+  /** Returns all available event log files. */\n+  def listEventLogFiles: Seq[FileStatus]\n+\n+  /** Returns the short compression name if being used. None if it's uncompressed. */\n+  def compression: Option[String]\n+\n+  /** Returns the size of all event log files. */\n+  def allSize: Long\n+}\n+\n+object EventLogFileReader {\n+  // A cache for compression codecs to avoid creating the same codec many times\n+  private val codecMap = Map.empty[String, CompressionCodec]\n+\n+  def getEventLogReader(\n+      fs: FileSystem,\n+      path: Path,\n+      lastSequence: Option[Long]): EventLogFileReader = {\n+    lastSequence match {\n+      case Some(_) => new RollingEventLogFilesFileReader(fs, path)\n+      case None => new SingleFileEventLogFileReader(fs, path)\n+    }\n+  }\n+\n+  def getEventLogReader(fs: FileSystem, path: Path): Option[EventLogFileReader] = {\n+    val status = fs.getFileStatus(path)\n+    if (isSingleEventLog(status)) {\n+      Some(new SingleFileEventLogFileReader(fs, path))\n+    } else if (isRollingEventLogs(status)) {\n+      Some(new RollingEventLogFilesFileReader(fs, path))\n+    } else {\n+      None\n+    }\n+  }\n+\n+  def getEventLogReader(fs: FileSystem, status: FileStatus): Option[EventLogFileReader] = {\n+    if (isSingleEventLog(status)) {\n+      Some(new SingleFileEventLogFileReader(fs, status.getPath))\n+    } else if (isRollingEventLogs(status)) {\n+      Some(new RollingEventLogFilesFileReader(fs, status.getPath))\n+    } else {\n+      None\n+    }\n+  }\n+\n+  /**\n+   * Opens an event log file and returns an input stream that contains the event data.\n+   *\n+   * @return input stream that holds one JSON record per line.\n+   */\n+  def openEventLog(log: Path, fs: FileSystem): InputStream = {\n+    val in = new BufferedInputStream(fs.open(log))\n+    try {\n+      val codec = codecName(log).map { c =>\n+        codecMap.getOrElseUpdate(c, CompressionCodec.createCodec(new SparkConf, c))\n+      }\n+      codec.map(_.compressedContinuousInputStream(in)).getOrElse(in)\n+    } catch {\n+      case e: Throwable =>\n+        in.close()\n+        throw e\n+    }\n+  }\n+\n+  private def isSingleEventLog(status: FileStatus): Boolean = {\n+    !status.isDirectory &&\n+      // FsHistoryProvider used to generate a hidden file which can't be read.  Accidentally\n+      // reading a garbage file is safe, but we would log an error which can be scary to\n+      // the end-user.\n+      !status.getPath.getName.startsWith(\".\")\n+  }\n+\n+  private def isRollingEventLogs(status: FileStatus): Boolean = {\n+    status.isDirectory && RollingEventLogFilesWriter.isEventLogDir(status)\n+  }\n+}\n+\n+/** The reader which will read the information of single event log file. */\n+class SingleFileEventLogFileReader(\n+    fs: FileSystem,\n+    path: Path) extends EventLogFileReader(fs, path) {\n+  // TODO: get stats with constructor and only call if it's needed?\n+  private lazy val stats = fileSystem.getFileStatus(rootPath)\n+\n+  override def lastSequence: Option[Long] = None\n+\n+  override def fileSizeForLastSequence: Long = stats.getLen\n+\n+  override def completed: Boolean = !rootPath.getName.endsWith(EventLogFileWriter.IN_PROGRESS)\n+\n+  override def fileSizeForLastSequenceForDFS: Option[Long] = fileSizeForDFS(rootPath)\n+\n+  override def modificationTime: Long = stats.getModificationTime\n+\n+  override def zipEventLogFiles(zipStream: ZipOutputStream): Unit =\n+    addFileAsZipEntry(zipStream, rootPath, rootPath.getName)\n+\n+  override def listEventLogFiles: Seq[FileStatus] = Seq(stats)\n+\n+  override def compression: Option[String] = EventLogFileWriter.codecName(rootPath)\n+\n+  override def allSize: Long = fileSizeForLastSequence\n+}\n+\n+/** The reader which will read the information of rolled multiple event log files. */\n+class RollingEventLogFilesFileReader(\n+    fs: FileSystem,\n+    path: Path) extends EventLogFileReader(fs, path) {\n+  import RollingEventLogFilesWriter._\n+\n+  private lazy val files: Seq[FileStatus] = {\n+    val ret = fs.listStatus(rootPath).toSeq\n+    require(ret.exists(isEventLogFile), \"Log directory must contain at least one event log file!\")\n+    require(ret.exists(isAppStatusFile), \"Log directory must contain an appstatus file!\")\n+    ret\n+  }\n+\n+  override def lastSequence: Option[Long] = {\n+    val maxSeq = files.filter(isEventLogFile)\n+      .map(stats => getSequence(stats.getPath.getName))"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Should only call this if app is not completed yet.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-17T20:28:25Z",
    "diffHunk": "@@ -0,0 +1,245 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io.{BufferedInputStream, InputStream}\n+import java.util.zip.{ZipEntry, ZipOutputStream}\n+\n+import scala.collection.mutable.Map\n+\n+import com.google.common.io.ByteStreams\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.apache.hadoop.hdfs.DFSInputStream\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.scheduler.EventLogFileWriter.codecName\n+import org.apache.spark.util.Utils\n+\n+/** The base class of reader which will read the information of event log file(s). */\n+abstract class EventLogFileReader(\n+    protected val fileSystem: FileSystem,\n+    val rootPath: Path) {\n+\n+  protected def fileSizeForDFS(path: Path): Option[Long] = {\n+    Utils.tryWithResource(fileSystem.open(path)) { in =>\n+      in.getWrappedStream match {\n+        case dfsIn: DFSInputStream => Some(dfsIn.getFileLength)\n+        case _ => None\n+      }\n+    }\n+  }\n+\n+  protected def addFileAsZipEntry(\n+      zipStream: ZipOutputStream,\n+      path: Path,\n+      entryName: String): Unit = {\n+    Utils.tryWithResource(fileSystem.open(path, 1 * 1024 * 1024)) { inputStream =>\n+      zipStream.putNextEntry(new ZipEntry(entryName))\n+      ByteStreams.copy(inputStream, zipStream)\n+      zipStream.closeEntry()\n+    }\n+  }\n+\n+  // ================ methods to be override ================\n+\n+  /** Returns the last sequence of event log files. None for single event log file. */\n+  def lastSequence: Option[Long]\n+\n+  /**\n+   * Returns the size of file for the last sequence of event log files. Returns its size for\n+   * single event log file.\n+   */\n+  def fileSizeForLastSequence: Long\n+\n+  /** Returns whether the application is completed. */\n+  def completed: Boolean\n+\n+  /**\n+   * Returns the size of file for the last sequence of event log files, only when\n+   * underlying input stream is DFSInputStream. Otherwise returns None.\n+   */\n+  def fileSizeForLastSequenceForDFS: Option[Long]\n+\n+  /** Returns the modification time for the last sequence of event log files. */\n+  def modificationTime: Long\n+\n+  /**\n+   * This method compresses the files passed in, and writes the compressed data out into the\n+   * ZipOutputStream passed in. Each file is written as a new ZipEntry with its name being\n+   * the name of the file being compressed.\n+   */\n+  def zipEventLogFiles(zipStream: ZipOutputStream): Unit\n+\n+  /** Returns all available event log files. */\n+  def listEventLogFiles: Seq[FileStatus]\n+\n+  /** Returns the short compression name if being used. None if it's uncompressed. */\n+  def compression: Option[String]\n+\n+  /** Returns the size of all event log files. */\n+  def allSize: Long\n+}\n+\n+object EventLogFileReader {\n+  // A cache for compression codecs to avoid creating the same codec many times\n+  private val codecMap = Map.empty[String, CompressionCodec]\n+\n+  def getEventLogReader(\n+      fs: FileSystem,\n+      path: Path,\n+      lastSequence: Option[Long]): EventLogFileReader = {\n+    lastSequence match {\n+      case Some(_) => new RollingEventLogFilesFileReader(fs, path)\n+      case None => new SingleFileEventLogFileReader(fs, path)\n+    }\n+  }\n+\n+  def getEventLogReader(fs: FileSystem, path: Path): Option[EventLogFileReader] = {\n+    val status = fs.getFileStatus(path)\n+    if (isSingleEventLog(status)) {\n+      Some(new SingleFileEventLogFileReader(fs, path))\n+    } else if (isRollingEventLogs(status)) {\n+      Some(new RollingEventLogFilesFileReader(fs, path))\n+    } else {\n+      None\n+    }\n+  }\n+\n+  def getEventLogReader(fs: FileSystem, status: FileStatus): Option[EventLogFileReader] = {\n+    if (isSingleEventLog(status)) {\n+      Some(new SingleFileEventLogFileReader(fs, status.getPath))\n+    } else if (isRollingEventLogs(status)) {\n+      Some(new RollingEventLogFilesFileReader(fs, status.getPath))\n+    } else {\n+      None\n+    }\n+  }\n+\n+  /**\n+   * Opens an event log file and returns an input stream that contains the event data.\n+   *\n+   * @return input stream that holds one JSON record per line.\n+   */\n+  def openEventLog(log: Path, fs: FileSystem): InputStream = {\n+    val in = new BufferedInputStream(fs.open(log))\n+    try {\n+      val codec = codecName(log).map { c =>\n+        codecMap.getOrElseUpdate(c, CompressionCodec.createCodec(new SparkConf, c))\n+      }\n+      codec.map(_.compressedContinuousInputStream(in)).getOrElse(in)\n+    } catch {\n+      case e: Throwable =>\n+        in.close()\n+        throw e\n+    }\n+  }\n+\n+  private def isSingleEventLog(status: FileStatus): Boolean = {\n+    !status.isDirectory &&\n+      // FsHistoryProvider used to generate a hidden file which can't be read.  Accidentally\n+      // reading a garbage file is safe, but we would log an error which can be scary to\n+      // the end-user.\n+      !status.getPath.getName.startsWith(\".\")\n+  }\n+\n+  private def isRollingEventLogs(status: FileStatus): Boolean = {\n+    status.isDirectory && RollingEventLogFilesWriter.isEventLogDir(status)\n+  }\n+}\n+\n+/** The reader which will read the information of single event log file. */\n+class SingleFileEventLogFileReader(\n+    fs: FileSystem,\n+    path: Path) extends EventLogFileReader(fs, path) {\n+  // TODO: get stats with constructor and only call if it's needed?\n+  private lazy val stats = fileSystem.getFileStatus(rootPath)\n+\n+  override def lastSequence: Option[Long] = None\n+\n+  override def fileSizeForLastSequence: Long = stats.getLen\n+\n+  override def completed: Boolean = !rootPath.getName.endsWith(EventLogFileWriter.IN_PROGRESS)\n+\n+  override def fileSizeForLastSequenceForDFS: Option[Long] = fileSizeForDFS(rootPath)\n+\n+  override def modificationTime: Long = stats.getModificationTime\n+\n+  override def zipEventLogFiles(zipStream: ZipOutputStream): Unit =\n+    addFileAsZipEntry(zipStream, rootPath, rootPath.getName)\n+\n+  override def listEventLogFiles: Seq[FileStatus] = Seq(stats)\n+\n+  override def compression: Option[String] = EventLogFileWriter.codecName(rootPath)\n+\n+  override def allSize: Long = fileSizeForLastSequence\n+}\n+\n+/** The reader which will read the information of rolled multiple event log files. */\n+class RollingEventLogFilesFileReader(\n+    fs: FileSystem,\n+    path: Path) extends EventLogFileReader(fs, path) {\n+  import RollingEventLogFilesWriter._\n+\n+  private lazy val files: Seq[FileStatus] = {\n+    val ret = fs.listStatus(rootPath).toSeq\n+    require(ret.exists(isEventLogFile), \"Log directory must contain at least one event log file!\")\n+    require(ret.exists(isAppStatusFile), \"Log directory must contain an appstatus file!\")\n+    ret\n+  }\n+\n+  override def lastSequence: Option[Long] = {\n+    val maxSeq = files.filter(isEventLogFile)\n+      .map(stats => getSequence(stats.getPath.getName))\n+      .max\n+    Some(maxSeq)\n+  }\n+\n+  override def fileSizeForLastSequence: Long = lastEventLogFile.getLen\n+\n+  override def completed: Boolean = {\n+    val appStatsFile = files.find(isAppStatusFile)\n+    require(appStatsFile.isDefined)\n+    appStatsFile.exists(!_.getPath.getName.endsWith(EventLogFileWriter.IN_PROGRESS))\n+  }\n+\n+  override def fileSizeForLastSequenceForDFS: Option[Long] =\n+    fileSizeForDFS(lastEventLogFile.getPath)"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "use full method declaration if your method doesn't fit in one line.\r\n\r\nAlso, `.sortBy { status => ... }`.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-17T20:29:31Z",
    "diffHunk": "@@ -0,0 +1,245 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io.{BufferedInputStream, InputStream}\n+import java.util.zip.{ZipEntry, ZipOutputStream}\n+\n+import scala.collection.mutable.Map\n+\n+import com.google.common.io.ByteStreams\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.apache.hadoop.hdfs.DFSInputStream\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.scheduler.EventLogFileWriter.codecName\n+import org.apache.spark.util.Utils\n+\n+/** The base class of reader which will read the information of event log file(s). */\n+abstract class EventLogFileReader(\n+    protected val fileSystem: FileSystem,\n+    val rootPath: Path) {\n+\n+  protected def fileSizeForDFS(path: Path): Option[Long] = {\n+    Utils.tryWithResource(fileSystem.open(path)) { in =>\n+      in.getWrappedStream match {\n+        case dfsIn: DFSInputStream => Some(dfsIn.getFileLength)\n+        case _ => None\n+      }\n+    }\n+  }\n+\n+  protected def addFileAsZipEntry(\n+      zipStream: ZipOutputStream,\n+      path: Path,\n+      entryName: String): Unit = {\n+    Utils.tryWithResource(fileSystem.open(path, 1 * 1024 * 1024)) { inputStream =>\n+      zipStream.putNextEntry(new ZipEntry(entryName))\n+      ByteStreams.copy(inputStream, zipStream)\n+      zipStream.closeEntry()\n+    }\n+  }\n+\n+  // ================ methods to be override ================\n+\n+  /** Returns the last sequence of event log files. None for single event log file. */\n+  def lastSequence: Option[Long]\n+\n+  /**\n+   * Returns the size of file for the last sequence of event log files. Returns its size for\n+   * single event log file.\n+   */\n+  def fileSizeForLastSequence: Long\n+\n+  /** Returns whether the application is completed. */\n+  def completed: Boolean\n+\n+  /**\n+   * Returns the size of file for the last sequence of event log files, only when\n+   * underlying input stream is DFSInputStream. Otherwise returns None.\n+   */\n+  def fileSizeForLastSequenceForDFS: Option[Long]\n+\n+  /** Returns the modification time for the last sequence of event log files. */\n+  def modificationTime: Long\n+\n+  /**\n+   * This method compresses the files passed in, and writes the compressed data out into the\n+   * ZipOutputStream passed in. Each file is written as a new ZipEntry with its name being\n+   * the name of the file being compressed.\n+   */\n+  def zipEventLogFiles(zipStream: ZipOutputStream): Unit\n+\n+  /** Returns all available event log files. */\n+  def listEventLogFiles: Seq[FileStatus]\n+\n+  /** Returns the short compression name if being used. None if it's uncompressed. */\n+  def compression: Option[String]\n+\n+  /** Returns the size of all event log files. */\n+  def allSize: Long\n+}\n+\n+object EventLogFileReader {\n+  // A cache for compression codecs to avoid creating the same codec many times\n+  private val codecMap = Map.empty[String, CompressionCodec]\n+\n+  def getEventLogReader(\n+      fs: FileSystem,\n+      path: Path,\n+      lastSequence: Option[Long]): EventLogFileReader = {\n+    lastSequence match {\n+      case Some(_) => new RollingEventLogFilesFileReader(fs, path)\n+      case None => new SingleFileEventLogFileReader(fs, path)\n+    }\n+  }\n+\n+  def getEventLogReader(fs: FileSystem, path: Path): Option[EventLogFileReader] = {\n+    val status = fs.getFileStatus(path)\n+    if (isSingleEventLog(status)) {\n+      Some(new SingleFileEventLogFileReader(fs, path))\n+    } else if (isRollingEventLogs(status)) {\n+      Some(new RollingEventLogFilesFileReader(fs, path))\n+    } else {\n+      None\n+    }\n+  }\n+\n+  def getEventLogReader(fs: FileSystem, status: FileStatus): Option[EventLogFileReader] = {\n+    if (isSingleEventLog(status)) {\n+      Some(new SingleFileEventLogFileReader(fs, status.getPath))\n+    } else if (isRollingEventLogs(status)) {\n+      Some(new RollingEventLogFilesFileReader(fs, status.getPath))\n+    } else {\n+      None\n+    }\n+  }\n+\n+  /**\n+   * Opens an event log file and returns an input stream that contains the event data.\n+   *\n+   * @return input stream that holds one JSON record per line.\n+   */\n+  def openEventLog(log: Path, fs: FileSystem): InputStream = {\n+    val in = new BufferedInputStream(fs.open(log))\n+    try {\n+      val codec = codecName(log).map { c =>\n+        codecMap.getOrElseUpdate(c, CompressionCodec.createCodec(new SparkConf, c))\n+      }\n+      codec.map(_.compressedContinuousInputStream(in)).getOrElse(in)\n+    } catch {\n+      case e: Throwable =>\n+        in.close()\n+        throw e\n+    }\n+  }\n+\n+  private def isSingleEventLog(status: FileStatus): Boolean = {\n+    !status.isDirectory &&\n+      // FsHistoryProvider used to generate a hidden file which can't be read.  Accidentally\n+      // reading a garbage file is safe, but we would log an error which can be scary to\n+      // the end-user.\n+      !status.getPath.getName.startsWith(\".\")\n+  }\n+\n+  private def isRollingEventLogs(status: FileStatus): Boolean = {\n+    status.isDirectory && RollingEventLogFilesWriter.isEventLogDir(status)\n+  }\n+}\n+\n+/** The reader which will read the information of single event log file. */\n+class SingleFileEventLogFileReader(\n+    fs: FileSystem,\n+    path: Path) extends EventLogFileReader(fs, path) {\n+  // TODO: get stats with constructor and only call if it's needed?\n+  private lazy val stats = fileSystem.getFileStatus(rootPath)\n+\n+  override def lastSequence: Option[Long] = None\n+\n+  override def fileSizeForLastSequence: Long = stats.getLen\n+\n+  override def completed: Boolean = !rootPath.getName.endsWith(EventLogFileWriter.IN_PROGRESS)\n+\n+  override def fileSizeForLastSequenceForDFS: Option[Long] = fileSizeForDFS(rootPath)\n+\n+  override def modificationTime: Long = stats.getModificationTime\n+\n+  override def zipEventLogFiles(zipStream: ZipOutputStream): Unit =\n+    addFileAsZipEntry(zipStream, rootPath, rootPath.getName)\n+\n+  override def listEventLogFiles: Seq[FileStatus] = Seq(stats)\n+\n+  override def compression: Option[String] = EventLogFileWriter.codecName(rootPath)\n+\n+  override def allSize: Long = fileSizeForLastSequence\n+}\n+\n+/** The reader which will read the information of rolled multiple event log files. */\n+class RollingEventLogFilesFileReader(\n+    fs: FileSystem,\n+    path: Path) extends EventLogFileReader(fs, path) {\n+  import RollingEventLogFilesWriter._\n+\n+  private lazy val files: Seq[FileStatus] = {\n+    val ret = fs.listStatus(rootPath).toSeq\n+    require(ret.exists(isEventLogFile), \"Log directory must contain at least one event log file!\")\n+    require(ret.exists(isAppStatusFile), \"Log directory must contain an appstatus file!\")\n+    ret\n+  }\n+\n+  override def lastSequence: Option[Long] = {\n+    val maxSeq = files.filter(isEventLogFile)\n+      .map(stats => getSequence(stats.getPath.getName))\n+      .max\n+    Some(maxSeq)\n+  }\n+\n+  override def fileSizeForLastSequence: Long = lastEventLogFile.getLen\n+\n+  override def completed: Boolean = {\n+    val appStatsFile = files.find(isAppStatusFile)\n+    require(appStatsFile.isDefined)\n+    appStatsFile.exists(!_.getPath.getName.endsWith(EventLogFileWriter.IN_PROGRESS))\n+  }\n+\n+  override def fileSizeForLastSequenceForDFS: Option[Long] =\n+    fileSizeForDFS(lastEventLogFile.getPath)\n+\n+  override def modificationTime: Long = lastEventLogFile.getModificationTime\n+\n+  override def zipEventLogFiles(zipStream: ZipOutputStream): Unit = {\n+    val dirEntryName = rootPath.getName + \"/\"\n+    zipStream.putNextEntry(new ZipEntry(dirEntryName))\n+    files.foreach { file =>\n+      addFileAsZipEntry(zipStream, file.getPath, dirEntryName + file.getPath.getName)\n+    }\n+  }\n+\n+  override def listEventLogFiles: Seq[FileStatus] = eventLogFiles\n+\n+  override def compression: Option[String] = EventLogFileWriter.codecName(\n+    eventLogFiles.head.getPath)\n+\n+  override def allSize: Long = eventLogFiles.map(_.getLen).sum\n+\n+  private def eventLogFiles: Seq[FileStatus] = files.filter(isEventLogFile)"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "I know the existing class does this, but I really dislike importing this class. If you end up exposing this in some public method called from another place, it makes things awkward in the caller.\r\n\r\nInstead, either import a specific implementation (like `HashMap`) or follow the pattern of using `mutable.[Hash]Map` where you want a mutable map.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-17T20:44:06Z",
    "diffHunk": "@@ -0,0 +1,245 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io.{BufferedInputStream, InputStream}\n+import java.util.zip.{ZipEntry, ZipOutputStream}\n+\n+import scala.collection.mutable.Map"
  }],
  "prId": 25670
}]