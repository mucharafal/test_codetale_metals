[{
  "comments": [{
    "author": {
      "login": "jose-torres"
    },
    "body": "Who's cleaning up the old one?",
    "commit": "56442dc1c7450518d9bc84b4bfeddb017daa967b",
    "createdAt": "2018-05-10T15:45:00Z",
    "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.lang.management.ManagementFactory\n+import java.nio.ByteBuffer\n+import java.util.Properties\n+\n+import scala.language.existentials\n+\n+import org.apache.spark._\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.shuffle.ShuffleWriter\n+\n+/**\n+ * A ShuffleMapTask divides the elements of an RDD into multiple buckets (based on a partitioner\n+ * specified in the ShuffleDependency).\n+ *\n+ * See [[org.apache.spark.scheduler.Task]] for more information.\n+ *\n+ * @param stageId id of the stage this task belongs to\n+ * @param stageAttemptId attempt id of the stage this task belongs to\n+ * @param taskBinary broadcast version of the RDD and the ShuffleDependency. Once deserialized,\n+ *                   the type should be (RDD[_], ShuffleDependency[_, _, _]).\n+ * @param partition partition of the RDD this task is associated with\n+ * @param locs preferred task execution locations for locality scheduling\n+ * @param localProperties copy of thread-local properties set by the user on the driver side.\n+ * @param serializedTaskMetrics a `TaskMetrics` that is created and serialized on the driver side\n+ *                              and sent to executor side.\n+ * @param totalShuffleNum total shuffle number for current job.\n+ *\n+ * The parameters below are optional:\n+ * @param jobId id of the job this task belongs to\n+ * @param appId id of the app this task belongs to\n+ * @param appAttemptId attempt id of the app this task belongs to\n+ */\n+private[spark] class ContinuousShuffleMapTask(\n+    stageId: Int,\n+    stageAttemptId: Int,\n+    taskBinary: Broadcast[Array[Byte]],\n+    partition: Partition,\n+    @transient private var locs: Seq[TaskLocation],\n+    localProperties: Properties,\n+    serializedTaskMetrics: Array[Byte],\n+    totalShuffleNum: Int,\n+    jobId: Option[Int] = None,\n+    appId: Option[String] = None,\n+    appAttemptId: Option[String] = None)\n+  extends Task[Unit](stageId, stageAttemptId, partition.index, localProperties,\n+    serializedTaskMetrics, jobId, appId, appAttemptId)\n+    with Logging {\n+\n+  /** A constructor used only in test suites. This does not require passing in an RDD. */\n+  def this(partitionId: Int, totalShuffleNum: Int) {\n+    this(0, 0, null, new Partition { override def index: Int = 0 }, null, new Properties,\n+      null, totalShuffleNum)\n+  }\n+\n+  @transient private val preferredLocs: Seq[TaskLocation] = {\n+    if (locs == null) Nil else locs.toSet.toSeq\n+  }\n+\n+  // TODO: Get current epoch from epoch coordinator while task restart, also epoch is Long, we\n+  //       should deal with it.\n+  var currentEpoch = context.getLocalProperty(SparkEnv.START_EPOCH_KEY).toInt\n+\n+  override def runTask(context: TaskContext): Unit = {\n+    // Deserialize the RDD using the broadcast variable.\n+    val threadMXBean = ManagementFactory.getThreadMXBean\n+    val deserializeStartTime = System.currentTimeMillis()\n+    val deserializeStartCpuTime = if (threadMXBean.isCurrentThreadCpuTimeSupported) {\n+      threadMXBean.getCurrentThreadCpuTime\n+    } else 0L\n+    val ser = SparkEnv.get.closureSerializer.newInstance()\n+    // TODO: rdd here should be a wrap of ShuffledRowRDD which never stop\n+    val (rdd, dep) = ser.deserialize[(RDD[_], ShuffleDependency[_, _, _])](\n+      ByteBuffer.wrap(taskBinary.value), Thread.currentThread.getContextClassLoader)\n+    _executorDeserializeTime = System.currentTimeMillis() - deserializeStartTime\n+    _executorDeserializeCpuTime = if (threadMXBean.isCurrentThreadCpuTimeSupported) {\n+      threadMXBean.getCurrentThreadCpuTime - deserializeStartCpuTime\n+    } else 0L\n+\n+    var writer: ShuffleWriter[Any, Any] = null\n+    val manager = SparkEnv.get.shuffleManager\n+    val mapOutputTracker = SparkEnv.get.mapOutputTracker\n+      .asInstanceOf[ContinuousMapOutputTrackerWorker]\n+\n+    while (!context.isCompleted() || !context.isInterrupted()) {\n+      try {\n+        // Create a ContinuousShuffleDependency which has new shuffleId based on continuous epoch.",
    "line": 107
  }, {
    "author": {
      "login": "xuanyuanking"
    },
    "body": "The old one(ShuffleDependency) register cleaner on driver side. ContinuousShuffleDependency we created on executor side, the tmp data delete by epoch coordinator receive commit event(Not in this PR).",
    "commit": "56442dc1c7450518d9bc84b4bfeddb017daa967b",
    "createdAt": "2018-05-11T12:25:23Z",
    "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.lang.management.ManagementFactory\n+import java.nio.ByteBuffer\n+import java.util.Properties\n+\n+import scala.language.existentials\n+\n+import org.apache.spark._\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.shuffle.ShuffleWriter\n+\n+/**\n+ * A ShuffleMapTask divides the elements of an RDD into multiple buckets (based on a partitioner\n+ * specified in the ShuffleDependency).\n+ *\n+ * See [[org.apache.spark.scheduler.Task]] for more information.\n+ *\n+ * @param stageId id of the stage this task belongs to\n+ * @param stageAttemptId attempt id of the stage this task belongs to\n+ * @param taskBinary broadcast version of the RDD and the ShuffleDependency. Once deserialized,\n+ *                   the type should be (RDD[_], ShuffleDependency[_, _, _]).\n+ * @param partition partition of the RDD this task is associated with\n+ * @param locs preferred task execution locations for locality scheduling\n+ * @param localProperties copy of thread-local properties set by the user on the driver side.\n+ * @param serializedTaskMetrics a `TaskMetrics` that is created and serialized on the driver side\n+ *                              and sent to executor side.\n+ * @param totalShuffleNum total shuffle number for current job.\n+ *\n+ * The parameters below are optional:\n+ * @param jobId id of the job this task belongs to\n+ * @param appId id of the app this task belongs to\n+ * @param appAttemptId attempt id of the app this task belongs to\n+ */\n+private[spark] class ContinuousShuffleMapTask(\n+    stageId: Int,\n+    stageAttemptId: Int,\n+    taskBinary: Broadcast[Array[Byte]],\n+    partition: Partition,\n+    @transient private var locs: Seq[TaskLocation],\n+    localProperties: Properties,\n+    serializedTaskMetrics: Array[Byte],\n+    totalShuffleNum: Int,\n+    jobId: Option[Int] = None,\n+    appId: Option[String] = None,\n+    appAttemptId: Option[String] = None)\n+  extends Task[Unit](stageId, stageAttemptId, partition.index, localProperties,\n+    serializedTaskMetrics, jobId, appId, appAttemptId)\n+    with Logging {\n+\n+  /** A constructor used only in test suites. This does not require passing in an RDD. */\n+  def this(partitionId: Int, totalShuffleNum: Int) {\n+    this(0, 0, null, new Partition { override def index: Int = 0 }, null, new Properties,\n+      null, totalShuffleNum)\n+  }\n+\n+  @transient private val preferredLocs: Seq[TaskLocation] = {\n+    if (locs == null) Nil else locs.toSet.toSeq\n+  }\n+\n+  // TODO: Get current epoch from epoch coordinator while task restart, also epoch is Long, we\n+  //       should deal with it.\n+  var currentEpoch = context.getLocalProperty(SparkEnv.START_EPOCH_KEY).toInt\n+\n+  override def runTask(context: TaskContext): Unit = {\n+    // Deserialize the RDD using the broadcast variable.\n+    val threadMXBean = ManagementFactory.getThreadMXBean\n+    val deserializeStartTime = System.currentTimeMillis()\n+    val deserializeStartCpuTime = if (threadMXBean.isCurrentThreadCpuTimeSupported) {\n+      threadMXBean.getCurrentThreadCpuTime\n+    } else 0L\n+    val ser = SparkEnv.get.closureSerializer.newInstance()\n+    // TODO: rdd here should be a wrap of ShuffledRowRDD which never stop\n+    val (rdd, dep) = ser.deserialize[(RDD[_], ShuffleDependency[_, _, _])](\n+      ByteBuffer.wrap(taskBinary.value), Thread.currentThread.getContextClassLoader)\n+    _executorDeserializeTime = System.currentTimeMillis() - deserializeStartTime\n+    _executorDeserializeCpuTime = if (threadMXBean.isCurrentThreadCpuTimeSupported) {\n+      threadMXBean.getCurrentThreadCpuTime - deserializeStartCpuTime\n+    } else 0L\n+\n+    var writer: ShuffleWriter[Any, Any] = null\n+    val manager = SparkEnv.get.shuffleManager\n+    val mapOutputTracker = SparkEnv.get.mapOutputTracker\n+      .asInstanceOf[ContinuousMapOutputTrackerWorker]\n+\n+    while (!context.isCompleted() || !context.isInterrupted()) {\n+      try {\n+        // Create a ContinuousShuffleDependency which has new shuffleId based on continuous epoch.",
    "line": 107
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "The cleanup should probably be in the same PR as the registration.\r\n\r\nWon't that strategy leak resources if the query ends up shut down before the commit event is received?",
    "commit": "56442dc1c7450518d9bc84b4bfeddb017daa967b",
    "createdAt": "2018-05-11T15:19:15Z",
    "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.lang.management.ManagementFactory\n+import java.nio.ByteBuffer\n+import java.util.Properties\n+\n+import scala.language.existentials\n+\n+import org.apache.spark._\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.shuffle.ShuffleWriter\n+\n+/**\n+ * A ShuffleMapTask divides the elements of an RDD into multiple buckets (based on a partitioner\n+ * specified in the ShuffleDependency).\n+ *\n+ * See [[org.apache.spark.scheduler.Task]] for more information.\n+ *\n+ * @param stageId id of the stage this task belongs to\n+ * @param stageAttemptId attempt id of the stage this task belongs to\n+ * @param taskBinary broadcast version of the RDD and the ShuffleDependency. Once deserialized,\n+ *                   the type should be (RDD[_], ShuffleDependency[_, _, _]).\n+ * @param partition partition of the RDD this task is associated with\n+ * @param locs preferred task execution locations for locality scheduling\n+ * @param localProperties copy of thread-local properties set by the user on the driver side.\n+ * @param serializedTaskMetrics a `TaskMetrics` that is created and serialized on the driver side\n+ *                              and sent to executor side.\n+ * @param totalShuffleNum total shuffle number for current job.\n+ *\n+ * The parameters below are optional:\n+ * @param jobId id of the job this task belongs to\n+ * @param appId id of the app this task belongs to\n+ * @param appAttemptId attempt id of the app this task belongs to\n+ */\n+private[spark] class ContinuousShuffleMapTask(\n+    stageId: Int,\n+    stageAttemptId: Int,\n+    taskBinary: Broadcast[Array[Byte]],\n+    partition: Partition,\n+    @transient private var locs: Seq[TaskLocation],\n+    localProperties: Properties,\n+    serializedTaskMetrics: Array[Byte],\n+    totalShuffleNum: Int,\n+    jobId: Option[Int] = None,\n+    appId: Option[String] = None,\n+    appAttemptId: Option[String] = None)\n+  extends Task[Unit](stageId, stageAttemptId, partition.index, localProperties,\n+    serializedTaskMetrics, jobId, appId, appAttemptId)\n+    with Logging {\n+\n+  /** A constructor used only in test suites. This does not require passing in an RDD. */\n+  def this(partitionId: Int, totalShuffleNum: Int) {\n+    this(0, 0, null, new Partition { override def index: Int = 0 }, null, new Properties,\n+      null, totalShuffleNum)\n+  }\n+\n+  @transient private val preferredLocs: Seq[TaskLocation] = {\n+    if (locs == null) Nil else locs.toSet.toSeq\n+  }\n+\n+  // TODO: Get current epoch from epoch coordinator while task restart, also epoch is Long, we\n+  //       should deal with it.\n+  var currentEpoch = context.getLocalProperty(SparkEnv.START_EPOCH_KEY).toInt\n+\n+  override def runTask(context: TaskContext): Unit = {\n+    // Deserialize the RDD using the broadcast variable.\n+    val threadMXBean = ManagementFactory.getThreadMXBean\n+    val deserializeStartTime = System.currentTimeMillis()\n+    val deserializeStartCpuTime = if (threadMXBean.isCurrentThreadCpuTimeSupported) {\n+      threadMXBean.getCurrentThreadCpuTime\n+    } else 0L\n+    val ser = SparkEnv.get.closureSerializer.newInstance()\n+    // TODO: rdd here should be a wrap of ShuffledRowRDD which never stop\n+    val (rdd, dep) = ser.deserialize[(RDD[_], ShuffleDependency[_, _, _])](\n+      ByteBuffer.wrap(taskBinary.value), Thread.currentThread.getContextClassLoader)\n+    _executorDeserializeTime = System.currentTimeMillis() - deserializeStartTime\n+    _executorDeserializeCpuTime = if (threadMXBean.isCurrentThreadCpuTimeSupported) {\n+      threadMXBean.getCurrentThreadCpuTime - deserializeStartCpuTime\n+    } else 0L\n+\n+    var writer: ShuffleWriter[Any, Any] = null\n+    val manager = SparkEnv.get.shuffleManager\n+    val mapOutputTracker = SparkEnv.get.mapOutputTracker\n+      .asInstanceOf[ContinuousMapOutputTrackerWorker]\n+\n+    while (!context.isCompleted() || !context.isInterrupted()) {\n+      try {\n+        // Create a ContinuousShuffleDependency which has new shuffleId based on continuous epoch.",
    "line": 107
  }],
  "prId": 21293
}, {
  "comments": [{
    "author": {
      "login": "jose-torres"
    },
    "body": "There's a lot of code here that has nothing to do with continuous processing. I'm not sure how we can be confident it's correct and will remain correct as the scheduler evolves.",
    "commit": "56442dc1c7450518d9bc84b4bfeddb017daa967b",
    "createdAt": "2018-05-10T15:47:19Z",
    "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.lang.management.ManagementFactory\n+import java.nio.ByteBuffer\n+import java.util.Properties\n+\n+import scala.language.existentials\n+\n+import org.apache.spark._\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.shuffle.ShuffleWriter\n+\n+/**\n+ * A ShuffleMapTask divides the elements of an RDD into multiple buckets (based on a partitioner\n+ * specified in the ShuffleDependency).\n+ *\n+ * See [[org.apache.spark.scheduler.Task]] for more information.\n+ *\n+ * @param stageId id of the stage this task belongs to\n+ * @param stageAttemptId attempt id of the stage this task belongs to\n+ * @param taskBinary broadcast version of the RDD and the ShuffleDependency. Once deserialized,\n+ *                   the type should be (RDD[_], ShuffleDependency[_, _, _]).\n+ * @param partition partition of the RDD this task is associated with\n+ * @param locs preferred task execution locations for locality scheduling\n+ * @param localProperties copy of thread-local properties set by the user on the driver side.\n+ * @param serializedTaskMetrics a `TaskMetrics` that is created and serialized on the driver side\n+ *                              and sent to executor side.\n+ * @param totalShuffleNum total shuffle number for current job.\n+ *\n+ * The parameters below are optional:\n+ * @param jobId id of the job this task belongs to\n+ * @param appId id of the app this task belongs to\n+ * @param appAttemptId attempt id of the app this task belongs to\n+ */\n+private[spark] class ContinuousShuffleMapTask(\n+    stageId: Int,\n+    stageAttemptId: Int,\n+    taskBinary: Broadcast[Array[Byte]],\n+    partition: Partition,\n+    @transient private var locs: Seq[TaskLocation],\n+    localProperties: Properties,\n+    serializedTaskMetrics: Array[Byte],\n+    totalShuffleNum: Int,\n+    jobId: Option[Int] = None,\n+    appId: Option[String] = None,\n+    appAttemptId: Option[String] = None)\n+  extends Task[Unit](stageId, stageAttemptId, partition.index, localProperties,\n+    serializedTaskMetrics, jobId, appId, appAttemptId)\n+    with Logging {\n+\n+  /** A constructor used only in test suites. This does not require passing in an RDD. */\n+  def this(partitionId: Int, totalShuffleNum: Int) {\n+    this(0, 0, null, new Partition { override def index: Int = 0 }, null, new Properties,\n+      null, totalShuffleNum)\n+  }\n+\n+  @transient private val preferredLocs: Seq[TaskLocation] = {\n+    if (locs == null) Nil else locs.toSet.toSeq\n+  }\n+\n+  // TODO: Get current epoch from epoch coordinator while task restart, also epoch is Long, we\n+  //       should deal with it.\n+  var currentEpoch = context.getLocalProperty(SparkEnv.START_EPOCH_KEY).toInt\n+\n+  override def runTask(context: TaskContext): Unit = {\n+    // Deserialize the RDD using the broadcast variable.\n+    val threadMXBean = ManagementFactory.getThreadMXBean\n+    val deserializeStartTime = System.currentTimeMillis()\n+    val deserializeStartCpuTime = if (threadMXBean.isCurrentThreadCpuTimeSupported) {\n+      threadMXBean.getCurrentThreadCpuTime\n+    } else 0L\n+    val ser = SparkEnv.get.closureSerializer.newInstance()\n+    // TODO: rdd here should be a wrap of ShuffledRowRDD which never stop\n+    val (rdd, dep) = ser.deserialize[(RDD[_], ShuffleDependency[_, _, _])](\n+      ByteBuffer.wrap(taskBinary.value), Thread.currentThread.getContextClassLoader)\n+    _executorDeserializeTime = System.currentTimeMillis() - deserializeStartTime\n+    _executorDeserializeCpuTime = if (threadMXBean.isCurrentThreadCpuTimeSupported) {\n+      threadMXBean.getCurrentThreadCpuTime - deserializeStartCpuTime\n+    } else 0L",
    "line": 98
  }, {
    "author": {
      "login": "xuanyuanking"
    },
    "body": "Currently we have passed some shuffle demo over CP mode by this way, of cause we need more check on both correctness of both batch and cp job. ",
    "commit": "56442dc1c7450518d9bc84b4bfeddb017daa967b",
    "createdAt": "2018-05-15T13:03:58Z",
    "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.lang.management.ManagementFactory\n+import java.nio.ByteBuffer\n+import java.util.Properties\n+\n+import scala.language.existentials\n+\n+import org.apache.spark._\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.shuffle.ShuffleWriter\n+\n+/**\n+ * A ShuffleMapTask divides the elements of an RDD into multiple buckets (based on a partitioner\n+ * specified in the ShuffleDependency).\n+ *\n+ * See [[org.apache.spark.scheduler.Task]] for more information.\n+ *\n+ * @param stageId id of the stage this task belongs to\n+ * @param stageAttemptId attempt id of the stage this task belongs to\n+ * @param taskBinary broadcast version of the RDD and the ShuffleDependency. Once deserialized,\n+ *                   the type should be (RDD[_], ShuffleDependency[_, _, _]).\n+ * @param partition partition of the RDD this task is associated with\n+ * @param locs preferred task execution locations for locality scheduling\n+ * @param localProperties copy of thread-local properties set by the user on the driver side.\n+ * @param serializedTaskMetrics a `TaskMetrics` that is created and serialized on the driver side\n+ *                              and sent to executor side.\n+ * @param totalShuffleNum total shuffle number for current job.\n+ *\n+ * The parameters below are optional:\n+ * @param jobId id of the job this task belongs to\n+ * @param appId id of the app this task belongs to\n+ * @param appAttemptId attempt id of the app this task belongs to\n+ */\n+private[spark] class ContinuousShuffleMapTask(\n+    stageId: Int,\n+    stageAttemptId: Int,\n+    taskBinary: Broadcast[Array[Byte]],\n+    partition: Partition,\n+    @transient private var locs: Seq[TaskLocation],\n+    localProperties: Properties,\n+    serializedTaskMetrics: Array[Byte],\n+    totalShuffleNum: Int,\n+    jobId: Option[Int] = None,\n+    appId: Option[String] = None,\n+    appAttemptId: Option[String] = None)\n+  extends Task[Unit](stageId, stageAttemptId, partition.index, localProperties,\n+    serializedTaskMetrics, jobId, appId, appAttemptId)\n+    with Logging {\n+\n+  /** A constructor used only in test suites. This does not require passing in an RDD. */\n+  def this(partitionId: Int, totalShuffleNum: Int) {\n+    this(0, 0, null, new Partition { override def index: Int = 0 }, null, new Properties,\n+      null, totalShuffleNum)\n+  }\n+\n+  @transient private val preferredLocs: Seq[TaskLocation] = {\n+    if (locs == null) Nil else locs.toSet.toSeq\n+  }\n+\n+  // TODO: Get current epoch from epoch coordinator while task restart, also epoch is Long, we\n+  //       should deal with it.\n+  var currentEpoch = context.getLocalProperty(SparkEnv.START_EPOCH_KEY).toInt\n+\n+  override def runTask(context: TaskContext): Unit = {\n+    // Deserialize the RDD using the broadcast variable.\n+    val threadMXBean = ManagementFactory.getThreadMXBean\n+    val deserializeStartTime = System.currentTimeMillis()\n+    val deserializeStartCpuTime = if (threadMXBean.isCurrentThreadCpuTimeSupported) {\n+      threadMXBean.getCurrentThreadCpuTime\n+    } else 0L\n+    val ser = SparkEnv.get.closureSerializer.newInstance()\n+    // TODO: rdd here should be a wrap of ShuffledRowRDD which never stop\n+    val (rdd, dep) = ser.deserialize[(RDD[_], ShuffleDependency[_, _, _])](\n+      ByteBuffer.wrap(taskBinary.value), Thread.currentThread.getContextClassLoader)\n+    _executorDeserializeTime = System.currentTimeMillis() - deserializeStartTime\n+    _executorDeserializeCpuTime = if (threadMXBean.isCurrentThreadCpuTimeSupported) {\n+      threadMXBean.getCurrentThreadCpuTime - deserializeStartCpuTime\n+    } else 0L",
    "line": 98
  }],
  "prId": 21293
}]