[{
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "hmm, doesn't this need to know whether we've computed the full partition for _all_ the rdds involved in this task, not just the last one?  Eg., what if an earlier rdd is cached, but the final one is not.  Or what if there is a coalesce, so the coalesced partition is not computed fully, but the partitions of the original rdds are computed fully?\n\n:(  kinda worried this is a critical issue ... \n",
    "commit": "59db4af635fd53e2723d187b60dee6856af74df4",
    "createdAt": "2016-03-08T23:16:28Z",
    "diffHunk": "@@ -66,7 +67,10 @@ private[spark] class ResultTask[T, U](\n     _executorDeserializeTime = System.currentTimeMillis() - deserializeStartTime\n \n     metrics = Some(context.taskMetrics)\n-    func(context, rdd.iterator(partition, context))\n+    val itr = rdd.iterator(partition, context)\n+    val result = func(context, itr)\n+    val computedFullPartition = itr.isEmpty || rdd.storageLevel != StorageLevel.NONE"
  }, {
    "author": {
      "login": "holdenk"
    },
    "body": "hmmm thats a good point - I'll move detecting computing the full partition being consumed to be per-RDD.\n",
    "commit": "59db4af635fd53e2723d187b60dee6856af74df4",
    "createdAt": "2016-03-08T23:37:28Z",
    "diffHunk": "@@ -66,7 +67,10 @@ private[spark] class ResultTask[T, U](\n     _executorDeserializeTime = System.currentTimeMillis() - deserializeStartTime\n \n     metrics = Some(context.taskMetrics)\n-    func(context, rdd.iterator(partition, context))\n+    val itr = rdd.iterator(partition, context)\n+    val result = func(context, itr)\n+    val computedFullPartition = itr.isEmpty || rdd.storageLevel != StorageLevel.NONE"
  }, {
    "author": {
      "login": "squito"
    },
    "body": "great.  would also like to see a test case that fails without this being addressed.  Sounds like you know how to workaround this, I was worried it might not be fixable.\n",
    "commit": "59db4af635fd53e2723d187b60dee6856af74df4",
    "createdAt": "2016-03-09T16:03:26Z",
    "diffHunk": "@@ -66,7 +67,10 @@ private[spark] class ResultTask[T, U](\n     _executorDeserializeTime = System.currentTimeMillis() - deserializeStartTime\n \n     metrics = Some(context.taskMetrics)\n-    func(context, rdd.iterator(partition, context))\n+    val itr = rdd.iterator(partition, context)\n+    val result = func(context, itr)\n+    val computedFullPartition = itr.isEmpty || rdd.storageLevel != StorageLevel.NONE"
  }, {
    "author": {
      "login": "holdenk"
    },
    "body": "So I've done an update so we don't depend on this used to not work with the cache + first + count example (but the new way of keeping track of when a partition is fully processed works too).\n",
    "commit": "59db4af635fd53e2723d187b60dee6856af74df4",
    "createdAt": "2016-03-13T09:48:38Z",
    "diffHunk": "@@ -66,7 +67,10 @@ private[spark] class ResultTask[T, U](\n     _executorDeserializeTime = System.currentTimeMillis() - deserializeStartTime\n \n     metrics = Some(context.taskMetrics)\n-    func(context, rdd.iterator(partition, context))\n+    val itr = rdd.iterator(partition, context)\n+    val result = func(context, itr)\n+    val computedFullPartition = itr.isEmpty || rdd.storageLevel != StorageLevel.NONE"
  }],
  "prId": 11105
}]