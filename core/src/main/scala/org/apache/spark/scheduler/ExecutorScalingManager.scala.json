[{
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "Consider calling this DynamicAllocationManager?  \"Executor scaling\" sound to me like making executors more scalable.\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-10T21:37:53Z",
    "diffHunk": "@@ -0,0 +1,324 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.util.{Timer, TimerTask}\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically scales the number of executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks has not\n+ * been drained for N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle, meaning it has not been scheduled\n+ * to run any tasks, for K seconds, then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * The relevant Spark properties include the following:\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *   spark.dynamicAllocation.addExecutorThreshold - How long before new executors are added (N)\n+ *   spark.dynamicAllocation.addExecutorInterval - How often to add new executors (M)\n+ *   spark.dynamicAllocation.removeExecutorThreshold - How long before an executor is removed (K)\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention only arises\n+ * if the application itself runs multiple jobs concurrently. Under normal circumstances, however,\n+ * synchronizing each method on this class should not be expensive assuming biased locking is\n+ * enabled in the JVM (on by default for Java 6+). Tighter locks are also used where possible.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorScalingManager(scheduler: TaskSchedulerImpl) extends Logging {"
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "Though I think we need some notion of `executor` in there. `DynamicExecutorAllocationManager`? `ExecutorAllocationManager`? `DynamicExecutorAllocator`?\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-11T03:22:00Z",
    "diffHunk": "@@ -0,0 +1,324 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.util.{Timer, TimerTask}\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically scales the number of executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks has not\n+ * been drained for N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle, meaning it has not been scheduled\n+ * to run any tasks, for K seconds, then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * The relevant Spark properties include the following:\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *   spark.dynamicAllocation.addExecutorThreshold - How long before new executors are added (N)\n+ *   spark.dynamicAllocation.addExecutorInterval - How often to add new executors (M)\n+ *   spark.dynamicAllocation.removeExecutorThreshold - How long before an executor is removed (K)\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention only arises\n+ * if the application itself runs multiple jobs concurrently. Under normal circumstances, however,\n+ * synchronizing each method on this class should not be expensive assuming biased locking is\n+ * enabled in the JVM (on by default for Java 6+). Tighter locks are also used where possible.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorScalingManager(scheduler: TaskSchedulerImpl) extends Logging {"
  }, {
    "author": {
      "login": "sryza"
    },
    "body": "All of those sound good to me.  The second one if I had to choose.\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-11T18:42:20Z",
    "diffHunk": "@@ -0,0 +1,324 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.util.{Timer, TimerTask}\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically scales the number of executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks has not\n+ * been drained for N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle, meaning it has not been scheduled\n+ * to run any tasks, for K seconds, then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * The relevant Spark properties include the following:\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *   spark.dynamicAllocation.addExecutorThreshold - How long before new executors are added (N)\n+ *   spark.dynamicAllocation.addExecutorInterval - How often to add new executors (M)\n+ *   spark.dynamicAllocation.removeExecutorThreshold - How long before an executor is removed (K)\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention only arises\n+ * if the application itself runs multiple jobs concurrently. Under normal circumstances, however,\n+ * synchronizing each method on this class should not be expensive assuming biased locking is\n+ * enabled in the JVM (on by default for Java 6+). Tighter locks are also used where possible.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorScalingManager(scheduler: TaskSchedulerImpl) extends Logging {"
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "Ok, I think that one sounds pretty reasonable too.\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-14T23:03:29Z",
    "diffHunk": "@@ -0,0 +1,324 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.util.{Timer, TimerTask}\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically scales the number of executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks has not\n+ * been drained for N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle, meaning it has not been scheduled\n+ * to run any tasks, for K seconds, then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * The relevant Spark properties include the following:\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *   spark.dynamicAllocation.addExecutorThreshold - How long before new executors are added (N)\n+ *   spark.dynamicAllocation.addExecutorInterval - How often to add new executors (M)\n+ *   spark.dynamicAllocation.removeExecutorThreshold - How long before an executor is removed (K)\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention only arises\n+ * if the application itself runs multiple jobs concurrently. Under normal circumstances, however,\n+ * synchronizing each method on this class should not be expensive assuming biased locking is\n+ * enabled in the JVM (on by default for Java 6+). Tighter locks are also used where possible.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorScalingManager(scheduler: TaskSchedulerImpl) extends Logging {"
  }],
  "prId": 2746
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "I don't see it documented in the style guidelines, but the convention I've observed for methods with no return types is to omit the \": Unit =\".  Any reason for including it in the methods here?\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-10T21:44:16Z",
    "diffHunk": "@@ -0,0 +1,324 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.util.{Timer, TimerTask}\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically scales the number of executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks has not\n+ * been drained for N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle, meaning it has not been scheduled\n+ * to run any tasks, for K seconds, then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * The relevant Spark properties include the following:\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *   spark.dynamicAllocation.addExecutorThreshold - How long before new executors are added (N)\n+ *   spark.dynamicAllocation.addExecutorInterval - How often to add new executors (M)\n+ *   spark.dynamicAllocation.removeExecutorThreshold - How long before an executor is removed (K)\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention only arises\n+ * if the application itself runs multiple jobs concurrently. Under normal circumstances, however,\n+ * synchronizing each method on this class should not be expensive assuming biased locking is\n+ * enabled in the JVM (on by default for Java 6+). Tighter locks are also used where possible.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorScalingManager(scheduler: TaskSchedulerImpl) extends Logging {\n+  private val conf = scheduler.conf\n+\n+  // Lower and upper bounds on the number of executors. These are required.\n+  private val minNumExecutors = conf.getInt(\"spark.dynamicAllocation.minExecutors\", -1)\n+  private val maxNumExecutors = conf.getInt(\"spark.dynamicAllocation.maxExecutors\", -1)\n+  if (minNumExecutors < 0 || maxNumExecutors < 0) {\n+    throw new SparkException(\"spark.dynamicAllocation.{min/max}Executors must be set!\")\n+  }\n+\n+  // How frequently to add and remove executors\n+  private val addExecutorThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorThreshold\", 60) // s\n+  private val addExecutorInterval =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorInterval\", addExecutorThreshold) // s\n+  private val removeExecutorThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorThreshold\", 300) // s\n+\n+  // Timers that keep track of when to add and remove executors\n+  private var addExecutorTimer: Option[Timer] = None\n+  private val removeExecutorTimers: mutable.Map[String, Timer] = new mutable.HashMap[String, Timer]\n+\n+  // Number of executors to add in the next round\n+  private var numExecutorsToAdd = 1\n+\n+  // The number of pending executors that have not actually been added/removed yet\n+  private var numExecutorsPendingToAdd = 0\n+  private var numExecutorsPendingToRemove = 0\n+\n+  // Keep track of all executors here to decouple us from the logic in TaskSchedulerImpl\n+  private val executorIds = new mutable.HashSet[String] ++= scheduler.executorIdToHost.keys\n+\n+  // Start idle timer for all new executors\n+  synchronized { executorIds.foreach(startRemoveExecutorTimer) }\n+\n+  /**\n+   * Start the add executor timer if it does not already exist.\n+   * This is called when a new pending task is added. The add is then triggered\n+   * if the pending tasks queue is not drained in `addExecutorThreshold` seconds.\n+   */\n+  def startAddExecutorTimer(): Unit = startAddExecutorTimer(addExecutorThreshold)\n+\n+  /**\n+   * Restart the add executor timer.\n+   * This is called when the previous add executor timer has expired but not canceled. The add\n+   * is then triggered again if all pending executors from the previous round have registered,\n+   * and the pending tasks queue is still not drained in `addExecutorInterval` seconds.\n+   */\n+  private def restartAddExecutorTimer(): Unit = startAddExecutorTimer(addExecutorInterval)\n+\n+  /**\n+   * Start the add executor timer using the given delay if the timer does not already exist.\n+   */\n+  private def startAddExecutorTimer(timerDelaySeconds: Long): Unit = {"
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "I'm following the guidelines here http://docs.scala-lang.org/style/declarations.html#procedure-syntax. We've started to enforce this through code reviews in other PRs. Maybe we should clearly document this in the Spark style guide too.\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-11T03:07:36Z",
    "diffHunk": "@@ -0,0 +1,324 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.util.{Timer, TimerTask}\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically scales the number of executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks has not\n+ * been drained for N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle, meaning it has not been scheduled\n+ * to run any tasks, for K seconds, then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * The relevant Spark properties include the following:\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *   spark.dynamicAllocation.addExecutorThreshold - How long before new executors are added (N)\n+ *   spark.dynamicAllocation.addExecutorInterval - How often to add new executors (M)\n+ *   spark.dynamicAllocation.removeExecutorThreshold - How long before an executor is removed (K)\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention only arises\n+ * if the application itself runs multiple jobs concurrently. Under normal circumstances, however,\n+ * synchronizing each method on this class should not be expensive assuming biased locking is\n+ * enabled in the JVM (on by default for Java 6+). Tighter locks are also used where possible.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorScalingManager(scheduler: TaskSchedulerImpl) extends Logging {\n+  private val conf = scheduler.conf\n+\n+  // Lower and upper bounds on the number of executors. These are required.\n+  private val minNumExecutors = conf.getInt(\"spark.dynamicAllocation.minExecutors\", -1)\n+  private val maxNumExecutors = conf.getInt(\"spark.dynamicAllocation.maxExecutors\", -1)\n+  if (minNumExecutors < 0 || maxNumExecutors < 0) {\n+    throw new SparkException(\"spark.dynamicAllocation.{min/max}Executors must be set!\")\n+  }\n+\n+  // How frequently to add and remove executors\n+  private val addExecutorThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorThreshold\", 60) // s\n+  private val addExecutorInterval =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorInterval\", addExecutorThreshold) // s\n+  private val removeExecutorThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorThreshold\", 300) // s\n+\n+  // Timers that keep track of when to add and remove executors\n+  private var addExecutorTimer: Option[Timer] = None\n+  private val removeExecutorTimers: mutable.Map[String, Timer] = new mutable.HashMap[String, Timer]\n+\n+  // Number of executors to add in the next round\n+  private var numExecutorsToAdd = 1\n+\n+  // The number of pending executors that have not actually been added/removed yet\n+  private var numExecutorsPendingToAdd = 0\n+  private var numExecutorsPendingToRemove = 0\n+\n+  // Keep track of all executors here to decouple us from the logic in TaskSchedulerImpl\n+  private val executorIds = new mutable.HashSet[String] ++= scheduler.executorIdToHost.keys\n+\n+  // Start idle timer for all new executors\n+  synchronized { executorIds.foreach(startRemoveExecutorTimer) }\n+\n+  /**\n+   * Start the add executor timer if it does not already exist.\n+   * This is called when a new pending task is added. The add is then triggered\n+   * if the pending tasks queue is not drained in `addExecutorThreshold` seconds.\n+   */\n+  def startAddExecutorTimer(): Unit = startAddExecutorTimer(addExecutorThreshold)\n+\n+  /**\n+   * Restart the add executor timer.\n+   * This is called when the previous add executor timer has expired but not canceled. The add\n+   * is then triggered again if all pending executors from the previous round have registered,\n+   * and the pending tasks queue is still not drained in `addExecutorInterval` seconds.\n+   */\n+  private def restartAddExecutorTimer(): Unit = startAddExecutorTimer(addExecutorInterval)\n+\n+  /**\n+   * Start the add executor timer using the given delay if the timer does not already exist.\n+   */\n+  private def startAddExecutorTimer(timerDelaySeconds: Long): Unit = {"
  }],
  "prId": 2746
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "It would be good to be clear that the executors aren't actually added yet.  Maybe \"Requesting... (new desired total is ...)\"?\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-10T21:50:00Z",
    "diffHunk": "@@ -0,0 +1,324 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.util.{Timer, TimerTask}\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically scales the number of executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks has not\n+ * been drained for N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle, meaning it has not been scheduled\n+ * to run any tasks, for K seconds, then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * The relevant Spark properties include the following:\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *   spark.dynamicAllocation.addExecutorThreshold - How long before new executors are added (N)\n+ *   spark.dynamicAllocation.addExecutorInterval - How often to add new executors (M)\n+ *   spark.dynamicAllocation.removeExecutorThreshold - How long before an executor is removed (K)\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention only arises\n+ * if the application itself runs multiple jobs concurrently. Under normal circumstances, however,\n+ * synchronizing each method on this class should not be expensive assuming biased locking is\n+ * enabled in the JVM (on by default for Java 6+). Tighter locks are also used where possible.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorScalingManager(scheduler: TaskSchedulerImpl) extends Logging {\n+  private val conf = scheduler.conf\n+\n+  // Lower and upper bounds on the number of executors. These are required.\n+  private val minNumExecutors = conf.getInt(\"spark.dynamicAllocation.minExecutors\", -1)\n+  private val maxNumExecutors = conf.getInt(\"spark.dynamicAllocation.maxExecutors\", -1)\n+  if (minNumExecutors < 0 || maxNumExecutors < 0) {\n+    throw new SparkException(\"spark.dynamicAllocation.{min/max}Executors must be set!\")\n+  }\n+\n+  // How frequently to add and remove executors\n+  private val addExecutorThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorThreshold\", 60) // s\n+  private val addExecutorInterval =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorInterval\", addExecutorThreshold) // s\n+  private val removeExecutorThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorThreshold\", 300) // s\n+\n+  // Timers that keep track of when to add and remove executors\n+  private var addExecutorTimer: Option[Timer] = None\n+  private val removeExecutorTimers: mutable.Map[String, Timer] = new mutable.HashMap[String, Timer]\n+\n+  // Number of executors to add in the next round\n+  private var numExecutorsToAdd = 1\n+\n+  // The number of pending executors that have not actually been added/removed yet\n+  private var numExecutorsPendingToAdd = 0\n+  private var numExecutorsPendingToRemove = 0\n+\n+  // Keep track of all executors here to decouple us from the logic in TaskSchedulerImpl\n+  private val executorIds = new mutable.HashSet[String] ++= scheduler.executorIdToHost.keys\n+\n+  // Start idle timer for all new executors\n+  synchronized { executorIds.foreach(startRemoveExecutorTimer) }\n+\n+  /**\n+   * Start the add executor timer if it does not already exist.\n+   * This is called when a new pending task is added. The add is then triggered\n+   * if the pending tasks queue is not drained in `addExecutorThreshold` seconds.\n+   */\n+  def startAddExecutorTimer(): Unit = startAddExecutorTimer(addExecutorThreshold)\n+\n+  /**\n+   * Restart the add executor timer.\n+   * This is called when the previous add executor timer has expired but not canceled. The add\n+   * is then triggered again if all pending executors from the previous round have registered,\n+   * and the pending tasks queue is still not drained in `addExecutorInterval` seconds.\n+   */\n+  private def restartAddExecutorTimer(): Unit = startAddExecutorTimer(addExecutorInterval)\n+\n+  /**\n+   * Start the add executor timer using the given delay if the timer does not already exist.\n+   */\n+  private def startAddExecutorTimer(timerDelaySeconds: Long): Unit = {\n+    addExecutorTimer.synchronized {\n+      if (addExecutorTimer.isEmpty) {\n+        logDebug(s\"Starting add executor timer (to expire in $timerDelaySeconds seconds)\")\n+        addExecutorTimer = Some(new Timer)\n+        addExecutorTimer.get.schedule(\n+          new AddExecutorTimerTask(numExecutorsToAdd), timerDelaySeconds * 1000)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Start a timer to remove the given executor if the timer does not already exist.\n+   * This is called when the executor initially registers with the driver or finishes running\n+   * a task. The removal is then triggered if the executor stays idle (i.e. not running a task)\n+   * for `removeExecutorThreshold` seconds.\n+   */\n+  def startRemoveExecutorTimer(executorId: String): Unit = {\n+    removeExecutorTimers.synchronized {\n+      if (!removeExecutorTimers.contains(executorId)) {\n+        logDebug(s\"Starting idle timer for executor $executorId \" +\n+          s\"(to expire in $removeExecutorThreshold seconds)\")\n+        removeExecutorTimers(executorId) = new Timer\n+        removeExecutorTimers(executorId).schedule(\n+          new RemoveExecutorTimerTask(executorId), removeExecutorThreshold * 1000)\n+      }\n+    }\n+    // Acquire a more general lock here because we might mutate `executorId`\n+    synchronized {\n+      if (!executorIds.contains(executorId)) {\n+        logWarning(s\"Started idle timer for unknown executor $executorId.\")\n+        executorIds.add(executorId)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Cancel any existing timer that adds executors.\n+   * This is called when the pending task queue is drained.\n+   */\n+  def cancelAddExecutorTimer(): Unit = addExecutorTimer.synchronized {\n+    addExecutorTimer.foreach { timer =>\n+      logDebug(\"Canceling add executor timer because task queue is drained!\")\n+      timer.cancel()\n+      numExecutorsToAdd = 1\n+      addExecutorTimer = None\n+    }\n+  }\n+\n+  /**\n+   * Cancel any existing timer that removes the given executor.\n+   * This is called when the executor is no longer idle.\n+   */\n+  def cancelRemoveExecutorTimer(executorId: String): Unit = removeExecutorTimers.synchronized {\n+    if (removeExecutorTimers.contains(executorId)) {\n+      logDebug(s\"Canceling idle timer for executor $executorId.\")\n+      removeExecutorTimers(executorId).cancel()\n+      removeExecutorTimers.remove(executorId)\n+    }\n+  }\n+\n+  /**\n+   * Negotiate with the scheduler backend to add new executors.\n+   * This ensures the resulting number of executors is correctly constrained by the upper bound.\n+   * Return the number of executors actually requested.\n+   */\n+  private def addExecutors(numExecutorsRequested: Int): Int = synchronized {\n+    val numExistingExecutors = executorIds.size + numExecutorsPendingToAdd\n+    val numExecutorsToAdd =\n+      if (numExistingExecutors + numExecutorsRequested <= maxNumExecutors) {\n+        numExecutorsRequested\n+      } else {\n+        // Add just enough to reach `maxNumExecutors`\n+        maxNumExecutors - numExistingExecutors\n+      }\n+    val newNumExecutors = numExistingExecutors + numExecutorsToAdd\n+\n+    if (numExecutorsToAdd > 0) {\n+      getCoarseGrainedBackend.foreach { backend =>\n+        logInfo(s\"Pending tasks are building up! \" +\n+          s\"Adding $numExecutorsToAdd new executor(s) (new total is $newNumExecutors).\")"
  }],
  "prId": 2746
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "This is a little bit cryptic\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-10T21:50:39Z",
    "diffHunk": "@@ -0,0 +1,324 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.util.{Timer, TimerTask}\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically scales the number of executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks has not\n+ * been drained for N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle, meaning it has not been scheduled\n+ * to run any tasks, for K seconds, then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * The relevant Spark properties include the following:\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *   spark.dynamicAllocation.addExecutorThreshold - How long before new executors are added (N)\n+ *   spark.dynamicAllocation.addExecutorInterval - How often to add new executors (M)\n+ *   spark.dynamicAllocation.removeExecutorThreshold - How long before an executor is removed (K)\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention only arises\n+ * if the application itself runs multiple jobs concurrently. Under normal circumstances, however,\n+ * synchronizing each method on this class should not be expensive assuming biased locking is\n+ * enabled in the JVM (on by default for Java 6+). Tighter locks are also used where possible.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorScalingManager(scheduler: TaskSchedulerImpl) extends Logging {\n+  private val conf = scheduler.conf\n+\n+  // Lower and upper bounds on the number of executors. These are required.\n+  private val minNumExecutors = conf.getInt(\"spark.dynamicAllocation.minExecutors\", -1)\n+  private val maxNumExecutors = conf.getInt(\"spark.dynamicAllocation.maxExecutors\", -1)\n+  if (minNumExecutors < 0 || maxNumExecutors < 0) {\n+    throw new SparkException(\"spark.dynamicAllocation.{min/max}Executors must be set!\")\n+  }\n+\n+  // How frequently to add and remove executors\n+  private val addExecutorThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorThreshold\", 60) // s\n+  private val addExecutorInterval =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorInterval\", addExecutorThreshold) // s\n+  private val removeExecutorThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorThreshold\", 300) // s\n+\n+  // Timers that keep track of when to add and remove executors\n+  private var addExecutorTimer: Option[Timer] = None\n+  private val removeExecutorTimers: mutable.Map[String, Timer] = new mutable.HashMap[String, Timer]\n+\n+  // Number of executors to add in the next round\n+  private var numExecutorsToAdd = 1\n+\n+  // The number of pending executors that have not actually been added/removed yet\n+  private var numExecutorsPendingToAdd = 0\n+  private var numExecutorsPendingToRemove = 0\n+\n+  // Keep track of all executors here to decouple us from the logic in TaskSchedulerImpl\n+  private val executorIds = new mutable.HashSet[String] ++= scheduler.executorIdToHost.keys\n+\n+  // Start idle timer for all new executors\n+  synchronized { executorIds.foreach(startRemoveExecutorTimer) }\n+\n+  /**\n+   * Start the add executor timer if it does not already exist.\n+   * This is called when a new pending task is added. The add is then triggered\n+   * if the pending tasks queue is not drained in `addExecutorThreshold` seconds.\n+   */\n+  def startAddExecutorTimer(): Unit = startAddExecutorTimer(addExecutorThreshold)\n+\n+  /**\n+   * Restart the add executor timer.\n+   * This is called when the previous add executor timer has expired but not canceled. The add\n+   * is then triggered again if all pending executors from the previous round have registered,\n+   * and the pending tasks queue is still not drained in `addExecutorInterval` seconds.\n+   */\n+  private def restartAddExecutorTimer(): Unit = startAddExecutorTimer(addExecutorInterval)\n+\n+  /**\n+   * Start the add executor timer using the given delay if the timer does not already exist.\n+   */\n+  private def startAddExecutorTimer(timerDelaySeconds: Long): Unit = {\n+    addExecutorTimer.synchronized {\n+      if (addExecutorTimer.isEmpty) {\n+        logDebug(s\"Starting add executor timer (to expire in $timerDelaySeconds seconds)\")\n+        addExecutorTimer = Some(new Timer)\n+        addExecutorTimer.get.schedule(\n+          new AddExecutorTimerTask(numExecutorsToAdd), timerDelaySeconds * 1000)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Start a timer to remove the given executor if the timer does not already exist.\n+   * This is called when the executor initially registers with the driver or finishes running\n+   * a task. The removal is then triggered if the executor stays idle (i.e. not running a task)\n+   * for `removeExecutorThreshold` seconds.\n+   */\n+  def startRemoveExecutorTimer(executorId: String): Unit = {\n+    removeExecutorTimers.synchronized {\n+      if (!removeExecutorTimers.contains(executorId)) {\n+        logDebug(s\"Starting idle timer for executor $executorId \" +\n+          s\"(to expire in $removeExecutorThreshold seconds)\")\n+        removeExecutorTimers(executorId) = new Timer\n+        removeExecutorTimers(executorId).schedule(\n+          new RemoveExecutorTimerTask(executorId), removeExecutorThreshold * 1000)\n+      }\n+    }\n+    // Acquire a more general lock here because we might mutate `executorId`\n+    synchronized {\n+      if (!executorIds.contains(executorId)) {\n+        logWarning(s\"Started idle timer for unknown executor $executorId.\")\n+        executorIds.add(executorId)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Cancel any existing timer that adds executors.\n+   * This is called when the pending task queue is drained.\n+   */\n+  def cancelAddExecutorTimer(): Unit = addExecutorTimer.synchronized {\n+    addExecutorTimer.foreach { timer =>\n+      logDebug(\"Canceling add executor timer because task queue is drained!\")\n+      timer.cancel()\n+      numExecutorsToAdd = 1\n+      addExecutorTimer = None\n+    }\n+  }\n+\n+  /**\n+   * Cancel any existing timer that removes the given executor.\n+   * This is called when the executor is no longer idle.\n+   */\n+  def cancelRemoveExecutorTimer(executorId: String): Unit = removeExecutorTimers.synchronized {\n+    if (removeExecutorTimers.contains(executorId)) {\n+      logDebug(s\"Canceling idle timer for executor $executorId.\")\n+      removeExecutorTimers(executorId).cancel()\n+      removeExecutorTimers.remove(executorId)\n+    }\n+  }\n+\n+  /**\n+   * Negotiate with the scheduler backend to add new executors.\n+   * This ensures the resulting number of executors is correctly constrained by the upper bound.\n+   * Return the number of executors actually requested.\n+   */\n+  private def addExecutors(numExecutorsRequested: Int): Int = synchronized {\n+    val numExistingExecutors = executorIds.size + numExecutorsPendingToAdd\n+    val numExecutorsToAdd =\n+      if (numExistingExecutors + numExecutorsRequested <= maxNumExecutors) {\n+        numExecutorsRequested\n+      } else {\n+        // Add just enough to reach `maxNumExecutors`\n+        maxNumExecutors - numExistingExecutors\n+      }\n+    val newNumExecutors = numExistingExecutors + numExecutorsToAdd\n+\n+    if (numExecutorsToAdd > 0) {\n+      getCoarseGrainedBackend.foreach { backend =>\n+        logInfo(s\"Pending tasks are building up! \" +\n+          s\"Adding $numExecutorsToAdd new executor(s) (new total is $newNumExecutors).\")\n+        numExecutorsPendingToAdd += numExecutorsToAdd\n+        backend.requestExecutors(numExecutorsToAdd)\n+        return numExecutorsToAdd\n+      }\n+    } else {\n+      logDebug(s\"Not adding executors because there are already $maxNumExecutors executors, \" +\n+        s\"which is the limit.\")\n+    }\n+    0\n+  }\n+\n+  /**\n+   * Negotiate with the scheduler backend to remove existing executors.\n+   * This ensures the resulting number of executors is correctly constrained by the lower bound.\n+   * Return whether the request to remove the executor is actually sent.\n+   */\n+  private def removeExecutor(executorId: String): Boolean = synchronized {\n+    val numExistingExecutors = executorIds.size - numExecutorsPendingToRemove\n+    if (numExistingExecutors - 1 >= minNumExecutors) {\n+      getCoarseGrainedBackend.foreach { backend =>\n+        logInfo(s\"Removing executor $executorId because it has been idle for \" +\n+          s\"$removeExecutorThreshold seconds (new total is ${numExistingExecutors - 1}).\")\n+        numExecutorsPendingToRemove += 1\n+        backend.killExecutor(executorId)\n+        return true\n+      }\n+    } else {\n+      logDebug(s\"Not removing idle executor $executorId because there are only $minNumExecutors \" +\n+        \"executor(s) left, which is the limit.\")\n+    }\n+    false\n+  }\n+\n+  /**\n+   * Callback for the scheduler to signal that the given executor has been added.\n+   */\n+  def executorAdded(executorId: String): Unit = synchronized {\n+    if (!executorIds.contains(executorId)) {\n+      logDebug(s\"New executor $executorId has registered.\")\n+      if (numExecutorsPendingToAdd > 0) {\n+        numExecutorsPendingToAdd -= 1\n+        logDebug(s\"Decrementing pending executors to add (now at $numExecutorsPendingToAdd).\")\n+      }\n+      executorIds.add(executorId)\n+      startRemoveExecutorTimer(executorId)\n+    }\n+  }\n+\n+  /**\n+   * Callback for the scheduler to signal that the given executor has been removed.\n+   */\n+  def executorRemoved(executorId: String): Unit = synchronized {\n+    if (executorIds.contains(executorId)) {\n+      logDebug(s\"Existing executor $executorId has been removed.\")\n+      executorIds.remove(executorId)\n+      if (numExecutorsPendingToRemove > 0) {\n+        numExecutorsPendingToRemove -= 1\n+        logDebug(s\"Decrementing pending executors to remove (now at $numExecutorsPendingToRemove).\")\n+      }\n+    } else {\n+      logWarning(s\"Not removing unknown executor $executorId\")"
  }],
  "prId": 2746
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "Nit: take out these curly braces?\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-10T21:51:50Z",
    "diffHunk": "@@ -0,0 +1,324 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.util.{Timer, TimerTask}\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically scales the number of executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks has not\n+ * been drained for N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle, meaning it has not been scheduled\n+ * to run any tasks, for K seconds, then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * The relevant Spark properties include the following:\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *   spark.dynamicAllocation.addExecutorThreshold - How long before new executors are added (N)\n+ *   spark.dynamicAllocation.addExecutorInterval - How often to add new executors (M)\n+ *   spark.dynamicAllocation.removeExecutorThreshold - How long before an executor is removed (K)\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention only arises\n+ * if the application itself runs multiple jobs concurrently. Under normal circumstances, however,\n+ * synchronizing each method on this class should not be expensive assuming biased locking is\n+ * enabled in the JVM (on by default for Java 6+). Tighter locks are also used where possible.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorScalingManager(scheduler: TaskSchedulerImpl) extends Logging {\n+  private val conf = scheduler.conf\n+\n+  // Lower and upper bounds on the number of executors. These are required.\n+  private val minNumExecutors = conf.getInt(\"spark.dynamicAllocation.minExecutors\", -1)\n+  private val maxNumExecutors = conf.getInt(\"spark.dynamicAllocation.maxExecutors\", -1)\n+  if (minNumExecutors < 0 || maxNumExecutors < 0) {\n+    throw new SparkException(\"spark.dynamicAllocation.{min/max}Executors must be set!\")\n+  }\n+\n+  // How frequently to add and remove executors\n+  private val addExecutorThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorThreshold\", 60) // s\n+  private val addExecutorInterval =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorInterval\", addExecutorThreshold) // s\n+  private val removeExecutorThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorThreshold\", 300) // s\n+\n+  // Timers that keep track of when to add and remove executors\n+  private var addExecutorTimer: Option[Timer] = None\n+  private val removeExecutorTimers: mutable.Map[String, Timer] = new mutable.HashMap[String, Timer]\n+\n+  // Number of executors to add in the next round\n+  private var numExecutorsToAdd = 1\n+\n+  // The number of pending executors that have not actually been added/removed yet\n+  private var numExecutorsPendingToAdd = 0\n+  private var numExecutorsPendingToRemove = 0\n+\n+  // Keep track of all executors here to decouple us from the logic in TaskSchedulerImpl\n+  private val executorIds = new mutable.HashSet[String] ++= scheduler.executorIdToHost.keys\n+\n+  // Start idle timer for all new executors\n+  synchronized { executorIds.foreach(startRemoveExecutorTimer) }\n+\n+  /**\n+   * Start the add executor timer if it does not already exist.\n+   * This is called when a new pending task is added. The add is then triggered\n+   * if the pending tasks queue is not drained in `addExecutorThreshold` seconds.\n+   */\n+  def startAddExecutorTimer(): Unit = startAddExecutorTimer(addExecutorThreshold)\n+\n+  /**\n+   * Restart the add executor timer.\n+   * This is called when the previous add executor timer has expired but not canceled. The add\n+   * is then triggered again if all pending executors from the previous round have registered,\n+   * and the pending tasks queue is still not drained in `addExecutorInterval` seconds.\n+   */\n+  private def restartAddExecutorTimer(): Unit = startAddExecutorTimer(addExecutorInterval)\n+\n+  /**\n+   * Start the add executor timer using the given delay if the timer does not already exist.\n+   */\n+  private def startAddExecutorTimer(timerDelaySeconds: Long): Unit = {\n+    addExecutorTimer.synchronized {\n+      if (addExecutorTimer.isEmpty) {\n+        logDebug(s\"Starting add executor timer (to expire in $timerDelaySeconds seconds)\")\n+        addExecutorTimer = Some(new Timer)\n+        addExecutorTimer.get.schedule(\n+          new AddExecutorTimerTask(numExecutorsToAdd), timerDelaySeconds * 1000)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Start a timer to remove the given executor if the timer does not already exist.\n+   * This is called when the executor initially registers with the driver or finishes running\n+   * a task. The removal is then triggered if the executor stays idle (i.e. not running a task)\n+   * for `removeExecutorThreshold` seconds.\n+   */\n+  def startRemoveExecutorTimer(executorId: String): Unit = {\n+    removeExecutorTimers.synchronized {\n+      if (!removeExecutorTimers.contains(executorId)) {\n+        logDebug(s\"Starting idle timer for executor $executorId \" +\n+          s\"(to expire in $removeExecutorThreshold seconds)\")\n+        removeExecutorTimers(executorId) = new Timer\n+        removeExecutorTimers(executorId).schedule(\n+          new RemoveExecutorTimerTask(executorId), removeExecutorThreshold * 1000)\n+      }\n+    }\n+    // Acquire a more general lock here because we might mutate `executorId`\n+    synchronized {\n+      if (!executorIds.contains(executorId)) {\n+        logWarning(s\"Started idle timer for unknown executor $executorId.\")\n+        executorIds.add(executorId)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Cancel any existing timer that adds executors.\n+   * This is called when the pending task queue is drained.\n+   */\n+  def cancelAddExecutorTimer(): Unit = addExecutorTimer.synchronized {\n+    addExecutorTimer.foreach { timer =>\n+      logDebug(\"Canceling add executor timer because task queue is drained!\")\n+      timer.cancel()\n+      numExecutorsToAdd = 1\n+      addExecutorTimer = None\n+    }\n+  }\n+\n+  /**\n+   * Cancel any existing timer that removes the given executor.\n+   * This is called when the executor is no longer idle.\n+   */\n+  def cancelRemoveExecutorTimer(executorId: String): Unit = removeExecutorTimers.synchronized {\n+    if (removeExecutorTimers.contains(executorId)) {\n+      logDebug(s\"Canceling idle timer for executor $executorId.\")\n+      removeExecutorTimers(executorId).cancel()\n+      removeExecutorTimers.remove(executorId)\n+    }\n+  }\n+\n+  /**\n+   * Negotiate with the scheduler backend to add new executors.\n+   * This ensures the resulting number of executors is correctly constrained by the upper bound.\n+   * Return the number of executors actually requested.\n+   */\n+  private def addExecutors(numExecutorsRequested: Int): Int = synchronized {\n+    val numExistingExecutors = executorIds.size + numExecutorsPendingToAdd\n+    val numExecutorsToAdd =\n+      if (numExistingExecutors + numExecutorsRequested <= maxNumExecutors) {\n+        numExecutorsRequested\n+      } else {\n+        // Add just enough to reach `maxNumExecutors`\n+        maxNumExecutors - numExistingExecutors\n+      }\n+    val newNumExecutors = numExistingExecutors + numExecutorsToAdd\n+\n+    if (numExecutorsToAdd > 0) {\n+      getCoarseGrainedBackend.foreach { backend =>\n+        logInfo(s\"Pending tasks are building up! \" +\n+          s\"Adding $numExecutorsToAdd new executor(s) (new total is $newNumExecutors).\")\n+        numExecutorsPendingToAdd += numExecutorsToAdd\n+        backend.requestExecutors(numExecutorsToAdd)\n+        return numExecutorsToAdd\n+      }\n+    } else {\n+      logDebug(s\"Not adding executors because there are already $maxNumExecutors executors, \" +\n+        s\"which is the limit.\")\n+    }\n+    0\n+  }\n+\n+  /**\n+   * Negotiate with the scheduler backend to remove existing executors.\n+   * This ensures the resulting number of executors is correctly constrained by the lower bound.\n+   * Return whether the request to remove the executor is actually sent.\n+   */\n+  private def removeExecutor(executorId: String): Boolean = synchronized {\n+    val numExistingExecutors = executorIds.size - numExecutorsPendingToRemove\n+    if (numExistingExecutors - 1 >= minNumExecutors) {\n+      getCoarseGrainedBackend.foreach { backend =>\n+        logInfo(s\"Removing executor $executorId because it has been idle for \" +\n+          s\"$removeExecutorThreshold seconds (new total is ${numExistingExecutors - 1}).\")\n+        numExecutorsPendingToRemove += 1\n+        backend.killExecutor(executorId)\n+        return true\n+      }\n+    } else {\n+      logDebug(s\"Not removing idle executor $executorId because there are only $minNumExecutors \" +\n+        \"executor(s) left, which is the limit.\")\n+    }\n+    false\n+  }\n+\n+  /**\n+   * Callback for the scheduler to signal that the given executor has been added.\n+   */\n+  def executorAdded(executorId: String): Unit = synchronized {\n+    if (!executorIds.contains(executorId)) {\n+      logDebug(s\"New executor $executorId has registered.\")\n+      if (numExecutorsPendingToAdd > 0) {\n+        numExecutorsPendingToAdd -= 1\n+        logDebug(s\"Decrementing pending executors to add (now at $numExecutorsPendingToAdd).\")\n+      }\n+      executorIds.add(executorId)\n+      startRemoveExecutorTimer(executorId)\n+    }\n+  }\n+\n+  /**\n+   * Callback for the scheduler to signal that the given executor has been removed.\n+   */\n+  def executorRemoved(executorId: String): Unit = synchronized {\n+    if (executorIds.contains(executorId)) {\n+      logDebug(s\"Existing executor $executorId has been removed.\")\n+      executorIds.remove(executorId)\n+      if (numExecutorsPendingToRemove > 0) {\n+        numExecutorsPendingToRemove -= 1\n+        logDebug(s\"Decrementing pending executors to remove (now at $numExecutorsPendingToRemove).\")\n+      }\n+    } else {\n+      logWarning(s\"Not removing unknown executor $executorId\")\n+    }\n+  }\n+\n+  /**\n+   * Return the backend as a CoarseGrainedSchedulerBackend if possible.\n+   * Otherwise, guard against the use of this feature either before the backend has initialized,\n+   * or because the scheduler is running in fine-grained mode. In the latter case, the executors\n+   * are already dynamically allocated by definition, so an appropriate exception is thrown.\n+   */\n+  private def getCoarseGrainedBackend: Option[CoarseGrainedSchedulerBackend] = {\n+    scheduler.backend match {\n+      case b: CoarseGrainedSchedulerBackend => Some(b)\n+      case null =>\n+        logWarning(\"Scheduler backend not initialized yet for dynamically scaling executors!\")\n+        None\n+      case _ =>\n+        throw new SparkException(\"Dynamic allocation of executors is not applicable to \" +\n+          \"fine-grained schedulers. Please set spark.dynamicAllocation.enabled to false.\")\n+    }\n+  }\n+\n+  /**\n+   * A timer task that adds the given number of executors.\n+   *\n+   * This task does not request new executors until the ones pending from the previous round have\n+   * all registered. Then, if the number of executors requested is as expected (i.e. the upper\n+   * bound is not reached), the number to request next round increases exponentially. Finally,\n+   * after requesting executors, this restarts the add executor timer unless the timer is canceled.\n+   */\n+  private class AddExecutorTimerTask(_numExecutorsToAdd: Int) extends TimerTask {\n+    override def run(): Unit = {\n+      // Whether we have successfully requested the expected number of executors\n+      var success = false\n+\n+      synchronized {\n+        // Do not add executors until those requested in the previous round have registered\n+        if (numExecutorsPendingToAdd == 0) {\n+          val numExecutorsAdded = addExecutors(_numExecutorsToAdd)\n+          success = numExecutorsAdded == _numExecutorsToAdd\n+        } else {\n+          logInfo(s\"Not adding new executors until all $numExecutorsPendingToAdd pending \" +\n+            \"executor(s) have registered.\")\n+        }\n+      }\n+\n+      addExecutorTimer.synchronized {\n+        // Do this check in case the timer has been canceled in the mean time\n+        if (addExecutorTimer.isDefined) {\n+          numExecutorsToAdd = if (success) { _numExecutorsToAdd * 2 } else 1"
  }],
  "prId": 2746
}]