[{
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "A lot of the logic in here seems related to retries. Why do we have to deal with retrying at all? What if we just do individual executor requests one at a time and we don't do any dynamic scaling until we get back the result of the last request? Could this simplify a lot of complexity here? My understanding is that YARN etc should be able to launch executors in order seconds, so we could add ten or more executors per minute... that seems pretty reasonable for the type of workloads this is targeting.\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-15T17:33:53Z",
    "diffHunk": "@@ -0,0 +1,496 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically allocates and removes executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks is not\n+ * drained in N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle for K seconds (meaning it has not\n+ * been scheduled to run any tasks), then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * Both add and remove attempts are retried on failure up to a maximum number of times.\n+ *\n+ * The relevant Spark properties include the following:\n+ *\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *\n+ *   spark.dynamicAllocation.addExecutorThreshold - How long before new executors are added (N)\n+ *   spark.dynamicAllocation.addExecutorInterval - How often to add new executors (M)\n+ *   spark.dynamicAllocation.removeExecutorThreshold - How long before an executor is removed (K)\n+ *\n+ *   spark.dynamicAllocation.addExecutorRetryInterval - How often to retry adding executors"
  }],
  "prId": 2746
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "just so I understand - are the semantics of this that once we hit a state of continued backlog (e.g. we are adding executors, we will continue to add executors every M seconds)?\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-15T18:29:31Z",
    "diffHunk": "@@ -0,0 +1,496 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically allocates and removes executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks is not\n+ * drained in N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle for K seconds (meaning it has not\n+ * been scheduled to run any tasks), then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * Both add and remove attempts are retried on failure up to a maximum number of times.\n+ *\n+ * The relevant Spark properties include the following:\n+ *\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *\n+ *   spark.dynamicAllocation.addExecutorThreshold - How long before new executors are added (N)\n+ *   spark.dynamicAllocation.addExecutorInterval - How often to add new executors (M)"
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "Yup. First N seconds (the threshold), then M seconds (the interval) every round thereafter.\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-15T21:05:52Z",
    "diffHunk": "@@ -0,0 +1,496 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically allocates and removes executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks is not\n+ * drained in N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle for K seconds (meaning it has not\n+ * been scheduled to run any tasks), then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * Both add and remove attempts are retried on failure up to a maximum number of times.\n+ *\n+ * The relevant Spark properties include the following:\n+ *\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *\n+ *   spark.dynamicAllocation.addExecutorThreshold - How long before new executors are added (N)\n+ *   spark.dynamicAllocation.addExecutorInterval - How often to add new executors (M)"
  }],
  "prId": 2746
}, {
  "comments": [{
    "author": {
      "login": "kayousterhout"
    },
    "body": "The number of configuration options here is a little scary.  Do we need all of these things to be configurable?  The first 3 seem like things we definitely need.  Some of the remaining things seem very tied to how long it takes a new executor to start up, and I wonder if we could hardcode them for now, and make them configurable later only if people find the defaults to be problematic.\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-15T20:34:40Z",
    "diffHunk": "@@ -0,0 +1,496 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically allocates and removes executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks is not\n+ * drained in N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle for K seconds (meaning it has not\n+ * been scheduled to run any tasks), then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * Both add and remove attempts are retried on failure up to a maximum number of times.\n+ *\n+ * The relevant Spark properties include the following:\n+ *"
  }],
  "prId": 2746
}, {
  "comments": [{
    "author": {
      "login": "kayousterhout"
    },
    "body": "This is a nit but is it too annoying to just add the unit to the parameter name (e.g., spark.dynamicAllocation.addExecutorThresholdSeconds\")? The extra time to type \"seconds\" seems significantly less than the time to lookup what the appropriate units are.\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-15T20:36:43Z",
    "diffHunk": "@@ -0,0 +1,496 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically allocates and removes executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks is not\n+ * drained in N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle for K seconds (meaning it has not\n+ * been scheduled to run any tasks), then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * Both add and remove attempts are retried on failure up to a maximum number of times.\n+ *\n+ * The relevant Spark properties include the following:\n+ *\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *\n+ *   spark.dynamicAllocation.addExecutorThreshold - How long before new executors are added (N)\n+ *   spark.dynamicAllocation.addExecutorInterval - How often to add new executors (M)\n+ *   spark.dynamicAllocation.removeExecutorThreshold - How long before an executor is removed (K)\n+ *\n+ *   spark.dynamicAllocation.addExecutorRetryInterval - How often to retry adding executors\n+ *   spark.dynamicAllocation.removeExecutorRetryInterval - How often to retry removing executors\n+ *   spark.dynamicAllocation.maxAddExecutorRetryAttempts - Max retries in re-adding executors\n+ *   spark.dynamicAllocation.maxRemoveExecutorRetryAttempts - Max retries in re-removing executors\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention should only\n+ * arise when new executors register or when existing executors have been removed, both of which\n+ * are relatively rare events with respect to task scheduling. Thus, synchronizing each method on\n+ * the same lock should not be expensive assuming biased locking is enabled in the JVM (on by\n+ * default for Java 6+). This may not be true, however, if the application itself runs multiple\n+ * jobs concurrently.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorAllocationManager(scheduler: TaskSchedulerImpl) extends Logging {\n+  private val conf = scheduler.conf\n+\n+  // Lower and upper bounds on the number of executors. These are required.\n+  private val minNumExecutors = conf.getInt(\"spark.dynamicAllocation.minExecutors\", -1)\n+  private val maxNumExecutors = conf.getInt(\"spark.dynamicAllocation.maxExecutors\", -1)\n+  if (minNumExecutors < 0 || maxNumExecutors < 0) {\n+    throw new SparkException(\"spark.dynamicAllocation.{min/max}Executors must be set!\")\n+  }\n+\n+  // How frequently to add and remove executors (seconds)\n+  private val addThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorThreshold\", 60)"
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "Yeah this is a broader issue I have filed at https://issues.apache.org/jira/browse/SPARK-3859. I thought about adding `Seconds` to the end but it makes the name really long. I'm still on the fence about that.\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-15T20:58:59Z",
    "diffHunk": "@@ -0,0 +1,496 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically allocates and removes executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks is not\n+ * drained in N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle for K seconds (meaning it has not\n+ * been scheduled to run any tasks), then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * Both add and remove attempts are retried on failure up to a maximum number of times.\n+ *\n+ * The relevant Spark properties include the following:\n+ *\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *\n+ *   spark.dynamicAllocation.addExecutorThreshold - How long before new executors are added (N)\n+ *   spark.dynamicAllocation.addExecutorInterval - How often to add new executors (M)\n+ *   spark.dynamicAllocation.removeExecutorThreshold - How long before an executor is removed (K)\n+ *\n+ *   spark.dynamicAllocation.addExecutorRetryInterval - How often to retry adding executors\n+ *   spark.dynamicAllocation.removeExecutorRetryInterval - How often to retry removing executors\n+ *   spark.dynamicAllocation.maxAddExecutorRetryAttempts - Max retries in re-adding executors\n+ *   spark.dynamicAllocation.maxRemoveExecutorRetryAttempts - Max retries in re-removing executors\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention should only\n+ * arise when new executors register or when existing executors have been removed, both of which\n+ * are relatively rare events with respect to task scheduling. Thus, synchronizing each method on\n+ * the same lock should not be expensive assuming biased locking is enabled in the JVM (on by\n+ * default for Java 6+). This may not be true, however, if the application itself runs multiple\n+ * jobs concurrently.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorAllocationManager(scheduler: TaskSchedulerImpl) extends Logging {\n+  private val conf = scheduler.conf\n+\n+  // Lower and upper bounds on the number of executors. These are required.\n+  private val minNumExecutors = conf.getInt(\"spark.dynamicAllocation.minExecutors\", -1)\n+  private val maxNumExecutors = conf.getInt(\"spark.dynamicAllocation.maxExecutors\", -1)\n+  if (minNumExecutors < 0 || maxNumExecutors < 0) {\n+    throw new SparkException(\"spark.dynamicAllocation.{min/max}Executors must be set!\")\n+  }\n+\n+  // How frequently to add and remove executors (seconds)\n+  private val addThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorThreshold\", 60)"
  }, {
    "author": {
      "login": "kayousterhout"
    },
    "body": "I'd go for it, especially after reading more of the code for this, since\nyou sometimes use millis and sometimes use seconds\n\nOn Wed, Oct 15, 2014 at 1:59 PM, andrewor14 notifications@github.com\nwrote:\n\n> In\n> core/src/main/scala/org/apache/spark/scheduler/ExecutorAllocationManager.scala:\n> \n> > - \\* request to add or remove executors. The mechanism to actually do this will be added separately,\n> > - \\* e.g. in SPARK-3822 for Yarn.\n> > - */\n> >   +private[scheduler] class ExecutorAllocationManager(scheduler: TaskSchedulerImpl) extends Logging {\n> > -  private val conf = scheduler.conf\n> >   +\n> > -  // Lower and upper bounds on the number of executors. These are required.\n> > -  private val minNumExecutors = conf.getInt(\"spark.dynamicAllocation.minExecutors\", -1)\n> > -  private val maxNumExecutors = conf.getInt(\"spark.dynamicAllocation.maxExecutors\", -1)\n> > -  if (minNumExecutors < 0 || maxNumExecutors < 0) {\n> > -    throw new SparkException(\"spark.dynamicAllocation.{min/max}Executors must be set!\")\n> > -  }\n> >   +\n> > -  // How frequently to add and remove executors (seconds)\n> > -  private val addThreshold =\n> > -    conf.getLong(\"spark.dynamicAllocation.addExecutorThreshold\", 60)\n> \n> Yeah this is a broader issue I have filed at\n> https://issues.apache.org/jira/browse/SPARK-3859. I thought about adding\n> Seconds to the end but it makes the name really long. I'm still on the\n> fence about that.\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/spark/pull/2746/files#r18922277.\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-15T21:00:31Z",
    "diffHunk": "@@ -0,0 +1,496 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically allocates and removes executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks is not\n+ * drained in N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle for K seconds (meaning it has not\n+ * been scheduled to run any tasks), then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * Both add and remove attempts are retried on failure up to a maximum number of times.\n+ *\n+ * The relevant Spark properties include the following:\n+ *\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *\n+ *   spark.dynamicAllocation.addExecutorThreshold - How long before new executors are added (N)\n+ *   spark.dynamicAllocation.addExecutorInterval - How often to add new executors (M)\n+ *   spark.dynamicAllocation.removeExecutorThreshold - How long before an executor is removed (K)\n+ *\n+ *   spark.dynamicAllocation.addExecutorRetryInterval - How often to retry adding executors\n+ *   spark.dynamicAllocation.removeExecutorRetryInterval - How often to retry removing executors\n+ *   spark.dynamicAllocation.maxAddExecutorRetryAttempts - Max retries in re-adding executors\n+ *   spark.dynamicAllocation.maxRemoveExecutorRetryAttempts - Max retries in re-removing executors\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention should only\n+ * arise when new executors register or when existing executors have been removed, both of which\n+ * are relatively rare events with respect to task scheduling. Thus, synchronizing each method on\n+ * the same lock should not be expensive assuming biased locking is enabled in the JVM (on by\n+ * default for Java 6+). This may not be true, however, if the application itself runs multiple\n+ * jobs concurrently.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorAllocationManager(scheduler: TaskSchedulerImpl) extends Logging {\n+  private val conf = scheduler.conf\n+\n+  // Lower and upper bounds on the number of executors. These are required.\n+  private val minNumExecutors = conf.getInt(\"spark.dynamicAllocation.minExecutors\", -1)\n+  private val maxNumExecutors = conf.getInt(\"spark.dynamicAllocation.maxExecutors\", -1)\n+  if (minNumExecutors < 0 || maxNumExecutors < 0) {\n+    throw new SparkException(\"spark.dynamicAllocation.{min/max}Executors must be set!\")\n+  }\n+\n+  // How frequently to add and remove executors (seconds)\n+  private val addThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorThreshold\", 60)"
  }],
  "prId": 2746
}, {
  "comments": [{
    "author": {
      "login": "kayousterhout"
    },
    "body": "It would be helpful to expand on what \"pending\" means in the comment here\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-15T20:37:42Z",
    "diffHunk": "@@ -0,0 +1,496 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically allocates and removes executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks is not\n+ * drained in N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle for K seconds (meaning it has not\n+ * been scheduled to run any tasks), then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * Both add and remove attempts are retried on failure up to a maximum number of times.\n+ *\n+ * The relevant Spark properties include the following:\n+ *\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *\n+ *   spark.dynamicAllocation.addExecutorThreshold - How long before new executors are added (N)\n+ *   spark.dynamicAllocation.addExecutorInterval - How often to add new executors (M)\n+ *   spark.dynamicAllocation.removeExecutorThreshold - How long before an executor is removed (K)\n+ *\n+ *   spark.dynamicAllocation.addExecutorRetryInterval - How often to retry adding executors\n+ *   spark.dynamicAllocation.removeExecutorRetryInterval - How often to retry removing executors\n+ *   spark.dynamicAllocation.maxAddExecutorRetryAttempts - Max retries in re-adding executors\n+ *   spark.dynamicAllocation.maxRemoveExecutorRetryAttempts - Max retries in re-removing executors\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention should only\n+ * arise when new executors register or when existing executors have been removed, both of which\n+ * are relatively rare events with respect to task scheduling. Thus, synchronizing each method on\n+ * the same lock should not be expensive assuming biased locking is enabled in the JVM (on by\n+ * default for Java 6+). This may not be true, however, if the application itself runs multiple\n+ * jobs concurrently.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorAllocationManager(scheduler: TaskSchedulerImpl) extends Logging {\n+  private val conf = scheduler.conf\n+\n+  // Lower and upper bounds on the number of executors. These are required.\n+  private val minNumExecutors = conf.getInt(\"spark.dynamicAllocation.minExecutors\", -1)\n+  private val maxNumExecutors = conf.getInt(\"spark.dynamicAllocation.maxExecutors\", -1)\n+  if (minNumExecutors < 0 || maxNumExecutors < 0) {\n+    throw new SparkException(\"spark.dynamicAllocation.{min/max}Executors must be set!\")\n+  }\n+\n+  // How frequently to add and remove executors (seconds)\n+  private val addThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorThreshold\", 60)\n+  private val addInterval =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorInterval\", addThreshold)\n+  private val addRetryInterval =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorRetryInterval\", addInterval)\n+  private val removeThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorThreshold\", 600)\n+  private val removeRetryInterval =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorRetryInterval\", 300)\n+\n+  // Number of executors to add in the next round\n+  private var numExecutorsToAdd = 1\n+\n+  // Pending executors that have not actually been added/removed yet"
  }],
  "prId": 2746
}, {
  "comments": [{
    "author": {
      "login": "kayousterhout"
    },
    "body": "Is it possible to expose this information somewhere in a more general way?  It seems a little hacky to duplicate tracking here and in the scheduler code.\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-15T20:39:04Z",
    "diffHunk": "@@ -0,0 +1,496 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically allocates and removes executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks is not\n+ * drained in N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle for K seconds (meaning it has not\n+ * been scheduled to run any tasks), then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * Both add and remove attempts are retried on failure up to a maximum number of times.\n+ *\n+ * The relevant Spark properties include the following:\n+ *\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *\n+ *   spark.dynamicAllocation.addExecutorThreshold - How long before new executors are added (N)\n+ *   spark.dynamicAllocation.addExecutorInterval - How often to add new executors (M)\n+ *   spark.dynamicAllocation.removeExecutorThreshold - How long before an executor is removed (K)\n+ *\n+ *   spark.dynamicAllocation.addExecutorRetryInterval - How often to retry adding executors\n+ *   spark.dynamicAllocation.removeExecutorRetryInterval - How often to retry removing executors\n+ *   spark.dynamicAllocation.maxAddExecutorRetryAttempts - Max retries in re-adding executors\n+ *   spark.dynamicAllocation.maxRemoveExecutorRetryAttempts - Max retries in re-removing executors\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention should only\n+ * arise when new executors register or when existing executors have been removed, both of which\n+ * are relatively rare events with respect to task scheduling. Thus, synchronizing each method on\n+ * the same lock should not be expensive assuming biased locking is enabled in the JVM (on by\n+ * default for Java 6+). This may not be true, however, if the application itself runs multiple\n+ * jobs concurrently.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorAllocationManager(scheduler: TaskSchedulerImpl) extends Logging {\n+  private val conf = scheduler.conf\n+\n+  // Lower and upper bounds on the number of executors. These are required.\n+  private val minNumExecutors = conf.getInt(\"spark.dynamicAllocation.minExecutors\", -1)\n+  private val maxNumExecutors = conf.getInt(\"spark.dynamicAllocation.maxExecutors\", -1)\n+  if (minNumExecutors < 0 || maxNumExecutors < 0) {\n+    throw new SparkException(\"spark.dynamicAllocation.{min/max}Executors must be set!\")\n+  }\n+\n+  // How frequently to add and remove executors (seconds)\n+  private val addThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorThreshold\", 60)\n+  private val addInterval =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorInterval\", addThreshold)\n+  private val addRetryInterval =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorRetryInterval\", addInterval)\n+  private val removeThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorThreshold\", 600)\n+  private val removeRetryInterval =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorRetryInterval\", 300)\n+\n+  // Number of executors to add in the next round\n+  private var numExecutorsToAdd = 1\n+\n+  // Pending executors that have not actually been added/removed yet\n+  private var numExecutorsPendingToAdd = 0\n+  private val executorsPendingToRemove = new mutable.HashSet[String]\n+\n+  // Retry attempts\n+  private var addRetryAttempts = 0\n+  private val removeRetryAttempts = new mutable.HashMap[String, Int]\n+  private val maxAddRetryAttempts =\n+    conf.getInt(\"spark.dynamicAllocation.maxAddExecutorRetryAttempts\", 10)\n+  private val maxRemoveRetryAttempts =\n+    conf.getInt(\"spark.dynamicAllocation.maxRemoveExecutorRetryAttempts\", 10)\n+\n+  // Keep track of all executors here to decouple us from the logic in TaskSchedulerImpl\n+  private val executorIds = new mutable.HashSet[String]"
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "The main reason for factoring this out here is for synchronization. If we rely on the corresponding map (`executorIdToHost`) in the `TaskSchedulerImpl` for this, then we need to synchronize on every usage there as well. In this PR I wanted to make the minimally invasive change to the scheduler code.\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-15T21:04:10Z",
    "diffHunk": "@@ -0,0 +1,496 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically allocates and removes executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks is not\n+ * drained in N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle for K seconds (meaning it has not\n+ * been scheduled to run any tasks), then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * Both add and remove attempts are retried on failure up to a maximum number of times.\n+ *\n+ * The relevant Spark properties include the following:\n+ *\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *\n+ *   spark.dynamicAllocation.addExecutorThreshold - How long before new executors are added (N)\n+ *   spark.dynamicAllocation.addExecutorInterval - How often to add new executors (M)\n+ *   spark.dynamicAllocation.removeExecutorThreshold - How long before an executor is removed (K)\n+ *\n+ *   spark.dynamicAllocation.addExecutorRetryInterval - How often to retry adding executors\n+ *   spark.dynamicAllocation.removeExecutorRetryInterval - How often to retry removing executors\n+ *   spark.dynamicAllocation.maxAddExecutorRetryAttempts - Max retries in re-adding executors\n+ *   spark.dynamicAllocation.maxRemoveExecutorRetryAttempts - Max retries in re-removing executors\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention should only\n+ * arise when new executors register or when existing executors have been removed, both of which\n+ * are relatively rare events with respect to task scheduling. Thus, synchronizing each method on\n+ * the same lock should not be expensive assuming biased locking is enabled in the JVM (on by\n+ * default for Java 6+). This may not be true, however, if the application itself runs multiple\n+ * jobs concurrently.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorAllocationManager(scheduler: TaskSchedulerImpl) extends Logging {\n+  private val conf = scheduler.conf\n+\n+  // Lower and upper bounds on the number of executors. These are required.\n+  private val minNumExecutors = conf.getInt(\"spark.dynamicAllocation.minExecutors\", -1)\n+  private val maxNumExecutors = conf.getInt(\"spark.dynamicAllocation.maxExecutors\", -1)\n+  if (minNumExecutors < 0 || maxNumExecutors < 0) {\n+    throw new SparkException(\"spark.dynamicAllocation.{min/max}Executors must be set!\")\n+  }\n+\n+  // How frequently to add and remove executors (seconds)\n+  private val addThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorThreshold\", 60)\n+  private val addInterval =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorInterval\", addThreshold)\n+  private val addRetryInterval =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorRetryInterval\", addInterval)\n+  private val removeThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorThreshold\", 600)\n+  private val removeRetryInterval =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorRetryInterval\", 300)\n+\n+  // Number of executors to add in the next round\n+  private var numExecutorsToAdd = 1\n+\n+  // Pending executors that have not actually been added/removed yet\n+  private var numExecutorsPendingToAdd = 0\n+  private val executorsPendingToRemove = new mutable.HashSet[String]\n+\n+  // Retry attempts\n+  private var addRetryAttempts = 0\n+  private val removeRetryAttempts = new mutable.HashMap[String, Int]\n+  private val maxAddRetryAttempts =\n+    conf.getInt(\"spark.dynamicAllocation.maxAddExecutorRetryAttempts\", 10)\n+  private val maxRemoveRetryAttempts =\n+    conf.getInt(\"spark.dynamicAllocation.maxRemoveExecutorRetryAttempts\", 10)\n+\n+  // Keep track of all executors here to decouple us from the logic in TaskSchedulerImpl\n+  private val executorIds = new mutable.HashSet[String]"
  }],
  "prId": 2746
}, {
  "comments": [{
    "author": {
      "login": "kayousterhout"
    },
    "body": "<3 units in the name here, but can you do \"Millis\" at least?\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-15T20:39:59Z",
    "diffHunk": "@@ -0,0 +1,496 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically allocates and removes executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks is not\n+ * drained in N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle for K seconds (meaning it has not\n+ * been scheduled to run any tasks), then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * Both add and remove attempts are retried on failure up to a maximum number of times.\n+ *\n+ * The relevant Spark properties include the following:\n+ *\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *\n+ *   spark.dynamicAllocation.addExecutorThreshold - How long before new executors are added (N)\n+ *   spark.dynamicAllocation.addExecutorInterval - How often to add new executors (M)\n+ *   spark.dynamicAllocation.removeExecutorThreshold - How long before an executor is removed (K)\n+ *\n+ *   spark.dynamicAllocation.addExecutorRetryInterval - How often to retry adding executors\n+ *   spark.dynamicAllocation.removeExecutorRetryInterval - How often to retry removing executors\n+ *   spark.dynamicAllocation.maxAddExecutorRetryAttempts - Max retries in re-adding executors\n+ *   spark.dynamicAllocation.maxRemoveExecutorRetryAttempts - Max retries in re-removing executors\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention should only\n+ * arise when new executors register or when existing executors have been removed, both of which\n+ * are relatively rare events with respect to task scheduling. Thus, synchronizing each method on\n+ * the same lock should not be expensive assuming biased locking is enabled in the JVM (on by\n+ * default for Java 6+). This may not be true, however, if the application itself runs multiple\n+ * jobs concurrently.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorAllocationManager(scheduler: TaskSchedulerImpl) extends Logging {\n+  private val conf = scheduler.conf\n+\n+  // Lower and upper bounds on the number of executors. These are required.\n+  private val minNumExecutors = conf.getInt(\"spark.dynamicAllocation.minExecutors\", -1)\n+  private val maxNumExecutors = conf.getInt(\"spark.dynamicAllocation.maxExecutors\", -1)\n+  if (minNumExecutors < 0 || maxNumExecutors < 0) {\n+    throw new SparkException(\"spark.dynamicAllocation.{min/max}Executors must be set!\")\n+  }\n+\n+  // How frequently to add and remove executors (seconds)\n+  private val addThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorThreshold\", 60)\n+  private val addInterval =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorInterval\", addThreshold)\n+  private val addRetryInterval =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorRetryInterval\", addInterval)\n+  private val removeThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorThreshold\", 600)\n+  private val removeRetryInterval =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorRetryInterval\", 300)\n+\n+  // Number of executors to add in the next round\n+  private var numExecutorsToAdd = 1\n+\n+  // Pending executors that have not actually been added/removed yet\n+  private var numExecutorsPendingToAdd = 0\n+  private val executorsPendingToRemove = new mutable.HashSet[String]\n+\n+  // Retry attempts\n+  private var addRetryAttempts = 0\n+  private val removeRetryAttempts = new mutable.HashMap[String, Int]\n+  private val maxAddRetryAttempts =\n+    conf.getInt(\"spark.dynamicAllocation.maxAddExecutorRetryAttempts\", 10)\n+  private val maxRemoveRetryAttempts =\n+    conf.getInt(\"spark.dynamicAllocation.maxRemoveExecutorRetryAttempts\", 10)\n+\n+  // Keep track of all executors here to decouple us from the logic in TaskSchedulerImpl\n+  private val executorIds = new mutable.HashSet[String]\n+\n+  // Timers for keeping track of when to add/remove executors (ms)\n+  private var addTimer = 0\n+  private var addRetryTimer = 0\n+  private val removeTimers = new mutable.HashMap[String, Long]\n+  private val retryRemoveTimers = new mutable.HashMap[String, Long]\n+\n+  // Additional variables used for adding executors\n+  private var addThresholdCrossed = false\n+  private var addTimerEnabled = false\n+  private var addRetryTimerEnabled = false\n+\n+  // Loop interval (ms)\n+  private val intervalMs = 100"
  }],
  "prId": 2746
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Would it be better to calculate the actual sleep time using the clock, preferrably a monotonic one (like `System.nanoTime`)?\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-15T20:50:50Z",
    "diffHunk": "@@ -0,0 +1,496 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically allocates and removes executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks is not\n+ * drained in N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle for K seconds (meaning it has not\n+ * been scheduled to run any tasks), then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * Both add and remove attempts are retried on failure up to a maximum number of times.\n+ *\n+ * The relevant Spark properties include the following:\n+ *\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *\n+ *   spark.dynamicAllocation.addExecutorThreshold - How long before new executors are added (N)\n+ *   spark.dynamicAllocation.addExecutorInterval - How often to add new executors (M)\n+ *   spark.dynamicAllocation.removeExecutorThreshold - How long before an executor is removed (K)\n+ *\n+ *   spark.dynamicAllocation.addExecutorRetryInterval - How often to retry adding executors\n+ *   spark.dynamicAllocation.removeExecutorRetryInterval - How often to retry removing executors\n+ *   spark.dynamicAllocation.maxAddExecutorRetryAttempts - Max retries in re-adding executors\n+ *   spark.dynamicAllocation.maxRemoveExecutorRetryAttempts - Max retries in re-removing executors\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention should only\n+ * arise when new executors register or when existing executors have been removed, both of which\n+ * are relatively rare events with respect to task scheduling. Thus, synchronizing each method on\n+ * the same lock should not be expensive assuming biased locking is enabled in the JVM (on by\n+ * default for Java 6+). This may not be true, however, if the application itself runs multiple\n+ * jobs concurrently.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorAllocationManager(scheduler: TaskSchedulerImpl) extends Logging {\n+  private val conf = scheduler.conf\n+\n+  // Lower and upper bounds on the number of executors. These are required.\n+  private val minNumExecutors = conf.getInt(\"spark.dynamicAllocation.minExecutors\", -1)\n+  private val maxNumExecutors = conf.getInt(\"spark.dynamicAllocation.maxExecutors\", -1)\n+  if (minNumExecutors < 0 || maxNumExecutors < 0) {\n+    throw new SparkException(\"spark.dynamicAllocation.{min/max}Executors must be set!\")\n+  }\n+\n+  // How frequently to add and remove executors (seconds)\n+  private val addThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorThreshold\", 60)\n+  private val addInterval =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorInterval\", addThreshold)\n+  private val addRetryInterval =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorRetryInterval\", addInterval)\n+  private val removeThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorThreshold\", 600)\n+  private val removeRetryInterval =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorRetryInterval\", 300)\n+\n+  // Number of executors to add in the next round\n+  private var numExecutorsToAdd = 1\n+\n+  // Pending executors that have not actually been added/removed yet\n+  private var numExecutorsPendingToAdd = 0\n+  private val executorsPendingToRemove = new mutable.HashSet[String]\n+\n+  // Retry attempts\n+  private var addRetryAttempts = 0\n+  private val removeRetryAttempts = new mutable.HashMap[String, Int]\n+  private val maxAddRetryAttempts =\n+    conf.getInt(\"spark.dynamicAllocation.maxAddExecutorRetryAttempts\", 10)\n+  private val maxRemoveRetryAttempts =\n+    conf.getInt(\"spark.dynamicAllocation.maxRemoveExecutorRetryAttempts\", 10)\n+\n+  // Keep track of all executors here to decouple us from the logic in TaskSchedulerImpl\n+  private val executorIds = new mutable.HashSet[String]\n+\n+  // Timers for keeping track of when to add/remove executors (ms)\n+  private var addTimer = 0\n+  private var addRetryTimer = 0\n+  private val removeTimers = new mutable.HashMap[String, Long]\n+  private val retryRemoveTimers = new mutable.HashMap[String, Long]\n+\n+  // Additional variables used for adding executors\n+  private var addThresholdCrossed = false\n+  private var addTimerEnabled = false\n+  private var addRetryTimerEnabled = false\n+\n+  // Loop interval (ms)\n+  private val intervalMs = 100\n+\n+  // Scheduler backend through which requests to add/remove executors are made\n+  // Note that this assumes the backend has already initialized when this is first used\n+  // Otherwise, an appropriate exception is thrown\n+  private lazy val backend = scheduler.backend match {\n+    case b: CoarseGrainedSchedulerBackend => b\n+    case null =>\n+      throw new SparkException(\"Scheduler backend not initialized yet!\")\n+    case _ =>\n+      throw new SparkException(\n+        \"Dynamic allocation of executors is not applicable to fine-grained schedulers. \" +\n+        \"Please set spark.dynamicAllocation.enabled to false.\")\n+  }\n+\n+  initialize()\n+\n+  private def initialize(): Unit = {\n+    // Keep track of all known executors\n+    scheduler.executorIdToHost.keys.foreach(executorAdded)\n+    startPolling()\n+  }\n+\n+  /**\n+   * Start the main polling thread that keeps track of when to add and remove executors.\n+   * During each interval, this thread checks if any of the timers have expired, and, if\n+   * so, triggers the relevant timer actions.\n+   */\n+  def startPolling(): Unit = {\n+    val thread = new Thread {\n+      override def run() {\n+        while (true) {\n+          try {\n+            if (addTimerEnabled) {\n+              val threshold = if (addThresholdCrossed) addInterval else addThreshold\n+              if (addTimer > threshold * 1000) {\n+                addThresholdCrossed = true\n+                addExecutors()\n+              }\n+            }\n+\n+            if (addRetryTimerEnabled) {\n+              if (addRetryTimer > addRetryInterval * 1000) {\n+                retryAddExecutors()\n+              }\n+            }\n+\n+            removeTimers.foreach { case (id, t) =>\n+              if (t > removeThreshold * 1000) {\n+                removeExecutor(id)\n+              }\n+            }\n+\n+            retryRemoveTimers.foreach { case (id, t) =>\n+              if (t > removeRetryInterval * 1000) {\n+                retryRemoveExecutors(id)\n+              }\n+            }\n+          } catch {\n+            case e: Exception =>\n+              logError(\"Exception encountered in dynamic executor allocation thread!\", e)\n+          } finally {\n+            // Advance all timers that are enabled\n+            Thread.sleep(intervalMs)\n+            if (addTimerEnabled) {\n+              addTimer += intervalMs"
  }],
  "prId": 2746
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Not sure why this is being reset here. IIUC you're just delaying the next round until the previous round has been completely added; but you still want to follow the exponential increase when you actually start the next round, no?\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-15T20:53:28Z",
    "diffHunk": "@@ -0,0 +1,496 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically allocates and removes executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks is not\n+ * drained in N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle for K seconds (meaning it has not\n+ * been scheduled to run any tasks), then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * Both add and remove attempts are retried on failure up to a maximum number of times.\n+ *\n+ * The relevant Spark properties include the following:\n+ *\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *\n+ *   spark.dynamicAllocation.addExecutorThreshold - How long before new executors are added (N)\n+ *   spark.dynamicAllocation.addExecutorInterval - How often to add new executors (M)\n+ *   spark.dynamicAllocation.removeExecutorThreshold - How long before an executor is removed (K)\n+ *\n+ *   spark.dynamicAllocation.addExecutorRetryInterval - How often to retry adding executors\n+ *   spark.dynamicAllocation.removeExecutorRetryInterval - How often to retry removing executors\n+ *   spark.dynamicAllocation.maxAddExecutorRetryAttempts - Max retries in re-adding executors\n+ *   spark.dynamicAllocation.maxRemoveExecutorRetryAttempts - Max retries in re-removing executors\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention should only\n+ * arise when new executors register or when existing executors have been removed, both of which\n+ * are relatively rare events with respect to task scheduling. Thus, synchronizing each method on\n+ * the same lock should not be expensive assuming biased locking is enabled in the JVM (on by\n+ * default for Java 6+). This may not be true, however, if the application itself runs multiple\n+ * jobs concurrently.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorAllocationManager(scheduler: TaskSchedulerImpl) extends Logging {\n+  private val conf = scheduler.conf\n+\n+  // Lower and upper bounds on the number of executors. These are required.\n+  private val minNumExecutors = conf.getInt(\"spark.dynamicAllocation.minExecutors\", -1)\n+  private val maxNumExecutors = conf.getInt(\"spark.dynamicAllocation.maxExecutors\", -1)\n+  if (minNumExecutors < 0 || maxNumExecutors < 0) {\n+    throw new SparkException(\"spark.dynamicAllocation.{min/max}Executors must be set!\")\n+  }\n+\n+  // How frequently to add and remove executors (seconds)\n+  private val addThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorThreshold\", 60)\n+  private val addInterval =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorInterval\", addThreshold)\n+  private val addRetryInterval =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorRetryInterval\", addInterval)\n+  private val removeThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorThreshold\", 600)\n+  private val removeRetryInterval =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorRetryInterval\", 300)\n+\n+  // Number of executors to add in the next round\n+  private var numExecutorsToAdd = 1\n+\n+  // Pending executors that have not actually been added/removed yet\n+  private var numExecutorsPendingToAdd = 0\n+  private val executorsPendingToRemove = new mutable.HashSet[String]\n+\n+  // Retry attempts\n+  private var addRetryAttempts = 0\n+  private val removeRetryAttempts = new mutable.HashMap[String, Int]\n+  private val maxAddRetryAttempts =\n+    conf.getInt(\"spark.dynamicAllocation.maxAddExecutorRetryAttempts\", 10)\n+  private val maxRemoveRetryAttempts =\n+    conf.getInt(\"spark.dynamicAllocation.maxRemoveExecutorRetryAttempts\", 10)\n+\n+  // Keep track of all executors here to decouple us from the logic in TaskSchedulerImpl\n+  private val executorIds = new mutable.HashSet[String]\n+\n+  // Timers for keeping track of when to add/remove executors (ms)\n+  private var addTimer = 0\n+  private var addRetryTimer = 0\n+  private val removeTimers = new mutable.HashMap[String, Long]\n+  private val retryRemoveTimers = new mutable.HashMap[String, Long]\n+\n+  // Additional variables used for adding executors\n+  private var addThresholdCrossed = false\n+  private var addTimerEnabled = false\n+  private var addRetryTimerEnabled = false\n+\n+  // Loop interval (ms)\n+  private val intervalMs = 100\n+\n+  // Scheduler backend through which requests to add/remove executors are made\n+  // Note that this assumes the backend has already initialized when this is first used\n+  // Otherwise, an appropriate exception is thrown\n+  private lazy val backend = scheduler.backend match {\n+    case b: CoarseGrainedSchedulerBackend => b\n+    case null =>\n+      throw new SparkException(\"Scheduler backend not initialized yet!\")\n+    case _ =>\n+      throw new SparkException(\n+        \"Dynamic allocation of executors is not applicable to fine-grained schedulers. \" +\n+        \"Please set spark.dynamicAllocation.enabled to false.\")\n+  }\n+\n+  initialize()\n+\n+  private def initialize(): Unit = {\n+    // Keep track of all known executors\n+    scheduler.executorIdToHost.keys.foreach(executorAdded)\n+    startPolling()\n+  }\n+\n+  /**\n+   * Start the main polling thread that keeps track of when to add and remove executors.\n+   * During each interval, this thread checks if any of the timers have expired, and, if\n+   * so, triggers the relevant timer actions.\n+   */\n+  def startPolling(): Unit = {\n+    val thread = new Thread {\n+      override def run() {\n+        while (true) {\n+          try {\n+            if (addTimerEnabled) {\n+              val threshold = if (addThresholdCrossed) addInterval else addThreshold\n+              if (addTimer > threshold * 1000) {\n+                addThresholdCrossed = true\n+                addExecutors()\n+              }\n+            }\n+\n+            if (addRetryTimerEnabled) {\n+              if (addRetryTimer > addRetryInterval * 1000) {\n+                retryAddExecutors()\n+              }\n+            }\n+\n+            removeTimers.foreach { case (id, t) =>\n+              if (t > removeThreshold * 1000) {\n+                removeExecutor(id)\n+              }\n+            }\n+\n+            retryRemoveTimers.foreach { case (id, t) =>\n+              if (t > removeRetryInterval * 1000) {\n+                retryRemoveExecutors(id)\n+              }\n+            }\n+          } catch {\n+            case e: Exception =>\n+              logError(\"Exception encountered in dynamic executor allocation thread!\", e)\n+          } finally {\n+            // Advance all timers that are enabled\n+            Thread.sleep(intervalMs)\n+            if (addTimerEnabled) {\n+              addTimer += intervalMs\n+            }\n+            if (addRetryTimerEnabled) {\n+              addRetryTimer += intervalMs\n+            }\n+            removeTimers.foreach { case (id, _) =>\n+              removeTimers(id) += intervalMs\n+            }\n+            retryRemoveTimers.foreach { case (id, _) =>\n+              retryRemoveTimers(id) += intervalMs\n+            }\n+          }\n+        }\n+      }\n+    }\n+    thread.setName(\"spark-dynamic-executor-allocation\")\n+    thread.setDaemon(true)\n+    thread.start()\n+  }\n+\n+  /**\n+   * Request a number of executors from the scheduler backend.\n+   * This automatically restarts the add timer unless it is explicitly canceled.\n+   */\n+  private def addExecutors(): Unit = synchronized {\n+    // Restart add timer because there are still pending tasks\n+    startAddTimer()\n+\n+    // Wait until the previous round of executors have registered\n+    if (numExecutorsPendingToAdd > 0) {\n+      logInfo(s\"Not adding executors because there are still \" +\n+        s\"$numExecutorsPendingToAdd request(s) in flight\")\n+      numExecutorsToAdd = 1"
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "Yeah, my original thinking is that if we're hitting this case then we're adding executors too quickly so we need to slow down, but in retrospect that might be too conservative. For instance, if we set the interval to a really low value then we'll basically always hit this.\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-15T21:08:05Z",
    "diffHunk": "@@ -0,0 +1,496 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically allocates and removes executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks is not\n+ * drained in N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle for K seconds (meaning it has not\n+ * been scheduled to run any tasks), then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * Both add and remove attempts are retried on failure up to a maximum number of times.\n+ *\n+ * The relevant Spark properties include the following:\n+ *\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *\n+ *   spark.dynamicAllocation.addExecutorThreshold - How long before new executors are added (N)\n+ *   spark.dynamicAllocation.addExecutorInterval - How often to add new executors (M)\n+ *   spark.dynamicAllocation.removeExecutorThreshold - How long before an executor is removed (K)\n+ *\n+ *   spark.dynamicAllocation.addExecutorRetryInterval - How often to retry adding executors\n+ *   spark.dynamicAllocation.removeExecutorRetryInterval - How often to retry removing executors\n+ *   spark.dynamicAllocation.maxAddExecutorRetryAttempts - Max retries in re-adding executors\n+ *   spark.dynamicAllocation.maxRemoveExecutorRetryAttempts - Max retries in re-removing executors\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention should only\n+ * arise when new executors register or when existing executors have been removed, both of which\n+ * are relatively rare events with respect to task scheduling. Thus, synchronizing each method on\n+ * the same lock should not be expensive assuming biased locking is enabled in the JVM (on by\n+ * default for Java 6+). This may not be true, however, if the application itself runs multiple\n+ * jobs concurrently.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorAllocationManager(scheduler: TaskSchedulerImpl) extends Logging {\n+  private val conf = scheduler.conf\n+\n+  // Lower and upper bounds on the number of executors. These are required.\n+  private val minNumExecutors = conf.getInt(\"spark.dynamicAllocation.minExecutors\", -1)\n+  private val maxNumExecutors = conf.getInt(\"spark.dynamicAllocation.maxExecutors\", -1)\n+  if (minNumExecutors < 0 || maxNumExecutors < 0) {\n+    throw new SparkException(\"spark.dynamicAllocation.{min/max}Executors must be set!\")\n+  }\n+\n+  // How frequently to add and remove executors (seconds)\n+  private val addThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorThreshold\", 60)\n+  private val addInterval =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorInterval\", addThreshold)\n+  private val addRetryInterval =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorRetryInterval\", addInterval)\n+  private val removeThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorThreshold\", 600)\n+  private val removeRetryInterval =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorRetryInterval\", 300)\n+\n+  // Number of executors to add in the next round\n+  private var numExecutorsToAdd = 1\n+\n+  // Pending executors that have not actually been added/removed yet\n+  private var numExecutorsPendingToAdd = 0\n+  private val executorsPendingToRemove = new mutable.HashSet[String]\n+\n+  // Retry attempts\n+  private var addRetryAttempts = 0\n+  private val removeRetryAttempts = new mutable.HashMap[String, Int]\n+  private val maxAddRetryAttempts =\n+    conf.getInt(\"spark.dynamicAllocation.maxAddExecutorRetryAttempts\", 10)\n+  private val maxRemoveRetryAttempts =\n+    conf.getInt(\"spark.dynamicAllocation.maxRemoveExecutorRetryAttempts\", 10)\n+\n+  // Keep track of all executors here to decouple us from the logic in TaskSchedulerImpl\n+  private val executorIds = new mutable.HashSet[String]\n+\n+  // Timers for keeping track of when to add/remove executors (ms)\n+  private var addTimer = 0\n+  private var addRetryTimer = 0\n+  private val removeTimers = new mutable.HashMap[String, Long]\n+  private val retryRemoveTimers = new mutable.HashMap[String, Long]\n+\n+  // Additional variables used for adding executors\n+  private var addThresholdCrossed = false\n+  private var addTimerEnabled = false\n+  private var addRetryTimerEnabled = false\n+\n+  // Loop interval (ms)\n+  private val intervalMs = 100\n+\n+  // Scheduler backend through which requests to add/remove executors are made\n+  // Note that this assumes the backend has already initialized when this is first used\n+  // Otherwise, an appropriate exception is thrown\n+  private lazy val backend = scheduler.backend match {\n+    case b: CoarseGrainedSchedulerBackend => b\n+    case null =>\n+      throw new SparkException(\"Scheduler backend not initialized yet!\")\n+    case _ =>\n+      throw new SparkException(\n+        \"Dynamic allocation of executors is not applicable to fine-grained schedulers. \" +\n+        \"Please set spark.dynamicAllocation.enabled to false.\")\n+  }\n+\n+  initialize()\n+\n+  private def initialize(): Unit = {\n+    // Keep track of all known executors\n+    scheduler.executorIdToHost.keys.foreach(executorAdded)\n+    startPolling()\n+  }\n+\n+  /**\n+   * Start the main polling thread that keeps track of when to add and remove executors.\n+   * During each interval, this thread checks if any of the timers have expired, and, if\n+   * so, triggers the relevant timer actions.\n+   */\n+  def startPolling(): Unit = {\n+    val thread = new Thread {\n+      override def run() {\n+        while (true) {\n+          try {\n+            if (addTimerEnabled) {\n+              val threshold = if (addThresholdCrossed) addInterval else addThreshold\n+              if (addTimer > threshold * 1000) {\n+                addThresholdCrossed = true\n+                addExecutors()\n+              }\n+            }\n+\n+            if (addRetryTimerEnabled) {\n+              if (addRetryTimer > addRetryInterval * 1000) {\n+                retryAddExecutors()\n+              }\n+            }\n+\n+            removeTimers.foreach { case (id, t) =>\n+              if (t > removeThreshold * 1000) {\n+                removeExecutor(id)\n+              }\n+            }\n+\n+            retryRemoveTimers.foreach { case (id, t) =>\n+              if (t > removeRetryInterval * 1000) {\n+                retryRemoveExecutors(id)\n+              }\n+            }\n+          } catch {\n+            case e: Exception =>\n+              logError(\"Exception encountered in dynamic executor allocation thread!\", e)\n+          } finally {\n+            // Advance all timers that are enabled\n+            Thread.sleep(intervalMs)\n+            if (addTimerEnabled) {\n+              addTimer += intervalMs\n+            }\n+            if (addRetryTimerEnabled) {\n+              addRetryTimer += intervalMs\n+            }\n+            removeTimers.foreach { case (id, _) =>\n+              removeTimers(id) += intervalMs\n+            }\n+            retryRemoveTimers.foreach { case (id, _) =>\n+              retryRemoveTimers(id) += intervalMs\n+            }\n+          }\n+        }\n+      }\n+    }\n+    thread.setName(\"spark-dynamic-executor-allocation\")\n+    thread.setDaemon(true)\n+    thread.start()\n+  }\n+\n+  /**\n+   * Request a number of executors from the scheduler backend.\n+   * This automatically restarts the add timer unless it is explicitly canceled.\n+   */\n+  private def addExecutors(): Unit = synchronized {\n+    // Restart add timer because there are still pending tasks\n+    startAddTimer()\n+\n+    // Wait until the previous round of executors have registered\n+    if (numExecutorsPendingToAdd > 0) {\n+      logInfo(s\"Not adding executors because there are still \" +\n+        s\"$numExecutorsPendingToAdd request(s) in flight\")\n+      numExecutorsToAdd = 1"
  }],
  "prId": 2746
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "small nit: above you check that `numExecutorsPendingToAdd` should be zero, otherwise you never reach this code. So this can just be an assignment.\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-15T20:57:38Z",
    "diffHunk": "@@ -0,0 +1,496 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically allocates and removes executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks is not\n+ * drained in N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle for K seconds (meaning it has not\n+ * been scheduled to run any tasks), then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * Both add and remove attempts are retried on failure up to a maximum number of times.\n+ *\n+ * The relevant Spark properties include the following:\n+ *\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *\n+ *   spark.dynamicAllocation.addExecutorThreshold - How long before new executors are added (N)\n+ *   spark.dynamicAllocation.addExecutorInterval - How often to add new executors (M)\n+ *   spark.dynamicAllocation.removeExecutorThreshold - How long before an executor is removed (K)\n+ *\n+ *   spark.dynamicAllocation.addExecutorRetryInterval - How often to retry adding executors\n+ *   spark.dynamicAllocation.removeExecutorRetryInterval - How often to retry removing executors\n+ *   spark.dynamicAllocation.maxAddExecutorRetryAttempts - Max retries in re-adding executors\n+ *   spark.dynamicAllocation.maxRemoveExecutorRetryAttempts - Max retries in re-removing executors\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention should only\n+ * arise when new executors register or when existing executors have been removed, both of which\n+ * are relatively rare events with respect to task scheduling. Thus, synchronizing each method on\n+ * the same lock should not be expensive assuming biased locking is enabled in the JVM (on by\n+ * default for Java 6+). This may not be true, however, if the application itself runs multiple\n+ * jobs concurrently.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorAllocationManager(scheduler: TaskSchedulerImpl) extends Logging {\n+  private val conf = scheduler.conf\n+\n+  // Lower and upper bounds on the number of executors. These are required.\n+  private val minNumExecutors = conf.getInt(\"spark.dynamicAllocation.minExecutors\", -1)\n+  private val maxNumExecutors = conf.getInt(\"spark.dynamicAllocation.maxExecutors\", -1)\n+  if (minNumExecutors < 0 || maxNumExecutors < 0) {\n+    throw new SparkException(\"spark.dynamicAllocation.{min/max}Executors must be set!\")\n+  }\n+\n+  // How frequently to add and remove executors (seconds)\n+  private val addThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorThreshold\", 60)\n+  private val addInterval =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorInterval\", addThreshold)\n+  private val addRetryInterval =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorRetryInterval\", addInterval)\n+  private val removeThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorThreshold\", 600)\n+  private val removeRetryInterval =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorRetryInterval\", 300)\n+\n+  // Number of executors to add in the next round\n+  private var numExecutorsToAdd = 1\n+\n+  // Pending executors that have not actually been added/removed yet\n+  private var numExecutorsPendingToAdd = 0\n+  private val executorsPendingToRemove = new mutable.HashSet[String]\n+\n+  // Retry attempts\n+  private var addRetryAttempts = 0\n+  private val removeRetryAttempts = new mutable.HashMap[String, Int]\n+  private val maxAddRetryAttempts =\n+    conf.getInt(\"spark.dynamicAllocation.maxAddExecutorRetryAttempts\", 10)\n+  private val maxRemoveRetryAttempts =\n+    conf.getInt(\"spark.dynamicAllocation.maxRemoveExecutorRetryAttempts\", 10)\n+\n+  // Keep track of all executors here to decouple us from the logic in TaskSchedulerImpl\n+  private val executorIds = new mutable.HashSet[String]\n+\n+  // Timers for keeping track of when to add/remove executors (ms)\n+  private var addTimer = 0\n+  private var addRetryTimer = 0\n+  private val removeTimers = new mutable.HashMap[String, Long]\n+  private val retryRemoveTimers = new mutable.HashMap[String, Long]\n+\n+  // Additional variables used for adding executors\n+  private var addThresholdCrossed = false\n+  private var addTimerEnabled = false\n+  private var addRetryTimerEnabled = false\n+\n+  // Loop interval (ms)\n+  private val intervalMs = 100\n+\n+  // Scheduler backend through which requests to add/remove executors are made\n+  // Note that this assumes the backend has already initialized when this is first used\n+  // Otherwise, an appropriate exception is thrown\n+  private lazy val backend = scheduler.backend match {\n+    case b: CoarseGrainedSchedulerBackend => b\n+    case null =>\n+      throw new SparkException(\"Scheduler backend not initialized yet!\")\n+    case _ =>\n+      throw new SparkException(\n+        \"Dynamic allocation of executors is not applicable to fine-grained schedulers. \" +\n+        \"Please set spark.dynamicAllocation.enabled to false.\")\n+  }\n+\n+  initialize()\n+\n+  private def initialize(): Unit = {\n+    // Keep track of all known executors\n+    scheduler.executorIdToHost.keys.foreach(executorAdded)\n+    startPolling()\n+  }\n+\n+  /**\n+   * Start the main polling thread that keeps track of when to add and remove executors.\n+   * During each interval, this thread checks if any of the timers have expired, and, if\n+   * so, triggers the relevant timer actions.\n+   */\n+  def startPolling(): Unit = {\n+    val thread = new Thread {\n+      override def run() {\n+        while (true) {\n+          try {\n+            if (addTimerEnabled) {\n+              val threshold = if (addThresholdCrossed) addInterval else addThreshold\n+              if (addTimer > threshold * 1000) {\n+                addThresholdCrossed = true\n+                addExecutors()\n+              }\n+            }\n+\n+            if (addRetryTimerEnabled) {\n+              if (addRetryTimer > addRetryInterval * 1000) {\n+                retryAddExecutors()\n+              }\n+            }\n+\n+            removeTimers.foreach { case (id, t) =>\n+              if (t > removeThreshold * 1000) {\n+                removeExecutor(id)\n+              }\n+            }\n+\n+            retryRemoveTimers.foreach { case (id, t) =>\n+              if (t > removeRetryInterval * 1000) {\n+                retryRemoveExecutors(id)\n+              }\n+            }\n+          } catch {\n+            case e: Exception =>\n+              logError(\"Exception encountered in dynamic executor allocation thread!\", e)\n+          } finally {\n+            // Advance all timers that are enabled\n+            Thread.sleep(intervalMs)\n+            if (addTimerEnabled) {\n+              addTimer += intervalMs\n+            }\n+            if (addRetryTimerEnabled) {\n+              addRetryTimer += intervalMs\n+            }\n+            removeTimers.foreach { case (id, _) =>\n+              removeTimers(id) += intervalMs\n+            }\n+            retryRemoveTimers.foreach { case (id, _) =>\n+              retryRemoveTimers(id) += intervalMs\n+            }\n+          }\n+        }\n+      }\n+    }\n+    thread.setName(\"spark-dynamic-executor-allocation\")\n+    thread.setDaemon(true)\n+    thread.start()\n+  }\n+\n+  /**\n+   * Request a number of executors from the scheduler backend.\n+   * This automatically restarts the add timer unless it is explicitly canceled.\n+   */\n+  private def addExecutors(): Unit = synchronized {\n+    // Restart add timer because there are still pending tasks\n+    startAddTimer()\n+\n+    // Wait until the previous round of executors have registered\n+    if (numExecutorsPendingToAdd > 0) {\n+      logInfo(s\"Not adding executors because there are still \" +\n+        s\"$numExecutorsPendingToAdd request(s) in flight\")\n+      numExecutorsToAdd = 1\n+      return\n+    }\n+\n+    // Do not request more executors if we have already reached the upper bound\n+    val numExistingExecutors = executorIds.size + numExecutorsPendingToAdd\n+    if (numExistingExecutors >= maxNumExecutors) {\n+      logInfo(s\"Not adding executors because there are already \" +\n+        s\"$maxNumExecutors executor(s), which is the limit\")\n+      numExecutorsToAdd = 1\n+      return\n+    }\n+\n+    // Request executors with respect to the upper bound\n+    // Start the retry timer in case this addition fails\n+    val actualNumExecutorsToAdd =\n+      math.min(numExistingExecutors + numExecutorsToAdd, maxNumExecutors) - numExistingExecutors\n+    val newTotalExecutors = numExistingExecutors + actualNumExecutorsToAdd\n+    logInfo(s\"Pending tasks are building up! Adding $actualNumExecutorsToAdd \" +\n+      s\"new executor(s) (new total is $newTotalExecutors)\")\n+    numExecutorsToAdd *= 2\n+    numExecutorsPendingToAdd += actualNumExecutorsToAdd"
  }],
  "prId": 2746
}, {
  "comments": [{
    "author": {
      "login": "kayousterhout"
    },
    "body": "is this data structure thread safe (i.e. safe to be accessed outside of taskschedulerimpl)?\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-15T21:20:54Z",
    "diffHunk": "@@ -0,0 +1,496 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically allocates and removes executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks is not\n+ * drained in N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle for K seconds (meaning it has not\n+ * been scheduled to run any tasks), then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * Both add and remove attempts are retried on failure up to a maximum number of times.\n+ *\n+ * The relevant Spark properties include the following:\n+ *\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *\n+ *   spark.dynamicAllocation.addExecutorThreshold - How long before new executors are added (N)\n+ *   spark.dynamicAllocation.addExecutorInterval - How often to add new executors (M)\n+ *   spark.dynamicAllocation.removeExecutorThreshold - How long before an executor is removed (K)\n+ *\n+ *   spark.dynamicAllocation.addExecutorRetryInterval - How often to retry adding executors\n+ *   spark.dynamicAllocation.removeExecutorRetryInterval - How often to retry removing executors\n+ *   spark.dynamicAllocation.maxAddExecutorRetryAttempts - Max retries in re-adding executors\n+ *   spark.dynamicAllocation.maxRemoveExecutorRetryAttempts - Max retries in re-removing executors\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention should only\n+ * arise when new executors register or when existing executors have been removed, both of which\n+ * are relatively rare events with respect to task scheduling. Thus, synchronizing each method on\n+ * the same lock should not be expensive assuming biased locking is enabled in the JVM (on by\n+ * default for Java 6+). This may not be true, however, if the application itself runs multiple\n+ * jobs concurrently.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorAllocationManager(scheduler: TaskSchedulerImpl) extends Logging {\n+  private val conf = scheduler.conf\n+\n+  // Lower and upper bounds on the number of executors. These are required.\n+  private val minNumExecutors = conf.getInt(\"spark.dynamicAllocation.minExecutors\", -1)\n+  private val maxNumExecutors = conf.getInt(\"spark.dynamicAllocation.maxExecutors\", -1)\n+  if (minNumExecutors < 0 || maxNumExecutors < 0) {\n+    throw new SparkException(\"spark.dynamicAllocation.{min/max}Executors must be set!\")\n+  }\n+\n+  // How frequently to add and remove executors (seconds)\n+  private val addThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorThreshold\", 60)\n+  private val addInterval =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorInterval\", addThreshold)\n+  private val addRetryInterval =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorRetryInterval\", addInterval)\n+  private val removeThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorThreshold\", 600)\n+  private val removeRetryInterval =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorRetryInterval\", 300)\n+\n+  // Number of executors to add in the next round\n+  private var numExecutorsToAdd = 1\n+\n+  // Pending executors that have not actually been added/removed yet\n+  private var numExecutorsPendingToAdd = 0\n+  private val executorsPendingToRemove = new mutable.HashSet[String]\n+\n+  // Retry attempts\n+  private var addRetryAttempts = 0\n+  private val removeRetryAttempts = new mutable.HashMap[String, Int]\n+  private val maxAddRetryAttempts =\n+    conf.getInt(\"spark.dynamicAllocation.maxAddExecutorRetryAttempts\", 10)\n+  private val maxRemoveRetryAttempts =\n+    conf.getInt(\"spark.dynamicAllocation.maxRemoveExecutorRetryAttempts\", 10)\n+\n+  // Keep track of all executors here to decouple us from the logic in TaskSchedulerImpl\n+  private val executorIds = new mutable.HashSet[String]\n+\n+  // Timers for keeping track of when to add/remove executors (ms)\n+  private var addTimer = 0\n+  private var addRetryTimer = 0\n+  private val removeTimers = new mutable.HashMap[String, Long]\n+  private val retryRemoveTimers = new mutable.HashMap[String, Long]\n+\n+  // Additional variables used for adding executors\n+  private var addThresholdCrossed = false\n+  private var addTimerEnabled = false\n+  private var addRetryTimerEnabled = false\n+\n+  // Loop interval (ms)\n+  private val intervalMs = 100\n+\n+  // Scheduler backend through which requests to add/remove executors are made\n+  // Note that this assumes the backend has already initialized when this is first used\n+  // Otherwise, an appropriate exception is thrown\n+  private lazy val backend = scheduler.backend match {\n+    case b: CoarseGrainedSchedulerBackend => b\n+    case null =>\n+      throw new SparkException(\"Scheduler backend not initialized yet!\")\n+    case _ =>\n+      throw new SparkException(\n+        \"Dynamic allocation of executors is not applicable to fine-grained schedulers. \" +\n+        \"Please set spark.dynamicAllocation.enabled to false.\")\n+  }\n+\n+  initialize()\n+\n+  private def initialize(): Unit = {\n+    // Keep track of all known executors\n+    scheduler.executorIdToHost.keys.foreach(executorAdded)"
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "It's not. Actually on second thought this doesn't really do anything, because at this point we haven't even started the executors yet. Also, we get the most refreshed list of executors every second or so through `TaskSchedulerImpl#resourceOffers`, so I think it's safe to remove this.\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-16T01:54:09Z",
    "diffHunk": "@@ -0,0 +1,496 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically allocates and removes executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks is not\n+ * drained in N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle for K seconds (meaning it has not\n+ * been scheduled to run any tasks), then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * Both add and remove attempts are retried on failure up to a maximum number of times.\n+ *\n+ * The relevant Spark properties include the following:\n+ *\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *\n+ *   spark.dynamicAllocation.addExecutorThreshold - How long before new executors are added (N)\n+ *   spark.dynamicAllocation.addExecutorInterval - How often to add new executors (M)\n+ *   spark.dynamicAllocation.removeExecutorThreshold - How long before an executor is removed (K)\n+ *\n+ *   spark.dynamicAllocation.addExecutorRetryInterval - How often to retry adding executors\n+ *   spark.dynamicAllocation.removeExecutorRetryInterval - How often to retry removing executors\n+ *   spark.dynamicAllocation.maxAddExecutorRetryAttempts - Max retries in re-adding executors\n+ *   spark.dynamicAllocation.maxRemoveExecutorRetryAttempts - Max retries in re-removing executors\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention should only\n+ * arise when new executors register or when existing executors have been removed, both of which\n+ * are relatively rare events with respect to task scheduling. Thus, synchronizing each method on\n+ * the same lock should not be expensive assuming biased locking is enabled in the JVM (on by\n+ * default for Java 6+). This may not be true, however, if the application itself runs multiple\n+ * jobs concurrently.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorAllocationManager(scheduler: TaskSchedulerImpl) extends Logging {\n+  private val conf = scheduler.conf\n+\n+  // Lower and upper bounds on the number of executors. These are required.\n+  private val minNumExecutors = conf.getInt(\"spark.dynamicAllocation.minExecutors\", -1)\n+  private val maxNumExecutors = conf.getInt(\"spark.dynamicAllocation.maxExecutors\", -1)\n+  if (minNumExecutors < 0 || maxNumExecutors < 0) {\n+    throw new SparkException(\"spark.dynamicAllocation.{min/max}Executors must be set!\")\n+  }\n+\n+  // How frequently to add and remove executors (seconds)\n+  private val addThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorThreshold\", 60)\n+  private val addInterval =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorInterval\", addThreshold)\n+  private val addRetryInterval =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorRetryInterval\", addInterval)\n+  private val removeThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorThreshold\", 600)\n+  private val removeRetryInterval =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorRetryInterval\", 300)\n+\n+  // Number of executors to add in the next round\n+  private var numExecutorsToAdd = 1\n+\n+  // Pending executors that have not actually been added/removed yet\n+  private var numExecutorsPendingToAdd = 0\n+  private val executorsPendingToRemove = new mutable.HashSet[String]\n+\n+  // Retry attempts\n+  private var addRetryAttempts = 0\n+  private val removeRetryAttempts = new mutable.HashMap[String, Int]\n+  private val maxAddRetryAttempts =\n+    conf.getInt(\"spark.dynamicAllocation.maxAddExecutorRetryAttempts\", 10)\n+  private val maxRemoveRetryAttempts =\n+    conf.getInt(\"spark.dynamicAllocation.maxRemoveExecutorRetryAttempts\", 10)\n+\n+  // Keep track of all executors here to decouple us from the logic in TaskSchedulerImpl\n+  private val executorIds = new mutable.HashSet[String]\n+\n+  // Timers for keeping track of when to add/remove executors (ms)\n+  private var addTimer = 0\n+  private var addRetryTimer = 0\n+  private val removeTimers = new mutable.HashMap[String, Long]\n+  private val retryRemoveTimers = new mutable.HashMap[String, Long]\n+\n+  // Additional variables used for adding executors\n+  private var addThresholdCrossed = false\n+  private var addTimerEnabled = false\n+  private var addRetryTimerEnabled = false\n+\n+  // Loop interval (ms)\n+  private val intervalMs = 100\n+\n+  // Scheduler backend through which requests to add/remove executors are made\n+  // Note that this assumes the backend has already initialized when this is first used\n+  // Otherwise, an appropriate exception is thrown\n+  private lazy val backend = scheduler.backend match {\n+    case b: CoarseGrainedSchedulerBackend => b\n+    case null =>\n+      throw new SparkException(\"Scheduler backend not initialized yet!\")\n+    case _ =>\n+      throw new SparkException(\n+        \"Dynamic allocation of executors is not applicable to fine-grained schedulers. \" +\n+        \"Please set spark.dynamicAllocation.enabled to false.\")\n+  }\n+\n+  initialize()\n+\n+  private def initialize(): Unit = {\n+    // Keep track of all known executors\n+    scheduler.executorIdToHost.keys.foreach(executorAdded)"
  }],
  "prId": 2746
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Shouldn't this be set to `removeThreshold`?\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-15T23:47:01Z",
    "diffHunk": "@@ -0,0 +1,496 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically allocates and removes executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks is not\n+ * drained in N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle for K seconds (meaning it has not\n+ * been scheduled to run any tasks), then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * Both add and remove attempts are retried on failure up to a maximum number of times.\n+ *\n+ * The relevant Spark properties include the following:\n+ *\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *\n+ *   spark.dynamicAllocation.addExecutorThreshold - How long before new executors are added (N)\n+ *   spark.dynamicAllocation.addExecutorInterval - How often to add new executors (M)\n+ *   spark.dynamicAllocation.removeExecutorThreshold - How long before an executor is removed (K)\n+ *\n+ *   spark.dynamicAllocation.addExecutorRetryInterval - How often to retry adding executors\n+ *   spark.dynamicAllocation.removeExecutorRetryInterval - How often to retry removing executors\n+ *   spark.dynamicAllocation.maxAddExecutorRetryAttempts - Max retries in re-adding executors\n+ *   spark.dynamicAllocation.maxRemoveExecutorRetryAttempts - Max retries in re-removing executors\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention should only\n+ * arise when new executors register or when existing executors have been removed, both of which\n+ * are relatively rare events with respect to task scheduling. Thus, synchronizing each method on\n+ * the same lock should not be expensive assuming biased locking is enabled in the JVM (on by\n+ * default for Java 6+). This may not be true, however, if the application itself runs multiple\n+ * jobs concurrently.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorAllocationManager(scheduler: TaskSchedulerImpl) extends Logging {\n+  private val conf = scheduler.conf\n+\n+  // Lower and upper bounds on the number of executors. These are required.\n+  private val minNumExecutors = conf.getInt(\"spark.dynamicAllocation.minExecutors\", -1)\n+  private val maxNumExecutors = conf.getInt(\"spark.dynamicAllocation.maxExecutors\", -1)\n+  if (minNumExecutors < 0 || maxNumExecutors < 0) {\n+    throw new SparkException(\"spark.dynamicAllocation.{min/max}Executors must be set!\")\n+  }\n+\n+  // How frequently to add and remove executors (seconds)\n+  private val addThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorThreshold\", 60)\n+  private val addInterval =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorInterval\", addThreshold)\n+  private val addRetryInterval =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorRetryInterval\", addInterval)\n+  private val removeThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorThreshold\", 600)\n+  private val removeRetryInterval =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorRetryInterval\", 300)\n+\n+  // Number of executors to add in the next round\n+  private var numExecutorsToAdd = 1\n+\n+  // Pending executors that have not actually been added/removed yet\n+  private var numExecutorsPendingToAdd = 0\n+  private val executorsPendingToRemove = new mutable.HashSet[String]\n+\n+  // Retry attempts\n+  private var addRetryAttempts = 0\n+  private val removeRetryAttempts = new mutable.HashMap[String, Int]\n+  private val maxAddRetryAttempts =\n+    conf.getInt(\"spark.dynamicAllocation.maxAddExecutorRetryAttempts\", 10)\n+  private val maxRemoveRetryAttempts =\n+    conf.getInt(\"spark.dynamicAllocation.maxRemoveExecutorRetryAttempts\", 10)\n+\n+  // Keep track of all executors here to decouple us from the logic in TaskSchedulerImpl\n+  private val executorIds = new mutable.HashSet[String]\n+\n+  // Timers for keeping track of when to add/remove executors (ms)\n+  private var addTimer = 0\n+  private var addRetryTimer = 0\n+  private val removeTimers = new mutable.HashMap[String, Long]\n+  private val retryRemoveTimers = new mutable.HashMap[String, Long]\n+\n+  // Additional variables used for adding executors\n+  private var addThresholdCrossed = false\n+  private var addTimerEnabled = false\n+  private var addRetryTimerEnabled = false\n+\n+  // Loop interval (ms)\n+  private val intervalMs = 100\n+\n+  // Scheduler backend through which requests to add/remove executors are made\n+  // Note that this assumes the backend has already initialized when this is first used\n+  // Otherwise, an appropriate exception is thrown\n+  private lazy val backend = scheduler.backend match {\n+    case b: CoarseGrainedSchedulerBackend => b\n+    case null =>\n+      throw new SparkException(\"Scheduler backend not initialized yet!\")\n+    case _ =>\n+      throw new SparkException(\n+        \"Dynamic allocation of executors is not applicable to fine-grained schedulers. \" +\n+        \"Please set spark.dynamicAllocation.enabled to false.\")\n+  }\n+\n+  initialize()\n+\n+  private def initialize(): Unit = {\n+    // Keep track of all known executors\n+    scheduler.executorIdToHost.keys.foreach(executorAdded)\n+    startPolling()\n+  }\n+\n+  /**\n+   * Start the main polling thread that keeps track of when to add and remove executors.\n+   * During each interval, this thread checks if any of the timers have expired, and, if\n+   * so, triggers the relevant timer actions.\n+   */\n+  def startPolling(): Unit = {\n+    val thread = new Thread {\n+      override def run() {\n+        while (true) {\n+          try {\n+            if (addTimerEnabled) {\n+              val threshold = if (addThresholdCrossed) addInterval else addThreshold\n+              if (addTimer > threshold * 1000) {\n+                addThresholdCrossed = true\n+                addExecutors()\n+              }\n+            }\n+\n+            if (addRetryTimerEnabled) {\n+              if (addRetryTimer > addRetryInterval * 1000) {\n+                retryAddExecutors()\n+              }\n+            }\n+\n+            removeTimers.foreach { case (id, t) =>\n+              if (t > removeThreshold * 1000) {\n+                removeExecutor(id)\n+              }\n+            }\n+\n+            retryRemoveTimers.foreach { case (id, t) =>\n+              if (t > removeRetryInterval * 1000) {\n+                retryRemoveExecutors(id)\n+              }\n+            }\n+          } catch {\n+            case e: Exception =>\n+              logError(\"Exception encountered in dynamic executor allocation thread!\", e)\n+          } finally {\n+            // Advance all timers that are enabled\n+            Thread.sleep(intervalMs)\n+            if (addTimerEnabled) {\n+              addTimer += intervalMs\n+            }\n+            if (addRetryTimerEnabled) {\n+              addRetryTimer += intervalMs\n+            }\n+            removeTimers.foreach { case (id, _) =>\n+              removeTimers(id) += intervalMs\n+            }\n+            retryRemoveTimers.foreach { case (id, _) =>\n+              retryRemoveTimers(id) += intervalMs\n+            }\n+          }\n+        }\n+      }\n+    }\n+    thread.setName(\"spark-dynamic-executor-allocation\")\n+    thread.setDaemon(true)\n+    thread.start()\n+  }\n+\n+  /**\n+   * Request a number of executors from the scheduler backend.\n+   * This automatically restarts the add timer unless it is explicitly canceled.\n+   */\n+  private def addExecutors(): Unit = synchronized {\n+    // Restart add timer because there are still pending tasks\n+    startAddTimer()\n+\n+    // Wait until the previous round of executors have registered\n+    if (numExecutorsPendingToAdd > 0) {\n+      logInfo(s\"Not adding executors because there are still \" +\n+        s\"$numExecutorsPendingToAdd request(s) in flight\")\n+      numExecutorsToAdd = 1\n+      return\n+    }\n+\n+    // Do not request more executors if we have already reached the upper bound\n+    val numExistingExecutors = executorIds.size + numExecutorsPendingToAdd\n+    if (numExistingExecutors >= maxNumExecutors) {\n+      logInfo(s\"Not adding executors because there are already \" +\n+        s\"$maxNumExecutors executor(s), which is the limit\")\n+      numExecutorsToAdd = 1\n+      return\n+    }\n+\n+    // Request executors with respect to the upper bound\n+    // Start the retry timer in case this addition fails\n+    val actualNumExecutorsToAdd =\n+      math.min(numExistingExecutors + numExecutorsToAdd, maxNumExecutors) - numExistingExecutors\n+    val newTotalExecutors = numExistingExecutors + actualNumExecutorsToAdd\n+    logInfo(s\"Pending tasks are building up! Adding $actualNumExecutorsToAdd \" +\n+      s\"new executor(s) (new total is $newTotalExecutors)\")\n+    numExecutorsToAdd *= 2\n+    numExecutorsPendingToAdd += actualNumExecutorsToAdd\n+    backend.requestExecutors(actualNumExecutorsToAdd)\n+    startAddRetryTimer()\n+  }\n+\n+  /**\n+   * Retry a previous executor request that has not been fulfilled.\n+   * This restarts the retry timer to keep trying up to a maximum number of attempts.\n+   */\n+  private def retryAddExecutors(): Unit = synchronized {\n+    // Do not retry if there are no executors pending to be added (should never happen)\n+    if (numExecutorsPendingToAdd == 0) {\n+      logWarning(\"Attempted to retry adding executors when there are none pending to be added\")\n+      cancelAddRetryTimer()\n+      return\n+    }\n+\n+    // Do not retry if we have already exceeded the maximum number of attempts\n+    addRetryAttempts += 1\n+    if (addRetryAttempts > maxAddRetryAttempts) {\n+      logInfo(s\"Giving up on adding $numExecutorsPendingToAdd executor(s) \" +\n+        s\"after $maxAddRetryAttempts failed attempts\")\n+      numExecutorsPendingToAdd = 0\n+      // Also cancel original add timer because the cluster is not granting us new executors\n+      cancelAddTimer()\n+      return\n+    }\n+\n+    // Retry a previous request, then restart the retry timer in case this retry also fails\n+    logInfo(s\"Previously requested executors have not all registered yet. \" +\n+      s\"Retrying to add $numExecutorsPendingToAdd executor(s) \" +\n+      s\"[attempt $addRetryAttempts/$maxAddRetryAttempts]\")\n+    backend.requestExecutors(numExecutorsPendingToAdd)\n+    startAddRetryTimer()\n+  }\n+\n+  /**\n+   * Request the scheduler backend to decommission the given executor.\n+   * This expires the remove timer unless the executor is kept alive intentionally.\n+   */\n+  private def removeExecutor(executorId: String): Unit = synchronized {\n+    // Do not kill the executor if we are not aware of it (should never happen)\n+    if (!executorIds.contains(executorId)) {\n+      logWarning(s\"Attempted to remove unknown executor $executorId\")\n+      cancelRemoveTimer(executorId)\n+      return\n+    }\n+\n+    // Do not kill the executor again if it is already pending to be killed (should never happen)\n+    if (executorsPendingToRemove.contains(executorId) ||\n+        removeRetryAttempts.contains(executorId) ||\n+        retryRemoveTimers.contains(executorId)) {\n+      logWarning(s\"Executor $executorId is already pending to be removed!\")\n+      cancelRemoveTimer(executorId)\n+      return\n+    }\n+\n+    // Do not kill the executor if we have already reached the lower bound\n+    val numExistingExecutors = executorIds.size - executorsPendingToRemove.size\n+    if (numExistingExecutors - 1 < minNumExecutors) {\n+      logDebug(s\"Not removing idle executor $executorId because there are only $minNumExecutors \" +\n+        \"executor(s) left, which is the limit\")\n+      // Restart the remove timer to keep the executor marked as idle\n+      // Otherwise we won't be able to remove this executor even after new executors have joined\n+      startRemoveTimer(executorId)\n+      return\n+    }\n+\n+    // Send a kill request to the backend for this executor\n+    // Start the retry timer in case this removal fails\n+    logInfo(s\"Removing executor $executorId because it has been idle for \" +\n+      s\"$removeThreshold seconds (new total is ${numExistingExecutors - 1})\")\n+    executorsPendingToRemove.add(executorId)\n+    backend.killExecutor(executorId)\n+    cancelRemoveTimer(executorId)\n+    startRemoveRetryTimer(executorId)\n+  }\n+\n+  /**\n+   * Retry a previous attempt to decommission the given executor.\n+   * This restarts the retry timer to keep trying up to a maximum number of attempts.\n+   */\n+  private def retryRemoveExecutors(executorId: String): Unit = synchronized {\n+    // Do not kill the executor if we are not aware of it (should never happen)\n+    if (!executorIds.contains(executorId)) {\n+      logWarning(s\"Attempted to retry removing unknown executor $executorId\")\n+      cancelRemoveRetryTimer(executorId)\n+      return\n+    }\n+\n+    // Do not retry if this executor is not pending to be killed (should never happen)\n+    if (!executorsPendingToRemove.contains(executorId)) {\n+      logDebug(s\"Attempted to retry removing executor $executorId when it's not to be removed!\")\n+      cancelRemoveRetryTimer(executorId)\n+      return\n+    }\n+\n+    // Do not retry if we have already exceeded the maximum number of attempts\n+    removeRetryAttempts(executorId) =\n+      removeRetryAttempts.getOrElse(executorId, 0) + 1\n+    if (removeRetryAttempts(executorId) > maxRemoveRetryAttempts) {\n+      logInfo(s\"Giving up on removing executor $executorId after \" +\n+        s\"$maxRemoveRetryAttempts failed attempts!\")\n+      executorsPendingToRemove.remove(executorId)\n+      cancelRemoveRetryTimer(executorId)\n+      return\n+    }\n+\n+    // Retry a previous kill request for this executor\n+    // Restart the retry timer in case this retry also fails\n+    logInfo(s\"Retrying previous attempt to remove $executorId \" +\n+      s\"[attempt ${removeRetryAttempts(executorId)}/$maxRemoveRetryAttempts]\")\n+    backend.killExecutor(executorId)\n+    startRemoveRetryTimer(executorId)\n+  }\n+\n+  /**\n+   * Callback for the scheduler to signal that the given executor has been added.\n+   */\n+  def executorAdded(executorId: String): Unit = synchronized {\n+    if (!executorIds.contains(executorId)) {\n+      logDebug(s\"New executor $executorId has registered\")\n+      if (numExecutorsPendingToAdd > 0) {\n+        numExecutorsPendingToAdd -= 1\n+        logDebug(s\"Decremented pending executors to add ($numExecutorsPendingToAdd left)\")\n+        if (numExecutorsPendingToAdd == 0) {\n+          logDebug(\"All previously pending executors have registered!\")\n+          cancelAddRetryTimer()\n+        }\n+      }\n+      executorIds.add(executorId)\n+      startRemoveTimer(executorId)\n+    }\n+  }\n+\n+  /**\n+   * Callback for the scheduler to signal that the given executor has been removed.\n+   */\n+  def executorRemoved(executorId: String): Unit = synchronized {\n+    if (executorIds.contains(executorId)) {\n+      logDebug(s\"Existing executor $executorId has been removed\")\n+      executorIds.remove(executorId)\n+      if (executorsPendingToRemove.contains(executorId)) {\n+        executorsPendingToRemove.remove(executorId)\n+        logDebug(s\"Removing executor $executorId from pending executors to remove \" +\n+          s\"(${executorsPendingToRemove.size} left)\")\n+        cancelRemoveRetryTimer(executorId)\n+      }\n+    } else {\n+      logWarning(s\"Unknown executor $executorId has been removed!\")\n+    }\n+  }\n+\n+  /**\n+   * Return whether the add timer is already running.\n+   */\n+  def isAddTimerRunning: Boolean = addTimerEnabled || addRetryTimerEnabled\n+\n+  /**\n+   * Return whether the remove timer for the given executor is already running.\n+   */\n+  def isRemoveTimerRunning(executorId: String): Boolean = {\n+    removeTimers.contains(executorId) || retryRemoveTimers.contains(executorId)\n+  }\n+\n+  /**\n+   * Start a timer to add executors, to expire in `addThreshold` seconds in the first\n+   * round, and `addInterval` seconds in every round thereafter. This is called when\n+   * the scheduler receives new pending tasks and the timer is not already started. This resets\n+   * the value of any existing add timer.\n+   */\n+  def startAddTimer(): Unit = {\n+    val threshold = if (addThresholdCrossed) addInterval else addThreshold\n+    logDebug(s\"Starting add executor timer (to expire in $threshold seconds)\")\n+    addTimer = 0\n+    addTimerEnabled = true\n+  }\n+\n+  /**\n+   * Start a timer to remove the given executor, to expire in `removeThreshold` seconds.\n+   * This is called when an executor registers or finishes running tasks, and the timer is not\n+   * already started. This resets the value of any existing timer to remove this executor.\n+   */\n+  def startRemoveTimer(executorId: String): Unit = {\n+    logDebug(s\"Starting remove executor timer for $executorId \" +\n+      s\"(to expire in $removeThreshold seconds)\")\n+    removeTimers(executorId) = 0"
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "No, if it's set to `removeThreshold` then you remove the executor right away. It's set to 0 to signal that the timer has just started, and this value is slowly incremented every loop iteration until it exceeds `removeThreshold` (and that's when you actually remove the executor).\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-16T00:24:44Z",
    "diffHunk": "@@ -0,0 +1,496 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically allocates and removes executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks is not\n+ * drained in N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle for K seconds (meaning it has not\n+ * been scheduled to run any tasks), then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * Both add and remove attempts are retried on failure up to a maximum number of times.\n+ *\n+ * The relevant Spark properties include the following:\n+ *\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *\n+ *   spark.dynamicAllocation.addExecutorThreshold - How long before new executors are added (N)\n+ *   spark.dynamicAllocation.addExecutorInterval - How often to add new executors (M)\n+ *   spark.dynamicAllocation.removeExecutorThreshold - How long before an executor is removed (K)\n+ *\n+ *   spark.dynamicAllocation.addExecutorRetryInterval - How often to retry adding executors\n+ *   spark.dynamicAllocation.removeExecutorRetryInterval - How often to retry removing executors\n+ *   spark.dynamicAllocation.maxAddExecutorRetryAttempts - Max retries in re-adding executors\n+ *   spark.dynamicAllocation.maxRemoveExecutorRetryAttempts - Max retries in re-removing executors\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention should only\n+ * arise when new executors register or when existing executors have been removed, both of which\n+ * are relatively rare events with respect to task scheduling. Thus, synchronizing each method on\n+ * the same lock should not be expensive assuming biased locking is enabled in the JVM (on by\n+ * default for Java 6+). This may not be true, however, if the application itself runs multiple\n+ * jobs concurrently.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorAllocationManager(scheduler: TaskSchedulerImpl) extends Logging {\n+  private val conf = scheduler.conf\n+\n+  // Lower and upper bounds on the number of executors. These are required.\n+  private val minNumExecutors = conf.getInt(\"spark.dynamicAllocation.minExecutors\", -1)\n+  private val maxNumExecutors = conf.getInt(\"spark.dynamicAllocation.maxExecutors\", -1)\n+  if (minNumExecutors < 0 || maxNumExecutors < 0) {\n+    throw new SparkException(\"spark.dynamicAllocation.{min/max}Executors must be set!\")\n+  }\n+\n+  // How frequently to add and remove executors (seconds)\n+  private val addThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorThreshold\", 60)\n+  private val addInterval =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorInterval\", addThreshold)\n+  private val addRetryInterval =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorRetryInterval\", addInterval)\n+  private val removeThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorThreshold\", 600)\n+  private val removeRetryInterval =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorRetryInterval\", 300)\n+\n+  // Number of executors to add in the next round\n+  private var numExecutorsToAdd = 1\n+\n+  // Pending executors that have not actually been added/removed yet\n+  private var numExecutorsPendingToAdd = 0\n+  private val executorsPendingToRemove = new mutable.HashSet[String]\n+\n+  // Retry attempts\n+  private var addRetryAttempts = 0\n+  private val removeRetryAttempts = new mutable.HashMap[String, Int]\n+  private val maxAddRetryAttempts =\n+    conf.getInt(\"spark.dynamicAllocation.maxAddExecutorRetryAttempts\", 10)\n+  private val maxRemoveRetryAttempts =\n+    conf.getInt(\"spark.dynamicAllocation.maxRemoveExecutorRetryAttempts\", 10)\n+\n+  // Keep track of all executors here to decouple us from the logic in TaskSchedulerImpl\n+  private val executorIds = new mutable.HashSet[String]\n+\n+  // Timers for keeping track of when to add/remove executors (ms)\n+  private var addTimer = 0\n+  private var addRetryTimer = 0\n+  private val removeTimers = new mutable.HashMap[String, Long]\n+  private val retryRemoveTimers = new mutable.HashMap[String, Long]\n+\n+  // Additional variables used for adding executors\n+  private var addThresholdCrossed = false\n+  private var addTimerEnabled = false\n+  private var addRetryTimerEnabled = false\n+\n+  // Loop interval (ms)\n+  private val intervalMs = 100\n+\n+  // Scheduler backend through which requests to add/remove executors are made\n+  // Note that this assumes the backend has already initialized when this is first used\n+  // Otherwise, an appropriate exception is thrown\n+  private lazy val backend = scheduler.backend match {\n+    case b: CoarseGrainedSchedulerBackend => b\n+    case null =>\n+      throw new SparkException(\"Scheduler backend not initialized yet!\")\n+    case _ =>\n+      throw new SparkException(\n+        \"Dynamic allocation of executors is not applicable to fine-grained schedulers. \" +\n+        \"Please set spark.dynamicAllocation.enabled to false.\")\n+  }\n+\n+  initialize()\n+\n+  private def initialize(): Unit = {\n+    // Keep track of all known executors\n+    scheduler.executorIdToHost.keys.foreach(executorAdded)\n+    startPolling()\n+  }\n+\n+  /**\n+   * Start the main polling thread that keeps track of when to add and remove executors.\n+   * During each interval, this thread checks if any of the timers have expired, and, if\n+   * so, triggers the relevant timer actions.\n+   */\n+  def startPolling(): Unit = {\n+    val thread = new Thread {\n+      override def run() {\n+        while (true) {\n+          try {\n+            if (addTimerEnabled) {\n+              val threshold = if (addThresholdCrossed) addInterval else addThreshold\n+              if (addTimer > threshold * 1000) {\n+                addThresholdCrossed = true\n+                addExecutors()\n+              }\n+            }\n+\n+            if (addRetryTimerEnabled) {\n+              if (addRetryTimer > addRetryInterval * 1000) {\n+                retryAddExecutors()\n+              }\n+            }\n+\n+            removeTimers.foreach { case (id, t) =>\n+              if (t > removeThreshold * 1000) {\n+                removeExecutor(id)\n+              }\n+            }\n+\n+            retryRemoveTimers.foreach { case (id, t) =>\n+              if (t > removeRetryInterval * 1000) {\n+                retryRemoveExecutors(id)\n+              }\n+            }\n+          } catch {\n+            case e: Exception =>\n+              logError(\"Exception encountered in dynamic executor allocation thread!\", e)\n+          } finally {\n+            // Advance all timers that are enabled\n+            Thread.sleep(intervalMs)\n+            if (addTimerEnabled) {\n+              addTimer += intervalMs\n+            }\n+            if (addRetryTimerEnabled) {\n+              addRetryTimer += intervalMs\n+            }\n+            removeTimers.foreach { case (id, _) =>\n+              removeTimers(id) += intervalMs\n+            }\n+            retryRemoveTimers.foreach { case (id, _) =>\n+              retryRemoveTimers(id) += intervalMs\n+            }\n+          }\n+        }\n+      }\n+    }\n+    thread.setName(\"spark-dynamic-executor-allocation\")\n+    thread.setDaemon(true)\n+    thread.start()\n+  }\n+\n+  /**\n+   * Request a number of executors from the scheduler backend.\n+   * This automatically restarts the add timer unless it is explicitly canceled.\n+   */\n+  private def addExecutors(): Unit = synchronized {\n+    // Restart add timer because there are still pending tasks\n+    startAddTimer()\n+\n+    // Wait until the previous round of executors have registered\n+    if (numExecutorsPendingToAdd > 0) {\n+      logInfo(s\"Not adding executors because there are still \" +\n+        s\"$numExecutorsPendingToAdd request(s) in flight\")\n+      numExecutorsToAdd = 1\n+      return\n+    }\n+\n+    // Do not request more executors if we have already reached the upper bound\n+    val numExistingExecutors = executorIds.size + numExecutorsPendingToAdd\n+    if (numExistingExecutors >= maxNumExecutors) {\n+      logInfo(s\"Not adding executors because there are already \" +\n+        s\"$maxNumExecutors executor(s), which is the limit\")\n+      numExecutorsToAdd = 1\n+      return\n+    }\n+\n+    // Request executors with respect to the upper bound\n+    // Start the retry timer in case this addition fails\n+    val actualNumExecutorsToAdd =\n+      math.min(numExistingExecutors + numExecutorsToAdd, maxNumExecutors) - numExistingExecutors\n+    val newTotalExecutors = numExistingExecutors + actualNumExecutorsToAdd\n+    logInfo(s\"Pending tasks are building up! Adding $actualNumExecutorsToAdd \" +\n+      s\"new executor(s) (new total is $newTotalExecutors)\")\n+    numExecutorsToAdd *= 2\n+    numExecutorsPendingToAdd += actualNumExecutorsToAdd\n+    backend.requestExecutors(actualNumExecutorsToAdd)\n+    startAddRetryTimer()\n+  }\n+\n+  /**\n+   * Retry a previous executor request that has not been fulfilled.\n+   * This restarts the retry timer to keep trying up to a maximum number of attempts.\n+   */\n+  private def retryAddExecutors(): Unit = synchronized {\n+    // Do not retry if there are no executors pending to be added (should never happen)\n+    if (numExecutorsPendingToAdd == 0) {\n+      logWarning(\"Attempted to retry adding executors when there are none pending to be added\")\n+      cancelAddRetryTimer()\n+      return\n+    }\n+\n+    // Do not retry if we have already exceeded the maximum number of attempts\n+    addRetryAttempts += 1\n+    if (addRetryAttempts > maxAddRetryAttempts) {\n+      logInfo(s\"Giving up on adding $numExecutorsPendingToAdd executor(s) \" +\n+        s\"after $maxAddRetryAttempts failed attempts\")\n+      numExecutorsPendingToAdd = 0\n+      // Also cancel original add timer because the cluster is not granting us new executors\n+      cancelAddTimer()\n+      return\n+    }\n+\n+    // Retry a previous request, then restart the retry timer in case this retry also fails\n+    logInfo(s\"Previously requested executors have not all registered yet. \" +\n+      s\"Retrying to add $numExecutorsPendingToAdd executor(s) \" +\n+      s\"[attempt $addRetryAttempts/$maxAddRetryAttempts]\")\n+    backend.requestExecutors(numExecutorsPendingToAdd)\n+    startAddRetryTimer()\n+  }\n+\n+  /**\n+   * Request the scheduler backend to decommission the given executor.\n+   * This expires the remove timer unless the executor is kept alive intentionally.\n+   */\n+  private def removeExecutor(executorId: String): Unit = synchronized {\n+    // Do not kill the executor if we are not aware of it (should never happen)\n+    if (!executorIds.contains(executorId)) {\n+      logWarning(s\"Attempted to remove unknown executor $executorId\")\n+      cancelRemoveTimer(executorId)\n+      return\n+    }\n+\n+    // Do not kill the executor again if it is already pending to be killed (should never happen)\n+    if (executorsPendingToRemove.contains(executorId) ||\n+        removeRetryAttempts.contains(executorId) ||\n+        retryRemoveTimers.contains(executorId)) {\n+      logWarning(s\"Executor $executorId is already pending to be removed!\")\n+      cancelRemoveTimer(executorId)\n+      return\n+    }\n+\n+    // Do not kill the executor if we have already reached the lower bound\n+    val numExistingExecutors = executorIds.size - executorsPendingToRemove.size\n+    if (numExistingExecutors - 1 < minNumExecutors) {\n+      logDebug(s\"Not removing idle executor $executorId because there are only $minNumExecutors \" +\n+        \"executor(s) left, which is the limit\")\n+      // Restart the remove timer to keep the executor marked as idle\n+      // Otherwise we won't be able to remove this executor even after new executors have joined\n+      startRemoveTimer(executorId)\n+      return\n+    }\n+\n+    // Send a kill request to the backend for this executor\n+    // Start the retry timer in case this removal fails\n+    logInfo(s\"Removing executor $executorId because it has been idle for \" +\n+      s\"$removeThreshold seconds (new total is ${numExistingExecutors - 1})\")\n+    executorsPendingToRemove.add(executorId)\n+    backend.killExecutor(executorId)\n+    cancelRemoveTimer(executorId)\n+    startRemoveRetryTimer(executorId)\n+  }\n+\n+  /**\n+   * Retry a previous attempt to decommission the given executor.\n+   * This restarts the retry timer to keep trying up to a maximum number of attempts.\n+   */\n+  private def retryRemoveExecutors(executorId: String): Unit = synchronized {\n+    // Do not kill the executor if we are not aware of it (should never happen)\n+    if (!executorIds.contains(executorId)) {\n+      logWarning(s\"Attempted to retry removing unknown executor $executorId\")\n+      cancelRemoveRetryTimer(executorId)\n+      return\n+    }\n+\n+    // Do not retry if this executor is not pending to be killed (should never happen)\n+    if (!executorsPendingToRemove.contains(executorId)) {\n+      logDebug(s\"Attempted to retry removing executor $executorId when it's not to be removed!\")\n+      cancelRemoveRetryTimer(executorId)\n+      return\n+    }\n+\n+    // Do not retry if we have already exceeded the maximum number of attempts\n+    removeRetryAttempts(executorId) =\n+      removeRetryAttempts.getOrElse(executorId, 0) + 1\n+    if (removeRetryAttempts(executorId) > maxRemoveRetryAttempts) {\n+      logInfo(s\"Giving up on removing executor $executorId after \" +\n+        s\"$maxRemoveRetryAttempts failed attempts!\")\n+      executorsPendingToRemove.remove(executorId)\n+      cancelRemoveRetryTimer(executorId)\n+      return\n+    }\n+\n+    // Retry a previous kill request for this executor\n+    // Restart the retry timer in case this retry also fails\n+    logInfo(s\"Retrying previous attempt to remove $executorId \" +\n+      s\"[attempt ${removeRetryAttempts(executorId)}/$maxRemoveRetryAttempts]\")\n+    backend.killExecutor(executorId)\n+    startRemoveRetryTimer(executorId)\n+  }\n+\n+  /**\n+   * Callback for the scheduler to signal that the given executor has been added.\n+   */\n+  def executorAdded(executorId: String): Unit = synchronized {\n+    if (!executorIds.contains(executorId)) {\n+      logDebug(s\"New executor $executorId has registered\")\n+      if (numExecutorsPendingToAdd > 0) {\n+        numExecutorsPendingToAdd -= 1\n+        logDebug(s\"Decremented pending executors to add ($numExecutorsPendingToAdd left)\")\n+        if (numExecutorsPendingToAdd == 0) {\n+          logDebug(\"All previously pending executors have registered!\")\n+          cancelAddRetryTimer()\n+        }\n+      }\n+      executorIds.add(executorId)\n+      startRemoveTimer(executorId)\n+    }\n+  }\n+\n+  /**\n+   * Callback for the scheduler to signal that the given executor has been removed.\n+   */\n+  def executorRemoved(executorId: String): Unit = synchronized {\n+    if (executorIds.contains(executorId)) {\n+      logDebug(s\"Existing executor $executorId has been removed\")\n+      executorIds.remove(executorId)\n+      if (executorsPendingToRemove.contains(executorId)) {\n+        executorsPendingToRemove.remove(executorId)\n+        logDebug(s\"Removing executor $executorId from pending executors to remove \" +\n+          s\"(${executorsPendingToRemove.size} left)\")\n+        cancelRemoveRetryTimer(executorId)\n+      }\n+    } else {\n+      logWarning(s\"Unknown executor $executorId has been removed!\")\n+    }\n+  }\n+\n+  /**\n+   * Return whether the add timer is already running.\n+   */\n+  def isAddTimerRunning: Boolean = addTimerEnabled || addRetryTimerEnabled\n+\n+  /**\n+   * Return whether the remove timer for the given executor is already running.\n+   */\n+  def isRemoveTimerRunning(executorId: String): Boolean = {\n+    removeTimers.contains(executorId) || retryRemoveTimers.contains(executorId)\n+  }\n+\n+  /**\n+   * Start a timer to add executors, to expire in `addThreshold` seconds in the first\n+   * round, and `addInterval` seconds in every round thereafter. This is called when\n+   * the scheduler receives new pending tasks and the timer is not already started. This resets\n+   * the value of any existing add timer.\n+   */\n+  def startAddTimer(): Unit = {\n+    val threshold = if (addThresholdCrossed) addInterval else addThreshold\n+    logDebug(s\"Starting add executor timer (to expire in $threshold seconds)\")\n+    addTimer = 0\n+    addTimerEnabled = true\n+  }\n+\n+  /**\n+   * Start a timer to remove the given executor, to expire in `removeThreshold` seconds.\n+   * This is called when an executor registers or finishes running tasks, and the timer is not\n+   * already started. This resets the value of any existing timer to remove this executor.\n+   */\n+  def startRemoveTimer(executorId: String): Unit = {\n+    logDebug(s\"Starting remove executor timer for $executorId \" +\n+      s\"(to expire in $removeThreshold seconds)\")\n+    removeTimers(executorId) = 0"
  }],
  "prId": 2746
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Could you clarify in the comment what does the timer value mean? Is it the absolute time when the timer should trigger, or a timeout (relative to what?)?\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-15T23:48:43Z",
    "diffHunk": "@@ -0,0 +1,496 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically allocates and removes executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks is not\n+ * drained in N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle for K seconds (meaning it has not\n+ * been scheduled to run any tasks), then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * Both add and remove attempts are retried on failure up to a maximum number of times.\n+ *\n+ * The relevant Spark properties include the following:\n+ *\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *\n+ *   spark.dynamicAllocation.addExecutorThreshold - How long before new executors are added (N)\n+ *   spark.dynamicAllocation.addExecutorInterval - How often to add new executors (M)\n+ *   spark.dynamicAllocation.removeExecutorThreshold - How long before an executor is removed (K)\n+ *\n+ *   spark.dynamicAllocation.addExecutorRetryInterval - How often to retry adding executors\n+ *   spark.dynamicAllocation.removeExecutorRetryInterval - How often to retry removing executors\n+ *   spark.dynamicAllocation.maxAddExecutorRetryAttempts - Max retries in re-adding executors\n+ *   spark.dynamicAllocation.maxRemoveExecutorRetryAttempts - Max retries in re-removing executors\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention should only\n+ * arise when new executors register or when existing executors have been removed, both of which\n+ * are relatively rare events with respect to task scheduling. Thus, synchronizing each method on\n+ * the same lock should not be expensive assuming biased locking is enabled in the JVM (on by\n+ * default for Java 6+). This may not be true, however, if the application itself runs multiple\n+ * jobs concurrently.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorAllocationManager(scheduler: TaskSchedulerImpl) extends Logging {\n+  private val conf = scheduler.conf\n+\n+  // Lower and upper bounds on the number of executors. These are required.\n+  private val minNumExecutors = conf.getInt(\"spark.dynamicAllocation.minExecutors\", -1)\n+  private val maxNumExecutors = conf.getInt(\"spark.dynamicAllocation.maxExecutors\", -1)\n+  if (minNumExecutors < 0 || maxNumExecutors < 0) {\n+    throw new SparkException(\"spark.dynamicAllocation.{min/max}Executors must be set!\")\n+  }\n+\n+  // How frequently to add and remove executors (seconds)\n+  private val addThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorThreshold\", 60)\n+  private val addInterval =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorInterval\", addThreshold)\n+  private val addRetryInterval =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorRetryInterval\", addInterval)\n+  private val removeThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorThreshold\", 600)\n+  private val removeRetryInterval =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorRetryInterval\", 300)\n+\n+  // Number of executors to add in the next round\n+  private var numExecutorsToAdd = 1\n+\n+  // Pending executors that have not actually been added/removed yet\n+  private var numExecutorsPendingToAdd = 0\n+  private val executorsPendingToRemove = new mutable.HashSet[String]\n+\n+  // Retry attempts\n+  private var addRetryAttempts = 0\n+  private val removeRetryAttempts = new mutable.HashMap[String, Int]\n+  private val maxAddRetryAttempts =\n+    conf.getInt(\"spark.dynamicAllocation.maxAddExecutorRetryAttempts\", 10)\n+  private val maxRemoveRetryAttempts =\n+    conf.getInt(\"spark.dynamicAllocation.maxRemoveExecutorRetryAttempts\", 10)\n+\n+  // Keep track of all executors here to decouple us from the logic in TaskSchedulerImpl\n+  private val executorIds = new mutable.HashSet[String]\n+\n+  // Timers for keeping track of when to add/remove executors (ms)"
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "Yeah I'll add a comment. Looks like multiple people here find this confusing.\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-16T00:25:29Z",
    "diffHunk": "@@ -0,0 +1,496 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically allocates and removes executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks is not\n+ * drained in N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle for K seconds (meaning it has not\n+ * been scheduled to run any tasks), then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * Both add and remove attempts are retried on failure up to a maximum number of times.\n+ *\n+ * The relevant Spark properties include the following:\n+ *\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *\n+ *   spark.dynamicAllocation.addExecutorThreshold - How long before new executors are added (N)\n+ *   spark.dynamicAllocation.addExecutorInterval - How often to add new executors (M)\n+ *   spark.dynamicAllocation.removeExecutorThreshold - How long before an executor is removed (K)\n+ *\n+ *   spark.dynamicAllocation.addExecutorRetryInterval - How often to retry adding executors\n+ *   spark.dynamicAllocation.removeExecutorRetryInterval - How often to retry removing executors\n+ *   spark.dynamicAllocation.maxAddExecutorRetryAttempts - Max retries in re-adding executors\n+ *   spark.dynamicAllocation.maxRemoveExecutorRetryAttempts - Max retries in re-removing executors\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention should only\n+ * arise when new executors register or when existing executors have been removed, both of which\n+ * are relatively rare events with respect to task scheduling. Thus, synchronizing each method on\n+ * the same lock should not be expensive assuming biased locking is enabled in the JVM (on by\n+ * default for Java 6+). This may not be true, however, if the application itself runs multiple\n+ * jobs concurrently.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorAllocationManager(scheduler: TaskSchedulerImpl) extends Logging {\n+  private val conf = scheduler.conf\n+\n+  // Lower and upper bounds on the number of executors. These are required.\n+  private val minNumExecutors = conf.getInt(\"spark.dynamicAllocation.minExecutors\", -1)\n+  private val maxNumExecutors = conf.getInt(\"spark.dynamicAllocation.maxExecutors\", -1)\n+  if (minNumExecutors < 0 || maxNumExecutors < 0) {\n+    throw new SparkException(\"spark.dynamicAllocation.{min/max}Executors must be set!\")\n+  }\n+\n+  // How frequently to add and remove executors (seconds)\n+  private val addThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorThreshold\", 60)\n+  private val addInterval =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorInterval\", addThreshold)\n+  private val addRetryInterval =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorRetryInterval\", addInterval)\n+  private val removeThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorThreshold\", 600)\n+  private val removeRetryInterval =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorRetryInterval\", 300)\n+\n+  // Number of executors to add in the next round\n+  private var numExecutorsToAdd = 1\n+\n+  // Pending executors that have not actually been added/removed yet\n+  private var numExecutorsPendingToAdd = 0\n+  private val executorsPendingToRemove = new mutable.HashSet[String]\n+\n+  // Retry attempts\n+  private var addRetryAttempts = 0\n+  private val removeRetryAttempts = new mutable.HashMap[String, Int]\n+  private val maxAddRetryAttempts =\n+    conf.getInt(\"spark.dynamicAllocation.maxAddExecutorRetryAttempts\", 10)\n+  private val maxRemoveRetryAttempts =\n+    conf.getInt(\"spark.dynamicAllocation.maxRemoveExecutorRetryAttempts\", 10)\n+\n+  // Keep track of all executors here to decouple us from the logic in TaskSchedulerImpl\n+  private val executorIds = new mutable.HashSet[String]\n+\n+  // Timers for keeping track of when to add/remove executors (ms)"
  }],
  "prId": 2746
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Hmmm... maybe this is explained somewhere else in the code, but why start a remove timer right when the executor is registered? Shouldn't you wait for some of the events described elsewhere to happen for that (like all tasks on that executor finishing)?\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-15T23:49:50Z",
    "diffHunk": "@@ -0,0 +1,496 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically allocates and removes executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks is not\n+ * drained in N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle for K seconds (meaning it has not\n+ * been scheduled to run any tasks), then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * Both add and remove attempts are retried on failure up to a maximum number of times.\n+ *\n+ * The relevant Spark properties include the following:\n+ *\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *\n+ *   spark.dynamicAllocation.addExecutorThreshold - How long before new executors are added (N)\n+ *   spark.dynamicAllocation.addExecutorInterval - How often to add new executors (M)\n+ *   spark.dynamicAllocation.removeExecutorThreshold - How long before an executor is removed (K)\n+ *\n+ *   spark.dynamicAllocation.addExecutorRetryInterval - How often to retry adding executors\n+ *   spark.dynamicAllocation.removeExecutorRetryInterval - How often to retry removing executors\n+ *   spark.dynamicAllocation.maxAddExecutorRetryAttempts - Max retries in re-adding executors\n+ *   spark.dynamicAllocation.maxRemoveExecutorRetryAttempts - Max retries in re-removing executors\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention should only\n+ * arise when new executors register or when existing executors have been removed, both of which\n+ * are relatively rare events with respect to task scheduling. Thus, synchronizing each method on\n+ * the same lock should not be expensive assuming biased locking is enabled in the JVM (on by\n+ * default for Java 6+). This may not be true, however, if the application itself runs multiple\n+ * jobs concurrently.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorAllocationManager(scheduler: TaskSchedulerImpl) extends Logging {\n+  private val conf = scheduler.conf\n+\n+  // Lower and upper bounds on the number of executors. These are required.\n+  private val minNumExecutors = conf.getInt(\"spark.dynamicAllocation.minExecutors\", -1)\n+  private val maxNumExecutors = conf.getInt(\"spark.dynamicAllocation.maxExecutors\", -1)\n+  if (minNumExecutors < 0 || maxNumExecutors < 0) {\n+    throw new SparkException(\"spark.dynamicAllocation.{min/max}Executors must be set!\")\n+  }\n+\n+  // How frequently to add and remove executors (seconds)\n+  private val addThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorThreshold\", 60)\n+  private val addInterval =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorInterval\", addThreshold)\n+  private val addRetryInterval =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorRetryInterval\", addInterval)\n+  private val removeThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorThreshold\", 600)\n+  private val removeRetryInterval =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorRetryInterval\", 300)\n+\n+  // Number of executors to add in the next round\n+  private var numExecutorsToAdd = 1\n+\n+  // Pending executors that have not actually been added/removed yet\n+  private var numExecutorsPendingToAdd = 0\n+  private val executorsPendingToRemove = new mutable.HashSet[String]\n+\n+  // Retry attempts\n+  private var addRetryAttempts = 0\n+  private val removeRetryAttempts = new mutable.HashMap[String, Int]\n+  private val maxAddRetryAttempts =\n+    conf.getInt(\"spark.dynamicAllocation.maxAddExecutorRetryAttempts\", 10)\n+  private val maxRemoveRetryAttempts =\n+    conf.getInt(\"spark.dynamicAllocation.maxRemoveExecutorRetryAttempts\", 10)\n+\n+  // Keep track of all executors here to decouple us from the logic in TaskSchedulerImpl\n+  private val executorIds = new mutable.HashSet[String]\n+\n+  // Timers for keeping track of when to add/remove executors (ms)\n+  private var addTimer = 0\n+  private var addRetryTimer = 0\n+  private val removeTimers = new mutable.HashMap[String, Long]\n+  private val retryRemoveTimers = new mutable.HashMap[String, Long]\n+\n+  // Additional variables used for adding executors\n+  private var addThresholdCrossed = false\n+  private var addTimerEnabled = false\n+  private var addRetryTimerEnabled = false\n+\n+  // Loop interval (ms)\n+  private val intervalMs = 100\n+\n+  // Scheduler backend through which requests to add/remove executors are made\n+  // Note that this assumes the backend has already initialized when this is first used\n+  // Otherwise, an appropriate exception is thrown\n+  private lazy val backend = scheduler.backend match {\n+    case b: CoarseGrainedSchedulerBackend => b\n+    case null =>\n+      throw new SparkException(\"Scheduler backend not initialized yet!\")\n+    case _ =>\n+      throw new SparkException(\n+        \"Dynamic allocation of executors is not applicable to fine-grained schedulers. \" +\n+        \"Please set spark.dynamicAllocation.enabled to false.\")\n+  }\n+\n+  initialize()\n+\n+  private def initialize(): Unit = {\n+    // Keep track of all known executors\n+    scheduler.executorIdToHost.keys.foreach(executorAdded)\n+    startPolling()\n+  }\n+\n+  /**\n+   * Start the main polling thread that keeps track of when to add and remove executors.\n+   * During each interval, this thread checks if any of the timers have expired, and, if\n+   * so, triggers the relevant timer actions.\n+   */\n+  def startPolling(): Unit = {\n+    val thread = new Thread {\n+      override def run() {\n+        while (true) {\n+          try {\n+            if (addTimerEnabled) {\n+              val threshold = if (addThresholdCrossed) addInterval else addThreshold\n+              if (addTimer > threshold * 1000) {\n+                addThresholdCrossed = true\n+                addExecutors()\n+              }\n+            }\n+\n+            if (addRetryTimerEnabled) {\n+              if (addRetryTimer > addRetryInterval * 1000) {\n+                retryAddExecutors()\n+              }\n+            }\n+\n+            removeTimers.foreach { case (id, t) =>\n+              if (t > removeThreshold * 1000) {\n+                removeExecutor(id)\n+              }\n+            }\n+\n+            retryRemoveTimers.foreach { case (id, t) =>\n+              if (t > removeRetryInterval * 1000) {\n+                retryRemoveExecutors(id)\n+              }\n+            }\n+          } catch {\n+            case e: Exception =>\n+              logError(\"Exception encountered in dynamic executor allocation thread!\", e)\n+          } finally {\n+            // Advance all timers that are enabled\n+            Thread.sleep(intervalMs)\n+            if (addTimerEnabled) {\n+              addTimer += intervalMs\n+            }\n+            if (addRetryTimerEnabled) {\n+              addRetryTimer += intervalMs\n+            }\n+            removeTimers.foreach { case (id, _) =>\n+              removeTimers(id) += intervalMs\n+            }\n+            retryRemoveTimers.foreach { case (id, _) =>\n+              retryRemoveTimers(id) += intervalMs\n+            }\n+          }\n+        }\n+      }\n+    }\n+    thread.setName(\"spark-dynamic-executor-allocation\")\n+    thread.setDaemon(true)\n+    thread.start()\n+  }\n+\n+  /**\n+   * Request a number of executors from the scheduler backend.\n+   * This automatically restarts the add timer unless it is explicitly canceled.\n+   */\n+  private def addExecutors(): Unit = synchronized {\n+    // Restart add timer because there are still pending tasks\n+    startAddTimer()\n+\n+    // Wait until the previous round of executors have registered\n+    if (numExecutorsPendingToAdd > 0) {\n+      logInfo(s\"Not adding executors because there are still \" +\n+        s\"$numExecutorsPendingToAdd request(s) in flight\")\n+      numExecutorsToAdd = 1\n+      return\n+    }\n+\n+    // Do not request more executors if we have already reached the upper bound\n+    val numExistingExecutors = executorIds.size + numExecutorsPendingToAdd\n+    if (numExistingExecutors >= maxNumExecutors) {\n+      logInfo(s\"Not adding executors because there are already \" +\n+        s\"$maxNumExecutors executor(s), which is the limit\")\n+      numExecutorsToAdd = 1\n+      return\n+    }\n+\n+    // Request executors with respect to the upper bound\n+    // Start the retry timer in case this addition fails\n+    val actualNumExecutorsToAdd =\n+      math.min(numExistingExecutors + numExecutorsToAdd, maxNumExecutors) - numExistingExecutors\n+    val newTotalExecutors = numExistingExecutors + actualNumExecutorsToAdd\n+    logInfo(s\"Pending tasks are building up! Adding $actualNumExecutorsToAdd \" +\n+      s\"new executor(s) (new total is $newTotalExecutors)\")\n+    numExecutorsToAdd *= 2\n+    numExecutorsPendingToAdd += actualNumExecutorsToAdd\n+    backend.requestExecutors(actualNumExecutorsToAdd)\n+    startAddRetryTimer()\n+  }\n+\n+  /**\n+   * Retry a previous executor request that has not been fulfilled.\n+   * This restarts the retry timer to keep trying up to a maximum number of attempts.\n+   */\n+  private def retryAddExecutors(): Unit = synchronized {\n+    // Do not retry if there are no executors pending to be added (should never happen)\n+    if (numExecutorsPendingToAdd == 0) {\n+      logWarning(\"Attempted to retry adding executors when there are none pending to be added\")\n+      cancelAddRetryTimer()\n+      return\n+    }\n+\n+    // Do not retry if we have already exceeded the maximum number of attempts\n+    addRetryAttempts += 1\n+    if (addRetryAttempts > maxAddRetryAttempts) {\n+      logInfo(s\"Giving up on adding $numExecutorsPendingToAdd executor(s) \" +\n+        s\"after $maxAddRetryAttempts failed attempts\")\n+      numExecutorsPendingToAdd = 0\n+      // Also cancel original add timer because the cluster is not granting us new executors\n+      cancelAddTimer()\n+      return\n+    }\n+\n+    // Retry a previous request, then restart the retry timer in case this retry also fails\n+    logInfo(s\"Previously requested executors have not all registered yet. \" +\n+      s\"Retrying to add $numExecutorsPendingToAdd executor(s) \" +\n+      s\"[attempt $addRetryAttempts/$maxAddRetryAttempts]\")\n+    backend.requestExecutors(numExecutorsPendingToAdd)\n+    startAddRetryTimer()\n+  }\n+\n+  /**\n+   * Request the scheduler backend to decommission the given executor.\n+   * This expires the remove timer unless the executor is kept alive intentionally.\n+   */\n+  private def removeExecutor(executorId: String): Unit = synchronized {\n+    // Do not kill the executor if we are not aware of it (should never happen)\n+    if (!executorIds.contains(executorId)) {\n+      logWarning(s\"Attempted to remove unknown executor $executorId\")\n+      cancelRemoveTimer(executorId)\n+      return\n+    }\n+\n+    // Do not kill the executor again if it is already pending to be killed (should never happen)\n+    if (executorsPendingToRemove.contains(executorId) ||\n+        removeRetryAttempts.contains(executorId) ||\n+        retryRemoveTimers.contains(executorId)) {\n+      logWarning(s\"Executor $executorId is already pending to be removed!\")\n+      cancelRemoveTimer(executorId)\n+      return\n+    }\n+\n+    // Do not kill the executor if we have already reached the lower bound\n+    val numExistingExecutors = executorIds.size - executorsPendingToRemove.size\n+    if (numExistingExecutors - 1 < minNumExecutors) {\n+      logDebug(s\"Not removing idle executor $executorId because there are only $minNumExecutors \" +\n+        \"executor(s) left, which is the limit\")\n+      // Restart the remove timer to keep the executor marked as idle\n+      // Otherwise we won't be able to remove this executor even after new executors have joined\n+      startRemoveTimer(executorId)\n+      return\n+    }\n+\n+    // Send a kill request to the backend for this executor\n+    // Start the retry timer in case this removal fails\n+    logInfo(s\"Removing executor $executorId because it has been idle for \" +\n+      s\"$removeThreshold seconds (new total is ${numExistingExecutors - 1})\")\n+    executorsPendingToRemove.add(executorId)\n+    backend.killExecutor(executorId)\n+    cancelRemoveTimer(executorId)\n+    startRemoveRetryTimer(executorId)\n+  }\n+\n+  /**\n+   * Retry a previous attempt to decommission the given executor.\n+   * This restarts the retry timer to keep trying up to a maximum number of attempts.\n+   */\n+  private def retryRemoveExecutors(executorId: String): Unit = synchronized {\n+    // Do not kill the executor if we are not aware of it (should never happen)\n+    if (!executorIds.contains(executorId)) {\n+      logWarning(s\"Attempted to retry removing unknown executor $executorId\")\n+      cancelRemoveRetryTimer(executorId)\n+      return\n+    }\n+\n+    // Do not retry if this executor is not pending to be killed (should never happen)\n+    if (!executorsPendingToRemove.contains(executorId)) {\n+      logDebug(s\"Attempted to retry removing executor $executorId when it's not to be removed!\")\n+      cancelRemoveRetryTimer(executorId)\n+      return\n+    }\n+\n+    // Do not retry if we have already exceeded the maximum number of attempts\n+    removeRetryAttempts(executorId) =\n+      removeRetryAttempts.getOrElse(executorId, 0) + 1\n+    if (removeRetryAttempts(executorId) > maxRemoveRetryAttempts) {\n+      logInfo(s\"Giving up on removing executor $executorId after \" +\n+        s\"$maxRemoveRetryAttempts failed attempts!\")\n+      executorsPendingToRemove.remove(executorId)\n+      cancelRemoveRetryTimer(executorId)\n+      return\n+    }\n+\n+    // Retry a previous kill request for this executor\n+    // Restart the retry timer in case this retry also fails\n+    logInfo(s\"Retrying previous attempt to remove $executorId \" +\n+      s\"[attempt ${removeRetryAttempts(executorId)}/$maxRemoveRetryAttempts]\")\n+    backend.killExecutor(executorId)\n+    startRemoveRetryTimer(executorId)\n+  }\n+\n+  /**\n+   * Callback for the scheduler to signal that the given executor has been added.\n+   */\n+  def executorAdded(executorId: String): Unit = synchronized {\n+    if (!executorIds.contains(executorId)) {\n+      logDebug(s\"New executor $executorId has registered\")\n+      if (numExecutorsPendingToAdd > 0) {\n+        numExecutorsPendingToAdd -= 1\n+        logDebug(s\"Decremented pending executors to add ($numExecutorsPendingToAdd left)\")\n+        if (numExecutorsPendingToAdd == 0) {\n+          logDebug(\"All previously pending executors have registered!\")\n+          cancelAddRetryTimer()\n+        }\n+      }\n+      executorIds.add(executorId)\n+      startRemoveTimer(executorId)"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "Nevermind, I think I get it after reading the scheduler code. Nevertheless, it would be nice to explain this in the comment (\"the executor is scheduled for removal right away, and its lifetime is prolonged as tasks are scheduled to run on it\" or something).\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-15T23:53:56Z",
    "diffHunk": "@@ -0,0 +1,496 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically allocates and removes executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks is not\n+ * drained in N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle for K seconds (meaning it has not\n+ * been scheduled to run any tasks), then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * Both add and remove attempts are retried on failure up to a maximum number of times.\n+ *\n+ * The relevant Spark properties include the following:\n+ *\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *\n+ *   spark.dynamicAllocation.addExecutorThreshold - How long before new executors are added (N)\n+ *   spark.dynamicAllocation.addExecutorInterval - How often to add new executors (M)\n+ *   spark.dynamicAllocation.removeExecutorThreshold - How long before an executor is removed (K)\n+ *\n+ *   spark.dynamicAllocation.addExecutorRetryInterval - How often to retry adding executors\n+ *   spark.dynamicAllocation.removeExecutorRetryInterval - How often to retry removing executors\n+ *   spark.dynamicAllocation.maxAddExecutorRetryAttempts - Max retries in re-adding executors\n+ *   spark.dynamicAllocation.maxRemoveExecutorRetryAttempts - Max retries in re-removing executors\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention should only\n+ * arise when new executors register or when existing executors have been removed, both of which\n+ * are relatively rare events with respect to task scheduling. Thus, synchronizing each method on\n+ * the same lock should not be expensive assuming biased locking is enabled in the JVM (on by\n+ * default for Java 6+). This may not be true, however, if the application itself runs multiple\n+ * jobs concurrently.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorAllocationManager(scheduler: TaskSchedulerImpl) extends Logging {\n+  private val conf = scheduler.conf\n+\n+  // Lower and upper bounds on the number of executors. These are required.\n+  private val minNumExecutors = conf.getInt(\"spark.dynamicAllocation.minExecutors\", -1)\n+  private val maxNumExecutors = conf.getInt(\"spark.dynamicAllocation.maxExecutors\", -1)\n+  if (minNumExecutors < 0 || maxNumExecutors < 0) {\n+    throw new SparkException(\"spark.dynamicAllocation.{min/max}Executors must be set!\")\n+  }\n+\n+  // How frequently to add and remove executors (seconds)\n+  private val addThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorThreshold\", 60)\n+  private val addInterval =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorInterval\", addThreshold)\n+  private val addRetryInterval =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorRetryInterval\", addInterval)\n+  private val removeThreshold =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorThreshold\", 600)\n+  private val removeRetryInterval =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorRetryInterval\", 300)\n+\n+  // Number of executors to add in the next round\n+  private var numExecutorsToAdd = 1\n+\n+  // Pending executors that have not actually been added/removed yet\n+  private var numExecutorsPendingToAdd = 0\n+  private val executorsPendingToRemove = new mutable.HashSet[String]\n+\n+  // Retry attempts\n+  private var addRetryAttempts = 0\n+  private val removeRetryAttempts = new mutable.HashMap[String, Int]\n+  private val maxAddRetryAttempts =\n+    conf.getInt(\"spark.dynamicAllocation.maxAddExecutorRetryAttempts\", 10)\n+  private val maxRemoveRetryAttempts =\n+    conf.getInt(\"spark.dynamicAllocation.maxRemoveExecutorRetryAttempts\", 10)\n+\n+  // Keep track of all executors here to decouple us from the logic in TaskSchedulerImpl\n+  private val executorIds = new mutable.HashSet[String]\n+\n+  // Timers for keeping track of when to add/remove executors (ms)\n+  private var addTimer = 0\n+  private var addRetryTimer = 0\n+  private val removeTimers = new mutable.HashMap[String, Long]\n+  private val retryRemoveTimers = new mutable.HashMap[String, Long]\n+\n+  // Additional variables used for adding executors\n+  private var addThresholdCrossed = false\n+  private var addTimerEnabled = false\n+  private var addRetryTimerEnabled = false\n+\n+  // Loop interval (ms)\n+  private val intervalMs = 100\n+\n+  // Scheduler backend through which requests to add/remove executors are made\n+  // Note that this assumes the backend has already initialized when this is first used\n+  // Otherwise, an appropriate exception is thrown\n+  private lazy val backend = scheduler.backend match {\n+    case b: CoarseGrainedSchedulerBackend => b\n+    case null =>\n+      throw new SparkException(\"Scheduler backend not initialized yet!\")\n+    case _ =>\n+      throw new SparkException(\n+        \"Dynamic allocation of executors is not applicable to fine-grained schedulers. \" +\n+        \"Please set spark.dynamicAllocation.enabled to false.\")\n+  }\n+\n+  initialize()\n+\n+  private def initialize(): Unit = {\n+    // Keep track of all known executors\n+    scheduler.executorIdToHost.keys.foreach(executorAdded)\n+    startPolling()\n+  }\n+\n+  /**\n+   * Start the main polling thread that keeps track of when to add and remove executors.\n+   * During each interval, this thread checks if any of the timers have expired, and, if\n+   * so, triggers the relevant timer actions.\n+   */\n+  def startPolling(): Unit = {\n+    val thread = new Thread {\n+      override def run() {\n+        while (true) {\n+          try {\n+            if (addTimerEnabled) {\n+              val threshold = if (addThresholdCrossed) addInterval else addThreshold\n+              if (addTimer > threshold * 1000) {\n+                addThresholdCrossed = true\n+                addExecutors()\n+              }\n+            }\n+\n+            if (addRetryTimerEnabled) {\n+              if (addRetryTimer > addRetryInterval * 1000) {\n+                retryAddExecutors()\n+              }\n+            }\n+\n+            removeTimers.foreach { case (id, t) =>\n+              if (t > removeThreshold * 1000) {\n+                removeExecutor(id)\n+              }\n+            }\n+\n+            retryRemoveTimers.foreach { case (id, t) =>\n+              if (t > removeRetryInterval * 1000) {\n+                retryRemoveExecutors(id)\n+              }\n+            }\n+          } catch {\n+            case e: Exception =>\n+              logError(\"Exception encountered in dynamic executor allocation thread!\", e)\n+          } finally {\n+            // Advance all timers that are enabled\n+            Thread.sleep(intervalMs)\n+            if (addTimerEnabled) {\n+              addTimer += intervalMs\n+            }\n+            if (addRetryTimerEnabled) {\n+              addRetryTimer += intervalMs\n+            }\n+            removeTimers.foreach { case (id, _) =>\n+              removeTimers(id) += intervalMs\n+            }\n+            retryRemoveTimers.foreach { case (id, _) =>\n+              retryRemoveTimers(id) += intervalMs\n+            }\n+          }\n+        }\n+      }\n+    }\n+    thread.setName(\"spark-dynamic-executor-allocation\")\n+    thread.setDaemon(true)\n+    thread.start()\n+  }\n+\n+  /**\n+   * Request a number of executors from the scheduler backend.\n+   * This automatically restarts the add timer unless it is explicitly canceled.\n+   */\n+  private def addExecutors(): Unit = synchronized {\n+    // Restart add timer because there are still pending tasks\n+    startAddTimer()\n+\n+    // Wait until the previous round of executors have registered\n+    if (numExecutorsPendingToAdd > 0) {\n+      logInfo(s\"Not adding executors because there are still \" +\n+        s\"$numExecutorsPendingToAdd request(s) in flight\")\n+      numExecutorsToAdd = 1\n+      return\n+    }\n+\n+    // Do not request more executors if we have already reached the upper bound\n+    val numExistingExecutors = executorIds.size + numExecutorsPendingToAdd\n+    if (numExistingExecutors >= maxNumExecutors) {\n+      logInfo(s\"Not adding executors because there are already \" +\n+        s\"$maxNumExecutors executor(s), which is the limit\")\n+      numExecutorsToAdd = 1\n+      return\n+    }\n+\n+    // Request executors with respect to the upper bound\n+    // Start the retry timer in case this addition fails\n+    val actualNumExecutorsToAdd =\n+      math.min(numExistingExecutors + numExecutorsToAdd, maxNumExecutors) - numExistingExecutors\n+    val newTotalExecutors = numExistingExecutors + actualNumExecutorsToAdd\n+    logInfo(s\"Pending tasks are building up! Adding $actualNumExecutorsToAdd \" +\n+      s\"new executor(s) (new total is $newTotalExecutors)\")\n+    numExecutorsToAdd *= 2\n+    numExecutorsPendingToAdd += actualNumExecutorsToAdd\n+    backend.requestExecutors(actualNumExecutorsToAdd)\n+    startAddRetryTimer()\n+  }\n+\n+  /**\n+   * Retry a previous executor request that has not been fulfilled.\n+   * This restarts the retry timer to keep trying up to a maximum number of attempts.\n+   */\n+  private def retryAddExecutors(): Unit = synchronized {\n+    // Do not retry if there are no executors pending to be added (should never happen)\n+    if (numExecutorsPendingToAdd == 0) {\n+      logWarning(\"Attempted to retry adding executors when there are none pending to be added\")\n+      cancelAddRetryTimer()\n+      return\n+    }\n+\n+    // Do not retry if we have already exceeded the maximum number of attempts\n+    addRetryAttempts += 1\n+    if (addRetryAttempts > maxAddRetryAttempts) {\n+      logInfo(s\"Giving up on adding $numExecutorsPendingToAdd executor(s) \" +\n+        s\"after $maxAddRetryAttempts failed attempts\")\n+      numExecutorsPendingToAdd = 0\n+      // Also cancel original add timer because the cluster is not granting us new executors\n+      cancelAddTimer()\n+      return\n+    }\n+\n+    // Retry a previous request, then restart the retry timer in case this retry also fails\n+    logInfo(s\"Previously requested executors have not all registered yet. \" +\n+      s\"Retrying to add $numExecutorsPendingToAdd executor(s) \" +\n+      s\"[attempt $addRetryAttempts/$maxAddRetryAttempts]\")\n+    backend.requestExecutors(numExecutorsPendingToAdd)\n+    startAddRetryTimer()\n+  }\n+\n+  /**\n+   * Request the scheduler backend to decommission the given executor.\n+   * This expires the remove timer unless the executor is kept alive intentionally.\n+   */\n+  private def removeExecutor(executorId: String): Unit = synchronized {\n+    // Do not kill the executor if we are not aware of it (should never happen)\n+    if (!executorIds.contains(executorId)) {\n+      logWarning(s\"Attempted to remove unknown executor $executorId\")\n+      cancelRemoveTimer(executorId)\n+      return\n+    }\n+\n+    // Do not kill the executor again if it is already pending to be killed (should never happen)\n+    if (executorsPendingToRemove.contains(executorId) ||\n+        removeRetryAttempts.contains(executorId) ||\n+        retryRemoveTimers.contains(executorId)) {\n+      logWarning(s\"Executor $executorId is already pending to be removed!\")\n+      cancelRemoveTimer(executorId)\n+      return\n+    }\n+\n+    // Do not kill the executor if we have already reached the lower bound\n+    val numExistingExecutors = executorIds.size - executorsPendingToRemove.size\n+    if (numExistingExecutors - 1 < minNumExecutors) {\n+      logDebug(s\"Not removing idle executor $executorId because there are only $minNumExecutors \" +\n+        \"executor(s) left, which is the limit\")\n+      // Restart the remove timer to keep the executor marked as idle\n+      // Otherwise we won't be able to remove this executor even after new executors have joined\n+      startRemoveTimer(executorId)\n+      return\n+    }\n+\n+    // Send a kill request to the backend for this executor\n+    // Start the retry timer in case this removal fails\n+    logInfo(s\"Removing executor $executorId because it has been idle for \" +\n+      s\"$removeThreshold seconds (new total is ${numExistingExecutors - 1})\")\n+    executorsPendingToRemove.add(executorId)\n+    backend.killExecutor(executorId)\n+    cancelRemoveTimer(executorId)\n+    startRemoveRetryTimer(executorId)\n+  }\n+\n+  /**\n+   * Retry a previous attempt to decommission the given executor.\n+   * This restarts the retry timer to keep trying up to a maximum number of attempts.\n+   */\n+  private def retryRemoveExecutors(executorId: String): Unit = synchronized {\n+    // Do not kill the executor if we are not aware of it (should never happen)\n+    if (!executorIds.contains(executorId)) {\n+      logWarning(s\"Attempted to retry removing unknown executor $executorId\")\n+      cancelRemoveRetryTimer(executorId)\n+      return\n+    }\n+\n+    // Do not retry if this executor is not pending to be killed (should never happen)\n+    if (!executorsPendingToRemove.contains(executorId)) {\n+      logDebug(s\"Attempted to retry removing executor $executorId when it's not to be removed!\")\n+      cancelRemoveRetryTimer(executorId)\n+      return\n+    }\n+\n+    // Do not retry if we have already exceeded the maximum number of attempts\n+    removeRetryAttempts(executorId) =\n+      removeRetryAttempts.getOrElse(executorId, 0) + 1\n+    if (removeRetryAttempts(executorId) > maxRemoveRetryAttempts) {\n+      logInfo(s\"Giving up on removing executor $executorId after \" +\n+        s\"$maxRemoveRetryAttempts failed attempts!\")\n+      executorsPendingToRemove.remove(executorId)\n+      cancelRemoveRetryTimer(executorId)\n+      return\n+    }\n+\n+    // Retry a previous kill request for this executor\n+    // Restart the retry timer in case this retry also fails\n+    logInfo(s\"Retrying previous attempt to remove $executorId \" +\n+      s\"[attempt ${removeRetryAttempts(executorId)}/$maxRemoveRetryAttempts]\")\n+    backend.killExecutor(executorId)\n+    startRemoveRetryTimer(executorId)\n+  }\n+\n+  /**\n+   * Callback for the scheduler to signal that the given executor has been added.\n+   */\n+  def executorAdded(executorId: String): Unit = synchronized {\n+    if (!executorIds.contains(executorId)) {\n+      logDebug(s\"New executor $executorId has registered\")\n+      if (numExecutorsPendingToAdd > 0) {\n+        numExecutorsPendingToAdd -= 1\n+        logDebug(s\"Decremented pending executors to add ($numExecutorsPendingToAdd left)\")\n+        if (numExecutorsPendingToAdd == 0) {\n+          logDebug(\"All previously pending executors have registered!\")\n+          cancelAddRetryTimer()\n+        }\n+      }\n+      executorIds.add(executorId)\n+      startRemoveTimer(executorId)"
  }],
  "prId": 2746
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "can you declare the type here?\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-22T05:03:48Z",
    "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically allocates and removes executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks is not\n+ * drained in N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle for K seconds (meaning it has not\n+ * been scheduled to run any tasks), then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * There is no retry logic in either case. Because the requests to the cluster manager are\n+ * asynchronous, this class does not know whether a request has been granted until later. For\n+ * this reason, both add and remove are treated as best-effort only.\n+ *\n+ * The relevant Spark properties include the following:\n+ *\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *\n+ *   spark.dynamicAllocation.addExecutorThresholdSeconds - How long before new executors are added\n+ *   spark.dynamicAllocation.addExecutorIntervalSeconds - How often to add new executors\n+ *   spark.dynamicAllocation.removeExecutorThresholdSeconds - How long before an executor is removed\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention should only\n+ * arise when new executors register or when existing executors are removed, both of which are\n+ * relatively rare events with respect to task scheduling. Thus, synchronizing each method on the\n+ * same lock should not be expensive assuming biased locking is enabled in the JVM (on by default\n+ * for Java 6+). This may not be true, however, if the application itself runs multiple jobs\n+ * concurrently.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorAllocationManager(scheduler: TaskSchedulerImpl) extends Logging {\n+  import ExecutorAllocationManager._\n+\n+  private val conf = scheduler.conf\n+\n+  // Lower and upper bounds on the number of executors. These are required.\n+  private val minNumExecutors = conf.getInt(\"spark.dynamicAllocation.minExecutors\", -1)\n+  private val maxNumExecutors = conf.getInt(\"spark.dynamicAllocation.maxExecutors\", -1)\n+  if (minNumExecutors < 0 || maxNumExecutors < 0) {\n+    throw new SparkException(\"spark.dynamicAllocation.{min/max}Executors must be set!\")\n+  }\n+\n+  // How frequently to add and remove executors (seconds)\n+  private val addThresholdSeconds =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorThresholdSeconds\", 60)\n+  private val addIntervalSeconds =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorIntervalSeconds\", addThresholdSeconds)\n+  private val removeThresholdSeconds =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorThresholdSeconds\", 600)\n+\n+  // Number of executors to add in the next round\n+  private var numExecutorsToAdd = 1\n+\n+  // Number of executors that have been requested but have not registered yet\n+  private var numExecutorsPendingToAdd = 0\n+\n+  // Executors that have been requested to be removed but have not been killed yet\n+  private val executorsPendingToRemove = new mutable.HashSet[String]\n+\n+  // Keep track of all executors here to decouple us from the logic in TaskSchedulerImpl\n+  private val executorIds = new mutable.HashSet[String]\n+\n+  // A timestamp of when the add timer should be triggered, or NOT_STARTED if the timer is not\n+  // started. This timer is started when there are pending tasks built up, and canceled when\n+  // there are no more pending tasks.\n+  private var addTime = NOT_STARTED"
  }],
  "prId": 2746
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "Using the word \"timer\" here doesn't convey much information. What about saying:\n\n```\nIf set, this value indicates the absolute time after which we should start adding executors.\nIt is updated in real time based on the current state of the task queue.\n```\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-22T05:05:57Z",
    "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically allocates and removes executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks is not\n+ * drained in N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle for K seconds (meaning it has not\n+ * been scheduled to run any tasks), then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * There is no retry logic in either case. Because the requests to the cluster manager are\n+ * asynchronous, this class does not know whether a request has been granted until later. For\n+ * this reason, both add and remove are treated as best-effort only.\n+ *\n+ * The relevant Spark properties include the following:\n+ *\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *\n+ *   spark.dynamicAllocation.addExecutorThresholdSeconds - How long before new executors are added\n+ *   spark.dynamicAllocation.addExecutorIntervalSeconds - How often to add new executors\n+ *   spark.dynamicAllocation.removeExecutorThresholdSeconds - How long before an executor is removed\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention should only\n+ * arise when new executors register or when existing executors are removed, both of which are\n+ * relatively rare events with respect to task scheduling. Thus, synchronizing each method on the\n+ * same lock should not be expensive assuming biased locking is enabled in the JVM (on by default\n+ * for Java 6+). This may not be true, however, if the application itself runs multiple jobs\n+ * concurrently.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorAllocationManager(scheduler: TaskSchedulerImpl) extends Logging {\n+  import ExecutorAllocationManager._\n+\n+  private val conf = scheduler.conf\n+\n+  // Lower and upper bounds on the number of executors. These are required.\n+  private val minNumExecutors = conf.getInt(\"spark.dynamicAllocation.minExecutors\", -1)\n+  private val maxNumExecutors = conf.getInt(\"spark.dynamicAllocation.maxExecutors\", -1)\n+  if (minNumExecutors < 0 || maxNumExecutors < 0) {\n+    throw new SparkException(\"spark.dynamicAllocation.{min/max}Executors must be set!\")\n+  }\n+\n+  // How frequently to add and remove executors (seconds)\n+  private val addThresholdSeconds =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorThresholdSeconds\", 60)\n+  private val addIntervalSeconds =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorIntervalSeconds\", addThresholdSeconds)\n+  private val removeThresholdSeconds =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorThresholdSeconds\", 600)\n+\n+  // Number of executors to add in the next round\n+  private var numExecutorsToAdd = 1\n+\n+  // Number of executors that have been requested but have not registered yet\n+  private var numExecutorsPendingToAdd = 0\n+\n+  // Executors that have been requested to be removed but have not been killed yet\n+  private val executorsPendingToRemove = new mutable.HashSet[String]\n+\n+  // Keep track of all executors here to decouple us from the logic in TaskSchedulerImpl\n+  private val executorIds = new mutable.HashSet[String]\n+\n+  // A timestamp of when the add timer should be triggered, or NOT_STARTED if the timer is not"
  }],
  "prId": 2746
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "I also wonder if the word \"timer\" should not be used here. To me \"timer\" means a clock that is counting up or down as time passes. Here it means an absolute time at which an event should trigger. I think in reality \"timer\" can mean both things (http://www.merriam-webster.com/dictionary/timer), but it might be good to avoid the name.\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-22T05:08:25Z",
    "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically allocates and removes executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks is not\n+ * drained in N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle for K seconds (meaning it has not\n+ * been scheduled to run any tasks), then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * There is no retry logic in either case. Because the requests to the cluster manager are\n+ * asynchronous, this class does not know whether a request has been granted until later. For\n+ * this reason, both add and remove are treated as best-effort only.\n+ *\n+ * The relevant Spark properties include the following:\n+ *\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *\n+ *   spark.dynamicAllocation.addExecutorThresholdSeconds - How long before new executors are added\n+ *   spark.dynamicAllocation.addExecutorIntervalSeconds - How often to add new executors\n+ *   spark.dynamicAllocation.removeExecutorThresholdSeconds - How long before an executor is removed\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention should only\n+ * arise when new executors register or when existing executors are removed, both of which are\n+ * relatively rare events with respect to task scheduling. Thus, synchronizing each method on the\n+ * same lock should not be expensive assuming biased locking is enabled in the JVM (on by default\n+ * for Java 6+). This may not be true, however, if the application itself runs multiple jobs\n+ * concurrently.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorAllocationManager(scheduler: TaskSchedulerImpl) extends Logging {\n+  import ExecutorAllocationManager._\n+\n+  private val conf = scheduler.conf\n+\n+  // Lower and upper bounds on the number of executors. These are required.\n+  private val minNumExecutors = conf.getInt(\"spark.dynamicAllocation.minExecutors\", -1)\n+  private val maxNumExecutors = conf.getInt(\"spark.dynamicAllocation.maxExecutors\", -1)\n+  if (minNumExecutors < 0 || maxNumExecutors < 0) {\n+    throw new SparkException(\"spark.dynamicAllocation.{min/max}Executors must be set!\")\n+  }\n+\n+  // How frequently to add and remove executors (seconds)\n+  private val addThresholdSeconds =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorThresholdSeconds\", 60)\n+  private val addIntervalSeconds =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorIntervalSeconds\", addThresholdSeconds)\n+  private val removeThresholdSeconds =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorThresholdSeconds\", 600)\n+\n+  // Number of executors to add in the next round\n+  private var numExecutorsToAdd = 1\n+\n+  // Number of executors that have been requested but have not registered yet\n+  private var numExecutorsPendingToAdd = 0\n+\n+  // Executors that have been requested to be removed but have not been killed yet\n+  private val executorsPendingToRemove = new mutable.HashSet[String]\n+\n+  // Keep track of all executors here to decouple us from the logic in TaskSchedulerImpl\n+  private val executorIds = new mutable.HashSet[String]\n+\n+  // A timestamp of when the add timer should be triggered, or NOT_STARTED if the timer is not\n+  // started. This timer is started when there are pending tasks built up, and canceled when\n+  // there are no more pending tasks.\n+  private var addTime = NOT_STARTED\n+\n+  // A timestamp for each executor of when the remove timer for that executor should be triggered."
  }],
  "prId": 2746
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "will this be replaced by directly adding against the SparkContext? I think that would be nicer since that way it's clear how users can implement similar functionality.\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-22T05:11:37Z",
    "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically allocates and removes executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks is not\n+ * drained in N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle for K seconds (meaning it has not\n+ * been scheduled to run any tasks), then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * There is no retry logic in either case. Because the requests to the cluster manager are\n+ * asynchronous, this class does not know whether a request has been granted until later. For\n+ * this reason, both add and remove are treated as best-effort only.\n+ *\n+ * The relevant Spark properties include the following:\n+ *\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *\n+ *   spark.dynamicAllocation.addExecutorThresholdSeconds - How long before new executors are added\n+ *   spark.dynamicAllocation.addExecutorIntervalSeconds - How often to add new executors\n+ *   spark.dynamicAllocation.removeExecutorThresholdSeconds - How long before an executor is removed\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention should only\n+ * arise when new executors register or when existing executors are removed, both of which are\n+ * relatively rare events with respect to task scheduling. Thus, synchronizing each method on the\n+ * same lock should not be expensive assuming biased locking is enabled in the JVM (on by default\n+ * for Java 6+). This may not be true, however, if the application itself runs multiple jobs\n+ * concurrently.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorAllocationManager(scheduler: TaskSchedulerImpl) extends Logging {\n+  import ExecutorAllocationManager._\n+\n+  private val conf = scheduler.conf\n+\n+  // Lower and upper bounds on the number of executors. These are required.\n+  private val minNumExecutors = conf.getInt(\"spark.dynamicAllocation.minExecutors\", -1)\n+  private val maxNumExecutors = conf.getInt(\"spark.dynamicAllocation.maxExecutors\", -1)\n+  if (minNumExecutors < 0 || maxNumExecutors < 0) {\n+    throw new SparkException(\"spark.dynamicAllocation.{min/max}Executors must be set!\")\n+  }\n+\n+  // How frequently to add and remove executors (seconds)\n+  private val addThresholdSeconds =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorThresholdSeconds\", 60)\n+  private val addIntervalSeconds =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorIntervalSeconds\", addThresholdSeconds)\n+  private val removeThresholdSeconds =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorThresholdSeconds\", 600)\n+\n+  // Number of executors to add in the next round\n+  private var numExecutorsToAdd = 1\n+\n+  // Number of executors that have been requested but have not registered yet\n+  private var numExecutorsPendingToAdd = 0\n+\n+  // Executors that have been requested to be removed but have not been killed yet\n+  private val executorsPendingToRemove = new mutable.HashSet[String]\n+\n+  // Keep track of all executors here to decouple us from the logic in TaskSchedulerImpl\n+  private val executorIds = new mutable.HashSet[String]\n+\n+  // A timestamp of when the add timer should be triggered, or NOT_STARTED if the timer is not\n+  // started. This timer is started when there are pending tasks built up, and canceled when\n+  // there are no more pending tasks.\n+  private var addTime = NOT_STARTED\n+\n+  // A timestamp for each executor of when the remove timer for that executor should be triggered.\n+  // Each remove timer is started when the executor first registers or when the executor finishes\n+  // running a task, and canceled when the executor is scheduled to run a new task.\n+  private val removeTimes = new mutable.HashMap[String, Long]\n+\n+  // A timestamp of when all pending add requests should expire\n+  private var pendingAddExpirationTime = NOT_STARTED\n+\n+  // A timestamp for each executor of when the pending remove request for the executor should expire\n+  private val pendingRemoveExpirationTimes = new mutable.HashMap[String, Long]\n+\n+  // How long before expiring pending requests to add or remove executors (seconds)\n+  private val pendingAddTimeoutSeconds = 300 // 5 min\n+  private val pendingRemoveTimeoutSeconds = 300\n+\n+  // Polling loop interval (ms)\n+  private val intervalMillis = 100\n+\n+  // Scheduler backend through which requests to add/remove executors are made\n+  // Note that this assumes the backend has already initialized when this is first used\n+  // Otherwise, an appropriate exception is thrown\n+  private lazy val backend = scheduler.backend match {"
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "Yeah I'll do that when I merge this with #2840 (whichever goes in later)\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-24T03:11:32Z",
    "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically allocates and removes executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks is not\n+ * drained in N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle for K seconds (meaning it has not\n+ * been scheduled to run any tasks), then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * There is no retry logic in either case. Because the requests to the cluster manager are\n+ * asynchronous, this class does not know whether a request has been granted until later. For\n+ * this reason, both add and remove are treated as best-effort only.\n+ *\n+ * The relevant Spark properties include the following:\n+ *\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *\n+ *   spark.dynamicAllocation.addExecutorThresholdSeconds - How long before new executors are added\n+ *   spark.dynamicAllocation.addExecutorIntervalSeconds - How often to add new executors\n+ *   spark.dynamicAllocation.removeExecutorThresholdSeconds - How long before an executor is removed\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention should only\n+ * arise when new executors register or when existing executors are removed, both of which are\n+ * relatively rare events with respect to task scheduling. Thus, synchronizing each method on the\n+ * same lock should not be expensive assuming biased locking is enabled in the JVM (on by default\n+ * for Java 6+). This may not be true, however, if the application itself runs multiple jobs\n+ * concurrently.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorAllocationManager(scheduler: TaskSchedulerImpl) extends Logging {\n+  import ExecutorAllocationManager._\n+\n+  private val conf = scheduler.conf\n+\n+  // Lower and upper bounds on the number of executors. These are required.\n+  private val minNumExecutors = conf.getInt(\"spark.dynamicAllocation.minExecutors\", -1)\n+  private val maxNumExecutors = conf.getInt(\"spark.dynamicAllocation.maxExecutors\", -1)\n+  if (minNumExecutors < 0 || maxNumExecutors < 0) {\n+    throw new SparkException(\"spark.dynamicAllocation.{min/max}Executors must be set!\")\n+  }\n+\n+  // How frequently to add and remove executors (seconds)\n+  private val addThresholdSeconds =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorThresholdSeconds\", 60)\n+  private val addIntervalSeconds =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorIntervalSeconds\", addThresholdSeconds)\n+  private val removeThresholdSeconds =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorThresholdSeconds\", 600)\n+\n+  // Number of executors to add in the next round\n+  private var numExecutorsToAdd = 1\n+\n+  // Number of executors that have been requested but have not registered yet\n+  private var numExecutorsPendingToAdd = 0\n+\n+  // Executors that have been requested to be removed but have not been killed yet\n+  private val executorsPendingToRemove = new mutable.HashSet[String]\n+\n+  // Keep track of all executors here to decouple us from the logic in TaskSchedulerImpl\n+  private val executorIds = new mutable.HashSet[String]\n+\n+  // A timestamp of when the add timer should be triggered, or NOT_STARTED if the timer is not\n+  // started. This timer is started when there are pending tasks built up, and canceled when\n+  // there are no more pending tasks.\n+  private var addTime = NOT_STARTED\n+\n+  // A timestamp for each executor of when the remove timer for that executor should be triggered.\n+  // Each remove timer is started when the executor first registers or when the executor finishes\n+  // running a task, and canceled when the executor is scheduled to run a new task.\n+  private val removeTimes = new mutable.HashMap[String, Long]\n+\n+  // A timestamp of when all pending add requests should expire\n+  private var pendingAddExpirationTime = NOT_STARTED\n+\n+  // A timestamp for each executor of when the pending remove request for the executor should expire\n+  private val pendingRemoveExpirationTimes = new mutable.HashMap[String, Long]\n+\n+  // How long before expiring pending requests to add or remove executors (seconds)\n+  private val pendingAddTimeoutSeconds = 300 // 5 min\n+  private val pendingRemoveTimeoutSeconds = 300\n+\n+  // Polling loop interval (ms)\n+  private val intervalMillis = 100\n+\n+  // Scheduler backend through which requests to add/remove executors are made\n+  // Note that this assumes the backend has already initialized when this is first used\n+  // Otherwise, an appropriate exception is thrown\n+  private lazy val backend = scheduler.backend match {"
  }],
  "prId": 2746
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "Here it would be good to log both the number of pending executors and the current number.\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-22T05:30:54Z",
    "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically allocates and removes executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks is not\n+ * drained in N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle for K seconds (meaning it has not\n+ * been scheduled to run any tasks), then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * There is no retry logic in either case. Because the requests to the cluster manager are\n+ * asynchronous, this class does not know whether a request has been granted until later. For\n+ * this reason, both add and remove are treated as best-effort only.\n+ *\n+ * The relevant Spark properties include the following:\n+ *\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *\n+ *   spark.dynamicAllocation.addExecutorThresholdSeconds - How long before new executors are added\n+ *   spark.dynamicAllocation.addExecutorIntervalSeconds - How often to add new executors\n+ *   spark.dynamicAllocation.removeExecutorThresholdSeconds - How long before an executor is removed\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention should only\n+ * arise when new executors register or when existing executors are removed, both of which are\n+ * relatively rare events with respect to task scheduling. Thus, synchronizing each method on the\n+ * same lock should not be expensive assuming biased locking is enabled in the JVM (on by default\n+ * for Java 6+). This may not be true, however, if the application itself runs multiple jobs\n+ * concurrently.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorAllocationManager(scheduler: TaskSchedulerImpl) extends Logging {\n+  import ExecutorAllocationManager._\n+\n+  private val conf = scheduler.conf\n+\n+  // Lower and upper bounds on the number of executors. These are required.\n+  private val minNumExecutors = conf.getInt(\"spark.dynamicAllocation.minExecutors\", -1)\n+  private val maxNumExecutors = conf.getInt(\"spark.dynamicAllocation.maxExecutors\", -1)\n+  if (minNumExecutors < 0 || maxNumExecutors < 0) {\n+    throw new SparkException(\"spark.dynamicAllocation.{min/max}Executors must be set!\")\n+  }\n+\n+  // How frequently to add and remove executors (seconds)\n+  private val addThresholdSeconds =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorThresholdSeconds\", 60)\n+  private val addIntervalSeconds =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorIntervalSeconds\", addThresholdSeconds)\n+  private val removeThresholdSeconds =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorThresholdSeconds\", 600)\n+\n+  // Number of executors to add in the next round\n+  private var numExecutorsToAdd = 1\n+\n+  // Number of executors that have been requested but have not registered yet\n+  private var numExecutorsPendingToAdd = 0\n+\n+  // Executors that have been requested to be removed but have not been killed yet\n+  private val executorsPendingToRemove = new mutable.HashSet[String]\n+\n+  // Keep track of all executors here to decouple us from the logic in TaskSchedulerImpl\n+  private val executorIds = new mutable.HashSet[String]\n+\n+  // A timestamp of when the add timer should be triggered, or NOT_STARTED if the timer is not\n+  // started. This timer is started when there are pending tasks built up, and canceled when\n+  // there are no more pending tasks.\n+  private var addTime = NOT_STARTED\n+\n+  // A timestamp for each executor of when the remove timer for that executor should be triggered.\n+  // Each remove timer is started when the executor first registers or when the executor finishes\n+  // running a task, and canceled when the executor is scheduled to run a new task.\n+  private val removeTimes = new mutable.HashMap[String, Long]\n+\n+  // A timestamp of when all pending add requests should expire\n+  private var pendingAddExpirationTime = NOT_STARTED\n+\n+  // A timestamp for each executor of when the pending remove request for the executor should expire\n+  private val pendingRemoveExpirationTimes = new mutable.HashMap[String, Long]\n+\n+  // How long before expiring pending requests to add or remove executors (seconds)\n+  private val pendingAddTimeoutSeconds = 300 // 5 min\n+  private val pendingRemoveTimeoutSeconds = 300\n+\n+  // Polling loop interval (ms)\n+  private val intervalMillis = 100\n+\n+  // Scheduler backend through which requests to add/remove executors are made\n+  // Note that this assumes the backend has already initialized when this is first used\n+  // Otherwise, an appropriate exception is thrown\n+  private lazy val backend = scheduler.backend match {\n+    case b: CoarseGrainedSchedulerBackend => b\n+    case null =>\n+      throw new SparkException(\"Scheduler backend not initialized yet!\")\n+    case _ =>\n+      throw new SparkException(\n+        \"Dynamic allocation of executors is not applicable to fine-grained schedulers. \" +\n+        \"Please set spark.dynamicAllocation.enabled to false.\")\n+  }\n+\n+  initialize()\n+\n+  /**\n+   * Start the main polling thread that keeps track of when to add and remove executors.\n+   * During each loop interval, this thread checks if any of the timers have timed out, and,\n+   * if so, triggers the relevant timer actions.\n+   */\n+  def initialize(): Unit = {\n+    val thread = new Thread {\n+      override def run(): Unit = {\n+        while (true) {\n+          ExecutorAllocationManager.this.synchronized {\n+            val now = System.currentTimeMillis\n+            try {\n+              // If the add timer has timed out, add executors and refresh the timer\n+              if (addTime != NOT_STARTED && now >= addTime) {\n+                addExecutors()\n+                logDebug(s\"Restarting add executor timer \" +\n+                  s\"(to be triggered in $addIntervalSeconds seconds)\")\n+                addTime += addIntervalSeconds * 1000\n+              }\n+\n+              // If a remove timer has timed out, remove the executor and cancel the timer\n+              removeTimes.foreach { case (executorId, triggerTime) =>\n+                if (now > triggerTime) {\n+                  removeExecutor(executorId)\n+                  cancelRemoveTimer(executorId)\n+                }\n+              }\n+\n+              // Expire any outstanding pending add requests that have timed out\n+              if (pendingAddExpirationTime != NOT_STARTED && now >= pendingAddExpirationTime) {\n+                logDebug(s\"Expiring all pending add requests because they have \" +\n+                  s\"not been fulfilled after $pendingAddTimeoutSeconds seconds\")\n+                numExecutorsPendingToAdd = 0\n+                pendingAddExpirationTime = NOT_STARTED\n+              }\n+\n+              // Expire any outstanding pending remove requests that have timed out\n+              pendingRemoveExpirationTimes.foreach { case (executorId, expirationTime) =>\n+                if (now > expirationTime) {\n+                  logDebug(s\"Expiring pending request to remove executor $executorId because \" +\n+                    s\"it has not been fulfilled after $pendingRemoveTimeoutSeconds seconds\")\n+                  executorsPendingToRemove.remove(executorId)\n+                  pendingRemoveExpirationTimes.remove(executorId)\n+                }\n+              }\n+            } catch {\n+              case e: Exception => logError(\"Exception in dynamic executor allocation thread!\", e)\n+            }\n+          }\n+          Thread.sleep(intervalMillis)\n+        }\n+      }\n+    }\n+    thread.setName(\"spark-dynamic-executor-allocation\")\n+    thread.setDaemon(true)\n+    thread.start()\n+  }\n+\n+  /**\n+   * Request a number of executors from the scheduler backend.\n+   * If the cap on the number of executors is reached, give up and reset the\n+   * number of executors to add next round instead of continuing to double it.\n+   */\n+  private def addExecutors(): Unit = synchronized {\n+    // Do not request more executors if we have already reached the upper bound\n+    val numExistingExecutors = executorIds.size + numExecutorsPendingToAdd\n+    if (numExistingExecutors >= maxNumExecutors) {\n+      logDebug(s\"Not adding executors because there are already \" +\n+        s\"$maxNumExecutors executor(s), which is the limit\")"
  }, {
    "author": {
      "login": "pwendell"
    },
    "body": "Also the word \"existing\" here is a bit confusing - what does it mean that they exist? It might be more clear to actually keep `execuotrIds.size` and `numExecutorsPendingToAdd` as separate variables. It will make the math below easier to understand.\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-22T05:33:40Z",
    "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically allocates and removes executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks is not\n+ * drained in N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle for K seconds (meaning it has not\n+ * been scheduled to run any tasks), then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * There is no retry logic in either case. Because the requests to the cluster manager are\n+ * asynchronous, this class does not know whether a request has been granted until later. For\n+ * this reason, both add and remove are treated as best-effort only.\n+ *\n+ * The relevant Spark properties include the following:\n+ *\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *\n+ *   spark.dynamicAllocation.addExecutorThresholdSeconds - How long before new executors are added\n+ *   spark.dynamicAllocation.addExecutorIntervalSeconds - How often to add new executors\n+ *   spark.dynamicAllocation.removeExecutorThresholdSeconds - How long before an executor is removed\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention should only\n+ * arise when new executors register or when existing executors are removed, both of which are\n+ * relatively rare events with respect to task scheduling. Thus, synchronizing each method on the\n+ * same lock should not be expensive assuming biased locking is enabled in the JVM (on by default\n+ * for Java 6+). This may not be true, however, if the application itself runs multiple jobs\n+ * concurrently.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorAllocationManager(scheduler: TaskSchedulerImpl) extends Logging {\n+  import ExecutorAllocationManager._\n+\n+  private val conf = scheduler.conf\n+\n+  // Lower and upper bounds on the number of executors. These are required.\n+  private val minNumExecutors = conf.getInt(\"spark.dynamicAllocation.minExecutors\", -1)\n+  private val maxNumExecutors = conf.getInt(\"spark.dynamicAllocation.maxExecutors\", -1)\n+  if (minNumExecutors < 0 || maxNumExecutors < 0) {\n+    throw new SparkException(\"spark.dynamicAllocation.{min/max}Executors must be set!\")\n+  }\n+\n+  // How frequently to add and remove executors (seconds)\n+  private val addThresholdSeconds =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorThresholdSeconds\", 60)\n+  private val addIntervalSeconds =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorIntervalSeconds\", addThresholdSeconds)\n+  private val removeThresholdSeconds =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorThresholdSeconds\", 600)\n+\n+  // Number of executors to add in the next round\n+  private var numExecutorsToAdd = 1\n+\n+  // Number of executors that have been requested but have not registered yet\n+  private var numExecutorsPendingToAdd = 0\n+\n+  // Executors that have been requested to be removed but have not been killed yet\n+  private val executorsPendingToRemove = new mutable.HashSet[String]\n+\n+  // Keep track of all executors here to decouple us from the logic in TaskSchedulerImpl\n+  private val executorIds = new mutable.HashSet[String]\n+\n+  // A timestamp of when the add timer should be triggered, or NOT_STARTED if the timer is not\n+  // started. This timer is started when there are pending tasks built up, and canceled when\n+  // there are no more pending tasks.\n+  private var addTime = NOT_STARTED\n+\n+  // A timestamp for each executor of when the remove timer for that executor should be triggered.\n+  // Each remove timer is started when the executor first registers or when the executor finishes\n+  // running a task, and canceled when the executor is scheduled to run a new task.\n+  private val removeTimes = new mutable.HashMap[String, Long]\n+\n+  // A timestamp of when all pending add requests should expire\n+  private var pendingAddExpirationTime = NOT_STARTED\n+\n+  // A timestamp for each executor of when the pending remove request for the executor should expire\n+  private val pendingRemoveExpirationTimes = new mutable.HashMap[String, Long]\n+\n+  // How long before expiring pending requests to add or remove executors (seconds)\n+  private val pendingAddTimeoutSeconds = 300 // 5 min\n+  private val pendingRemoveTimeoutSeconds = 300\n+\n+  // Polling loop interval (ms)\n+  private val intervalMillis = 100\n+\n+  // Scheduler backend through which requests to add/remove executors are made\n+  // Note that this assumes the backend has already initialized when this is first used\n+  // Otherwise, an appropriate exception is thrown\n+  private lazy val backend = scheduler.backend match {\n+    case b: CoarseGrainedSchedulerBackend => b\n+    case null =>\n+      throw new SparkException(\"Scheduler backend not initialized yet!\")\n+    case _ =>\n+      throw new SparkException(\n+        \"Dynamic allocation of executors is not applicable to fine-grained schedulers. \" +\n+        \"Please set spark.dynamicAllocation.enabled to false.\")\n+  }\n+\n+  initialize()\n+\n+  /**\n+   * Start the main polling thread that keeps track of when to add and remove executors.\n+   * During each loop interval, this thread checks if any of the timers have timed out, and,\n+   * if so, triggers the relevant timer actions.\n+   */\n+  def initialize(): Unit = {\n+    val thread = new Thread {\n+      override def run(): Unit = {\n+        while (true) {\n+          ExecutorAllocationManager.this.synchronized {\n+            val now = System.currentTimeMillis\n+            try {\n+              // If the add timer has timed out, add executors and refresh the timer\n+              if (addTime != NOT_STARTED && now >= addTime) {\n+                addExecutors()\n+                logDebug(s\"Restarting add executor timer \" +\n+                  s\"(to be triggered in $addIntervalSeconds seconds)\")\n+                addTime += addIntervalSeconds * 1000\n+              }\n+\n+              // If a remove timer has timed out, remove the executor and cancel the timer\n+              removeTimes.foreach { case (executorId, triggerTime) =>\n+                if (now > triggerTime) {\n+                  removeExecutor(executorId)\n+                  cancelRemoveTimer(executorId)\n+                }\n+              }\n+\n+              // Expire any outstanding pending add requests that have timed out\n+              if (pendingAddExpirationTime != NOT_STARTED && now >= pendingAddExpirationTime) {\n+                logDebug(s\"Expiring all pending add requests because they have \" +\n+                  s\"not been fulfilled after $pendingAddTimeoutSeconds seconds\")\n+                numExecutorsPendingToAdd = 0\n+                pendingAddExpirationTime = NOT_STARTED\n+              }\n+\n+              // Expire any outstanding pending remove requests that have timed out\n+              pendingRemoveExpirationTimes.foreach { case (executorId, expirationTime) =>\n+                if (now > expirationTime) {\n+                  logDebug(s\"Expiring pending request to remove executor $executorId because \" +\n+                    s\"it has not been fulfilled after $pendingRemoveTimeoutSeconds seconds\")\n+                  executorsPendingToRemove.remove(executorId)\n+                  pendingRemoveExpirationTimes.remove(executorId)\n+                }\n+              }\n+            } catch {\n+              case e: Exception => logError(\"Exception in dynamic executor allocation thread!\", e)\n+            }\n+          }\n+          Thread.sleep(intervalMillis)\n+        }\n+      }\n+    }\n+    thread.setName(\"spark-dynamic-executor-allocation\")\n+    thread.setDaemon(true)\n+    thread.start()\n+  }\n+\n+  /**\n+   * Request a number of executors from the scheduler backend.\n+   * If the cap on the number of executors is reached, give up and reset the\n+   * number of executors to add next round instead of continuing to double it.\n+   */\n+  private def addExecutors(): Unit = synchronized {\n+    // Do not request more executors if we have already reached the upper bound\n+    val numExistingExecutors = executorIds.size + numExecutorsPendingToAdd\n+    if (numExistingExecutors >= maxNumExecutors) {\n+      logDebug(s\"Not adding executors because there are already \" +\n+        s\"$maxNumExecutors executor(s), which is the limit\")"
  }],
  "prId": 2746
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "what about just `numExecutorsPending`? \"To add\" seems imperative to me, i.e. I should add these later.\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-22T05:41:58Z",
    "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically allocates and removes executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks is not\n+ * drained in N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle for K seconds (meaning it has not\n+ * been scheduled to run any tasks), then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * There is no retry logic in either case. Because the requests to the cluster manager are\n+ * asynchronous, this class does not know whether a request has been granted until later. For\n+ * this reason, both add and remove are treated as best-effort only.\n+ *\n+ * The relevant Spark properties include the following:\n+ *\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *\n+ *   spark.dynamicAllocation.addExecutorThresholdSeconds - How long before new executors are added\n+ *   spark.dynamicAllocation.addExecutorIntervalSeconds - How often to add new executors\n+ *   spark.dynamicAllocation.removeExecutorThresholdSeconds - How long before an executor is removed\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention should only\n+ * arise when new executors register or when existing executors are removed, both of which are\n+ * relatively rare events with respect to task scheduling. Thus, synchronizing each method on the\n+ * same lock should not be expensive assuming biased locking is enabled in the JVM (on by default\n+ * for Java 6+). This may not be true, however, if the application itself runs multiple jobs\n+ * concurrently.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorAllocationManager(scheduler: TaskSchedulerImpl) extends Logging {\n+  import ExecutorAllocationManager._\n+\n+  private val conf = scheduler.conf\n+\n+  // Lower and upper bounds on the number of executors. These are required.\n+  private val minNumExecutors = conf.getInt(\"spark.dynamicAllocation.minExecutors\", -1)\n+  private val maxNumExecutors = conf.getInt(\"spark.dynamicAllocation.maxExecutors\", -1)\n+  if (minNumExecutors < 0 || maxNumExecutors < 0) {\n+    throw new SparkException(\"spark.dynamicAllocation.{min/max}Executors must be set!\")\n+  }\n+\n+  // How frequently to add and remove executors (seconds)\n+  private val addThresholdSeconds =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorThresholdSeconds\", 60)\n+  private val addIntervalSeconds =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorIntervalSeconds\", addThresholdSeconds)\n+  private val removeThresholdSeconds =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorThresholdSeconds\", 600)\n+\n+  // Number of executors to add in the next round\n+  private var numExecutorsToAdd = 1\n+\n+  // Number of executors that have been requested but have not registered yet\n+  private var numExecutorsPendingToAdd = 0"
  }],
  "prId": 2746
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "Would it be better to store the number added in the previous round rather than the number that _should_ be added in the next round. It's a bit confusing because we won't actually add this number in some cases. (on this one I didn't think through all the implications, so it might be simpler as-is).\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-22T05:42:43Z",
    "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically allocates and removes executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks is not\n+ * drained in N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle for K seconds (meaning it has not\n+ * been scheduled to run any tasks), then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * There is no retry logic in either case. Because the requests to the cluster manager are\n+ * asynchronous, this class does not know whether a request has been granted until later. For\n+ * this reason, both add and remove are treated as best-effort only.\n+ *\n+ * The relevant Spark properties include the following:\n+ *\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *\n+ *   spark.dynamicAllocation.addExecutorThresholdSeconds - How long before new executors are added\n+ *   spark.dynamicAllocation.addExecutorIntervalSeconds - How often to add new executors\n+ *   spark.dynamicAllocation.removeExecutorThresholdSeconds - How long before an executor is removed\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention should only\n+ * arise when new executors register or when existing executors are removed, both of which are\n+ * relatively rare events with respect to task scheduling. Thus, synchronizing each method on the\n+ * same lock should not be expensive assuming biased locking is enabled in the JVM (on by default\n+ * for Java 6+). This may not be true, however, if the application itself runs multiple jobs\n+ * concurrently.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorAllocationManager(scheduler: TaskSchedulerImpl) extends Logging {\n+  import ExecutorAllocationManager._\n+\n+  private val conf = scheduler.conf\n+\n+  // Lower and upper bounds on the number of executors. These are required.\n+  private val minNumExecutors = conf.getInt(\"spark.dynamicAllocation.minExecutors\", -1)\n+  private val maxNumExecutors = conf.getInt(\"spark.dynamicAllocation.maxExecutors\", -1)\n+  if (minNumExecutors < 0 || maxNumExecutors < 0) {\n+    throw new SparkException(\"spark.dynamicAllocation.{min/max}Executors must be set!\")\n+  }\n+\n+  // How frequently to add and remove executors (seconds)\n+  private val addThresholdSeconds =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorThresholdSeconds\", 60)\n+  private val addIntervalSeconds =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorIntervalSeconds\", addThresholdSeconds)\n+  private val removeThresholdSeconds =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorThresholdSeconds\", 600)\n+\n+  // Number of executors to add in the next round\n+  private var numExecutorsToAdd = 1"
  }],
  "prId": 2746
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "This expression is quite complicated, so I'd break it up a bit:\n\n```\n// Number to add if continuing exponential increase\nval targetNumExecutors = executorIds.size + numExecutorsPending + numExecutorsToAdd\n// Take into account max\nval adjustedTargetNumExecutors = math.min(targetNumExecutors, maxNumExecutors) \n// Compute delta\nval adjustedNumExecutorsToAdd = adjustedTargetNumExecutors - numExistingExecutors\n```\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-22T05:45:01Z",
    "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically allocates and removes executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks is not\n+ * drained in N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle for K seconds (meaning it has not\n+ * been scheduled to run any tasks), then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * There is no retry logic in either case. Because the requests to the cluster manager are\n+ * asynchronous, this class does not know whether a request has been granted until later. For\n+ * this reason, both add and remove are treated as best-effort only.\n+ *\n+ * The relevant Spark properties include the following:\n+ *\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *\n+ *   spark.dynamicAllocation.addExecutorThresholdSeconds - How long before new executors are added\n+ *   spark.dynamicAllocation.addExecutorIntervalSeconds - How often to add new executors\n+ *   spark.dynamicAllocation.removeExecutorThresholdSeconds - How long before an executor is removed\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention should only\n+ * arise when new executors register or when existing executors are removed, both of which are\n+ * relatively rare events with respect to task scheduling. Thus, synchronizing each method on the\n+ * same lock should not be expensive assuming biased locking is enabled in the JVM (on by default\n+ * for Java 6+). This may not be true, however, if the application itself runs multiple jobs\n+ * concurrently.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorAllocationManager(scheduler: TaskSchedulerImpl) extends Logging {\n+  import ExecutorAllocationManager._\n+\n+  private val conf = scheduler.conf\n+\n+  // Lower and upper bounds on the number of executors. These are required.\n+  private val minNumExecutors = conf.getInt(\"spark.dynamicAllocation.minExecutors\", -1)\n+  private val maxNumExecutors = conf.getInt(\"spark.dynamicAllocation.maxExecutors\", -1)\n+  if (minNumExecutors < 0 || maxNumExecutors < 0) {\n+    throw new SparkException(\"spark.dynamicAllocation.{min/max}Executors must be set!\")\n+  }\n+\n+  // How frequently to add and remove executors (seconds)\n+  private val addThresholdSeconds =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorThresholdSeconds\", 60)\n+  private val addIntervalSeconds =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorIntervalSeconds\", addThresholdSeconds)\n+  private val removeThresholdSeconds =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorThresholdSeconds\", 600)\n+\n+  // Number of executors to add in the next round\n+  private var numExecutorsToAdd = 1\n+\n+  // Number of executors that have been requested but have not registered yet\n+  private var numExecutorsPendingToAdd = 0\n+\n+  // Executors that have been requested to be removed but have not been killed yet\n+  private val executorsPendingToRemove = new mutable.HashSet[String]\n+\n+  // Keep track of all executors here to decouple us from the logic in TaskSchedulerImpl\n+  private val executorIds = new mutable.HashSet[String]\n+\n+  // A timestamp of when the add timer should be triggered, or NOT_STARTED if the timer is not\n+  // started. This timer is started when there are pending tasks built up, and canceled when\n+  // there are no more pending tasks.\n+  private var addTime = NOT_STARTED\n+\n+  // A timestamp for each executor of when the remove timer for that executor should be triggered.\n+  // Each remove timer is started when the executor first registers or when the executor finishes\n+  // running a task, and canceled when the executor is scheduled to run a new task.\n+  private val removeTimes = new mutable.HashMap[String, Long]\n+\n+  // A timestamp of when all pending add requests should expire\n+  private var pendingAddExpirationTime = NOT_STARTED\n+\n+  // A timestamp for each executor of when the pending remove request for the executor should expire\n+  private val pendingRemoveExpirationTimes = new mutable.HashMap[String, Long]\n+\n+  // How long before expiring pending requests to add or remove executors (seconds)\n+  private val pendingAddTimeoutSeconds = 300 // 5 min\n+  private val pendingRemoveTimeoutSeconds = 300\n+\n+  // Polling loop interval (ms)\n+  private val intervalMillis = 100\n+\n+  // Scheduler backend through which requests to add/remove executors are made\n+  // Note that this assumes the backend has already initialized when this is first used\n+  // Otherwise, an appropriate exception is thrown\n+  private lazy val backend = scheduler.backend match {\n+    case b: CoarseGrainedSchedulerBackend => b\n+    case null =>\n+      throw new SparkException(\"Scheduler backend not initialized yet!\")\n+    case _ =>\n+      throw new SparkException(\n+        \"Dynamic allocation of executors is not applicable to fine-grained schedulers. \" +\n+        \"Please set spark.dynamicAllocation.enabled to false.\")\n+  }\n+\n+  initialize()\n+\n+  /**\n+   * Start the main polling thread that keeps track of when to add and remove executors.\n+   * During each loop interval, this thread checks if any of the timers have timed out, and,\n+   * if so, triggers the relevant timer actions.\n+   */\n+  def initialize(): Unit = {\n+    val thread = new Thread {\n+      override def run(): Unit = {\n+        while (true) {\n+          ExecutorAllocationManager.this.synchronized {\n+            val now = System.currentTimeMillis\n+            try {\n+              // If the add timer has timed out, add executors and refresh the timer\n+              if (addTime != NOT_STARTED && now >= addTime) {\n+                addExecutors()\n+                logDebug(s\"Restarting add executor timer \" +\n+                  s\"(to be triggered in $addIntervalSeconds seconds)\")\n+                addTime += addIntervalSeconds * 1000\n+              }\n+\n+              // If a remove timer has timed out, remove the executor and cancel the timer\n+              removeTimes.foreach { case (executorId, triggerTime) =>\n+                if (now > triggerTime) {\n+                  removeExecutor(executorId)\n+                  cancelRemoveTimer(executorId)\n+                }\n+              }\n+\n+              // Expire any outstanding pending add requests that have timed out\n+              if (pendingAddExpirationTime != NOT_STARTED && now >= pendingAddExpirationTime) {\n+                logDebug(s\"Expiring all pending add requests because they have \" +\n+                  s\"not been fulfilled after $pendingAddTimeoutSeconds seconds\")\n+                numExecutorsPendingToAdd = 0\n+                pendingAddExpirationTime = NOT_STARTED\n+              }\n+\n+              // Expire any outstanding pending remove requests that have timed out\n+              pendingRemoveExpirationTimes.foreach { case (executorId, expirationTime) =>\n+                if (now > expirationTime) {\n+                  logDebug(s\"Expiring pending request to remove executor $executorId because \" +\n+                    s\"it has not been fulfilled after $pendingRemoveTimeoutSeconds seconds\")\n+                  executorsPendingToRemove.remove(executorId)\n+                  pendingRemoveExpirationTimes.remove(executorId)\n+                }\n+              }\n+            } catch {\n+              case e: Exception => logError(\"Exception in dynamic executor allocation thread!\", e)\n+            }\n+          }\n+          Thread.sleep(intervalMillis)\n+        }\n+      }\n+    }\n+    thread.setName(\"spark-dynamic-executor-allocation\")\n+    thread.setDaemon(true)\n+    thread.start()\n+  }\n+\n+  /**\n+   * Request a number of executors from the scheduler backend.\n+   * If the cap on the number of executors is reached, give up and reset the\n+   * number of executors to add next round instead of continuing to double it.\n+   */\n+  private def addExecutors(): Unit = synchronized {\n+    // Do not request more executors if we have already reached the upper bound\n+    val numExistingExecutors = executorIds.size + numExecutorsPendingToAdd\n+    if (numExistingExecutors >= maxNumExecutors) {\n+      logDebug(s\"Not adding executors because there are already \" +\n+        s\"$maxNumExecutors executor(s), which is the limit\")\n+      numExecutorsToAdd = 1\n+      return\n+    }\n+\n+    // Request executors with respect to the upper bound\n+    val actualNumExecutorsToAdd =\n+      math.min(numExistingExecutors + numExecutorsToAdd, maxNumExecutors) - numExistingExecutors"
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "I think it's even clearer if we avoid using `math.min`:\n\n```\nval actualNumExecutorsToAdd =\n  if (numExistingExecutors + numExecutorsToAdd > maxNumExecutors) {\n    // upper bound reached\n    maxNumExecutors - numExistingExecutors\n  } else {\n    numExecutorsToAdd\n  }\n```\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-22T17:45:45Z",
    "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically allocates and removes executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks is not\n+ * drained in N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle for K seconds (meaning it has not\n+ * been scheduled to run any tasks), then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * There is no retry logic in either case. Because the requests to the cluster manager are\n+ * asynchronous, this class does not know whether a request has been granted until later. For\n+ * this reason, both add and remove are treated as best-effort only.\n+ *\n+ * The relevant Spark properties include the following:\n+ *\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *\n+ *   spark.dynamicAllocation.addExecutorThresholdSeconds - How long before new executors are added\n+ *   spark.dynamicAllocation.addExecutorIntervalSeconds - How often to add new executors\n+ *   spark.dynamicAllocation.removeExecutorThresholdSeconds - How long before an executor is removed\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention should only\n+ * arise when new executors register or when existing executors are removed, both of which are\n+ * relatively rare events with respect to task scheduling. Thus, synchronizing each method on the\n+ * same lock should not be expensive assuming biased locking is enabled in the JVM (on by default\n+ * for Java 6+). This may not be true, however, if the application itself runs multiple jobs\n+ * concurrently.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorAllocationManager(scheduler: TaskSchedulerImpl) extends Logging {\n+  import ExecutorAllocationManager._\n+\n+  private val conf = scheduler.conf\n+\n+  // Lower and upper bounds on the number of executors. These are required.\n+  private val minNumExecutors = conf.getInt(\"spark.dynamicAllocation.minExecutors\", -1)\n+  private val maxNumExecutors = conf.getInt(\"spark.dynamicAllocation.maxExecutors\", -1)\n+  if (minNumExecutors < 0 || maxNumExecutors < 0) {\n+    throw new SparkException(\"spark.dynamicAllocation.{min/max}Executors must be set!\")\n+  }\n+\n+  // How frequently to add and remove executors (seconds)\n+  private val addThresholdSeconds =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorThresholdSeconds\", 60)\n+  private val addIntervalSeconds =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorIntervalSeconds\", addThresholdSeconds)\n+  private val removeThresholdSeconds =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorThresholdSeconds\", 600)\n+\n+  // Number of executors to add in the next round\n+  private var numExecutorsToAdd = 1\n+\n+  // Number of executors that have been requested but have not registered yet\n+  private var numExecutorsPendingToAdd = 0\n+\n+  // Executors that have been requested to be removed but have not been killed yet\n+  private val executorsPendingToRemove = new mutable.HashSet[String]\n+\n+  // Keep track of all executors here to decouple us from the logic in TaskSchedulerImpl\n+  private val executorIds = new mutable.HashSet[String]\n+\n+  // A timestamp of when the add timer should be triggered, or NOT_STARTED if the timer is not\n+  // started. This timer is started when there are pending tasks built up, and canceled when\n+  // there are no more pending tasks.\n+  private var addTime = NOT_STARTED\n+\n+  // A timestamp for each executor of when the remove timer for that executor should be triggered.\n+  // Each remove timer is started when the executor first registers or when the executor finishes\n+  // running a task, and canceled when the executor is scheduled to run a new task.\n+  private val removeTimes = new mutable.HashMap[String, Long]\n+\n+  // A timestamp of when all pending add requests should expire\n+  private var pendingAddExpirationTime = NOT_STARTED\n+\n+  // A timestamp for each executor of when the pending remove request for the executor should expire\n+  private val pendingRemoveExpirationTimes = new mutable.HashMap[String, Long]\n+\n+  // How long before expiring pending requests to add or remove executors (seconds)\n+  private val pendingAddTimeoutSeconds = 300 // 5 min\n+  private val pendingRemoveTimeoutSeconds = 300\n+\n+  // Polling loop interval (ms)\n+  private val intervalMillis = 100\n+\n+  // Scheduler backend through which requests to add/remove executors are made\n+  // Note that this assumes the backend has already initialized when this is first used\n+  // Otherwise, an appropriate exception is thrown\n+  private lazy val backend = scheduler.backend match {\n+    case b: CoarseGrainedSchedulerBackend => b\n+    case null =>\n+      throw new SparkException(\"Scheduler backend not initialized yet!\")\n+    case _ =>\n+      throw new SparkException(\n+        \"Dynamic allocation of executors is not applicable to fine-grained schedulers. \" +\n+        \"Please set spark.dynamicAllocation.enabled to false.\")\n+  }\n+\n+  initialize()\n+\n+  /**\n+   * Start the main polling thread that keeps track of when to add and remove executors.\n+   * During each loop interval, this thread checks if any of the timers have timed out, and,\n+   * if so, triggers the relevant timer actions.\n+   */\n+  def initialize(): Unit = {\n+    val thread = new Thread {\n+      override def run(): Unit = {\n+        while (true) {\n+          ExecutorAllocationManager.this.synchronized {\n+            val now = System.currentTimeMillis\n+            try {\n+              // If the add timer has timed out, add executors and refresh the timer\n+              if (addTime != NOT_STARTED && now >= addTime) {\n+                addExecutors()\n+                logDebug(s\"Restarting add executor timer \" +\n+                  s\"(to be triggered in $addIntervalSeconds seconds)\")\n+                addTime += addIntervalSeconds * 1000\n+              }\n+\n+              // If a remove timer has timed out, remove the executor and cancel the timer\n+              removeTimes.foreach { case (executorId, triggerTime) =>\n+                if (now > triggerTime) {\n+                  removeExecutor(executorId)\n+                  cancelRemoveTimer(executorId)\n+                }\n+              }\n+\n+              // Expire any outstanding pending add requests that have timed out\n+              if (pendingAddExpirationTime != NOT_STARTED && now >= pendingAddExpirationTime) {\n+                logDebug(s\"Expiring all pending add requests because they have \" +\n+                  s\"not been fulfilled after $pendingAddTimeoutSeconds seconds\")\n+                numExecutorsPendingToAdd = 0\n+                pendingAddExpirationTime = NOT_STARTED\n+              }\n+\n+              // Expire any outstanding pending remove requests that have timed out\n+              pendingRemoveExpirationTimes.foreach { case (executorId, expirationTime) =>\n+                if (now > expirationTime) {\n+                  logDebug(s\"Expiring pending request to remove executor $executorId because \" +\n+                    s\"it has not been fulfilled after $pendingRemoveTimeoutSeconds seconds\")\n+                  executorsPendingToRemove.remove(executorId)\n+                  pendingRemoveExpirationTimes.remove(executorId)\n+                }\n+              }\n+            } catch {\n+              case e: Exception => logError(\"Exception in dynamic executor allocation thread!\", e)\n+            }\n+          }\n+          Thread.sleep(intervalMillis)\n+        }\n+      }\n+    }\n+    thread.setName(\"spark-dynamic-executor-allocation\")\n+    thread.setDaemon(true)\n+    thread.start()\n+  }\n+\n+  /**\n+   * Request a number of executors from the scheduler backend.\n+   * If the cap on the number of executors is reached, give up and reset the\n+   * number of executors to add next round instead of continuing to double it.\n+   */\n+  private def addExecutors(): Unit = synchronized {\n+    // Do not request more executors if we have already reached the upper bound\n+    val numExistingExecutors = executorIds.size + numExecutorsPendingToAdd\n+    if (numExistingExecutors >= maxNumExecutors) {\n+      logDebug(s\"Not adding executors because there are already \" +\n+        s\"$maxNumExecutors executor(s), which is the limit\")\n+      numExecutorsToAdd = 1\n+      return\n+    }\n+\n+    // Request executors with respect to the upper bound\n+    val actualNumExecutorsToAdd =\n+      math.min(numExistingExecutors + numExecutorsToAdd, maxNumExecutors) - numExistingExecutors"
  }],
  "prId": 2746
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "s/Adding/Requesting/g and  s/new total/new desired total/g to not to give false hope?\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-23T05:40:06Z",
    "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically allocates and removes executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks is not\n+ * drained in N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle for K seconds (meaning it has not\n+ * been scheduled to run any tasks), then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * There is no retry logic in either case. Because the requests to the cluster manager are\n+ * asynchronous, this class does not know whether a request has been granted until later. For\n+ * this reason, both add and remove are treated as best-effort only.\n+ *\n+ * The relevant Spark properties include the following:\n+ *\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *\n+ *   spark.dynamicAllocation.addExecutorThresholdSeconds - How long before new executors are added\n+ *   spark.dynamicAllocation.addExecutorIntervalSeconds - How often to add new executors\n+ *   spark.dynamicAllocation.removeExecutorThresholdSeconds - How long before an executor is removed\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention should only\n+ * arise when new executors register or when existing executors are removed, both of which are\n+ * relatively rare events with respect to task scheduling. Thus, synchronizing each method on the\n+ * same lock should not be expensive assuming biased locking is enabled in the JVM (on by default\n+ * for Java 6+). This may not be true, however, if the application itself runs multiple jobs\n+ * concurrently.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorAllocationManager(scheduler: TaskSchedulerImpl) extends Logging {\n+  import ExecutorAllocationManager._\n+\n+  private val conf = scheduler.conf\n+\n+  // Lower and upper bounds on the number of executors. These are required.\n+  private val minNumExecutors = conf.getInt(\"spark.dynamicAllocation.minExecutors\", -1)\n+  private val maxNumExecutors = conf.getInt(\"spark.dynamicAllocation.maxExecutors\", -1)\n+  if (minNumExecutors < 0 || maxNumExecutors < 0) {\n+    throw new SparkException(\"spark.dynamicAllocation.{min/max}Executors must be set!\")\n+  }\n+\n+  // How frequently to add and remove executors (seconds)\n+  private val addThresholdSeconds =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorThresholdSeconds\", 60)\n+  private val addIntervalSeconds =\n+    conf.getLong(\"spark.dynamicAllocation.addExecutorIntervalSeconds\", addThresholdSeconds)\n+  private val removeThresholdSeconds =\n+    conf.getLong(\"spark.dynamicAllocation.removeExecutorThresholdSeconds\", 600)\n+\n+  // Number of executors to add in the next round\n+  private var numExecutorsToAdd = 1\n+\n+  // Number of executors that have been requested but have not registered yet\n+  private var numExecutorsPendingToAdd = 0\n+\n+  // Executors that have been requested to be removed but have not been killed yet\n+  private val executorsPendingToRemove = new mutable.HashSet[String]\n+\n+  // Keep track of all executors here to decouple us from the logic in TaskSchedulerImpl\n+  private val executorIds = new mutable.HashSet[String]\n+\n+  // A timestamp of when the add timer should be triggered, or NOT_STARTED if the timer is not\n+  // started. This timer is started when there are pending tasks built up, and canceled when\n+  // there are no more pending tasks.\n+  private var addTime = NOT_STARTED\n+\n+  // A timestamp for each executor of when the remove timer for that executor should be triggered.\n+  // Each remove timer is started when the executor first registers or when the executor finishes\n+  // running a task, and canceled when the executor is scheduled to run a new task.\n+  private val removeTimes = new mutable.HashMap[String, Long]\n+\n+  // A timestamp of when all pending add requests should expire\n+  private var pendingAddExpirationTime = NOT_STARTED\n+\n+  // A timestamp for each executor of when the pending remove request for the executor should expire\n+  private val pendingRemoveExpirationTimes = new mutable.HashMap[String, Long]\n+\n+  // How long before expiring pending requests to add or remove executors (seconds)\n+  private val pendingAddTimeoutSeconds = 300 // 5 min\n+  private val pendingRemoveTimeoutSeconds = 300\n+\n+  // Polling loop interval (ms)\n+  private val intervalMillis = 100\n+\n+  // Scheduler backend through which requests to add/remove executors are made\n+  // Note that this assumes the backend has already initialized when this is first used\n+  // Otherwise, an appropriate exception is thrown\n+  private lazy val backend = scheduler.backend match {\n+    case b: CoarseGrainedSchedulerBackend => b\n+    case null =>\n+      throw new SparkException(\"Scheduler backend not initialized yet!\")\n+    case _ =>\n+      throw new SparkException(\n+        \"Dynamic allocation of executors is not applicable to fine-grained schedulers. \" +\n+        \"Please set spark.dynamicAllocation.enabled to false.\")\n+  }\n+\n+  initialize()\n+\n+  /**\n+   * Start the main polling thread that keeps track of when to add and remove executors.\n+   * During each loop interval, this thread checks if any of the timers have timed out, and,\n+   * if so, triggers the relevant timer actions.\n+   */\n+  def initialize(): Unit = {\n+    val thread = new Thread {\n+      override def run(): Unit = {\n+        while (true) {\n+          ExecutorAllocationManager.this.synchronized {\n+            val now = System.currentTimeMillis\n+            try {\n+              // If the add timer has timed out, add executors and refresh the timer\n+              if (addTime != NOT_STARTED && now >= addTime) {\n+                addExecutors()\n+                logDebug(s\"Restarting add executor timer \" +\n+                  s\"(to be triggered in $addIntervalSeconds seconds)\")\n+                addTime += addIntervalSeconds * 1000\n+              }\n+\n+              // If a remove timer has timed out, remove the executor and cancel the timer\n+              removeTimes.foreach { case (executorId, triggerTime) =>\n+                if (now > triggerTime) {\n+                  removeExecutor(executorId)\n+                  cancelRemoveTimer(executorId)\n+                }\n+              }\n+\n+              // Expire any outstanding pending add requests that have timed out\n+              if (pendingAddExpirationTime != NOT_STARTED && now >= pendingAddExpirationTime) {\n+                logDebug(s\"Expiring all pending add requests because they have \" +\n+                  s\"not been fulfilled after $pendingAddTimeoutSeconds seconds\")\n+                numExecutorsPendingToAdd = 0\n+                pendingAddExpirationTime = NOT_STARTED\n+              }\n+\n+              // Expire any outstanding pending remove requests that have timed out\n+              pendingRemoveExpirationTimes.foreach { case (executorId, expirationTime) =>\n+                if (now > expirationTime) {\n+                  logDebug(s\"Expiring pending request to remove executor $executorId because \" +\n+                    s\"it has not been fulfilled after $pendingRemoveTimeoutSeconds seconds\")\n+                  executorsPendingToRemove.remove(executorId)\n+                  pendingRemoveExpirationTimes.remove(executorId)\n+                }\n+              }\n+            } catch {\n+              case e: Exception => logError(\"Exception in dynamic executor allocation thread!\", e)\n+            }\n+          }\n+          Thread.sleep(intervalMillis)\n+        }\n+      }\n+    }\n+    thread.setName(\"spark-dynamic-executor-allocation\")\n+    thread.setDaemon(true)\n+    thread.start()\n+  }\n+\n+  /**\n+   * Request a number of executors from the scheduler backend.\n+   * If the cap on the number of executors is reached, give up and reset the\n+   * number of executors to add next round instead of continuing to double it.\n+   */\n+  private def addExecutors(): Unit = synchronized {\n+    // Do not request more executors if we have already reached the upper bound\n+    val numExistingExecutors = executorIds.size + numExecutorsPendingToAdd\n+    if (numExistingExecutors >= maxNumExecutors) {\n+      logDebug(s\"Not adding executors because there are already \" +\n+        s\"$maxNumExecutors executor(s), which is the limit\")\n+      numExecutorsToAdd = 1\n+      return\n+    }\n+\n+    // Request executors with respect to the upper bound\n+    val actualNumExecutorsToAdd =\n+      math.min(numExistingExecutors + numExecutorsToAdd, maxNumExecutors) - numExistingExecutors\n+    val newTotalExecutors = numExistingExecutors + actualNumExecutorsToAdd\n+    logInfo(s\"Pending tasks are building up! Adding $actualNumExecutorsToAdd \" +\n+      s\"new executor(s) (new total will be $newTotalExecutors)\")"
  }],
  "prId": 2746
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "Any particular rationale for these defaults?  My intuition would be that the addThreshold should be something close to how long it takes for a new executor to launch and drain its capacity in tasks from the queue.  This shouldn't take more than a couple seconds.\n\nA remove threshold of 10 minutes also seems high to me. The threshold should be something like a standard deviation up from the typical amount of time that a user would wait before running another query / job after one completes.  I'd imagine a minute or so would suffice. \n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-24T08:10:57Z",
    "diffHunk": "@@ -0,0 +1,313 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically allocates and removes executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks is not\n+ * drained in N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle for K seconds (meaning it has not\n+ * been scheduled to run any tasks), then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * There is no retry logic in either case. Because the requests to the cluster manager are\n+ * asynchronous, this class does not know whether a request has been granted until later. For\n+ * this reason, both add and remove are treated as best-effort only.\n+ *\n+ * The relevant Spark properties include the following:\n+ *\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *\n+ *   spark.dynamicAllocation.addExecutorThresholdSeconds - How long before new executors are added\n+ *   spark.dynamicAllocation.addExecutorIntervalSeconds - How often to add new executors\n+ *   spark.dynamicAllocation.removeExecutorThresholdSeconds - How long before an executor is removed\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention should only\n+ * arise when new executors register or when existing executors are removed, both of which are\n+ * relatively rare events with respect to task scheduling. Thus, synchronizing each method on the\n+ * same lock should not be expensive assuming biased locking is enabled in the JVM (on by default\n+ * for Java 6+). This may not be true, however, if the application itself runs multiple jobs\n+ * concurrently.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorAllocationManager(scheduler: TaskSchedulerImpl) extends Logging {\n+  import ExecutorAllocationManager._\n+\n+  private val conf = scheduler.conf\n+\n+  // Lower and upper bounds on the number of executors. These are required.\n+  private val minNumExecutors = conf.getInt(\"spark.dynamicAllocation.minExecutors\", -1)\n+  private val maxNumExecutors = conf.getInt(\"spark.dynamicAllocation.maxExecutors\", -1)\n+  if (minNumExecutors < 0 || maxNumExecutors < 0) {\n+    throw new SparkException(\"spark.dynamicAllocation.{min/max}Executors must be set!\")\n+  }\n+\n+  // How frequently to add and remove executors (seconds)\n+  private val addThresholdSeconds ="
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "Sounds reasonable. These are just tentative values that haven't been given a ton of thought yet\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-24T17:24:27Z",
    "diffHunk": "@@ -0,0 +1,313 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically allocates and removes executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks is not\n+ * drained in N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle for K seconds (meaning it has not\n+ * been scheduled to run any tasks), then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * There is no retry logic in either case. Because the requests to the cluster manager are\n+ * asynchronous, this class does not know whether a request has been granted until later. For\n+ * this reason, both add and remove are treated as best-effort only.\n+ *\n+ * The relevant Spark properties include the following:\n+ *\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *\n+ *   spark.dynamicAllocation.addExecutorThresholdSeconds - How long before new executors are added\n+ *   spark.dynamicAllocation.addExecutorIntervalSeconds - How often to add new executors\n+ *   spark.dynamicAllocation.removeExecutorThresholdSeconds - How long before an executor is removed\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention should only\n+ * arise when new executors register or when existing executors are removed, both of which are\n+ * relatively rare events with respect to task scheduling. Thus, synchronizing each method on the\n+ * same lock should not be expensive assuming biased locking is enabled in the JVM (on by default\n+ * for Java 6+). This may not be true, however, if the application itself runs multiple jobs\n+ * concurrently.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorAllocationManager(scheduler: TaskSchedulerImpl) extends Logging {\n+  import ExecutorAllocationManager._\n+\n+  private val conf = scheduler.conf\n+\n+  // Lower and upper bounds on the number of executors. These are required.\n+  private val minNumExecutors = conf.getInt(\"spark.dynamicAllocation.minExecutors\", -1)\n+  private val maxNumExecutors = conf.getInt(\"spark.dynamicAllocation.maxExecutors\", -1)\n+  if (minNumExecutors < 0 || maxNumExecutors < 0) {\n+    throw new SparkException(\"spark.dynamicAllocation.{min/max}Executors must be set!\")\n+  }\n+\n+  // How frequently to add and remove executors (seconds)\n+  private val addThresholdSeconds ="
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "@pwendell What do you think?\n",
    "commit": "8a4fdaad14c546f90acffef274355d884f7e6a59",
    "createdAt": "2014-10-28T03:25:38Z",
    "diffHunk": "@@ -0,0 +1,313 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+/**\n+ * An agent that dynamically allocates and removes executors based on the workload.\n+ *\n+ * The add policy depends on the number of pending tasks. If the queue of pending tasks is not\n+ * drained in N seconds, then new executors are added. If the queue persists for another M\n+ * seconds, then more executors are added and so on. The number added in each round increases\n+ * exponentially from the previous round until an upper bound on the number of executors has\n+ * been reached.\n+ *\n+ * The rationale for the exponential increase is twofold: (1) Executors should be added slowly\n+ * in the beginning in case the number of extra executors needed turns out to be small. Otherwise,\n+ * we may add more executors than we need just to remove them later. (2) Executors should be added\n+ * quickly over time in case the maximum number of executors is very high. Otherwise, it will take\n+ * a long time to ramp up under heavy workloads.\n+ *\n+ * The remove policy is simpler: If an executor has been idle for K seconds (meaning it has not\n+ * been scheduled to run any tasks), then it is removed. This requires starting a timer on each\n+ * executor instead of just starting a global one as in the add case.\n+ *\n+ * There is no retry logic in either case. Because the requests to the cluster manager are\n+ * asynchronous, this class does not know whether a request has been granted until later. For\n+ * this reason, both add and remove are treated as best-effort only.\n+ *\n+ * The relevant Spark properties include the following:\n+ *\n+ *   spark.dynamicAllocation.enabled - Whether this feature is enabled\n+ *   spark.dynamicAllocation.minExecutors - Lower bound on the number of executors\n+ *   spark.dynamicAllocation.maxExecutors - Upper bound on the number of executors\n+ *\n+ *   spark.dynamicAllocation.addExecutorThresholdSeconds - How long before new executors are added\n+ *   spark.dynamicAllocation.addExecutorIntervalSeconds - How often to add new executors\n+ *   spark.dynamicAllocation.removeExecutorThresholdSeconds - How long before an executor is removed\n+ *\n+ * Synchronization: Because the schedulers in Spark are single-threaded, contention should only\n+ * arise when new executors register or when existing executors are removed, both of which are\n+ * relatively rare events with respect to task scheduling. Thus, synchronizing each method on the\n+ * same lock should not be expensive assuming biased locking is enabled in the JVM (on by default\n+ * for Java 6+). This may not be true, however, if the application itself runs multiple jobs\n+ * concurrently.\n+ *\n+ * Note: This is part of a larger implementation (SPARK-3174) and currently does not actually\n+ * request to add or remove executors. The mechanism to actually do this will be added separately,\n+ * e.g. in SPARK-3822 for Yarn.\n+ */\n+private[scheduler] class ExecutorAllocationManager(scheduler: TaskSchedulerImpl) extends Logging {\n+  import ExecutorAllocationManager._\n+\n+  private val conf = scheduler.conf\n+\n+  // Lower and upper bounds on the number of executors. These are required.\n+  private val minNumExecutors = conf.getInt(\"spark.dynamicAllocation.minExecutors\", -1)\n+  private val maxNumExecutors = conf.getInt(\"spark.dynamicAllocation.maxExecutors\", -1)\n+  if (minNumExecutors < 0 || maxNumExecutors < 0) {\n+    throw new SparkException(\"spark.dynamicAllocation.{min/max}Executors must be set!\")\n+  }\n+\n+  // How frequently to add and remove executors (seconds)\n+  private val addThresholdSeconds ="
  }],
  "prId": 2746
}]