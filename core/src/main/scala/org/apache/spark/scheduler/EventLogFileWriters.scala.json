[{
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Same comment.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-17T20:44:37Z",
    "diffHunk": "@@ -0,0 +1,444 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io._\n+import java.net.URI\n+\n+import scala.collection.mutable.Map"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "indent more. I'd keep the whole return type (including the `:`) in a separate line if it doesn't fit.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-17T20:47:57Z",
    "diffHunk": "@@ -0,0 +1,444 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io._\n+import java.net.URI\n+\n+import scala.collection.mutable.Map\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[scheduler] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path): (Option[FSDataOutputStream],\n+    Option[CountingOutputStream]) = {"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Same comment.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-17T20:52:56Z",
    "diffHunk": "@@ -0,0 +1,444 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io._\n+import java.net.URI\n+\n+import scala.collection.mutable.Map\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[scheduler] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path): (Option[FSDataOutputStream],\n+    Option[CountingOutputStream]) = {\n+\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    var hadoopDataStream: Option[FSDataOutputStream] = None\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(if (shouldAllowECLogs) {\n+          fileSystem.create(path)\n+        } else {\n+          SparkHadoopUtil.createNonECFile(fileSystem, path)\n+        })\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      val ostream = new CountingOutputStream(bstream)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+\n+      (hadoopDataStream, Some(ostream))\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it, -and which support setTimes(); it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  // ================ methods to be override ================"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Sentences are redundant. Pick one.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-17T20:53:16Z",
    "diffHunk": "@@ -0,0 +1,444 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io._\n+import java.net.URI\n+\n+import scala.collection.mutable.Map\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[scheduler] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path): (Option[FSDataOutputStream],\n+    Option[CountingOutputStream]) = {\n+\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    var hadoopDataStream: Option[FSDataOutputStream] = None\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(if (shouldAllowECLogs) {\n+          fileSystem.create(path)\n+        } else {\n+          SparkHadoopUtil.createNonECFile(fileSystem, path)\n+        })\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      val ostream = new CountingOutputStream(bstream)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+\n+      (hadoopDataStream, Some(ostream))\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it, -and which support setTimes(); it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  // ================ methods to be override ================\n+\n+  /** starts writer instance - initialize writer for event logging */"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Same comment about using `apply`.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-17T20:54:51Z",
    "diffHunk": "@@ -0,0 +1,444 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io._\n+import java.net.URI\n+\n+import scala.collection.mutable.Map\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[scheduler] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path): (Option[FSDataOutputStream],\n+    Option[CountingOutputStream]) = {\n+\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    var hadoopDataStream: Option[FSDataOutputStream] = None\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(if (shouldAllowECLogs) {\n+          fileSystem.create(path)\n+        } else {\n+          SparkHadoopUtil.createNonECFile(fileSystem, path)\n+        })\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      val ostream = new CountingOutputStream(bstream)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+\n+      (hadoopDataStream, Some(ostream))\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it, -and which support setTimes(); it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  // ================ methods to be override ================\n+\n+  /** starts writer instance - initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def createEventLogFileWriter("
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Base class already extends `Logging`.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-17T20:55:41Z",
    "diffHunk": "@@ -0,0 +1,444 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io._\n+import java.net.URI\n+\n+import scala.collection.mutable.Map\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[scheduler] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path): (Option[FSDataOutputStream],\n+    Option[CountingOutputStream]) = {\n+\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    var hadoopDataStream: Option[FSDataOutputStream] = None\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(if (shouldAllowECLogs) {\n+          fileSystem.create(path)\n+        } else {\n+          SparkHadoopUtil.createNonECFile(fileSystem, path)\n+        })\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      val ostream = new CountingOutputStream(bstream)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+\n+      (hadoopDataStream, Some(ostream))\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it, -and which support setTimes(); it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  // ================ methods to be override ================\n+\n+  /** starts writer instance - initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def createEventLogFileWriter(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) with Logging {"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`val (hadoopStream, outputStream) = initLogFile...`",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-17T20:56:40Z",
    "diffHunk": "@@ -0,0 +1,444 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io._\n+import java.net.URI\n+\n+import scala.collection.mutable.Map\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[scheduler] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path): (Option[FSDataOutputStream],\n+    Option[CountingOutputStream]) = {\n+\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    var hadoopDataStream: Option[FSDataOutputStream] = None\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(if (shouldAllowECLogs) {\n+          fileSystem.create(path)\n+        } else {\n+          SparkHadoopUtil.createNonECFile(fileSystem, path)\n+        })\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      val ostream = new CountingOutputStream(bstream)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+\n+      (hadoopDataStream, Some(ostream))\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it, -and which support setTimes(); it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  // ================ methods to be override ================\n+\n+  /** starts writer instance - initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def createEventLogFileWriter(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) with Logging {\n+\n+  override val logPath: String = SingleEventLogFileWriter.getLogPath(logBaseDir, appId,\n+    appAttemptId, compressionCodecName)\n+\n+  private val inProgressPath = logPath + EventLogFileWriter.IN_PROGRESS\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+\n+  private var writer: Option[PrintWriter] = None\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    val streams = initLogFile(new Path(inProgressPath))"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "If it's not defined, you're failing to initialize `writer`. BTW, shouldn't that one always be defined, meaning it shouldn't even be an `Option`?",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-17T20:57:46Z",
    "diffHunk": "@@ -0,0 +1,444 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io._\n+import java.net.URI\n+\n+import scala.collection.mutable.Map\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[scheduler] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path): (Option[FSDataOutputStream],\n+    Option[CountingOutputStream]) = {\n+\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    var hadoopDataStream: Option[FSDataOutputStream] = None\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(if (shouldAllowECLogs) {\n+          fileSystem.create(path)\n+        } else {\n+          SparkHadoopUtil.createNonECFile(fileSystem, path)\n+        })\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      val ostream = new CountingOutputStream(bstream)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+\n+      (hadoopDataStream, Some(ostream))\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it, -and which support setTimes(); it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  // ================ methods to be override ================\n+\n+  /** starts writer instance - initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def createEventLogFileWriter(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) with Logging {\n+\n+  override val logPath: String = SingleEventLogFileWriter.getLogPath(logBaseDir, appId,\n+    appAttemptId, compressionCodecName)\n+\n+  private val inProgressPath = logPath + EventLogFileWriter.IN_PROGRESS\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+\n+  private var writer: Option[PrintWriter] = None\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    val streams = initLogFile(new Path(inProgressPath))\n+    hadoopDataStream = streams._1\n+    if (streams._2.isDefined) {"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Uh, maybe I misunderstood the existing code. `writer` was `Option` so I thought it tolerates the case which it fails to initialize writer. Turned out it was just for classifying the thing before calling start(). I'll change `initLogFile` to not have `Option`.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-18T01:13:37Z",
    "diffHunk": "@@ -0,0 +1,444 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io._\n+import java.net.URI\n+\n+import scala.collection.mutable.Map\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[scheduler] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path): (Option[FSDataOutputStream],\n+    Option[CountingOutputStream]) = {\n+\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    var hadoopDataStream: Option[FSDataOutputStream] = None\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(if (shouldAllowECLogs) {\n+          fileSystem.create(path)\n+        } else {\n+          SparkHadoopUtil.createNonECFile(fileSystem, path)\n+        })\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      val ostream = new CountingOutputStream(bstream)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+\n+      (hadoopDataStream, Some(ostream))\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it, -and which support setTimes(); it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  // ================ methods to be override ================\n+\n+  /** starts writer instance - initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def createEventLogFileWriter(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) with Logging {\n+\n+  override val logPath: String = SingleEventLogFileWriter.getLogPath(logBaseDir, appId,\n+    appAttemptId, compressionCodecName)\n+\n+  private val inProgressPath = logPath + EventLogFileWriter.IN_PROGRESS\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+\n+  private var writer: Option[PrintWriter] = None\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    val streams = initLogFile(new Path(inProgressPath))\n+    hadoopDataStream = streams._1\n+    if (streams._2.isDefined) {"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "This is a long standing bug in the original code, but this should be explicitly setting the charset to UTF-8 (using `new PrintWriter(new OutputStreamWriter(...)`).\r\n\r\nThe reader side should too, although doing that now could potentially break old logs... we should open a bug for this.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-17T21:00:27Z",
    "diffHunk": "@@ -0,0 +1,444 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io._\n+import java.net.URI\n+\n+import scala.collection.mutable.Map\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[scheduler] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path): (Option[FSDataOutputStream],\n+    Option[CountingOutputStream]) = {\n+\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    var hadoopDataStream: Option[FSDataOutputStream] = None\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(if (shouldAllowECLogs) {\n+          fileSystem.create(path)\n+        } else {\n+          SparkHadoopUtil.createNonECFile(fileSystem, path)\n+        })\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      val ostream = new CountingOutputStream(bstream)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+\n+      (hadoopDataStream, Some(ostream))\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it, -and which support setTimes(); it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  // ================ methods to be override ================\n+\n+  /** starts writer instance - initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def createEventLogFileWriter(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) with Logging {\n+\n+  override val logPath: String = SingleEventLogFileWriter.getLogPath(logBaseDir, appId,\n+    appAttemptId, compressionCodecName)\n+\n+  private val inProgressPath = logPath + EventLogFileWriter.IN_PROGRESS\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+\n+  private var writer: Option[PrintWriter] = None\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    val streams = initLogFile(new Path(inProgressPath))\n+    hadoopDataStream = streams._1\n+    if (streams._2.isDefined) {\n+      writer = Some(new PrintWriter(streams._2.get))"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "OK. I'll not deal with this in PR, but file an issue for this. Thanks for letting me know. Btw possibility of breaking old logs may make us hesitate to apply the change.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-18T01:22:57Z",
    "diffHunk": "@@ -0,0 +1,444 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io._\n+import java.net.URI\n+\n+import scala.collection.mutable.Map\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[scheduler] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path): (Option[FSDataOutputStream],\n+    Option[CountingOutputStream]) = {\n+\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    var hadoopDataStream: Option[FSDataOutputStream] = None\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(if (shouldAllowECLogs) {\n+          fileSystem.create(path)\n+        } else {\n+          SparkHadoopUtil.createNonECFile(fileSystem, path)\n+        })\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      val ostream = new CountingOutputStream(bstream)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+\n+      (hadoopDataStream, Some(ostream))\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it, -and which support setTimes(); it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  // ================ methods to be override ================\n+\n+  /** starts writer instance - initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def createEventLogFileWriter(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) with Logging {\n+\n+  override val logPath: String = SingleEventLogFileWriter.getLogPath(logBaseDir, appId,\n+    appAttemptId, compressionCodecName)\n+\n+  private val inProgressPath = logPath + EventLogFileWriter.IN_PROGRESS\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+\n+  private var writer: Option[PrintWriter] = None\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    val streams = initLogFile(new Path(inProgressPath))\n+    hadoopDataStream = streams._1\n+    if (streams._2.isDefined) {\n+      writer = Some(new PrintWriter(streams._2.get))"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "https://issues.apache.org/jira/browse/SPARK-29160",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-18T22:51:50Z",
    "diffHunk": "@@ -0,0 +1,444 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io._\n+import java.net.URI\n+\n+import scala.collection.mutable.Map\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[scheduler] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path): (Option[FSDataOutputStream],\n+    Option[CountingOutputStream]) = {\n+\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    var hadoopDataStream: Option[FSDataOutputStream] = None\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(if (shouldAllowECLogs) {\n+          fileSystem.create(path)\n+        } else {\n+          SparkHadoopUtil.createNonECFile(fileSystem, path)\n+        })\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      val ostream = new CountingOutputStream(bstream)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+\n+      (hadoopDataStream, Some(ostream))\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it, -and which support setTimes(); it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  // ================ methods to be override ================\n+\n+  /** starts writer instance - initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def createEventLogFileWriter(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) with Logging {\n+\n+  override val logPath: String = SingleEventLogFileWriter.getLogPath(logBaseDir, appId,\n+    appAttemptId, compressionCodecName)\n+\n+  private val inProgressPath = logPath + EventLogFileWriter.IN_PROGRESS\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+\n+  private var writer: Option[PrintWriter] = None\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    val streams = initLogFile(new Path(inProgressPath))\n+    hadoopDataStream = streams._1\n+    if (streams._2.isDefined) {\n+      writer = Some(new PrintWriter(streams._2.get))"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "one directory per",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-17T21:01:40Z",
    "diffHunk": "@@ -0,0 +1,444 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io._\n+import java.net.URI\n+\n+import scala.collection.mutable.Map\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[scheduler] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path): (Option[FSDataOutputStream],\n+    Option[CountingOutputStream]) = {\n+\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    var hadoopDataStream: Option[FSDataOutputStream] = None\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(if (shouldAllowECLogs) {\n+          fileSystem.create(path)\n+        } else {\n+          SparkHadoopUtil.createNonECFile(fileSystem, path)\n+        })\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      val ostream = new CountingOutputStream(bstream)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+\n+      (hadoopDataStream, Some(ostream))\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it, -and which support setTimes(); it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  // ================ methods to be override ================\n+\n+  /** starts writer instance - initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def createEventLogFileWriter(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) with Logging {\n+\n+  override val logPath: String = SingleEventLogFileWriter.getLogPath(logBaseDir, appId,\n+    appAttemptId, compressionCodecName)\n+\n+  private val inProgressPath = logPath + EventLogFileWriter.IN_PROGRESS\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+\n+  private var writer: Option[PrintWriter] = None\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    val streams = initLogFile(new Path(inProgressPath))\n+    hadoopDataStream = streams._1\n+    if (streams._2.isDefined) {\n+      writer = Some(new PrintWriter(streams._2.get))\n+    }\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    // scalastyle:off println\n+    writer.foreach(_.println(eventJson))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  /**\n+   * Stop logging events. The event log file will be renamed so that it loses the\n+   * \".inprogress\" suffix.\n+   */\n+  override def stop(): Unit = {\n+    writer.foreach(_.close())\n+    renameFile(new Path(inProgressPath), new Path(logPath), shouldOverwrite)\n+  }\n+}\n+\n+object SingleEventLogFileWriter {\n+  /**\n+   * Return a file-system-safe path to the log file for the given application.\n+   *\n+   * Note that because we currently only create a single log file for each application,\n+   * we must encode all the information needed to parse this event log in the file name\n+   * instead of within the file itself. Otherwise, if the file is compressed, for instance,\n+   * we won't know which codec to use to decompress the metadata needed to open the file in\n+   * the first place.\n+   *\n+   * The log file name will identify the compression codec used for the contents, if any.\n+   * For example, app_123 for an uncompressed log, app_123.lzf for an LZF-compressed log.\n+   *\n+   * @param logBaseDir Directory where the log file will be written.\n+   * @param appId A unique app ID.\n+   * @param appAttemptId A unique attempt id of appId. May be the empty string.\n+   * @param compressionCodecName Name to identify the codec used to compress the contents\n+   *                             of the log, or None if compression is not enabled.\n+   * @return A path which consists of file-system-safe characters.\n+   */\n+  def getLogPath(\n+      logBaseDir: URI,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      compressionCodecName: Option[String] = None): String = {\n+    val codec = compressionCodecName.map(\".\" + _).getOrElse(\"\")\n+    new Path(logBaseDir).toString.stripSuffix(\"/\") + \"/\" +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId) + codec\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into multiple log files, rolled over via configured size.\n+ *\n+ * The class creates each directory per application, and stores event log files as well as"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "\"currently\" doesn't really explain things. Maybe you want to say \"count of bytes written before compression is applied\".",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-17T21:03:43Z",
    "diffHunk": "@@ -0,0 +1,444 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io._\n+import java.net.URI\n+\n+import scala.collection.mutable.Map\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[scheduler] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path): (Option[FSDataOutputStream],\n+    Option[CountingOutputStream]) = {\n+\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    var hadoopDataStream: Option[FSDataOutputStream] = None\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(if (shouldAllowECLogs) {\n+          fileSystem.create(path)\n+        } else {\n+          SparkHadoopUtil.createNonECFile(fileSystem, path)\n+        })\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      val ostream = new CountingOutputStream(bstream)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+\n+      (hadoopDataStream, Some(ostream))\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it, -and which support setTimes(); it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  // ================ methods to be override ================\n+\n+  /** starts writer instance - initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def createEventLogFileWriter(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) with Logging {\n+\n+  override val logPath: String = SingleEventLogFileWriter.getLogPath(logBaseDir, appId,\n+    appAttemptId, compressionCodecName)\n+\n+  private val inProgressPath = logPath + EventLogFileWriter.IN_PROGRESS\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+\n+  private var writer: Option[PrintWriter] = None\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    val streams = initLogFile(new Path(inProgressPath))\n+    hadoopDataStream = streams._1\n+    if (streams._2.isDefined) {\n+      writer = Some(new PrintWriter(streams._2.get))\n+    }\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    // scalastyle:off println\n+    writer.foreach(_.println(eventJson))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  /**\n+   * Stop logging events. The event log file will be renamed so that it loses the\n+   * \".inprogress\" suffix.\n+   */\n+  override def stop(): Unit = {\n+    writer.foreach(_.close())\n+    renameFile(new Path(inProgressPath), new Path(logPath), shouldOverwrite)\n+  }\n+}\n+\n+object SingleEventLogFileWriter {\n+  /**\n+   * Return a file-system-safe path to the log file for the given application.\n+   *\n+   * Note that because we currently only create a single log file for each application,\n+   * we must encode all the information needed to parse this event log in the file name\n+   * instead of within the file itself. Otherwise, if the file is compressed, for instance,\n+   * we won't know which codec to use to decompress the metadata needed to open the file in\n+   * the first place.\n+   *\n+   * The log file name will identify the compression codec used for the contents, if any.\n+   * For example, app_123 for an uncompressed log, app_123.lzf for an LZF-compressed log.\n+   *\n+   * @param logBaseDir Directory where the log file will be written.\n+   * @param appId A unique app ID.\n+   * @param appAttemptId A unique attempt id of appId. May be the empty string.\n+   * @param compressionCodecName Name to identify the codec used to compress the contents\n+   *                             of the log, or None if compression is not enabled.\n+   * @return A path which consists of file-system-safe characters.\n+   */\n+  def getLogPath(\n+      logBaseDir: URI,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      compressionCodecName: Option[String] = None): String = {\n+    val codec = compressionCodecName.map(\".\" + _).getOrElse(\"\")\n+    new Path(logBaseDir).toString.stripSuffix(\"/\") + \"/\" +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId) + codec\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into multiple log files, rolled over via configured size.\n+ *\n+ * The class creates each directory per application, and stores event log files as well as\n+ * metadata files. The name of directory and files in the directory would follow:\n+ *\n+ * - The name of directory: eventlog_v2_appId(_[appAttemptId])\n+ * - The prefix of name on event files: events_[sequence]_[appId](_[appAttemptId])(.[codec])\n+ *   - \"sequence\" would be monotonically increasing value\n+ * - The name of metadata (app. status) file name: appstatus_[appId](_[appAttemptId])(.inprogress)\n+ *\n+ * The writer will roll over the event log file when configured size is reached. Note that the\n+ * writer doesn't check the size on file being open for write: the writer tracks the count of bytes\n+ * being written currently."
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Repeating config names in doc strings (and their meaning) just makes it painful to keep things in sync.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-17T21:04:29Z",
    "diffHunk": "@@ -0,0 +1,444 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io._\n+import java.net.URI\n+\n+import scala.collection.mutable.Map\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[scheduler] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path): (Option[FSDataOutputStream],\n+    Option[CountingOutputStream]) = {\n+\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    var hadoopDataStream: Option[FSDataOutputStream] = None\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(if (shouldAllowECLogs) {\n+          fileSystem.create(path)\n+        } else {\n+          SparkHadoopUtil.createNonECFile(fileSystem, path)\n+        })\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      val ostream = new CountingOutputStream(bstream)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+\n+      (hadoopDataStream, Some(ostream))\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it, -and which support setTimes(); it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  // ================ methods to be override ================\n+\n+  /** starts writer instance - initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def createEventLogFileWriter(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) with Logging {\n+\n+  override val logPath: String = SingleEventLogFileWriter.getLogPath(logBaseDir, appId,\n+    appAttemptId, compressionCodecName)\n+\n+  private val inProgressPath = logPath + EventLogFileWriter.IN_PROGRESS\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+\n+  private var writer: Option[PrintWriter] = None\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    val streams = initLogFile(new Path(inProgressPath))\n+    hadoopDataStream = streams._1\n+    if (streams._2.isDefined) {\n+      writer = Some(new PrintWriter(streams._2.get))\n+    }\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    // scalastyle:off println\n+    writer.foreach(_.println(eventJson))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  /**\n+   * Stop logging events. The event log file will be renamed so that it loses the\n+   * \".inprogress\" suffix.\n+   */\n+  override def stop(): Unit = {\n+    writer.foreach(_.close())\n+    renameFile(new Path(inProgressPath), new Path(logPath), shouldOverwrite)\n+  }\n+}\n+\n+object SingleEventLogFileWriter {\n+  /**\n+   * Return a file-system-safe path to the log file for the given application.\n+   *\n+   * Note that because we currently only create a single log file for each application,\n+   * we must encode all the information needed to parse this event log in the file name\n+   * instead of within the file itself. Otherwise, if the file is compressed, for instance,\n+   * we won't know which codec to use to decompress the metadata needed to open the file in\n+   * the first place.\n+   *\n+   * The log file name will identify the compression codec used for the contents, if any.\n+   * For example, app_123 for an uncompressed log, app_123.lzf for an LZF-compressed log.\n+   *\n+   * @param logBaseDir Directory where the log file will be written.\n+   * @param appId A unique app ID.\n+   * @param appAttemptId A unique attempt id of appId. May be the empty string.\n+   * @param compressionCodecName Name to identify the codec used to compress the contents\n+   *                             of the log, or None if compression is not enabled.\n+   * @return A path which consists of file-system-safe characters.\n+   */\n+  def getLogPath(\n+      logBaseDir: URI,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      compressionCodecName: Option[String] = None): String = {\n+    val codec = compressionCodecName.map(\".\" + _).getOrElse(\"\")\n+    new Path(logBaseDir).toString.stripSuffix(\"/\") + \"/\" +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId) + codec\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into multiple log files, rolled over via configured size.\n+ *\n+ * The class creates each directory per application, and stores event log files as well as\n+ * metadata files. The name of directory and files in the directory would follow:\n+ *\n+ * - The name of directory: eventlog_v2_appId(_[appAttemptId])\n+ * - The prefix of name on event files: events_[sequence]_[appId](_[appAttemptId])(.[codec])\n+ *   - \"sequence\" would be monotonically increasing value\n+ * - The name of metadata (app. status) file name: appstatus_[appId](_[appAttemptId])(.inprogress)\n+ *\n+ * The writer will roll over the event log file when configured size is reached. Note that the\n+ * writer doesn't check the size on file being open for write: the writer tracks the count of bytes\n+ * being written currently.\n+ *\n+ * For metadata files, the class will leverage zero-byte file, as it provides minimized cost.\n+ *\n+ * The writer leverages the following configurable parameters:\n+ *   spark.eventLog.rollLog - Whether rolling over event log files is enabled."
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Agreed. While I'll remove these configs description here, do we want to remove existing altogether?",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-18T01:25:14Z",
    "diffHunk": "@@ -0,0 +1,444 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io._\n+import java.net.URI\n+\n+import scala.collection.mutable.Map\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[scheduler] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path): (Option[FSDataOutputStream],\n+    Option[CountingOutputStream]) = {\n+\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    var hadoopDataStream: Option[FSDataOutputStream] = None\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(if (shouldAllowECLogs) {\n+          fileSystem.create(path)\n+        } else {\n+          SparkHadoopUtil.createNonECFile(fileSystem, path)\n+        })\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      val ostream = new CountingOutputStream(bstream)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+\n+      (hadoopDataStream, Some(ostream))\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it, -and which support setTimes(); it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  // ================ methods to be override ================\n+\n+  /** starts writer instance - initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def createEventLogFileWriter(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) with Logging {\n+\n+  override val logPath: String = SingleEventLogFileWriter.getLogPath(logBaseDir, appId,\n+    appAttemptId, compressionCodecName)\n+\n+  private val inProgressPath = logPath + EventLogFileWriter.IN_PROGRESS\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+\n+  private var writer: Option[PrintWriter] = None\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    val streams = initLogFile(new Path(inProgressPath))\n+    hadoopDataStream = streams._1\n+    if (streams._2.isDefined) {\n+      writer = Some(new PrintWriter(streams._2.get))\n+    }\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    // scalastyle:off println\n+    writer.foreach(_.println(eventJson))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  /**\n+   * Stop logging events. The event log file will be renamed so that it loses the\n+   * \".inprogress\" suffix.\n+   */\n+  override def stop(): Unit = {\n+    writer.foreach(_.close())\n+    renameFile(new Path(inProgressPath), new Path(logPath), shouldOverwrite)\n+  }\n+}\n+\n+object SingleEventLogFileWriter {\n+  /**\n+   * Return a file-system-safe path to the log file for the given application.\n+   *\n+   * Note that because we currently only create a single log file for each application,\n+   * we must encode all the information needed to parse this event log in the file name\n+   * instead of within the file itself. Otherwise, if the file is compressed, for instance,\n+   * we won't know which codec to use to decompress the metadata needed to open the file in\n+   * the first place.\n+   *\n+   * The log file name will identify the compression codec used for the contents, if any.\n+   * For example, app_123 for an uncompressed log, app_123.lzf for an LZF-compressed log.\n+   *\n+   * @param logBaseDir Directory where the log file will be written.\n+   * @param appId A unique app ID.\n+   * @param appAttemptId A unique attempt id of appId. May be the empty string.\n+   * @param compressionCodecName Name to identify the codec used to compress the contents\n+   *                             of the log, or None if compression is not enabled.\n+   * @return A path which consists of file-system-safe characters.\n+   */\n+  def getLogPath(\n+      logBaseDir: URI,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      compressionCodecName: Option[String] = None): String = {\n+    val codec = compressionCodecName.map(\".\" + _).getOrElse(\"\")\n+    new Path(logBaseDir).toString.stripSuffix(\"/\") + \"/\" +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId) + codec\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into multiple log files, rolled over via configured size.\n+ *\n+ * The class creates each directory per application, and stores event log files as well as\n+ * metadata files. The name of directory and files in the directory would follow:\n+ *\n+ * - The name of directory: eventlog_v2_appId(_[appAttemptId])\n+ * - The prefix of name on event files: events_[sequence]_[appId](_[appAttemptId])(.[codec])\n+ *   - \"sequence\" would be monotonically increasing value\n+ * - The name of metadata (app. status) file name: appstatus_[appId](_[appAttemptId])(.inprogress)\n+ *\n+ * The writer will roll over the event log file when configured size is reached. Note that the\n+ * writer doesn't check the size on file being open for write: the writer tracks the count of bytes\n+ * being written currently.\n+ *\n+ * For metadata files, the class will leverage zero-byte file, as it provides minimized cost.\n+ *\n+ * The writer leverages the following configurable parameters:\n+ *   spark.eventLog.rollLog - Whether rolling over event log files is enabled."
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Just repeats the code.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-17T21:07:00Z",
    "diffHunk": "@@ -0,0 +1,444 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io._\n+import java.net.URI\n+\n+import scala.collection.mutable.Map\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[scheduler] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path): (Option[FSDataOutputStream],\n+    Option[CountingOutputStream]) = {\n+\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    var hadoopDataStream: Option[FSDataOutputStream] = None\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(if (shouldAllowECLogs) {\n+          fileSystem.create(path)\n+        } else {\n+          SparkHadoopUtil.createNonECFile(fileSystem, path)\n+        })\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      val ostream = new CountingOutputStream(bstream)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+\n+      (hadoopDataStream, Some(ostream))\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it, -and which support setTimes(); it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  // ================ methods to be override ================\n+\n+  /** starts writer instance - initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def createEventLogFileWriter(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) with Logging {\n+\n+  override val logPath: String = SingleEventLogFileWriter.getLogPath(logBaseDir, appId,\n+    appAttemptId, compressionCodecName)\n+\n+  private val inProgressPath = logPath + EventLogFileWriter.IN_PROGRESS\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+\n+  private var writer: Option[PrintWriter] = None\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    val streams = initLogFile(new Path(inProgressPath))\n+    hadoopDataStream = streams._1\n+    if (streams._2.isDefined) {\n+      writer = Some(new PrintWriter(streams._2.get))\n+    }\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    // scalastyle:off println\n+    writer.foreach(_.println(eventJson))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  /**\n+   * Stop logging events. The event log file will be renamed so that it loses the\n+   * \".inprogress\" suffix.\n+   */\n+  override def stop(): Unit = {\n+    writer.foreach(_.close())\n+    renameFile(new Path(inProgressPath), new Path(logPath), shouldOverwrite)\n+  }\n+}\n+\n+object SingleEventLogFileWriter {\n+  /**\n+   * Return a file-system-safe path to the log file for the given application.\n+   *\n+   * Note that because we currently only create a single log file for each application,\n+   * we must encode all the information needed to parse this event log in the file name\n+   * instead of within the file itself. Otherwise, if the file is compressed, for instance,\n+   * we won't know which codec to use to decompress the metadata needed to open the file in\n+   * the first place.\n+   *\n+   * The log file name will identify the compression codec used for the contents, if any.\n+   * For example, app_123 for an uncompressed log, app_123.lzf for an LZF-compressed log.\n+   *\n+   * @param logBaseDir Directory where the log file will be written.\n+   * @param appId A unique app ID.\n+   * @param appAttemptId A unique attempt id of appId. May be the empty string.\n+   * @param compressionCodecName Name to identify the codec used to compress the contents\n+   *                             of the log, or None if compression is not enabled.\n+   * @return A path which consists of file-system-safe characters.\n+   */\n+  def getLogPath(\n+      logBaseDir: URI,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      compressionCodecName: Option[String] = None): String = {\n+    val codec = compressionCodecName.map(\".\" + _).getOrElse(\"\")\n+    new Path(logBaseDir).toString.stripSuffix(\"/\") + \"/\" +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId) + codec\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into multiple log files, rolled over via configured size.\n+ *\n+ * The class creates each directory per application, and stores event log files as well as\n+ * metadata files. The name of directory and files in the directory would follow:\n+ *\n+ * - The name of directory: eventlog_v2_appId(_[appAttemptId])\n+ * - The prefix of name on event files: events_[sequence]_[appId](_[appAttemptId])(.[codec])\n+ *   - \"sequence\" would be monotonically increasing value\n+ * - The name of metadata (app. status) file name: appstatus_[appId](_[appAttemptId])(.inprogress)\n+ *\n+ * The writer will roll over the event log file when configured size is reached. Note that the\n+ * writer doesn't check the size on file being open for write: the writer tracks the count of bytes\n+ * being written currently.\n+ *\n+ * For metadata files, the class will leverage zero-byte file, as it provides minimized cost.\n+ *\n+ * The writer leverages the following configurable parameters:\n+ *   spark.eventLog.rollLog - Whether rolling over event log files is enabled.\n+ *   spark.eventLog.rollLog.maxFileSize - The max size of event log file to be rolled over.\n+ */\n+class RollingEventLogFilesWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  import RollingEventLogFilesWriter._\n+\n+  private val eventFileMaxLengthKiB = sparkConf.get(EVENT_LOG_ROLLED_EVENT_LOG_MAX_FILE_SIZE)\n+\n+  private val logDirForAppPath = getAppEventLogDirPath(logBaseDir, appId, appAttemptId)\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+  private var countingOutputStream: Option[CountingOutputStream] = None\n+  private var writer: Option[PrintWriter] = None\n+\n+  // seq and event log path will be updated soon in rollNewEventLogFile, which `start` will call\n+  private var sequence: Long = 0L\n+  private var currentEventLogFilePath: Path = logDirForAppPath\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    if (fileSystem.exists(logDirForAppPath) && shouldOverwrite) {\n+      // try to delete the directory"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "That's not the only way this can happen. Just delete the comment.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-17T21:07:21Z",
    "diffHunk": "@@ -0,0 +1,444 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io._\n+import java.net.URI\n+\n+import scala.collection.mutable.Map\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[scheduler] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path): (Option[FSDataOutputStream],\n+    Option[CountingOutputStream]) = {\n+\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    var hadoopDataStream: Option[FSDataOutputStream] = None\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(if (shouldAllowECLogs) {\n+          fileSystem.create(path)\n+        } else {\n+          SparkHadoopUtil.createNonECFile(fileSystem, path)\n+        })\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      val ostream = new CountingOutputStream(bstream)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+\n+      (hadoopDataStream, Some(ostream))\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it, -and which support setTimes(); it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  // ================ methods to be override ================\n+\n+  /** starts writer instance - initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def createEventLogFileWriter(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) with Logging {\n+\n+  override val logPath: String = SingleEventLogFileWriter.getLogPath(logBaseDir, appId,\n+    appAttemptId, compressionCodecName)\n+\n+  private val inProgressPath = logPath + EventLogFileWriter.IN_PROGRESS\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+\n+  private var writer: Option[PrintWriter] = None\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    val streams = initLogFile(new Path(inProgressPath))\n+    hadoopDataStream = streams._1\n+    if (streams._2.isDefined) {\n+      writer = Some(new PrintWriter(streams._2.get))\n+    }\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    // scalastyle:off println\n+    writer.foreach(_.println(eventJson))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  /**\n+   * Stop logging events. The event log file will be renamed so that it loses the\n+   * \".inprogress\" suffix.\n+   */\n+  override def stop(): Unit = {\n+    writer.foreach(_.close())\n+    renameFile(new Path(inProgressPath), new Path(logPath), shouldOverwrite)\n+  }\n+}\n+\n+object SingleEventLogFileWriter {\n+  /**\n+   * Return a file-system-safe path to the log file for the given application.\n+   *\n+   * Note that because we currently only create a single log file for each application,\n+   * we must encode all the information needed to parse this event log in the file name\n+   * instead of within the file itself. Otherwise, if the file is compressed, for instance,\n+   * we won't know which codec to use to decompress the metadata needed to open the file in\n+   * the first place.\n+   *\n+   * The log file name will identify the compression codec used for the contents, if any.\n+   * For example, app_123 for an uncompressed log, app_123.lzf for an LZF-compressed log.\n+   *\n+   * @param logBaseDir Directory where the log file will be written.\n+   * @param appId A unique app ID.\n+   * @param appAttemptId A unique attempt id of appId. May be the empty string.\n+   * @param compressionCodecName Name to identify the codec used to compress the contents\n+   *                             of the log, or None if compression is not enabled.\n+   * @return A path which consists of file-system-safe characters.\n+   */\n+  def getLogPath(\n+      logBaseDir: URI,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      compressionCodecName: Option[String] = None): String = {\n+    val codec = compressionCodecName.map(\".\" + _).getOrElse(\"\")\n+    new Path(logBaseDir).toString.stripSuffix(\"/\") + \"/\" +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId) + codec\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into multiple log files, rolled over via configured size.\n+ *\n+ * The class creates each directory per application, and stores event log files as well as\n+ * metadata files. The name of directory and files in the directory would follow:\n+ *\n+ * - The name of directory: eventlog_v2_appId(_[appAttemptId])\n+ * - The prefix of name on event files: events_[sequence]_[appId](_[appAttemptId])(.[codec])\n+ *   - \"sequence\" would be monotonically increasing value\n+ * - The name of metadata (app. status) file name: appstatus_[appId](_[appAttemptId])(.inprogress)\n+ *\n+ * The writer will roll over the event log file when configured size is reached. Note that the\n+ * writer doesn't check the size on file being open for write: the writer tracks the count of bytes\n+ * being written currently.\n+ *\n+ * For metadata files, the class will leverage zero-byte file, as it provides minimized cost.\n+ *\n+ * The writer leverages the following configurable parameters:\n+ *   spark.eventLog.rollLog - Whether rolling over event log files is enabled.\n+ *   spark.eventLog.rollLog.maxFileSize - The max size of event log file to be rolled over.\n+ */\n+class RollingEventLogFilesWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  import RollingEventLogFilesWriter._\n+\n+  private val eventFileMaxLengthKiB = sparkConf.get(EVENT_LOG_ROLLED_EVENT_LOG_MAX_FILE_SIZE)\n+\n+  private val logDirForAppPath = getAppEventLogDirPath(logBaseDir, appId, appAttemptId)\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+  private var countingOutputStream: Option[CountingOutputStream] = None\n+  private var writer: Option[PrintWriter] = None\n+\n+  // seq and event log path will be updated soon in rollNewEventLogFile, which `start` will call\n+  private var sequence: Long = 0L\n+  private var currentEventLogFilePath: Path = logDirForAppPath\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    if (fileSystem.exists(logDirForAppPath) && shouldOverwrite) {\n+      // try to delete the directory\n+      fileSystem.delete(logDirForAppPath, true)\n+    }\n+\n+    if (fileSystem.exists(logDirForAppPath)) {\n+      // we tried to delete the existing one, but failed"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`else` is unnecessary.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-17T21:07:36Z",
    "diffHunk": "@@ -0,0 +1,444 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io._\n+import java.net.URI\n+\n+import scala.collection.mutable.Map\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[scheduler] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path): (Option[FSDataOutputStream],\n+    Option[CountingOutputStream]) = {\n+\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    var hadoopDataStream: Option[FSDataOutputStream] = None\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(if (shouldAllowECLogs) {\n+          fileSystem.create(path)\n+        } else {\n+          SparkHadoopUtil.createNonECFile(fileSystem, path)\n+        })\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      val ostream = new CountingOutputStream(bstream)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+\n+      (hadoopDataStream, Some(ostream))\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it, -and which support setTimes(); it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  // ================ methods to be override ================\n+\n+  /** starts writer instance - initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def createEventLogFileWriter(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) with Logging {\n+\n+  override val logPath: String = SingleEventLogFileWriter.getLogPath(logBaseDir, appId,\n+    appAttemptId, compressionCodecName)\n+\n+  private val inProgressPath = logPath + EventLogFileWriter.IN_PROGRESS\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+\n+  private var writer: Option[PrintWriter] = None\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    val streams = initLogFile(new Path(inProgressPath))\n+    hadoopDataStream = streams._1\n+    if (streams._2.isDefined) {\n+      writer = Some(new PrintWriter(streams._2.get))\n+    }\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    // scalastyle:off println\n+    writer.foreach(_.println(eventJson))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  /**\n+   * Stop logging events. The event log file will be renamed so that it loses the\n+   * \".inprogress\" suffix.\n+   */\n+  override def stop(): Unit = {\n+    writer.foreach(_.close())\n+    renameFile(new Path(inProgressPath), new Path(logPath), shouldOverwrite)\n+  }\n+}\n+\n+object SingleEventLogFileWriter {\n+  /**\n+   * Return a file-system-safe path to the log file for the given application.\n+   *\n+   * Note that because we currently only create a single log file for each application,\n+   * we must encode all the information needed to parse this event log in the file name\n+   * instead of within the file itself. Otherwise, if the file is compressed, for instance,\n+   * we won't know which codec to use to decompress the metadata needed to open the file in\n+   * the first place.\n+   *\n+   * The log file name will identify the compression codec used for the contents, if any.\n+   * For example, app_123 for an uncompressed log, app_123.lzf for an LZF-compressed log.\n+   *\n+   * @param logBaseDir Directory where the log file will be written.\n+   * @param appId A unique app ID.\n+   * @param appAttemptId A unique attempt id of appId. May be the empty string.\n+   * @param compressionCodecName Name to identify the codec used to compress the contents\n+   *                             of the log, or None if compression is not enabled.\n+   * @return A path which consists of file-system-safe characters.\n+   */\n+  def getLogPath(\n+      logBaseDir: URI,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      compressionCodecName: Option[String] = None): String = {\n+    val codec = compressionCodecName.map(\".\" + _).getOrElse(\"\")\n+    new Path(logBaseDir).toString.stripSuffix(\"/\") + \"/\" +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId) + codec\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into multiple log files, rolled over via configured size.\n+ *\n+ * The class creates each directory per application, and stores event log files as well as\n+ * metadata files. The name of directory and files in the directory would follow:\n+ *\n+ * - The name of directory: eventlog_v2_appId(_[appAttemptId])\n+ * - The prefix of name on event files: events_[sequence]_[appId](_[appAttemptId])(.[codec])\n+ *   - \"sequence\" would be monotonically increasing value\n+ * - The name of metadata (app. status) file name: appstatus_[appId](_[appAttemptId])(.inprogress)\n+ *\n+ * The writer will roll over the event log file when configured size is reached. Note that the\n+ * writer doesn't check the size on file being open for write: the writer tracks the count of bytes\n+ * being written currently.\n+ *\n+ * For metadata files, the class will leverage zero-byte file, as it provides minimized cost.\n+ *\n+ * The writer leverages the following configurable parameters:\n+ *   spark.eventLog.rollLog - Whether rolling over event log files is enabled.\n+ *   spark.eventLog.rollLog.maxFileSize - The max size of event log file to be rolled over.\n+ */\n+class RollingEventLogFilesWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  import RollingEventLogFilesWriter._\n+\n+  private val eventFileMaxLengthKiB = sparkConf.get(EVENT_LOG_ROLLED_EVENT_LOG_MAX_FILE_SIZE)\n+\n+  private val logDirForAppPath = getAppEventLogDirPath(logBaseDir, appId, appAttemptId)\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+  private var countingOutputStream: Option[CountingOutputStream] = None\n+  private var writer: Option[PrintWriter] = None\n+\n+  // seq and event log path will be updated soon in rollNewEventLogFile, which `start` will call\n+  private var sequence: Long = 0L\n+  private var currentEventLogFilePath: Path = logDirForAppPath\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    if (fileSystem.exists(logDirForAppPath) && shouldOverwrite) {\n+      // try to delete the directory\n+      fileSystem.delete(logDirForAppPath, true)\n+    }\n+\n+    if (fileSystem.exists(logDirForAppPath)) {\n+      // we tried to delete the existing one, but failed\n+      throw new IOException(s\"Target log directory already exists ($logDirForAppPath)\")\n+    } else {"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Is this comment necessary? That's the whole purpose of this class...",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-17T21:08:58Z",
    "diffHunk": "@@ -0,0 +1,444 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io._\n+import java.net.URI\n+\n+import scala.collection.mutable.Map\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[scheduler] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path): (Option[FSDataOutputStream],\n+    Option[CountingOutputStream]) = {\n+\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    var hadoopDataStream: Option[FSDataOutputStream] = None\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(if (shouldAllowECLogs) {\n+          fileSystem.create(path)\n+        } else {\n+          SparkHadoopUtil.createNonECFile(fileSystem, path)\n+        })\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      val ostream = new CountingOutputStream(bstream)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+\n+      (hadoopDataStream, Some(ostream))\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it, -and which support setTimes(); it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  // ================ methods to be override ================\n+\n+  /** starts writer instance - initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def createEventLogFileWriter(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) with Logging {\n+\n+  override val logPath: String = SingleEventLogFileWriter.getLogPath(logBaseDir, appId,\n+    appAttemptId, compressionCodecName)\n+\n+  private val inProgressPath = logPath + EventLogFileWriter.IN_PROGRESS\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+\n+  private var writer: Option[PrintWriter] = None\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    val streams = initLogFile(new Path(inProgressPath))\n+    hadoopDataStream = streams._1\n+    if (streams._2.isDefined) {\n+      writer = Some(new PrintWriter(streams._2.get))\n+    }\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    // scalastyle:off println\n+    writer.foreach(_.println(eventJson))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  /**\n+   * Stop logging events. The event log file will be renamed so that it loses the\n+   * \".inprogress\" suffix.\n+   */\n+  override def stop(): Unit = {\n+    writer.foreach(_.close())\n+    renameFile(new Path(inProgressPath), new Path(logPath), shouldOverwrite)\n+  }\n+}\n+\n+object SingleEventLogFileWriter {\n+  /**\n+   * Return a file-system-safe path to the log file for the given application.\n+   *\n+   * Note that because we currently only create a single log file for each application,\n+   * we must encode all the information needed to parse this event log in the file name\n+   * instead of within the file itself. Otherwise, if the file is compressed, for instance,\n+   * we won't know which codec to use to decompress the metadata needed to open the file in\n+   * the first place.\n+   *\n+   * The log file name will identify the compression codec used for the contents, if any.\n+   * For example, app_123 for an uncompressed log, app_123.lzf for an LZF-compressed log.\n+   *\n+   * @param logBaseDir Directory where the log file will be written.\n+   * @param appId A unique app ID.\n+   * @param appAttemptId A unique attempt id of appId. May be the empty string.\n+   * @param compressionCodecName Name to identify the codec used to compress the contents\n+   *                             of the log, or None if compression is not enabled.\n+   * @return A path which consists of file-system-safe characters.\n+   */\n+  def getLogPath(\n+      logBaseDir: URI,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      compressionCodecName: Option[String] = None): String = {\n+    val codec = compressionCodecName.map(\".\" + _).getOrElse(\"\")\n+    new Path(logBaseDir).toString.stripSuffix(\"/\") + \"/\" +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId) + codec\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into multiple log files, rolled over via configured size.\n+ *\n+ * The class creates each directory per application, and stores event log files as well as\n+ * metadata files. The name of directory and files in the directory would follow:\n+ *\n+ * - The name of directory: eventlog_v2_appId(_[appAttemptId])\n+ * - The prefix of name on event files: events_[sequence]_[appId](_[appAttemptId])(.[codec])\n+ *   - \"sequence\" would be monotonically increasing value\n+ * - The name of metadata (app. status) file name: appstatus_[appId](_[appAttemptId])(.inprogress)\n+ *\n+ * The writer will roll over the event log file when configured size is reached. Note that the\n+ * writer doesn't check the size on file being open for write: the writer tracks the count of bytes\n+ * being written currently.\n+ *\n+ * For metadata files, the class will leverage zero-byte file, as it provides minimized cost.\n+ *\n+ * The writer leverages the following configurable parameters:\n+ *   spark.eventLog.rollLog - Whether rolling over event log files is enabled.\n+ *   spark.eventLog.rollLog.maxFileSize - The max size of event log file to be rolled over.\n+ */\n+class RollingEventLogFilesWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  import RollingEventLogFilesWriter._\n+\n+  private val eventFileMaxLengthKiB = sparkConf.get(EVENT_LOG_ROLLED_EVENT_LOG_MAX_FILE_SIZE)\n+\n+  private val logDirForAppPath = getAppEventLogDirPath(logBaseDir, appId, appAttemptId)\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+  private var countingOutputStream: Option[CountingOutputStream] = None\n+  private var writer: Option[PrintWriter] = None\n+\n+  // seq and event log path will be updated soon in rollNewEventLogFile, which `start` will call\n+  private var sequence: Long = 0L\n+  private var currentEventLogFilePath: Path = logDirForAppPath\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    if (fileSystem.exists(logDirForAppPath) && shouldOverwrite) {\n+      // try to delete the directory\n+      fileSystem.delete(logDirForAppPath, true)\n+    }\n+\n+    if (fileSystem.exists(logDirForAppPath)) {\n+      // we tried to delete the existing one, but failed\n+      throw new IOException(s\"Target log directory already exists ($logDirForAppPath)\")\n+    } else {\n+      fileSystem.mkdirs(logDirForAppPath, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      createAppStatusFile(inProgress = true)\n+      rollNewEventLogFile(None)\n+    }\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    writer.foreach { w =>\n+      val currentLen = countingOutputStream.get.getBytesWritten\n+      if (currentLen + eventJson.length > eventFileMaxLengthKiB * 1024) {\n+        rollNewEventLogFile(Some(w))\n+      }\n+    }\n+\n+    // if the event log file is rolled over, writer will refer next event log file"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Yeah I'm trying to add comments as many as possible so that someone would understand without going through design doc, but agree these comments are too verbose. Will remove.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-18T01:27:18Z",
    "diffHunk": "@@ -0,0 +1,444 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io._\n+import java.net.URI\n+\n+import scala.collection.mutable.Map\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[scheduler] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path): (Option[FSDataOutputStream],\n+    Option[CountingOutputStream]) = {\n+\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    var hadoopDataStream: Option[FSDataOutputStream] = None\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(if (shouldAllowECLogs) {\n+          fileSystem.create(path)\n+        } else {\n+          SparkHadoopUtil.createNonECFile(fileSystem, path)\n+        })\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      val ostream = new CountingOutputStream(bstream)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+\n+      (hadoopDataStream, Some(ostream))\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it, -and which support setTimes(); it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  // ================ methods to be override ================\n+\n+  /** starts writer instance - initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def createEventLogFileWriter(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) with Logging {\n+\n+  override val logPath: String = SingleEventLogFileWriter.getLogPath(logBaseDir, appId,\n+    appAttemptId, compressionCodecName)\n+\n+  private val inProgressPath = logPath + EventLogFileWriter.IN_PROGRESS\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+\n+  private var writer: Option[PrintWriter] = None\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    val streams = initLogFile(new Path(inProgressPath))\n+    hadoopDataStream = streams._1\n+    if (streams._2.isDefined) {\n+      writer = Some(new PrintWriter(streams._2.get))\n+    }\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    // scalastyle:off println\n+    writer.foreach(_.println(eventJson))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  /**\n+   * Stop logging events. The event log file will be renamed so that it loses the\n+   * \".inprogress\" suffix.\n+   */\n+  override def stop(): Unit = {\n+    writer.foreach(_.close())\n+    renameFile(new Path(inProgressPath), new Path(logPath), shouldOverwrite)\n+  }\n+}\n+\n+object SingleEventLogFileWriter {\n+  /**\n+   * Return a file-system-safe path to the log file for the given application.\n+   *\n+   * Note that because we currently only create a single log file for each application,\n+   * we must encode all the information needed to parse this event log in the file name\n+   * instead of within the file itself. Otherwise, if the file is compressed, for instance,\n+   * we won't know which codec to use to decompress the metadata needed to open the file in\n+   * the first place.\n+   *\n+   * The log file name will identify the compression codec used for the contents, if any.\n+   * For example, app_123 for an uncompressed log, app_123.lzf for an LZF-compressed log.\n+   *\n+   * @param logBaseDir Directory where the log file will be written.\n+   * @param appId A unique app ID.\n+   * @param appAttemptId A unique attempt id of appId. May be the empty string.\n+   * @param compressionCodecName Name to identify the codec used to compress the contents\n+   *                             of the log, or None if compression is not enabled.\n+   * @return A path which consists of file-system-safe characters.\n+   */\n+  def getLogPath(\n+      logBaseDir: URI,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      compressionCodecName: Option[String] = None): String = {\n+    val codec = compressionCodecName.map(\".\" + _).getOrElse(\"\")\n+    new Path(logBaseDir).toString.stripSuffix(\"/\") + \"/\" +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId) + codec\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into multiple log files, rolled over via configured size.\n+ *\n+ * The class creates each directory per application, and stores event log files as well as\n+ * metadata files. The name of directory and files in the directory would follow:\n+ *\n+ * - The name of directory: eventlog_v2_appId(_[appAttemptId])\n+ * - The prefix of name on event files: events_[sequence]_[appId](_[appAttemptId])(.[codec])\n+ *   - \"sequence\" would be monotonically increasing value\n+ * - The name of metadata (app. status) file name: appstatus_[appId](_[appAttemptId])(.inprogress)\n+ *\n+ * The writer will roll over the event log file when configured size is reached. Note that the\n+ * writer doesn't check the size on file being open for write: the writer tracks the count of bytes\n+ * being written currently.\n+ *\n+ * For metadata files, the class will leverage zero-byte file, as it provides minimized cost.\n+ *\n+ * The writer leverages the following configurable parameters:\n+ *   spark.eventLog.rollLog - Whether rolling over event log files is enabled.\n+ *   spark.eventLog.rollLog.maxFileSize - The max size of event log file to be rolled over.\n+ */\n+class RollingEventLogFilesWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  import RollingEventLogFilesWriter._\n+\n+  private val eventFileMaxLengthKiB = sparkConf.get(EVENT_LOG_ROLLED_EVENT_LOG_MAX_FILE_SIZE)\n+\n+  private val logDirForAppPath = getAppEventLogDirPath(logBaseDir, appId, appAttemptId)\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+  private var countingOutputStream: Option[CountingOutputStream] = None\n+  private var writer: Option[PrintWriter] = None\n+\n+  // seq and event log path will be updated soon in rollNewEventLogFile, which `start` will call\n+  private var sequence: Long = 0L\n+  private var currentEventLogFilePath: Path = logDirForAppPath\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    if (fileSystem.exists(logDirForAppPath) && shouldOverwrite) {\n+      // try to delete the directory\n+      fileSystem.delete(logDirForAppPath, true)\n+    }\n+\n+    if (fileSystem.exists(logDirForAppPath)) {\n+      // we tried to delete the existing one, but failed\n+      throw new IOException(s\"Target log directory already exists ($logDirForAppPath)\")\n+    } else {\n+      fileSystem.mkdirs(logDirForAppPath, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      createAppStatusFile(inProgress = true)\n+      rollNewEventLogFile(None)\n+    }\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    writer.foreach { w =>\n+      val currentLen = countingOutputStream.get.getBytesWritten\n+      if (currentLen + eventJson.length > eventFileMaxLengthKiB * 1024) {\n+        rollNewEventLogFile(Some(w))\n+      }\n+    }\n+\n+    // if the event log file is rolled over, writer will refer next event log file"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Same comment as before (`val (blah1, blah2) = ...`).\r\n\r\nAlso, isn't `CountingOutputStream` only needed by this class? So why not add that wrapper here instead of in the parent class?",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-17T21:11:08Z",
    "diffHunk": "@@ -0,0 +1,444 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io._\n+import java.net.URI\n+\n+import scala.collection.mutable.Map\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[scheduler] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path): (Option[FSDataOutputStream],\n+    Option[CountingOutputStream]) = {\n+\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    var hadoopDataStream: Option[FSDataOutputStream] = None\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(if (shouldAllowECLogs) {\n+          fileSystem.create(path)\n+        } else {\n+          SparkHadoopUtil.createNonECFile(fileSystem, path)\n+        })\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      val ostream = new CountingOutputStream(bstream)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+\n+      (hadoopDataStream, Some(ostream))\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it, -and which support setTimes(); it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  // ================ methods to be override ================\n+\n+  /** starts writer instance - initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def createEventLogFileWriter(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) with Logging {\n+\n+  override val logPath: String = SingleEventLogFileWriter.getLogPath(logBaseDir, appId,\n+    appAttemptId, compressionCodecName)\n+\n+  private val inProgressPath = logPath + EventLogFileWriter.IN_PROGRESS\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+\n+  private var writer: Option[PrintWriter] = None\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    val streams = initLogFile(new Path(inProgressPath))\n+    hadoopDataStream = streams._1\n+    if (streams._2.isDefined) {\n+      writer = Some(new PrintWriter(streams._2.get))\n+    }\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    // scalastyle:off println\n+    writer.foreach(_.println(eventJson))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  /**\n+   * Stop logging events. The event log file will be renamed so that it loses the\n+   * \".inprogress\" suffix.\n+   */\n+  override def stop(): Unit = {\n+    writer.foreach(_.close())\n+    renameFile(new Path(inProgressPath), new Path(logPath), shouldOverwrite)\n+  }\n+}\n+\n+object SingleEventLogFileWriter {\n+  /**\n+   * Return a file-system-safe path to the log file for the given application.\n+   *\n+   * Note that because we currently only create a single log file for each application,\n+   * we must encode all the information needed to parse this event log in the file name\n+   * instead of within the file itself. Otherwise, if the file is compressed, for instance,\n+   * we won't know which codec to use to decompress the metadata needed to open the file in\n+   * the first place.\n+   *\n+   * The log file name will identify the compression codec used for the contents, if any.\n+   * For example, app_123 for an uncompressed log, app_123.lzf for an LZF-compressed log.\n+   *\n+   * @param logBaseDir Directory where the log file will be written.\n+   * @param appId A unique app ID.\n+   * @param appAttemptId A unique attempt id of appId. May be the empty string.\n+   * @param compressionCodecName Name to identify the codec used to compress the contents\n+   *                             of the log, or None if compression is not enabled.\n+   * @return A path which consists of file-system-safe characters.\n+   */\n+  def getLogPath(\n+      logBaseDir: URI,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      compressionCodecName: Option[String] = None): String = {\n+    val codec = compressionCodecName.map(\".\" + _).getOrElse(\"\")\n+    new Path(logBaseDir).toString.stripSuffix(\"/\") + \"/\" +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId) + codec\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into multiple log files, rolled over via configured size.\n+ *\n+ * The class creates each directory per application, and stores event log files as well as\n+ * metadata files. The name of directory and files in the directory would follow:\n+ *\n+ * - The name of directory: eventlog_v2_appId(_[appAttemptId])\n+ * - The prefix of name on event files: events_[sequence]_[appId](_[appAttemptId])(.[codec])\n+ *   - \"sequence\" would be monotonically increasing value\n+ * - The name of metadata (app. status) file name: appstatus_[appId](_[appAttemptId])(.inprogress)\n+ *\n+ * The writer will roll over the event log file when configured size is reached. Note that the\n+ * writer doesn't check the size on file being open for write: the writer tracks the count of bytes\n+ * being written currently.\n+ *\n+ * For metadata files, the class will leverage zero-byte file, as it provides minimized cost.\n+ *\n+ * The writer leverages the following configurable parameters:\n+ *   spark.eventLog.rollLog - Whether rolling over event log files is enabled.\n+ *   spark.eventLog.rollLog.maxFileSize - The max size of event log file to be rolled over.\n+ */\n+class RollingEventLogFilesWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  import RollingEventLogFilesWriter._\n+\n+  private val eventFileMaxLengthKiB = sparkConf.get(EVENT_LOG_ROLLED_EVENT_LOG_MAX_FILE_SIZE)\n+\n+  private val logDirForAppPath = getAppEventLogDirPath(logBaseDir, appId, appAttemptId)\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+  private var countingOutputStream: Option[CountingOutputStream] = None\n+  private var writer: Option[PrintWriter] = None\n+\n+  // seq and event log path will be updated soon in rollNewEventLogFile, which `start` will call\n+  private var sequence: Long = 0L\n+  private var currentEventLogFilePath: Path = logDirForAppPath\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    if (fileSystem.exists(logDirForAppPath) && shouldOverwrite) {\n+      // try to delete the directory\n+      fileSystem.delete(logDirForAppPath, true)\n+    }\n+\n+    if (fileSystem.exists(logDirForAppPath)) {\n+      // we tried to delete the existing one, but failed\n+      throw new IOException(s\"Target log directory already exists ($logDirForAppPath)\")\n+    } else {\n+      fileSystem.mkdirs(logDirForAppPath, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      createAppStatusFile(inProgress = true)\n+      rollNewEventLogFile(None)\n+    }\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    writer.foreach { w =>\n+      val currentLen = countingOutputStream.get.getBytesWritten\n+      if (currentLen + eventJson.length > eventFileMaxLengthKiB * 1024) {\n+        rollNewEventLogFile(Some(w))\n+      }\n+    }\n+\n+    // if the event log file is rolled over, writer will refer next event log file\n+\n+    // scalastyle:off println\n+    writer.foreach(_.println(eventJson))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  private def rollNewEventLogFile(w: Option[PrintWriter]): Unit = {\n+    writer.foreach(_.close())\n+\n+    sequence += 1\n+    currentEventLogFilePath = getEventLogFilePath(logDirForAppPath, appId, appAttemptId, sequence,\n+      compressionCodecName)\n+\n+    val streams = initLogFile(currentEventLogFilePath)"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Ah yes that sounds much better! Will apply.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-18T01:30:37Z",
    "diffHunk": "@@ -0,0 +1,444 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io._\n+import java.net.URI\n+\n+import scala.collection.mutable.Map\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[scheduler] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path): (Option[FSDataOutputStream],\n+    Option[CountingOutputStream]) = {\n+\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    var hadoopDataStream: Option[FSDataOutputStream] = None\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(if (shouldAllowECLogs) {\n+          fileSystem.create(path)\n+        } else {\n+          SparkHadoopUtil.createNonECFile(fileSystem, path)\n+        })\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      val ostream = new CountingOutputStream(bstream)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+\n+      (hadoopDataStream, Some(ostream))\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it, -and which support setTimes(); it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  // ================ methods to be override ================\n+\n+  /** starts writer instance - initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def createEventLogFileWriter(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) with Logging {\n+\n+  override val logPath: String = SingleEventLogFileWriter.getLogPath(logBaseDir, appId,\n+    appAttemptId, compressionCodecName)\n+\n+  private val inProgressPath = logPath + EventLogFileWriter.IN_PROGRESS\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+\n+  private var writer: Option[PrintWriter] = None\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    val streams = initLogFile(new Path(inProgressPath))\n+    hadoopDataStream = streams._1\n+    if (streams._2.isDefined) {\n+      writer = Some(new PrintWriter(streams._2.get))\n+    }\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    // scalastyle:off println\n+    writer.foreach(_.println(eventJson))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  /**\n+   * Stop logging events. The event log file will be renamed so that it loses the\n+   * \".inprogress\" suffix.\n+   */\n+  override def stop(): Unit = {\n+    writer.foreach(_.close())\n+    renameFile(new Path(inProgressPath), new Path(logPath), shouldOverwrite)\n+  }\n+}\n+\n+object SingleEventLogFileWriter {\n+  /**\n+   * Return a file-system-safe path to the log file for the given application.\n+   *\n+   * Note that because we currently only create a single log file for each application,\n+   * we must encode all the information needed to parse this event log in the file name\n+   * instead of within the file itself. Otherwise, if the file is compressed, for instance,\n+   * we won't know which codec to use to decompress the metadata needed to open the file in\n+   * the first place.\n+   *\n+   * The log file name will identify the compression codec used for the contents, if any.\n+   * For example, app_123 for an uncompressed log, app_123.lzf for an LZF-compressed log.\n+   *\n+   * @param logBaseDir Directory where the log file will be written.\n+   * @param appId A unique app ID.\n+   * @param appAttemptId A unique attempt id of appId. May be the empty string.\n+   * @param compressionCodecName Name to identify the codec used to compress the contents\n+   *                             of the log, or None if compression is not enabled.\n+   * @return A path which consists of file-system-safe characters.\n+   */\n+  def getLogPath(\n+      logBaseDir: URI,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      compressionCodecName: Option[String] = None): String = {\n+    val codec = compressionCodecName.map(\".\" + _).getOrElse(\"\")\n+    new Path(logBaseDir).toString.stripSuffix(\"/\") + \"/\" +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId) + codec\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into multiple log files, rolled over via configured size.\n+ *\n+ * The class creates each directory per application, and stores event log files as well as\n+ * metadata files. The name of directory and files in the directory would follow:\n+ *\n+ * - The name of directory: eventlog_v2_appId(_[appAttemptId])\n+ * - The prefix of name on event files: events_[sequence]_[appId](_[appAttemptId])(.[codec])\n+ *   - \"sequence\" would be monotonically increasing value\n+ * - The name of metadata (app. status) file name: appstatus_[appId](_[appAttemptId])(.inprogress)\n+ *\n+ * The writer will roll over the event log file when configured size is reached. Note that the\n+ * writer doesn't check the size on file being open for write: the writer tracks the count of bytes\n+ * being written currently.\n+ *\n+ * For metadata files, the class will leverage zero-byte file, as it provides minimized cost.\n+ *\n+ * The writer leverages the following configurable parameters:\n+ *   spark.eventLog.rollLog - Whether rolling over event log files is enabled.\n+ *   spark.eventLog.rollLog.maxFileSize - The max size of event log file to be rolled over.\n+ */\n+class RollingEventLogFilesWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  import RollingEventLogFilesWriter._\n+\n+  private val eventFileMaxLengthKiB = sparkConf.get(EVENT_LOG_ROLLED_EVENT_LOG_MAX_FILE_SIZE)\n+\n+  private val logDirForAppPath = getAppEventLogDirPath(logBaseDir, appId, appAttemptId)\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+  private var countingOutputStream: Option[CountingOutputStream] = None\n+  private var writer: Option[PrintWriter] = None\n+\n+  // seq and event log path will be updated soon in rollNewEventLogFile, which `start` will call\n+  private var sequence: Long = 0L\n+  private var currentEventLogFilePath: Path = logDirForAppPath\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    if (fileSystem.exists(logDirForAppPath) && shouldOverwrite) {\n+      // try to delete the directory\n+      fileSystem.delete(logDirForAppPath, true)\n+    }\n+\n+    if (fileSystem.exists(logDirForAppPath)) {\n+      // we tried to delete the existing one, but failed\n+      throw new IOException(s\"Target log directory already exists ($logDirForAppPath)\")\n+    } else {\n+      fileSystem.mkdirs(logDirForAppPath, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      createAppStatusFile(inProgress = true)\n+      rollNewEventLogFile(None)\n+    }\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    writer.foreach { w =>\n+      val currentLen = countingOutputStream.get.getBytesWritten\n+      if (currentLen + eventJson.length > eventFileMaxLengthKiB * 1024) {\n+        rollNewEventLogFile(Some(w))\n+      }\n+    }\n+\n+    // if the event log file is rolled over, writer will refer next event log file\n+\n+    // scalastyle:off println\n+    writer.foreach(_.println(eventJson))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  private def rollNewEventLogFile(w: Option[PrintWriter]): Unit = {\n+    writer.foreach(_.close())\n+\n+    sequence += 1\n+    currentEventLogFilePath = getEventLogFilePath(logDirForAppPath, appId, appAttemptId, sequence,\n+      compressionCodecName)\n+\n+    val streams = initLogFile(currentEventLogFilePath)"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Same as before. This feels like it should never happen, and that the output should not be an `Option`.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-17T21:13:08Z",
    "diffHunk": "@@ -0,0 +1,444 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io._\n+import java.net.URI\n+\n+import scala.collection.mutable.Map\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[scheduler] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path): (Option[FSDataOutputStream],\n+    Option[CountingOutputStream]) = {\n+\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    var hadoopDataStream: Option[FSDataOutputStream] = None\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(if (shouldAllowECLogs) {\n+          fileSystem.create(path)\n+        } else {\n+          SparkHadoopUtil.createNonECFile(fileSystem, path)\n+        })\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      val ostream = new CountingOutputStream(bstream)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+\n+      (hadoopDataStream, Some(ostream))\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it, -and which support setTimes(); it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  // ================ methods to be override ================\n+\n+  /** starts writer instance - initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def createEventLogFileWriter(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) with Logging {\n+\n+  override val logPath: String = SingleEventLogFileWriter.getLogPath(logBaseDir, appId,\n+    appAttemptId, compressionCodecName)\n+\n+  private val inProgressPath = logPath + EventLogFileWriter.IN_PROGRESS\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+\n+  private var writer: Option[PrintWriter] = None\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    val streams = initLogFile(new Path(inProgressPath))\n+    hadoopDataStream = streams._1\n+    if (streams._2.isDefined) {\n+      writer = Some(new PrintWriter(streams._2.get))\n+    }\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    // scalastyle:off println\n+    writer.foreach(_.println(eventJson))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  /**\n+   * Stop logging events. The event log file will be renamed so that it loses the\n+   * \".inprogress\" suffix.\n+   */\n+  override def stop(): Unit = {\n+    writer.foreach(_.close())\n+    renameFile(new Path(inProgressPath), new Path(logPath), shouldOverwrite)\n+  }\n+}\n+\n+object SingleEventLogFileWriter {\n+  /**\n+   * Return a file-system-safe path to the log file for the given application.\n+   *\n+   * Note that because we currently only create a single log file for each application,\n+   * we must encode all the information needed to parse this event log in the file name\n+   * instead of within the file itself. Otherwise, if the file is compressed, for instance,\n+   * we won't know which codec to use to decompress the metadata needed to open the file in\n+   * the first place.\n+   *\n+   * The log file name will identify the compression codec used for the contents, if any.\n+   * For example, app_123 for an uncompressed log, app_123.lzf for an LZF-compressed log.\n+   *\n+   * @param logBaseDir Directory where the log file will be written.\n+   * @param appId A unique app ID.\n+   * @param appAttemptId A unique attempt id of appId. May be the empty string.\n+   * @param compressionCodecName Name to identify the codec used to compress the contents\n+   *                             of the log, or None if compression is not enabled.\n+   * @return A path which consists of file-system-safe characters.\n+   */\n+  def getLogPath(\n+      logBaseDir: URI,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      compressionCodecName: Option[String] = None): String = {\n+    val codec = compressionCodecName.map(\".\" + _).getOrElse(\"\")\n+    new Path(logBaseDir).toString.stripSuffix(\"/\") + \"/\" +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId) + codec\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into multiple log files, rolled over via configured size.\n+ *\n+ * The class creates each directory per application, and stores event log files as well as\n+ * metadata files. The name of directory and files in the directory would follow:\n+ *\n+ * - The name of directory: eventlog_v2_appId(_[appAttemptId])\n+ * - The prefix of name on event files: events_[sequence]_[appId](_[appAttemptId])(.[codec])\n+ *   - \"sequence\" would be monotonically increasing value\n+ * - The name of metadata (app. status) file name: appstatus_[appId](_[appAttemptId])(.inprogress)\n+ *\n+ * The writer will roll over the event log file when configured size is reached. Note that the\n+ * writer doesn't check the size on file being open for write: the writer tracks the count of bytes\n+ * being written currently.\n+ *\n+ * For metadata files, the class will leverage zero-byte file, as it provides minimized cost.\n+ *\n+ * The writer leverages the following configurable parameters:\n+ *   spark.eventLog.rollLog - Whether rolling over event log files is enabled.\n+ *   spark.eventLog.rollLog.maxFileSize - The max size of event log file to be rolled over.\n+ */\n+class RollingEventLogFilesWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  import RollingEventLogFilesWriter._\n+\n+  private val eventFileMaxLengthKiB = sparkConf.get(EVENT_LOG_ROLLED_EVENT_LOG_MAX_FILE_SIZE)\n+\n+  private val logDirForAppPath = getAppEventLogDirPath(logBaseDir, appId, appAttemptId)\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+  private var countingOutputStream: Option[CountingOutputStream] = None\n+  private var writer: Option[PrintWriter] = None\n+\n+  // seq and event log path will be updated soon in rollNewEventLogFile, which `start` will call\n+  private var sequence: Long = 0L\n+  private var currentEventLogFilePath: Path = logDirForAppPath\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    if (fileSystem.exists(logDirForAppPath) && shouldOverwrite) {\n+      // try to delete the directory\n+      fileSystem.delete(logDirForAppPath, true)\n+    }\n+\n+    if (fileSystem.exists(logDirForAppPath)) {\n+      // we tried to delete the existing one, but failed\n+      throw new IOException(s\"Target log directory already exists ($logDirForAppPath)\")\n+    } else {\n+      fileSystem.mkdirs(logDirForAppPath, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      createAppStatusFile(inProgress = true)\n+      rollNewEventLogFile(None)\n+    }\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    writer.foreach { w =>\n+      val currentLen = countingOutputStream.get.getBytesWritten\n+      if (currentLen + eventJson.length > eventFileMaxLengthKiB * 1024) {\n+        rollNewEventLogFile(Some(w))\n+      }\n+    }\n+\n+    // if the event log file is rolled over, writer will refer next event log file\n+\n+    // scalastyle:off println\n+    writer.foreach(_.println(eventJson))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  private def rollNewEventLogFile(w: Option[PrintWriter]): Unit = {\n+    writer.foreach(_.close())\n+\n+    sequence += 1\n+    currentEventLogFilePath = getEventLogFilePath(logDirForAppPath, appId, appAttemptId, sequence,\n+      compressionCodecName)\n+\n+    val streams = initLogFile(currentEventLogFilePath)\n+    hadoopDataStream = streams._1\n+    countingOutputStream = streams._2\n+    if (countingOutputStream.isDefined) {\n+      writer = Some(new PrintWriter(streams._2.get))\n+    } else {\n+      writer = None"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Isn't this always going to close the current writer and replace it with a new one?\r\n\r\nSo the parameter seems unnecessary.\r\n\r\nAlso, the `New` in the name is redundant.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-17T21:13:36Z",
    "diffHunk": "@@ -0,0 +1,444 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io._\n+import java.net.URI\n+\n+import scala.collection.mutable.Map\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[scheduler] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path): (Option[FSDataOutputStream],\n+    Option[CountingOutputStream]) = {\n+\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    var hadoopDataStream: Option[FSDataOutputStream] = None\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(if (shouldAllowECLogs) {\n+          fileSystem.create(path)\n+        } else {\n+          SparkHadoopUtil.createNonECFile(fileSystem, path)\n+        })\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      val ostream = new CountingOutputStream(bstream)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+\n+      (hadoopDataStream, Some(ostream))\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it, -and which support setTimes(); it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  // ================ methods to be override ================\n+\n+  /** starts writer instance - initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def createEventLogFileWriter(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) with Logging {\n+\n+  override val logPath: String = SingleEventLogFileWriter.getLogPath(logBaseDir, appId,\n+    appAttemptId, compressionCodecName)\n+\n+  private val inProgressPath = logPath + EventLogFileWriter.IN_PROGRESS\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+\n+  private var writer: Option[PrintWriter] = None\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    val streams = initLogFile(new Path(inProgressPath))\n+    hadoopDataStream = streams._1\n+    if (streams._2.isDefined) {\n+      writer = Some(new PrintWriter(streams._2.get))\n+    }\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    // scalastyle:off println\n+    writer.foreach(_.println(eventJson))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  /**\n+   * Stop logging events. The event log file will be renamed so that it loses the\n+   * \".inprogress\" suffix.\n+   */\n+  override def stop(): Unit = {\n+    writer.foreach(_.close())\n+    renameFile(new Path(inProgressPath), new Path(logPath), shouldOverwrite)\n+  }\n+}\n+\n+object SingleEventLogFileWriter {\n+  /**\n+   * Return a file-system-safe path to the log file for the given application.\n+   *\n+   * Note that because we currently only create a single log file for each application,\n+   * we must encode all the information needed to parse this event log in the file name\n+   * instead of within the file itself. Otherwise, if the file is compressed, for instance,\n+   * we won't know which codec to use to decompress the metadata needed to open the file in\n+   * the first place.\n+   *\n+   * The log file name will identify the compression codec used for the contents, if any.\n+   * For example, app_123 for an uncompressed log, app_123.lzf for an LZF-compressed log.\n+   *\n+   * @param logBaseDir Directory where the log file will be written.\n+   * @param appId A unique app ID.\n+   * @param appAttemptId A unique attempt id of appId. May be the empty string.\n+   * @param compressionCodecName Name to identify the codec used to compress the contents\n+   *                             of the log, or None if compression is not enabled.\n+   * @return A path which consists of file-system-safe characters.\n+   */\n+  def getLogPath(\n+      logBaseDir: URI,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      compressionCodecName: Option[String] = None): String = {\n+    val codec = compressionCodecName.map(\".\" + _).getOrElse(\"\")\n+    new Path(logBaseDir).toString.stripSuffix(\"/\") + \"/\" +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId) + codec\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into multiple log files, rolled over via configured size.\n+ *\n+ * The class creates each directory per application, and stores event log files as well as\n+ * metadata files. The name of directory and files in the directory would follow:\n+ *\n+ * - The name of directory: eventlog_v2_appId(_[appAttemptId])\n+ * - The prefix of name on event files: events_[sequence]_[appId](_[appAttemptId])(.[codec])\n+ *   - \"sequence\" would be monotonically increasing value\n+ * - The name of metadata (app. status) file name: appstatus_[appId](_[appAttemptId])(.inprogress)\n+ *\n+ * The writer will roll over the event log file when configured size is reached. Note that the\n+ * writer doesn't check the size on file being open for write: the writer tracks the count of bytes\n+ * being written currently.\n+ *\n+ * For metadata files, the class will leverage zero-byte file, as it provides minimized cost.\n+ *\n+ * The writer leverages the following configurable parameters:\n+ *   spark.eventLog.rollLog - Whether rolling over event log files is enabled.\n+ *   spark.eventLog.rollLog.maxFileSize - The max size of event log file to be rolled over.\n+ */\n+class RollingEventLogFilesWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  import RollingEventLogFilesWriter._\n+\n+  private val eventFileMaxLengthKiB = sparkConf.get(EVENT_LOG_ROLLED_EVENT_LOG_MAX_FILE_SIZE)\n+\n+  private val logDirForAppPath = getAppEventLogDirPath(logBaseDir, appId, appAttemptId)\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+  private var countingOutputStream: Option[CountingOutputStream] = None\n+  private var writer: Option[PrintWriter] = None\n+\n+  // seq and event log path will be updated soon in rollNewEventLogFile, which `start` will call\n+  private var sequence: Long = 0L\n+  private var currentEventLogFilePath: Path = logDirForAppPath\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    if (fileSystem.exists(logDirForAppPath) && shouldOverwrite) {\n+      // try to delete the directory\n+      fileSystem.delete(logDirForAppPath, true)\n+    }\n+\n+    if (fileSystem.exists(logDirForAppPath)) {\n+      // we tried to delete the existing one, but failed\n+      throw new IOException(s\"Target log directory already exists ($logDirForAppPath)\")\n+    } else {\n+      fileSystem.mkdirs(logDirForAppPath, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      createAppStatusFile(inProgress = true)\n+      rollNewEventLogFile(None)\n+    }\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    writer.foreach { w =>\n+      val currentLen = countingOutputStream.get.getBytesWritten\n+      if (currentLen + eventJson.length > eventFileMaxLengthKiB * 1024) {\n+        rollNewEventLogFile(Some(w))\n+      }\n+    }\n+\n+    // if the event log file is rolled over, writer will refer next event log file\n+\n+    // scalastyle:off println\n+    writer.foreach(_.println(eventJson))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  private def rollNewEventLogFile(w: Option[PrintWriter]): Unit = {"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Nice finding! I guess I was trying to do something and missed to remove parameter. Will remove.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-18T01:34:19Z",
    "diffHunk": "@@ -0,0 +1,444 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io._\n+import java.net.URI\n+\n+import scala.collection.mutable.Map\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[scheduler] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path): (Option[FSDataOutputStream],\n+    Option[CountingOutputStream]) = {\n+\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    var hadoopDataStream: Option[FSDataOutputStream] = None\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(if (shouldAllowECLogs) {\n+          fileSystem.create(path)\n+        } else {\n+          SparkHadoopUtil.createNonECFile(fileSystem, path)\n+        })\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      val ostream = new CountingOutputStream(bstream)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+\n+      (hadoopDataStream, Some(ostream))\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it, -and which support setTimes(); it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  // ================ methods to be override ================\n+\n+  /** starts writer instance - initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def createEventLogFileWriter(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) with Logging {\n+\n+  override val logPath: String = SingleEventLogFileWriter.getLogPath(logBaseDir, appId,\n+    appAttemptId, compressionCodecName)\n+\n+  private val inProgressPath = logPath + EventLogFileWriter.IN_PROGRESS\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+\n+  private var writer: Option[PrintWriter] = None\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    val streams = initLogFile(new Path(inProgressPath))\n+    hadoopDataStream = streams._1\n+    if (streams._2.isDefined) {\n+      writer = Some(new PrintWriter(streams._2.get))\n+    }\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    // scalastyle:off println\n+    writer.foreach(_.println(eventJson))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  /**\n+   * Stop logging events. The event log file will be renamed so that it loses the\n+   * \".inprogress\" suffix.\n+   */\n+  override def stop(): Unit = {\n+    writer.foreach(_.close())\n+    renameFile(new Path(inProgressPath), new Path(logPath), shouldOverwrite)\n+  }\n+}\n+\n+object SingleEventLogFileWriter {\n+  /**\n+   * Return a file-system-safe path to the log file for the given application.\n+   *\n+   * Note that because we currently only create a single log file for each application,\n+   * we must encode all the information needed to parse this event log in the file name\n+   * instead of within the file itself. Otherwise, if the file is compressed, for instance,\n+   * we won't know which codec to use to decompress the metadata needed to open the file in\n+   * the first place.\n+   *\n+   * The log file name will identify the compression codec used for the contents, if any.\n+   * For example, app_123 for an uncompressed log, app_123.lzf for an LZF-compressed log.\n+   *\n+   * @param logBaseDir Directory where the log file will be written.\n+   * @param appId A unique app ID.\n+   * @param appAttemptId A unique attempt id of appId. May be the empty string.\n+   * @param compressionCodecName Name to identify the codec used to compress the contents\n+   *                             of the log, or None if compression is not enabled.\n+   * @return A path which consists of file-system-safe characters.\n+   */\n+  def getLogPath(\n+      logBaseDir: URI,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      compressionCodecName: Option[String] = None): String = {\n+    val codec = compressionCodecName.map(\".\" + _).getOrElse(\"\")\n+    new Path(logBaseDir).toString.stripSuffix(\"/\") + \"/\" +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId) + codec\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into multiple log files, rolled over via configured size.\n+ *\n+ * The class creates each directory per application, and stores event log files as well as\n+ * metadata files. The name of directory and files in the directory would follow:\n+ *\n+ * - The name of directory: eventlog_v2_appId(_[appAttemptId])\n+ * - The prefix of name on event files: events_[sequence]_[appId](_[appAttemptId])(.[codec])\n+ *   - \"sequence\" would be monotonically increasing value\n+ * - The name of metadata (app. status) file name: appstatus_[appId](_[appAttemptId])(.inprogress)\n+ *\n+ * The writer will roll over the event log file when configured size is reached. Note that the\n+ * writer doesn't check the size on file being open for write: the writer tracks the count of bytes\n+ * being written currently.\n+ *\n+ * For metadata files, the class will leverage zero-byte file, as it provides minimized cost.\n+ *\n+ * The writer leverages the following configurable parameters:\n+ *   spark.eventLog.rollLog - Whether rolling over event log files is enabled.\n+ *   spark.eventLog.rollLog.maxFileSize - The max size of event log file to be rolled over.\n+ */\n+class RollingEventLogFilesWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  import RollingEventLogFilesWriter._\n+\n+  private val eventFileMaxLengthKiB = sparkConf.get(EVENT_LOG_ROLLED_EVENT_LOG_MAX_FILE_SIZE)\n+\n+  private val logDirForAppPath = getAppEventLogDirPath(logBaseDir, appId, appAttemptId)\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+  private var countingOutputStream: Option[CountingOutputStream] = None\n+  private var writer: Option[PrintWriter] = None\n+\n+  // seq and event log path will be updated soon in rollNewEventLogFile, which `start` will call\n+  private var sequence: Long = 0L\n+  private var currentEventLogFilePath: Path = logDirForAppPath\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    if (fileSystem.exists(logDirForAppPath) && shouldOverwrite) {\n+      // try to delete the directory\n+      fileSystem.delete(logDirForAppPath, true)\n+    }\n+\n+    if (fileSystem.exists(logDirForAppPath)) {\n+      // we tried to delete the existing one, but failed\n+      throw new IOException(s\"Target log directory already exists ($logDirForAppPath)\")\n+    } else {\n+      fileSystem.mkdirs(logDirForAppPath, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      createAppStatusFile(inProgress = true)\n+      rollNewEventLogFile(None)\n+    }\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    writer.foreach { w =>\n+      val currentLen = countingOutputStream.get.getBytesWritten\n+      if (currentLen + eventJson.length > eventFileMaxLengthKiB * 1024) {\n+        rollNewEventLogFile(Some(w))\n+      }\n+    }\n+\n+    // if the event log file is rolled over, writer will refer next event log file\n+\n+    // scalastyle:off println\n+    writer.foreach(_.println(eventJson))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  private def rollNewEventLogFile(w: Option[PrintWriter]): Unit = {"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Is there any point in calling `initLogFile` here instead of just `fs.create()`?",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-17T21:15:57Z",
    "diffHunk": "@@ -0,0 +1,444 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io._\n+import java.net.URI\n+\n+import scala.collection.mutable.Map\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[scheduler] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path): (Option[FSDataOutputStream],\n+    Option[CountingOutputStream]) = {\n+\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    var hadoopDataStream: Option[FSDataOutputStream] = None\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(if (shouldAllowECLogs) {\n+          fileSystem.create(path)\n+        } else {\n+          SparkHadoopUtil.createNonECFile(fileSystem, path)\n+        })\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      val ostream = new CountingOutputStream(bstream)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+\n+      (hadoopDataStream, Some(ostream))\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it, -and which support setTimes(); it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  // ================ methods to be override ================\n+\n+  /** starts writer instance - initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def createEventLogFileWriter(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) with Logging {\n+\n+  override val logPath: String = SingleEventLogFileWriter.getLogPath(logBaseDir, appId,\n+    appAttemptId, compressionCodecName)\n+\n+  private val inProgressPath = logPath + EventLogFileWriter.IN_PROGRESS\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+\n+  private var writer: Option[PrintWriter] = None\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    val streams = initLogFile(new Path(inProgressPath))\n+    hadoopDataStream = streams._1\n+    if (streams._2.isDefined) {\n+      writer = Some(new PrintWriter(streams._2.get))\n+    }\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    // scalastyle:off println\n+    writer.foreach(_.println(eventJson))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  /**\n+   * Stop logging events. The event log file will be renamed so that it loses the\n+   * \".inprogress\" suffix.\n+   */\n+  override def stop(): Unit = {\n+    writer.foreach(_.close())\n+    renameFile(new Path(inProgressPath), new Path(logPath), shouldOverwrite)\n+  }\n+}\n+\n+object SingleEventLogFileWriter {\n+  /**\n+   * Return a file-system-safe path to the log file for the given application.\n+   *\n+   * Note that because we currently only create a single log file for each application,\n+   * we must encode all the information needed to parse this event log in the file name\n+   * instead of within the file itself. Otherwise, if the file is compressed, for instance,\n+   * we won't know which codec to use to decompress the metadata needed to open the file in\n+   * the first place.\n+   *\n+   * The log file name will identify the compression codec used for the contents, if any.\n+   * For example, app_123 for an uncompressed log, app_123.lzf for an LZF-compressed log.\n+   *\n+   * @param logBaseDir Directory where the log file will be written.\n+   * @param appId A unique app ID.\n+   * @param appAttemptId A unique attempt id of appId. May be the empty string.\n+   * @param compressionCodecName Name to identify the codec used to compress the contents\n+   *                             of the log, or None if compression is not enabled.\n+   * @return A path which consists of file-system-safe characters.\n+   */\n+  def getLogPath(\n+      logBaseDir: URI,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      compressionCodecName: Option[String] = None): String = {\n+    val codec = compressionCodecName.map(\".\" + _).getOrElse(\"\")\n+    new Path(logBaseDir).toString.stripSuffix(\"/\") + \"/\" +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId) + codec\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into multiple log files, rolled over via configured size.\n+ *\n+ * The class creates each directory per application, and stores event log files as well as\n+ * metadata files. The name of directory and files in the directory would follow:\n+ *\n+ * - The name of directory: eventlog_v2_appId(_[appAttemptId])\n+ * - The prefix of name on event files: events_[sequence]_[appId](_[appAttemptId])(.[codec])\n+ *   - \"sequence\" would be monotonically increasing value\n+ * - The name of metadata (app. status) file name: appstatus_[appId](_[appAttemptId])(.inprogress)\n+ *\n+ * The writer will roll over the event log file when configured size is reached. Note that the\n+ * writer doesn't check the size on file being open for write: the writer tracks the count of bytes\n+ * being written currently.\n+ *\n+ * For metadata files, the class will leverage zero-byte file, as it provides minimized cost.\n+ *\n+ * The writer leverages the following configurable parameters:\n+ *   spark.eventLog.rollLog - Whether rolling over event log files is enabled.\n+ *   spark.eventLog.rollLog.maxFileSize - The max size of event log file to be rolled over.\n+ */\n+class RollingEventLogFilesWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  import RollingEventLogFilesWriter._\n+\n+  private val eventFileMaxLengthKiB = sparkConf.get(EVENT_LOG_ROLLED_EVENT_LOG_MAX_FILE_SIZE)\n+\n+  private val logDirForAppPath = getAppEventLogDirPath(logBaseDir, appId, appAttemptId)\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+  private var countingOutputStream: Option[CountingOutputStream] = None\n+  private var writer: Option[PrintWriter] = None\n+\n+  // seq and event log path will be updated soon in rollNewEventLogFile, which `start` will call\n+  private var sequence: Long = 0L\n+  private var currentEventLogFilePath: Path = logDirForAppPath\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    if (fileSystem.exists(logDirForAppPath) && shouldOverwrite) {\n+      // try to delete the directory\n+      fileSystem.delete(logDirForAppPath, true)\n+    }\n+\n+    if (fileSystem.exists(logDirForAppPath)) {\n+      // we tried to delete the existing one, but failed\n+      throw new IOException(s\"Target log directory already exists ($logDirForAppPath)\")\n+    } else {\n+      fileSystem.mkdirs(logDirForAppPath, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      createAppStatusFile(inProgress = true)\n+      rollNewEventLogFile(None)\n+    }\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    writer.foreach { w =>\n+      val currentLen = countingOutputStream.get.getBytesWritten\n+      if (currentLen + eventJson.length > eventFileMaxLengthKiB * 1024) {\n+        rollNewEventLogFile(Some(w))\n+      }\n+    }\n+\n+    // if the event log file is rolled over, writer will refer next event log file\n+\n+    // scalastyle:off println\n+    writer.foreach(_.println(eventJson))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  private def rollNewEventLogFile(w: Option[PrintWriter]): Unit = {\n+    writer.foreach(_.close())\n+\n+    sequence += 1\n+    currentEventLogFilePath = getEventLogFilePath(logDirForAppPath, appId, appAttemptId, sequence,\n+      compressionCodecName)\n+\n+    val streams = initLogFile(currentEventLogFilePath)\n+    hadoopDataStream = streams._1\n+    countingOutputStream = streams._2\n+    if (countingOutputStream.isDefined) {\n+      writer = Some(new PrintWriter(streams._2.get))\n+    } else {\n+      writer = None\n+    }\n+  }\n+\n+  override def stop(): Unit = {\n+    writer.foreach(_.close())\n+    val appStatusPathIncomplete = getAppStatusFilePath(logDirForAppPath, appId, appAttemptId,\n+      inProgress = true)\n+    val appStatusPathComplete = getAppStatusFilePath(logDirForAppPath, appId, appAttemptId,\n+      inProgress = false)\n+    renameFile(appStatusPathIncomplete, appStatusPathComplete, overwrite = true)\n+  }\n+\n+  override def logPath: String = logDirForAppPath.toString\n+\n+  private def createAppStatusFile(inProgress: Boolean): Unit = {\n+    val appStatusPath = getAppStatusFilePath(logDirForAppPath, appId, appAttemptId, inProgress)\n+    val streams = initLogFile(appStatusPath)"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "You're right. Will change.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-18T01:37:36Z",
    "diffHunk": "@@ -0,0 +1,444 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io._\n+import java.net.URI\n+\n+import scala.collection.mutable.Map\n+\n+import org.apache.commons.compress.utils.CountingOutputStream\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, FSDataOutputStream, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The base class of writer which will write event logs into file.\n+ *\n+ * The following configurable parameters are available to tune the behavior of writing:\n+ *   spark.eventLog.compress - Whether to compress logged events\n+ *   spark.eventLog.compression.codec - The codec to compress logged events\n+ *   spark.eventLog.overwrite - Whether to overwrite any existing files\n+ *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams\n+ *\n+ * Note that descendant classes can maintain its own parameters: refer the javadoc of each class\n+ * for more details.\n+ *\n+ * NOTE: CountingOutputStream being returned by \"initLogFile\" counts \"non-compressed\" bytes.\n+ */\n+abstract class EventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration) extends Logging {\n+\n+  protected val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)\n+  protected val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)\n+  protected val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)\n+  protected val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt\n+  protected val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)\n+  protected val compressionCodec =\n+    if (shouldCompress) {\n+      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))\n+    } else {\n+      None\n+    }\n+\n+  private[scheduler] val compressionCodecName = compressionCodec.map { c =>\n+    CompressionCodec.getShortName(c.getClass.getName)\n+  }\n+\n+  protected def requireLogBaseDirAsDirectory(): Unit = {\n+    if (!fileSystem.getFileStatus(new Path(logBaseDir)).isDirectory) {\n+      throw new IllegalArgumentException(s\"Log directory $logBaseDir is not a directory.\")\n+    }\n+  }\n+\n+  protected def initLogFile(path: Path): (Option[FSDataOutputStream],\n+    Option[CountingOutputStream]) = {\n+\n+    if (shouldOverwrite && fileSystem.delete(path, true)) {\n+      logWarning(s\"Event log $path already exists. Overwriting...\")\n+    }\n+\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+    val uri = path.toUri\n+\n+    var hadoopDataStream: Option[FSDataOutputStream] = None\n+    /* The Hadoop LocalFileSystem (r1.0.4) has known issues with syncing (HADOOP-7844).\n+     * Therefore, for local files, use FileOutputStream instead. */\n+    val dstream =\n+      if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+        new FileOutputStream(uri.getPath)\n+      } else {\n+        hadoopDataStream = Some(if (shouldAllowECLogs) {\n+          fileSystem.create(path)\n+        } else {\n+          SparkHadoopUtil.createNonECFile(fileSystem, path)\n+        })\n+        hadoopDataStream.get\n+      }\n+\n+    try {\n+      val cstream = compressionCodec.map(_.compressedOutputStream(dstream)).getOrElse(dstream)\n+      val bstream = new BufferedOutputStream(cstream, outputBufferSize)\n+      val ostream = new CountingOutputStream(bstream)\n+      fileSystem.setPermission(path, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      logInfo(s\"Logging events to $path\")\n+\n+      (hadoopDataStream, Some(ostream))\n+    } catch {\n+      case e: Exception =>\n+        dstream.close()\n+        throw e\n+    }\n+  }\n+\n+  protected def renameFile(src: Path, dest: Path, overwrite: Boolean): Unit = {\n+    if (fileSystem.exists(dest)) {\n+      if (overwrite) {\n+        logWarning(s\"Event log $dest already exists. Overwriting...\")\n+        if (!fileSystem.delete(dest, true)) {\n+          logWarning(s\"Error deleting $dest\")\n+        }\n+      } else {\n+        throw new IOException(s\"Target log file already exists ($dest)\")\n+      }\n+    }\n+    fileSystem.rename(src, dest)\n+    // touch file to ensure modtime is current across those filesystems where rename()\n+    // does not set it, -and which support setTimes(); it's a no-op on most object stores\n+    try {\n+      fileSystem.setTimes(dest, System.currentTimeMillis(), -1)\n+    } catch {\n+      case e: Exception => logDebug(s\"failed to set time of $dest\", e)\n+    }\n+  }\n+\n+  // ================ methods to be override ================\n+\n+  /** starts writer instance - initialize writer for event logging */\n+  def start(): Unit\n+\n+  /** writes JSON format of event to file */\n+  def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit\n+\n+  /** stops writer - indicating the application has been completed */\n+  def stop(): Unit\n+\n+  /** returns representative path of log */\n+  def logPath: String\n+}\n+\n+object EventLogFileWriter {\n+  // Suffix applied to the names of files still being written by applications.\n+  val IN_PROGRESS = \".inprogress\"\n+\n+  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)\n+\n+  def createEventLogFileWriter(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    if (sparkConf.get(EVENT_LOG_ENABLE_ROLLING)) {\n+      new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    } else {\n+      new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+    }\n+  }\n+\n+  def nameForAppAndAttempt(appId: String, appAttemptId: Option[String]): String = {\n+    val base = Utils.sanitizeDirName(appId)\n+    if (appAttemptId.isDefined) {\n+      base + \"_\" + Utils.sanitizeDirName(appAttemptId.get)\n+    } else {\n+      base\n+    }\n+  }\n+\n+  def codecName(log: Path): Option[String] = {\n+    // Compression codec is encoded as an extension, e.g. app_123.lzf\n+    // Since we sanitize the app ID to not include periods, it is safe to split on it\n+    val logName = log.getName.stripSuffix(IN_PROGRESS)\n+    logName.split(\"\\\\.\").tail.lastOption\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into single file.\n+ */\n+class SingleEventLogFileWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) with Logging {\n+\n+  override val logPath: String = SingleEventLogFileWriter.getLogPath(logBaseDir, appId,\n+    appAttemptId, compressionCodecName)\n+\n+  private val inProgressPath = logPath + EventLogFileWriter.IN_PROGRESS\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+\n+  private var writer: Option[PrintWriter] = None\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    val streams = initLogFile(new Path(inProgressPath))\n+    hadoopDataStream = streams._1\n+    if (streams._2.isDefined) {\n+      writer = Some(new PrintWriter(streams._2.get))\n+    }\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    // scalastyle:off println\n+    writer.foreach(_.println(eventJson))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  /**\n+   * Stop logging events. The event log file will be renamed so that it loses the\n+   * \".inprogress\" suffix.\n+   */\n+  override def stop(): Unit = {\n+    writer.foreach(_.close())\n+    renameFile(new Path(inProgressPath), new Path(logPath), shouldOverwrite)\n+  }\n+}\n+\n+object SingleEventLogFileWriter {\n+  /**\n+   * Return a file-system-safe path to the log file for the given application.\n+   *\n+   * Note that because we currently only create a single log file for each application,\n+   * we must encode all the information needed to parse this event log in the file name\n+   * instead of within the file itself. Otherwise, if the file is compressed, for instance,\n+   * we won't know which codec to use to decompress the metadata needed to open the file in\n+   * the first place.\n+   *\n+   * The log file name will identify the compression codec used for the contents, if any.\n+   * For example, app_123 for an uncompressed log, app_123.lzf for an LZF-compressed log.\n+   *\n+   * @param logBaseDir Directory where the log file will be written.\n+   * @param appId A unique app ID.\n+   * @param appAttemptId A unique attempt id of appId. May be the empty string.\n+   * @param compressionCodecName Name to identify the codec used to compress the contents\n+   *                             of the log, or None if compression is not enabled.\n+   * @return A path which consists of file-system-safe characters.\n+   */\n+  def getLogPath(\n+      logBaseDir: URI,\n+      appId: String,\n+      appAttemptId: Option[String],\n+      compressionCodecName: Option[String] = None): String = {\n+    val codec = compressionCodecName.map(\".\" + _).getOrElse(\"\")\n+    new Path(logBaseDir).toString.stripSuffix(\"/\") + \"/\" +\n+      EventLogFileWriter.nameForAppAndAttempt(appId, appAttemptId) + codec\n+  }\n+}\n+\n+/**\n+ * The writer to write event logs into multiple log files, rolled over via configured size.\n+ *\n+ * The class creates each directory per application, and stores event log files as well as\n+ * metadata files. The name of directory and files in the directory would follow:\n+ *\n+ * - The name of directory: eventlog_v2_appId(_[appAttemptId])\n+ * - The prefix of name on event files: events_[sequence]_[appId](_[appAttemptId])(.[codec])\n+ *   - \"sequence\" would be monotonically increasing value\n+ * - The name of metadata (app. status) file name: appstatus_[appId](_[appAttemptId])(.inprogress)\n+ *\n+ * The writer will roll over the event log file when configured size is reached. Note that the\n+ * writer doesn't check the size on file being open for write: the writer tracks the count of bytes\n+ * being written currently.\n+ *\n+ * For metadata files, the class will leverage zero-byte file, as it provides minimized cost.\n+ *\n+ * The writer leverages the following configurable parameters:\n+ *   spark.eventLog.rollLog - Whether rolling over event log files is enabled.\n+ *   spark.eventLog.rollLog.maxFileSize - The max size of event log file to be rolled over.\n+ */\n+class RollingEventLogFilesWriter(\n+    appId: String,\n+    appAttemptId : Option[String],\n+    logBaseDir: URI,\n+    sparkConf: SparkConf,\n+    hadoopConf: Configuration)\n+  extends EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {\n+\n+  import RollingEventLogFilesWriter._\n+\n+  private val eventFileMaxLengthKiB = sparkConf.get(EVENT_LOG_ROLLED_EVENT_LOG_MAX_FILE_SIZE)\n+\n+  private val logDirForAppPath = getAppEventLogDirPath(logBaseDir, appId, appAttemptId)\n+\n+  // Only defined if the file system scheme is not local\n+  private var hadoopDataStream: Option[FSDataOutputStream] = None\n+  private var countingOutputStream: Option[CountingOutputStream] = None\n+  private var writer: Option[PrintWriter] = None\n+\n+  // seq and event log path will be updated soon in rollNewEventLogFile, which `start` will call\n+  private var sequence: Long = 0L\n+  private var currentEventLogFilePath: Path = logDirForAppPath\n+\n+  override def start(): Unit = {\n+    requireLogBaseDirAsDirectory()\n+\n+    if (fileSystem.exists(logDirForAppPath) && shouldOverwrite) {\n+      // try to delete the directory\n+      fileSystem.delete(logDirForAppPath, true)\n+    }\n+\n+    if (fileSystem.exists(logDirForAppPath)) {\n+      // we tried to delete the existing one, but failed\n+      throw new IOException(s\"Target log directory already exists ($logDirForAppPath)\")\n+    } else {\n+      fileSystem.mkdirs(logDirForAppPath, EventLogFileWriter.LOG_FILE_PERMISSIONS)\n+      createAppStatusFile(inProgress = true)\n+      rollNewEventLogFile(None)\n+    }\n+  }\n+\n+  override def writeEvent(eventJson: String, flushLogger: Boolean = false): Unit = {\n+    writer.foreach { w =>\n+      val currentLen = countingOutputStream.get.getBytesWritten\n+      if (currentLen + eventJson.length > eventFileMaxLengthKiB * 1024) {\n+        rollNewEventLogFile(Some(w))\n+      }\n+    }\n+\n+    // if the event log file is rolled over, writer will refer next event log file\n+\n+    // scalastyle:off println\n+    writer.foreach(_.println(eventJson))\n+    // scalastyle:on println\n+    if (flushLogger) {\n+      writer.foreach(_.flush())\n+      hadoopDataStream.foreach(_.hflush())\n+    }\n+  }\n+\n+  private def rollNewEventLogFile(w: Option[PrintWriter]): Unit = {\n+    writer.foreach(_.close())\n+\n+    sequence += 1\n+    currentEventLogFilePath = getEventLogFilePath(logDirForAppPath, appId, appAttemptId, sequence,\n+      compressionCodecName)\n+\n+    val streams = initLogFile(currentEventLogFilePath)\n+    hadoopDataStream = streams._1\n+    countingOutputStream = streams._2\n+    if (countingOutputStream.isDefined) {\n+      writer = Some(new PrintWriter(streams._2.get))\n+    } else {\n+      writer = None\n+    }\n+  }\n+\n+  override def stop(): Unit = {\n+    writer.foreach(_.close())\n+    val appStatusPathIncomplete = getAppStatusFilePath(logDirForAppPath, appId, appAttemptId,\n+      inProgress = true)\n+    val appStatusPathComplete = getAppStatusFilePath(logDirForAppPath, appId, appAttemptId,\n+      inProgress = false)\n+    renameFile(appStatusPathIncomplete, appStatusPathComplete, overwrite = true)\n+  }\n+\n+  override def logPath: String = logDirForAppPath.toString\n+\n+  private def createAppStatusFile(inProgress: Boolean): Unit = {\n+    val appStatusPath = getAppStatusFilePath(logDirForAppPath, appId, appAttemptId, inProgress)\n+    val streams = initLogFile(appStatusPath)"
  }],
  "prId": 25670
}]