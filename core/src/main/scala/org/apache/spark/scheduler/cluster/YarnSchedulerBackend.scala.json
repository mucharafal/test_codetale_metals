[{
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "could you just reuse the other endpoint? It already talks to the AM.\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-08-13T02:31:43Z",
    "diffHunk": "@@ -91,6 +92,51 @@ private[spark] abstract class YarnSchedulerBackend(\n   }\n \n   /**\n+   * Override the DriverEndpoint to add extra logic for the case when an executor is disconnected.\n+   * We should check the cluster manager and find if the loss of the executor was caused by YARN\n+   * force killing it due to preemption.\n+   */\n+  private class YarnDriverEndpoint(rpcEnv: RpcEnv, sparkProperties: ArrayBuffer[(String, String)])"
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "Which other endpoint? The YarnSchedulerEndpoint? The main problem is that onDisconnected events are received at the DriverEndpoint when the executors exit from preemption. The superclass just removes the executors and marks them as SlaveLost but I want to override this.\n\nI'll document this accordingly.\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-08-13T18:45:59Z",
    "diffHunk": "@@ -91,6 +92,51 @@ private[spark] abstract class YarnSchedulerBackend(\n   }\n \n   /**\n+   * Override the DriverEndpoint to add extra logic for the case when an executor is disconnected.\n+   * We should check the cluster manager and find if the loss of the executor was caused by YARN\n+   * force killing it due to preemption.\n+   */\n+  private class YarnDriverEndpoint(rpcEnv: RpcEnv, sparkProperties: ArrayBuffer[(String, String)])"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "Wait, I'm confused. `YarnSchedulerEndpoint` and `DriverEndpoint` are different things; `YarnSchedulerEndpoint` is a communication channel between the driver in YARN mode and the YARN AM, there are no executors involved. Why wouldn't that work here?\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-08-18T21:15:07Z",
    "diffHunk": "@@ -91,6 +92,51 @@ private[spark] abstract class YarnSchedulerBackend(\n   }\n \n   /**\n+   * Override the DriverEndpoint to add extra logic for the case when an executor is disconnected.\n+   * We should check the cluster manager and find if the loss of the executor was caused by YARN\n+   * force killing it due to preemption.\n+   */\n+  private class YarnDriverEndpoint(rpcEnv: RpcEnv, sparkProperties: ArrayBuffer[(String, String)])"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "Ah, nevermind. You're extending DriverEndpoint and overriding `onDisconnected`. I should read the rest of the code before commenting on stuff. :-/\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-08-18T21:16:44Z",
    "diffHunk": "@@ -91,6 +92,51 @@ private[spark] abstract class YarnSchedulerBackend(\n   }\n \n   /**\n+   * Override the DriverEndpoint to add extra logic for the case when an executor is disconnected.\n+   * We should check the cluster manager and find if the loss of the executor was caused by YARN\n+   * force killing it due to preemption.\n+   */\n+  private class YarnDriverEndpoint(rpcEnv: RpcEnv, sparkProperties: ArrayBuffer[(String, String)])"
  }],
  "prId": 8007
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "need a huge java doc to explain what your'e doing here and why\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-08-13T02:32:04Z",
    "diffHunk": "@@ -91,6 +92,51 @@ private[spark] abstract class YarnSchedulerBackend(\n   }\n \n   /**\n+   * Override the DriverEndpoint to add extra logic for the case when an executor is disconnected.\n+   * We should check the cluster manager and find if the loss of the executor was caused by YARN\n+   * force killing it due to preemption.\n+   */\n+  private class YarnDriverEndpoint(rpcEnv: RpcEnv, sparkProperties: ArrayBuffer[(String, String)])\n+      extends DriverEndpoint(rpcEnv, sparkProperties) {\n+\n+    private val pendingDisconnectedExecutors = new HashSet[String]\n+    private val handleDisconnectedExecutorThreadPool =\n+      ThreadUtils.newDaemonCachedThreadPool(\"yarn-driver-handle-lost-executor-thread-pool\")\n+\n+    override def onDisconnected(rpcAddress: RpcAddress): Unit = {"
  }],
  "prId": 8007
}, {
  "comments": [{
    "author": {
      "login": "markgrover"
    },
    "body": "Am I correct in understanding that CoarseGrainedSchedulerBackend::onDisconnected() would never be called in YARN mode, instead this onDisconnected() will be called? If so, does it make sense to assert that in CoarseGrainedSchedulerBackend::onDisconnected()?\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-08-13T21:49:11Z",
    "diffHunk": "@@ -91,6 +92,66 @@ private[spark] abstract class YarnSchedulerBackend(\n   }\n \n   /**\n+   * Override the DriverEndpoint to add extra logic for the case when an executor is disconnected.\n+   * We should check the cluster manager and find if the loss of the executor was caused by YARN\n+   * force killing it due to preemption.\n+   */\n+  private class YarnDriverEndpoint(rpcEnv: RpcEnv, sparkProperties: ArrayBuffer[(String, String)])\n+      extends DriverEndpoint(rpcEnv, sparkProperties) {\n+\n+    private val pendingDisconnectedExecutors = new HashSet[String]\n+    private val handleDisconnectedExecutorThreadPool =\n+      ThreadUtils.newDaemonCachedThreadPool(\"yarn-driver-handle-lost-executor-thread-pool\")\n+\n+    /**\n+     * When onDisconnected is received at the driver endpoint, the superclass DriverEndpoint\n+     * handles it by assuming the Executor was lost for a bad reason and removes the executor\n+     * immediately.\n+     *\n+     * In YARN's case however it is crucial to talk to the application master and ask why the\n+     * executor had exited. In particular, the executor may have exited due to the executor\n+     * having been preempted. If the executor \"exited normally\" according to the application\n+     * master then we pass that information down to the TaskSetManager to inform the\n+     * TaskSetManager that tasks on that lost executor should not count towards a job failure.\n+     */\n+    override def onDisconnected(rpcAddress: RpcAddress): Unit = {"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "One side-effect of this is that there will be a delay between the executor disconnecting from the driver and it being marked as unavailable for running tasks. So isn't it possible that while we're waiting for the AM to reply, the scheduler will try to run tasks on that executor?\n\nIs there some sort of \"soft unregistration\" that could be done here so that the executor is not used for new tasks, but we still haven't failed the existing tasks pending figuring out the exit reason?\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-08-18T21:25:05Z",
    "diffHunk": "@@ -91,6 +92,66 @@ private[spark] abstract class YarnSchedulerBackend(\n   }\n \n   /**\n+   * Override the DriverEndpoint to add extra logic for the case when an executor is disconnected.\n+   * We should check the cluster manager and find if the loss of the executor was caused by YARN\n+   * force killing it due to preemption.\n+   */\n+  private class YarnDriverEndpoint(rpcEnv: RpcEnv, sparkProperties: ArrayBuffer[(String, String)])\n+      extends DriverEndpoint(rpcEnv, sparkProperties) {\n+\n+    private val pendingDisconnectedExecutors = new HashSet[String]\n+    private val handleDisconnectedExecutorThreadPool =\n+      ThreadUtils.newDaemonCachedThreadPool(\"yarn-driver-handle-lost-executor-thread-pool\")\n+\n+    /**\n+     * When onDisconnected is received at the driver endpoint, the superclass DriverEndpoint\n+     * handles it by assuming the Executor was lost for a bad reason and removes the executor\n+     * immediately.\n+     *\n+     * In YARN's case however it is crucial to talk to the application master and ask why the\n+     * executor had exited. In particular, the executor may have exited due to the executor\n+     * having been preempted. If the executor \"exited normally\" according to the application\n+     * master then we pass that information down to the TaskSetManager to inform the\n+     * TaskSetManager that tasks on that lost executor should not count towards a job failure.\n+     */\n+    override def onDisconnected(rpcAddress: RpcAddress): Unit = {"
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "I guess there will be wasted work in the sense that the tasks will get allocated to the bad executor and then the executor will be removed and all of those tasks are relocated to the healthy ones. That's fine from a correctness standpoint but might create a bit of a performance latency... I'm open to the discussion of doing another architecture overhaul to get the soft-unregistration construct done.\n\nThe other thing I'm wondering is if it's even worth offloading this communicate-with-AM logic to be asynchronous at all. How big of a performance penalty would it be to block the event loop with the request to the AM for the get-executor-loss-reason? I presumed that it was unacceptable to do that blocking request on the main event loop though.\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-08-18T21:45:07Z",
    "diffHunk": "@@ -91,6 +92,66 @@ private[spark] abstract class YarnSchedulerBackend(\n   }\n \n   /**\n+   * Override the DriverEndpoint to add extra logic for the case when an executor is disconnected.\n+   * We should check the cluster manager and find if the loss of the executor was caused by YARN\n+   * force killing it due to preemption.\n+   */\n+  private class YarnDriverEndpoint(rpcEnv: RpcEnv, sparkProperties: ArrayBuffer[(String, String)])\n+      extends DriverEndpoint(rpcEnv, sparkProperties) {\n+\n+    private val pendingDisconnectedExecutors = new HashSet[String]\n+    private val handleDisconnectedExecutorThreadPool =\n+      ThreadUtils.newDaemonCachedThreadPool(\"yarn-driver-handle-lost-executor-thread-pool\")\n+\n+    /**\n+     * When onDisconnected is received at the driver endpoint, the superclass DriverEndpoint\n+     * handles it by assuming the Executor was lost for a bad reason and removes the executor\n+     * immediately.\n+     *\n+     * In YARN's case however it is crucial to talk to the application master and ask why the\n+     * executor had exited. In particular, the executor may have exited due to the executor\n+     * having been preempted. If the executor \"exited normally\" according to the application\n+     * master then we pass that information down to the TaskSetManager to inform the\n+     * TaskSetManager that tasks on that lost executor should not count towards a job failure.\n+     */\n+    override def onDisconnected(rpcAddress: RpcAddress): Unit = {"
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "I guess the scary part is that if the executor died from an actual failure and tasks run on that bad executor, then we get more tasks that are marked as failed than we would have otherwise.\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-08-18T21:49:17Z",
    "diffHunk": "@@ -91,6 +92,66 @@ private[spark] abstract class YarnSchedulerBackend(\n   }\n \n   /**\n+   * Override the DriverEndpoint to add extra logic for the case when an executor is disconnected.\n+   * We should check the cluster manager and find if the loss of the executor was caused by YARN\n+   * force killing it due to preemption.\n+   */\n+  private class YarnDriverEndpoint(rpcEnv: RpcEnv, sparkProperties: ArrayBuffer[(String, String)])\n+      extends DriverEndpoint(rpcEnv, sparkProperties) {\n+\n+    private val pendingDisconnectedExecutors = new HashSet[String]\n+    private val handleDisconnectedExecutorThreadPool =\n+      ThreadUtils.newDaemonCachedThreadPool(\"yarn-driver-handle-lost-executor-thread-pool\")\n+\n+    /**\n+     * When onDisconnected is received at the driver endpoint, the superclass DriverEndpoint\n+     * handles it by assuming the Executor was lost for a bad reason and removes the executor\n+     * immediately.\n+     *\n+     * In YARN's case however it is crucial to talk to the application master and ask why the\n+     * executor had exited. In particular, the executor may have exited due to the executor\n+     * having been preempted. If the executor \"exited normally\" according to the application\n+     * master then we pass that information down to the TaskSetManager to inform the\n+     * TaskSetManager that tasks on that lost executor should not count towards a job failure.\n+     */\n+    override def onDisconnected(rpcAddress: RpcAddress): Unit = {"
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "To @markgrover's question, yes, by overriding the method then only this implementation will be invoked.\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-08-18T21:51:23Z",
    "diffHunk": "@@ -91,6 +92,66 @@ private[spark] abstract class YarnSchedulerBackend(\n   }\n \n   /**\n+   * Override the DriverEndpoint to add extra logic for the case when an executor is disconnected.\n+   * We should check the cluster manager and find if the loss of the executor was caused by YARN\n+   * force killing it due to preemption.\n+   */\n+  private class YarnDriverEndpoint(rpcEnv: RpcEnv, sparkProperties: ArrayBuffer[(String, String)])\n+      extends DriverEndpoint(rpcEnv, sparkProperties) {\n+\n+    private val pendingDisconnectedExecutors = new HashSet[String]\n+    private val handleDisconnectedExecutorThreadPool =\n+      ThreadUtils.newDaemonCachedThreadPool(\"yarn-driver-handle-lost-executor-thread-pool\")\n+\n+    /**\n+     * When onDisconnected is received at the driver endpoint, the superclass DriverEndpoint\n+     * handles it by assuming the Executor was lost for a bad reason and removes the executor\n+     * immediately.\n+     *\n+     * In YARN's case however it is crucial to talk to the application master and ask why the\n+     * executor had exited. In particular, the executor may have exited due to the executor\n+     * having been preempted. If the executor \"exited normally\" according to the application\n+     * master then we pass that information down to the TaskSetManager to inform the\n+     * TaskSetManager that tasks on that lost executor should not count towards a job failure.\n+     */\n+    override def onDisconnected(rpcAddress: RpcAddress): Unit = {"
  }],
  "prId": 8007
}, {
  "comments": [{
    "author": {
      "login": "markgrover"
    },
    "body": "Do we need to remove executorID from this set? What if a second request for disconnected executor is a delayed and comes after you've done the handling of the disconnected executor in the first pass. Is it ok to do it again or is it safeguarded somehow from not happening again?\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-08-13T21:49:34Z",
    "diffHunk": "@@ -91,6 +92,66 @@ private[spark] abstract class YarnSchedulerBackend(\n   }\n \n   /**\n+   * Override the DriverEndpoint to add extra logic for the case when an executor is disconnected.\n+   * We should check the cluster manager and find if the loss of the executor was caused by YARN\n+   * force killing it due to preemption.\n+   */\n+  private class YarnDriverEndpoint(rpcEnv: RpcEnv, sparkProperties: ArrayBuffer[(String, String)])\n+      extends DriverEndpoint(rpcEnv, sparkProperties) {\n+\n+    private val pendingDisconnectedExecutors = new HashSet[String]\n+    private val handleDisconnectedExecutorThreadPool =\n+      ThreadUtils.newDaemonCachedThreadPool(\"yarn-driver-handle-lost-executor-thread-pool\")\n+\n+    /**\n+     * When onDisconnected is received at the driver endpoint, the superclass DriverEndpoint\n+     * handles it by assuming the Executor was lost for a bad reason and removes the executor\n+     * immediately.\n+     *\n+     * In YARN's case however it is crucial to talk to the application master and ask why the\n+     * executor had exited. In particular, the executor may have exited due to the executor\n+     * having been preempted. If the executor \"exited normally\" according to the application\n+     * master then we pass that information down to the TaskSetManager to inform the\n+     * TaskSetManager that tasks on that lost executor should not count towards a job failure.\n+     */\n+    override def onDisconnected(rpcAddress: RpcAddress): Unit = {\n+      addressToExecutorId.get(rpcAddress).foreach({ executorId =>\n+        pendingDisconnectedExecutors.synchronized {\n+          // onDisconnected could be fired multiple times from the same executor while we're\n+          // asynchronously contacting the AM. So keep track of the executors we're trying to\n+          // find loss reasons for and don't duplicate the work\n+          if (!pendingDisconnectedExecutors.contains(executorId)) {\n+            pendingDisconnectedExecutors.add(executorId)\n+            handleDisconnectedExecutorThreadPool.submit(new Runnable() {\n+              override def run(): Unit = {\n+                val executorLossReason =\n+                // Check for the loss reason and pass the loss reason to driverEndpoint\n+                  yarnSchedulerEndpoint.askWithRetry[Option[ExecutorLossReason]](\n+                      GetExecutorLossReason(executorId))\n+                executorLossReason match {\n+                  case Some(reason) =>\n+                    driverEndpoint.askWithRetry[Boolean](RemoveExecutor(executorId, reason))\n+                  case None =>\n+                    logWarning(s\"Attempted to get executor loss reason\" +\n+                      s\" for $rpcAddress but got no response. Marking as slave lost.\")\n+                    driverEndpoint.askWithRetry[Boolean](RemoveExecutor(executorId, SlaveLost()))\n+                }\n+                pendingDisconnectedExecutors.synchronized {\n+                  pendingDisconnectedExecutors.remove(executorId)"
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "Good question - we can solve this by doing a subtle switch in where I put the synchronized block for pendingDisconnectedExecutors. If I move pendingDisconnectedExecutors.synchronized { to before the addressToExecutorId.get(rpcAddress).foreach call, then we can guarantee not duplicating work. The reason is that removeExecutor removes the address from addressToExecutorId. I'll update the PR and put a comment discussing this.\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-08-13T23:44:08Z",
    "diffHunk": "@@ -91,6 +92,66 @@ private[spark] abstract class YarnSchedulerBackend(\n   }\n \n   /**\n+   * Override the DriverEndpoint to add extra logic for the case when an executor is disconnected.\n+   * We should check the cluster manager and find if the loss of the executor was caused by YARN\n+   * force killing it due to preemption.\n+   */\n+  private class YarnDriverEndpoint(rpcEnv: RpcEnv, sparkProperties: ArrayBuffer[(String, String)])\n+      extends DriverEndpoint(rpcEnv, sparkProperties) {\n+\n+    private val pendingDisconnectedExecutors = new HashSet[String]\n+    private val handleDisconnectedExecutorThreadPool =\n+      ThreadUtils.newDaemonCachedThreadPool(\"yarn-driver-handle-lost-executor-thread-pool\")\n+\n+    /**\n+     * When onDisconnected is received at the driver endpoint, the superclass DriverEndpoint\n+     * handles it by assuming the Executor was lost for a bad reason and removes the executor\n+     * immediately.\n+     *\n+     * In YARN's case however it is crucial to talk to the application master and ask why the\n+     * executor had exited. In particular, the executor may have exited due to the executor\n+     * having been preempted. If the executor \"exited normally\" according to the application\n+     * master then we pass that information down to the TaskSetManager to inform the\n+     * TaskSetManager that tasks on that lost executor should not count towards a job failure.\n+     */\n+    override def onDisconnected(rpcAddress: RpcAddress): Unit = {\n+      addressToExecutorId.get(rpcAddress).foreach({ executorId =>\n+        pendingDisconnectedExecutors.synchronized {\n+          // onDisconnected could be fired multiple times from the same executor while we're\n+          // asynchronously contacting the AM. So keep track of the executors we're trying to\n+          // find loss reasons for and don't duplicate the work\n+          if (!pendingDisconnectedExecutors.contains(executorId)) {\n+            pendingDisconnectedExecutors.add(executorId)\n+            handleDisconnectedExecutorThreadPool.submit(new Runnable() {\n+              override def run(): Unit = {\n+                val executorLossReason =\n+                // Check for the loss reason and pass the loss reason to driverEndpoint\n+                  yarnSchedulerEndpoint.askWithRetry[Option[ExecutorLossReason]](\n+                      GetExecutorLossReason(executorId))\n+                executorLossReason match {\n+                  case Some(reason) =>\n+                    driverEndpoint.askWithRetry[Boolean](RemoveExecutor(executorId, reason))\n+                  case None =>\n+                    logWarning(s\"Attempted to get executor loss reason\" +\n+                      s\" for $rpcAddress but got no response. Marking as slave lost.\")\n+                    driverEndpoint.askWithRetry[Boolean](RemoveExecutor(executorId, SlaveLost()))\n+                }\n+                pendingDisconnectedExecutors.synchronized {\n+                  pendingDisconnectedExecutors.remove(executorId)"
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "Actually I'm second guessing this. Will need to reason a little more.\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-08-13T23:50:39Z",
    "diffHunk": "@@ -91,6 +92,66 @@ private[spark] abstract class YarnSchedulerBackend(\n   }\n \n   /**\n+   * Override the DriverEndpoint to add extra logic for the case when an executor is disconnected.\n+   * We should check the cluster manager and find if the loss of the executor was caused by YARN\n+   * force killing it due to preemption.\n+   */\n+  private class YarnDriverEndpoint(rpcEnv: RpcEnv, sparkProperties: ArrayBuffer[(String, String)])\n+      extends DriverEndpoint(rpcEnv, sparkProperties) {\n+\n+    private val pendingDisconnectedExecutors = new HashSet[String]\n+    private val handleDisconnectedExecutorThreadPool =\n+      ThreadUtils.newDaemonCachedThreadPool(\"yarn-driver-handle-lost-executor-thread-pool\")\n+\n+    /**\n+     * When onDisconnected is received at the driver endpoint, the superclass DriverEndpoint\n+     * handles it by assuming the Executor was lost for a bad reason and removes the executor\n+     * immediately.\n+     *\n+     * In YARN's case however it is crucial to talk to the application master and ask why the\n+     * executor had exited. In particular, the executor may have exited due to the executor\n+     * having been preempted. If the executor \"exited normally\" according to the application\n+     * master then we pass that information down to the TaskSetManager to inform the\n+     * TaskSetManager that tasks on that lost executor should not count towards a job failure.\n+     */\n+    override def onDisconnected(rpcAddress: RpcAddress): Unit = {\n+      addressToExecutorId.get(rpcAddress).foreach({ executorId =>\n+        pendingDisconnectedExecutors.synchronized {\n+          // onDisconnected could be fired multiple times from the same executor while we're\n+          // asynchronously contacting the AM. So keep track of the executors we're trying to\n+          // find loss reasons for and don't duplicate the work\n+          if (!pendingDisconnectedExecutors.contains(executorId)) {\n+            pendingDisconnectedExecutors.add(executorId)\n+            handleDisconnectedExecutorThreadPool.submit(new Runnable() {\n+              override def run(): Unit = {\n+                val executorLossReason =\n+                // Check for the loss reason and pass the loss reason to driverEndpoint\n+                  yarnSchedulerEndpoint.askWithRetry[Option[ExecutorLossReason]](\n+                      GetExecutorLossReason(executorId))\n+                executorLossReason match {\n+                  case Some(reason) =>\n+                    driverEndpoint.askWithRetry[Boolean](RemoveExecutor(executorId, reason))\n+                  case None =>\n+                    logWarning(s\"Attempted to get executor loss reason\" +\n+                      s\" for $rpcAddress but got no response. Marking as slave lost.\")\n+                    driverEndpoint.askWithRetry[Boolean](RemoveExecutor(executorId, SlaveLost()))\n+                }\n+                pendingDisconnectedExecutors.synchronized {\n+                  pendingDisconnectedExecutors.remove(executorId)"
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "After reasoning to myself I believe this logic is correct. Adding to pendingDisconnectedExecutors before running the checking logic maintains the invariant that nothing can enter until RemoveExecutor is finished. And once RemoveExecutor is called then the addressToExecutorId.get(rpcAddress) won't find anything for that address ever again. So at the end it is safe to remove the executor id from pendingDIsconnectedExecutors because of the invariant created by invoking removeExecutor.\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-08-14T00:28:58Z",
    "diffHunk": "@@ -91,6 +92,66 @@ private[spark] abstract class YarnSchedulerBackend(\n   }\n \n   /**\n+   * Override the DriverEndpoint to add extra logic for the case when an executor is disconnected.\n+   * We should check the cluster manager and find if the loss of the executor was caused by YARN\n+   * force killing it due to preemption.\n+   */\n+  private class YarnDriverEndpoint(rpcEnv: RpcEnv, sparkProperties: ArrayBuffer[(String, String)])\n+      extends DriverEndpoint(rpcEnv, sparkProperties) {\n+\n+    private val pendingDisconnectedExecutors = new HashSet[String]\n+    private val handleDisconnectedExecutorThreadPool =\n+      ThreadUtils.newDaemonCachedThreadPool(\"yarn-driver-handle-lost-executor-thread-pool\")\n+\n+    /**\n+     * When onDisconnected is received at the driver endpoint, the superclass DriverEndpoint\n+     * handles it by assuming the Executor was lost for a bad reason and removes the executor\n+     * immediately.\n+     *\n+     * In YARN's case however it is crucial to talk to the application master and ask why the\n+     * executor had exited. In particular, the executor may have exited due to the executor\n+     * having been preempted. If the executor \"exited normally\" according to the application\n+     * master then we pass that information down to the TaskSetManager to inform the\n+     * TaskSetManager that tasks on that lost executor should not count towards a job failure.\n+     */\n+    override def onDisconnected(rpcAddress: RpcAddress): Unit = {\n+      addressToExecutorId.get(rpcAddress).foreach({ executorId =>\n+        pendingDisconnectedExecutors.synchronized {\n+          // onDisconnected could be fired multiple times from the same executor while we're\n+          // asynchronously contacting the AM. So keep track of the executors we're trying to\n+          // find loss reasons for and don't duplicate the work\n+          if (!pendingDisconnectedExecutors.contains(executorId)) {\n+            pendingDisconnectedExecutors.add(executorId)\n+            handleDisconnectedExecutorThreadPool.submit(new Runnable() {\n+              override def run(): Unit = {\n+                val executorLossReason =\n+                // Check for the loss reason and pass the loss reason to driverEndpoint\n+                  yarnSchedulerEndpoint.askWithRetry[Option[ExecutorLossReason]](\n+                      GetExecutorLossReason(executorId))\n+                executorLossReason match {\n+                  case Some(reason) =>\n+                    driverEndpoint.askWithRetry[Boolean](RemoveExecutor(executorId, reason))\n+                  case None =>\n+                    logWarning(s\"Attempted to get executor loss reason\" +\n+                      s\" for $rpcAddress but got no response. Marking as slave lost.\")\n+                    driverEndpoint.askWithRetry[Boolean](RemoveExecutor(executorId, SlaveLost()))\n+                }\n+                pendingDisconnectedExecutors.synchronized {\n+                  pendingDisconnectedExecutors.remove(executorId)"
  }, {
    "author": {
      "login": "markgrover"
    },
    "body": "ok, sounds good.\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-08-14T17:04:25Z",
    "diffHunk": "@@ -91,6 +92,66 @@ private[spark] abstract class YarnSchedulerBackend(\n   }\n \n   /**\n+   * Override the DriverEndpoint to add extra logic for the case when an executor is disconnected.\n+   * We should check the cluster manager and find if the loss of the executor was caused by YARN\n+   * force killing it due to preemption.\n+   */\n+  private class YarnDriverEndpoint(rpcEnv: RpcEnv, sparkProperties: ArrayBuffer[(String, String)])\n+      extends DriverEndpoint(rpcEnv, sparkProperties) {\n+\n+    private val pendingDisconnectedExecutors = new HashSet[String]\n+    private val handleDisconnectedExecutorThreadPool =\n+      ThreadUtils.newDaemonCachedThreadPool(\"yarn-driver-handle-lost-executor-thread-pool\")\n+\n+    /**\n+     * When onDisconnected is received at the driver endpoint, the superclass DriverEndpoint\n+     * handles it by assuming the Executor was lost for a bad reason and removes the executor\n+     * immediately.\n+     *\n+     * In YARN's case however it is crucial to talk to the application master and ask why the\n+     * executor had exited. In particular, the executor may have exited due to the executor\n+     * having been preempted. If the executor \"exited normally\" according to the application\n+     * master then we pass that information down to the TaskSetManager to inform the\n+     * TaskSetManager that tasks on that lost executor should not count towards a job failure.\n+     */\n+    override def onDisconnected(rpcAddress: RpcAddress): Unit = {\n+      addressToExecutorId.get(rpcAddress).foreach({ executorId =>\n+        pendingDisconnectedExecutors.synchronized {\n+          // onDisconnected could be fired multiple times from the same executor while we're\n+          // asynchronously contacting the AM. So keep track of the executors we're trying to\n+          // find loss reasons for and don't duplicate the work\n+          if (!pendingDisconnectedExecutors.contains(executorId)) {\n+            pendingDisconnectedExecutors.add(executorId)\n+            handleDisconnectedExecutorThreadPool.submit(new Runnable() {\n+              override def run(): Unit = {\n+                val executorLossReason =\n+                // Check for the loss reason and pass the loss reason to driverEndpoint\n+                  yarnSchedulerEndpoint.askWithRetry[Option[ExecutorLossReason]](\n+                      GetExecutorLossReason(executorId))\n+                executorLossReason match {\n+                  case Some(reason) =>\n+                    driverEndpoint.askWithRetry[Boolean](RemoveExecutor(executorId, reason))\n+                  case None =>\n+                    logWarning(s\"Attempted to get executor loss reason\" +\n+                      s\" for $rpcAddress but got no response. Marking as slave lost.\")\n+                    driverEndpoint.askWithRetry[Boolean](RemoveExecutor(executorId, SlaveLost()))\n+                }\n+                pendingDisconnectedExecutors.synchronized {\n+                  pendingDisconnectedExecutors.remove(executorId)"
  }],
  "prId": 8007
}, {
  "comments": [{
    "author": {
      "login": "markgrover"
    },
    "body": "Also, I noticed that this synchronized is nested in the outer synchronized on the same map. Is that necessary?\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-08-14T19:04:51Z",
    "diffHunk": "@@ -91,6 +92,66 @@ private[spark] abstract class YarnSchedulerBackend(\n   }\n \n   /**\n+   * Override the DriverEndpoint to add extra logic for the case when an executor is disconnected.\n+   * We should check the cluster manager and find if the loss of the executor was caused by YARN\n+   * force killing it due to preemption.\n+   */\n+  private class YarnDriverEndpoint(rpcEnv: RpcEnv, sparkProperties: ArrayBuffer[(String, String)])\n+      extends DriverEndpoint(rpcEnv, sparkProperties) {\n+\n+    private val pendingDisconnectedExecutors = new HashSet[String]\n+    private val handleDisconnectedExecutorThreadPool =\n+      ThreadUtils.newDaemonCachedThreadPool(\"yarn-driver-handle-lost-executor-thread-pool\")\n+\n+    /**\n+     * When onDisconnected is received at the driver endpoint, the superclass DriverEndpoint\n+     * handles it by assuming the Executor was lost for a bad reason and removes the executor\n+     * immediately.\n+     *\n+     * In YARN's case however it is crucial to talk to the application master and ask why the\n+     * executor had exited. In particular, the executor may have exited due to the executor\n+     * having been preempted. If the executor \"exited normally\" according to the application\n+     * master then we pass that information down to the TaskSetManager to inform the\n+     * TaskSetManager that tasks on that lost executor should not count towards a job failure.\n+     */\n+    override def onDisconnected(rpcAddress: RpcAddress): Unit = {\n+      addressToExecutorId.get(rpcAddress).foreach({ executorId =>\n+        pendingDisconnectedExecutors.synchronized {\n+          // onDisconnected could be fired multiple times from the same executor while we're\n+          // asynchronously contacting the AM. So keep track of the executors we're trying to\n+          // find loss reasons for and don't duplicate the work\n+          if (!pendingDisconnectedExecutors.contains(executorId)) {\n+            pendingDisconnectedExecutors.add(executorId)\n+            handleDisconnectedExecutorThreadPool.submit(new Runnable() {\n+              override def run(): Unit = {\n+                val executorLossReason =\n+                // Check for the loss reason and pass the loss reason to driverEndpoint\n+                  yarnSchedulerEndpoint.askWithRetry[Option[ExecutorLossReason]](\n+                      GetExecutorLossReason(executorId))\n+                executorLossReason match {\n+                  case Some(reason) =>\n+                    driverEndpoint.askWithRetry[Boolean](RemoveExecutor(executorId, reason))\n+                  case None =>\n+                    logWarning(s\"Attempted to get executor loss reason\" +\n+                      s\" for $rpcAddress but got no response. Marking as slave lost.\")\n+                    driverEndpoint.askWithRetry[Boolean](RemoveExecutor(executorId, SlaveLost()))\n+                }\n+                pendingDisconnectedExecutors.synchronized {"
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "The inner synchronized is inside a block that would run asynchronously.\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-08-17T18:18:54Z",
    "diffHunk": "@@ -91,6 +92,66 @@ private[spark] abstract class YarnSchedulerBackend(\n   }\n \n   /**\n+   * Override the DriverEndpoint to add extra logic for the case when an executor is disconnected.\n+   * We should check the cluster manager and find if the loss of the executor was caused by YARN\n+   * force killing it due to preemption.\n+   */\n+  private class YarnDriverEndpoint(rpcEnv: RpcEnv, sparkProperties: ArrayBuffer[(String, String)])\n+      extends DriverEndpoint(rpcEnv, sparkProperties) {\n+\n+    private val pendingDisconnectedExecutors = new HashSet[String]\n+    private val handleDisconnectedExecutorThreadPool =\n+      ThreadUtils.newDaemonCachedThreadPool(\"yarn-driver-handle-lost-executor-thread-pool\")\n+\n+    /**\n+     * When onDisconnected is received at the driver endpoint, the superclass DriverEndpoint\n+     * handles it by assuming the Executor was lost for a bad reason and removes the executor\n+     * immediately.\n+     *\n+     * In YARN's case however it is crucial to talk to the application master and ask why the\n+     * executor had exited. In particular, the executor may have exited due to the executor\n+     * having been preempted. If the executor \"exited normally\" according to the application\n+     * master then we pass that information down to the TaskSetManager to inform the\n+     * TaskSetManager that tasks on that lost executor should not count towards a job failure.\n+     */\n+    override def onDisconnected(rpcAddress: RpcAddress): Unit = {\n+      addressToExecutorId.get(rpcAddress).foreach({ executorId =>\n+        pendingDisconnectedExecutors.synchronized {\n+          // onDisconnected could be fired multiple times from the same executor while we're\n+          // asynchronously contacting the AM. So keep track of the executors we're trying to\n+          // find loss reasons for and don't duplicate the work\n+          if (!pendingDisconnectedExecutors.contains(executorId)) {\n+            pendingDisconnectedExecutors.add(executorId)\n+            handleDisconnectedExecutorThreadPool.submit(new Runnable() {\n+              override def run(): Unit = {\n+                val executorLossReason =\n+                // Check for the loss reason and pass the loss reason to driverEndpoint\n+                  yarnSchedulerEndpoint.askWithRetry[Option[ExecutorLossReason]](\n+                      GetExecutorLossReason(executorId))\n+                executorLossReason match {\n+                  case Some(reason) =>\n+                    driverEndpoint.askWithRetry[Boolean](RemoveExecutor(executorId, reason))\n+                  case None =>\n+                    logWarning(s\"Attempted to get executor loss reason\" +\n+                      s\" for $rpcAddress but got no response. Marking as slave lost.\")\n+                    driverEndpoint.askWithRetry[Boolean](RemoveExecutor(executorId, SlaveLost()))\n+                }\n+                pendingDisconnectedExecutors.synchronized {"
  }],
  "prId": 8007
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "style: `.foreach { executorId =>`\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-08-18T21:17:26Z",
    "diffHunk": "@@ -91,6 +92,68 @@ private[spark] abstract class YarnSchedulerBackend(\n   }\n \n   /**\n+   * Override the DriverEndpoint to add extra logic for the case when an executor is disconnected.\n+   * We should check the cluster manager and find if the loss of the executor was caused by YARN\n+   * force killing it due to preemption.\n+   */\n+  private class YarnDriverEndpoint(rpcEnv: RpcEnv, sparkProperties: ArrayBuffer[(String, String)])\n+      extends DriverEndpoint(rpcEnv, sparkProperties) {\n+\n+    private val pendingDisconnectedExecutors = new HashSet[String]\n+    private val handleDisconnectedExecutorThreadPool =\n+      ThreadUtils.newDaemonCachedThreadPool(\"yarn-driver-handle-lost-executor-thread-pool\")\n+\n+    /**\n+     * When onDisconnected is received at the driver endpoint, the superclass DriverEndpoint\n+     * handles it by assuming the Executor was lost for a bad reason and removes the executor\n+     * immediately.\n+     *\n+     * In YARN's case however it is crucial to talk to the application master and ask why the\n+     * executor had exited. In particular, the executor may have exited due to the executor\n+     * having been preempted. If the executor \"exited normally\" according to the application\n+     * master then we pass that information down to the TaskSetManager to inform the\n+     * TaskSetManager that tasks on that lost executor should not count towards a job failure.\n+     */\n+    override def onDisconnected(rpcAddress: RpcAddress): Unit = {\n+      addressToExecutorId.get(rpcAddress).foreach({ executorId =>"
  }],
  "prId": 8007
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: indent this more (or move the comment before the `val` declaration).\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-08-18T21:18:19Z",
    "diffHunk": "@@ -91,6 +92,68 @@ private[spark] abstract class YarnSchedulerBackend(\n   }\n \n   /**\n+   * Override the DriverEndpoint to add extra logic for the case when an executor is disconnected.\n+   * We should check the cluster manager and find if the loss of the executor was caused by YARN\n+   * force killing it due to preemption.\n+   */\n+  private class YarnDriverEndpoint(rpcEnv: RpcEnv, sparkProperties: ArrayBuffer[(String, String)])\n+      extends DriverEndpoint(rpcEnv, sparkProperties) {\n+\n+    private val pendingDisconnectedExecutors = new HashSet[String]\n+    private val handleDisconnectedExecutorThreadPool =\n+      ThreadUtils.newDaemonCachedThreadPool(\"yarn-driver-handle-lost-executor-thread-pool\")\n+\n+    /**\n+     * When onDisconnected is received at the driver endpoint, the superclass DriverEndpoint\n+     * handles it by assuming the Executor was lost for a bad reason and removes the executor\n+     * immediately.\n+     *\n+     * In YARN's case however it is crucial to talk to the application master and ask why the\n+     * executor had exited. In particular, the executor may have exited due to the executor\n+     * having been preempted. If the executor \"exited normally\" according to the application\n+     * master then we pass that information down to the TaskSetManager to inform the\n+     * TaskSetManager that tasks on that lost executor should not count towards a job failure.\n+     */\n+    override def onDisconnected(rpcAddress: RpcAddress): Unit = {\n+      addressToExecutorId.get(rpcAddress).foreach({ executorId =>\n+        // onDisconnected could be fired multiple times from the same executor while we're\n+        // asynchronously contacting the AM. So keep track of the executors we're trying to\n+        // find loss reasons for and don't duplicate the work\n+        if (!pendingDisconnectedExecutors.contains(executorId)) {\n+          pendingDisconnectedExecutors.add(executorId)\n+          handleDisconnectedExecutorThreadPool.submit(new Runnable() {\n+            override def run(): Unit = {\n+              val executorLossReason =\n+              // Check for the loss reason and pass the loss reason to driverEndpoint"
  }],
  "prId": 8007
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Can you call `super.removeExecutor()` directly here, instead of doing the round-trip through the RPC layer? (Might need to check whether that method is thread-safe.)\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-08-18T21:20:35Z",
    "diffHunk": "@@ -91,6 +92,68 @@ private[spark] abstract class YarnSchedulerBackend(\n   }\n \n   /**\n+   * Override the DriverEndpoint to add extra logic for the case when an executor is disconnected.\n+   * We should check the cluster manager and find if the loss of the executor was caused by YARN\n+   * force killing it due to preemption.\n+   */\n+  private class YarnDriverEndpoint(rpcEnv: RpcEnv, sparkProperties: ArrayBuffer[(String, String)])\n+      extends DriverEndpoint(rpcEnv, sparkProperties) {\n+\n+    private val pendingDisconnectedExecutors = new HashSet[String]\n+    private val handleDisconnectedExecutorThreadPool =\n+      ThreadUtils.newDaemonCachedThreadPool(\"yarn-driver-handle-lost-executor-thread-pool\")\n+\n+    /**\n+     * When onDisconnected is received at the driver endpoint, the superclass DriverEndpoint\n+     * handles it by assuming the Executor was lost for a bad reason and removes the executor\n+     * immediately.\n+     *\n+     * In YARN's case however it is crucial to talk to the application master and ask why the\n+     * executor had exited. In particular, the executor may have exited due to the executor\n+     * having been preempted. If the executor \"exited normally\" according to the application\n+     * master then we pass that information down to the TaskSetManager to inform the\n+     * TaskSetManager that tasks on that lost executor should not count towards a job failure.\n+     */\n+    override def onDisconnected(rpcAddress: RpcAddress): Unit = {\n+      addressToExecutorId.get(rpcAddress).foreach({ executorId =>\n+        // onDisconnected could be fired multiple times from the same executor while we're\n+        // asynchronously contacting the AM. So keep track of the executors we're trying to\n+        // find loss reasons for and don't duplicate the work\n+        if (!pendingDisconnectedExecutors.contains(executorId)) {\n+          pendingDisconnectedExecutors.add(executorId)\n+          handleDisconnectedExecutorThreadPool.submit(new Runnable() {\n+            override def run(): Unit = {\n+              val executorLossReason =\n+              // Check for the loss reason and pass the loss reason to driverEndpoint\n+                yarnSchedulerEndpoint.askWithRetry[Option[ExecutorLossReason]](\n+                    GetExecutorLossReason(executorId))\n+              executorLossReason match {\n+                case Some(reason) =>\n+                  driverEndpoint.askWithRetry[Boolean](RemoveExecutor(executorId, reason))\n+                case None =>\n+                  logWarning(s\"Attempted to get executor loss reason\" +\n+                    s\" for $rpcAddress but got no response. Marking as slave lost.\")\n+                  driverEndpoint.askWithRetry[Boolean](RemoveExecutor(executorId, SlaveLost()))"
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "Definitely don't think that's thread safe. It touches things like addressToExecutorId, which as we can see in the onDisconnected method itself is accessed in the event loop.\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-08-18T21:22:39Z",
    "diffHunk": "@@ -91,6 +92,68 @@ private[spark] abstract class YarnSchedulerBackend(\n   }\n \n   /**\n+   * Override the DriverEndpoint to add extra logic for the case when an executor is disconnected.\n+   * We should check the cluster manager and find if the loss of the executor was caused by YARN\n+   * force killing it due to preemption.\n+   */\n+  private class YarnDriverEndpoint(rpcEnv: RpcEnv, sparkProperties: ArrayBuffer[(String, String)])\n+      extends DriverEndpoint(rpcEnv, sparkProperties) {\n+\n+    private val pendingDisconnectedExecutors = new HashSet[String]\n+    private val handleDisconnectedExecutorThreadPool =\n+      ThreadUtils.newDaemonCachedThreadPool(\"yarn-driver-handle-lost-executor-thread-pool\")\n+\n+    /**\n+     * When onDisconnected is received at the driver endpoint, the superclass DriverEndpoint\n+     * handles it by assuming the Executor was lost for a bad reason and removes the executor\n+     * immediately.\n+     *\n+     * In YARN's case however it is crucial to talk to the application master and ask why the\n+     * executor had exited. In particular, the executor may have exited due to the executor\n+     * having been preempted. If the executor \"exited normally\" according to the application\n+     * master then we pass that information down to the TaskSetManager to inform the\n+     * TaskSetManager that tasks on that lost executor should not count towards a job failure.\n+     */\n+    override def onDisconnected(rpcAddress: RpcAddress): Unit = {\n+      addressToExecutorId.get(rpcAddress).foreach({ executorId =>\n+        // onDisconnected could be fired multiple times from the same executor while we're\n+        // asynchronously contacting the AM. So keep track of the executors we're trying to\n+        // find loss reasons for and don't duplicate the work\n+        if (!pendingDisconnectedExecutors.contains(executorId)) {\n+          pendingDisconnectedExecutors.add(executorId)\n+          handleDisconnectedExecutorThreadPool.submit(new Runnable() {\n+            override def run(): Unit = {\n+              val executorLossReason =\n+              // Check for the loss reason and pass the loss reason to driverEndpoint\n+                yarnSchedulerEndpoint.askWithRetry[Option[ExecutorLossReason]](\n+                    GetExecutorLossReason(executorId))\n+              executorLossReason match {\n+                case Some(reason) =>\n+                  driverEndpoint.askWithRetry[Boolean](RemoveExecutor(executorId, reason))\n+                case None =>\n+                  logWarning(s\"Attempted to get executor loss reason\" +\n+                    s\" for $rpcAddress but got no response. Marking as slave lost.\")\n+                  driverEndpoint.askWithRetry[Boolean](RemoveExecutor(executorId, SlaveLost()))"
  }],
  "prId": 8007
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "This is not your fault, but this is starting to get pretty noisy. We need a better way to do this check.\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-08-18T21:27:28Z",
    "diffHunk": "@@ -144,6 +207,21 @@ private[spark] abstract class YarnSchedulerBackend(\n             context.reply(false)\n         }\n \n+      case c: GetExecutorLossReason =>\n+        amEndpoint match {\n+          case Some(am) =>\n+            Future {\n+              context.reply(am.askWithRetry[Option[ExecutorLossReason]](c))\n+            } onFailure {\n+              case NonFatal(e) =>\n+                logError(s\"Finding the executor loss reason was unsuccessful\", e)\n+                context.sendFailure(e)\n+            }\n+          case None =>"
  }],
  "prId": 8007
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "long line, will fail style checks.\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-08-24T20:25:35Z",
    "diffHunk": "@@ -91,6 +92,52 @@ private[spark] abstract class YarnSchedulerBackend(\n   }\n \n   /**\n+   * Override the DriverEndpoint to add extra logic for the case when an executor is disconnected.\n+   * We should check the cluster manager and find if the loss of the executor was caused by YARN\n+   * force killing it due to preemption.\n+   */\n+  private class YarnDriverEndpoint(rpcEnv: RpcEnv, sparkProperties: ArrayBuffer[(String, String)])\n+      extends DriverEndpoint(rpcEnv, sparkProperties) {\n+\n+    private val handleDisconnectedExecutorThreadPool =\n+      ThreadUtils.newDaemonCachedThreadPool(\"yarn-driver-handle-lost-executor-thread-pool\")\n+    implicit val askSchedulerExecutor = ExecutionContext.fromExecutor(handleDisconnectedExecutorThreadPool)\n+\n+    /**\n+     * When onDisconnected is received at the driver endpoint, the superclass DriverEndpoint\n+     * handles it by assuming the Executor was lost for a bad reason and removes the executor\n+     * immediately.\n+     *\n+     * In YARN's case however it is crucial to talk to the application master and ask why the\n+     * executor had exited. In particular, the executor may have exited due to the executor\n+     * having been preempted. If the executor \"exited normally\" according to the application\n+     * master then we pass that information down to the TaskSetManager to inform the\n+     * TaskSetManager that tasks on that lost executor should not count towards a job failure.\n+     */\n+    override def onDisconnected(rpcAddress: RpcAddress): Unit = {\n+      addressToExecutorId.get(rpcAddress).foreach({ executorId =>\n+        val future = yarnSchedulerEndpoint.ask[ExecutorLossReason](GetExecutorLossReason(executorId), askTimeout)"
  }],
  "prId": 8007
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "If you pull `amEndpoint` out of `YarnSchedulerEndpoint` you can avoid this extra hop through the RPC layer, since we know both `YarnDriverEndpoint` and `YarnSchedulerEndpoint` are running in the same VM.\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-08-24T20:31:03Z",
    "diffHunk": "@@ -91,6 +92,52 @@ private[spark] abstract class YarnSchedulerBackend(\n   }\n \n   /**\n+   * Override the DriverEndpoint to add extra logic for the case when an executor is disconnected.\n+   * We should check the cluster manager and find if the loss of the executor was caused by YARN\n+   * force killing it due to preemption.\n+   */\n+  private class YarnDriverEndpoint(rpcEnv: RpcEnv, sparkProperties: ArrayBuffer[(String, String)])\n+      extends DriverEndpoint(rpcEnv, sparkProperties) {\n+\n+    private val handleDisconnectedExecutorThreadPool =\n+      ThreadUtils.newDaemonCachedThreadPool(\"yarn-driver-handle-lost-executor-thread-pool\")\n+    implicit val askSchedulerExecutor = ExecutionContext.fromExecutor(handleDisconnectedExecutorThreadPool)\n+\n+    /**\n+     * When onDisconnected is received at the driver endpoint, the superclass DriverEndpoint\n+     * handles it by assuming the Executor was lost for a bad reason and removes the executor\n+     * immediately.\n+     *\n+     * In YARN's case however it is crucial to talk to the application master and ask why the\n+     * executor had exited. In particular, the executor may have exited due to the executor\n+     * having been preempted. If the executor \"exited normally\" according to the application\n+     * master then we pass that information down to the TaskSetManager to inform the\n+     * TaskSetManager that tasks on that lost executor should not count towards a job failure.\n+     */\n+    override def onDisconnected(rpcAddress: RpcAddress): Unit = {\n+      addressToExecutorId.get(rpcAddress).foreach({ executorId =>\n+        val future = yarnSchedulerEndpoint.ask[ExecutorLossReason](GetExecutorLossReason(executorId), askTimeout)\n+        future onSuccess {\n+          case reason: ExecutorLossReason =>\n+            driverEndpoint.askWithRetry[Boolean](RemoveExecutor(executorId, reason))"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "(Or you could call some method in `YarnSchedulerEndpoint` directly, but the gist is, you don't need to send another RPC for this.)\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-08-24T20:31:36Z",
    "diffHunk": "@@ -91,6 +92,52 @@ private[spark] abstract class YarnSchedulerBackend(\n   }\n \n   /**\n+   * Override the DriverEndpoint to add extra logic for the case when an executor is disconnected.\n+   * We should check the cluster manager and find if the loss of the executor was caused by YARN\n+   * force killing it due to preemption.\n+   */\n+  private class YarnDriverEndpoint(rpcEnv: RpcEnv, sparkProperties: ArrayBuffer[(String, String)])\n+      extends DriverEndpoint(rpcEnv, sparkProperties) {\n+\n+    private val handleDisconnectedExecutorThreadPool =\n+      ThreadUtils.newDaemonCachedThreadPool(\"yarn-driver-handle-lost-executor-thread-pool\")\n+    implicit val askSchedulerExecutor = ExecutionContext.fromExecutor(handleDisconnectedExecutorThreadPool)\n+\n+    /**\n+     * When onDisconnected is received at the driver endpoint, the superclass DriverEndpoint\n+     * handles it by assuming the Executor was lost for a bad reason and removes the executor\n+     * immediately.\n+     *\n+     * In YARN's case however it is crucial to talk to the application master and ask why the\n+     * executor had exited. In particular, the executor may have exited due to the executor\n+     * having been preempted. If the executor \"exited normally\" according to the application\n+     * master then we pass that information down to the TaskSetManager to inform the\n+     * TaskSetManager that tasks on that lost executor should not count towards a job failure.\n+     */\n+    override def onDisconnected(rpcAddress: RpcAddress): Unit = {\n+      addressToExecutorId.get(rpcAddress).foreach({ executorId =>\n+        val future = yarnSchedulerEndpoint.ask[ExecutorLossReason](GetExecutorLossReason(executorId), askTimeout)\n+        future onSuccess {\n+          case reason: ExecutorLossReason =>\n+            driverEndpoint.askWithRetry[Boolean](RemoveExecutor(executorId, reason))"
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "Slightly concerning is the fact that amEndpoint is set in YarnSchedulerEndpoint's event loop (YarnSchedulerEndpoint.receive), but would be used in YarnDriverEndpoint.onDisconnected. I would need to handle the race conditions possibly caused by setting the endpoint and trying to use it in two event processing loops, right?\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-08-24T21:23:48Z",
    "diffHunk": "@@ -91,6 +92,52 @@ private[spark] abstract class YarnSchedulerBackend(\n   }\n \n   /**\n+   * Override the DriverEndpoint to add extra logic for the case when an executor is disconnected.\n+   * We should check the cluster manager and find if the loss of the executor was caused by YARN\n+   * force killing it due to preemption.\n+   */\n+  private class YarnDriverEndpoint(rpcEnv: RpcEnv, sparkProperties: ArrayBuffer[(String, String)])\n+      extends DriverEndpoint(rpcEnv, sparkProperties) {\n+\n+    private val handleDisconnectedExecutorThreadPool =\n+      ThreadUtils.newDaemonCachedThreadPool(\"yarn-driver-handle-lost-executor-thread-pool\")\n+    implicit val askSchedulerExecutor = ExecutionContext.fromExecutor(handleDisconnectedExecutorThreadPool)\n+\n+    /**\n+     * When onDisconnected is received at the driver endpoint, the superclass DriverEndpoint\n+     * handles it by assuming the Executor was lost for a bad reason and removes the executor\n+     * immediately.\n+     *\n+     * In YARN's case however it is crucial to talk to the application master and ask why the\n+     * executor had exited. In particular, the executor may have exited due to the executor\n+     * having been preempted. If the executor \"exited normally\" according to the application\n+     * master then we pass that information down to the TaskSetManager to inform the\n+     * TaskSetManager that tasks on that lost executor should not count towards a job failure.\n+     */\n+    override def onDisconnected(rpcAddress: RpcAddress): Unit = {\n+      addressToExecutorId.get(rpcAddress).foreach({ executorId =>\n+        val future = yarnSchedulerEndpoint.ask[ExecutorLossReason](GetExecutorLossReason(executorId), askTimeout)\n+        future onSuccess {\n+          case reason: ExecutorLossReason =>\n+            driverEndpoint.askWithRetry[Boolean](RemoveExecutor(executorId, reason))"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "(BTW, my comment should have been on L119, where you send `GetExecutorLossReason`.)\n\nFollowing the code in `ApplicationMaster`, it doesn't look like it would be possible for that situation to happen. The AM endpoint is started (and registers with the driver endpoint) before the AM starts allocating executors for the application.\n\nMaybe adding an assert (that `amEndpoint` is set) would suffice. Also, just want to point out that, if the race really exists, going through the RPC layer wouldn't fix it, just make it less likely. Ignoring the initialization order in `ApplicationMaster` for a second, the message could still reach the other endpoint before the AM has registered.\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-08-24T21:42:45Z",
    "diffHunk": "@@ -91,6 +92,52 @@ private[spark] abstract class YarnSchedulerBackend(\n   }\n \n   /**\n+   * Override the DriverEndpoint to add extra logic for the case when an executor is disconnected.\n+   * We should check the cluster manager and find if the loss of the executor was caused by YARN\n+   * force killing it due to preemption.\n+   */\n+  private class YarnDriverEndpoint(rpcEnv: RpcEnv, sparkProperties: ArrayBuffer[(String, String)])\n+      extends DriverEndpoint(rpcEnv, sparkProperties) {\n+\n+    private val handleDisconnectedExecutorThreadPool =\n+      ThreadUtils.newDaemonCachedThreadPool(\"yarn-driver-handle-lost-executor-thread-pool\")\n+    implicit val askSchedulerExecutor = ExecutionContext.fromExecutor(handleDisconnectedExecutorThreadPool)\n+\n+    /**\n+     * When onDisconnected is received at the driver endpoint, the superclass DriverEndpoint\n+     * handles it by assuming the Executor was lost for a bad reason and removes the executor\n+     * immediately.\n+     *\n+     * In YARN's case however it is crucial to talk to the application master and ask why the\n+     * executor had exited. In particular, the executor may have exited due to the executor\n+     * having been preempted. If the executor \"exited normally\" according to the application\n+     * master then we pass that information down to the TaskSetManager to inform the\n+     * TaskSetManager that tasks on that lost executor should not count towards a job failure.\n+     */\n+    override def onDisconnected(rpcAddress: RpcAddress): Unit = {\n+      addressToExecutorId.get(rpcAddress).foreach({ executorId =>\n+        val future = yarnSchedulerEndpoint.ask[ExecutorLossReason](GetExecutorLossReason(executorId), askTimeout)\n+        future onSuccess {\n+          case reason: ExecutorLossReason =>\n+            driverEndpoint.askWithRetry[Boolean](RemoveExecutor(executorId, reason))"
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "Interestingly enough though, I've seen the initialization code get re-entered when an ApplicationMaster gets preempted and it is re-initialized on some other NodeManager when resources are released by another job. But perhaps this is a special case.\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-08-24T22:03:23Z",
    "diffHunk": "@@ -91,6 +92,52 @@ private[spark] abstract class YarnSchedulerBackend(\n   }\n \n   /**\n+   * Override the DriverEndpoint to add extra logic for the case when an executor is disconnected.\n+   * We should check the cluster manager and find if the loss of the executor was caused by YARN\n+   * force killing it due to preemption.\n+   */\n+  private class YarnDriverEndpoint(rpcEnv: RpcEnv, sparkProperties: ArrayBuffer[(String, String)])\n+      extends DriverEndpoint(rpcEnv, sparkProperties) {\n+\n+    private val handleDisconnectedExecutorThreadPool =\n+      ThreadUtils.newDaemonCachedThreadPool(\"yarn-driver-handle-lost-executor-thread-pool\")\n+    implicit val askSchedulerExecutor = ExecutionContext.fromExecutor(handleDisconnectedExecutorThreadPool)\n+\n+    /**\n+     * When onDisconnected is received at the driver endpoint, the superclass DriverEndpoint\n+     * handles it by assuming the Executor was lost for a bad reason and removes the executor\n+     * immediately.\n+     *\n+     * In YARN's case however it is crucial to talk to the application master and ask why the\n+     * executor had exited. In particular, the executor may have exited due to the executor\n+     * having been preempted. If the executor \"exited normally\" according to the application\n+     * master then we pass that information down to the TaskSetManager to inform the\n+     * TaskSetManager that tasks on that lost executor should not count towards a job failure.\n+     */\n+    override def onDisconnected(rpcAddress: RpcAddress): Unit = {\n+      addressToExecutorId.get(rpcAddress).foreach({ executorId =>\n+        val future = yarnSchedulerEndpoint.ask[ExecutorLossReason](GetExecutorLossReason(executorId), askTimeout)\n+        future onSuccess {\n+          case reason: ExecutorLossReason =>\n+            driverEndpoint.askWithRetry[Boolean](RemoveExecutor(executorId, reason))"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "That would only potentially cause issues in client mode, right? Preempting the AM in cluster mode means the driver also dies.\n\nIn the client mode case, wouldn't preempting the AM cause other issues? Won't the AM lose track of a lot of state regarding allocated containers?\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-08-24T22:07:37Z",
    "diffHunk": "@@ -91,6 +92,52 @@ private[spark] abstract class YarnSchedulerBackend(\n   }\n \n   /**\n+   * Override the DriverEndpoint to add extra logic for the case when an executor is disconnected.\n+   * We should check the cluster manager and find if the loss of the executor was caused by YARN\n+   * force killing it due to preemption.\n+   */\n+  private class YarnDriverEndpoint(rpcEnv: RpcEnv, sparkProperties: ArrayBuffer[(String, String)])\n+      extends DriverEndpoint(rpcEnv, sparkProperties) {\n+\n+    private val handleDisconnectedExecutorThreadPool =\n+      ThreadUtils.newDaemonCachedThreadPool(\"yarn-driver-handle-lost-executor-thread-pool\")\n+    implicit val askSchedulerExecutor = ExecutionContext.fromExecutor(handleDisconnectedExecutorThreadPool)\n+\n+    /**\n+     * When onDisconnected is received at the driver endpoint, the superclass DriverEndpoint\n+     * handles it by assuming the Executor was lost for a bad reason and removes the executor\n+     * immediately.\n+     *\n+     * In YARN's case however it is crucial to talk to the application master and ask why the\n+     * executor had exited. In particular, the executor may have exited due to the executor\n+     * having been preempted. If the executor \"exited normally\" according to the application\n+     * master then we pass that information down to the TaskSetManager to inform the\n+     * TaskSetManager that tasks on that lost executor should not count towards a job failure.\n+     */\n+    override def onDisconnected(rpcAddress: RpcAddress): Unit = {\n+      addressToExecutorId.get(rpcAddress).foreach({ executorId =>\n+        val future = yarnSchedulerEndpoint.ask[ExecutorLossReason](GetExecutorLossReason(executorId), askTimeout)\n+        future onSuccess {\n+          case reason: ExecutorLossReason =>\n+            driverEndpoint.askWithRetry[Boolean](RemoveExecutor(executorId, reason))"
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "Yeah, I found that cases when the ApplicationMaster was preempted didn't end well in my testing of this PR. But it wasn't immediately clear if it was because of the PR logic itself or if it's just a case that Spark on YARN client-mode doesn't handle robustly at this time.\n\nIn any case, I agree that removing the hop is the right way to go.\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-08-24T22:12:39Z",
    "diffHunk": "@@ -91,6 +92,52 @@ private[spark] abstract class YarnSchedulerBackend(\n   }\n \n   /**\n+   * Override the DriverEndpoint to add extra logic for the case when an executor is disconnected.\n+   * We should check the cluster manager and find if the loss of the executor was caused by YARN\n+   * force killing it due to preemption.\n+   */\n+  private class YarnDriverEndpoint(rpcEnv: RpcEnv, sparkProperties: ArrayBuffer[(String, String)])\n+      extends DriverEndpoint(rpcEnv, sparkProperties) {\n+\n+    private val handleDisconnectedExecutorThreadPool =\n+      ThreadUtils.newDaemonCachedThreadPool(\"yarn-driver-handle-lost-executor-thread-pool\")\n+    implicit val askSchedulerExecutor = ExecutionContext.fromExecutor(handleDisconnectedExecutorThreadPool)\n+\n+    /**\n+     * When onDisconnected is received at the driver endpoint, the superclass DriverEndpoint\n+     * handles it by assuming the Executor was lost for a bad reason and removes the executor\n+     * immediately.\n+     *\n+     * In YARN's case however it is crucial to talk to the application master and ask why the\n+     * executor had exited. In particular, the executor may have exited due to the executor\n+     * having been preempted. If the executor \"exited normally\" according to the application\n+     * master then we pass that information down to the TaskSetManager to inform the\n+     * TaskSetManager that tasks on that lost executor should not count towards a job failure.\n+     */\n+    override def onDisconnected(rpcAddress: RpcAddress): Unit = {\n+      addressToExecutorId.get(rpcAddress).foreach({ executorId =>\n+        val future = yarnSchedulerEndpoint.ask[ExecutorLossReason](GetExecutorLossReason(executorId), askTimeout)\n+        future onSuccess {\n+          case reason: ExecutorLossReason =>\n+            driverEndpoint.askWithRetry[Boolean](RemoveExecutor(executorId, reason))"
  }],
  "prId": 8007
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Man this is pretty hard to read. I wonder if something like this would compile:\n\n```\n  addressToExecutorId.get(rpcAddress).foreach { executorId =>\n    yarnSchedulerEndpoint.askForExecutorLossReason(executorId, \n      reason => driverEndpoint.askWithRetry[Boolean](RemoveExecutor(executorId, reason),\n      e => {\n        logWarning(s\"Attempted to get executor loss reason\" +\n          s\" for $rpcAddress but got no response. Marking as slave lost.\", e)\n        driverEndpoint.askWithRetry[Boolean](RemoveExecutor(executorId, SlaveLost()))\n      })\n  }\n```\n\nSince this is the only call site for `askForExecutorLossReason` that I can see, it might be better to just inline the code. Should make it more readable.\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-09-02T02:50:55Z",
    "diffHunk": "@@ -91,6 +96,43 @@ private[spark] abstract class YarnSchedulerBackend(\n   }\n \n   /**\n+   * Override the DriverEndpoint to add extra logic for the case when an executor is disconnected.\n+   * We should check the cluster manager and find if the loss of the executor was caused by YARN\n+   * force killing it due to preemption.\n+   */\n+  private class YarnDriverEndpoint(rpcEnv: RpcEnv, sparkProperties: ArrayBuffer[(String, String)])\n+      extends DriverEndpoint(rpcEnv, sparkProperties) {\n+\n+    /**\n+     * When onDisconnected is received at the driver endpoint, the superclass DriverEndpoint\n+     * handles it by assuming the Executor was lost for a bad reason and removes the executor\n+     * immediately.\n+     *\n+     * In YARN's case however it is crucial to talk to the application master and ask why the\n+     * executor had exited. In particular, the executor may have exited due to the executor\n+     * having been preempted. If the executor \"exited normally\" according to the application\n+     * master then we pass that information down to the TaskSetManager to inform the\n+     * TaskSetManager that tasks on that lost executor should not count towards a job failure.\n+     */\n+    override def onDisconnected(rpcAddress: RpcAddress): Unit = {\n+      addressToExecutorId.get(rpcAddress).foreach { executorId =>"
  }],
  "prId": 8007
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: not needed\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-09-02T02:51:33Z",
    "diffHunk": "@@ -113,6 +177,7 @@ private[spark] abstract class YarnSchedulerBackend(\n         removeExecutor(executorId, reason)\n     }\n \n+",
    "line": 126
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "@mccheah can you kill this line and other unnecessary blank lines you added\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-09-09T21:18:34Z",
    "diffHunk": "@@ -113,6 +177,7 @@ private[spark] abstract class YarnSchedulerBackend(\n         removeExecutor(executorId, reason)\n     }\n \n+",
    "line": 126
  }],
  "prId": 8007
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Is this needed? If so, add a comment?\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-09-02T02:51:57Z",
    "diffHunk": "@@ -144,6 +209,7 @@ private[spark] abstract class YarnSchedulerBackend(\n             context.reply(false)\n         }\n \n+      case c: GetExecutorLossReason =>"
  }],
  "prId": 8007
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "Can you make it explicit who this endpoint talks to? Otherwise it's kinda confusing to see two endpoints in the same class on the driver.\n\n```\nAn [[RpcEndpoint]] that communicates with the executors and used internally within the driver.\nHere we override the DriverEndpoint to ...\n```\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-09-03T01:33:46Z",
    "diffHunk": "@@ -91,6 +94,36 @@ private[spark] abstract class YarnSchedulerBackend(\n   }\n \n   /**\n+   * Override the DriverEndpoint to add extra logic for the case when an executor is disconnected."
  }],
  "prId": 8007
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "nit: could you move this above the class definition of `YarnDriverEndpoint` so it's easier to follow?\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-09-03T01:34:18Z",
    "diffHunk": "@@ -91,6 +94,36 @@ private[spark] abstract class YarnSchedulerBackend(\n   }\n \n   /**\n+   * Override the DriverEndpoint to add extra logic for the case when an executor is disconnected.\n+   * We should check the cluster manager and find if the loss of the executor was caused by YARN\n+   * force killing it due to preemption.\n+   */\n+  private class YarnDriverEndpoint(rpcEnv: RpcEnv, sparkProperties: Seq[(String, String)])\n+      extends DriverEndpoint(rpcEnv, sparkProperties) {\n+\n+    /**\n+     * When onDisconnected is received at the driver endpoint, the superclass DriverEndpoint\n+     * handles it by assuming the Executor was lost for a bad reason and removes the executor\n+     * immediately.\n+     *\n+     * In YARN's case however it is crucial to talk to the application master and ask why the\n+     * executor had exited. In particular, the executor may have exited due to the executor\n+     * having been preempted. If the executor \"exited normally\" according to the application\n+     * master then we pass that information down to the TaskSetManager to inform the\n+     * TaskSetManager that tasks on that lost executor should not count towards a job failure.\n+     */\n+    override def onDisconnected(rpcAddress: RpcAddress): Unit = {\n+      addressToExecutorId.get(rpcAddress).foreach { executorId =>\n+        yarnSchedulerEndpoint.handleExecutorDisconnectedFromDriver(executorId, rpcAddress)\n+      }\n+    }\n+  }\n+\n+  override def createDriverEndpoint(properties: Seq[(String, String)]): DriverEndpoint = {\n+    new YarnDriverEndpoint(rpcEnv, properties)\n+  }"
  }],
  "prId": 8007
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "I still think you can merge the two endpoints in this file. In particular, you can make the other `YarnSchedulerEndpoint` extend `DriverEndpoint` and override `onDisconnected` just like how you did it here. Then you'll need to update `ApplicationMaster` to call into `CoarseGrainedSchedulerBackend.ENDPOINT_NAME` instead. Would that work?\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-09-03T01:42:05Z",
    "diffHunk": "@@ -91,6 +94,36 @@ private[spark] abstract class YarnSchedulerBackend(\n   }\n \n   /**\n+   * Override the DriverEndpoint to add extra logic for the case when an executor is disconnected.\n+   * We should check the cluster manager and find if the loss of the executor was caused by YARN\n+   * force killing it due to preemption.\n+   */\n+  private class YarnDriverEndpoint(rpcEnv: RpcEnv, sparkProperties: Seq[(String, String)])\n+      extends DriverEndpoint(rpcEnv, sparkProperties) {"
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "The superclass, CoarseGrainedSchedulerBackend, will use the driver endpoint for messaging as well, right? So then the YarnSchedulerBackend's driver endpoint will now handle both calls for RequestExecutors, KillExecutors, etc. - while it also has to handle CoarseGrainedSchedulerBackend.DriverEndpoint's messages as well. I think the separation of concerns of what endpoints process what messages makes it sensible to have the two endpoints.\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-09-03T02:17:40Z",
    "diffHunk": "@@ -91,6 +94,36 @@ private[spark] abstract class YarnSchedulerBackend(\n   }\n \n   /**\n+   * Override the DriverEndpoint to add extra logic for the case when an executor is disconnected.\n+   * We should check the cluster manager and find if the loss of the executor was caused by YARN\n+   * force killing it due to preemption.\n+   */\n+  private class YarnDriverEndpoint(rpcEnv: RpcEnv, sparkProperties: Seq[(String, String)])\n+      extends DriverEndpoint(rpcEnv, sparkProperties) {"
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "I don't see the benefit in keeping them separate. Now the developer has to worry about which one to send a message to. If s/he accidentally picks the wrong one then we'll just time out on a future, which is hard to debug. Also they're not actually totally separate, since one calls the other.\n\nAnyway I don't feel strongly about this because keeping them separate is actually just preserving existing behavior. I just find the introduction of another endpoint in the same file a little unnecessary.\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-09-03T04:36:52Z",
    "diffHunk": "@@ -91,6 +94,36 @@ private[spark] abstract class YarnSchedulerBackend(\n   }\n \n   /**\n+   * Override the DriverEndpoint to add extra logic for the case when an executor is disconnected.\n+   * We should check the cluster manager and find if the loss of the executor was caused by YARN\n+   * force killing it due to preemption.\n+   */\n+  private class YarnDriverEndpoint(rpcEnv: RpcEnv, sparkProperties: Seq[(String, String)])\n+      extends DriverEndpoint(rpcEnv, sparkProperties) {"
  }],
  "prId": 8007
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "this can just be `handleDisconnectedExecutor`. Also please keep it private\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-09-03T01:43:42Z",
    "diffHunk": "@@ -101,6 +134,32 @@ private[spark] abstract class YarnSchedulerBackend(\n       ThreadUtils.newDaemonCachedThreadPool(\"yarn-scheduler-ask-am-thread-pool\")\n     implicit val askAmExecutor = ExecutionContext.fromExecutor(askAmThreadPool)\n \n+    def handleExecutorDisconnectedFromDriver("
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "Can't be private because YarnDriverEndpoint accesses it.\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-09-03T02:12:37Z",
    "diffHunk": "@@ -101,6 +134,32 @@ private[spark] abstract class YarnSchedulerBackend(\n       ThreadUtils.newDaemonCachedThreadPool(\"yarn-scheduler-ask-am-thread-pool\")\n     implicit val askAmExecutor = ExecutionContext.fromExecutor(askAmThreadPool)\n \n+    def handleExecutorDisconnectedFromDriver("
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "yeah this comment was assuming that the two endpoints are merged, so don't worry about it\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-09-03T04:38:44Z",
    "diffHunk": "@@ -101,6 +134,32 @@ private[spark] abstract class YarnSchedulerBackend(\n       ThreadUtils.newDaemonCachedThreadPool(\"yarn-scheduler-ask-am-thread-pool\")\n     implicit val askAmExecutor = ExecutionContext.fromExecutor(askAmThreadPool)\n \n+    def handleExecutorDisconnectedFromDriver("
  }],
  "prId": 8007
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "delete line\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-09-09T21:10:49Z",
    "diffHunk": "@@ -90,6 +93,41 @@ private[spark] abstract class YarnSchedulerBackend(\n     }\n   }\n \n+  override def createDriverEndpoint(properties: Seq[(String, String)]): DriverEndpoint = {\n+    new YarnDriverEndpoint(rpcEnv, properties)\n+  }\n+\n+  /**\n+   * Override the DriverEndpoint to add extra logic for the case when an executor is disconnected.\n+   * We should check the cluster manager and find if the loss of the executor was caused by YARN\n+   * force killing it due to preemption.\n+   *\n+   * Specifically, this endpoint is the endpoint used by CoarseGrainedSchedulerBackend. Unlike\n+   * YarnSchedulerEndpoint, it handles messages from executors the driver is connected to. It\n+   * does query the application master when the onDisconnected event is received, however.\n+   *"
  }],
  "prId": 8007
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "This paragraph is kinda long-winded. I think it's OK to just say\n\n```\nThis endpoint communicates with the executors and queries the AM for an executor's exit\nstatus when the executor is disconnected.\n```\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-09-09T21:13:16Z",
    "diffHunk": "@@ -90,6 +93,41 @@ private[spark] abstract class YarnSchedulerBackend(\n     }\n   }\n \n+  override def createDriverEndpoint(properties: Seq[(String, String)]): DriverEndpoint = {\n+    new YarnDriverEndpoint(rpcEnv, properties)\n+  }\n+\n+  /**\n+   * Override the DriverEndpoint to add extra logic for the case when an executor is disconnected.\n+   * We should check the cluster manager and find if the loss of the executor was caused by YARN\n+   * force killing it due to preemption.\n+   *\n+   * Specifically, this endpoint is the endpoint used by CoarseGrainedSchedulerBackend. Unlike\n+   * YarnSchedulerEndpoint, it handles messages from executors the driver is connected to. It\n+   * does query the application master when the onDisconnected event is received, however."
  }],
  "prId": 8007
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "can you add a TODO here that succinctly describes the potential race condition?\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-09-09T21:14:11Z",
    "diffHunk": "@@ -90,6 +93,41 @@ private[spark] abstract class YarnSchedulerBackend(\n     }\n   }\n \n+  override def createDriverEndpoint(properties: Seq[(String, String)]): DriverEndpoint = {\n+    new YarnDriverEndpoint(rpcEnv, properties)\n+  }\n+\n+  /**\n+   * Override the DriverEndpoint to add extra logic for the case when an executor is disconnected.\n+   * We should check the cluster manager and find if the loss of the executor was caused by YARN\n+   * force killing it due to preemption.\n+   *\n+   * Specifically, this endpoint is the endpoint used by CoarseGrainedSchedulerBackend. Unlike\n+   * YarnSchedulerEndpoint, it handles messages from executors the driver is connected to. It\n+   * does query the application master when the onDisconnected event is received, however.\n+   *\n+   */\n+  private class YarnDriverEndpoint(rpcEnv: RpcEnv, sparkProperties: Seq[(String, String)])\n+      extends DriverEndpoint(rpcEnv, sparkProperties) {\n+\n+    /**\n+     * When onDisconnected is received at the driver endpoint, the superclass DriverEndpoint\n+     * handles it by assuming the Executor was lost for a bad reason and removes the executor\n+     * immediately.\n+     *\n+     * In YARN's case however it is crucial to talk to the application master and ask why the\n+     * executor had exited. In particular, the executor may have exited due to the executor\n+     * having been preempted. If the executor \"exited normally\" according to the application\n+     * master then we pass that information down to the TaskSetManager to inform the\n+     * TaskSetManager that tasks on that lost executor should not count towards a job failure.\n+     */"
  }],
  "prId": 8007
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "unlike `catch`, this actually swallows fatal exceptions. Unfortunately you need to add a\n\n```\ncase t => throw t\n```\n",
    "commit": "ee6fffa9d088271aafe13d15f4db326ed2a0c0c4",
    "createdAt": "2015-09-09T21:18:18Z",
    "diffHunk": "@@ -101,6 +139,32 @@ private[spark] abstract class YarnSchedulerBackend(\n       ThreadUtils.newDaemonCachedThreadPool(\"yarn-scheduler-ask-am-thread-pool\")\n     implicit val askAmExecutor = ExecutionContext.fromExecutor(askAmThreadPool)\n \n+    private[YarnSchedulerBackend] def handleExecutorDisconnectedFromDriver(\n+        executorId: String,\n+        executorRpcAddress: RpcAddress): Unit = {\n+      amEndpoint match {\n+        case Some(am) =>\n+          val lossReasonRequest = GetExecutorLossReason(executorId)\n+          val future = am.ask[ExecutorLossReason](lossReasonRequest, askTimeout)\n+          future onSuccess {\n+            case reason: ExecutorLossReason => {\n+              driverEndpoint.askWithRetry[Boolean](RemoveExecutor(executorId, reason))\n+            }\n+          }\n+          future onFailure {\n+            case NonFatal(e) => {\n+              logWarning(s\"Attempted to get executor loss reason\" +\n+                s\" for executor id ${executorId} at RPC address ${executorRpcAddress},\" +\n+                s\" but got no response. Marking as slave lost.\", e)\n+              driverEndpoint.askWithRetry[Boolean](RemoveExecutor(executorId, SlaveLost()))\n+            }",
    "line": 110
  }],
  "prId": 8007
}]