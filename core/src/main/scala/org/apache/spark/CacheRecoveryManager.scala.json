[{
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "executor's",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-10-10T17:33:01Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "fixed",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-05T15:49:43Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Same thing as before with the weird name. \"cache-recovery-manager-pool\" is a much better name.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-10-10T17:34:06Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "fixed",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-05T15:50:06Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "You never use the return value, why not `Unit`?",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-10-10T17:38:36Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "fixed",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-05T15:50:40Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`: Unit =`",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-10-10T17:39:39Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "this method is gone",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-05T15:51:31Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "The method doesn't return the list, so its short description should more accurately reflect what it actually does.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-10-10T17:40:30Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "This method is gone now.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-05T15:51:13Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Why isn't this imported at the top? (And you know scala allows you to rename types on import, in case there's a conflict, right?)",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-10-10T17:41:59Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+    case (executorId, NoMoreBlocks | NotEnoughMemory) => state.killExecutor(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    import scala.util.Success"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "changed this and renamed Success => Succ",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-05T15:52:02Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+    case (executorId, NoMoreBlocks | NotEnoughMemory) => state.killExecutor(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    import scala.util.Success"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "s/exec/executor. That is if you even want to keep this log around...",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-10-10T17:44:02Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+    case (executorId, NoMoreBlocks | NotEnoughMemory) => state.killExecutor(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    import scala.util.Success\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "fixed",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-05T15:52:38Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+    case (executorId, NoMoreBlocks | NotEnoughMemory) => state.killExecutor(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    import scala.util.Success\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "one more indent level",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-10-10T17:45:45Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+    case (executorId, NoMoreBlocks | NotEnoughMemory) => state.killExecutor(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    import scala.util.Success\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case Success(false) =>\n+        checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object CacheRecoveryManager {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): CacheRecoveryManager = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new CacheRecoveryManagerState(bmme, eam, conf)\n+    new CacheRecoveryManager(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ *\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class CacheRecoveryManagerState(\n+   blockManagerMasterEndpoint: RpcEndpointRef,"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "removed this",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-05T15:52:59Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+    case (executorId, NoMoreBlocks | NotEnoughMemory) => state.killExecutor(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    import scala.util.Success\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case Success(false) =>\n+        checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object CacheRecoveryManager {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): CacheRecoveryManager = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new CacheRecoveryManagerState(bmme, eam, conf)\n+    new CacheRecoveryManager(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ *\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class CacheRecoveryManagerState(\n+   blockManagerMasterEndpoint: RpcEndpointRef,"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "indent should follow same style as the class at the top of this file",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-10-10T17:46:18Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+    case (executorId, NoMoreBlocks | NotEnoughMemory) => state.killExecutor(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    import scala.util.Success\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case Success(false) =>\n+        checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object CacheRecoveryManager {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): CacheRecoveryManager = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new CacheRecoveryManagerState(bmme, eam, conf)\n+    new CacheRecoveryManager(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ *\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class CacheRecoveryManagerState(\n+   blockManagerMasterEndpoint: RpcEndpointRef,\n+   executorAllocationManager: ExecutorAllocationManager,\n+   conf: SparkConf\n+ ) extends Logging {"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "removed this",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-05T15:53:11Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+    case (executorId, NoMoreBlocks | NotEnoughMemory) => state.killExecutor(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    import scala.util.Success\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case Success(false) =>\n+        checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object CacheRecoveryManager {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): CacheRecoveryManager = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new CacheRecoveryManagerState(bmme, eam, conf)\n+    new CacheRecoveryManager(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ *\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class CacheRecoveryManagerState(\n+   blockManagerMasterEndpoint: RpcEndpointRef,\n+   executorAllocationManager: ExecutorAllocationManager,\n+   conf: SparkConf\n+ ) extends Logging {"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "s/recover-cache/cache-recovery",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-10-10T17:46:57Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+    case (executorId, NoMoreBlocks | NotEnoughMemory) => state.killExecutor(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    import scala.util.Success\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case Success(false) =>\n+        checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object CacheRecoveryManager {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): CacheRecoveryManager = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new CacheRecoveryManagerState(bmme, eam, conf)\n+    new CacheRecoveryManager(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ *\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class CacheRecoveryManagerState(\n+   blockManagerMasterEndpoint: RpcEndpointRef,\n+   executorAllocationManager: ExecutorAllocationManager,\n+   conf: SparkConf\n+ ) extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"recover-cache-shutdown-timers\")"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "fixed",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-05T15:53:37Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+    case (executorId, NoMoreBlocks | NotEnoughMemory) => state.killExecutor(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    import scala.util.Success\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case Success(false) =>\n+        checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object CacheRecoveryManager {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): CacheRecoveryManager = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new CacheRecoveryManagerState(bmme, eam, conf)\n+    new CacheRecoveryManager(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ *\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class CacheRecoveryManagerState(\n+   blockManagerMasterEndpoint: RpcEndpointRef,\n+   executorAllocationManager: ExecutorAllocationManager,\n+   conf: SparkConf\n+ ) extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"recover-cache-shutdown-timers\")"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "s/RECOVER_CACHE/CACHE_RECOVERY",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-10-10T17:47:17Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "fixed",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-05T15:48:51Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "\"...enough memory in the remaining executors to hold the cached blocks on the given executors.\"",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-10-10T17:50:33Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+    case (executorId, NoMoreBlocks | NotEnoughMemory) => state.killExecutor(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    import scala.util.Success\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case Success(false) =>\n+        checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object CacheRecoveryManager {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): CacheRecoveryManager = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new CacheRecoveryManagerState(bmme, eam, conf)\n+    new CacheRecoveryManager(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ *\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class CacheRecoveryManagerState(\n+   blockManagerMasterEndpoint: RpcEndpointRef,\n+   executorAllocationManager: ExecutorAllocationManager,\n+   conf: SparkConf\n+ ) extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"recover-cache-shutdown-timers\")\n+\n+    private val blocksToSave = new mutable.HashMap[String, mutable.PriorityQueue[RDDBlockId]]\n+    private val savedBlocks = new mutable.HashMap[String, mutable.HashSet[RDDBlockId]]\n+    private val killTimers = new mutable.HashMap[String, ScheduledFuture[_]]\n+\n+  /**\n+   * Query Block Manager Master for cached blocks.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executorId to its replication state.\n+   */\n+  def getBlocks(execIds: Seq[String]): Map[String, ExecutorReplicationState] = synchronized {\n+    val isThereEnoughMemory = checkFreeMem(execIds)\n+\n+    isThereEnoughMemory.map {\n+      case (execId, true) =>\n+        updateBlockState(execId)\n+        val blocksRemain = blocksToSave.get(execId).exists(_.nonEmpty)\n+        execId -> (if (blocksRemain) HasCachedBlocks else NoMoreBlocks)\n+      case (execId, false) =>\n+        execId -> NotEnoughMemory\n+    }\n+  }\n+\n+  /**\n+   * Checks to see if there is enough memory on the cluster to hold the cached blocks on these"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "changed",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-05T17:36:37Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+    case (executorId, NoMoreBlocks | NotEnoughMemory) => state.killExecutor(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    import scala.util.Success\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case Success(false) =>\n+        checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object CacheRecoveryManager {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): CacheRecoveryManager = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new CacheRecoveryManagerState(bmme, eam, conf)\n+    new CacheRecoveryManager(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ *\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class CacheRecoveryManagerState(\n+   blockManagerMasterEndpoint: RpcEndpointRef,\n+   executorAllocationManager: ExecutorAllocationManager,\n+   conf: SparkConf\n+ ) extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"recover-cache-shutdown-timers\")\n+\n+    private val blocksToSave = new mutable.HashMap[String, mutable.PriorityQueue[RDDBlockId]]\n+    private val savedBlocks = new mutable.HashMap[String, mutable.HashSet[RDDBlockId]]\n+    private val killTimers = new mutable.HashMap[String, ScheduledFuture[_]]\n+\n+  /**\n+   * Query Block Manager Master for cached blocks.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executorId to its replication state.\n+   */\n+  def getBlocks(execIds: Seq[String]): Map[String, ExecutorReplicationState] = synchronized {\n+    val isThereEnoughMemory = checkFreeMem(execIds)\n+\n+    isThereEnoughMemory.map {\n+      case (execId, true) =>\n+        updateBlockState(execId)\n+        val blocksRemain = blocksToSave.get(execId).exists(_.nonEmpty)\n+        execId -> (if (blocksRemain) HasCachedBlocks else NoMoreBlocks)\n+      case (execId, false) =>\n+        execId -> NotEnoughMemory\n+    }\n+  }\n+\n+  /**\n+   * Checks to see if there is enough memory on the cluster to hold the cached blocks on these"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: `case (_, v)`",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-10-10T18:05:53Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+    case (executorId, NoMoreBlocks | NotEnoughMemory) => state.killExecutor(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    import scala.util.Success\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case Success(false) =>\n+        checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object CacheRecoveryManager {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): CacheRecoveryManager = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new CacheRecoveryManagerState(bmme, eam, conf)\n+    new CacheRecoveryManager(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ *\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class CacheRecoveryManagerState(\n+   blockManagerMasterEndpoint: RpcEndpointRef,\n+   executorAllocationManager: ExecutorAllocationManager,\n+   conf: SparkConf\n+ ) extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"recover-cache-shutdown-timers\")\n+\n+    private val blocksToSave = new mutable.HashMap[String, mutable.PriorityQueue[RDDBlockId]]\n+    private val savedBlocks = new mutable.HashMap[String, mutable.HashSet[RDDBlockId]]\n+    private val killTimers = new mutable.HashMap[String, ScheduledFuture[_]]\n+\n+  /**\n+   * Query Block Manager Master for cached blocks.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executorId to its replication state.\n+   */\n+  def getBlocks(execIds: Seq[String]): Map[String, ExecutorReplicationState] = synchronized {\n+    val isThereEnoughMemory = checkFreeMem(execIds)\n+\n+    isThereEnoughMemory.map {\n+      case (execId, true) =>\n+        updateBlockState(execId)\n+        val blocksRemain = blocksToSave.get(execId).exists(_.nonEmpty)\n+        execId -> (if (blocksRemain) HasCachedBlocks else NoMoreBlocks)\n+      case (execId, false) =>\n+        execId -> NotEnoughMemory\n+    }\n+  }\n+\n+  /**\n+   * Checks to see if there is enough memory on the cluster to hold the cached blocks on these\n+   * executors. Checks on a per executor basis from the smallest to the largest.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executor ids to whether or not there is enough free memory for them.\n+   */\n+  private def checkFreeMem(execIds: Seq[String]): Map[String, Boolean] = {\n+    val execIdsToShutDown = execIds.toSet\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+\n+    val (expiringMemStatus, remainingMemStatus) = allExecMemStatus.partition {\n+      case (k, v) => execIdsToShutDown.contains(k)\n+    }\n+    val freeMemOnRemaining = remainingMemStatus.values.map(_._2).sum\n+\n+    val alreadyReplicatedBlocks: Map[String, Set[RDDBlockId]] =\n+      savedBlocks.filterKeys(execIdsToShutDown).map { case (k, v) => k -> v.toSet }.toMap\n+\n+    val getSizes = GetSizeOfBlocks(alreadyReplicatedBlocks)\n+    val alreadyReplicatedBytes = blockManagerMasterEndpoint.askSync[Map[String, Long]](getSizes)\n+\n+    val bytesToReplicate: Seq[(String, Long)] =\n+      expiringMemStatus.map { case (execId, (maxMem, remainingMem)) =>\n+        val alreadyReplicated = alreadyReplicatedBytes.getOrElse(execId, 0L)\n+        val usedMem = maxMem - remainingMem\n+        val toBeReplicatedMem = usedMem - alreadyReplicated\n+        execId -> toBeReplicatedMem\n+      }.toSeq.sortBy { case (k, v) => v }"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "fixed",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-05T17:38:44Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+    case (executorId, NoMoreBlocks | NotEnoughMemory) => state.killExecutor(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    import scala.util.Success\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case Success(false) =>\n+        checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object CacheRecoveryManager {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): CacheRecoveryManager = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new CacheRecoveryManagerState(bmme, eam, conf)\n+    new CacheRecoveryManager(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ *\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class CacheRecoveryManagerState(\n+   blockManagerMasterEndpoint: RpcEndpointRef,\n+   executorAllocationManager: ExecutorAllocationManager,\n+   conf: SparkConf\n+ ) extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"recover-cache-shutdown-timers\")\n+\n+    private val blocksToSave = new mutable.HashMap[String, mutable.PriorityQueue[RDDBlockId]]\n+    private val savedBlocks = new mutable.HashMap[String, mutable.HashSet[RDDBlockId]]\n+    private val killTimers = new mutable.HashMap[String, ScheduledFuture[_]]\n+\n+  /**\n+   * Query Block Manager Master for cached blocks.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executorId to its replication state.\n+   */\n+  def getBlocks(execIds: Seq[String]): Map[String, ExecutorReplicationState] = synchronized {\n+    val isThereEnoughMemory = checkFreeMem(execIds)\n+\n+    isThereEnoughMemory.map {\n+      case (execId, true) =>\n+        updateBlockState(execId)\n+        val blocksRemain = blocksToSave.get(execId).exists(_.nonEmpty)\n+        execId -> (if (blocksRemain) HasCachedBlocks else NoMoreBlocks)\n+      case (execId, false) =>\n+        execId -> NotEnoughMemory\n+    }\n+  }\n+\n+  /**\n+   * Checks to see if there is enough memory on the cluster to hold the cached blocks on these\n+   * executors. Checks on a per executor basis from the smallest to the largest.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executor ids to whether or not there is enough free memory for them.\n+   */\n+  private def checkFreeMem(execIds: Seq[String]): Map[String, Boolean] = {\n+    val execIdsToShutDown = execIds.toSet\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+\n+    val (expiringMemStatus, remainingMemStatus) = allExecMemStatus.partition {\n+      case (k, v) => execIdsToShutDown.contains(k)\n+    }\n+    val freeMemOnRemaining = remainingMemStatus.values.map(_._2).sum\n+\n+    val alreadyReplicatedBlocks: Map[String, Set[RDDBlockId]] =\n+      savedBlocks.filterKeys(execIdsToShutDown).map { case (k, v) => k -> v.toSet }.toMap\n+\n+    val getSizes = GetSizeOfBlocks(alreadyReplicatedBlocks)\n+    val alreadyReplicatedBytes = blockManagerMasterEndpoint.askSync[Map[String, Long]](getSizes)\n+\n+    val bytesToReplicate: Seq[(String, Long)] =\n+      expiringMemStatus.map { case (execId, (maxMem, remainingMem)) =>\n+        val alreadyReplicated = alreadyReplicatedBytes.getOrElse(execId, 0L)\n+        val usedMem = maxMem - remainingMem\n+        val toBeReplicatedMem = usedMem - alreadyReplicated\n+        execId -> toBeReplicatedMem\n+      }.toSeq.sortBy { case (k, v) => v }"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "I prefer to indent these a little bit differently:\r\n\r\n```\r\nbytesToReplicate\r\n  .scan { ... =>\r\n  }\r\n  .drop(1)\r\n   ...\r\n```\r\n\r\nAlso, probably not a big issue here, but I'd rather avoid `mapValues`. It causes stack overflows when you have lots of elements, so better to just avoid it and use `map` instead.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-10-10T18:14:22Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+    case (executorId, NoMoreBlocks | NotEnoughMemory) => state.killExecutor(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    import scala.util.Success\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case Success(false) =>\n+        checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object CacheRecoveryManager {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): CacheRecoveryManager = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new CacheRecoveryManagerState(bmme, eam, conf)\n+    new CacheRecoveryManager(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ *\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class CacheRecoveryManagerState(\n+   blockManagerMasterEndpoint: RpcEndpointRef,\n+   executorAllocationManager: ExecutorAllocationManager,\n+   conf: SparkConf\n+ ) extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"recover-cache-shutdown-timers\")\n+\n+    private val blocksToSave = new mutable.HashMap[String, mutable.PriorityQueue[RDDBlockId]]\n+    private val savedBlocks = new mutable.HashMap[String, mutable.HashSet[RDDBlockId]]\n+    private val killTimers = new mutable.HashMap[String, ScheduledFuture[_]]\n+\n+  /**\n+   * Query Block Manager Master for cached blocks.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executorId to its replication state.\n+   */\n+  def getBlocks(execIds: Seq[String]): Map[String, ExecutorReplicationState] = synchronized {\n+    val isThereEnoughMemory = checkFreeMem(execIds)\n+\n+    isThereEnoughMemory.map {\n+      case (execId, true) =>\n+        updateBlockState(execId)\n+        val blocksRemain = blocksToSave.get(execId).exists(_.nonEmpty)\n+        execId -> (if (blocksRemain) HasCachedBlocks else NoMoreBlocks)\n+      case (execId, false) =>\n+        execId -> NotEnoughMemory\n+    }\n+  }\n+\n+  /**\n+   * Checks to see if there is enough memory on the cluster to hold the cached blocks on these\n+   * executors. Checks on a per executor basis from the smallest to the largest.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executor ids to whether or not there is enough free memory for them.\n+   */\n+  private def checkFreeMem(execIds: Seq[String]): Map[String, Boolean] = {\n+    val execIdsToShutDown = execIds.toSet\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+\n+    val (expiringMemStatus, remainingMemStatus) = allExecMemStatus.partition {\n+      case (k, v) => execIdsToShutDown.contains(k)\n+    }\n+    val freeMemOnRemaining = remainingMemStatus.values.map(_._2).sum\n+\n+    val alreadyReplicatedBlocks: Map[String, Set[RDDBlockId]] =\n+      savedBlocks.filterKeys(execIdsToShutDown).map { case (k, v) => k -> v.toSet }.toMap\n+\n+    val getSizes = GetSizeOfBlocks(alreadyReplicatedBlocks)\n+    val alreadyReplicatedBytes = blockManagerMasterEndpoint.askSync[Map[String, Long]](getSizes)\n+\n+    val bytesToReplicate: Seq[(String, Long)] =\n+      expiringMemStatus.map { case (execId, (maxMem, remainingMem)) =>\n+        val alreadyReplicated = alreadyReplicatedBytes.getOrElse(execId, 0L)\n+        val usedMem = maxMem - remainingMem\n+        val toBeReplicatedMem = usedMem - alreadyReplicated\n+        execId -> toBeReplicatedMem\n+      }.toSeq.sortBy { case (k, v) => v }\n+\n+    bytesToReplicate.scan((\"start\", freeMemOnRemaining)) {"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "fixed",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-05T17:40:52Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+    case (executorId, NoMoreBlocks | NotEnoughMemory) => state.killExecutor(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    import scala.util.Success\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case Success(false) =>\n+        checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object CacheRecoveryManager {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): CacheRecoveryManager = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new CacheRecoveryManagerState(bmme, eam, conf)\n+    new CacheRecoveryManager(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ *\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class CacheRecoveryManagerState(\n+   blockManagerMasterEndpoint: RpcEndpointRef,\n+   executorAllocationManager: ExecutorAllocationManager,\n+   conf: SparkConf\n+ ) extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"recover-cache-shutdown-timers\")\n+\n+    private val blocksToSave = new mutable.HashMap[String, mutable.PriorityQueue[RDDBlockId]]\n+    private val savedBlocks = new mutable.HashMap[String, mutable.HashSet[RDDBlockId]]\n+    private val killTimers = new mutable.HashMap[String, ScheduledFuture[_]]\n+\n+  /**\n+   * Query Block Manager Master for cached blocks.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executorId to its replication state.\n+   */\n+  def getBlocks(execIds: Seq[String]): Map[String, ExecutorReplicationState] = synchronized {\n+    val isThereEnoughMemory = checkFreeMem(execIds)\n+\n+    isThereEnoughMemory.map {\n+      case (execId, true) =>\n+        updateBlockState(execId)\n+        val blocksRemain = blocksToSave.get(execId).exists(_.nonEmpty)\n+        execId -> (if (blocksRemain) HasCachedBlocks else NoMoreBlocks)\n+      case (execId, false) =>\n+        execId -> NotEnoughMemory\n+    }\n+  }\n+\n+  /**\n+   * Checks to see if there is enough memory on the cluster to hold the cached blocks on these\n+   * executors. Checks on a per executor basis from the smallest to the largest.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executor ids to whether or not there is enough free memory for them.\n+   */\n+  private def checkFreeMem(execIds: Seq[String]): Map[String, Boolean] = {\n+    val execIdsToShutDown = execIds.toSet\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+\n+    val (expiringMemStatus, remainingMemStatus) = allExecMemStatus.partition {\n+      case (k, v) => execIdsToShutDown.contains(k)\n+    }\n+    val freeMemOnRemaining = remainingMemStatus.values.map(_._2).sum\n+\n+    val alreadyReplicatedBlocks: Map[String, Set[RDDBlockId]] =\n+      savedBlocks.filterKeys(execIdsToShutDown).map { case (k, v) => k -> v.toSet }.toMap\n+\n+    val getSizes = GetSizeOfBlocks(alreadyReplicatedBlocks)\n+    val alreadyReplicatedBytes = blockManagerMasterEndpoint.askSync[Map[String, Long]](getSizes)\n+\n+    val bytesToReplicate: Seq[(String, Long)] =\n+      expiringMemStatus.map { case (execId, (maxMem, remainingMem)) =>\n+        val alreadyReplicated = alreadyReplicatedBytes.getOrElse(execId, 0L)\n+        val usedMem = maxMem - remainingMem\n+        val toBeReplicatedMem = usedMem - alreadyReplicated\n+        execId -> toBeReplicatedMem\n+      }.toSeq.sortBy { case (k, v) => v }\n+\n+    bytesToReplicate.scan((\"start\", freeMemOnRemaining)) {"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "This should be `> 0` right?",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-10-10T18:15:59Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+    case (executorId, NoMoreBlocks | NotEnoughMemory) => state.killExecutor(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    import scala.util.Success\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case Success(false) =>\n+        checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object CacheRecoveryManager {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): CacheRecoveryManager = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new CacheRecoveryManagerState(bmme, eam, conf)\n+    new CacheRecoveryManager(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ *\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class CacheRecoveryManagerState(\n+   blockManagerMasterEndpoint: RpcEndpointRef,\n+   executorAllocationManager: ExecutorAllocationManager,\n+   conf: SparkConf\n+ ) extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"recover-cache-shutdown-timers\")\n+\n+    private val blocksToSave = new mutable.HashMap[String, mutable.PriorityQueue[RDDBlockId]]\n+    private val savedBlocks = new mutable.HashMap[String, mutable.HashSet[RDDBlockId]]\n+    private val killTimers = new mutable.HashMap[String, ScheduledFuture[_]]\n+\n+  /**\n+   * Query Block Manager Master for cached blocks.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executorId to its replication state.\n+   */\n+  def getBlocks(execIds: Seq[String]): Map[String, ExecutorReplicationState] = synchronized {\n+    val isThereEnoughMemory = checkFreeMem(execIds)\n+\n+    isThereEnoughMemory.map {\n+      case (execId, true) =>\n+        updateBlockState(execId)\n+        val blocksRemain = blocksToSave.get(execId).exists(_.nonEmpty)\n+        execId -> (if (blocksRemain) HasCachedBlocks else NoMoreBlocks)\n+      case (execId, false) =>\n+        execId -> NotEnoughMemory\n+    }\n+  }\n+\n+  /**\n+   * Checks to see if there is enough memory on the cluster to hold the cached blocks on these\n+   * executors. Checks on a per executor basis from the smallest to the largest.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executor ids to whether or not there is enough free memory for them.\n+   */\n+  private def checkFreeMem(execIds: Seq[String]): Map[String, Boolean] = {\n+    val execIdsToShutDown = execIds.toSet\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+\n+    val (expiringMemStatus, remainingMemStatus) = allExecMemStatus.partition {\n+      case (k, v) => execIdsToShutDown.contains(k)\n+    }\n+    val freeMemOnRemaining = remainingMemStatus.values.map(_._2).sum\n+\n+    val alreadyReplicatedBlocks: Map[String, Set[RDDBlockId]] =\n+      savedBlocks.filterKeys(execIdsToShutDown).map { case (k, v) => k -> v.toSet }.toMap\n+\n+    val getSizes = GetSizeOfBlocks(alreadyReplicatedBlocks)\n+    val alreadyReplicatedBytes = blockManagerMasterEndpoint.askSync[Map[String, Long]](getSizes)\n+\n+    val bytesToReplicate: Seq[(String, Long)] =\n+      expiringMemStatus.map { case (execId, (maxMem, remainingMem)) =>\n+        val alreadyReplicated = alreadyReplicatedBytes.getOrElse(execId, 0L)\n+        val usedMem = maxMem - remainingMem\n+        val toBeReplicatedMem = usedMem - alreadyReplicated\n+        execId -> toBeReplicatedMem\n+      }.toSeq.sortBy { case (k, v) => v }\n+\n+    bytesToReplicate.scan((\"start\", freeMemOnRemaining)) {\n+      case ((_, remainingMem), (execId, mem)) => (execId, remainingMem - mem)\n+    }.drop(1)\n+      .toMap\n+      .mapValues(freeMem => freeMem >= 0)"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "fixed",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-05T17:44:51Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+    case (executorId, NoMoreBlocks | NotEnoughMemory) => state.killExecutor(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    import scala.util.Success\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case Success(false) =>\n+        checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object CacheRecoveryManager {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): CacheRecoveryManager = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new CacheRecoveryManagerState(bmme, eam, conf)\n+    new CacheRecoveryManager(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ *\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class CacheRecoveryManagerState(\n+   blockManagerMasterEndpoint: RpcEndpointRef,\n+   executorAllocationManager: ExecutorAllocationManager,\n+   conf: SparkConf\n+ ) extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"recover-cache-shutdown-timers\")\n+\n+    private val blocksToSave = new mutable.HashMap[String, mutable.PriorityQueue[RDDBlockId]]\n+    private val savedBlocks = new mutable.HashMap[String, mutable.HashSet[RDDBlockId]]\n+    private val killTimers = new mutable.HashMap[String, ScheduledFuture[_]]\n+\n+  /**\n+   * Query Block Manager Master for cached blocks.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executorId to its replication state.\n+   */\n+  def getBlocks(execIds: Seq[String]): Map[String, ExecutorReplicationState] = synchronized {\n+    val isThereEnoughMemory = checkFreeMem(execIds)\n+\n+    isThereEnoughMemory.map {\n+      case (execId, true) =>\n+        updateBlockState(execId)\n+        val blocksRemain = blocksToSave.get(execId).exists(_.nonEmpty)\n+        execId -> (if (blocksRemain) HasCachedBlocks else NoMoreBlocks)\n+      case (execId, false) =>\n+        execId -> NotEnoughMemory\n+    }\n+  }\n+\n+  /**\n+   * Checks to see if there is enough memory on the cluster to hold the cached blocks on these\n+   * executors. Checks on a per executor basis from the smallest to the largest.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executor ids to whether or not there is enough free memory for them.\n+   */\n+  private def checkFreeMem(execIds: Seq[String]): Map[String, Boolean] = {\n+    val execIdsToShutDown = execIds.toSet\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+\n+    val (expiringMemStatus, remainingMemStatus) = allExecMemStatus.partition {\n+      case (k, v) => execIdsToShutDown.contains(k)\n+    }\n+    val freeMemOnRemaining = remainingMemStatus.values.map(_._2).sum\n+\n+    val alreadyReplicatedBlocks: Map[String, Set[RDDBlockId]] =\n+      savedBlocks.filterKeys(execIdsToShutDown).map { case (k, v) => k -> v.toSet }.toMap\n+\n+    val getSizes = GetSizeOfBlocks(alreadyReplicatedBlocks)\n+    val alreadyReplicatedBytes = blockManagerMasterEndpoint.askSync[Map[String, Long]](getSizes)\n+\n+    val bytesToReplicate: Seq[(String, Long)] =\n+      expiringMemStatus.map { case (execId, (maxMem, remainingMem)) =>\n+        val alreadyReplicated = alreadyReplicatedBytes.getOrElse(execId, 0L)\n+        val usedMem = maxMem - remainingMem\n+        val toBeReplicatedMem = usedMem - alreadyReplicated\n+        execId -> toBeReplicatedMem\n+      }.toSeq.sortBy { case (k, v) => v }\n+\n+    bytesToReplicate.scan((\"start\", freeMemOnRemaining)) {\n+      case ((_, remainingMem), (execId, mem)) => (execId, remainingMem - mem)\n+    }.drop(1)\n+      .toMap\n+      .mapValues(freeMem => freeMem >= 0)"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "\"Updates the internal state to track the blocks from the given executor that need recovery.\"\r\n\r\nAlways start with what the method's purpose, not how it achieves that.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-10-10T18:16:45Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+    case (executorId, NoMoreBlocks | NotEnoughMemory) => state.killExecutor(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    import scala.util.Success\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case Success(false) =>\n+        checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object CacheRecoveryManager {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): CacheRecoveryManager = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new CacheRecoveryManagerState(bmme, eam, conf)\n+    new CacheRecoveryManager(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ *\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class CacheRecoveryManagerState(\n+   blockManagerMasterEndpoint: RpcEndpointRef,\n+   executorAllocationManager: ExecutorAllocationManager,\n+   conf: SparkConf\n+ ) extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"recover-cache-shutdown-timers\")\n+\n+    private val blocksToSave = new mutable.HashMap[String, mutable.PriorityQueue[RDDBlockId]]\n+    private val savedBlocks = new mutable.HashMap[String, mutable.HashSet[RDDBlockId]]\n+    private val killTimers = new mutable.HashMap[String, ScheduledFuture[_]]\n+\n+  /**\n+   * Query Block Manager Master for cached blocks.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executorId to its replication state.\n+   */\n+  def getBlocks(execIds: Seq[String]): Map[String, ExecutorReplicationState] = synchronized {\n+    val isThereEnoughMemory = checkFreeMem(execIds)\n+\n+    isThereEnoughMemory.map {\n+      case (execId, true) =>\n+        updateBlockState(execId)\n+        val blocksRemain = blocksToSave.get(execId).exists(_.nonEmpty)\n+        execId -> (if (blocksRemain) HasCachedBlocks else NoMoreBlocks)\n+      case (execId, false) =>\n+        execId -> NotEnoughMemory\n+    }\n+  }\n+\n+  /**\n+   * Checks to see if there is enough memory on the cluster to hold the cached blocks on these\n+   * executors. Checks on a per executor basis from the smallest to the largest.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executor ids to whether or not there is enough free memory for them.\n+   */\n+  private def checkFreeMem(execIds: Seq[String]): Map[String, Boolean] = {\n+    val execIdsToShutDown = execIds.toSet\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+\n+    val (expiringMemStatus, remainingMemStatus) = allExecMemStatus.partition {\n+      case (k, v) => execIdsToShutDown.contains(k)\n+    }\n+    val freeMemOnRemaining = remainingMemStatus.values.map(_._2).sum\n+\n+    val alreadyReplicatedBlocks: Map[String, Set[RDDBlockId]] =\n+      savedBlocks.filterKeys(execIdsToShutDown).map { case (k, v) => k -> v.toSet }.toMap\n+\n+    val getSizes = GetSizeOfBlocks(alreadyReplicatedBlocks)\n+    val alreadyReplicatedBytes = blockManagerMasterEndpoint.askSync[Map[String, Long]](getSizes)\n+\n+    val bytesToReplicate: Seq[(String, Long)] =\n+      expiringMemStatus.map { case (execId, (maxMem, remainingMem)) =>\n+        val alreadyReplicated = alreadyReplicatedBytes.getOrElse(execId, 0L)\n+        val usedMem = maxMem - remainingMem\n+        val toBeReplicatedMem = usedMem - alreadyReplicated\n+        execId -> toBeReplicatedMem\n+      }.toSeq.sortBy { case (k, v) => v }\n+\n+    bytesToReplicate.scan((\"start\", freeMemOnRemaining)) {\n+      case ((_, remainingMem), (execId, mem)) => (execId, remainingMem - mem)\n+    }.drop(1)\n+      .toMap\n+      .mapValues(freeMem => freeMem >= 0)\n+  }\n+\n+  /**\n+   * Gets a list of cached blocks from the block manager master and updates."
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "removed this method",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-05T17:45:11Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+    case (executorId, NoMoreBlocks | NotEnoughMemory) => state.killExecutor(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    import scala.util.Success\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case Success(false) =>\n+        checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object CacheRecoveryManager {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): CacheRecoveryManager = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new CacheRecoveryManagerState(bmme, eam, conf)\n+    new CacheRecoveryManager(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ *\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class CacheRecoveryManagerState(\n+   blockManagerMasterEndpoint: RpcEndpointRef,\n+   executorAllocationManager: ExecutorAllocationManager,\n+   conf: SparkConf\n+ ) extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"recover-cache-shutdown-timers\")\n+\n+    private val blocksToSave = new mutable.HashMap[String, mutable.PriorityQueue[RDDBlockId]]\n+    private val savedBlocks = new mutable.HashMap[String, mutable.HashSet[RDDBlockId]]\n+    private val killTimers = new mutable.HashMap[String, ScheduledFuture[_]]\n+\n+  /**\n+   * Query Block Manager Master for cached blocks.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executorId to its replication state.\n+   */\n+  def getBlocks(execIds: Seq[String]): Map[String, ExecutorReplicationState] = synchronized {\n+    val isThereEnoughMemory = checkFreeMem(execIds)\n+\n+    isThereEnoughMemory.map {\n+      case (execId, true) =>\n+        updateBlockState(execId)\n+        val blocksRemain = blocksToSave.get(execId).exists(_.nonEmpty)\n+        execId -> (if (blocksRemain) HasCachedBlocks else NoMoreBlocks)\n+      case (execId, false) =>\n+        execId -> NotEnoughMemory\n+    }\n+  }\n+\n+  /**\n+   * Checks to see if there is enough memory on the cluster to hold the cached blocks on these\n+   * executors. Checks on a per executor basis from the smallest to the largest.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executor ids to whether or not there is enough free memory for them.\n+   */\n+  private def checkFreeMem(execIds: Seq[String]): Map[String, Boolean] = {\n+    val execIdsToShutDown = execIds.toSet\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+\n+    val (expiringMemStatus, remainingMemStatus) = allExecMemStatus.partition {\n+      case (k, v) => execIdsToShutDown.contains(k)\n+    }\n+    val freeMemOnRemaining = remainingMemStatus.values.map(_._2).sum\n+\n+    val alreadyReplicatedBlocks: Map[String, Set[RDDBlockId]] =\n+      savedBlocks.filterKeys(execIdsToShutDown).map { case (k, v) => k -> v.toSet }.toMap\n+\n+    val getSizes = GetSizeOfBlocks(alreadyReplicatedBlocks)\n+    val alreadyReplicatedBytes = blockManagerMasterEndpoint.askSync[Map[String, Long]](getSizes)\n+\n+    val bytesToReplicate: Seq[(String, Long)] =\n+      expiringMemStatus.map { case (execId, (maxMem, remainingMem)) =>\n+        val alreadyReplicated = alreadyReplicatedBytes.getOrElse(execId, 0L)\n+        val usedMem = maxMem - remainingMem\n+        val toBeReplicatedMem = usedMem - alreadyReplicated\n+        execId -> toBeReplicatedMem\n+      }.toSeq.sortBy { case (k, v) => v }\n+\n+    bytesToReplicate.scan((\"start\", freeMemOnRemaining)) {\n+      case ((_, remainingMem), (execId, mem)) => (execId, remainingMem - mem)\n+    }.drop(1)\n+      .toMap\n+      .mapValues(freeMem => freeMem >= 0)\n+  }\n+\n+  /**\n+   * Gets a list of cached blocks from the block manager master and updates."
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "```\r\nsavedBlocks.get(execId).foreach { s => blocks --= s }\r\n```\r\n\r\nOr something.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-10-10T18:18:52Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+    case (executorId, NoMoreBlocks | NotEnoughMemory) => state.killExecutor(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    import scala.util.Success\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case Success(false) =>\n+        checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object CacheRecoveryManager {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): CacheRecoveryManager = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new CacheRecoveryManagerState(bmme, eam, conf)\n+    new CacheRecoveryManager(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ *\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class CacheRecoveryManagerState(\n+   blockManagerMasterEndpoint: RpcEndpointRef,\n+   executorAllocationManager: ExecutorAllocationManager,\n+   conf: SparkConf\n+ ) extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"recover-cache-shutdown-timers\")\n+\n+    private val blocksToSave = new mutable.HashMap[String, mutable.PriorityQueue[RDDBlockId]]\n+    private val savedBlocks = new mutable.HashMap[String, mutable.HashSet[RDDBlockId]]\n+    private val killTimers = new mutable.HashMap[String, ScheduledFuture[_]]\n+\n+  /**\n+   * Query Block Manager Master for cached blocks.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executorId to its replication state.\n+   */\n+  def getBlocks(execIds: Seq[String]): Map[String, ExecutorReplicationState] = synchronized {\n+    val isThereEnoughMemory = checkFreeMem(execIds)\n+\n+    isThereEnoughMemory.map {\n+      case (execId, true) =>\n+        updateBlockState(execId)\n+        val blocksRemain = blocksToSave.get(execId).exists(_.nonEmpty)\n+        execId -> (if (blocksRemain) HasCachedBlocks else NoMoreBlocks)\n+      case (execId, false) =>\n+        execId -> NotEnoughMemory\n+    }\n+  }\n+\n+  /**\n+   * Checks to see if there is enough memory on the cluster to hold the cached blocks on these\n+   * executors. Checks on a per executor basis from the smallest to the largest.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executor ids to whether or not there is enough free memory for them.\n+   */\n+  private def checkFreeMem(execIds: Seq[String]): Map[String, Boolean] = {\n+    val execIdsToShutDown = execIds.toSet\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+\n+    val (expiringMemStatus, remainingMemStatus) = allExecMemStatus.partition {\n+      case (k, v) => execIdsToShutDown.contains(k)\n+    }\n+    val freeMemOnRemaining = remainingMemStatus.values.map(_._2).sum\n+\n+    val alreadyReplicatedBlocks: Map[String, Set[RDDBlockId]] =\n+      savedBlocks.filterKeys(execIdsToShutDown).map { case (k, v) => k -> v.toSet }.toMap\n+\n+    val getSizes = GetSizeOfBlocks(alreadyReplicatedBlocks)\n+    val alreadyReplicatedBytes = blockManagerMasterEndpoint.askSync[Map[String, Long]](getSizes)\n+\n+    val bytesToReplicate: Seq[(String, Long)] =\n+      expiringMemStatus.map { case (execId, (maxMem, remainingMem)) =>\n+        val alreadyReplicated = alreadyReplicatedBytes.getOrElse(execId, 0L)\n+        val usedMem = maxMem - remainingMem\n+        val toBeReplicatedMem = usedMem - alreadyReplicated\n+        execId -> toBeReplicatedMem\n+      }.toSeq.sortBy { case (k, v) => v }\n+\n+    bytesToReplicate.scan((\"start\", freeMemOnRemaining)) {\n+      case ((_, remainingMem), (execId, mem)) => (execId, remainingMem - mem)\n+    }.drop(1)\n+      .toMap\n+      .mapValues(freeMem => freeMem >= 0)\n+  }\n+\n+  /**\n+   * Gets a list of cached blocks from the block manager master and updates.\n+   * CacheRecoveryManagerState.\n+   *\n+   * @param execId an executor Id\n+   */\n+  private def updateBlockState(execId: String): Unit = {\n+    val blocks: mutable.Set[RDDBlockId] = blockManagerMasterEndpoint\n+      .askSync[collection.Set[BlockId]](GetCachedBlocks(execId))\n+      .flatMap(_.asRDDId)(collection.breakOut)\n+\n+    blocks --= savedBlocks.getOrElse(execId, new mutable.HashSet)"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "removed this method",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-05T17:45:28Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+    case (executorId, NoMoreBlocks | NotEnoughMemory) => state.killExecutor(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    import scala.util.Success\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case Success(false) =>\n+        checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object CacheRecoveryManager {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): CacheRecoveryManager = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new CacheRecoveryManagerState(bmme, eam, conf)\n+    new CacheRecoveryManager(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ *\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class CacheRecoveryManagerState(\n+   blockManagerMasterEndpoint: RpcEndpointRef,\n+   executorAllocationManager: ExecutorAllocationManager,\n+   conf: SparkConf\n+ ) extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"recover-cache-shutdown-timers\")\n+\n+    private val blocksToSave = new mutable.HashMap[String, mutable.PriorityQueue[RDDBlockId]]\n+    private val savedBlocks = new mutable.HashMap[String, mutable.HashSet[RDDBlockId]]\n+    private val killTimers = new mutable.HashMap[String, ScheduledFuture[_]]\n+\n+  /**\n+   * Query Block Manager Master for cached blocks.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executorId to its replication state.\n+   */\n+  def getBlocks(execIds: Seq[String]): Map[String, ExecutorReplicationState] = synchronized {\n+    val isThereEnoughMemory = checkFreeMem(execIds)\n+\n+    isThereEnoughMemory.map {\n+      case (execId, true) =>\n+        updateBlockState(execId)\n+        val blocksRemain = blocksToSave.get(execId).exists(_.nonEmpty)\n+        execId -> (if (blocksRemain) HasCachedBlocks else NoMoreBlocks)\n+      case (execId, false) =>\n+        execId -> NotEnoughMemory\n+    }\n+  }\n+\n+  /**\n+   * Checks to see if there is enough memory on the cluster to hold the cached blocks on these\n+   * executors. Checks on a per executor basis from the smallest to the largest.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executor ids to whether or not there is enough free memory for them.\n+   */\n+  private def checkFreeMem(execIds: Seq[String]): Map[String, Boolean] = {\n+    val execIdsToShutDown = execIds.toSet\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+\n+    val (expiringMemStatus, remainingMemStatus) = allExecMemStatus.partition {\n+      case (k, v) => execIdsToShutDown.contains(k)\n+    }\n+    val freeMemOnRemaining = remainingMemStatus.values.map(_._2).sum\n+\n+    val alreadyReplicatedBlocks: Map[String, Set[RDDBlockId]] =\n+      savedBlocks.filterKeys(execIdsToShutDown).map { case (k, v) => k -> v.toSet }.toMap\n+\n+    val getSizes = GetSizeOfBlocks(alreadyReplicatedBlocks)\n+    val alreadyReplicatedBytes = blockManagerMasterEndpoint.askSync[Map[String, Long]](getSizes)\n+\n+    val bytesToReplicate: Seq[(String, Long)] =\n+      expiringMemStatus.map { case (execId, (maxMem, remainingMem)) =>\n+        val alreadyReplicated = alreadyReplicatedBytes.getOrElse(execId, 0L)\n+        val usedMem = maxMem - remainingMem\n+        val toBeReplicatedMem = usedMem - alreadyReplicated\n+        execId -> toBeReplicatedMem\n+      }.toSeq.sortBy { case (k, v) => v }\n+\n+    bytesToReplicate.scan((\"start\", freeMemOnRemaining)) {\n+      case ((_, remainingMem), (execId, mem)) => (execId, remainingMem - mem)\n+    }.drop(1)\n+      .toMap\n+      .mapValues(freeMem => freeMem >= 0)\n+  }\n+\n+  /**\n+   * Gets a list of cached blocks from the block manager master and updates.\n+   * CacheRecoveryManagerState.\n+   *\n+   * @param execId an executor Id\n+   */\n+  private def updateBlockState(execId: String): Unit = {\n+    val blocks: mutable.Set[RDDBlockId] = blockManagerMasterEndpoint\n+      .askSync[collection.Set[BlockId]](GetCachedBlocks(execId))\n+      .flatMap(_.asRDDId)(collection.breakOut)\n+\n+    blocks --= savedBlocks.getOrElse(execId, new mutable.HashSet)"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`: Unit =`",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-10-10T18:20:51Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+    case (executorId, NoMoreBlocks | NotEnoughMemory) => state.killExecutor(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    import scala.util.Success\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case Success(false) =>\n+        checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object CacheRecoveryManager {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): CacheRecoveryManager = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new CacheRecoveryManagerState(bmme, eam, conf)\n+    new CacheRecoveryManager(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ *\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class CacheRecoveryManagerState(\n+   blockManagerMasterEndpoint: RpcEndpointRef,\n+   executorAllocationManager: ExecutorAllocationManager,\n+   conf: SparkConf\n+ ) extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"recover-cache-shutdown-timers\")\n+\n+    private val blocksToSave = new mutable.HashMap[String, mutable.PriorityQueue[RDDBlockId]]\n+    private val savedBlocks = new mutable.HashMap[String, mutable.HashSet[RDDBlockId]]\n+    private val killTimers = new mutable.HashMap[String, ScheduledFuture[_]]\n+\n+  /**\n+   * Query Block Manager Master for cached blocks.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executorId to its replication state.\n+   */\n+  def getBlocks(execIds: Seq[String]): Map[String, ExecutorReplicationState] = synchronized {\n+    val isThereEnoughMemory = checkFreeMem(execIds)\n+\n+    isThereEnoughMemory.map {\n+      case (execId, true) =>\n+        updateBlockState(execId)\n+        val blocksRemain = blocksToSave.get(execId).exists(_.nonEmpty)\n+        execId -> (if (blocksRemain) HasCachedBlocks else NoMoreBlocks)\n+      case (execId, false) =>\n+        execId -> NotEnoughMemory\n+    }\n+  }\n+\n+  /**\n+   * Checks to see if there is enough memory on the cluster to hold the cached blocks on these\n+   * executors. Checks on a per executor basis from the smallest to the largest.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executor ids to whether or not there is enough free memory for them.\n+   */\n+  private def checkFreeMem(execIds: Seq[String]): Map[String, Boolean] = {\n+    val execIdsToShutDown = execIds.toSet\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+\n+    val (expiringMemStatus, remainingMemStatus) = allExecMemStatus.partition {\n+      case (k, v) => execIdsToShutDown.contains(k)\n+    }\n+    val freeMemOnRemaining = remainingMemStatus.values.map(_._2).sum\n+\n+    val alreadyReplicatedBlocks: Map[String, Set[RDDBlockId]] =\n+      savedBlocks.filterKeys(execIdsToShutDown).map { case (k, v) => k -> v.toSet }.toMap\n+\n+    val getSizes = GetSizeOfBlocks(alreadyReplicatedBlocks)\n+    val alreadyReplicatedBytes = blockManagerMasterEndpoint.askSync[Map[String, Long]](getSizes)\n+\n+    val bytesToReplicate: Seq[(String, Long)] =\n+      expiringMemStatus.map { case (execId, (maxMem, remainingMem)) =>\n+        val alreadyReplicated = alreadyReplicatedBytes.getOrElse(execId, 0L)\n+        val usedMem = maxMem - remainingMem\n+        val toBeReplicatedMem = usedMem - alreadyReplicated\n+        execId -> toBeReplicatedMem\n+      }.toSeq.sortBy { case (k, v) => v }\n+\n+    bytesToReplicate.scan((\"start\", freeMemOnRemaining)) {\n+      case ((_, remainingMem), (execId, mem)) => (execId, remainingMem - mem)\n+    }.drop(1)\n+      .toMap\n+      .mapValues(freeMem => freeMem >= 0)\n+  }\n+\n+  /**\n+   * Gets a list of cached blocks from the block manager master and updates.\n+   * CacheRecoveryManagerState.\n+   *\n+   * @param execId an executor Id\n+   */\n+  private def updateBlockState(execId: String): Unit = {\n+    val blocks: mutable.Set[RDDBlockId] = blockManagerMasterEndpoint\n+      .askSync[collection.Set[BlockId]](GetCachedBlocks(execId))\n+      .flatMap(_.asRDDId)(collection.breakOut)\n+\n+    blocks --= savedBlocks.getOrElse(execId, new mutable.HashSet)\n+    blocksToSave(execId) = mutable.PriorityQueue[RDDBlockId](blocks.toSeq: _*)(Ordering.by(_.rddId))\n+    killTimers.getOrElseUpdate(execId, scheduleKill(new KillRunner(execId)))\n+  }\n+\n+  class KillRunner(execId: String) extends Runnable { def run(): Unit = killExecutor(execId) }\n+  private def scheduleKill(k: KillRunner) = scheduler.schedule(k, forceKillAfterS, TimeUnit.SECONDS)"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "removed this class",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-05T17:46:09Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+    case (executorId, NoMoreBlocks | NotEnoughMemory) => state.killExecutor(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    import scala.util.Success\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case Success(false) =>\n+        checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object CacheRecoveryManager {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): CacheRecoveryManager = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new CacheRecoveryManagerState(bmme, eam, conf)\n+    new CacheRecoveryManager(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ *\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class CacheRecoveryManagerState(\n+   blockManagerMasterEndpoint: RpcEndpointRef,\n+   executorAllocationManager: ExecutorAllocationManager,\n+   conf: SparkConf\n+ ) extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"recover-cache-shutdown-timers\")\n+\n+    private val blocksToSave = new mutable.HashMap[String, mutable.PriorityQueue[RDDBlockId]]\n+    private val savedBlocks = new mutable.HashMap[String, mutable.HashSet[RDDBlockId]]\n+    private val killTimers = new mutable.HashMap[String, ScheduledFuture[_]]\n+\n+  /**\n+   * Query Block Manager Master for cached blocks.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executorId to its replication state.\n+   */\n+  def getBlocks(execIds: Seq[String]): Map[String, ExecutorReplicationState] = synchronized {\n+    val isThereEnoughMemory = checkFreeMem(execIds)\n+\n+    isThereEnoughMemory.map {\n+      case (execId, true) =>\n+        updateBlockState(execId)\n+        val blocksRemain = blocksToSave.get(execId).exists(_.nonEmpty)\n+        execId -> (if (blocksRemain) HasCachedBlocks else NoMoreBlocks)\n+      case (execId, false) =>\n+        execId -> NotEnoughMemory\n+    }\n+  }\n+\n+  /**\n+   * Checks to see if there is enough memory on the cluster to hold the cached blocks on these\n+   * executors. Checks on a per executor basis from the smallest to the largest.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executor ids to whether or not there is enough free memory for them.\n+   */\n+  private def checkFreeMem(execIds: Seq[String]): Map[String, Boolean] = {\n+    val execIdsToShutDown = execIds.toSet\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+\n+    val (expiringMemStatus, remainingMemStatus) = allExecMemStatus.partition {\n+      case (k, v) => execIdsToShutDown.contains(k)\n+    }\n+    val freeMemOnRemaining = remainingMemStatus.values.map(_._2).sum\n+\n+    val alreadyReplicatedBlocks: Map[String, Set[RDDBlockId]] =\n+      savedBlocks.filterKeys(execIdsToShutDown).map { case (k, v) => k -> v.toSet }.toMap\n+\n+    val getSizes = GetSizeOfBlocks(alreadyReplicatedBlocks)\n+    val alreadyReplicatedBytes = blockManagerMasterEndpoint.askSync[Map[String, Long]](getSizes)\n+\n+    val bytesToReplicate: Seq[(String, Long)] =\n+      expiringMemStatus.map { case (execId, (maxMem, remainingMem)) =>\n+        val alreadyReplicated = alreadyReplicatedBytes.getOrElse(execId, 0L)\n+        val usedMem = maxMem - remainingMem\n+        val toBeReplicatedMem = usedMem - alreadyReplicated\n+        execId -> toBeReplicatedMem\n+      }.toSeq.sortBy { case (k, v) => v }\n+\n+    bytesToReplicate.scan((\"start\", freeMemOnRemaining)) {\n+      case ((_, remainingMem), (execId, mem)) => (execId, remainingMem - mem)\n+    }.drop(1)\n+      .toMap\n+      .mapValues(freeMem => freeMem >= 0)\n+  }\n+\n+  /**\n+   * Gets a list of cached blocks from the block manager master and updates.\n+   * CacheRecoveryManagerState.\n+   *\n+   * @param execId an executor Id\n+   */\n+  private def updateBlockState(execId: String): Unit = {\n+    val blocks: mutable.Set[RDDBlockId] = blockManagerMasterEndpoint\n+      .askSync[collection.Set[BlockId]](GetCachedBlocks(execId))\n+      .flatMap(_.asRDDId)(collection.breakOut)\n+\n+    blocks --= savedBlocks.getOrElse(execId, new mutable.HashSet)\n+    blocksToSave(execId) = mutable.PriorityQueue[RDDBlockId](blocks.toSeq: _*)(Ordering.by(_.rddId))\n+    killTimers.getOrElseUpdate(execId, scheduleKill(new KillRunner(execId)))\n+  }\n+\n+  class KillRunner(execId: String) extends Runnable { def run(): Unit = killExecutor(execId) }\n+  private def scheduleKill(k: KillRunner) = scheduler.schedule(k, forceKillAfterS, TimeUnit.SECONDS)"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "This does not return a boolean.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-10-10T18:21:51Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+    case (executorId, NoMoreBlocks | NotEnoughMemory) => state.killExecutor(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    import scala.util.Success\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case Success(false) =>\n+        checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object CacheRecoveryManager {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): CacheRecoveryManager = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new CacheRecoveryManagerState(bmme, eam, conf)\n+    new CacheRecoveryManager(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ *\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class CacheRecoveryManagerState(\n+   blockManagerMasterEndpoint: RpcEndpointRef,\n+   executorAllocationManager: ExecutorAllocationManager,\n+   conf: SparkConf\n+ ) extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"recover-cache-shutdown-timers\")\n+\n+    private val blocksToSave = new mutable.HashMap[String, mutable.PriorityQueue[RDDBlockId]]\n+    private val savedBlocks = new mutable.HashMap[String, mutable.HashSet[RDDBlockId]]\n+    private val killTimers = new mutable.HashMap[String, ScheduledFuture[_]]\n+\n+  /**\n+   * Query Block Manager Master for cached blocks.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executorId to its replication state.\n+   */\n+  def getBlocks(execIds: Seq[String]): Map[String, ExecutorReplicationState] = synchronized {\n+    val isThereEnoughMemory = checkFreeMem(execIds)\n+\n+    isThereEnoughMemory.map {\n+      case (execId, true) =>\n+        updateBlockState(execId)\n+        val blocksRemain = blocksToSave.get(execId).exists(_.nonEmpty)\n+        execId -> (if (blocksRemain) HasCachedBlocks else NoMoreBlocks)\n+      case (execId, false) =>\n+        execId -> NotEnoughMemory\n+    }\n+  }\n+\n+  /**\n+   * Checks to see if there is enough memory on the cluster to hold the cached blocks on these\n+   * executors. Checks on a per executor basis from the smallest to the largest.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executor ids to whether or not there is enough free memory for them.\n+   */\n+  private def checkFreeMem(execIds: Seq[String]): Map[String, Boolean] = {\n+    val execIdsToShutDown = execIds.toSet\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+\n+    val (expiringMemStatus, remainingMemStatus) = allExecMemStatus.partition {\n+      case (k, v) => execIdsToShutDown.contains(k)\n+    }\n+    val freeMemOnRemaining = remainingMemStatus.values.map(_._2).sum\n+\n+    val alreadyReplicatedBlocks: Map[String, Set[RDDBlockId]] =\n+      savedBlocks.filterKeys(execIdsToShutDown).map { case (k, v) => k -> v.toSet }.toMap\n+\n+    val getSizes = GetSizeOfBlocks(alreadyReplicatedBlocks)\n+    val alreadyReplicatedBytes = blockManagerMasterEndpoint.askSync[Map[String, Long]](getSizes)\n+\n+    val bytesToReplicate: Seq[(String, Long)] =\n+      expiringMemStatus.map { case (execId, (maxMem, remainingMem)) =>\n+        val alreadyReplicated = alreadyReplicatedBytes.getOrElse(execId, 0L)\n+        val usedMem = maxMem - remainingMem\n+        val toBeReplicatedMem = usedMem - alreadyReplicated\n+        execId -> toBeReplicatedMem\n+      }.toSeq.sortBy { case (k, v) => v }\n+\n+    bytesToReplicate.scan((\"start\", freeMemOnRemaining)) {\n+      case ((_, remainingMem), (execId, mem)) => (execId, remainingMem - mem)\n+    }.drop(1)\n+      .toMap\n+      .mapValues(freeMem => freeMem >= 0)\n+  }\n+\n+  /**\n+   * Gets a list of cached blocks from the block manager master and updates.\n+   * CacheRecoveryManagerState.\n+   *\n+   * @param execId an executor Id\n+   */\n+  private def updateBlockState(execId: String): Unit = {\n+    val blocks: mutable.Set[RDDBlockId] = blockManagerMasterEndpoint\n+      .askSync[collection.Set[BlockId]](GetCachedBlocks(execId))\n+      .flatMap(_.asRDDId)(collection.breakOut)\n+\n+    blocks --= savedBlocks.getOrElse(execId, new mutable.HashSet)\n+    blocksToSave(execId) = mutable.PriorityQueue[RDDBlockId](blocks.toSeq: _*)(Ordering.by(_.rddId))\n+    killTimers.getOrElseUpdate(execId, scheduleKill(new KillRunner(execId)))\n+  }\n+\n+  class KillRunner(execId: String) extends Runnable { def run(): Unit = killExecutor(execId) }\n+  private def scheduleKill(k: KillRunner) = scheduler.schedule(k, forceKillAfterS, TimeUnit.SECONDS)\n+\n+  /**\n+   * Ask block manager master to replicate one cached block.\n+   *\n+   * @param execId the executor to replicate a block from\n+   * @return false if there are no more blocks or if there is an issue"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "fixed",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-05T17:53:23Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+    case (executorId, NoMoreBlocks | NotEnoughMemory) => state.killExecutor(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    import scala.util.Success\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case Success(false) =>\n+        checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object CacheRecoveryManager {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): CacheRecoveryManager = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new CacheRecoveryManagerState(bmme, eam, conf)\n+    new CacheRecoveryManager(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ *\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class CacheRecoveryManagerState(\n+   blockManagerMasterEndpoint: RpcEndpointRef,\n+   executorAllocationManager: ExecutorAllocationManager,\n+   conf: SparkConf\n+ ) extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"recover-cache-shutdown-timers\")\n+\n+    private val blocksToSave = new mutable.HashMap[String, mutable.PriorityQueue[RDDBlockId]]\n+    private val savedBlocks = new mutable.HashMap[String, mutable.HashSet[RDDBlockId]]\n+    private val killTimers = new mutable.HashMap[String, ScheduledFuture[_]]\n+\n+  /**\n+   * Query Block Manager Master for cached blocks.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executorId to its replication state.\n+   */\n+  def getBlocks(execIds: Seq[String]): Map[String, ExecutorReplicationState] = synchronized {\n+    val isThereEnoughMemory = checkFreeMem(execIds)\n+\n+    isThereEnoughMemory.map {\n+      case (execId, true) =>\n+        updateBlockState(execId)\n+        val blocksRemain = blocksToSave.get(execId).exists(_.nonEmpty)\n+        execId -> (if (blocksRemain) HasCachedBlocks else NoMoreBlocks)\n+      case (execId, false) =>\n+        execId -> NotEnoughMemory\n+    }\n+  }\n+\n+  /**\n+   * Checks to see if there is enough memory on the cluster to hold the cached blocks on these\n+   * executors. Checks on a per executor basis from the smallest to the largest.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executor ids to whether or not there is enough free memory for them.\n+   */\n+  private def checkFreeMem(execIds: Seq[String]): Map[String, Boolean] = {\n+    val execIdsToShutDown = execIds.toSet\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+\n+    val (expiringMemStatus, remainingMemStatus) = allExecMemStatus.partition {\n+      case (k, v) => execIdsToShutDown.contains(k)\n+    }\n+    val freeMemOnRemaining = remainingMemStatus.values.map(_._2).sum\n+\n+    val alreadyReplicatedBlocks: Map[String, Set[RDDBlockId]] =\n+      savedBlocks.filterKeys(execIdsToShutDown).map { case (k, v) => k -> v.toSet }.toMap\n+\n+    val getSizes = GetSizeOfBlocks(alreadyReplicatedBlocks)\n+    val alreadyReplicatedBytes = blockManagerMasterEndpoint.askSync[Map[String, Long]](getSizes)\n+\n+    val bytesToReplicate: Seq[(String, Long)] =\n+      expiringMemStatus.map { case (execId, (maxMem, remainingMem)) =>\n+        val alreadyReplicated = alreadyReplicatedBytes.getOrElse(execId, 0L)\n+        val usedMem = maxMem - remainingMem\n+        val toBeReplicatedMem = usedMem - alreadyReplicated\n+        execId -> toBeReplicatedMem\n+      }.toSeq.sortBy { case (k, v) => v }\n+\n+    bytesToReplicate.scan((\"start\", freeMemOnRemaining)) {\n+      case ((_, remainingMem), (execId, mem)) => (execId, remainingMem - mem)\n+    }.drop(1)\n+      .toMap\n+      .mapValues(freeMem => freeMem >= 0)\n+  }\n+\n+  /**\n+   * Gets a list of cached blocks from the block manager master and updates.\n+   * CacheRecoveryManagerState.\n+   *\n+   * @param execId an executor Id\n+   */\n+  private def updateBlockState(execId: String): Unit = {\n+    val blocks: mutable.Set[RDDBlockId] = blockManagerMasterEndpoint\n+      .askSync[collection.Set[BlockId]](GetCachedBlocks(execId))\n+      .flatMap(_.asRDDId)(collection.breakOut)\n+\n+    blocks --= savedBlocks.getOrElse(execId, new mutable.HashSet)\n+    blocksToSave(execId) = mutable.PriorityQueue[RDDBlockId](blocks.toSeq: _*)(Ordering.by(_.rddId))\n+    killTimers.getOrElseUpdate(execId, scheduleKill(new KillRunner(execId)))\n+  }\n+\n+  class KillRunner(execId: String) extends Runnable { def run(): Unit = killExecutor(execId) }\n+  private def scheduleKill(k: KillRunner) = scheduler.schedule(k, forceKillAfterS, TimeUnit.SECONDS)\n+\n+  /**\n+   * Ask block manager master to replicate one cached block.\n+   *\n+   * @param execId the executor to replicate a block from\n+   * @return false if there are no more blocks or if there is an issue"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "This method is only called in one place. It may be simpler to just inline it there so you don't have to explain this odd return type.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-10-10T18:25:01Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+    case (executorId, NoMoreBlocks | NotEnoughMemory) => state.killExecutor(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    import scala.util.Success\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case Success(false) =>\n+        checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object CacheRecoveryManager {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): CacheRecoveryManager = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new CacheRecoveryManagerState(bmme, eam, conf)\n+    new CacheRecoveryManager(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ *\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class CacheRecoveryManagerState(\n+   blockManagerMasterEndpoint: RpcEndpointRef,\n+   executorAllocationManager: ExecutorAllocationManager,\n+   conf: SparkConf\n+ ) extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"recover-cache-shutdown-timers\")\n+\n+    private val blocksToSave = new mutable.HashMap[String, mutable.PriorityQueue[RDDBlockId]]\n+    private val savedBlocks = new mutable.HashMap[String, mutable.HashSet[RDDBlockId]]\n+    private val killTimers = new mutable.HashMap[String, ScheduledFuture[_]]\n+\n+  /**\n+   * Query Block Manager Master for cached blocks.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executorId to its replication state.\n+   */\n+  def getBlocks(execIds: Seq[String]): Map[String, ExecutorReplicationState] = synchronized {\n+    val isThereEnoughMemory = checkFreeMem(execIds)\n+\n+    isThereEnoughMemory.map {\n+      case (execId, true) =>\n+        updateBlockState(execId)\n+        val blocksRemain = blocksToSave.get(execId).exists(_.nonEmpty)\n+        execId -> (if (blocksRemain) HasCachedBlocks else NoMoreBlocks)\n+      case (execId, false) =>\n+        execId -> NotEnoughMemory\n+    }\n+  }\n+\n+  /**\n+   * Checks to see if there is enough memory on the cluster to hold the cached blocks on these\n+   * executors. Checks on a per executor basis from the smallest to the largest.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executor ids to whether or not there is enough free memory for them.\n+   */\n+  private def checkFreeMem(execIds: Seq[String]): Map[String, Boolean] = {\n+    val execIdsToShutDown = execIds.toSet\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+\n+    val (expiringMemStatus, remainingMemStatus) = allExecMemStatus.partition {\n+      case (k, v) => execIdsToShutDown.contains(k)\n+    }\n+    val freeMemOnRemaining = remainingMemStatus.values.map(_._2).sum\n+\n+    val alreadyReplicatedBlocks: Map[String, Set[RDDBlockId]] =\n+      savedBlocks.filterKeys(execIdsToShutDown).map { case (k, v) => k -> v.toSet }.toMap\n+\n+    val getSizes = GetSizeOfBlocks(alreadyReplicatedBlocks)\n+    val alreadyReplicatedBytes = blockManagerMasterEndpoint.askSync[Map[String, Long]](getSizes)\n+\n+    val bytesToReplicate: Seq[(String, Long)] =\n+      expiringMemStatus.map { case (execId, (maxMem, remainingMem)) =>\n+        val alreadyReplicated = alreadyReplicatedBytes.getOrElse(execId, 0L)\n+        val usedMem = maxMem - remainingMem\n+        val toBeReplicatedMem = usedMem - alreadyReplicated\n+        execId -> toBeReplicatedMem\n+      }.toSeq.sortBy { case (k, v) => v }\n+\n+    bytesToReplicate.scan((\"start\", freeMemOnRemaining)) {\n+      case ((_, remainingMem), (execId, mem)) => (execId, remainingMem - mem)\n+    }.drop(1)\n+      .toMap\n+      .mapValues(freeMem => freeMem >= 0)\n+  }\n+\n+  /**\n+   * Gets a list of cached blocks from the block manager master and updates.\n+   * CacheRecoveryManagerState.\n+   *\n+   * @param execId an executor Id\n+   */\n+  private def updateBlockState(execId: String): Unit = {\n+    val blocks: mutable.Set[RDDBlockId] = blockManagerMasterEndpoint\n+      .askSync[collection.Set[BlockId]](GetCachedBlocks(execId))\n+      .flatMap(_.asRDDId)(collection.breakOut)\n+\n+    blocks --= savedBlocks.getOrElse(execId, new mutable.HashSet)\n+    blocksToSave(execId) = mutable.PriorityQueue[RDDBlockId](blocks.toSeq: _*)(Ordering.by(_.rddId))\n+    killTimers.getOrElseUpdate(execId, scheduleKill(new KillRunner(execId)))\n+  }\n+\n+  class KillRunner(execId: String) extends Runnable { def run(): Unit = killExecutor(execId) }\n+  private def scheduleKill(k: KillRunner) = scheduler.schedule(k, forceKillAfterS, TimeUnit.SECONDS)\n+\n+  /**\n+   * Ask block manager master to replicate one cached block.\n+   *\n+   * @param execId the executor to replicate a block from\n+   * @return false if there are no more blocks or if there is an issue\n+   */\n+  def replicateFirstBlock(execId: String): (Future[Boolean], Option[RDDBlockId]) = synchronized {"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "this return type is simpler now.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-05T17:56:38Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+    case (executorId, NoMoreBlocks | NotEnoughMemory) => state.killExecutor(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    import scala.util.Success\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case Success(false) =>\n+        checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object CacheRecoveryManager {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): CacheRecoveryManager = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new CacheRecoveryManagerState(bmme, eam, conf)\n+    new CacheRecoveryManager(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ *\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class CacheRecoveryManagerState(\n+   blockManagerMasterEndpoint: RpcEndpointRef,\n+   executorAllocationManager: ExecutorAllocationManager,\n+   conf: SparkConf\n+ ) extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"recover-cache-shutdown-timers\")\n+\n+    private val blocksToSave = new mutable.HashMap[String, mutable.PriorityQueue[RDDBlockId]]\n+    private val savedBlocks = new mutable.HashMap[String, mutable.HashSet[RDDBlockId]]\n+    private val killTimers = new mutable.HashMap[String, ScheduledFuture[_]]\n+\n+  /**\n+   * Query Block Manager Master for cached blocks.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executorId to its replication state.\n+   */\n+  def getBlocks(execIds: Seq[String]): Map[String, ExecutorReplicationState] = synchronized {\n+    val isThereEnoughMemory = checkFreeMem(execIds)\n+\n+    isThereEnoughMemory.map {\n+      case (execId, true) =>\n+        updateBlockState(execId)\n+        val blocksRemain = blocksToSave.get(execId).exists(_.nonEmpty)\n+        execId -> (if (blocksRemain) HasCachedBlocks else NoMoreBlocks)\n+      case (execId, false) =>\n+        execId -> NotEnoughMemory\n+    }\n+  }\n+\n+  /**\n+   * Checks to see if there is enough memory on the cluster to hold the cached blocks on these\n+   * executors. Checks on a per executor basis from the smallest to the largest.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executor ids to whether or not there is enough free memory for them.\n+   */\n+  private def checkFreeMem(execIds: Seq[String]): Map[String, Boolean] = {\n+    val execIdsToShutDown = execIds.toSet\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+\n+    val (expiringMemStatus, remainingMemStatus) = allExecMemStatus.partition {\n+      case (k, v) => execIdsToShutDown.contains(k)\n+    }\n+    val freeMemOnRemaining = remainingMemStatus.values.map(_._2).sum\n+\n+    val alreadyReplicatedBlocks: Map[String, Set[RDDBlockId]] =\n+      savedBlocks.filterKeys(execIdsToShutDown).map { case (k, v) => k -> v.toSet }.toMap\n+\n+    val getSizes = GetSizeOfBlocks(alreadyReplicatedBlocks)\n+    val alreadyReplicatedBytes = blockManagerMasterEndpoint.askSync[Map[String, Long]](getSizes)\n+\n+    val bytesToReplicate: Seq[(String, Long)] =\n+      expiringMemStatus.map { case (execId, (maxMem, remainingMem)) =>\n+        val alreadyReplicated = alreadyReplicatedBytes.getOrElse(execId, 0L)\n+        val usedMem = maxMem - remainingMem\n+        val toBeReplicatedMem = usedMem - alreadyReplicated\n+        execId -> toBeReplicatedMem\n+      }.toSeq.sortBy { case (k, v) => v }\n+\n+    bytesToReplicate.scan((\"start\", freeMemOnRemaining)) {\n+      case ((_, remainingMem), (execId, mem)) => (execId, remainingMem - mem)\n+    }.drop(1)\n+      .toMap\n+      .mapValues(freeMem => freeMem >= 0)\n+  }\n+\n+  /**\n+   * Gets a list of cached blocks from the block manager master and updates.\n+   * CacheRecoveryManagerState.\n+   *\n+   * @param execId an executor Id\n+   */\n+  private def updateBlockState(execId: String): Unit = {\n+    val blocks: mutable.Set[RDDBlockId] = blockManagerMasterEndpoint\n+      .askSync[collection.Set[BlockId]](GetCachedBlocks(execId))\n+      .flatMap(_.asRDDId)(collection.breakOut)\n+\n+    blocks --= savedBlocks.getOrElse(execId, new mutable.HashSet)\n+    blocksToSave(execId) = mutable.PriorityQueue[RDDBlockId](blocks.toSeq: _*)(Ordering.by(_.rddId))\n+    killTimers.getOrElseUpdate(execId, scheduleKill(new KillRunner(execId)))\n+  }\n+\n+  class KillRunner(execId: String) extends Runnable { def run(): Unit = killExecutor(execId) }\n+  private def scheduleKill(k: KillRunner) = scheduler.schedule(k, forceKillAfterS, TimeUnit.SECONDS)\n+\n+  /**\n+   * Ask block manager master to replicate one cached block.\n+   *\n+   * @param execId the executor to replicate a block from\n+   * @return false if there are no more blocks or if there is an issue\n+   */\n+  def replicateFirstBlock(execId: String): (Future[Boolean], Option[RDDBlockId]) = synchronized {"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "It's a little weird to just return one of the input parameters. Might be easy to simplify this at the call site.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-10-10T18:27:48Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+    case (executorId, NoMoreBlocks | NotEnoughMemory) => state.killExecutor(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    import scala.util.Success\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case Success(false) =>\n+        checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object CacheRecoveryManager {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): CacheRecoveryManager = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new CacheRecoveryManagerState(bmme, eam, conf)\n+    new CacheRecoveryManager(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ *\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class CacheRecoveryManagerState(\n+   blockManagerMasterEndpoint: RpcEndpointRef,\n+   executorAllocationManager: ExecutorAllocationManager,\n+   conf: SparkConf\n+ ) extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"recover-cache-shutdown-timers\")\n+\n+    private val blocksToSave = new mutable.HashMap[String, mutable.PriorityQueue[RDDBlockId]]\n+    private val savedBlocks = new mutable.HashMap[String, mutable.HashSet[RDDBlockId]]\n+    private val killTimers = new mutable.HashMap[String, ScheduledFuture[_]]\n+\n+  /**\n+   * Query Block Manager Master for cached blocks.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executorId to its replication state.\n+   */\n+  def getBlocks(execIds: Seq[String]): Map[String, ExecutorReplicationState] = synchronized {\n+    val isThereEnoughMemory = checkFreeMem(execIds)\n+\n+    isThereEnoughMemory.map {\n+      case (execId, true) =>\n+        updateBlockState(execId)\n+        val blocksRemain = blocksToSave.get(execId).exists(_.nonEmpty)\n+        execId -> (if (blocksRemain) HasCachedBlocks else NoMoreBlocks)\n+      case (execId, false) =>\n+        execId -> NotEnoughMemory\n+    }\n+  }\n+\n+  /**\n+   * Checks to see if there is enough memory on the cluster to hold the cached blocks on these\n+   * executors. Checks on a per executor basis from the smallest to the largest.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executor ids to whether or not there is enough free memory for them.\n+   */\n+  private def checkFreeMem(execIds: Seq[String]): Map[String, Boolean] = {\n+    val execIdsToShutDown = execIds.toSet\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+\n+    val (expiringMemStatus, remainingMemStatus) = allExecMemStatus.partition {\n+      case (k, v) => execIdsToShutDown.contains(k)\n+    }\n+    val freeMemOnRemaining = remainingMemStatus.values.map(_._2).sum\n+\n+    val alreadyReplicatedBlocks: Map[String, Set[RDDBlockId]] =\n+      savedBlocks.filterKeys(execIdsToShutDown).map { case (k, v) => k -> v.toSet }.toMap\n+\n+    val getSizes = GetSizeOfBlocks(alreadyReplicatedBlocks)\n+    val alreadyReplicatedBytes = blockManagerMasterEndpoint.askSync[Map[String, Long]](getSizes)\n+\n+    val bytesToReplicate: Seq[(String, Long)] =\n+      expiringMemStatus.map { case (execId, (maxMem, remainingMem)) =>\n+        val alreadyReplicated = alreadyReplicatedBytes.getOrElse(execId, 0L)\n+        val usedMem = maxMem - remainingMem\n+        val toBeReplicatedMem = usedMem - alreadyReplicated\n+        execId -> toBeReplicatedMem\n+      }.toSeq.sortBy { case (k, v) => v }\n+\n+    bytesToReplicate.scan((\"start\", freeMemOnRemaining)) {\n+      case ((_, remainingMem), (execId, mem)) => (execId, remainingMem - mem)\n+    }.drop(1)\n+      .toMap\n+      .mapValues(freeMem => freeMem >= 0)\n+  }\n+\n+  /**\n+   * Gets a list of cached blocks from the block manager master and updates.\n+   * CacheRecoveryManagerState.\n+   *\n+   * @param execId an executor Id\n+   */\n+  private def updateBlockState(execId: String): Unit = {\n+    val blocks: mutable.Set[RDDBlockId] = blockManagerMasterEndpoint\n+      .askSync[collection.Set[BlockId]](GetCachedBlocks(execId))\n+      .flatMap(_.asRDDId)(collection.breakOut)\n+\n+    blocks --= savedBlocks.getOrElse(execId, new mutable.HashSet)\n+    blocksToSave(execId) = mutable.PriorityQueue[RDDBlockId](blocks.toSeq: _*)(Ordering.by(_.rddId))\n+    killTimers.getOrElseUpdate(execId, scheduleKill(new KillRunner(execId)))\n+  }\n+\n+  class KillRunner(execId: String) extends Runnable { def run(): Unit = killExecutor(execId) }\n+  private def scheduleKill(k: KillRunner) = scheduler.schedule(k, forceKillAfterS, TimeUnit.SECONDS)\n+\n+  /**\n+   * Ask block manager master to replicate one cached block.\n+   *\n+   * @param execId the executor to replicate a block from\n+   * @return false if there are no more blocks or if there is an issue\n+   */\n+  def replicateFirstBlock(execId: String): (Future[Boolean], Option[RDDBlockId]) = synchronized {\n+    logDebug(s\"Replicate block on executor $execId.\")\n+    blocksToSave.get(execId) match {\n+      case Some(p) if p.nonEmpty => doReplication(execId, p.dequeue())\n+      case _ => (Future.successful(false), None)\n+    }\n+  }\n+\n+  private def doReplication(\n+      execId: String,\n+      blockId: RDDBlockId): (Future[Boolean], Option[RDDBlockId]) = {\n+    val savedBlocksForExecutor = savedBlocks.getOrElseUpdate(execId, new mutable.HashSet)\n+    savedBlocksForExecutor += blockId\n+    val replicateMessage = ReplicateOneBlock(execId, blockId, blocksToSave.keys.toSeq)\n+    logTrace(s\"Started replicating block $blockId on exec $execId.\")\n+    val future = blockManagerMasterEndpoint.ask[Boolean](replicateMessage)\n+    (future, Some(blockId))"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "this is gone",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-05T18:07:37Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+    case (executorId, NoMoreBlocks | NotEnoughMemory) => state.killExecutor(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    import scala.util.Success\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case Success(false) =>\n+        checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object CacheRecoveryManager {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): CacheRecoveryManager = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new CacheRecoveryManagerState(bmme, eam, conf)\n+    new CacheRecoveryManager(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ *\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class CacheRecoveryManagerState(\n+   blockManagerMasterEndpoint: RpcEndpointRef,\n+   executorAllocationManager: ExecutorAllocationManager,\n+   conf: SparkConf\n+ ) extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"recover-cache-shutdown-timers\")\n+\n+    private val blocksToSave = new mutable.HashMap[String, mutable.PriorityQueue[RDDBlockId]]\n+    private val savedBlocks = new mutable.HashMap[String, mutable.HashSet[RDDBlockId]]\n+    private val killTimers = new mutable.HashMap[String, ScheduledFuture[_]]\n+\n+  /**\n+   * Query Block Manager Master for cached blocks.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executorId to its replication state.\n+   */\n+  def getBlocks(execIds: Seq[String]): Map[String, ExecutorReplicationState] = synchronized {\n+    val isThereEnoughMemory = checkFreeMem(execIds)\n+\n+    isThereEnoughMemory.map {\n+      case (execId, true) =>\n+        updateBlockState(execId)\n+        val blocksRemain = blocksToSave.get(execId).exists(_.nonEmpty)\n+        execId -> (if (blocksRemain) HasCachedBlocks else NoMoreBlocks)\n+      case (execId, false) =>\n+        execId -> NotEnoughMemory\n+    }\n+  }\n+\n+  /**\n+   * Checks to see if there is enough memory on the cluster to hold the cached blocks on these\n+   * executors. Checks on a per executor basis from the smallest to the largest.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executor ids to whether or not there is enough free memory for them.\n+   */\n+  private def checkFreeMem(execIds: Seq[String]): Map[String, Boolean] = {\n+    val execIdsToShutDown = execIds.toSet\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+\n+    val (expiringMemStatus, remainingMemStatus) = allExecMemStatus.partition {\n+      case (k, v) => execIdsToShutDown.contains(k)\n+    }\n+    val freeMemOnRemaining = remainingMemStatus.values.map(_._2).sum\n+\n+    val alreadyReplicatedBlocks: Map[String, Set[RDDBlockId]] =\n+      savedBlocks.filterKeys(execIdsToShutDown).map { case (k, v) => k -> v.toSet }.toMap\n+\n+    val getSizes = GetSizeOfBlocks(alreadyReplicatedBlocks)\n+    val alreadyReplicatedBytes = blockManagerMasterEndpoint.askSync[Map[String, Long]](getSizes)\n+\n+    val bytesToReplicate: Seq[(String, Long)] =\n+      expiringMemStatus.map { case (execId, (maxMem, remainingMem)) =>\n+        val alreadyReplicated = alreadyReplicatedBytes.getOrElse(execId, 0L)\n+        val usedMem = maxMem - remainingMem\n+        val toBeReplicatedMem = usedMem - alreadyReplicated\n+        execId -> toBeReplicatedMem\n+      }.toSeq.sortBy { case (k, v) => v }\n+\n+    bytesToReplicate.scan((\"start\", freeMemOnRemaining)) {\n+      case ((_, remainingMem), (execId, mem)) => (execId, remainingMem - mem)\n+    }.drop(1)\n+      .toMap\n+      .mapValues(freeMem => freeMem >= 0)\n+  }\n+\n+  /**\n+   * Gets a list of cached blocks from the block manager master and updates.\n+   * CacheRecoveryManagerState.\n+   *\n+   * @param execId an executor Id\n+   */\n+  private def updateBlockState(execId: String): Unit = {\n+    val blocks: mutable.Set[RDDBlockId] = blockManagerMasterEndpoint\n+      .askSync[collection.Set[BlockId]](GetCachedBlocks(execId))\n+      .flatMap(_.asRDDId)(collection.breakOut)\n+\n+    blocks --= savedBlocks.getOrElse(execId, new mutable.HashSet)\n+    blocksToSave(execId) = mutable.PriorityQueue[RDDBlockId](blocks.toSeq: _*)(Ordering.by(_.rddId))\n+    killTimers.getOrElseUpdate(execId, scheduleKill(new KillRunner(execId)))\n+  }\n+\n+  class KillRunner(execId: String) extends Runnable { def run(): Unit = killExecutor(execId) }\n+  private def scheduleKill(k: KillRunner) = scheduler.schedule(k, forceKillAfterS, TimeUnit.SECONDS)\n+\n+  /**\n+   * Ask block manager master to replicate one cached block.\n+   *\n+   * @param execId the executor to replicate a block from\n+   * @return false if there are no more blocks or if there is an issue\n+   */\n+  def replicateFirstBlock(execId: String): (Future[Boolean], Option[RDDBlockId]) = synchronized {\n+    logDebug(s\"Replicate block on executor $execId.\")\n+    blocksToSave.get(execId) match {\n+      case Some(p) if p.nonEmpty => doReplication(execId, p.dequeue())\n+      case _ => (Future.successful(false), None)\n+    }\n+  }\n+\n+  private def doReplication(\n+      execId: String,\n+      blockId: RDDBlockId): (Future[Boolean], Option[RDDBlockId]) = {\n+    val savedBlocksForExecutor = savedBlocks.getOrElseUpdate(execId, new mutable.HashSet)\n+    savedBlocksForExecutor += blockId\n+    val replicateMessage = ReplicateOneBlock(execId, blockId, blocksToSave.keys.toSeq)\n+    logTrace(s\"Started replicating block $blockId on exec $execId.\")\n+    val future = blockManagerMasterEndpoint.ask[Boolean](replicateMessage)\n+    (future, Some(blockId))"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: empty line before param list",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-10-10T18:28:08Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+    case (executorId, NoMoreBlocks | NotEnoughMemory) => state.killExecutor(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    import scala.util.Success\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case Success(false) =>\n+        checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object CacheRecoveryManager {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): CacheRecoveryManager = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new CacheRecoveryManagerState(bmme, eam, conf)\n+    new CacheRecoveryManager(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ *\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class CacheRecoveryManagerState(\n+   blockManagerMasterEndpoint: RpcEndpointRef,\n+   executorAllocationManager: ExecutorAllocationManager,\n+   conf: SparkConf\n+ ) extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"recover-cache-shutdown-timers\")\n+\n+    private val blocksToSave = new mutable.HashMap[String, mutable.PriorityQueue[RDDBlockId]]\n+    private val savedBlocks = new mutable.HashMap[String, mutable.HashSet[RDDBlockId]]\n+    private val killTimers = new mutable.HashMap[String, ScheduledFuture[_]]\n+\n+  /**\n+   * Query Block Manager Master for cached blocks.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executorId to its replication state.\n+   */\n+  def getBlocks(execIds: Seq[String]): Map[String, ExecutorReplicationState] = synchronized {\n+    val isThereEnoughMemory = checkFreeMem(execIds)\n+\n+    isThereEnoughMemory.map {\n+      case (execId, true) =>\n+        updateBlockState(execId)\n+        val blocksRemain = blocksToSave.get(execId).exists(_.nonEmpty)\n+        execId -> (if (blocksRemain) HasCachedBlocks else NoMoreBlocks)\n+      case (execId, false) =>\n+        execId -> NotEnoughMemory\n+    }\n+  }\n+\n+  /**\n+   * Checks to see if there is enough memory on the cluster to hold the cached blocks on these\n+   * executors. Checks on a per executor basis from the smallest to the largest.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executor ids to whether or not there is enough free memory for them.\n+   */\n+  private def checkFreeMem(execIds: Seq[String]): Map[String, Boolean] = {\n+    val execIdsToShutDown = execIds.toSet\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+\n+    val (expiringMemStatus, remainingMemStatus) = allExecMemStatus.partition {\n+      case (k, v) => execIdsToShutDown.contains(k)\n+    }\n+    val freeMemOnRemaining = remainingMemStatus.values.map(_._2).sum\n+\n+    val alreadyReplicatedBlocks: Map[String, Set[RDDBlockId]] =\n+      savedBlocks.filterKeys(execIdsToShutDown).map { case (k, v) => k -> v.toSet }.toMap\n+\n+    val getSizes = GetSizeOfBlocks(alreadyReplicatedBlocks)\n+    val alreadyReplicatedBytes = blockManagerMasterEndpoint.askSync[Map[String, Long]](getSizes)\n+\n+    val bytesToReplicate: Seq[(String, Long)] =\n+      expiringMemStatus.map { case (execId, (maxMem, remainingMem)) =>\n+        val alreadyReplicated = alreadyReplicatedBytes.getOrElse(execId, 0L)\n+        val usedMem = maxMem - remainingMem\n+        val toBeReplicatedMem = usedMem - alreadyReplicated\n+        execId -> toBeReplicatedMem\n+      }.toSeq.sortBy { case (k, v) => v }\n+\n+    bytesToReplicate.scan((\"start\", freeMemOnRemaining)) {\n+      case ((_, remainingMem), (execId, mem)) => (execId, remainingMem - mem)\n+    }.drop(1)\n+      .toMap\n+      .mapValues(freeMem => freeMem >= 0)\n+  }\n+\n+  /**\n+   * Gets a list of cached blocks from the block manager master and updates.\n+   * CacheRecoveryManagerState.\n+   *\n+   * @param execId an executor Id\n+   */\n+  private def updateBlockState(execId: String): Unit = {\n+    val blocks: mutable.Set[RDDBlockId] = blockManagerMasterEndpoint\n+      .askSync[collection.Set[BlockId]](GetCachedBlocks(execId))\n+      .flatMap(_.asRDDId)(collection.breakOut)\n+\n+    blocks --= savedBlocks.getOrElse(execId, new mutable.HashSet)\n+    blocksToSave(execId) = mutable.PriorityQueue[RDDBlockId](blocks.toSeq: _*)(Ordering.by(_.rddId))\n+    killTimers.getOrElseUpdate(execId, scheduleKill(new KillRunner(execId)))\n+  }\n+\n+  class KillRunner(execId: String) extends Runnable { def run(): Unit = killExecutor(execId) }\n+  private def scheduleKill(k: KillRunner) = scheduler.schedule(k, forceKillAfterS, TimeUnit.SECONDS)\n+\n+  /**\n+   * Ask block manager master to replicate one cached block.\n+   *\n+   * @param execId the executor to replicate a block from\n+   * @return false if there are no more blocks or if there is an issue\n+   */\n+  def replicateFirstBlock(execId: String): (Future[Boolean], Option[RDDBlockId]) = synchronized {\n+    logDebug(s\"Replicate block on executor $execId.\")\n+    blocksToSave.get(execId) match {\n+      case Some(p) if p.nonEmpty => doReplication(execId, p.dequeue())\n+      case _ => (Future.successful(false), None)\n+    }\n+  }\n+\n+  private def doReplication(\n+      execId: String,\n+      blockId: RDDBlockId): (Future[Boolean], Option[RDDBlockId]) = {\n+    val savedBlocksForExecutor = savedBlocks.getOrElseUpdate(execId, new mutable.HashSet)\n+    savedBlocksForExecutor += blockId\n+    val replicateMessage = ReplicateOneBlock(execId, blockId, blocksToSave.keys.toSeq)\n+    logTrace(s\"Started replicating block $blockId on exec $execId.\")\n+    val future = blockManagerMasterEndpoint.ask[Boolean](replicateMessage)\n+    (future, Some(blockId))\n+  }\n+\n+  /**\n+   * Ask ExecutorAllocationManager to kill executor and clean up state\n+   * Note executorAllocationManger.killExecutors is blocking, this function should be fixed\n+   * to use a non blocking version\n+   * @param execId the executor to kill"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "this is gone",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-05T18:08:02Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+    case (executorId, NoMoreBlocks | NotEnoughMemory) => state.killExecutor(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    import scala.util.Success\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case Success(false) =>\n+        checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object CacheRecoveryManager {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): CacheRecoveryManager = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new CacheRecoveryManagerState(bmme, eam, conf)\n+    new CacheRecoveryManager(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ *\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class CacheRecoveryManagerState(\n+   blockManagerMasterEndpoint: RpcEndpointRef,\n+   executorAllocationManager: ExecutorAllocationManager,\n+   conf: SparkConf\n+ ) extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"recover-cache-shutdown-timers\")\n+\n+    private val blocksToSave = new mutable.HashMap[String, mutable.PriorityQueue[RDDBlockId]]\n+    private val savedBlocks = new mutable.HashMap[String, mutable.HashSet[RDDBlockId]]\n+    private val killTimers = new mutable.HashMap[String, ScheduledFuture[_]]\n+\n+  /**\n+   * Query Block Manager Master for cached blocks.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executorId to its replication state.\n+   */\n+  def getBlocks(execIds: Seq[String]): Map[String, ExecutorReplicationState] = synchronized {\n+    val isThereEnoughMemory = checkFreeMem(execIds)\n+\n+    isThereEnoughMemory.map {\n+      case (execId, true) =>\n+        updateBlockState(execId)\n+        val blocksRemain = blocksToSave.get(execId).exists(_.nonEmpty)\n+        execId -> (if (blocksRemain) HasCachedBlocks else NoMoreBlocks)\n+      case (execId, false) =>\n+        execId -> NotEnoughMemory\n+    }\n+  }\n+\n+  /**\n+   * Checks to see if there is enough memory on the cluster to hold the cached blocks on these\n+   * executors. Checks on a per executor basis from the smallest to the largest.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executor ids to whether or not there is enough free memory for them.\n+   */\n+  private def checkFreeMem(execIds: Seq[String]): Map[String, Boolean] = {\n+    val execIdsToShutDown = execIds.toSet\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+\n+    val (expiringMemStatus, remainingMemStatus) = allExecMemStatus.partition {\n+      case (k, v) => execIdsToShutDown.contains(k)\n+    }\n+    val freeMemOnRemaining = remainingMemStatus.values.map(_._2).sum\n+\n+    val alreadyReplicatedBlocks: Map[String, Set[RDDBlockId]] =\n+      savedBlocks.filterKeys(execIdsToShutDown).map { case (k, v) => k -> v.toSet }.toMap\n+\n+    val getSizes = GetSizeOfBlocks(alreadyReplicatedBlocks)\n+    val alreadyReplicatedBytes = blockManagerMasterEndpoint.askSync[Map[String, Long]](getSizes)\n+\n+    val bytesToReplicate: Seq[(String, Long)] =\n+      expiringMemStatus.map { case (execId, (maxMem, remainingMem)) =>\n+        val alreadyReplicated = alreadyReplicatedBytes.getOrElse(execId, 0L)\n+        val usedMem = maxMem - remainingMem\n+        val toBeReplicatedMem = usedMem - alreadyReplicated\n+        execId -> toBeReplicatedMem\n+      }.toSeq.sortBy { case (k, v) => v }\n+\n+    bytesToReplicate.scan((\"start\", freeMemOnRemaining)) {\n+      case ((_, remainingMem), (execId, mem)) => (execId, remainingMem - mem)\n+    }.drop(1)\n+      .toMap\n+      .mapValues(freeMem => freeMem >= 0)\n+  }\n+\n+  /**\n+   * Gets a list of cached blocks from the block manager master and updates.\n+   * CacheRecoveryManagerState.\n+   *\n+   * @param execId an executor Id\n+   */\n+  private def updateBlockState(execId: String): Unit = {\n+    val blocks: mutable.Set[RDDBlockId] = blockManagerMasterEndpoint\n+      .askSync[collection.Set[BlockId]](GetCachedBlocks(execId))\n+      .flatMap(_.asRDDId)(collection.breakOut)\n+\n+    blocks --= savedBlocks.getOrElse(execId, new mutable.HashSet)\n+    blocksToSave(execId) = mutable.PriorityQueue[RDDBlockId](blocks.toSeq: _*)(Ordering.by(_.rddId))\n+    killTimers.getOrElseUpdate(execId, scheduleKill(new KillRunner(execId)))\n+  }\n+\n+  class KillRunner(execId: String) extends Runnable { def run(): Unit = killExecutor(execId) }\n+  private def scheduleKill(k: KillRunner) = scheduler.schedule(k, forceKillAfterS, TimeUnit.SECONDS)\n+\n+  /**\n+   * Ask block manager master to replicate one cached block.\n+   *\n+   * @param execId the executor to replicate a block from\n+   * @return false if there are no more blocks or if there is an issue\n+   */\n+  def replicateFirstBlock(execId: String): (Future[Boolean], Option[RDDBlockId]) = synchronized {\n+    logDebug(s\"Replicate block on executor $execId.\")\n+    blocksToSave.get(execId) match {\n+      case Some(p) if p.nonEmpty => doReplication(execId, p.dequeue())\n+      case _ => (Future.successful(false), None)\n+    }\n+  }\n+\n+  private def doReplication(\n+      execId: String,\n+      blockId: RDDBlockId): (Future[Boolean], Option[RDDBlockId]) = {\n+    val savedBlocksForExecutor = savedBlocks.getOrElseUpdate(execId, new mutable.HashSet)\n+    savedBlocksForExecutor += blockId\n+    val replicateMessage = ReplicateOneBlock(execId, blockId, blocksToSave.keys.toSeq)\n+    logTrace(s\"Started replicating block $blockId on exec $execId.\")\n+    val future = blockManagerMasterEndpoint.ask[Boolean](replicateMessage)\n+    (future, Some(blockId))\n+  }\n+\n+  /**\n+   * Ask ExecutorAllocationManager to kill executor and clean up state\n+   * Note executorAllocationManger.killExecutors is blocking, this function should be fixed\n+   * to use a non blocking version\n+   * @param execId the executor to kill"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Same thing, don't see you using the return value anywhere, so `Unit`.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-10-10T18:28:37Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+    case (executorId, NoMoreBlocks | NotEnoughMemory) => state.killExecutor(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    import scala.util.Success\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case Success(false) =>\n+        checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object CacheRecoveryManager {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): CacheRecoveryManager = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new CacheRecoveryManagerState(bmme, eam, conf)\n+    new CacheRecoveryManager(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ *\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class CacheRecoveryManagerState(\n+   blockManagerMasterEndpoint: RpcEndpointRef,\n+   executorAllocationManager: ExecutorAllocationManager,\n+   conf: SparkConf\n+ ) extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"recover-cache-shutdown-timers\")\n+\n+    private val blocksToSave = new mutable.HashMap[String, mutable.PriorityQueue[RDDBlockId]]\n+    private val savedBlocks = new mutable.HashMap[String, mutable.HashSet[RDDBlockId]]\n+    private val killTimers = new mutable.HashMap[String, ScheduledFuture[_]]\n+\n+  /**\n+   * Query Block Manager Master for cached blocks.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executorId to its replication state.\n+   */\n+  def getBlocks(execIds: Seq[String]): Map[String, ExecutorReplicationState] = synchronized {\n+    val isThereEnoughMemory = checkFreeMem(execIds)\n+\n+    isThereEnoughMemory.map {\n+      case (execId, true) =>\n+        updateBlockState(execId)\n+        val blocksRemain = blocksToSave.get(execId).exists(_.nonEmpty)\n+        execId -> (if (blocksRemain) HasCachedBlocks else NoMoreBlocks)\n+      case (execId, false) =>\n+        execId -> NotEnoughMemory\n+    }\n+  }\n+\n+  /**\n+   * Checks to see if there is enough memory on the cluster to hold the cached blocks on these\n+   * executors. Checks on a per executor basis from the smallest to the largest.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executor ids to whether or not there is enough free memory for them.\n+   */\n+  private def checkFreeMem(execIds: Seq[String]): Map[String, Boolean] = {\n+    val execIdsToShutDown = execIds.toSet\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+\n+    val (expiringMemStatus, remainingMemStatus) = allExecMemStatus.partition {\n+      case (k, v) => execIdsToShutDown.contains(k)\n+    }\n+    val freeMemOnRemaining = remainingMemStatus.values.map(_._2).sum\n+\n+    val alreadyReplicatedBlocks: Map[String, Set[RDDBlockId]] =\n+      savedBlocks.filterKeys(execIdsToShutDown).map { case (k, v) => k -> v.toSet }.toMap\n+\n+    val getSizes = GetSizeOfBlocks(alreadyReplicatedBlocks)\n+    val alreadyReplicatedBytes = blockManagerMasterEndpoint.askSync[Map[String, Long]](getSizes)\n+\n+    val bytesToReplicate: Seq[(String, Long)] =\n+      expiringMemStatus.map { case (execId, (maxMem, remainingMem)) =>\n+        val alreadyReplicated = alreadyReplicatedBytes.getOrElse(execId, 0L)\n+        val usedMem = maxMem - remainingMem\n+        val toBeReplicatedMem = usedMem - alreadyReplicated\n+        execId -> toBeReplicatedMem\n+      }.toSeq.sortBy { case (k, v) => v }\n+\n+    bytesToReplicate.scan((\"start\", freeMemOnRemaining)) {\n+      case ((_, remainingMem), (execId, mem)) => (execId, remainingMem - mem)\n+    }.drop(1)\n+      .toMap\n+      .mapValues(freeMem => freeMem >= 0)\n+  }\n+\n+  /**\n+   * Gets a list of cached blocks from the block manager master and updates.\n+   * CacheRecoveryManagerState.\n+   *\n+   * @param execId an executor Id\n+   */\n+  private def updateBlockState(execId: String): Unit = {\n+    val blocks: mutable.Set[RDDBlockId] = blockManagerMasterEndpoint\n+      .askSync[collection.Set[BlockId]](GetCachedBlocks(execId))\n+      .flatMap(_.asRDDId)(collection.breakOut)\n+\n+    blocks --= savedBlocks.getOrElse(execId, new mutable.HashSet)\n+    blocksToSave(execId) = mutable.PriorityQueue[RDDBlockId](blocks.toSeq: _*)(Ordering.by(_.rddId))\n+    killTimers.getOrElseUpdate(execId, scheduleKill(new KillRunner(execId)))\n+  }\n+\n+  class KillRunner(execId: String) extends Runnable { def run(): Unit = killExecutor(execId) }\n+  private def scheduleKill(k: KillRunner) = scheduler.schedule(k, forceKillAfterS, TimeUnit.SECONDS)\n+\n+  /**\n+   * Ask block manager master to replicate one cached block.\n+   *\n+   * @param execId the executor to replicate a block from\n+   * @return false if there are no more blocks or if there is an issue\n+   */\n+  def replicateFirstBlock(execId: String): (Future[Boolean], Option[RDDBlockId]) = synchronized {\n+    logDebug(s\"Replicate block on executor $execId.\")\n+    blocksToSave.get(execId) match {\n+      case Some(p) if p.nonEmpty => doReplication(execId, p.dequeue())\n+      case _ => (Future.successful(false), None)\n+    }\n+  }\n+\n+  private def doReplication(\n+      execId: String,\n+      blockId: RDDBlockId): (Future[Boolean], Option[RDDBlockId]) = {\n+    val savedBlocksForExecutor = savedBlocks.getOrElseUpdate(execId, new mutable.HashSet)\n+    savedBlocksForExecutor += blockId\n+    val replicateMessage = ReplicateOneBlock(execId, blockId, blocksToSave.keys.toSeq)\n+    logTrace(s\"Started replicating block $blockId on exec $execId.\")\n+    val future = blockManagerMasterEndpoint.ask[Boolean](replicateMessage)\n+    (future, Some(blockId))\n+  }\n+\n+  /**\n+   * Ask ExecutorAllocationManager to kill executor and clean up state\n+   * Note executorAllocationManger.killExecutors is blocking, this function should be fixed\n+   * to use a non blocking version\n+   * @param execId the executor to kill\n+   */\n+  def killExecutor(execId: String): Unit = synchronized {\n+    logDebug(s\"Send request to kill executor $execId.\")\n+    killTimers.remove(execId).foreach(_.cancel(false))\n+    blocksToSave -= execId\n+    savedBlocks -= execId\n+    executorAllocationManager.killExecutors(Seq(execId), forceIfPending = true)\n+  }\n+\n+  def stop(): java.util.List[Runnable] = scheduler.shutdownNow()"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "fixed",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-05T18:08:17Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    checkForReplicableBlocks(execIds)\n+  }\n+\n+  /**\n+   * Stops all thread pools\n+   *\n+   * @return\n+   */\n+  def stop(): java.util.List[Runnable] = {\n+    threadPool.shutdownNow()\n+    state.stop()\n+  }\n+\n+  /**\n+   * Get list of cached blocks from BlockManagerMaster. If there are cached blocks, replicate them,\n+   * otherwise kill the executors\n+   *\n+   * @param execIds the executors to check\n+   */\n+  private def checkForReplicableBlocks(execIds: Seq[String]) = state.getBlocks(execIds).foreach {\n+    case (executorId, HasCachedBlocks) => replicateBlocks(executorId)\n+    case (executorId, NoMoreBlocks | NotEnoughMemory) => state.killExecutor(executorId)\n+  }\n+\n+  /**\n+   * Replicate one cached block on an executor. If there are more, repeat. If there are none, check\n+   * with the block manager master again. If there is an error, go ahead and kill executor.\n+   *\n+   * @param execId the executor to save a block one\n+   */\n+  private def replicateBlocks(execId: String): Unit = {\n+    import scala.util.Success\n+    val (response, blockId) = state.replicateFirstBlock(execId)\n+    response.onComplete {\n+      case Success(true) =>\n+        logTrace(s\"Finished replicating block ${blockId.getOrElse(\"unknown\")} on exec $execId.\")\n+        replicateBlocks(execId)\n+      case Success(false) =>\n+        checkForReplicableBlocks(Seq(execId))\n+      case Failure(f) =>\n+        logWarning(s\"Error trying to replicate block ${blockId.getOrElse(\"unknown\")}.\", f)\n+        state.killExecutor(execId)\n+    }\n+  }\n+}\n+\n+private object CacheRecoveryManager {\n+  def apply(eam: ExecutorAllocationManager, conf: SparkConf): CacheRecoveryManager = {\n+    val bmme = SparkEnv.get.blockManager.master.driverEndpoint\n+    val state = new CacheRecoveryManagerState(bmme, eam, conf)\n+    new CacheRecoveryManager(state, conf)\n+  }\n+}\n+\n+/**\n+ * Private class that holds state for all the executors being shutdown.\n+ *\n+ * @param blockManagerMasterEndpoint blockManagerMasterEndpoint\n+ * @param executorAllocationManager ExecutorAllocationManager\n+ * @param conf spark conf\n+ */\n+final private class CacheRecoveryManagerState(\n+   blockManagerMasterEndpoint: RpcEndpointRef,\n+   executorAllocationManager: ExecutorAllocationManager,\n+   conf: SparkConf\n+ ) extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"recover-cache-shutdown-timers\")\n+\n+    private val blocksToSave = new mutable.HashMap[String, mutable.PriorityQueue[RDDBlockId]]\n+    private val savedBlocks = new mutable.HashMap[String, mutable.HashSet[RDDBlockId]]\n+    private val killTimers = new mutable.HashMap[String, ScheduledFuture[_]]\n+\n+  /**\n+   * Query Block Manager Master for cached blocks.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executorId to its replication state.\n+   */\n+  def getBlocks(execIds: Seq[String]): Map[String, ExecutorReplicationState] = synchronized {\n+    val isThereEnoughMemory = checkFreeMem(execIds)\n+\n+    isThereEnoughMemory.map {\n+      case (execId, true) =>\n+        updateBlockState(execId)\n+        val blocksRemain = blocksToSave.get(execId).exists(_.nonEmpty)\n+        execId -> (if (blocksRemain) HasCachedBlocks else NoMoreBlocks)\n+      case (execId, false) =>\n+        execId -> NotEnoughMemory\n+    }\n+  }\n+\n+  /**\n+   * Checks to see if there is enough memory on the cluster to hold the cached blocks on these\n+   * executors. Checks on a per executor basis from the smallest to the largest.\n+   *\n+   * @param execIds the executors to query\n+   * @return a map of executor ids to whether or not there is enough free memory for them.\n+   */\n+  private def checkFreeMem(execIds: Seq[String]): Map[String, Boolean] = {\n+    val execIdsToShutDown = execIds.toSet\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+\n+    val (expiringMemStatus, remainingMemStatus) = allExecMemStatus.partition {\n+      case (k, v) => execIdsToShutDown.contains(k)\n+    }\n+    val freeMemOnRemaining = remainingMemStatus.values.map(_._2).sum\n+\n+    val alreadyReplicatedBlocks: Map[String, Set[RDDBlockId]] =\n+      savedBlocks.filterKeys(execIdsToShutDown).map { case (k, v) => k -> v.toSet }.toMap\n+\n+    val getSizes = GetSizeOfBlocks(alreadyReplicatedBlocks)\n+    val alreadyReplicatedBytes = blockManagerMasterEndpoint.askSync[Map[String, Long]](getSizes)\n+\n+    val bytesToReplicate: Seq[(String, Long)] =\n+      expiringMemStatus.map { case (execId, (maxMem, remainingMem)) =>\n+        val alreadyReplicated = alreadyReplicatedBytes.getOrElse(execId, 0L)\n+        val usedMem = maxMem - remainingMem\n+        val toBeReplicatedMem = usedMem - alreadyReplicated\n+        execId -> toBeReplicatedMem\n+      }.toSeq.sortBy { case (k, v) => v }\n+\n+    bytesToReplicate.scan((\"start\", freeMemOnRemaining)) {\n+      case ((_, remainingMem), (execId, mem)) => (execId, remainingMem - mem)\n+    }.drop(1)\n+      .toMap\n+      .mapValues(freeMem => freeMem >= 0)\n+  }\n+\n+  /**\n+   * Gets a list of cached blocks from the block manager master and updates.\n+   * CacheRecoveryManagerState.\n+   *\n+   * @param execId an executor Id\n+   */\n+  private def updateBlockState(execId: String): Unit = {\n+    val blocks: mutable.Set[RDDBlockId] = blockManagerMasterEndpoint\n+      .askSync[collection.Set[BlockId]](GetCachedBlocks(execId))\n+      .flatMap(_.asRDDId)(collection.breakOut)\n+\n+    blocks --= savedBlocks.getOrElse(execId, new mutable.HashSet)\n+    blocksToSave(execId) = mutable.PriorityQueue[RDDBlockId](blocks.toSeq: _*)(Ordering.by(_.rddId))\n+    killTimers.getOrElseUpdate(execId, scheduleKill(new KillRunner(execId)))\n+  }\n+\n+  class KillRunner(execId: String) extends Runnable { def run(): Unit = killExecutor(execId) }\n+  private def scheduleKill(k: KillRunner) = scheduler.schedule(k, forceKillAfterS, TimeUnit.SECONDS)\n+\n+  /**\n+   * Ask block manager master to replicate one cached block.\n+   *\n+   * @param execId the executor to replicate a block from\n+   * @return false if there are no more blocks or if there is an issue\n+   */\n+  def replicateFirstBlock(execId: String): (Future[Boolean], Option[RDDBlockId]) = synchronized {\n+    logDebug(s\"Replicate block on executor $execId.\")\n+    blocksToSave.get(execId) match {\n+      case Some(p) if p.nonEmpty => doReplication(execId, p.dequeue())\n+      case _ => (Future.successful(false), None)\n+    }\n+  }\n+\n+  private def doReplication(\n+      execId: String,\n+      blockId: RDDBlockId): (Future[Boolean], Option[RDDBlockId]) = {\n+    val savedBlocksForExecutor = savedBlocks.getOrElseUpdate(execId, new mutable.HashSet)\n+    savedBlocksForExecutor += blockId\n+    val replicateMessage = ReplicateOneBlock(execId, blockId, blocksToSave.keys.toSeq)\n+    logTrace(s\"Started replicating block $blockId on exec $execId.\")\n+    val future = blockManagerMasterEndpoint.ask[Boolean](replicateMessage)\n+    (future, Some(blockId))\n+  }\n+\n+  /**\n+   * Ask ExecutorAllocationManager to kill executor and clean up state\n+   * Note executorAllocationManger.killExecutors is blocking, this function should be fixed\n+   * to use a non blocking version\n+   * @param execId the executor to kill\n+   */\n+  def killExecutor(execId: String): Unit = synchronized {\n+    logDebug(s\"Send request to kill executor $execId.\")\n+    killTimers.remove(execId).foreach(_.cancel(false))\n+    blocksToSave -= execId\n+    savedBlocks -= execId\n+    executorAllocationManager.killExecutors(Seq(execId), forceIfPending = true)\n+  }\n+\n+  def stop(): java.util.List[Runnable] = scheduler.shutdownNow()"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Probably better called `startCacheRecovery`.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-10-10T18:36:03Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "fixed",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-05T15:50:22Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_RECOVER_CACHE_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages.{GetCachedBlocks, GetMemoryStatus, GetSizeOfBlocks, ReplicateOneBlock}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executors cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    state: CacheRecoveryManagerState,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"recover-cache-shutdown-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startExecutorKill(execIds: Seq[String]): Unit = {"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "This is very minor, but because you don't have the full picture here, it could happen that some of the blocks in the \"chosen\" executors cannot be replicated (e.g. because a block is too large for any of the remaining executors to hold), and you could potentially replicate blocks from these \"loser\" executors in that case.\r\n\r\nOk not to handle this, but it'd be nice to add a comment (for a future enhancement, maybe).",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-06T22:10:18Z",
    "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ConcurrentHashMap, ScheduledFuture, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.{Success => Succ}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages._\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executor's cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    blockManagerMasterEndpoint: RpcEndpointRef,\n+    executorAllocationManager: ExecutorAllocationManager,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT)\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"cache-recovery-manager-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"cache-recovery-shutdown-timers\")\n+  private val recoveringExecutors: mutable.Set[String] =\n+    ConcurrentHashMap.newKeySet[String]().asScala\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startCacheRecovery(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    val canBeRecovered = checkMem(execIds)\n+    recoveringExecutors ++= canBeRecovered\n+    val executorsWithKillTimers = canBeRecovered.zip(canBeRecovered.map(startKillTimer))\n+    executorsWithKillTimers.foreach((replicateUntilDone _).tupled)\n+  }\n+\n+  /**\n+   * Given a list of executors that will be shut down, check if there is enough free memory on the\n+   * rest of the cluster to hold their data. Return a list of just the executors for which there\n+   * will be enough space. Executors are included smallest first.\n+   *\n+   * @param execIds executors which will be shut down\n+   * @return a Seq of the executors we do have room for\n+   */\n+  private def checkMem(execIds: Seq[String]): Seq[String] = {\n+    val execsToShutDown = execIds.toSet\n+    // Memory Status is a map of executor Id to a tuple of Max Memory and remaining memory on that\n+    // executor.\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+\n+    val (expiringMemStatus, remainingMemStatus) = allExecMemStatus.partition {\n+      case (execId, _) => execsToShutDown.contains(execId)\n+    }\n+    val freeMemOnRemaining = remainingMemStatus.values.map(_._2).sum\n+\n+    // The used mem on each executor sorted from least used mem to greatest\n+    val executorAndUsedMem: Seq[(String, Long)] =\n+      expiringMemStatus.map { case (execId, (maxMem, remainingMem)) =>\n+        val usedMem = maxMem - remainingMem\n+        execId -> usedMem\n+      }.toSeq.sortBy { case (_, usedMem) => usedMem }\n+\n+    executorAndUsedMem\n+      .scan((\"start\", freeMemOnRemaining)) {\n+        case ((_, freeMem), (execId, usedMem)) => (execId, freeMem - usedMem)\n+      }\n+      .drop(1)\n+      .filter { case (_, freeMem) => freeMem > 0 }"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "added comment",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-12T16:21:19Z",
    "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ConcurrentHashMap, ScheduledFuture, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.{Success => Succ}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages._\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executor's cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    blockManagerMasterEndpoint: RpcEndpointRef,\n+    executorAllocationManager: ExecutorAllocationManager,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT)\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"cache-recovery-manager-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"cache-recovery-shutdown-timers\")\n+  private val recoveringExecutors: mutable.Set[String] =\n+    ConcurrentHashMap.newKeySet[String]().asScala\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startCacheRecovery(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    val canBeRecovered = checkMem(execIds)\n+    recoveringExecutors ++= canBeRecovered\n+    val executorsWithKillTimers = canBeRecovered.zip(canBeRecovered.map(startKillTimer))\n+    executorsWithKillTimers.foreach((replicateUntilDone _).tupled)\n+  }\n+\n+  /**\n+   * Given a list of executors that will be shut down, check if there is enough free memory on the\n+   * rest of the cluster to hold their data. Return a list of just the executors for which there\n+   * will be enough space. Executors are included smallest first.\n+   *\n+   * @param execIds executors which will be shut down\n+   * @return a Seq of the executors we do have room for\n+   */\n+  private def checkMem(execIds: Seq[String]): Seq[String] = {\n+    val execsToShutDown = execIds.toSet\n+    // Memory Status is a map of executor Id to a tuple of Max Memory and remaining memory on that\n+    // executor.\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+\n+    val (expiringMemStatus, remainingMemStatus) = allExecMemStatus.partition {\n+      case (execId, _) => execsToShutDown.contains(execId)\n+    }\n+    val freeMemOnRemaining = remainingMemStatus.values.map(_._2).sum\n+\n+    // The used mem on each executor sorted from least used mem to greatest\n+    val executorAndUsedMem: Seq[(String, Long)] =\n+      expiringMemStatus.map { case (execId, (maxMem, remainingMem)) =>\n+        val usedMem = maxMem - remainingMem\n+        execId -> usedMem\n+      }.toSeq.sortBy { case (_, usedMem) => usedMem }\n+\n+    executorAndUsedMem\n+      .scan((\"start\", freeMemOnRemaining)) {\n+        case ((_, freeMem), (execId, usedMem)) => (execId, freeMem - usedMem)\n+      }\n+      .drop(1)\n+      .filter { case (_, freeMem) => freeMem > 0 }"
  }, {
    "author": {
      "login": "squito"
    },
    "body": "conversely, you might decide to not even try to replicate some executors, because *all* of their data can't be replicated, but in fact *some* of the blocks on the executor could be replicated (maybe even the most useful ones).\r\n\r\ngiven the limitations & complications of this method, I'm wondering whether its even worth it to do this filtering?",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2018-01-31T22:37:36Z",
    "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ConcurrentHashMap, ScheduledFuture, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.{Success => Succ}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages._\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executor's cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    blockManagerMasterEndpoint: RpcEndpointRef,\n+    executorAllocationManager: ExecutorAllocationManager,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT)\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"cache-recovery-manager-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"cache-recovery-shutdown-timers\")\n+  private val recoveringExecutors: mutable.Set[String] =\n+    ConcurrentHashMap.newKeySet[String]().asScala\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startCacheRecovery(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    val canBeRecovered = checkMem(execIds)\n+    recoveringExecutors ++= canBeRecovered\n+    val executorsWithKillTimers = canBeRecovered.zip(canBeRecovered.map(startKillTimer))\n+    executorsWithKillTimers.foreach((replicateUntilDone _).tupled)\n+  }\n+\n+  /**\n+   * Given a list of executors that will be shut down, check if there is enough free memory on the\n+   * rest of the cluster to hold their data. Return a list of just the executors for which there\n+   * will be enough space. Executors are included smallest first.\n+   *\n+   * @param execIds executors which will be shut down\n+   * @return a Seq of the executors we do have room for\n+   */\n+  private def checkMem(execIds: Seq[String]): Seq[String] = {\n+    val execsToShutDown = execIds.toSet\n+    // Memory Status is a map of executor Id to a tuple of Max Memory and remaining memory on that\n+    // executor.\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+\n+    val (expiringMemStatus, remainingMemStatus) = allExecMemStatus.partition {\n+      case (execId, _) => execsToShutDown.contains(execId)\n+    }\n+    val freeMemOnRemaining = remainingMemStatus.values.map(_._2).sum\n+\n+    // The used mem on each executor sorted from least used mem to greatest\n+    val executorAndUsedMem: Seq[(String, Long)] =\n+      expiringMemStatus.map { case (execId, (maxMem, remainingMem)) =>\n+        val usedMem = maxMem - remainingMem\n+        execId -> usedMem\n+      }.toSeq.sortBy { case (_, usedMem) => usedMem }\n+\n+    executorAndUsedMem\n+      .scan((\"start\", freeMemOnRemaining)) {\n+        case ((_, freeMem), (execId, usedMem)) => (execId, freeMem - usedMem)\n+      }\n+      .drop(1)\n+      .filter { case (_, freeMem) => freeMem > 0 }"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "That's a good point. I was really concerned with not hurting the stability of the cluster. So in the case where we are running out of cluster memory, I thought it would be safer to fail fast and give up on replication. \r\n\r\nYour right it would probably be more useful to get as many complete RDDs recovered as possible, and this method is complicated. I can test taking out this check and see what happens.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2018-03-16T15:21:06Z",
    "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ConcurrentHashMap, ScheduledFuture, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.{Success => Succ}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages._\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executor's cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    blockManagerMasterEndpoint: RpcEndpointRef,\n+    executorAllocationManager: ExecutorAllocationManager,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT)\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"cache-recovery-manager-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"cache-recovery-shutdown-timers\")\n+  private val recoveringExecutors: mutable.Set[String] =\n+    ConcurrentHashMap.newKeySet[String]().asScala\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startCacheRecovery(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    val canBeRecovered = checkMem(execIds)\n+    recoveringExecutors ++= canBeRecovered\n+    val executorsWithKillTimers = canBeRecovered.zip(canBeRecovered.map(startKillTimer))\n+    executorsWithKillTimers.foreach((replicateUntilDone _).tupled)\n+  }\n+\n+  /**\n+   * Given a list of executors that will be shut down, check if there is enough free memory on the\n+   * rest of the cluster to hold their data. Return a list of just the executors for which there\n+   * will be enough space. Executors are included smallest first.\n+   *\n+   * @param execIds executors which will be shut down\n+   * @return a Seq of the executors we do have room for\n+   */\n+  private def checkMem(execIds: Seq[String]): Seq[String] = {\n+    val execsToShutDown = execIds.toSet\n+    // Memory Status is a map of executor Id to a tuple of Max Memory and remaining memory on that\n+    // executor.\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+\n+    val (expiringMemStatus, remainingMemStatus) = allExecMemStatus.partition {\n+      case (execId, _) => execsToShutDown.contains(execId)\n+    }\n+    val freeMemOnRemaining = remainingMemStatus.values.map(_._2).sum\n+\n+    // The used mem on each executor sorted from least used mem to greatest\n+    val executorAndUsedMem: Seq[(String, Long)] =\n+      expiringMemStatus.map { case (execId, (maxMem, remainingMem)) =>\n+        val usedMem = maxMem - remainingMem\n+        execId -> usedMem\n+      }.toSeq.sortBy { case (_, usedMem) => usedMem }\n+\n+    executorAndUsedMem\n+      .scan((\"start\", freeMemOnRemaining)) {\n+        case ((_, freeMem), (execId, usedMem)) => (execId, freeMem - usedMem)\n+      }\n+      .drop(1)\n+      .filter { case (_, freeMem) => freeMem > 0 }"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "I find this more readable:\r\n\r\n```\r\ncanBeRecovered.foreach { execId =>\r\n  replicateUntilDone(execId, startKillTimer(execId))\r\n}\r\n```",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-06T22:14:42Z",
    "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ConcurrentHashMap, ScheduledFuture, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.{Success => Succ}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages._\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executor's cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    blockManagerMasterEndpoint: RpcEndpointRef,\n+    executorAllocationManager: ExecutorAllocationManager,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT)\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"cache-recovery-manager-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"cache-recovery-shutdown-timers\")\n+  private val recoveringExecutors: mutable.Set[String] =\n+    ConcurrentHashMap.newKeySet[String]().asScala\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startCacheRecovery(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    val canBeRecovered = checkMem(execIds)\n+    recoveringExecutors ++= canBeRecovered\n+    val executorsWithKillTimers = canBeRecovered.zip(canBeRecovered.map(startKillTimer))\n+    executorsWithKillTimers.foreach((replicateUntilDone _).tupled)"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "thats much better! fixed.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-12T16:20:45Z",
    "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ConcurrentHashMap, ScheduledFuture, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.{Success => Succ}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages._\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executor's cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    blockManagerMasterEndpoint: RpcEndpointRef,\n+    executorAllocationManager: ExecutorAllocationManager,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT)\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"cache-recovery-manager-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"cache-recovery-shutdown-timers\")\n+  private val recoveringExecutors: mutable.Set[String] =\n+    ConcurrentHashMap.newKeySet[String]().asScala\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startCacheRecovery(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    val canBeRecovered = checkMem(execIds)\n+    recoveringExecutors ++= canBeRecovered\n+    val executorsWithKillTimers = canBeRecovered.zip(canBeRecovered.map(startKillTimer))\n+    executorsWithKillTimers.foreach((replicateUntilDone _).tupled)"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: can go in previous import.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-06T22:17:18Z",
    "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ConcurrentHashMap, ScheduledFuture, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.{Success => Succ}\n+import scala.util.Failure"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "fixed",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-12T16:19:45Z",
    "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ConcurrentHashMap, ScheduledFuture, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.{Success => Succ}\n+import scala.util.Failure"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: `TrySuccess` (more verbose but less cryptic).",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-06T22:17:38Z",
    "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ConcurrentHashMap, ScheduledFuture, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.{Success => Succ}"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "fixed",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-12T16:19:26Z",
    "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ConcurrentHashMap, ScheduledFuture, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.{Success => Succ}"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Call this only if the timer was successfully stopped? (Same above.)",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-06T22:18:43Z",
    "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ConcurrentHashMap, ScheduledFuture, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.{Success => Succ}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages._\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executor's cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    blockManagerMasterEndpoint: RpcEndpointRef,\n+    executorAllocationManager: ExecutorAllocationManager,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT)\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"cache-recovery-manager-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"cache-recovery-shutdown-timers\")\n+  private val recoveringExecutors: mutable.Set[String] =\n+    ConcurrentHashMap.newKeySet[String]().asScala\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startCacheRecovery(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    val canBeRecovered = checkMem(execIds)\n+    recoveringExecutors ++= canBeRecovered\n+    val executorsWithKillTimers = canBeRecovered.zip(canBeRecovered.map(startKillTimer))\n+    executorsWithKillTimers.foreach((replicateUntilDone _).tupled)\n+  }\n+\n+  /**\n+   * Given a list of executors that will be shut down, check if there is enough free memory on the\n+   * rest of the cluster to hold their data. Return a list of just the executors for which there\n+   * will be enough space. Executors are included smallest first.\n+   *\n+   * @param execIds executors which will be shut down\n+   * @return a Seq of the executors we do have room for\n+   */\n+  private def checkMem(execIds: Seq[String]): Seq[String] = {\n+    val execsToShutDown = execIds.toSet\n+    // Memory Status is a map of executor Id to a tuple of Max Memory and remaining memory on that\n+    // executor.\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+\n+    val (expiringMemStatus, remainingMemStatus) = allExecMemStatus.partition {\n+      case (execId, _) => execsToShutDown.contains(execId)\n+    }\n+    val freeMemOnRemaining = remainingMemStatus.values.map(_._2).sum\n+\n+    // The used mem on each executor sorted from least used mem to greatest\n+    val executorAndUsedMem: Seq[(String, Long)] =\n+      expiringMemStatus.map { case (execId, (maxMem, remainingMem)) =>\n+        val usedMem = maxMem - remainingMem\n+        execId -> usedMem\n+      }.toSeq.sortBy { case (_, usedMem) => usedMem }\n+\n+    executorAndUsedMem\n+      .scan((\"start\", freeMemOnRemaining)) {\n+        case ((_, freeMem), (execId, usedMem)) => (execId, freeMem - usedMem)\n+      }\n+      .drop(1)\n+      .filter { case (_, freeMem) => freeMem > 0 }\n+      .map(_._1)\n+  }\n+\n+  /**\n+   * Given an executor id, start a timer that will kill the given executor after the configured\n+   * timeout\n+   *\n+   * @param execId The id of the executor to be killed\n+   * @return a future representing the timer\n+   */\n+  private def startKillTimer(execId: String): ScheduledFuture[_] = {\n+    val killer = new Runnable {\n+      def run(): Unit = {\n+        logDebug(s\"Killing $execId because timeout for recovering cached data has expired\")\n+        kill(execId)\n+      }\n+    }\n+    scheduler.schedule(killer, forceKillAfterS, TimeUnit.SECONDS)\n+  }\n+\n+  /**\n+   * Recover cached RDD blocks off of an executor until there are no more, or until\n+   * there is an error\n+   *\n+   * @param execId the id of the executor to be killed\n+   * @param killTimer The runnable scheduled to kill this executor. Cancel it if we finish before\n+   *                  it does.\n+   */\n+  private def replicateUntilDone(execId: String, killTimer: ScheduledFuture[_]): Unit = {\n+    recoverLatestBlock(execId).onComplete {\n+      case Succ(Some(blockId)) =>\n+        logTrace(s\"Replicated block $blockId on executor $execId\")\n+        replicateUntilDone(execId, killTimer)\n+      case Succ(None) =>\n+        killTimer.cancel(false)\n+        kill(execId)\n+      case Failure(e) =>\n+        logWarning(\"Failure recovering cached data before executor $execId shutdown\", e)\n+        killTimer.cancel(false)\n+        kill(execId)"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "good idea, fixed.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-12T16:21:39Z",
    "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ConcurrentHashMap, ScheduledFuture, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.{Success => Succ}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages._\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executor's cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    blockManagerMasterEndpoint: RpcEndpointRef,\n+    executorAllocationManager: ExecutorAllocationManager,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT)\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"cache-recovery-manager-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"cache-recovery-shutdown-timers\")\n+  private val recoveringExecutors: mutable.Set[String] =\n+    ConcurrentHashMap.newKeySet[String]().asScala\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startCacheRecovery(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    val canBeRecovered = checkMem(execIds)\n+    recoveringExecutors ++= canBeRecovered\n+    val executorsWithKillTimers = canBeRecovered.zip(canBeRecovered.map(startKillTimer))\n+    executorsWithKillTimers.foreach((replicateUntilDone _).tupled)\n+  }\n+\n+  /**\n+   * Given a list of executors that will be shut down, check if there is enough free memory on the\n+   * rest of the cluster to hold their data. Return a list of just the executors for which there\n+   * will be enough space. Executors are included smallest first.\n+   *\n+   * @param execIds executors which will be shut down\n+   * @return a Seq of the executors we do have room for\n+   */\n+  private def checkMem(execIds: Seq[String]): Seq[String] = {\n+    val execsToShutDown = execIds.toSet\n+    // Memory Status is a map of executor Id to a tuple of Max Memory and remaining memory on that\n+    // executor.\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+\n+    val (expiringMemStatus, remainingMemStatus) = allExecMemStatus.partition {\n+      case (execId, _) => execsToShutDown.contains(execId)\n+    }\n+    val freeMemOnRemaining = remainingMemStatus.values.map(_._2).sum\n+\n+    // The used mem on each executor sorted from least used mem to greatest\n+    val executorAndUsedMem: Seq[(String, Long)] =\n+      expiringMemStatus.map { case (execId, (maxMem, remainingMem)) =>\n+        val usedMem = maxMem - remainingMem\n+        execId -> usedMem\n+      }.toSeq.sortBy { case (_, usedMem) => usedMem }\n+\n+    executorAndUsedMem\n+      .scan((\"start\", freeMemOnRemaining)) {\n+        case ((_, freeMem), (execId, usedMem)) => (execId, freeMem - usedMem)\n+      }\n+      .drop(1)\n+      .filter { case (_, freeMem) => freeMem > 0 }\n+      .map(_._1)\n+  }\n+\n+  /**\n+   * Given an executor id, start a timer that will kill the given executor after the configured\n+   * timeout\n+   *\n+   * @param execId The id of the executor to be killed\n+   * @return a future representing the timer\n+   */\n+  private def startKillTimer(execId: String): ScheduledFuture[_] = {\n+    val killer = new Runnable {\n+      def run(): Unit = {\n+        logDebug(s\"Killing $execId because timeout for recovering cached data has expired\")\n+        kill(execId)\n+      }\n+    }\n+    scheduler.schedule(killer, forceKillAfterS, TimeUnit.SECONDS)\n+  }\n+\n+  /**\n+   * Recover cached RDD blocks off of an executor until there are no more, or until\n+   * there is an error\n+   *\n+   * @param execId the id of the executor to be killed\n+   * @param killTimer The runnable scheduled to kill this executor. Cancel it if we finish before\n+   *                  it does.\n+   */\n+  private def replicateUntilDone(execId: String, killTimer: ScheduledFuture[_]): Unit = {\n+    recoverLatestBlock(execId).onComplete {\n+      case Succ(Some(blockId)) =>\n+        logTrace(s\"Replicated block $blockId on executor $execId\")\n+        replicateUntilDone(execId, killTimer)\n+      case Succ(None) =>\n+        killTimer.cancel(false)\n+        kill(execId)\n+      case Failure(e) =>\n+        logWarning(\"Failure recovering cached data before executor $execId shutdown\", e)\n+        killTimer.cancel(false)\n+        kill(execId)"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "You can avoid this call if the executor has already been removed.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-06T22:19:31Z",
    "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ConcurrentHashMap, ScheduledFuture, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.{Success => Succ}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages._\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executor's cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    blockManagerMasterEndpoint: RpcEndpointRef,\n+    executorAllocationManager: ExecutorAllocationManager,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT)\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"cache-recovery-manager-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"cache-recovery-shutdown-timers\")\n+  private val recoveringExecutors: mutable.Set[String] =\n+    ConcurrentHashMap.newKeySet[String]().asScala\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startCacheRecovery(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    val canBeRecovered = checkMem(execIds)\n+    recoveringExecutors ++= canBeRecovered\n+    val executorsWithKillTimers = canBeRecovered.zip(canBeRecovered.map(startKillTimer))\n+    executorsWithKillTimers.foreach((replicateUntilDone _).tupled)\n+  }\n+\n+  /**\n+   * Given a list of executors that will be shut down, check if there is enough free memory on the\n+   * rest of the cluster to hold their data. Return a list of just the executors for which there\n+   * will be enough space. Executors are included smallest first.\n+   *\n+   * @param execIds executors which will be shut down\n+   * @return a Seq of the executors we do have room for\n+   */\n+  private def checkMem(execIds: Seq[String]): Seq[String] = {\n+    val execsToShutDown = execIds.toSet\n+    // Memory Status is a map of executor Id to a tuple of Max Memory and remaining memory on that\n+    // executor.\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+\n+    val (expiringMemStatus, remainingMemStatus) = allExecMemStatus.partition {\n+      case (execId, _) => execsToShutDown.contains(execId)\n+    }\n+    val freeMemOnRemaining = remainingMemStatus.values.map(_._2).sum\n+\n+    // The used mem on each executor sorted from least used mem to greatest\n+    val executorAndUsedMem: Seq[(String, Long)] =\n+      expiringMemStatus.map { case (execId, (maxMem, remainingMem)) =>\n+        val usedMem = maxMem - remainingMem\n+        execId -> usedMem\n+      }.toSeq.sortBy { case (_, usedMem) => usedMem }\n+\n+    executorAndUsedMem\n+      .scan((\"start\", freeMemOnRemaining)) {\n+        case ((_, freeMem), (execId, usedMem)) => (execId, freeMem - usedMem)\n+      }\n+      .drop(1)\n+      .filter { case (_, freeMem) => freeMem > 0 }\n+      .map(_._1)\n+  }\n+\n+  /**\n+   * Given an executor id, start a timer that will kill the given executor after the configured\n+   * timeout\n+   *\n+   * @param execId The id of the executor to be killed\n+   * @return a future representing the timer\n+   */\n+  private def startKillTimer(execId: String): ScheduledFuture[_] = {\n+    val killer = new Runnable {\n+      def run(): Unit = {\n+        logDebug(s\"Killing $execId because timeout for recovering cached data has expired\")\n+        kill(execId)\n+      }\n+    }\n+    scheduler.schedule(killer, forceKillAfterS, TimeUnit.SECONDS)\n+  }\n+\n+  /**\n+   * Recover cached RDD blocks off of an executor until there are no more, or until\n+   * there is an error\n+   *\n+   * @param execId the id of the executor to be killed\n+   * @param killTimer The runnable scheduled to kill this executor. Cancel it if we finish before\n+   *                  it does.\n+   */\n+  private def replicateUntilDone(execId: String, killTimer: ScheduledFuture[_]): Unit = {\n+    recoverLatestBlock(execId).onComplete {\n+      case Succ(Some(blockId)) =>\n+        logTrace(s\"Replicated block $blockId on executor $execId\")\n+        replicateUntilDone(execId, killTimer)\n+      case Succ(None) =>\n+        killTimer.cancel(false)\n+        kill(execId)\n+      case Failure(e) =>\n+        logWarning(\"Failure recovering cached data before executor $execId shutdown\", e)\n+        killTimer.cancel(false)\n+        kill(execId)\n+    }\n+  }\n+\n+  /**\n+   * Replicate the latest cached rdd block off of this executor on to a surviving executor, and then\n+   * remove the block from this executor\n+   *\n+   * @param execId the executor to recover a block from\n+   * @return A future holding the id of the block that was recovered or None if there were no blocks\n+   *         to recover.\n+   */\n+  private def recoverLatestBlock(execId: String): Future[Option[RDDBlockId]] = {\n+    blockManagerMasterEndpoint\n+      .ask[Option[RDDBlockId]](ReplicateLatestRDDBlock(execId, recoveringExecutors.toSeq))\n+      .andThen {\n+        case Succ(Some(blockId)) =>\n+          blockManagerMasterEndpoint.ask[Boolean](RemoveBlockFromExecutor(execId, blockId))\n+        case _ => // do nothing\n+      }\n+  }\n+\n+  /**\n+   * Remove the executor from the list of currently recovering executors and then kill it.\n+   *\n+   * @param execId the id of the executor to be killed\n+   */\n+  private def kill(execId: String): Unit = {\n+    recoveringExecutors.remove(execId)\n+    executorAllocationManager.killExecutors(Seq(execId))"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "Not sure what you are recommending here. I take care not to call this method twice in CacheRecoveryManager. Should I be keep track of the executors i have killed in the past?",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-18T20:45:38Z",
    "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ConcurrentHashMap, ScheduledFuture, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.{Success => Succ}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages._\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executor's cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    blockManagerMasterEndpoint: RpcEndpointRef,\n+    executorAllocationManager: ExecutorAllocationManager,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT)\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"cache-recovery-manager-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"cache-recovery-shutdown-timers\")\n+  private val recoveringExecutors: mutable.Set[String] =\n+    ConcurrentHashMap.newKeySet[String]().asScala\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startCacheRecovery(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    val canBeRecovered = checkMem(execIds)\n+    recoveringExecutors ++= canBeRecovered\n+    val executorsWithKillTimers = canBeRecovered.zip(canBeRecovered.map(startKillTimer))\n+    executorsWithKillTimers.foreach((replicateUntilDone _).tupled)\n+  }\n+\n+  /**\n+   * Given a list of executors that will be shut down, check if there is enough free memory on the\n+   * rest of the cluster to hold their data. Return a list of just the executors for which there\n+   * will be enough space. Executors are included smallest first.\n+   *\n+   * @param execIds executors which will be shut down\n+   * @return a Seq of the executors we do have room for\n+   */\n+  private def checkMem(execIds: Seq[String]): Seq[String] = {\n+    val execsToShutDown = execIds.toSet\n+    // Memory Status is a map of executor Id to a tuple of Max Memory and remaining memory on that\n+    // executor.\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+\n+    val (expiringMemStatus, remainingMemStatus) = allExecMemStatus.partition {\n+      case (execId, _) => execsToShutDown.contains(execId)\n+    }\n+    val freeMemOnRemaining = remainingMemStatus.values.map(_._2).sum\n+\n+    // The used mem on each executor sorted from least used mem to greatest\n+    val executorAndUsedMem: Seq[(String, Long)] =\n+      expiringMemStatus.map { case (execId, (maxMem, remainingMem)) =>\n+        val usedMem = maxMem - remainingMem\n+        execId -> usedMem\n+      }.toSeq.sortBy { case (_, usedMem) => usedMem }\n+\n+    executorAndUsedMem\n+      .scan((\"start\", freeMemOnRemaining)) {\n+        case ((_, freeMem), (execId, usedMem)) => (execId, freeMem - usedMem)\n+      }\n+      .drop(1)\n+      .filter { case (_, freeMem) => freeMem > 0 }\n+      .map(_._1)\n+  }\n+\n+  /**\n+   * Given an executor id, start a timer that will kill the given executor after the configured\n+   * timeout\n+   *\n+   * @param execId The id of the executor to be killed\n+   * @return a future representing the timer\n+   */\n+  private def startKillTimer(execId: String): ScheduledFuture[_] = {\n+    val killer = new Runnable {\n+      def run(): Unit = {\n+        logDebug(s\"Killing $execId because timeout for recovering cached data has expired\")\n+        kill(execId)\n+      }\n+    }\n+    scheduler.schedule(killer, forceKillAfterS, TimeUnit.SECONDS)\n+  }\n+\n+  /**\n+   * Recover cached RDD blocks off of an executor until there are no more, or until\n+   * there is an error\n+   *\n+   * @param execId the id of the executor to be killed\n+   * @param killTimer The runnable scheduled to kill this executor. Cancel it if we finish before\n+   *                  it does.\n+   */\n+  private def replicateUntilDone(execId: String, killTimer: ScheduledFuture[_]): Unit = {\n+    recoverLatestBlock(execId).onComplete {\n+      case Succ(Some(blockId)) =>\n+        logTrace(s\"Replicated block $blockId on executor $execId\")\n+        replicateUntilDone(execId, killTimer)\n+      case Succ(None) =>\n+        killTimer.cancel(false)\n+        kill(execId)\n+      case Failure(e) =>\n+        logWarning(\"Failure recovering cached data before executor $execId shutdown\", e)\n+        killTimer.cancel(false)\n+        kill(execId)\n+    }\n+  }\n+\n+  /**\n+   * Replicate the latest cached rdd block off of this executor on to a surviving executor, and then\n+   * remove the block from this executor\n+   *\n+   * @param execId the executor to recover a block from\n+   * @return A future holding the id of the block that was recovered or None if there were no blocks\n+   *         to recover.\n+   */\n+  private def recoverLatestBlock(execId: String): Future[Option[RDDBlockId]] = {\n+    blockManagerMasterEndpoint\n+      .ask[Option[RDDBlockId]](ReplicateLatestRDDBlock(execId, recoveringExecutors.toSeq))\n+      .andThen {\n+        case Succ(Some(blockId)) =>\n+          blockManagerMasterEndpoint.ask[Boolean](RemoveBlockFromExecutor(execId, blockId))\n+        case _ => // do nothing\n+      }\n+  }\n+\n+  /**\n+   * Remove the executor from the list of currently recovering executors and then kill it.\n+   *\n+   * @param execId the id of the executor to be killed\n+   */\n+  private def kill(execId: String): Unit = {\n+    recoveringExecutors.remove(execId)\n+    executorAllocationManager.killExecutors(Seq(execId))"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Don't you need this call to be synchronous for things to work? (I guess the RPC layer enforces ordering for single-threaded endpoints, but I don't think this would work if the block manager was multi-threaded.)",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-06T22:46:56Z",
    "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ConcurrentHashMap, ScheduledFuture, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.{Success => Succ}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages._\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executor's cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    blockManagerMasterEndpoint: RpcEndpointRef,\n+    executorAllocationManager: ExecutorAllocationManager,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT)\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"cache-recovery-manager-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"cache-recovery-shutdown-timers\")\n+  private val recoveringExecutors: mutable.Set[String] =\n+    ConcurrentHashMap.newKeySet[String]().asScala\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startCacheRecovery(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    val canBeRecovered = checkMem(execIds)\n+    recoveringExecutors ++= canBeRecovered\n+    val executorsWithKillTimers = canBeRecovered.zip(canBeRecovered.map(startKillTimer))\n+    executorsWithKillTimers.foreach((replicateUntilDone _).tupled)\n+  }\n+\n+  /**\n+   * Given a list of executors that will be shut down, check if there is enough free memory on the\n+   * rest of the cluster to hold their data. Return a list of just the executors for which there\n+   * will be enough space. Executors are included smallest first.\n+   *\n+   * @param execIds executors which will be shut down\n+   * @return a Seq of the executors we do have room for\n+   */\n+  private def checkMem(execIds: Seq[String]): Seq[String] = {\n+    val execsToShutDown = execIds.toSet\n+    // Memory Status is a map of executor Id to a tuple of Max Memory and remaining memory on that\n+    // executor.\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+\n+    val (expiringMemStatus, remainingMemStatus) = allExecMemStatus.partition {\n+      case (execId, _) => execsToShutDown.contains(execId)\n+    }\n+    val freeMemOnRemaining = remainingMemStatus.values.map(_._2).sum\n+\n+    // The used mem on each executor sorted from least used mem to greatest\n+    val executorAndUsedMem: Seq[(String, Long)] =\n+      expiringMemStatus.map { case (execId, (maxMem, remainingMem)) =>\n+        val usedMem = maxMem - remainingMem\n+        execId -> usedMem\n+      }.toSeq.sortBy { case (_, usedMem) => usedMem }\n+\n+    executorAndUsedMem\n+      .scan((\"start\", freeMemOnRemaining)) {\n+        case ((_, freeMem), (execId, usedMem)) => (execId, freeMem - usedMem)\n+      }\n+      .drop(1)\n+      .filter { case (_, freeMem) => freeMem > 0 }\n+      .map(_._1)\n+  }\n+\n+  /**\n+   * Given an executor id, start a timer that will kill the given executor after the configured\n+   * timeout\n+   *\n+   * @param execId The id of the executor to be killed\n+   * @return a future representing the timer\n+   */\n+  private def startKillTimer(execId: String): ScheduledFuture[_] = {\n+    val killer = new Runnable {\n+      def run(): Unit = {\n+        logDebug(s\"Killing $execId because timeout for recovering cached data has expired\")\n+        kill(execId)\n+      }\n+    }\n+    scheduler.schedule(killer, forceKillAfterS, TimeUnit.SECONDS)\n+  }\n+\n+  /**\n+   * Recover cached RDD blocks off of an executor until there are no more, or until\n+   * there is an error\n+   *\n+   * @param execId the id of the executor to be killed\n+   * @param killTimer The runnable scheduled to kill this executor. Cancel it if we finish before\n+   *                  it does.\n+   */\n+  private def replicateUntilDone(execId: String, killTimer: ScheduledFuture[_]): Unit = {\n+    recoverLatestBlock(execId).onComplete {\n+      case Succ(Some(blockId)) =>\n+        logTrace(s\"Replicated block $blockId on executor $execId\")\n+        replicateUntilDone(execId, killTimer)\n+      case Succ(None) =>\n+        killTimer.cancel(false)\n+        kill(execId)\n+      case Failure(e) =>\n+        logWarning(\"Failure recovering cached data before executor $execId shutdown\", e)\n+        killTimer.cancel(false)\n+        kill(execId)\n+    }\n+  }\n+\n+  /**\n+   * Replicate the latest cached rdd block off of this executor on to a surviving executor, and then\n+   * remove the block from this executor\n+   *\n+   * @param execId the executor to recover a block from\n+   * @return A future holding the id of the block that was recovered or None if there were no blocks\n+   *         to recover.\n+   */\n+  private def recoverLatestBlock(execId: String): Future[Option[RDDBlockId]] = {\n+    blockManagerMasterEndpoint\n+      .ask[Option[RDDBlockId]](ReplicateLatestRDDBlock(execId, recoveringExecutors.toSeq))\n+      .andThen {\n+        case Succ(Some(blockId)) =>\n+          blockManagerMasterEndpoint.ask[Boolean](RemoveBlockFromExecutor(execId, blockId))"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "I think this would be fine, the block removal would always happen after the replicate was done, but I think I'm just going to merge those two messages based on your later comment. ",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-12T16:30:56Z",
    "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ConcurrentHashMap, ScheduledFuture, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.{Success => Succ}\n+import scala.util.Failure\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.{BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages._\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executor's cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    blockManagerMasterEndpoint: RpcEndpointRef,\n+    executorAllocationManager: ExecutorAllocationManager,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT)\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"cache-recovery-manager-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"cache-recovery-shutdown-timers\")\n+  private val recoveringExecutors: mutable.Set[String] =\n+    ConcurrentHashMap.newKeySet[String]().asScala\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   */\n+  def startCacheRecovery(execIds: Seq[String]): Unit = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    val canBeRecovered = checkMem(execIds)\n+    recoveringExecutors ++= canBeRecovered\n+    val executorsWithKillTimers = canBeRecovered.zip(canBeRecovered.map(startKillTimer))\n+    executorsWithKillTimers.foreach((replicateUntilDone _).tupled)\n+  }\n+\n+  /**\n+   * Given a list of executors that will be shut down, check if there is enough free memory on the\n+   * rest of the cluster to hold their data. Return a list of just the executors for which there\n+   * will be enough space. Executors are included smallest first.\n+   *\n+   * @param execIds executors which will be shut down\n+   * @return a Seq of the executors we do have room for\n+   */\n+  private def checkMem(execIds: Seq[String]): Seq[String] = {\n+    val execsToShutDown = execIds.toSet\n+    // Memory Status is a map of executor Id to a tuple of Max Memory and remaining memory on that\n+    // executor.\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+\n+    val (expiringMemStatus, remainingMemStatus) = allExecMemStatus.partition {\n+      case (execId, _) => execsToShutDown.contains(execId)\n+    }\n+    val freeMemOnRemaining = remainingMemStatus.values.map(_._2).sum\n+\n+    // The used mem on each executor sorted from least used mem to greatest\n+    val executorAndUsedMem: Seq[(String, Long)] =\n+      expiringMemStatus.map { case (execId, (maxMem, remainingMem)) =>\n+        val usedMem = maxMem - remainingMem\n+        execId -> usedMem\n+      }.toSeq.sortBy { case (_, usedMem) => usedMem }\n+\n+    executorAndUsedMem\n+      .scan((\"start\", freeMemOnRemaining)) {\n+        case ((_, freeMem), (execId, usedMem)) => (execId, freeMem - usedMem)\n+      }\n+      .drop(1)\n+      .filter { case (_, freeMem) => freeMem > 0 }\n+      .map(_._1)\n+  }\n+\n+  /**\n+   * Given an executor id, start a timer that will kill the given executor after the configured\n+   * timeout\n+   *\n+   * @param execId The id of the executor to be killed\n+   * @return a future representing the timer\n+   */\n+  private def startKillTimer(execId: String): ScheduledFuture[_] = {\n+    val killer = new Runnable {\n+      def run(): Unit = {\n+        logDebug(s\"Killing $execId because timeout for recovering cached data has expired\")\n+        kill(execId)\n+      }\n+    }\n+    scheduler.schedule(killer, forceKillAfterS, TimeUnit.SECONDS)\n+  }\n+\n+  /**\n+   * Recover cached RDD blocks off of an executor until there are no more, or until\n+   * there is an error\n+   *\n+   * @param execId the id of the executor to be killed\n+   * @param killTimer The runnable scheduled to kill this executor. Cancel it if we finish before\n+   *                  it does.\n+   */\n+  private def replicateUntilDone(execId: String, killTimer: ScheduledFuture[_]): Unit = {\n+    recoverLatestBlock(execId).onComplete {\n+      case Succ(Some(blockId)) =>\n+        logTrace(s\"Replicated block $blockId on executor $execId\")\n+        replicateUntilDone(execId, killTimer)\n+      case Succ(None) =>\n+        killTimer.cancel(false)\n+        kill(execId)\n+      case Failure(e) =>\n+        logWarning(\"Failure recovering cached data before executor $execId shutdown\", e)\n+        killTimer.cancel(false)\n+        kill(execId)\n+    }\n+  }\n+\n+  /**\n+   * Replicate the latest cached rdd block off of this executor on to a surviving executor, and then\n+   * remove the block from this executor\n+   *\n+   * @param execId the executor to recover a block from\n+   * @return A future holding the id of the block that was recovered or None if there were no blocks\n+   *         to recover.\n+   */\n+  private def recoverLatestBlock(execId: String): Future[Option[RDDBlockId]] = {\n+    blockManagerMasterEndpoint\n+      .ask[Option[RDDBlockId]](ReplicateLatestRDDBlock(execId, recoveringExecutors.toSeq))\n+      .andThen {\n+        case Succ(Some(blockId)) =>\n+          blockManagerMasterEndpoint.ask[Boolean](RemoveBlockFromExecutor(execId, blockId))"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "It's very rare for us to mark a class as final, especially a private one. Is there a reason for that?\r\n\r\nIt makes other things (like mocking the class in tests) more complicated, for example.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2018-01-22T19:24:16Z",
    "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import com.google.common.cache.CacheBuilder\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.BlockManagerId\n+import org.apache.spark.storage.BlockManagerMessages._\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executor's cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager("
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "Makes sense, removed the final.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2018-02-07T02:43:56Z",
    "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import com.google.common.cache.CacheBuilder\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.BlockManagerId\n+import org.apache.spark.storage.BlockManagerMessages._\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executor's cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager("
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "This fits in one line if you drop the type, which is kinda redundant in this context anyway.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2018-01-22T19:26:07Z",
    "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import com.google.common.cache.CacheBuilder\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.BlockManagerId\n+import org.apache.spark.storage.BlockManagerMessages._\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executor's cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    blockManagerMasterEndpoint: RpcEndpointRef,\n+    executorAllocationManager: ExecutorAllocationManager,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT)\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"cache-recovery-manager-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext ="
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "removed",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2018-02-07T02:44:24Z",
    "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import com.google.common.cache.CacheBuilder\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.BlockManagerId\n+import org.apache.spark.storage.BlockManagerMessages._\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executor's cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    blockManagerMasterEndpoint: RpcEndpointRef,\n+    executorAllocationManager: ExecutorAllocationManager,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT)\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"cache-recovery-manager-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext ="
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "> // Cache value never used and must extend Object\r\n\r\nJust use `Any` then? Another more common approach is to map the key to itself in these cases.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2018-01-22T19:27:53Z",
    "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import com.google.common.cache.CacheBuilder\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.BlockManagerId\n+import org.apache.spark.storage.BlockManagerMessages._\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executor's cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    blockManagerMasterEndpoint: RpcEndpointRef,\n+    executorAllocationManager: ExecutorAllocationManager,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT)\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"cache-recovery-manager-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"cache-recovery-shutdown-timers\")\n+  private val recoveringExecutors = CacheBuilder.newBuilder()\n+    .expireAfterWrite(forceKillAfterS * 2, TimeUnit.SECONDS)\n+    .build[String, java.lang.Boolean]() // Cache value never used and must extend Object"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "Good idea about making the key map to itself. Thanks.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2018-02-07T02:45:24Z",
    "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import com.google.common.cache.CacheBuilder\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.BlockManagerId\n+import org.apache.spark.storage.BlockManagerMessages._\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executor's cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    blockManagerMasterEndpoint: RpcEndpointRef,\n+    executorAllocationManager: ExecutorAllocationManager,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT)\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"cache-recovery-manager-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"cache-recovery-shutdown-timers\")\n+  private val recoveringExecutors = CacheBuilder.newBuilder()\n+    .expireAfterWrite(forceKillAfterS * 2, TimeUnit.SECONDS)\n+    .build[String, java.lang.Boolean]() // Cache value never used and must extend Object"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "I'm a little worried about this call. It's called from a synchronized block in the executor allocation manager, which means you'll block that object until the RPC finishes.\r\n\r\nIt seems to be a process-local RPC, so it's faster, but this is still kind of a sketchy thing to do (can it deadlock? can it take a while to finish? etc etc).\r\n\r\nIs there an easy way you could make the `startCacheRecovery` method asynchronous?",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2018-01-22T19:35:43Z",
    "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import com.google.common.cache.CacheBuilder\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.BlockManagerId\n+import org.apache.spark.storage.BlockManagerMessages._\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executor's cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    blockManagerMasterEndpoint: RpcEndpointRef,\n+    executorAllocationManager: ExecutorAllocationManager,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT)\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"cache-recovery-manager-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"cache-recovery-shutdown-timers\")\n+  private val recoveringExecutors = CacheBuilder.newBuilder()\n+    .expireAfterWrite(forceKillAfterS * 2, TimeUnit.SECONDS)\n+    .build[String, java.lang.Boolean]() // Cache value never used and must extend Object\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   * @return a sequence of futures representing the kill process.\n+   */\n+  def startCacheRecovery(execIds: Seq[String]): Seq[Future[Boolean]] = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    val canBeRecovered = checkMem(execIds)\n+    canBeRecovered.foreach { execId => recoveringExecutors.put(execId, true)}\n+    canBeRecovered.map { execId =>\n+      val killTimer = startKillTimer(execId)\n+      replicateUntilDone(execId)\n+        .andThen {\n+          // don't log if kill timer is done, because it would be redundant\n+          case Failure(e) if !killTimer.isDone =>\n+              logWarning(s\"Failure recovering cached data before executor $execId shutdown\", e)\n+        }\n+        .andThen {\n+          case _ =>\n+            if (killTimer.cancel(false)) { kill(execId) }\n+        }\n+    }\n+  }\n+\n+  /**\n+   * Given a list of executors that will be shut down, check if there is enough free memory on the\n+   * rest of the cluster to hold their data. Return a list of just the executors for which there\n+   * will be enough space. Executors are included smallest first.\n+   *\n+   * This is a best guess implementation and it is not guaranteed that all returned executors\n+   * will succeed. For example a block might be too big to fit on any one specific executor.\n+   *\n+   * @param execIds executors which will be shut down\n+   * @return a Seq of the executors we do have room for\n+   */\n+  private def checkMem(execIds: Seq[String]): Seq[String] = {\n+    val execsToShutDown = execIds.toSet\n+    // Memory Status is a map of executor Id to a tuple of Max Memory and remaining memory on that\n+    // executor.\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "made this asynchronous",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2018-03-27T19:25:10Z",
    "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import com.google.common.cache.CacheBuilder\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.BlockManagerId\n+import org.apache.spark.storage.BlockManagerMessages._\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executor's cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    blockManagerMasterEndpoint: RpcEndpointRef,\n+    executorAllocationManager: ExecutorAllocationManager,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT)\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"cache-recovery-manager-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"cache-recovery-shutdown-timers\")\n+  private val recoveringExecutors = CacheBuilder.newBuilder()\n+    .expireAfterWrite(forceKillAfterS * 2, TimeUnit.SECONDS)\n+    .build[String, java.lang.Boolean]() // Cache value never used and must extend Object\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   * @return a sequence of futures representing the kill process.\n+   */\n+  def startCacheRecovery(execIds: Seq[String]): Seq[Future[Boolean]] = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    val canBeRecovered = checkMem(execIds)\n+    canBeRecovered.foreach { execId => recoveringExecutors.put(execId, true)}\n+    canBeRecovered.map { execId =>\n+      val killTimer = startKillTimer(execId)\n+      replicateUntilDone(execId)\n+        .andThen {\n+          // don't log if kill timer is done, because it would be redundant\n+          case Failure(e) if !killTimer.isDone =>\n+              logWarning(s\"Failure recovering cached data before executor $execId shutdown\", e)\n+        }\n+        .andThen {\n+          case _ =>\n+            if (killTimer.cancel(false)) { kill(execId) }\n+        }\n+    }\n+  }\n+\n+  /**\n+   * Given a list of executors that will be shut down, check if there is enough free memory on the\n+   * rest of the cluster to hold their data. Return a list of just the executors for which there\n+   * will be enough space. Executors are included smallest first.\n+   *\n+   * This is a best guess implementation and it is not guaranteed that all returned executors\n+   * will succeed. For example a block might be too big to fit on any one specific executor.\n+   *\n+   * @param execIds executors which will be shut down\n+   * @return a Seq of the executors we do have room for\n+   */\n+  private def checkMem(execIds: Seq[String]): Seq[String] = {\n+    val execsToShutDown = execIds.toSet\n+    // Memory Status is a map of executor Id to a tuple of Max Memory and remaining memory on that\n+    // executor.\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Block is indented too far.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2018-01-22T19:37:06Z",
    "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import com.google.common.cache.CacheBuilder\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.BlockManagerId\n+import org.apache.spark.storage.BlockManagerMessages._\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executor's cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    blockManagerMasterEndpoint: RpcEndpointRef,\n+    executorAllocationManager: ExecutorAllocationManager,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT)\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"cache-recovery-manager-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"cache-recovery-shutdown-timers\")\n+  private val recoveringExecutors = CacheBuilder.newBuilder()\n+    .expireAfterWrite(forceKillAfterS * 2, TimeUnit.SECONDS)\n+    .build[String, java.lang.Boolean]() // Cache value never used and must extend Object\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   * @return a sequence of futures representing the kill process.\n+   */\n+  def startCacheRecovery(execIds: Seq[String]): Seq[Future[Boolean]] = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    val canBeRecovered = checkMem(execIds)\n+    canBeRecovered.foreach { execId => recoveringExecutors.put(execId, true)}\n+    canBeRecovered.map { execId =>\n+      val killTimer = startKillTimer(execId)\n+      replicateUntilDone(execId)\n+        .andThen {\n+          // don't log if kill timer is done, because it would be redundant\n+          case Failure(e) if !killTimer.isDone =>\n+              logWarning(s\"Failure recovering cached data before executor $execId shutdown\", e)\n+        }\n+        .andThen {\n+          case _ =>\n+            if (killTimer.cancel(false)) { kill(execId) }\n+        }\n+    }\n+  }\n+\n+  /**\n+   * Given a list of executors that will be shut down, check if there is enough free memory on the\n+   * rest of the cluster to hold their data. Return a list of just the executors for which there\n+   * will be enough space. Executors are included smallest first.\n+   *\n+   * This is a best guess implementation and it is not guaranteed that all returned executors\n+   * will succeed. For example a block might be too big to fit on any one specific executor.\n+   *\n+   * @param execIds executors which will be shut down\n+   * @return a Seq of the executors we do have room for\n+   */\n+  private def checkMem(execIds: Seq[String]): Seq[String] = {\n+    val execsToShutDown = execIds.toSet\n+    // Memory Status is a map of executor Id to a tuple of Max Memory and remaining memory on that\n+    // executor.\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+\n+    val (expiringMemStatus, remainingMemStatus) = allExecMemStatus.partition {\n+      case (execId, _) => execsToShutDown.contains(execId)\n+    }\n+    val freeMemOnRemaining = remainingMemStatus.values.map(_._2).sum\n+\n+    // The used mem on each executor sorted from least used mem to greatest\n+    val executorAndUsedMem: Seq[(String, Long)] =\n+      expiringMemStatus.map { case (execId, (maxMem, remainingMem)) =>\n+        val usedMem = maxMem - remainingMem\n+        execId -> usedMem\n+      }.toSeq.sortBy { case (_, usedMem) => usedMem }\n+\n+    executorAndUsedMem\n+      .scan((\"start\", freeMemOnRemaining)) {\n+        case ((_, freeMem), (execId, usedMem)) => (execId, freeMem - usedMem)\n+      }\n+      .drop(1)\n+      .filter { case (_, freeMem) => freeMem > 0 }\n+      .map(_._1)\n+  }\n+\n+  /**\n+   * Given an executor id, start a timer that will kill the given executor after the configured\n+   * timeout\n+   *\n+   * @param execId The id of the executor to be killed\n+   * @return a future representing the timer\n+   */\n+  private def startKillTimer(execId: String): ScheduledFuture[_] = {\n+    val killer = new Runnable {\n+      def run(): Unit = {\n+        logDebug(s\"Killing $execId because timeout for recovering cached data has expired\")\n+        kill(execId)\n+      }\n+    }\n+    scheduler.schedule(killer, forceKillAfterS, TimeUnit.SECONDS)\n+  }\n+\n+  /**\n+   * Recover cached RDD blocks off of an executor until there are no more, or until\n+   * there is an error\n+   *\n+   * @param execId the id of the executor to be killed\n+   * @return a Future of Boolean that returns true if a block was copied and false if there were\n+   *         no more blocks\n+   */\n+  private def replicateUntilDone(execId: String): Future[Boolean] = {\n+      recoverLatestBlock(execId).flatMap { moreBlocks =>"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "oops fixed",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2018-02-07T03:01:18Z",
    "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import com.google.common.cache.CacheBuilder\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.BlockManagerId\n+import org.apache.spark.storage.BlockManagerMessages._\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executor's cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    blockManagerMasterEndpoint: RpcEndpointRef,\n+    executorAllocationManager: ExecutorAllocationManager,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT)\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"cache-recovery-manager-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"cache-recovery-shutdown-timers\")\n+  private val recoveringExecutors = CacheBuilder.newBuilder()\n+    .expireAfterWrite(forceKillAfterS * 2, TimeUnit.SECONDS)\n+    .build[String, java.lang.Boolean]() // Cache value never used and must extend Object\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   * @return a sequence of futures representing the kill process.\n+   */\n+  def startCacheRecovery(execIds: Seq[String]): Seq[Future[Boolean]] = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    val canBeRecovered = checkMem(execIds)\n+    canBeRecovered.foreach { execId => recoveringExecutors.put(execId, true)}\n+    canBeRecovered.map { execId =>\n+      val killTimer = startKillTimer(execId)\n+      replicateUntilDone(execId)\n+        .andThen {\n+          // don't log if kill timer is done, because it would be redundant\n+          case Failure(e) if !killTimer.isDone =>\n+              logWarning(s\"Failure recovering cached data before executor $execId shutdown\", e)\n+        }\n+        .andThen {\n+          case _ =>\n+            if (killTimer.cancel(false)) { kill(execId) }\n+        }\n+    }\n+  }\n+\n+  /**\n+   * Given a list of executors that will be shut down, check if there is enough free memory on the\n+   * rest of the cluster to hold their data. Return a list of just the executors for which there\n+   * will be enough space. Executors are included smallest first.\n+   *\n+   * This is a best guess implementation and it is not guaranteed that all returned executors\n+   * will succeed. For example a block might be too big to fit on any one specific executor.\n+   *\n+   * @param execIds executors which will be shut down\n+   * @return a Seq of the executors we do have room for\n+   */\n+  private def checkMem(execIds: Seq[String]): Seq[String] = {\n+    val execsToShutDown = execIds.toSet\n+    // Memory Status is a map of executor Id to a tuple of Max Memory and remaining memory on that\n+    // executor.\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+\n+    val (expiringMemStatus, remainingMemStatus) = allExecMemStatus.partition {\n+      case (execId, _) => execsToShutDown.contains(execId)\n+    }\n+    val freeMemOnRemaining = remainingMemStatus.values.map(_._2).sum\n+\n+    // The used mem on each executor sorted from least used mem to greatest\n+    val executorAndUsedMem: Seq[(String, Long)] =\n+      expiringMemStatus.map { case (execId, (maxMem, remainingMem)) =>\n+        val usedMem = maxMem - remainingMem\n+        execId -> usedMem\n+      }.toSeq.sortBy { case (_, usedMem) => usedMem }\n+\n+    executorAndUsedMem\n+      .scan((\"start\", freeMemOnRemaining)) {\n+        case ((_, freeMem), (execId, usedMem)) => (execId, freeMem - usedMem)\n+      }\n+      .drop(1)\n+      .filter { case (_, freeMem) => freeMem > 0 }\n+      .map(_._1)\n+  }\n+\n+  /**\n+   * Given an executor id, start a timer that will kill the given executor after the configured\n+   * timeout\n+   *\n+   * @param execId The id of the executor to be killed\n+   * @return a future representing the timer\n+   */\n+  private def startKillTimer(execId: String): ScheduledFuture[_] = {\n+    val killer = new Runnable {\n+      def run(): Unit = {\n+        logDebug(s\"Killing $execId because timeout for recovering cached data has expired\")\n+        kill(execId)\n+      }\n+    }\n+    scheduler.schedule(killer, forceKillAfterS, TimeUnit.SECONDS)\n+  }\n+\n+  /**\n+   * Recover cached RDD blocks off of an executor until there are no more, or until\n+   * there is an error\n+   *\n+   * @param execId the id of the executor to be killed\n+   * @return a Future of Boolean that returns true if a block was copied and false if there were\n+   *         no more blocks\n+   */\n+  private def replicateUntilDone(execId: String): Future[Boolean] = {\n+      recoverLatestBlock(execId).flatMap { moreBlocks =>"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "The return value description sounds weird (two different unrelated things for true and false).\r\n\r\nMaybe just: A future that returns whether there are more blocks to replicate.\r\n\r\nAlthough you don't really use that return value outside of this method. So you could just return a `Future[Unit]` here, and handle the boolean internally.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2018-01-22T19:39:14Z",
    "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import com.google.common.cache.CacheBuilder\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.BlockManagerId\n+import org.apache.spark.storage.BlockManagerMessages._\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executor's cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    blockManagerMasterEndpoint: RpcEndpointRef,\n+    executorAllocationManager: ExecutorAllocationManager,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT)\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"cache-recovery-manager-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"cache-recovery-shutdown-timers\")\n+  private val recoveringExecutors = CacheBuilder.newBuilder()\n+    .expireAfterWrite(forceKillAfterS * 2, TimeUnit.SECONDS)\n+    .build[String, java.lang.Boolean]() // Cache value never used and must extend Object\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   * @return a sequence of futures representing the kill process.\n+   */\n+  def startCacheRecovery(execIds: Seq[String]): Seq[Future[Boolean]] = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    val canBeRecovered = checkMem(execIds)\n+    canBeRecovered.foreach { execId => recoveringExecutors.put(execId, true)}\n+    canBeRecovered.map { execId =>\n+      val killTimer = startKillTimer(execId)\n+      replicateUntilDone(execId)\n+        .andThen {\n+          // don't log if kill timer is done, because it would be redundant\n+          case Failure(e) if !killTimer.isDone =>\n+              logWarning(s\"Failure recovering cached data before executor $execId shutdown\", e)\n+        }\n+        .andThen {\n+          case _ =>\n+            if (killTimer.cancel(false)) { kill(execId) }\n+        }\n+    }\n+  }\n+\n+  /**\n+   * Given a list of executors that will be shut down, check if there is enough free memory on the\n+   * rest of the cluster to hold their data. Return a list of just the executors for which there\n+   * will be enough space. Executors are included smallest first.\n+   *\n+   * This is a best guess implementation and it is not guaranteed that all returned executors\n+   * will succeed. For example a block might be too big to fit on any one specific executor.\n+   *\n+   * @param execIds executors which will be shut down\n+   * @return a Seq of the executors we do have room for\n+   */\n+  private def checkMem(execIds: Seq[String]): Seq[String] = {\n+    val execsToShutDown = execIds.toSet\n+    // Memory Status is a map of executor Id to a tuple of Max Memory and remaining memory on that\n+    // executor.\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+\n+    val (expiringMemStatus, remainingMemStatus) = allExecMemStatus.partition {\n+      case (execId, _) => execsToShutDown.contains(execId)\n+    }\n+    val freeMemOnRemaining = remainingMemStatus.values.map(_._2).sum\n+\n+    // The used mem on each executor sorted from least used mem to greatest\n+    val executorAndUsedMem: Seq[(String, Long)] =\n+      expiringMemStatus.map { case (execId, (maxMem, remainingMem)) =>\n+        val usedMem = maxMem - remainingMem\n+        execId -> usedMem\n+      }.toSeq.sortBy { case (_, usedMem) => usedMem }\n+\n+    executorAndUsedMem\n+      .scan((\"start\", freeMemOnRemaining)) {\n+        case ((_, freeMem), (execId, usedMem)) => (execId, freeMem - usedMem)\n+      }\n+      .drop(1)\n+      .filter { case (_, freeMem) => freeMem > 0 }\n+      .map(_._1)\n+  }\n+\n+  /**\n+   * Given an executor id, start a timer that will kill the given executor after the configured\n+   * timeout\n+   *\n+   * @param execId The id of the executor to be killed\n+   * @return a future representing the timer\n+   */\n+  private def startKillTimer(execId: String): ScheduledFuture[_] = {\n+    val killer = new Runnable {\n+      def run(): Unit = {\n+        logDebug(s\"Killing $execId because timeout for recovering cached data has expired\")\n+        kill(execId)\n+      }\n+    }\n+    scheduler.schedule(killer, forceKillAfterS, TimeUnit.SECONDS)\n+  }\n+\n+  /**\n+   * Recover cached RDD blocks off of an executor until there are no more, or until\n+   * there is an error\n+   *\n+   * @param execId the id of the executor to be killed\n+   * @return a Future of Boolean that returns true if a block was copied and false if there were"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "Made it a Future[Unit]",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2018-02-07T02:47:40Z",
    "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import com.google.common.cache.CacheBuilder\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.BlockManagerId\n+import org.apache.spark.storage.BlockManagerMessages._\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executor's cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    blockManagerMasterEndpoint: RpcEndpointRef,\n+    executorAllocationManager: ExecutorAllocationManager,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT)\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"cache-recovery-manager-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"cache-recovery-shutdown-timers\")\n+  private val recoveringExecutors = CacheBuilder.newBuilder()\n+    .expireAfterWrite(forceKillAfterS * 2, TimeUnit.SECONDS)\n+    .build[String, java.lang.Boolean]() // Cache value never used and must extend Object\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   * @return a sequence of futures representing the kill process.\n+   */\n+  def startCacheRecovery(execIds: Seq[String]): Seq[Future[Boolean]] = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    val canBeRecovered = checkMem(execIds)\n+    canBeRecovered.foreach { execId => recoveringExecutors.put(execId, true)}\n+    canBeRecovered.map { execId =>\n+      val killTimer = startKillTimer(execId)\n+      replicateUntilDone(execId)\n+        .andThen {\n+          // don't log if kill timer is done, because it would be redundant\n+          case Failure(e) if !killTimer.isDone =>\n+              logWarning(s\"Failure recovering cached data before executor $execId shutdown\", e)\n+        }\n+        .andThen {\n+          case _ =>\n+            if (killTimer.cancel(false)) { kill(execId) }\n+        }\n+    }\n+  }\n+\n+  /**\n+   * Given a list of executors that will be shut down, check if there is enough free memory on the\n+   * rest of the cluster to hold their data. Return a list of just the executors for which there\n+   * will be enough space. Executors are included smallest first.\n+   *\n+   * This is a best guess implementation and it is not guaranteed that all returned executors\n+   * will succeed. For example a block might be too big to fit on any one specific executor.\n+   *\n+   * @param execIds executors which will be shut down\n+   * @return a Seq of the executors we do have room for\n+   */\n+  private def checkMem(execIds: Seq[String]): Seq[String] = {\n+    val execsToShutDown = execIds.toSet\n+    // Memory Status is a map of executor Id to a tuple of Max Memory and remaining memory on that\n+    // executor.\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+\n+    val (expiringMemStatus, remainingMemStatus) = allExecMemStatus.partition {\n+      case (execId, _) => execsToShutDown.contains(execId)\n+    }\n+    val freeMemOnRemaining = remainingMemStatus.values.map(_._2).sum\n+\n+    // The used mem on each executor sorted from least used mem to greatest\n+    val executorAndUsedMem: Seq[(String, Long)] =\n+      expiringMemStatus.map { case (execId, (maxMem, remainingMem)) =>\n+        val usedMem = maxMem - remainingMem\n+        execId -> usedMem\n+      }.toSeq.sortBy { case (_, usedMem) => usedMem }\n+\n+    executorAndUsedMem\n+      .scan((\"start\", freeMemOnRemaining)) {\n+        case ((_, freeMem), (execId, usedMem)) => (execId, freeMem - usedMem)\n+      }\n+      .drop(1)\n+      .filter { case (_, freeMem) => freeMem > 0 }\n+      .map(_._1)\n+  }\n+\n+  /**\n+   * Given an executor id, start a timer that will kill the given executor after the configured\n+   * timeout\n+   *\n+   * @param execId The id of the executor to be killed\n+   * @return a future representing the timer\n+   */\n+  private def startKillTimer(execId: String): ScheduledFuture[_] = {\n+    val killer = new Runnable {\n+      def run(): Unit = {\n+        logDebug(s\"Killing $execId because timeout for recovering cached data has expired\")\n+        kill(execId)\n+      }\n+    }\n+    scheduler.schedule(killer, forceKillAfterS, TimeUnit.SECONDS)\n+  }\n+\n+  /**\n+   * Recover cached RDD blocks off of an executor until there are no more, or until\n+   * there is an error\n+   *\n+   * @param execId the id of the executor to be killed\n+   * @return a Future of Boolean that returns true if a block was copied and false if there were"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "This method is not doing that, right? It's just asking the block manager to do stuff.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2018-01-22T19:41:24Z",
    "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import com.google.common.cache.CacheBuilder\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.BlockManagerId\n+import org.apache.spark.storage.BlockManagerMessages._\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executor's cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    blockManagerMasterEndpoint: RpcEndpointRef,\n+    executorAllocationManager: ExecutorAllocationManager,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT)\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"cache-recovery-manager-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"cache-recovery-shutdown-timers\")\n+  private val recoveringExecutors = CacheBuilder.newBuilder()\n+    .expireAfterWrite(forceKillAfterS * 2, TimeUnit.SECONDS)\n+    .build[String, java.lang.Boolean]() // Cache value never used and must extend Object\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   * @return a sequence of futures representing the kill process.\n+   */\n+  def startCacheRecovery(execIds: Seq[String]): Seq[Future[Boolean]] = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    val canBeRecovered = checkMem(execIds)\n+    canBeRecovered.foreach { execId => recoveringExecutors.put(execId, true)}\n+    canBeRecovered.map { execId =>\n+      val killTimer = startKillTimer(execId)\n+      replicateUntilDone(execId)\n+        .andThen {\n+          // don't log if kill timer is done, because it would be redundant\n+          case Failure(e) if !killTimer.isDone =>\n+              logWarning(s\"Failure recovering cached data before executor $execId shutdown\", e)\n+        }\n+        .andThen {\n+          case _ =>\n+            if (killTimer.cancel(false)) { kill(execId) }\n+        }\n+    }\n+  }\n+\n+  /**\n+   * Given a list of executors that will be shut down, check if there is enough free memory on the\n+   * rest of the cluster to hold their data. Return a list of just the executors for which there\n+   * will be enough space. Executors are included smallest first.\n+   *\n+   * This is a best guess implementation and it is not guaranteed that all returned executors\n+   * will succeed. For example a block might be too big to fit on any one specific executor.\n+   *\n+   * @param execIds executors which will be shut down\n+   * @return a Seq of the executors we do have room for\n+   */\n+  private def checkMem(execIds: Seq[String]): Seq[String] = {\n+    val execsToShutDown = execIds.toSet\n+    // Memory Status is a map of executor Id to a tuple of Max Memory and remaining memory on that\n+    // executor.\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+\n+    val (expiringMemStatus, remainingMemStatus) = allExecMemStatus.partition {\n+      case (execId, _) => execsToShutDown.contains(execId)\n+    }\n+    val freeMemOnRemaining = remainingMemStatus.values.map(_._2).sum\n+\n+    // The used mem on each executor sorted from least used mem to greatest\n+    val executorAndUsedMem: Seq[(String, Long)] =\n+      expiringMemStatus.map { case (execId, (maxMem, remainingMem)) =>\n+        val usedMem = maxMem - remainingMem\n+        execId -> usedMem\n+      }.toSeq.sortBy { case (_, usedMem) => usedMem }\n+\n+    executorAndUsedMem\n+      .scan((\"start\", freeMemOnRemaining)) {\n+        case ((_, freeMem), (execId, usedMem)) => (execId, freeMem - usedMem)\n+      }\n+      .drop(1)\n+      .filter { case (_, freeMem) => freeMem > 0 }\n+      .map(_._1)\n+  }\n+\n+  /**\n+   * Given an executor id, start a timer that will kill the given executor after the configured\n+   * timeout\n+   *\n+   * @param execId The id of the executor to be killed\n+   * @return a future representing the timer\n+   */\n+  private def startKillTimer(execId: String): ScheduledFuture[_] = {\n+    val killer = new Runnable {\n+      def run(): Unit = {\n+        logDebug(s\"Killing $execId because timeout for recovering cached data has expired\")\n+        kill(execId)\n+      }\n+    }\n+    scheduler.schedule(killer, forceKillAfterS, TimeUnit.SECONDS)\n+  }\n+\n+  /**\n+   * Recover cached RDD blocks off of an executor until there are no more, or until\n+   * there is an error\n+   *\n+   * @param execId the id of the executor to be killed\n+   * @return a Future of Boolean that returns true if a block was copied and false if there were\n+   *         no more blocks\n+   */\n+  private def replicateUntilDone(execId: String): Future[Boolean] = {\n+      recoverLatestBlock(execId).flatMap { moreBlocks =>\n+        if (moreBlocks) replicateUntilDone(execId) else Future.successful(false)\n+      }\n+  }\n+\n+  /**\n+   * Replicate the latest cached rdd block off of this executor on to a surviving executor, and then"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "updated comment",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2018-02-07T03:01:30Z",
    "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import com.google.common.cache.CacheBuilder\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.BlockManagerId\n+import org.apache.spark.storage.BlockManagerMessages._\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executor's cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    blockManagerMasterEndpoint: RpcEndpointRef,\n+    executorAllocationManager: ExecutorAllocationManager,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT)\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"cache-recovery-manager-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"cache-recovery-shutdown-timers\")\n+  private val recoveringExecutors = CacheBuilder.newBuilder()\n+    .expireAfterWrite(forceKillAfterS * 2, TimeUnit.SECONDS)\n+    .build[String, java.lang.Boolean]() // Cache value never used and must extend Object\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   * @return a sequence of futures representing the kill process.\n+   */\n+  def startCacheRecovery(execIds: Seq[String]): Seq[Future[Boolean]] = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    val canBeRecovered = checkMem(execIds)\n+    canBeRecovered.foreach { execId => recoveringExecutors.put(execId, true)}\n+    canBeRecovered.map { execId =>\n+      val killTimer = startKillTimer(execId)\n+      replicateUntilDone(execId)\n+        .andThen {\n+          // don't log if kill timer is done, because it would be redundant\n+          case Failure(e) if !killTimer.isDone =>\n+              logWarning(s\"Failure recovering cached data before executor $execId shutdown\", e)\n+        }\n+        .andThen {\n+          case _ =>\n+            if (killTimer.cancel(false)) { kill(execId) }\n+        }\n+    }\n+  }\n+\n+  /**\n+   * Given a list of executors that will be shut down, check if there is enough free memory on the\n+   * rest of the cluster to hold their data. Return a list of just the executors for which there\n+   * will be enough space. Executors are included smallest first.\n+   *\n+   * This is a best guess implementation and it is not guaranteed that all returned executors\n+   * will succeed. For example a block might be too big to fit on any one specific executor.\n+   *\n+   * @param execIds executors which will be shut down\n+   * @return a Seq of the executors we do have room for\n+   */\n+  private def checkMem(execIds: Seq[String]): Seq[String] = {\n+    val execsToShutDown = execIds.toSet\n+    // Memory Status is a map of executor Id to a tuple of Max Memory and remaining memory on that\n+    // executor.\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+\n+    val (expiringMemStatus, remainingMemStatus) = allExecMemStatus.partition {\n+      case (execId, _) => execsToShutDown.contains(execId)\n+    }\n+    val freeMemOnRemaining = remainingMemStatus.values.map(_._2).sum\n+\n+    // The used mem on each executor sorted from least used mem to greatest\n+    val executorAndUsedMem: Seq[(String, Long)] =\n+      expiringMemStatus.map { case (execId, (maxMem, remainingMem)) =>\n+        val usedMem = maxMem - remainingMem\n+        execId -> usedMem\n+      }.toSeq.sortBy { case (_, usedMem) => usedMem }\n+\n+    executorAndUsedMem\n+      .scan((\"start\", freeMemOnRemaining)) {\n+        case ((_, freeMem), (execId, usedMem)) => (execId, freeMem - usedMem)\n+      }\n+      .drop(1)\n+      .filter { case (_, freeMem) => freeMem > 0 }\n+      .map(_._1)\n+  }\n+\n+  /**\n+   * Given an executor id, start a timer that will kill the given executor after the configured\n+   * timeout\n+   *\n+   * @param execId The id of the executor to be killed\n+   * @return a future representing the timer\n+   */\n+  private def startKillTimer(execId: String): ScheduledFuture[_] = {\n+    val killer = new Runnable {\n+      def run(): Unit = {\n+        logDebug(s\"Killing $execId because timeout for recovering cached data has expired\")\n+        kill(execId)\n+      }\n+    }\n+    scheduler.schedule(killer, forceKillAfterS, TimeUnit.SECONDS)\n+  }\n+\n+  /**\n+   * Recover cached RDD blocks off of an executor until there are no more, or until\n+   * there is an error\n+   *\n+   * @param execId the id of the executor to be killed\n+   * @return a Future of Boolean that returns true if a block was copied and false if there were\n+   *         no more blocks\n+   */\n+  private def replicateUntilDone(execId: String): Future[Boolean] = {\n+      recoverLatestBlock(execId).flatMap { moreBlocks =>\n+        if (moreBlocks) replicateUntilDone(execId) else Future.successful(false)\n+      }\n+  }\n+\n+  /**\n+   * Replicate the latest cached rdd block off of this executor on to a surviving executor, and then"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Return value doesn't seem to match the code.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2018-01-22T19:41:39Z",
    "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import com.google.common.cache.CacheBuilder\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.BlockManagerId\n+import org.apache.spark.storage.BlockManagerMessages._\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executor's cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    blockManagerMasterEndpoint: RpcEndpointRef,\n+    executorAllocationManager: ExecutorAllocationManager,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT)\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"cache-recovery-manager-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"cache-recovery-shutdown-timers\")\n+  private val recoveringExecutors = CacheBuilder.newBuilder()\n+    .expireAfterWrite(forceKillAfterS * 2, TimeUnit.SECONDS)\n+    .build[String, java.lang.Boolean]() // Cache value never used and must extend Object\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   * @return a sequence of futures representing the kill process.\n+   */\n+  def startCacheRecovery(execIds: Seq[String]): Seq[Future[Boolean]] = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    val canBeRecovered = checkMem(execIds)\n+    canBeRecovered.foreach { execId => recoveringExecutors.put(execId, true)}\n+    canBeRecovered.map { execId =>\n+      val killTimer = startKillTimer(execId)\n+      replicateUntilDone(execId)\n+        .andThen {\n+          // don't log if kill timer is done, because it would be redundant\n+          case Failure(e) if !killTimer.isDone =>\n+              logWarning(s\"Failure recovering cached data before executor $execId shutdown\", e)\n+        }\n+        .andThen {\n+          case _ =>\n+            if (killTimer.cancel(false)) { kill(execId) }\n+        }\n+    }\n+  }\n+\n+  /**\n+   * Given a list of executors that will be shut down, check if there is enough free memory on the\n+   * rest of the cluster to hold their data. Return a list of just the executors for which there\n+   * will be enough space. Executors are included smallest first.\n+   *\n+   * This is a best guess implementation and it is not guaranteed that all returned executors\n+   * will succeed. For example a block might be too big to fit on any one specific executor.\n+   *\n+   * @param execIds executors which will be shut down\n+   * @return a Seq of the executors we do have room for\n+   */\n+  private def checkMem(execIds: Seq[String]): Seq[String] = {\n+    val execsToShutDown = execIds.toSet\n+    // Memory Status is a map of executor Id to a tuple of Max Memory and remaining memory on that\n+    // executor.\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+\n+    val (expiringMemStatus, remainingMemStatus) = allExecMemStatus.partition {\n+      case (execId, _) => execsToShutDown.contains(execId)\n+    }\n+    val freeMemOnRemaining = remainingMemStatus.values.map(_._2).sum\n+\n+    // The used mem on each executor sorted from least used mem to greatest\n+    val executorAndUsedMem: Seq[(String, Long)] =\n+      expiringMemStatus.map { case (execId, (maxMem, remainingMem)) =>\n+        val usedMem = maxMem - remainingMem\n+        execId -> usedMem\n+      }.toSeq.sortBy { case (_, usedMem) => usedMem }\n+\n+    executorAndUsedMem\n+      .scan((\"start\", freeMemOnRemaining)) {\n+        case ((_, freeMem), (execId, usedMem)) => (execId, freeMem - usedMem)\n+      }\n+      .drop(1)\n+      .filter { case (_, freeMem) => freeMem > 0 }\n+      .map(_._1)\n+  }\n+\n+  /**\n+   * Given an executor id, start a timer that will kill the given executor after the configured\n+   * timeout\n+   *\n+   * @param execId The id of the executor to be killed\n+   * @return a future representing the timer\n+   */\n+  private def startKillTimer(execId: String): ScheduledFuture[_] = {\n+    val killer = new Runnable {\n+      def run(): Unit = {\n+        logDebug(s\"Killing $execId because timeout for recovering cached data has expired\")\n+        kill(execId)\n+      }\n+    }\n+    scheduler.schedule(killer, forceKillAfterS, TimeUnit.SECONDS)\n+  }\n+\n+  /**\n+   * Recover cached RDD blocks off of an executor until there are no more, or until\n+   * there is an error\n+   *\n+   * @param execId the id of the executor to be killed\n+   * @return a Future of Boolean that returns true if a block was copied and false if there were\n+   *         no more blocks\n+   */\n+  private def replicateUntilDone(execId: String): Future[Boolean] = {\n+      recoverLatestBlock(execId).flatMap { moreBlocks =>\n+        if (moreBlocks) replicateUntilDone(execId) else Future.successful(false)\n+      }\n+  }\n+\n+  /**\n+   * Replicate the latest cached rdd block off of this executor on to a surviving executor, and then\n+   * remove the block from this executor\n+   *\n+   * @param execId the executor to recover a block from\n+   * @return A future holding the id of the block that was recovered or None if there were no blocks"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "this matches now",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2018-03-16T15:26:43Z",
    "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import com.google.common.cache.CacheBuilder\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.BlockManagerId\n+import org.apache.spark.storage.BlockManagerMessages._\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executor's cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    blockManagerMasterEndpoint: RpcEndpointRef,\n+    executorAllocationManager: ExecutorAllocationManager,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT)\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"cache-recovery-manager-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"cache-recovery-shutdown-timers\")\n+  private val recoveringExecutors = CacheBuilder.newBuilder()\n+    .expireAfterWrite(forceKillAfterS * 2, TimeUnit.SECONDS)\n+    .build[String, java.lang.Boolean]() // Cache value never used and must extend Object\n+\n+  /**\n+   * Start the recover cache shutdown process for these executors\n+   *\n+   * @param execIds the executors to start shutting down\n+   * @return a sequence of futures representing the kill process.\n+   */\n+  def startCacheRecovery(execIds: Seq[String]): Seq[Future[Boolean]] = {\n+    logDebug(s\"Recover cached data before shutting down executors ${execIds.mkString(\", \")}.\")\n+    val canBeRecovered = checkMem(execIds)\n+    canBeRecovered.foreach { execId => recoveringExecutors.put(execId, true)}\n+    canBeRecovered.map { execId =>\n+      val killTimer = startKillTimer(execId)\n+      replicateUntilDone(execId)\n+        .andThen {\n+          // don't log if kill timer is done, because it would be redundant\n+          case Failure(e) if !killTimer.isDone =>\n+              logWarning(s\"Failure recovering cached data before executor $execId shutdown\", e)\n+        }\n+        .andThen {\n+          case _ =>\n+            if (killTimer.cancel(false)) { kill(execId) }\n+        }\n+    }\n+  }\n+\n+  /**\n+   * Given a list of executors that will be shut down, check if there is enough free memory on the\n+   * rest of the cluster to hold their data. Return a list of just the executors for which there\n+   * will be enough space. Executors are included smallest first.\n+   *\n+   * This is a best guess implementation and it is not guaranteed that all returned executors\n+   * will succeed. For example a block might be too big to fit on any one specific executor.\n+   *\n+   * @param execIds executors which will be shut down\n+   * @return a Seq of the executors we do have room for\n+   */\n+  private def checkMem(execIds: Seq[String]): Seq[String] = {\n+    val execsToShutDown = execIds.toSet\n+    // Memory Status is a map of executor Id to a tuple of Max Memory and remaining memory on that\n+    // executor.\n+    val allExecMemStatus: Map[String, (Long, Long)] = blockManagerMasterEndpoint\n+      .askSync[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)\n+      .map { case (blockManagerId, mem) => blockManagerId.executorId -> mem }\n+\n+    val (expiringMemStatus, remainingMemStatus) = allExecMemStatus.partition {\n+      case (execId, _) => execsToShutDown.contains(execId)\n+    }\n+    val freeMemOnRemaining = remainingMemStatus.values.map(_._2).sum\n+\n+    // The used mem on each executor sorted from least used mem to greatest\n+    val executorAndUsedMem: Seq[(String, Long)] =\n+      expiringMemStatus.map { case (execId, (maxMem, remainingMem)) =>\n+        val usedMem = maxMem - remainingMem\n+        execId -> usedMem\n+      }.toSeq.sortBy { case (_, usedMem) => usedMem }\n+\n+    executorAndUsedMem\n+      .scan((\"start\", freeMemOnRemaining)) {\n+        case ((_, freeMem), (execId, usedMem)) => (execId, freeMem - usedMem)\n+      }\n+      .drop(1)\n+      .filter { case (_, freeMem) => freeMem > 0 }\n+      .map(_._1)\n+  }\n+\n+  /**\n+   * Given an executor id, start a timer that will kill the given executor after the configured\n+   * timeout\n+   *\n+   * @param execId The id of the executor to be killed\n+   * @return a future representing the timer\n+   */\n+  private def startKillTimer(execId: String): ScheduledFuture[_] = {\n+    val killer = new Runnable {\n+      def run(): Unit = {\n+        logDebug(s\"Killing $execId because timeout for recovering cached data has expired\")\n+        kill(execId)\n+      }\n+    }\n+    scheduler.schedule(killer, forceKillAfterS, TimeUnit.SECONDS)\n+  }\n+\n+  /**\n+   * Recover cached RDD blocks off of an executor until there are no more, or until\n+   * there is an error\n+   *\n+   * @param execId the id of the executor to be killed\n+   * @return a Future of Boolean that returns true if a block was copied and false if there were\n+   *         no more blocks\n+   */\n+  private def replicateUntilDone(execId: String): Future[Boolean] = {\n+      recoverLatestBlock(execId).flatMap { moreBlocks =>\n+        if (moreBlocks) replicateUntilDone(execId) else Future.successful(false)\n+      }\n+  }\n+\n+  /**\n+   * Replicate the latest cached rdd block off of this executor on to a surviving executor, and then\n+   * remove the block from this executor\n+   *\n+   * @param execId the executor to recover a block from\n+   * @return A future holding the id of the block that was recovered or None if there were no blocks"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Do you need this cache to auto cleanup? Seems like you could do the cleanup in your kill timer + the callbacks in  `startCacheRecovery` and avoid whatever extra overhead the cache here has to automatically do this for you.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2018-01-22T19:47:11Z",
    "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import com.google.common.cache.CacheBuilder\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.BlockManagerId\n+import org.apache.spark.storage.BlockManagerMessages._\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executor's cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    blockManagerMasterEndpoint: RpcEndpointRef,\n+    executorAllocationManager: ExecutorAllocationManager,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT)\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"cache-recovery-manager-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"cache-recovery-shutdown-timers\")\n+  private val recoveringExecutors = CacheBuilder.newBuilder()\n+    .expireAfterWrite(forceKillAfterS * 2, TimeUnit.SECONDS)"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "I added this cache because I found a bug where there was a slight gap between when CacheRecoveryManager finished recovering an executor and asked it to be removed and when that executor actually stops showing up as a peer in the BlockManagerMaster. During this gap we might try to replicate blocks to that executor and they would get lost. \r\n\r\nThe simplest way I saw to fix this was to let executors linger in \"recoveringExecutors\" for a while to cover that gap. I chose a guava cache because it has lightweight time-based eviction. It just cleans up expired cache entries on writes, there isn't a big backing thread or anything doing the cleanup. ",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2018-02-07T05:10:33Z",
    "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.Failure\n+\n+import com.google.common.cache.CacheBuilder\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.storage.BlockManagerId\n+import org.apache.spark.storage.BlockManagerMessages._\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Responsible for asynchronously replicating all of an executor's cached blocks, and then shutting\n+ * it down.\n+ */\n+final private class CacheRecoveryManager(\n+    blockManagerMasterEndpoint: RpcEndpointRef,\n+    executorAllocationManager: ExecutorAllocationManager,\n+    conf: SparkConf)\n+  extends Logging {\n+\n+  private val forceKillAfterS = conf.get(DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT)\n+  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(\"cache-recovery-manager-pool\")\n+  private implicit val asyncExecutionContext: ExecutionContext =\n+    ExecutionContext.fromExecutorService(threadPool)\n+  private val scheduler =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"cache-recovery-shutdown-timers\")\n+  private val recoveringExecutors = CacheBuilder.newBuilder()\n+    .expireAfterWrite(forceKillAfterS * 2, TimeUnit.SECONDS)"
  }],
  "prId": 19041
}]