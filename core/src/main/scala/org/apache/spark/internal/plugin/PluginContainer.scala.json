[{
  "comments": [{
    "author": {
      "login": "tgravescs"
    },
    "body": "remove extra newline",
    "commit": "37ad680ec33ec6afac9d031897a549321e782d9c",
    "createdAt": "2019-10-22T13:06:43Z",
    "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.plugin\n+\n+import scala.collection.JavaConverters._\n+import scala.util.{Either, Left, Right}\n+\n+import org.apache.spark.{SparkContext, SparkEnv}\n+import org.apache.spark.api.plugin._\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.util.Utils\n+\n+sealed abstract class PluginContainer {\n+\n+  def shutdown(): Unit\n+\n+}\n+\n+private class DriverPluginContainer(sc: SparkContext, plugins: Seq[SparkPlugin])\n+  extends PluginContainer with Logging {\n+\n+  private val driverPlugins: Seq[(String, DriverPlugin)] = plugins.flatMap { p =>\n+    val driverPlugin = p.driverPlugin()\n+    if (driverPlugin != null) {\n+      val name = p.getClass().getName()\n+      val ctx = new PluginContextImpl(name, sc.env.rpcEnv, sc.env.metricsSystem, sc.conf,\n+        sc.env.executorId)\n+\n+      val extraConf = driverPlugin.init(sc, ctx)\n+      if (extraConf != null) {\n+        extraConf.asScala.foreach { case (k, v) =>\n+          sc.conf.set(s\"${PluginContainer.EXTRA_CONF_PREFIX}$name.$k\", v)\n+        }\n+      }\n+      ctx.registerMetrics()\n+      logInfo(s\"Initialized driver component for plugin $name.\")\n+      Some(p.getClass().getName() -> driverPlugin)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  if (driverPlugins.nonEmpty) {\n+    sc.env.rpcEnv.setupEndpoint(classOf[PluginEndpoint].getName(),\n+      new PluginEndpoint(driverPlugins.toMap, sc.env.rpcEnv))\n+  }\n+\n+  override def shutdown(): Unit = {\n+    driverPlugins.foreach { case (name, plugin) =>\n+      try {\n+        plugin.shutdown()\n+      } catch {\n+        case t: Throwable =>\n+          logInfo(s\"Exception while shutting down plugin $name.\", t)\n+      }\n+    }\n+  }\n+",
    "line": 83
  }],
  "prId": 26170
}, {
  "comments": [{
    "author": {
      "login": "tgravescs"
    },
    "body": "instead of __conf__ should we use like .internal.  I assume these aren't meant for users to set directly",
    "commit": "37ad680ec33ec6afac9d031897a549321e782d9c",
    "createdAt": "2019-10-22T13:08:54Z",
    "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.plugin\n+\n+import scala.collection.JavaConverters._\n+import scala.util.{Either, Left, Right}\n+\n+import org.apache.spark.{SparkContext, SparkEnv}\n+import org.apache.spark.api.plugin._\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.util.Utils\n+\n+sealed abstract class PluginContainer {\n+\n+  def shutdown(): Unit\n+\n+}\n+\n+private class DriverPluginContainer(sc: SparkContext, plugins: Seq[SparkPlugin])\n+  extends PluginContainer with Logging {\n+\n+  private val driverPlugins: Seq[(String, DriverPlugin)] = plugins.flatMap { p =>\n+    val driverPlugin = p.driverPlugin()\n+    if (driverPlugin != null) {\n+      val name = p.getClass().getName()\n+      val ctx = new PluginContextImpl(name, sc.env.rpcEnv, sc.env.metricsSystem, sc.conf,\n+        sc.env.executorId)\n+\n+      val extraConf = driverPlugin.init(sc, ctx)\n+      if (extraConf != null) {\n+        extraConf.asScala.foreach { case (k, v) =>\n+          sc.conf.set(s\"${PluginContainer.EXTRA_CONF_PREFIX}$name.$k\", v)\n+        }\n+      }\n+      ctx.registerMetrics()\n+      logInfo(s\"Initialized driver component for plugin $name.\")\n+      Some(p.getClass().getName() -> driverPlugin)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  if (driverPlugins.nonEmpty) {\n+    sc.env.rpcEnv.setupEndpoint(classOf[PluginEndpoint].getName(),\n+      new PluginEndpoint(driverPlugins.toMap, sc.env.rpcEnv))\n+  }\n+\n+  override def shutdown(): Unit = {\n+    driverPlugins.foreach { case (name, plugin) =>\n+      try {\n+        plugin.shutdown()\n+      } catch {\n+        case t: Throwable =>\n+          logInfo(s\"Exception while shutting down plugin $name.\", t)\n+      }\n+    }\n+  }\n+\n+}\n+\n+private class ExecutorPluginContainer(env: SparkEnv, plugins: Seq[SparkPlugin])\n+  extends PluginContainer with Logging {\n+\n+  private val executorPlugins: Seq[(String, ExecutorPlugin)] = {\n+    val allExtraConf = env.conf.getAllWithPrefix(PluginContainer.EXTRA_CONF_PREFIX)\n+\n+    plugins.flatMap { p =>\n+      val executorPlugin = p.executorPlugin()\n+      if (executorPlugin != null) {\n+        val name = p.getClass().getName()\n+        val prefix = name + \".\"\n+        val extraConf = allExtraConf\n+          .filter { case (k, v) => k.startsWith(prefix) }\n+          .map { case (k, v) => k.substring(prefix.length()) -> v }\n+          .toMap\n+          .asJava\n+        val ctx = new PluginContextImpl(name, env.rpcEnv, env.metricsSystem, env.conf,\n+          env.executorId)\n+        executorPlugin.init(ctx, extraConf)\n+        ctx.registerMetrics()\n+\n+        logInfo(s\"Initialized executor component for plugin $name.\")\n+        Some(p.getClass().getName() -> executorPlugin)\n+      } else {\n+        None\n+      }\n+    }\n+  }\n+\n+  override def shutdown(): Unit = {\n+    executorPlugins.foreach { case (name, plugin) =>\n+      try {\n+        logDebug(s\"Stopping plugin $name.\")\n+        plugin.shutdown()\n+      } catch {\n+        case t: Throwable =>\n+          logInfo(s\"Exception while shutting down plugin $name.\", t)\n+      }\n+    }\n+  }\n+\n+}\n+\n+object PluginContainer {\n+\n+  val EXTRA_CONF_PREFIX = \"spark.plugins.__conf__.\""
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "I've always used the underscores to mean \"internal\" since I started programming in C... but sure.",
    "commit": "37ad680ec33ec6afac9d031897a549321e782d9c",
    "createdAt": "2019-10-23T16:01:03Z",
    "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.plugin\n+\n+import scala.collection.JavaConverters._\n+import scala.util.{Either, Left, Right}\n+\n+import org.apache.spark.{SparkContext, SparkEnv}\n+import org.apache.spark.api.plugin._\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.util.Utils\n+\n+sealed abstract class PluginContainer {\n+\n+  def shutdown(): Unit\n+\n+}\n+\n+private class DriverPluginContainer(sc: SparkContext, plugins: Seq[SparkPlugin])\n+  extends PluginContainer with Logging {\n+\n+  private val driverPlugins: Seq[(String, DriverPlugin)] = plugins.flatMap { p =>\n+    val driverPlugin = p.driverPlugin()\n+    if (driverPlugin != null) {\n+      val name = p.getClass().getName()\n+      val ctx = new PluginContextImpl(name, sc.env.rpcEnv, sc.env.metricsSystem, sc.conf,\n+        sc.env.executorId)\n+\n+      val extraConf = driverPlugin.init(sc, ctx)\n+      if (extraConf != null) {\n+        extraConf.asScala.foreach { case (k, v) =>\n+          sc.conf.set(s\"${PluginContainer.EXTRA_CONF_PREFIX}$name.$k\", v)\n+        }\n+      }\n+      ctx.registerMetrics()\n+      logInfo(s\"Initialized driver component for plugin $name.\")\n+      Some(p.getClass().getName() -> driverPlugin)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  if (driverPlugins.nonEmpty) {\n+    sc.env.rpcEnv.setupEndpoint(classOf[PluginEndpoint].getName(),\n+      new PluginEndpoint(driverPlugins.toMap, sc.env.rpcEnv))\n+  }\n+\n+  override def shutdown(): Unit = {\n+    driverPlugins.foreach { case (name, plugin) =>\n+      try {\n+        plugin.shutdown()\n+      } catch {\n+        case t: Throwable =>\n+          logInfo(s\"Exception while shutting down plugin $name.\", t)\n+      }\n+    }\n+  }\n+\n+}\n+\n+private class ExecutorPluginContainer(env: SparkEnv, plugins: Seq[SparkPlugin])\n+  extends PluginContainer with Logging {\n+\n+  private val executorPlugins: Seq[(String, ExecutorPlugin)] = {\n+    val allExtraConf = env.conf.getAllWithPrefix(PluginContainer.EXTRA_CONF_PREFIX)\n+\n+    plugins.flatMap { p =>\n+      val executorPlugin = p.executorPlugin()\n+      if (executorPlugin != null) {\n+        val name = p.getClass().getName()\n+        val prefix = name + \".\"\n+        val extraConf = allExtraConf\n+          .filter { case (k, v) => k.startsWith(prefix) }\n+          .map { case (k, v) => k.substring(prefix.length()) -> v }\n+          .toMap\n+          .asJava\n+        val ctx = new PluginContextImpl(name, env.rpcEnv, env.metricsSystem, env.conf,\n+          env.executorId)\n+        executorPlugin.init(ctx, extraConf)\n+        ctx.registerMetrics()\n+\n+        logInfo(s\"Initialized executor component for plugin $name.\")\n+        Some(p.getClass().getName() -> executorPlugin)\n+      } else {\n+        None\n+      }\n+    }\n+  }\n+\n+  override def shutdown(): Unit = {\n+    executorPlugins.foreach { case (name, plugin) =>\n+      try {\n+        logDebug(s\"Stopping plugin $name.\")\n+        plugin.shutdown()\n+      } catch {\n+        case t: Throwable =>\n+          logInfo(s\"Exception while shutting down plugin $name.\", t)\n+      }\n+    }\n+  }\n+\n+}\n+\n+object PluginContainer {\n+\n+  val EXTRA_CONF_PREFIX = \"spark.plugins.__conf__.\""
  }],
  "prId": 26170
}, {
  "comments": [{
    "author": {
      "login": "tgravescs"
    },
    "body": "add debug message like executors have",
    "commit": "37ad680ec33ec6afac9d031897a549321e782d9c",
    "createdAt": "2019-10-22T13:38:16Z",
    "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.plugin\n+\n+import scala.collection.JavaConverters._\n+import scala.util.{Either, Left, Right}\n+\n+import org.apache.spark.{SparkContext, SparkEnv}\n+import org.apache.spark.api.plugin._\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.util.Utils\n+\n+sealed abstract class PluginContainer {\n+\n+  def shutdown(): Unit\n+\n+}\n+\n+private class DriverPluginContainer(sc: SparkContext, plugins: Seq[SparkPlugin])\n+  extends PluginContainer with Logging {\n+\n+  private val driverPlugins: Seq[(String, DriverPlugin)] = plugins.flatMap { p =>\n+    val driverPlugin = p.driverPlugin()\n+    if (driverPlugin != null) {\n+      val name = p.getClass().getName()\n+      val ctx = new PluginContextImpl(name, sc.env.rpcEnv, sc.env.metricsSystem, sc.conf,\n+        sc.env.executorId)\n+\n+      val extraConf = driverPlugin.init(sc, ctx)\n+      if (extraConf != null) {\n+        extraConf.asScala.foreach { case (k, v) =>\n+          sc.conf.set(s\"${PluginContainer.EXTRA_CONF_PREFIX}$name.$k\", v)\n+        }\n+      }\n+      ctx.registerMetrics()\n+      logInfo(s\"Initialized driver component for plugin $name.\")\n+      Some(p.getClass().getName() -> driverPlugin)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  if (driverPlugins.nonEmpty) {\n+    sc.env.rpcEnv.setupEndpoint(classOf[PluginEndpoint].getName(),\n+      new PluginEndpoint(driverPlugins.toMap, sc.env.rpcEnv))\n+  }\n+\n+  override def shutdown(): Unit = {\n+    driverPlugins.foreach { case (name, plugin) =>\n+      try {\n+        plugin.shutdown()",
    "line": 76
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "Weird I have that message locally. Not sure how it's not here.",
    "commit": "37ad680ec33ec6afac9d031897a549321e782d9c",
    "createdAt": "2019-10-23T16:02:28Z",
    "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.plugin\n+\n+import scala.collection.JavaConverters._\n+import scala.util.{Either, Left, Right}\n+\n+import org.apache.spark.{SparkContext, SparkEnv}\n+import org.apache.spark.api.plugin._\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.util.Utils\n+\n+sealed abstract class PluginContainer {\n+\n+  def shutdown(): Unit\n+\n+}\n+\n+private class DriverPluginContainer(sc: SparkContext, plugins: Seq[SparkPlugin])\n+  extends PluginContainer with Logging {\n+\n+  private val driverPlugins: Seq[(String, DriverPlugin)] = plugins.flatMap { p =>\n+    val driverPlugin = p.driverPlugin()\n+    if (driverPlugin != null) {\n+      val name = p.getClass().getName()\n+      val ctx = new PluginContextImpl(name, sc.env.rpcEnv, sc.env.metricsSystem, sc.conf,\n+        sc.env.executorId)\n+\n+      val extraConf = driverPlugin.init(sc, ctx)\n+      if (extraConf != null) {\n+        extraConf.asScala.foreach { case (k, v) =>\n+          sc.conf.set(s\"${PluginContainer.EXTRA_CONF_PREFIX}$name.$k\", v)\n+        }\n+      }\n+      ctx.registerMetrics()\n+      logInfo(s\"Initialized driver component for plugin $name.\")\n+      Some(p.getClass().getName() -> driverPlugin)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  if (driverPlugins.nonEmpty) {\n+    sc.env.rpcEnv.setupEndpoint(classOf[PluginEndpoint].getName(),\n+      new PluginEndpoint(driverPlugins.toMap, sc.env.rpcEnv))\n+  }\n+\n+  override def shutdown(): Unit = {\n+    driverPlugins.foreach { case (name, plugin) =>\n+      try {\n+        plugin.shutdown()",
    "line": 76
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "Err looked in the wrong spot.",
    "commit": "37ad680ec33ec6afac9d031897a549321e782d9c",
    "createdAt": "2019-10-23T16:06:49Z",
    "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.plugin\n+\n+import scala.collection.JavaConverters._\n+import scala.util.{Either, Left, Right}\n+\n+import org.apache.spark.{SparkContext, SparkEnv}\n+import org.apache.spark.api.plugin._\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.util.Utils\n+\n+sealed abstract class PluginContainer {\n+\n+  def shutdown(): Unit\n+\n+}\n+\n+private class DriverPluginContainer(sc: SparkContext, plugins: Seq[SparkPlugin])\n+  extends PluginContainer with Logging {\n+\n+  private val driverPlugins: Seq[(String, DriverPlugin)] = plugins.flatMap { p =>\n+    val driverPlugin = p.driverPlugin()\n+    if (driverPlugin != null) {\n+      val name = p.getClass().getName()\n+      val ctx = new PluginContextImpl(name, sc.env.rpcEnv, sc.env.metricsSystem, sc.conf,\n+        sc.env.executorId)\n+\n+      val extraConf = driverPlugin.init(sc, ctx)\n+      if (extraConf != null) {\n+        extraConf.asScala.foreach { case (k, v) =>\n+          sc.conf.set(s\"${PluginContainer.EXTRA_CONF_PREFIX}$name.$k\", v)\n+        }\n+      }\n+      ctx.registerMetrics()\n+      logInfo(s\"Initialized driver component for plugin $name.\")\n+      Some(p.getClass().getName() -> driverPlugin)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  if (driverPlugins.nonEmpty) {\n+    sc.env.rpcEnv.setupEndpoint(classOf[PluginEndpoint].getName(),\n+      new PluginEndpoint(driverPlugins.toMap, sc.env.rpcEnv))\n+  }\n+\n+  override def shutdown(): Unit = {\n+    driverPlugins.foreach { case (name, plugin) =>\n+      try {\n+        plugin.shutdown()",
    "line": 76
  }],
  "prId": 26170
}, {
  "comments": [{
    "author": {
      "login": "tgravescs"
    },
    "body": "sorry I was thinking just spark.plugins.internal.conf.  We have the internal() option to config builder so figured it kind of matched.   I don't have a super strong opinion on this as long as we try to keep it consistent.  I know we use __xxx__ for various internal things - files directories -  but didn't think we had any for configs.  thoughts?",
    "commit": "37ad680ec33ec6afac9d031897a549321e782d9c",
    "createdAt": "2019-10-23T19:06:04Z",
    "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.plugin\n+\n+import scala.collection.JavaConverters._\n+import scala.util.{Either, Left, Right}\n+\n+import org.apache.spark.{SparkContext, SparkEnv}\n+import org.apache.spark.api.plugin._\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.util.Utils\n+\n+sealed abstract class PluginContainer {\n+\n+  def shutdown(): Unit\n+\n+}\n+\n+private class DriverPluginContainer(sc: SparkContext, plugins: Seq[SparkPlugin])\n+  extends PluginContainer with Logging {\n+\n+  private val driverPlugins: Seq[(String, DriverPlugin)] = plugins.flatMap { p =>\n+    val driverPlugin = p.driverPlugin()\n+    if (driverPlugin != null) {\n+      val name = p.getClass().getName()\n+      val ctx = new PluginContextImpl(name, sc.env.rpcEnv, sc.env.metricsSystem, sc.conf,\n+        sc.env.executorId)\n+\n+      val extraConf = driverPlugin.init(sc, ctx)\n+      if (extraConf != null) {\n+        extraConf.asScala.foreach { case (k, v) =>\n+          sc.conf.set(s\"${PluginContainer.EXTRA_CONF_PREFIX}$name.$k\", v)\n+        }\n+      }\n+      ctx.registerMetrics()\n+      logInfo(s\"Initialized driver component for plugin $name.\")\n+      Some(p.getClass().getName() -> driverPlugin)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  if (driverPlugins.nonEmpty) {\n+    sc.env.rpcEnv.setupEndpoint(classOf[PluginEndpoint].getName(),\n+      new PluginEndpoint(driverPlugins.toMap, sc.env.rpcEnv))\n+  }\n+\n+  override def shutdown(): Unit = {\n+    driverPlugins.foreach { case (name, plugin) =>\n+      try {\n+        logDebug(s\"Stopping plugin $name.\")\n+        plugin.shutdown()\n+      } catch {\n+        case t: Throwable =>\n+          logInfo(s\"Exception while shutting down plugin $name.\", t)\n+      }\n+    }\n+  }\n+\n+}\n+\n+private class ExecutorPluginContainer(env: SparkEnv, plugins: Seq[SparkPlugin])\n+  extends PluginContainer with Logging {\n+\n+  private val executorPlugins: Seq[(String, ExecutorPlugin)] = {\n+    val allExtraConf = env.conf.getAllWithPrefix(PluginContainer.EXTRA_CONF_PREFIX)\n+\n+    plugins.flatMap { p =>\n+      val executorPlugin = p.executorPlugin()\n+      if (executorPlugin != null) {\n+        val name = p.getClass().getName()\n+        val prefix = name + \".\"\n+        val extraConf = allExtraConf\n+          .filter { case (k, v) => k.startsWith(prefix) }\n+          .map { case (k, v) => k.substring(prefix.length()) -> v }\n+          .toMap\n+          .asJava\n+        val ctx = new PluginContextImpl(name, env.rpcEnv, env.metricsSystem, env.conf,\n+          env.executorId)\n+        executorPlugin.init(ctx, extraConf)\n+        ctx.registerMetrics()\n+\n+        logInfo(s\"Initialized executor component for plugin $name.\")\n+        Some(p.getClass().getName() -> executorPlugin)\n+      } else {\n+        None\n+      }\n+    }\n+  }\n+\n+  override def shutdown(): Unit = {\n+    executorPlugins.foreach { case (name, plugin) =>\n+      try {\n+        logDebug(s\"Stopping plugin $name.\")\n+        plugin.shutdown()\n+      } catch {\n+        case t: Throwable =>\n+          logInfo(s\"Exception while shutting down plugin $name.\", t)\n+      }\n+    }\n+  }\n+}\n+\n+object PluginContainer {\n+\n+  val EXTRA_CONF_PREFIX = \"spark.plugins.__internal_conf__.\""
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "The `internal()` thing does not modify the config names. Nor does it have any other effect aside from being informational (except for SQL configs, which are hidden from the \"set -v\" output).\r\n\r\nSo does it really matter what this name is? It's internal, it's not supposed to be set or read by anything other than internal Spark code, and people who end up seeing them should just ignore them.",
    "commit": "37ad680ec33ec6afac9d031897a549321e782d9c",
    "createdAt": "2019-10-23T20:58:31Z",
    "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.plugin\n+\n+import scala.collection.JavaConverters._\n+import scala.util.{Either, Left, Right}\n+\n+import org.apache.spark.{SparkContext, SparkEnv}\n+import org.apache.spark.api.plugin._\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.util.Utils\n+\n+sealed abstract class PluginContainer {\n+\n+  def shutdown(): Unit\n+\n+}\n+\n+private class DriverPluginContainer(sc: SparkContext, plugins: Seq[SparkPlugin])\n+  extends PluginContainer with Logging {\n+\n+  private val driverPlugins: Seq[(String, DriverPlugin)] = plugins.flatMap { p =>\n+    val driverPlugin = p.driverPlugin()\n+    if (driverPlugin != null) {\n+      val name = p.getClass().getName()\n+      val ctx = new PluginContextImpl(name, sc.env.rpcEnv, sc.env.metricsSystem, sc.conf,\n+        sc.env.executorId)\n+\n+      val extraConf = driverPlugin.init(sc, ctx)\n+      if (extraConf != null) {\n+        extraConf.asScala.foreach { case (k, v) =>\n+          sc.conf.set(s\"${PluginContainer.EXTRA_CONF_PREFIX}$name.$k\", v)\n+        }\n+      }\n+      ctx.registerMetrics()\n+      logInfo(s\"Initialized driver component for plugin $name.\")\n+      Some(p.getClass().getName() -> driverPlugin)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  if (driverPlugins.nonEmpty) {\n+    sc.env.rpcEnv.setupEndpoint(classOf[PluginEndpoint].getName(),\n+      new PluginEndpoint(driverPlugins.toMap, sc.env.rpcEnv))\n+  }\n+\n+  override def shutdown(): Unit = {\n+    driverPlugins.foreach { case (name, plugin) =>\n+      try {\n+        logDebug(s\"Stopping plugin $name.\")\n+        plugin.shutdown()\n+      } catch {\n+        case t: Throwable =>\n+          logInfo(s\"Exception while shutting down plugin $name.\", t)\n+      }\n+    }\n+  }\n+\n+}\n+\n+private class ExecutorPluginContainer(env: SparkEnv, plugins: Seq[SparkPlugin])\n+  extends PluginContainer with Logging {\n+\n+  private val executorPlugins: Seq[(String, ExecutorPlugin)] = {\n+    val allExtraConf = env.conf.getAllWithPrefix(PluginContainer.EXTRA_CONF_PREFIX)\n+\n+    plugins.flatMap { p =>\n+      val executorPlugin = p.executorPlugin()\n+      if (executorPlugin != null) {\n+        val name = p.getClass().getName()\n+        val prefix = name + \".\"\n+        val extraConf = allExtraConf\n+          .filter { case (k, v) => k.startsWith(prefix) }\n+          .map { case (k, v) => k.substring(prefix.length()) -> v }\n+          .toMap\n+          .asJava\n+        val ctx = new PluginContextImpl(name, env.rpcEnv, env.metricsSystem, env.conf,\n+          env.executorId)\n+        executorPlugin.init(ctx, extraConf)\n+        ctx.registerMetrics()\n+\n+        logInfo(s\"Initialized executor component for plugin $name.\")\n+        Some(p.getClass().getName() -> executorPlugin)\n+      } else {\n+        None\n+      }\n+    }\n+  }\n+\n+  override def shutdown(): Unit = {\n+    executorPlugins.foreach { case (name, plugin) =>\n+      try {\n+        logDebug(s\"Stopping plugin $name.\")\n+        plugin.shutdown()\n+      } catch {\n+        case t: Throwable =>\n+          logInfo(s\"Exception while shutting down plugin $name.\", t)\n+      }\n+    }\n+  }\n+}\n+\n+object PluginContainer {\n+\n+  val EXTRA_CONF_PREFIX = \"spark.plugins.__internal_conf__.\""
  }, {
    "author": {
      "login": "tgravescs"
    },
    "body": "I get that that internal() doesn't mean anything to the config name, maybe a bad comparison, its more of keeping naming consistent.  To me its a lot more obvious if a config has .internal. in its name that its internal to spark.  Users should ignore those.  that is why I suggested it.  If its .internal.  I could also programmatically \"grep\" for all internal configs fairly easily.  Not sure why I would want to do this, other than maybe hide them from user. \r\n\r\n All the other spark configs either follow format x.y.z with the last one optionally camel case, so why not keep that consistent instead of breaking that convention with the __something__ format.  I know our internal configs now have no special name on them, which personally I don't like either as its not obvious its meant to be internal. The only benefit to that is you can easily change to not be internal if you want without changing the name.",
    "commit": "37ad680ec33ec6afac9d031897a549321e782d9c",
    "createdAt": "2019-10-24T13:38:55Z",
    "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.plugin\n+\n+import scala.collection.JavaConverters._\n+import scala.util.{Either, Left, Right}\n+\n+import org.apache.spark.{SparkContext, SparkEnv}\n+import org.apache.spark.api.plugin._\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.util.Utils\n+\n+sealed abstract class PluginContainer {\n+\n+  def shutdown(): Unit\n+\n+}\n+\n+private class DriverPluginContainer(sc: SparkContext, plugins: Seq[SparkPlugin])\n+  extends PluginContainer with Logging {\n+\n+  private val driverPlugins: Seq[(String, DriverPlugin)] = plugins.flatMap { p =>\n+    val driverPlugin = p.driverPlugin()\n+    if (driverPlugin != null) {\n+      val name = p.getClass().getName()\n+      val ctx = new PluginContextImpl(name, sc.env.rpcEnv, sc.env.metricsSystem, sc.conf,\n+        sc.env.executorId)\n+\n+      val extraConf = driverPlugin.init(sc, ctx)\n+      if (extraConf != null) {\n+        extraConf.asScala.foreach { case (k, v) =>\n+          sc.conf.set(s\"${PluginContainer.EXTRA_CONF_PREFIX}$name.$k\", v)\n+        }\n+      }\n+      ctx.registerMetrics()\n+      logInfo(s\"Initialized driver component for plugin $name.\")\n+      Some(p.getClass().getName() -> driverPlugin)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  if (driverPlugins.nonEmpty) {\n+    sc.env.rpcEnv.setupEndpoint(classOf[PluginEndpoint].getName(),\n+      new PluginEndpoint(driverPlugins.toMap, sc.env.rpcEnv))\n+  }\n+\n+  override def shutdown(): Unit = {\n+    driverPlugins.foreach { case (name, plugin) =>\n+      try {\n+        logDebug(s\"Stopping plugin $name.\")\n+        plugin.shutdown()\n+      } catch {\n+        case t: Throwable =>\n+          logInfo(s\"Exception while shutting down plugin $name.\", t)\n+      }\n+    }\n+  }\n+\n+}\n+\n+private class ExecutorPluginContainer(env: SparkEnv, plugins: Seq[SparkPlugin])\n+  extends PluginContainer with Logging {\n+\n+  private val executorPlugins: Seq[(String, ExecutorPlugin)] = {\n+    val allExtraConf = env.conf.getAllWithPrefix(PluginContainer.EXTRA_CONF_PREFIX)\n+\n+    plugins.flatMap { p =>\n+      val executorPlugin = p.executorPlugin()\n+      if (executorPlugin != null) {\n+        val name = p.getClass().getName()\n+        val prefix = name + \".\"\n+        val extraConf = allExtraConf\n+          .filter { case (k, v) => k.startsWith(prefix) }\n+          .map { case (k, v) => k.substring(prefix.length()) -> v }\n+          .toMap\n+          .asJava\n+        val ctx = new PluginContextImpl(name, env.rpcEnv, env.metricsSystem, env.conf,\n+          env.executorId)\n+        executorPlugin.init(ctx, extraConf)\n+        ctx.registerMetrics()\n+\n+        logInfo(s\"Initialized executor component for plugin $name.\")\n+        Some(p.getClass().getName() -> executorPlugin)\n+      } else {\n+        None\n+      }\n+    }\n+  }\n+\n+  override def shutdown(): Unit = {\n+    executorPlugins.foreach { case (name, plugin) =>\n+      try {\n+        logDebug(s\"Stopping plugin $name.\")\n+        plugin.shutdown()\n+      } catch {\n+        case t: Throwable =>\n+          logInfo(s\"Exception while shutting down plugin $name.\", t)\n+      }\n+    }\n+  }\n+}\n+\n+object PluginContainer {\n+\n+  val EXTRA_CONF_PREFIX = \"spark.plugins.__internal_conf__.\""
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "Sure, if you want that, I don't care about the name. I'm just pointing out that there isn't a pattern to follow here.",
    "commit": "37ad680ec33ec6afac9d031897a549321e782d9c",
    "createdAt": "2019-10-24T16:54:47Z",
    "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.plugin\n+\n+import scala.collection.JavaConverters._\n+import scala.util.{Either, Left, Right}\n+\n+import org.apache.spark.{SparkContext, SparkEnv}\n+import org.apache.spark.api.plugin._\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.util.Utils\n+\n+sealed abstract class PluginContainer {\n+\n+  def shutdown(): Unit\n+\n+}\n+\n+private class DriverPluginContainer(sc: SparkContext, plugins: Seq[SparkPlugin])\n+  extends PluginContainer with Logging {\n+\n+  private val driverPlugins: Seq[(String, DriverPlugin)] = plugins.flatMap { p =>\n+    val driverPlugin = p.driverPlugin()\n+    if (driverPlugin != null) {\n+      val name = p.getClass().getName()\n+      val ctx = new PluginContextImpl(name, sc.env.rpcEnv, sc.env.metricsSystem, sc.conf,\n+        sc.env.executorId)\n+\n+      val extraConf = driverPlugin.init(sc, ctx)\n+      if (extraConf != null) {\n+        extraConf.asScala.foreach { case (k, v) =>\n+          sc.conf.set(s\"${PluginContainer.EXTRA_CONF_PREFIX}$name.$k\", v)\n+        }\n+      }\n+      ctx.registerMetrics()\n+      logInfo(s\"Initialized driver component for plugin $name.\")\n+      Some(p.getClass().getName() -> driverPlugin)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  if (driverPlugins.nonEmpty) {\n+    sc.env.rpcEnv.setupEndpoint(classOf[PluginEndpoint].getName(),\n+      new PluginEndpoint(driverPlugins.toMap, sc.env.rpcEnv))\n+  }\n+\n+  override def shutdown(): Unit = {\n+    driverPlugins.foreach { case (name, plugin) =>\n+      try {\n+        logDebug(s\"Stopping plugin $name.\")\n+        plugin.shutdown()\n+      } catch {\n+        case t: Throwable =>\n+          logInfo(s\"Exception while shutting down plugin $name.\", t)\n+      }\n+    }\n+  }\n+\n+}\n+\n+private class ExecutorPluginContainer(env: SparkEnv, plugins: Seq[SparkPlugin])\n+  extends PluginContainer with Logging {\n+\n+  private val executorPlugins: Seq[(String, ExecutorPlugin)] = {\n+    val allExtraConf = env.conf.getAllWithPrefix(PluginContainer.EXTRA_CONF_PREFIX)\n+\n+    plugins.flatMap { p =>\n+      val executorPlugin = p.executorPlugin()\n+      if (executorPlugin != null) {\n+        val name = p.getClass().getName()\n+        val prefix = name + \".\"\n+        val extraConf = allExtraConf\n+          .filter { case (k, v) => k.startsWith(prefix) }\n+          .map { case (k, v) => k.substring(prefix.length()) -> v }\n+          .toMap\n+          .asJava\n+        val ctx = new PluginContextImpl(name, env.rpcEnv, env.metricsSystem, env.conf,\n+          env.executorId)\n+        executorPlugin.init(ctx, extraConf)\n+        ctx.registerMetrics()\n+\n+        logInfo(s\"Initialized executor component for plugin $name.\")\n+        Some(p.getClass().getName() -> executorPlugin)\n+      } else {\n+        None\n+      }\n+    }\n+  }\n+\n+  override def shutdown(): Unit = {\n+    executorPlugins.foreach { case (name, plugin) =>\n+      try {\n+        logDebug(s\"Stopping plugin $name.\")\n+        plugin.shutdown()\n+      } catch {\n+        case t: Throwable =>\n+          logInfo(s\"Exception while shutting down plugin $name.\", t)\n+      }\n+    }\n+  }\n+}\n+\n+object PluginContainer {\n+\n+  val EXTRA_CONF_PREFIX = \"spark.plugins.__internal_conf__.\""
  }],
  "prId": 26170
}, {
  "comments": [{
    "author": {
      "login": "LucaCanali"
    },
    "body": "With the current implementation of executor.plugins we use plugin.getClass().getSimpleName() instead of getName. The advantage of getSimpleName is that it is more compact, it does not have \".\" characters, so it is easy to process when handling metrics data + when using getName, we will have long names and they will be repated for all emitted values.",
    "commit": "37ad680ec33ec6afac9d031897a549321e782d9c",
    "createdAt": "2019-10-24T20:02:13Z",
    "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.plugin\n+\n+import scala.collection.JavaConverters._\n+import scala.util.{Either, Left, Right}\n+\n+import org.apache.spark.{SparkContext, SparkEnv}\n+import org.apache.spark.api.plugin._\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.util.Utils\n+\n+sealed abstract class PluginContainer {\n+\n+  def shutdown(): Unit\n+\n+}\n+\n+private class DriverPluginContainer(sc: SparkContext, plugins: Seq[SparkPlugin])\n+  extends PluginContainer with Logging {\n+\n+  private val driverPlugins: Seq[(String, DriverPlugin)] = plugins.flatMap { p =>\n+    val driverPlugin = p.driverPlugin()\n+    if (driverPlugin != null) {\n+      val name = p.getClass().getName()\n+      val ctx = new PluginContextImpl(name, sc.env.rpcEnv, sc.env.metricsSystem, sc.conf,\n+        sc.env.executorId)\n+\n+      val extraConf = driverPlugin.init(sc, ctx)\n+      if (extraConf != null) {\n+        extraConf.asScala.foreach { case (k, v) =>\n+          sc.conf.set(s\"${PluginContainer.EXTRA_CONF_PREFIX}$name.$k\", v)\n+        }\n+      }\n+      ctx.registerMetrics()\n+      logInfo(s\"Initialized driver component for plugin $name.\")\n+      Some(p.getClass().getName() -> driverPlugin)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  if (driverPlugins.nonEmpty) {\n+    sc.env.rpcEnv.setupEndpoint(classOf[PluginEndpoint].getName(),\n+      new PluginEndpoint(driverPlugins.toMap, sc.env.rpcEnv))\n+  }\n+\n+  override def shutdown(): Unit = {\n+    driverPlugins.foreach { case (name, plugin) =>\n+      try {\n+        logDebug(s\"Stopping plugin $name.\")\n+        plugin.shutdown()\n+      } catch {\n+        case t: Throwable =>\n+          logInfo(s\"Exception while shutting down plugin $name.\", t)\n+      }\n+    }\n+  }\n+\n+}\n+\n+private class ExecutorPluginContainer(env: SparkEnv, plugins: Seq[SparkPlugin])\n+  extends PluginContainer with Logging {\n+\n+  private val executorPlugins: Seq[(String, ExecutorPlugin)] = {\n+    val allExtraConf = env.conf.getAllWithPrefix(PluginContainer.EXTRA_CONF_PREFIX)\n+\n+    plugins.flatMap { p =>\n+      val executorPlugin = p.executorPlugin()\n+      if (executorPlugin != null) {\n+        val name = p.getClass().getName()\n+        val prefix = name + \".\"\n+        val extraConf = allExtraConf\n+          .filter { case (k, v) => k.startsWith(prefix) }\n+          .map { case (k, v) => k.substring(prefix.length()) -> v }\n+          .toMap\n+          .asJava\n+        val ctx = new PluginContextImpl(name, env.rpcEnv, env.metricsSystem, env.conf,\n+          env.executorId)\n+        executorPlugin.init(ctx, extraConf)\n+        ctx.registerMetrics()\n+\n+        logInfo(s\"Initialized executor component for plugin $name.\")\n+        Some(p.getClass().getName() -> executorPlugin)",
    "line": 108
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "This was intentional. The full name has a much higher chance of being unique. I don't really see the advantages you mention; the dots don't make it any more complicated to process, and it's easy to get just the \"simple name\" if you want to, while the other way around is impossible.",
    "commit": "37ad680ec33ec6afac9d031897a549321e782d9c",
    "createdAt": "2019-10-24T20:06:46Z",
    "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.plugin\n+\n+import scala.collection.JavaConverters._\n+import scala.util.{Either, Left, Right}\n+\n+import org.apache.spark.{SparkContext, SparkEnv}\n+import org.apache.spark.api.plugin._\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.util.Utils\n+\n+sealed abstract class PluginContainer {\n+\n+  def shutdown(): Unit\n+\n+}\n+\n+private class DriverPluginContainer(sc: SparkContext, plugins: Seq[SparkPlugin])\n+  extends PluginContainer with Logging {\n+\n+  private val driverPlugins: Seq[(String, DriverPlugin)] = plugins.flatMap { p =>\n+    val driverPlugin = p.driverPlugin()\n+    if (driverPlugin != null) {\n+      val name = p.getClass().getName()\n+      val ctx = new PluginContextImpl(name, sc.env.rpcEnv, sc.env.metricsSystem, sc.conf,\n+        sc.env.executorId)\n+\n+      val extraConf = driverPlugin.init(sc, ctx)\n+      if (extraConf != null) {\n+        extraConf.asScala.foreach { case (k, v) =>\n+          sc.conf.set(s\"${PluginContainer.EXTRA_CONF_PREFIX}$name.$k\", v)\n+        }\n+      }\n+      ctx.registerMetrics()\n+      logInfo(s\"Initialized driver component for plugin $name.\")\n+      Some(p.getClass().getName() -> driverPlugin)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  if (driverPlugins.nonEmpty) {\n+    sc.env.rpcEnv.setupEndpoint(classOf[PluginEndpoint].getName(),\n+      new PluginEndpoint(driverPlugins.toMap, sc.env.rpcEnv))\n+  }\n+\n+  override def shutdown(): Unit = {\n+    driverPlugins.foreach { case (name, plugin) =>\n+      try {\n+        logDebug(s\"Stopping plugin $name.\")\n+        plugin.shutdown()\n+      } catch {\n+        case t: Throwable =>\n+          logInfo(s\"Exception while shutting down plugin $name.\", t)\n+      }\n+    }\n+  }\n+\n+}\n+\n+private class ExecutorPluginContainer(env: SparkEnv, plugins: Seq[SparkPlugin])\n+  extends PluginContainer with Logging {\n+\n+  private val executorPlugins: Seq[(String, ExecutorPlugin)] = {\n+    val allExtraConf = env.conf.getAllWithPrefix(PluginContainer.EXTRA_CONF_PREFIX)\n+\n+    plugins.flatMap { p =>\n+      val executorPlugin = p.executorPlugin()\n+      if (executorPlugin != null) {\n+        val name = p.getClass().getName()\n+        val prefix = name + \".\"\n+        val extraConf = allExtraConf\n+          .filter { case (k, v) => k.startsWith(prefix) }\n+          .map { case (k, v) => k.substring(prefix.length()) -> v }\n+          .toMap\n+          .asJava\n+        val ctx = new PluginContextImpl(name, env.rpcEnv, env.metricsSystem, env.conf,\n+          env.executorId)\n+        executorPlugin.init(ctx, extraConf)\n+        ctx.registerMetrics()\n+\n+        logInfo(s\"Initialized executor component for plugin $name.\")\n+        Some(p.getClass().getName() -> executorPlugin)",
    "line": 108
  }, {
    "author": {
      "login": "LucaCanali"
    },
    "body": "Good point.",
    "commit": "37ad680ec33ec6afac9d031897a549321e782d9c",
    "createdAt": "2019-10-28T13:40:11Z",
    "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.plugin\n+\n+import scala.collection.JavaConverters._\n+import scala.util.{Either, Left, Right}\n+\n+import org.apache.spark.{SparkContext, SparkEnv}\n+import org.apache.spark.api.plugin._\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.util.Utils\n+\n+sealed abstract class PluginContainer {\n+\n+  def shutdown(): Unit\n+\n+}\n+\n+private class DriverPluginContainer(sc: SparkContext, plugins: Seq[SparkPlugin])\n+  extends PluginContainer with Logging {\n+\n+  private val driverPlugins: Seq[(String, DriverPlugin)] = plugins.flatMap { p =>\n+    val driverPlugin = p.driverPlugin()\n+    if (driverPlugin != null) {\n+      val name = p.getClass().getName()\n+      val ctx = new PluginContextImpl(name, sc.env.rpcEnv, sc.env.metricsSystem, sc.conf,\n+        sc.env.executorId)\n+\n+      val extraConf = driverPlugin.init(sc, ctx)\n+      if (extraConf != null) {\n+        extraConf.asScala.foreach { case (k, v) =>\n+          sc.conf.set(s\"${PluginContainer.EXTRA_CONF_PREFIX}$name.$k\", v)\n+        }\n+      }\n+      ctx.registerMetrics()\n+      logInfo(s\"Initialized driver component for plugin $name.\")\n+      Some(p.getClass().getName() -> driverPlugin)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  if (driverPlugins.nonEmpty) {\n+    sc.env.rpcEnv.setupEndpoint(classOf[PluginEndpoint].getName(),\n+      new PluginEndpoint(driverPlugins.toMap, sc.env.rpcEnv))\n+  }\n+\n+  override def shutdown(): Unit = {\n+    driverPlugins.foreach { case (name, plugin) =>\n+      try {\n+        logDebug(s\"Stopping plugin $name.\")\n+        plugin.shutdown()\n+      } catch {\n+        case t: Throwable =>\n+          logInfo(s\"Exception while shutting down plugin $name.\", t)\n+      }\n+    }\n+  }\n+\n+}\n+\n+private class ExecutorPluginContainer(env: SparkEnv, plugins: Seq[SparkPlugin])\n+  extends PluginContainer with Logging {\n+\n+  private val executorPlugins: Seq[(String, ExecutorPlugin)] = {\n+    val allExtraConf = env.conf.getAllWithPrefix(PluginContainer.EXTRA_CONF_PREFIX)\n+\n+    plugins.flatMap { p =>\n+      val executorPlugin = p.executorPlugin()\n+      if (executorPlugin != null) {\n+        val name = p.getClass().getName()\n+        val prefix = name + \".\"\n+        val extraConf = allExtraConf\n+          .filter { case (k, v) => k.startsWith(prefix) }\n+          .map { case (k, v) => k.substring(prefix.length()) -> v }\n+          .toMap\n+          .asJava\n+        val ctx = new PluginContextImpl(name, env.rpcEnv, env.metricsSystem, env.conf,\n+          env.executorId)\n+        executorPlugin.init(ctx, extraConf)\n+        ctx.registerMetrics()\n+\n+        logInfo(s\"Initialized executor component for plugin $name.\")\n+        Some(p.getClass().getName() -> executorPlugin)",
    "line": 108
  }],
  "prId": 26170
}, {
  "comments": [{
    "author": {
      "login": "LucaCanali"
    },
    "body": "See comment about getName vs. getSimpleName below.",
    "commit": "37ad680ec33ec6afac9d031897a549321e782d9c",
    "createdAt": "2019-10-24T20:02:58Z",
    "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.plugin\n+\n+import scala.collection.JavaConverters._\n+import scala.util.{Either, Left, Right}\n+\n+import org.apache.spark.{SparkContext, SparkEnv}\n+import org.apache.spark.api.plugin._\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.util.Utils\n+\n+sealed abstract class PluginContainer {\n+\n+  def shutdown(): Unit\n+\n+}\n+\n+private class DriverPluginContainer(sc: SparkContext, plugins: Seq[SparkPlugin])\n+  extends PluginContainer with Logging {\n+\n+  private val driverPlugins: Seq[(String, DriverPlugin)] = plugins.flatMap { p =>\n+    val driverPlugin = p.driverPlugin()\n+    if (driverPlugin != null) {\n+      val name = p.getClass().getName()\n+      val ctx = new PluginContextImpl(name, sc.env.rpcEnv, sc.env.metricsSystem, sc.conf,\n+        sc.env.executorId)\n+\n+      val extraConf = driverPlugin.init(sc, ctx)\n+      if (extraConf != null) {\n+        extraConf.asScala.foreach { case (k, v) =>\n+          sc.conf.set(s\"${PluginContainer.EXTRA_CONF_PREFIX}$name.$k\", v)\n+        }\n+      }\n+      ctx.registerMetrics()\n+      logInfo(s\"Initialized driver component for plugin $name.\")\n+      Some(p.getClass().getName() -> driverPlugin)"
  }],
  "prId": 26170
}]