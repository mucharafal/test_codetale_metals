[{
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "would it make more sense to call it SparkHadoopMapReduceWriter to be more consistent? I understand we use \"new\" vs \"old\" in the RDD API, but that's always been fairly confusing to me.\n",
    "commit": "9380f91281587867d8a630199c01c4263bc2e197",
    "createdAt": "2016-11-05T20:11:08Z",
    "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io\n+\n+import java.text.SimpleDateFormat\n+import java.util.{Date, Locale}\n+\n+import scala.reflect.ClassTag\n+import scala.util.DynamicVariable\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapred.{JobConf, JobID}\n+import org.apache.hadoop.mapreduce._\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkException, TaskContext}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.executor.OutputMetrics\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.{SerializableConfiguration, Utils}\n+\n+/**\n+ * A helper object that saves an RDD using a Hadoop OutputFormat\n+ * (from the newer mapreduce API, not the old mapred API).\n+ */\n+private[spark]\n+object SparkNewHadoopWriter extends Logging {"
  }],
  "prId": 15769
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "maybe just remove this, since you have only 3 items here. In SQL there were a lot of items.\n",
    "commit": "9380f91281587867d8a630199c01c4263bc2e197",
    "createdAt": "2016-11-05T20:11:29Z",
    "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io\n+\n+import java.text.SimpleDateFormat\n+import java.util.{Date, Locale}\n+\n+import scala.reflect.ClassTag\n+import scala.util.DynamicVariable\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapred.{JobConf, JobID}\n+import org.apache.hadoop.mapreduce._\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkException, TaskContext}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.executor.OutputMetrics\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.{SerializableConfiguration, Utils}\n+\n+/**\n+ * A helper object that saves an RDD using a Hadoop OutputFormat\n+ * (from the newer mapreduce API, not the old mapred API).\n+ */\n+private[spark]\n+object SparkNewHadoopWriter extends Logging {\n+\n+  /** A shared job description for all the write tasks. */\n+  private class WriteJobDescription[K, V]("
  }],
  "prId": 15769
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "i'd move the creation of the commit protocol here here. The reason I put it outside in SQL was because streaming and batch needed to specify different protocols, but that problem doesn't exist in core.\n",
    "commit": "9380f91281587867d8a630199c01c4263bc2e197",
    "createdAt": "2016-11-05T20:12:06Z",
    "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io\n+\n+import java.text.SimpleDateFormat\n+import java.util.{Date, Locale}\n+\n+import scala.reflect.ClassTag\n+import scala.util.DynamicVariable\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapred.{JobConf, JobID}\n+import org.apache.hadoop.mapreduce._\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkException, TaskContext}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.executor.OutputMetrics\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.{SerializableConfiguration, Utils}\n+\n+/**\n+ * A helper object that saves an RDD using a Hadoop OutputFormat\n+ * (from the newer mapreduce API, not the old mapred API).\n+ */\n+private[spark]\n+object SparkNewHadoopWriter extends Logging {\n+\n+  /** A shared job description for all the write tasks. */\n+  private class WriteJobDescription[K, V](\n+      val jobTrackerId: String,\n+      val serializableHadoopConf: SerializableConfiguration,\n+      val outputFormat: Class[_ <: OutputFormat[K, V]])\n+    extends Serializable {\n+  }\n+\n+  /**\n+   * Basic work flow of this command is:\n+   * 1. Driver side setup, including output committer initialization and data source specific\n+   *    preparation work for the write job to be issued.\n+   * 2. Issues a write job consists of one or more executor side tasks, each of which writes all\n+   *    rows within an RDD partition.\n+   * 3. If no exception is thrown in a task, commits that task, otherwise aborts that task;  If any\n+   *    exception is thrown during task commitment, also aborts that task.\n+   * 4. If all tasks are committed, commit the job, otherwise aborts the job;  If any exception is\n+   *    thrown during job commitment, also aborts the job.\n+   */\n+  def write[K, V: ClassTag](\n+      sparkContext: SparkContext,\n+      rdd: RDD[(K, V)],\n+      committer: HadoopMapReduceCommitProtocol,"
  }],
  "prId": 15769
}]