[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "is it safe?",
    "commit": "e89293781c1feaed0bb7683d0059fe9663188833",
    "createdAt": "2018-06-21T20:18:08Z",
    "diffHunk": "@@ -104,12 +104,12 @@ object SparkHadoopWriter extends Logging {\n       jobTrackerId: String,\n       commitJobId: Int,\n       sparkPartitionId: Int,\n-      sparkAttemptNumber: Int,\n+      sparkTaskId: Long,\n       committer: FileCommitProtocol,\n       iterator: Iterator[(K, V)]): TaskCommitMessage = {\n     // Set up a task.\n     val taskContext = config.createTaskAttemptContext(\n-      jobTrackerId, commitJobId, sparkPartitionId, sparkAttemptNumber)\n+      jobTrackerId, commitJobId, sparkPartitionId, sparkTaskId.toInt)"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "task id is unique across the entire Spark application life cycle, which means we may have very large task id in a long-running micro-batch streaming application.\r\n\r\nIf we do need an int here, I'd suggest we combine `stageAttemptNumber` and `taskAttemptNumber` into a int, which is much less risky.(Spark won't have a lot of stage/task attempts)",
    "commit": "e89293781c1feaed0bb7683d0059fe9663188833",
    "createdAt": "2018-06-21T20:23:50Z",
    "diffHunk": "@@ -104,12 +104,12 @@ object SparkHadoopWriter extends Logging {\n       jobTrackerId: String,\n       commitJobId: Int,\n       sparkPartitionId: Int,\n-      sparkAttemptNumber: Int,\n+      sparkTaskId: Long,\n       committer: FileCommitProtocol,\n       iterator: Iterator[(K, V)]): TaskCommitMessage = {\n     // Set up a task.\n     val taskContext = config.createTaskAttemptContext(\n-      jobTrackerId, commitJobId, sparkPartitionId, sparkAttemptNumber)\n+      jobTrackerId, commitJobId, sparkPartitionId, sparkTaskId.toInt)"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "Streaming still generates separate jobs / stages for each batch, right?\r\n\r\nIn that case this should be fine; this would only be a problem if a single stage has enough tasks to cover all the integer space (4 billion tasks). That shouldn't be even possible since I doubt that you'd be able to have more than `Integer.MAX_VALUE` tasks (and even that is unlikely to ever happen).\r\n\r\nI could use `abs` here (and in the sql code) to avoid a negative value (potentially avoiding weird file names).",
    "commit": "e89293781c1feaed0bb7683d0059fe9663188833",
    "createdAt": "2018-06-21T21:09:43Z",
    "diffHunk": "@@ -104,12 +104,12 @@ object SparkHadoopWriter extends Logging {\n       jobTrackerId: String,\n       commitJobId: Int,\n       sparkPartitionId: Int,\n-      sparkAttemptNumber: Int,\n+      sparkTaskId: Long,\n       committer: FileCommitProtocol,\n       iterator: Iterator[(K, V)]): TaskCommitMessage = {\n     // Set up a task.\n     val taskContext = config.createTaskAttemptContext(\n-      jobTrackerId, commitJobId, sparkPartitionId, sparkAttemptNumber)\n+      jobTrackerId, commitJobId, sparkPartitionId, sparkTaskId.toInt)"
  }, {
    "author": {
      "login": "tgravescs"
    },
    "body": "I don't follow, the task ids increment across jobs.  so if you have a very long running application that continues to start new jobs you could potentially run out.",
    "commit": "e89293781c1feaed0bb7683d0059fe9663188833",
    "createdAt": "2018-06-21T21:37:39Z",
    "diffHunk": "@@ -104,12 +104,12 @@ object SparkHadoopWriter extends Logging {\n       jobTrackerId: String,\n       commitJobId: Int,\n       sparkPartitionId: Int,\n-      sparkAttemptNumber: Int,\n+      sparkTaskId: Long,\n       committer: FileCommitProtocol,\n       iterator: Iterator[(K, V)]): TaskCommitMessage = {\n     // Set up a task.\n     val taskContext = config.createTaskAttemptContext(\n-      jobTrackerId, commitJobId, sparkPartitionId, sparkAttemptNumber)\n+      jobTrackerId, commitJobId, sparkPartitionId, sparkTaskId.toInt)"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "But what does \"run out\" mean?\r\n\r\nIf your task ID goes past `Int.MaxValue`, you'll start getting negative values here. Eventually you'll get to a long value that wraps back again to `0` when cast to an integer:\r\n\r\n```\r\n(2L + Int.MaxValue + Int.MaxValue).toInt\r\nres2: Int = 0\r\n```\r\n\r\nSo for this to \"not work\", which means you'd have a conflict where two tasks will generate the same output file name based on all these values (stage, task, partition, etc, etc), you need that situation to happen, which means you need about 4 billion tasks in the same stage for this to be a problem.\r\n\r\nIn other situations, you may get weird values because of the cast, but it should still work.",
    "commit": "e89293781c1feaed0bb7683d0059fe9663188833",
    "createdAt": "2018-06-21T21:44:08Z",
    "diffHunk": "@@ -104,12 +104,12 @@ object SparkHadoopWriter extends Logging {\n       jobTrackerId: String,\n       commitJobId: Int,\n       sparkPartitionId: Int,\n-      sparkAttemptNumber: Int,\n+      sparkTaskId: Long,\n       committer: FileCommitProtocol,\n       iterator: Iterator[(K, V)]): TaskCommitMessage = {\n     // Set up a task.\n     val taskContext = config.createTaskAttemptContext(\n-      jobTrackerId, commitJobId, sparkPartitionId, sparkAttemptNumber)\n+      jobTrackerId, commitJobId, sparkPartitionId, sparkTaskId.toInt)"
  }, {
    "author": {
      "login": "tgravescs"
    },
    "body": "Ah I see what you are saying, we just need to make sure it going negative doesn't cause any side affects or anything unpexected",
    "commit": "e89293781c1feaed0bb7683d0059fe9663188833",
    "createdAt": "2018-06-21T21:53:30Z",
    "diffHunk": "@@ -104,12 +104,12 @@ object SparkHadoopWriter extends Logging {\n       jobTrackerId: String,\n       commitJobId: Int,\n       sparkPartitionId: Int,\n-      sparkAttemptNumber: Int,\n+      sparkTaskId: Long,\n       committer: FileCommitProtocol,\n       iterator: Iterator[(K, V)]): TaskCommitMessage = {\n     // Set up a task.\n     val taskContext = config.createTaskAttemptContext(\n-      jobTrackerId, commitJobId, sparkPartitionId, sparkAttemptNumber)\n+      jobTrackerId, commitJobId, sparkPartitionId, sparkTaskId.toInt)"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "I commented before I saw this thread, but I think it is better to use the TID because that is already exposed in the UI so it is better for tracking between UI tasks and logs. The combined attempt number isn't used anywhere so this would introduce another number to identify a task. And, shifting by 16 means that these grow huge anyway.",
    "commit": "e89293781c1feaed0bb7683d0059fe9663188833",
    "createdAt": "2018-06-22T19:11:19Z",
    "diffHunk": "@@ -104,12 +104,12 @@ object SparkHadoopWriter extends Logging {\n       jobTrackerId: String,\n       commitJobId: Int,\n       sparkPartitionId: Int,\n-      sparkAttemptNumber: Int,\n+      sparkTaskId: Long,\n       committer: FileCommitProtocol,\n       iterator: Iterator[(K, V)]): TaskCommitMessage = {\n     // Set up a task.\n     val taskContext = config.createTaskAttemptContext(\n-      jobTrackerId, commitJobId, sparkPartitionId, sparkAttemptNumber)\n+      jobTrackerId, commitJobId, sparkPartitionId, sparkTaskId.toInt)"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "To backport this, can we use the `.toInt` version? I think that should be safe.",
    "commit": "e89293781c1feaed0bb7683d0059fe9663188833",
    "createdAt": "2018-06-22T19:14:57Z",
    "diffHunk": "@@ -104,12 +104,12 @@ object SparkHadoopWriter extends Logging {\n       jobTrackerId: String,\n       commitJobId: Int,\n       sparkPartitionId: Int,\n-      sparkAttemptNumber: Int,\n+      sparkTaskId: Long,\n       committer: FileCommitProtocol,\n       iterator: Iterator[(K, V)]): TaskCommitMessage = {\n     // Set up a task.\n     val taskContext = config.createTaskAttemptContext(\n-      jobTrackerId, commitJobId, sparkPartitionId, sparkAttemptNumber)\n+      jobTrackerId, commitJobId, sparkPartitionId, sparkTaskId.toInt)"
  }],
  "prId": 21606
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "since it's not a simple `toInt` anymore, how about we combine stage and task attempt number?\r\n```\r\nval stageAttemptNumer = ...\r\nval taskAttempNumber = ...\r\nassert(stageAttemptNumer <= Short.MaxValue)\r\nassert(taskAttempNumber <= Short.MaxValue)\r\nval sparkAttempNumber = (stageAttemptNumer << 16) | taskAttempNumber\r\n```",
    "commit": "e89293781c1feaed0bb7683d0059fe9663188833",
    "createdAt": "2018-06-21T23:55:56Z",
    "diffHunk": "@@ -76,13 +76,29 @@ object SparkHadoopWriter extends Logging {\n     // Try to write all RDD partitions as a Hadoop OutputFormat.\n     try {\n       val ret = sparkContext.runJob(rdd, (context: TaskContext, iter: Iterator[(K, V)]) => {\n+        // Generate a positive integer task ID that is unique for the current stage. This makes a\n+        // few assumptions:\n+        // - the task ID is always positive\n+        // - stages cannot have more than Int.MaxValue\n+        // - the sum of task counts of all active stages doesn't exceed Int.MaxValue\n+        //\n+        // The first two are currently the case in Spark, while the last one is very unlikely to\n+        // occur. If it does, two tasks IDs on a single stage could have a clashing integer value,\n+        // which could lead to code that generates clashing file names for different tasks. Still,\n+        // if the commit coordinator is enabled, only one task would be allowed to commit."
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "we can also remove the assert and assume that, even we have so many attempts, they are not all active.",
    "commit": "e89293781c1feaed0bb7683d0059fe9663188833",
    "createdAt": "2018-06-21T23:57:12Z",
    "diffHunk": "@@ -76,13 +76,29 @@ object SparkHadoopWriter extends Logging {\n     // Try to write all RDD partitions as a Hadoop OutputFormat.\n     try {\n       val ret = sparkContext.runJob(rdd, (context: TaskContext, iter: Iterator[(K, V)]) => {\n+        // Generate a positive integer task ID that is unique for the current stage. This makes a\n+        // few assumptions:\n+        // - the task ID is always positive\n+        // - stages cannot have more than Int.MaxValue\n+        // - the sum of task counts of all active stages doesn't exceed Int.MaxValue\n+        //\n+        // The first two are currently the case in Spark, while the last one is very unlikely to\n+        // occur. If it does, two tasks IDs on a single stage could have a clashing integer value,\n+        // which could lead to code that generates clashing file names for different tasks. Still,\n+        // if the commit coordinator is enabled, only one task would be allowed to commit."
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "Ok, I'll use that. I think Spark might fail everything before you even go that high in attempt numbers anyway...",
    "commit": "e89293781c1feaed0bb7683d0059fe9663188833",
    "createdAt": "2018-06-22T01:00:48Z",
    "diffHunk": "@@ -76,13 +76,29 @@ object SparkHadoopWriter extends Logging {\n     // Try to write all RDD partitions as a Hadoop OutputFormat.\n     try {\n       val ret = sparkContext.runJob(rdd, (context: TaskContext, iter: Iterator[(K, V)]) => {\n+        // Generate a positive integer task ID that is unique for the current stage. This makes a\n+        // few assumptions:\n+        // - the task ID is always positive\n+        // - stages cannot have more than Int.MaxValue\n+        // - the sum of task counts of all active stages doesn't exceed Int.MaxValue\n+        //\n+        // The first two are currently the case in Spark, while the last one is very unlikely to\n+        // occur. If it does, two tasks IDs on a single stage could have a clashing integer value,\n+        // which could lead to code that generates clashing file names for different tasks. Still,\n+        // if the commit coordinator is enabled, only one task would be allowed to commit."
  }],
  "prId": 21606
}, {
  "comments": [{
    "author": {
      "login": "tgravescs"
    },
    "body": "perhaps we should rename taskId to be something more unique so we don't confuse it",
    "commit": "e89293781c1feaed0bb7683d0059fe9663188833",
    "createdAt": "2018-06-22T13:52:13Z",
    "diffHunk": "@@ -76,13 +76,17 @@ object SparkHadoopWriter extends Logging {\n     // Try to write all RDD partitions as a Hadoop OutputFormat.\n     try {\n       val ret = sparkContext.runJob(rdd, (context: TaskContext, iter: Iterator[(K, V)]) => {\n+        // SPARK-24552: Generate a unique \"task ID\" based on the stage and task atempt numbers.\n+        // Assumes that there won't be more than Short.MaxValue attempts, at least not concurrently.\n+        val taskId = (context.stageAttemptNumber << 16) | context.attemptNumber"
  }, {
    "author": {
      "login": "tgravescs"
    },
    "body": "maybe just something like uniqueTaskId or specialTaskId but not a big deal.",
    "commit": "e89293781c1feaed0bb7683d0059fe9663188833",
    "createdAt": "2018-06-22T13:52:59Z",
    "diffHunk": "@@ -76,13 +76,17 @@ object SparkHadoopWriter extends Logging {\n     // Try to write all RDD partitions as a Hadoop OutputFormat.\n     try {\n       val ret = sparkContext.runJob(rdd, (context: TaskContext, iter: Iterator[(K, V)]) => {\n+        // SPARK-24552: Generate a unique \"task ID\" based on the stage and task atempt numbers.\n+        // Assumes that there won't be more than Short.MaxValue attempts, at least not concurrently.\n+        val taskId = (context.stageAttemptNumber << 16) | context.attemptNumber"
  }],
  "prId": 21606
}, {
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "I don't think we should generate an ID this way. We already have a unique ID that is exposed in the Spark UI. I'd much rather make it clear that the TID passed to committers as an attempt ID is the same as the TID in the stage view. That makes debugging easier. Going with this approach just introduces yet another number to track an attempt.",
    "commit": "e89293781c1feaed0bb7683d0059fe9663188833",
    "createdAt": "2018-06-22T19:04:28Z",
    "diffHunk": "@@ -76,13 +76,17 @@ object SparkHadoopWriter extends Logging {\n     // Try to write all RDD partitions as a Hadoop OutputFormat.\n     try {\n       val ret = sparkContext.runJob(rdd, (context: TaskContext, iter: Iterator[(K, V)]) => {\n+        // SPARK-24552: Generate a unique \"attempt ID\" based on the stage and task atempt numbers.\n+        // Assumes that there won't be more than Short.MaxValue attempts, at least not concurrently.\n+        val attemptId = (context.stageAttemptNumber << 16) | context.attemptNumber"
  }, {
    "author": {
      "login": "tgravescs"
    },
    "body": "the problem is that taskid is a long, we can't change the hadoop api for that, and to me its more possible to have a valid task id > 2^32.    It might not be ideal to do it this way but I think its a good bug fix especially for now, we can file a follow on to improve if we have ideas or want to change interface",
    "commit": "e89293781c1feaed0bb7683d0059fe9663188833",
    "createdAt": "2018-06-22T19:23:26Z",
    "diffHunk": "@@ -76,13 +76,17 @@ object SparkHadoopWriter extends Logging {\n     // Try to write all RDD partitions as a Hadoop OutputFormat.\n     try {\n       val ret = sparkContext.runJob(rdd, (context: TaskContext, iter: Iterator[(K, V)]) => {\n+        // SPARK-24552: Generate a unique \"attempt ID\" based on the stage and task atempt numbers.\n+        // Assumes that there won't be more than Short.MaxValue attempts, at least not concurrently.\n+        val attemptId = (context.stageAttemptNumber << 16) | context.attemptNumber"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "Okay, that makes sense if this is just for Hadoop attempt IDs. Maybe that's a good thing to put in the comment as well?",
    "commit": "e89293781c1feaed0bb7683d0059fe9663188833",
    "createdAt": "2018-06-22T19:29:08Z",
    "diffHunk": "@@ -76,13 +76,17 @@ object SparkHadoopWriter extends Logging {\n     // Try to write all RDD partitions as a Hadoop OutputFormat.\n     try {\n       val ret = sparkContext.runJob(rdd, (context: TaskContext, iter: Iterator[(K, V)]) => {\n+        // SPARK-24552: Generate a unique \"attempt ID\" based on the stage and task atempt numbers.\n+        // Assumes that there won't be more than Short.MaxValue attempts, at least not concurrently.\n+        val attemptId = (context.stageAttemptNumber << 16) | context.attemptNumber"
  }],
  "prId": 21606
}]