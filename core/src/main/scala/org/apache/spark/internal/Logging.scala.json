[{
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "I think this block should be the only one you have. It would someone manually adding filters outside of Spark, and also fixes this match not being exhaustive (it doesn't handle `ca.getFirstFilter()` being null, which shouldn't happen but can if code outside Spark messes with the appender directly, like this code).",
    "commit": "893ff759cff0df3aa1d2773de21e3f4adcfed0c8",
    "createdAt": "2019-01-29T00:19:29Z",
    "diffHunk": "@@ -213,7 +205,21 @@ private[spark] object Logging {\n         rootLogger.setLevel(defaultRootLevel)\n         rootLogger.getAllAppenders().asScala.foreach {\n           case ca: ConsoleAppender =>\n-            ca.setThreshold(consoleAppenderToThreshold.get(ca))\n+            // SparkShellLoggingFilter is the last filter\n+            ca.getFirstFilter() match {\n+              case ssf: SparkShellLoggingFilter =>\n+                ca.clearFilters()\n+              case f: org.apache.log4j.spi.Filter =>\n+                var previous = f"
  }, {
    "author": {
      "login": "ankuriitg"
    },
    "body": "I added another check to make sure it is exhaustive",
    "commit": "893ff759cff0df3aa1d2773de21e3f4adcfed0c8",
    "createdAt": "2019-01-29T18:28:34Z",
    "diffHunk": "@@ -213,7 +205,21 @@ private[spark] object Logging {\n         rootLogger.setLevel(defaultRootLevel)\n         rootLogger.getAllAppenders().asScala.foreach {\n           case ca: ConsoleAppender =>\n-            ca.setThreshold(consoleAppenderToThreshold.get(ca))\n+            // SparkShellLoggingFilter is the last filter\n+            ca.getFirstFilter() match {\n+              case ssf: SparkShellLoggingFilter =>\n+                ca.clearFilters()\n+              case f: org.apache.log4j.spi.Filter =>\n+                var previous = f"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "I still think the match is not correct. e.g. if the first filter is the one you added, you remove all filters, regardless of whether it's the first one or not.\r\n\r\nThe code in this `case` correctly handles all possible combinations.",
    "commit": "893ff759cff0df3aa1d2773de21e3f4adcfed0c8",
    "createdAt": "2019-01-29T20:31:43Z",
    "diffHunk": "@@ -213,7 +205,21 @@ private[spark] object Logging {\n         rootLogger.setLevel(defaultRootLevel)\n         rootLogger.getAllAppenders().asScala.foreach {\n           case ca: ConsoleAppender =>\n-            ca.setThreshold(consoleAppenderToThreshold.get(ca))\n+            // SparkShellLoggingFilter is the last filter\n+            ca.getFirstFilter() match {\n+              case ssf: SparkShellLoggingFilter =>\n+                ca.clearFilters()\n+              case f: org.apache.log4j.spi.Filter =>\n+                var previous = f"
  }, {
    "author": {
      "login": "ankuriitg"
    },
    "body": "I just realized that resetting `sparkShellThresholdLevel` is all that I need here (based on your other comment for the test case).",
    "commit": "893ff759cff0df3aa1d2773de21e3f4adcfed0c8",
    "createdAt": "2019-01-29T21:06:00Z",
    "diffHunk": "@@ -213,7 +205,21 @@ private[spark] object Logging {\n         rootLogger.setLevel(defaultRootLevel)\n         rootLogger.getAllAppenders().asScala.foreach {\n           case ca: ConsoleAppender =>\n-            ca.setThreshold(consoleAppenderToThreshold.get(ca))\n+            // SparkShellLoggingFilter is the last filter\n+            ca.getFirstFilter() match {\n+              case ssf: SparkShellLoggingFilter =>\n+                ca.clearFilters()\n+              case f: org.apache.log4j.spi.Filter =>\n+                var previous = f"
  }],
  "prId": 23675
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Doesn't just `!loggingEvent.getLevel().eq(rootLevel)` cover all cases?\r\n\r\nIf the log message level is different than the root level, then the only way it can happen is if the logger was customized.\r\n\r\nIf it's equal, then you need to check whether the logger's level was customized, do decide whether to filter based on the shell threshold.\r\n\r\n(BTW the method comment doesn't fully explain the logic here.)",
    "commit": "893ff759cff0df3aa1d2773de21e3f4adcfed0c8",
    "createdAt": "2019-01-29T20:37:29Z",
    "diffHunk": "@@ -229,3 +237,33 @@ private[spark] object Logging {\n     \"org.slf4j.impl.Log4jLoggerFactory\".equals(binderClass)\n   }\n }\n+\n+private class SparkShellLoggingFilter() extends Filter {\n+\n+  /**\n+   * If sparkShellThresholdLevel is not defined, this filter is a no-op.\n+   * If log level of event is lower than thresholdLevel, then the decision is made based on\n+   * whether the log came from root or some custom configuration\n+   * @param loggingEvent\n+   * @return decision for accept/deny log event\n+   */\n+  def decide(loggingEvent: LoggingEvent): Int = {\n+    val thresholdLevel = Logging.sparkShellThresholdLevel\n+    if (thresholdLevel == null) {\n+      return Filter.NEUTRAL\n+    }\n+    val rootLevel = LogManager.getRootLogger().getLevel()\n+    if (loggingEvent.getLevel().isGreaterOrEqual(thresholdLevel) ||\n+      !loggingEvent.getLevel().eq(rootLevel)) {"
  }, {
    "author": {
      "login": "ankuriitg"
    },
    "body": "So, the `loggingEvent.getLevel().isGreaterOrEqual(thresholdLevel)` is just an optimization. If the log message level is same as root level but it is greater than threshold level, then we still allow the event to pass through, irrespective of whether it was customized or not.",
    "commit": "893ff759cff0df3aa1d2773de21e3f4adcfed0c8",
    "createdAt": "2019-01-29T20:42:47Z",
    "diffHunk": "@@ -229,3 +237,33 @@ private[spark] object Logging {\n     \"org.slf4j.impl.Log4jLoggerFactory\".equals(binderClass)\n   }\n }\n+\n+private class SparkShellLoggingFilter() extends Filter {\n+\n+  /**\n+   * If sparkShellThresholdLevel is not defined, this filter is a no-op.\n+   * If log level of event is lower than thresholdLevel, then the decision is made based on\n+   * whether the log came from root or some custom configuration\n+   * @param loggingEvent\n+   * @return decision for accept/deny log event\n+   */\n+  def decide(loggingEvent: LoggingEvent): Int = {\n+    val thresholdLevel = Logging.sparkShellThresholdLevel\n+    if (thresholdLevel == null) {\n+      return Filter.NEUTRAL\n+    }\n+    val rootLevel = LogManager.getRootLogger().getLevel()\n+    if (loggingEvent.getLevel().isGreaterOrEqual(thresholdLevel) ||\n+      !loggingEvent.getLevel().eq(rootLevel)) {"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "That's my question. Doesn't `!loggingEvent.getLevel().eq(rootLevel)` already cover that?\r\n\r\nThe other check is only an optimization if `thresholdLevel` is the same as `rootLevel`, and in that case a better optimization would be to not set `thresholdLevel` (so the null check triggers instead).\r\n\r\nUsing the good old truth table:\r\n\r\n```\r\nLevel       Thresh      Root      Action\r\nINFO       WARN       INFO     Check for override\r\nWARN     WARN       INFO     Allow\r\nDEBUG   WARN       INFO     Allow\r\n```\r\n\r\nSo the resulting action doesn't seem dependent on the threshold level at all, assuming it's always different than the root logger's level.",
    "commit": "893ff759cff0df3aa1d2773de21e3f4adcfed0c8",
    "createdAt": "2019-01-29T20:50:52Z",
    "diffHunk": "@@ -229,3 +237,33 @@ private[spark] object Logging {\n     \"org.slf4j.impl.Log4jLoggerFactory\".equals(binderClass)\n   }\n }\n+\n+private class SparkShellLoggingFilter() extends Filter {\n+\n+  /**\n+   * If sparkShellThresholdLevel is not defined, this filter is a no-op.\n+   * If log level of event is lower than thresholdLevel, then the decision is made based on\n+   * whether the log came from root or some custom configuration\n+   * @param loggingEvent\n+   * @return decision for accept/deny log event\n+   */\n+  def decide(loggingEvent: LoggingEvent): Int = {\n+    val thresholdLevel = Logging.sparkShellThresholdLevel\n+    if (thresholdLevel == null) {\n+      return Filter.NEUTRAL\n+    }\n+    val rootLevel = LogManager.getRootLogger().getLevel()\n+    if (loggingEvent.getLevel().isGreaterOrEqual(thresholdLevel) ||\n+      !loggingEvent.getLevel().eq(rootLevel)) {"
  }, {
    "author": {
      "login": "ankuriitg"
    },
    "body": "Got it. I just checked and `thresholdLevel` is only set when it is different from `rootLevel`. So, this check is not needed and I will remove it.",
    "commit": "893ff759cff0df3aa1d2773de21e3f4adcfed0c8",
    "createdAt": "2019-01-29T21:00:57Z",
    "diffHunk": "@@ -229,3 +237,33 @@ private[spark] object Logging {\n     \"org.slf4j.impl.Log4jLoggerFactory\".equals(binderClass)\n   }\n }\n+\n+private class SparkShellLoggingFilter() extends Filter {\n+\n+  /**\n+   * If sparkShellThresholdLevel is not defined, this filter is a no-op.\n+   * If log level of event is lower than thresholdLevel, then the decision is made based on\n+   * whether the log came from root or some custom configuration\n+   * @param loggingEvent\n+   * @return decision for accept/deny log event\n+   */\n+  def decide(loggingEvent: LoggingEvent): Int = {\n+    val thresholdLevel = Logging.sparkShellThresholdLevel\n+    if (thresholdLevel == null) {\n+      return Filter.NEUTRAL\n+    }\n+    val rootLevel = LogManager.getRootLogger().getLevel()\n+    if (loggingEvent.getLevel().isGreaterOrEqual(thresholdLevel) ||\n+      !loggingEvent.getLevel().eq(rootLevel)) {"
  }],
  "prId": 23675
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "remove `()`",
    "commit": "893ff759cff0df3aa1d2773de21e3f4adcfed0c8",
    "createdAt": "2019-01-30T18:48:34Z",
    "diffHunk": "@@ -229,3 +218,31 @@ private[spark] object Logging {\n     \"org.slf4j.impl.Log4jLoggerFactory\".equals(binderClass)\n   }\n }\n+\n+private class SparkShellLoggingFilter() extends Filter {",
    "line": 59
  }],
  "prId": 23675
}]