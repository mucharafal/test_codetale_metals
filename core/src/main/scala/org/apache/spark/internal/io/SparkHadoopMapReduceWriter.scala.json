[{
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "I just decided to resemble [FileFormatWriter.scala#L187-L206](https://github.com/apache/spark/blob/1ae4652b7e1f77a984b8459c778cb06c814192c5/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatWriter.scala#L187-L206).",
    "commit": "48048622067f092ed247bc555e5461c073894a9c",
    "createdAt": "2016-12-01T13:32:45Z",
    "diffHunk": "@@ -163,12 +163,22 @@ object SparkHadoopMapReduceWriter extends Logging {\n             outputMetricsAndBytesWrittenCallback, recordsWritten)\n           recordsWritten += 1\n         }\n-\n+        if (writer != null) {\n+          writer.close(taskContext)\n+          writer = null\n+        }\n         committer.commitTask(taskContext)\n       }(catchBlock = {\n-        committer.abortTask(taskContext)\n-        logError(s\"Task ${taskContext.getTaskAttemptID} aborted.\")\n-      }, finallyBlock = writer.close(taskContext))\n+        try {\n+          if (writer != null) {\n+            writer.close(taskContext)\n+            writer = null\n+          }\n+        } finally {\n+          committer.abortTask(taskContext)\n+          logError(s\"Task ${taskContext.getTaskAttemptID} aborted.\")\n+        }\n+      })",
    "line": 41
  }, {
    "author": {
      "login": "srowen"
    },
    "body": "What about something like\r\n\r\n```\r\nval committer = ...\r\ntry {\r\n  ...\r\n  val writer = ...\r\n  try {\r\n    ...\r\n  } finally {\r\n    writer.close()\r\n  }\r\n  committer.commitTask(...)\r\n} catch {\r\n  ...\r\n  commiter.abortTask(...)\r\n}\r\n```\r\n\r\nIt's simpler but am I missing why it wouldn't work?",
    "commit": "48048622067f092ed247bc555e5461c073894a9c",
    "createdAt": "2016-12-02T11:17:46Z",
    "diffHunk": "@@ -163,12 +163,22 @@ object SparkHadoopMapReduceWriter extends Logging {\n             outputMetricsAndBytesWrittenCallback, recordsWritten)\n           recordsWritten += 1\n         }\n-\n+        if (writer != null) {\n+          writer.close(taskContext)\n+          writer = null\n+        }\n         committer.commitTask(taskContext)\n       }(catchBlock = {\n-        committer.abortTask(taskContext)\n-        logError(s\"Task ${taskContext.getTaskAttemptID} aborted.\")\n-      }, finallyBlock = writer.close(taskContext))\n+        try {\n+          if (writer != null) {\n+            writer.close(taskContext)\n+            writer = null\n+          }\n+        } finally {\n+          committer.abortTask(taskContext)\n+          logError(s\"Task ${taskContext.getTaskAttemptID} aborted.\")\n+        }\n+      })",
    "line": 41
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "I think I was worried of trivial and useless things. Let me try to follow this suggestion.",
    "commit": "48048622067f092ed247bc555e5461c073894a9c",
    "createdAt": "2016-12-02T12:31:33Z",
    "diffHunk": "@@ -163,12 +163,22 @@ object SparkHadoopMapReduceWriter extends Logging {\n             outputMetricsAndBytesWrittenCallback, recordsWritten)\n           recordsWritten += 1\n         }\n-\n+        if (writer != null) {\n+          writer.close(taskContext)\n+          writer = null\n+        }\n         committer.commitTask(taskContext)\n       }(catchBlock = {\n-        committer.abortTask(taskContext)\n-        logError(s\"Task ${taskContext.getTaskAttemptID} aborted.\")\n-      }, finallyBlock = writer.close(taskContext))\n+        try {\n+          if (writer != null) {\n+            writer.close(taskContext)\n+            writer = null\n+          }\n+        } finally {\n+          committer.abortTask(taskContext)\n+          logError(s\"Task ${taskContext.getTaskAttemptID} aborted.\")\n+        }\n+      })",
    "line": 41
  }],
  "prId": 16098
}, {
  "comments": [{
    "author": {
      "login": "mridulm"
    },
    "body": "There is a behavior change here, inability to create record writer will cause abortTask. Is that expected ?",
    "commit": "48048622067f092ed247bc555e5461c073894a9c",
    "createdAt": "2016-12-02T14:26:35Z",
    "diffHunk": "@@ -146,29 +146,32 @@ object SparkHadoopMapReduceWriter extends Logging {\n       case c: Configurable => c.setConf(hadoopConf)\n       case _ => ()\n     }\n-    val writer = taskFormat.getRecordWriter(taskContext)\n-      .asInstanceOf[RecordWriter[K, V]]\n-    require(writer != null, \"Unable to obtain RecordWriter\")\n     var recordsWritten = 0L\n \n     // Write all rows in RDD partition.\n     try {\n       val ret = Utils.tryWithSafeFinallyAndFailureCallbacks {\n-        while (iterator.hasNext) {\n-          val pair = iterator.next()\n-          writer.write(pair._1, pair._2)\n-\n-          // Update bytes written metric every few records\n-          SparkHadoopWriterUtils.maybeUpdateOutputMetrics(\n-            outputMetricsAndBytesWrittenCallback, recordsWritten)\n-          recordsWritten += 1\n+        val writer = taskFormat.getRecordWriter(taskContext)\n+        require(writer != null, \"Unable to obtain RecordWriter\")"
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Ah, actually, I left a comment about this and then removed back because I thought I might be too much worried.\r\n\r\nI at least checked `TextOutputFormat.getRecordWriter(..)` is able to create the attempt folder and `FileOutputCommitter.abortTask(..)` removes this. If this is located in the original place, it'd not remove the directory (although this case would be really rare). ",
    "commit": "48048622067f092ed247bc555e5461c073894a9c",
    "createdAt": "2016-12-02T15:22:57Z",
    "diffHunk": "@@ -146,29 +146,32 @@ object SparkHadoopMapReduceWriter extends Logging {\n       case c: Configurable => c.setConf(hadoopConf)\n       case _ => ()\n     }\n-    val writer = taskFormat.getRecordWriter(taskContext)\n-      .asInstanceOf[RecordWriter[K, V]]\n-    require(writer != null, \"Unable to obtain RecordWriter\")\n     var recordsWritten = 0L\n \n     // Write all rows in RDD partition.\n     try {\n       val ret = Utils.tryWithSafeFinallyAndFailureCallbacks {\n-        while (iterator.hasNext) {\n-          val pair = iterator.next()\n-          writer.write(pair._1, pair._2)\n-\n-          // Update bytes written metric every few records\n-          SparkHadoopWriterUtils.maybeUpdateOutputMetrics(\n-            outputMetricsAndBytesWrittenCallback, recordsWritten)\n-          recordsWritten += 1\n+        val writer = taskFormat.getRecordWriter(taskContext)\n+        require(writer != null, \"Unable to obtain RecordWriter\")"
  }],
  "prId": 16098
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "I don't think writer can be null here, but no big deal.",
    "commit": "48048622067f092ed247bc555e5461c073894a9c",
    "createdAt": "2016-12-02T16:41:50Z",
    "diffHunk": "@@ -163,12 +164,23 @@ object SparkHadoopMapReduceWriter extends Logging {\n             outputMetricsAndBytesWrittenCallback, recordsWritten)\n           recordsWritten += 1\n         }\n-\n+        if (writer != null) {",
    "line": 22
  }],
  "prId": 16098
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "So this is also adding a defensive close before aborting, not just before committing? if that's important at all, OK. It didn't do that before.",
    "commit": "48048622067f092ed247bc555e5461c073894a9c",
    "createdAt": "2016-12-02T16:42:21Z",
    "diffHunk": "@@ -163,12 +164,23 @@ object SparkHadoopMapReduceWriter extends Logging {\n             outputMetricsAndBytesWrittenCallback, recordsWritten)\n           recordsWritten += 1\n         }\n-\n+        if (writer != null) {\n+          writer.close(taskContext)\n+          writer = null\n+        }\n         committer.commitTask(taskContext)\n       }(catchBlock = {\n-        committer.abortTask(taskContext)\n-        logError(s\"Task ${taskContext.getTaskAttemptID} aborted.\")\n-      }, finallyBlock = writer.close(taskContext))\n+        // If there is an error, release resource and then abort the task.\n+        try {\n+          if (writer != null) {",
    "line": 33
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Oh yes. Thank you. ~~Will address the comment above.~~",
    "commit": "48048622067f092ed247bc555e5461c073894a9c",
    "createdAt": "2016-12-02T16:48:28Z",
    "diffHunk": "@@ -163,12 +164,23 @@ object SparkHadoopMapReduceWriter extends Logging {\n             outputMetricsAndBytesWrittenCallback, recordsWritten)\n           recordsWritten += 1\n         }\n-\n+        if (writer != null) {\n+          writer.close(taskContext)\n+          writer = null\n+        }\n         committer.commitTask(taskContext)\n       }(catchBlock = {\n-        committer.abortTask(taskContext)\n-        logError(s\"Task ${taskContext.getTaskAttemptID} aborted.\")\n-      }, finallyBlock = writer.close(taskContext))\n+        // If there is an error, release resource and then abort the task.\n+        try {\n+          if (writer != null) {",
    "line": 33
  }],
  "prId": 16098
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Ah, but do you still want to null it here so that it doesn't also cause a close() in the catch block? the commit could fail.",
    "commit": "48048622067f092ed247bc555e5461c073894a9c",
    "createdAt": "2016-12-02T16:58:21Z",
    "diffHunk": "@@ -163,12 +164,20 @@ object SparkHadoopMapReduceWriter extends Logging {\n             outputMetricsAndBytesWrittenCallback, recordsWritten)\n           recordsWritten += 1\n         }\n-\n+        writer.close(taskContext)"
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Doh!",
    "commit": "48048622067f092ed247bc555e5461c073894a9c",
    "createdAt": "2016-12-02T17:00:27Z",
    "diffHunk": "@@ -163,12 +164,20 @@ object SparkHadoopMapReduceWriter extends Logging {\n             outputMetricsAndBytesWrittenCallback, recordsWritten)\n           recordsWritten += 1\n         }\n-\n+        writer.close(taskContext)"
  }],
  "prId": 16098
}]