[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "One difference from the previous API: we can't have a general `setValue` method, as it needs the intermedia type which is not exposed by the new API. For example, `AverageAccumulator` doesn't have `setValue`\n",
    "commit": "124568b3eeb7e0a657b2fbe4f54bb85543b7ffa3",
    "createdAt": "2016-04-22T15:01:24Z",
    "diffHunk": "@@ -0,0 +1,299 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.{lang => jl}\n+import java.io.{ObjectInputStream, ObjectOutputStream}\n+import java.util.concurrent.atomic.AtomicLong\n+import javax.annotation.concurrent.GuardedBy\n+\n+import org.apache.spark.scheduler.AccumulableInfo\n+import org.apache.spark.util.Utils\n+\n+\n+private[spark] case class AccumulatorMetadata(\n+    id: Long,\n+    name: Option[String],\n+    countFailedValues: Boolean) extends Serializable\n+\n+\n+abstract class NewAccumulator[IN, OUT] extends Serializable {\n+  private[spark] var metadata: AccumulatorMetadata = _\n+\n+  private[spark] def register(\n+      sc: SparkContext,\n+      id: Long = AccumulatorContext.newId(),\n+      name: Option[String] = None,\n+      countFailedValues: Boolean = false): Unit = {\n+    if (this.metadata != null) {\n+      throw new IllegalStateException(\"Cannot register an Accumulator twice.\")\n+    }\n+    this.metadata = AccumulatorMetadata(id, name, countFailedValues)\n+    AccumulatorContext.register(this)\n+    sc.cleaner.foreach(_.registerAccumulatorForCleanup(this))\n+  }\n+\n+  private[spark] def assertRegistered(): Unit = {\n+    if (metadata == null) {\n+      throw new IllegalStateException(\"Accumulator is not registered yet\")\n+    }\n+  }\n+\n+  def id: Long = {\n+    assertRegistered()\n+    metadata.id\n+  }\n+\n+  def initialize(): Unit = {}\n+\n+  def add(v: IN): Unit\n+\n+  def +=(v: IN): Unit = add(v)\n+\n+  def merge(other: NewAccumulator[IN, OUT]): Unit\n+\n+  def ++=(other: NewAccumulator[IN, OUT]): Unit = merge(other)\n+\n+  def value: OUT\n+\n+  private[spark] def toInfo(update: Option[Any], value: Option[Any]): AccumulableInfo = {\n+    assertRegistered()\n+    val isInternal = metadata.name.exists(_.startsWith(InternalAccumulator.METRICS_PREFIX))\n+    new AccumulableInfo(\n+      metadata.id, metadata.name, update, value, isInternal, metadata.countFailedValues)\n+  }\n+\n+  // Called by Java when serializing an object\n+  private def writeObject(out: ObjectOutputStream): Unit = Utils.tryOrIOException {\n+    assertRegistered()\n+    out.defaultWriteObject()\n+  }\n+\n+  // Called by Java when deserializing an object\n+  private def readObject(in: ObjectInputStream): Unit = Utils.tryOrIOException {\n+    in.defaultReadObject()\n+    initialize()\n+\n+    // Automatically register the accumulator when it is deserialized with the task closure.\n+    // This is for external accumulators and internal ones that do not represent task level\n+    // metrics, e.g. internal SQL metrics, which are per-operator.\n+    val taskContext = TaskContext.get()\n+    if (taskContext != null) {\n+      taskContext.registerAccumulator(this)\n+    }\n+  }\n+}\n+\n+object AccumulatorContext {\n+\n+  /**\n+   * This global map holds the original accumulator objects that are created on the driver.\n+   * It keeps weak references to these objects so that accumulators can be garbage-collected\n+   * once the RDDs and user-code that reference them are cleaned up.\n+   * TODO: Don't use a global map; these should be tied to a SparkContext (SPARK-13051).\n+   */\n+  @GuardedBy(\"AccumulatorContext\")\n+  private val originals = new java.util.HashMap[Long, jl.ref.WeakReference[NewAccumulator[_, _]]]\n+\n+  private[this] val nextId = new AtomicLong(0L)\n+\n+  /**\n+   * Return a globally unique ID for a new [[NewAccumulator]].\n+   * Note: Once you copy the [[NewAccumulator]] the ID is no longer unique.\n+   */\n+  def newId(): Long = nextId.getAndIncrement\n+\n+  /**\n+   * Register an [[NewAccumulator]] created on the driver such that it can be used on the executors.\n+   *\n+   * All accumulators registered here can later be used as a container for accumulating partial\n+   * values across multiple tasks. This is what [[org.apache.spark.scheduler.DAGScheduler]] does.\n+   * Note: if an accumulator is registered here, it should also be registered with the active\n+   * context cleaner for cleanup so as to avoid memory leaks.\n+   *\n+   * If an [[NewAccumulator]] with the same ID was already registered, this does nothing instead\n+   * of overwriting it. This happens when we copy accumulators, e.g. when we reconstruct\n+   * [[org.apache.spark.executor.TaskMetrics]] from accumulator updates.\n+   */\n+  def register(a: NewAccumulator[_, _]): Unit = synchronized {\n+    if (!originals.containsKey(a.id)) {\n+      originals.put(a.id, new jl.ref.WeakReference[NewAccumulator[_, _]](a))\n+    }\n+  }\n+\n+  /**\n+   * Unregister the [[NewAccumulator]] with the given ID, if any.\n+   */\n+  def remove(id: Long): Unit = synchronized {\n+    originals.remove(id)\n+  }\n+\n+  /**\n+   * Return the [[NewAccumulator]] registered with the given ID, if any.\n+   */\n+  def get(id: Long): Option[NewAccumulator[_, _]] = synchronized {\n+    Option(originals.get(id)).map { ref =>\n+      // Since we are storing weak references, we must check whether the underlying data is valid.\n+      val acc = ref.get\n+      if (acc eq null) {\n+        throw new IllegalAccessError(s\"Attempted to access garbage collected accumulator $id\")\n+      }\n+      acc\n+    }\n+  }\n+\n+  /**\n+   * Clear all registered [[NewAccumulator]]s. For testing only.\n+   */\n+  def clear(): Unit = synchronized {\n+    originals.clear()\n+  }\n+}\n+\n+\n+class IntAccumulator extends NewAccumulator[jl.Integer, jl.Integer] {\n+  @transient private[this] var _sum = 0\n+\n+  override def add(v: jl.Integer): Unit = {\n+    _sum += v\n+  }\n+\n+  override def merge(other: NewAccumulator[jl.Integer, jl.Integer]): Unit = other match {\n+    case o: IntAccumulator => _sum += o.sum\n+    case _ => throw new UnsupportedOperationException(\n+      s\"Cannot merge ${this.getClass.getName} with ${other.getClass.getName}\")\n+  }\n+\n+  override def value: jl.Integer = _sum\n+\n+  def sum: Int = _sum\n+\n+  def setValue(newValue: Int): Unit = _sum = newValue\n+}\n+\n+\n+class LongAccumulator extends NewAccumulator[jl.Long, jl.Long] {\n+  @transient private[this] var _sum = 0L\n+\n+  override def add(v: jl.Long): Unit = {\n+    _sum += v\n+  }\n+\n+  override def merge(other: NewAccumulator[jl.Long, jl.Long]): Unit = other match {\n+    case o: LongAccumulator => _sum += o.sum\n+    case _ => throw new UnsupportedOperationException(\n+      s\"Cannot merge ${this.getClass.getName} with ${other.getClass.getName}\")\n+  }\n+\n+  override def value: jl.Long = _sum\n+\n+  def sum: Long = _sum\n+\n+  def setValue(newValue: Long): Unit = _sum = newValue\n+}\n+\n+\n+class DoubleAccumulator extends NewAccumulator[jl.Double, jl.Double] {\n+  @transient private[this] var _sum = 0.0\n+\n+  override def add(v: jl.Double): Unit = {\n+    _sum += v\n+  }\n+\n+  override def merge(other: NewAccumulator[jl.Double, jl.Double]): Unit = other match {\n+    case o: DoubleAccumulator => _sum += o.sum\n+    case _ => throw new UnsupportedOperationException(\n+      s\"Cannot merge ${this.getClass.getName} with ${other.getClass.getName}\")\n+  }\n+\n+  override def value: jl.Double = _sum\n+\n+  def sum: Double = _sum\n+\n+  def setValue(newValue: Double): Unit = _sum = newValue\n+}\n+\n+\n+class AverageAccumulator extends NewAccumulator[jl.Double, jl.Double] {"
  }, {
    "author": {
      "login": "holdenk"
    },
    "body": "I think getting rid of `setValue` is great, in the consistent accumulators based under the old API I had to just throw an exception if people were using `setValue`\n",
    "commit": "124568b3eeb7e0a657b2fbe4f54bb85543b7ffa3",
    "createdAt": "2016-04-23T03:59:45Z",
    "diffHunk": "@@ -0,0 +1,299 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.{lang => jl}\n+import java.io.{ObjectInputStream, ObjectOutputStream}\n+import java.util.concurrent.atomic.AtomicLong\n+import javax.annotation.concurrent.GuardedBy\n+\n+import org.apache.spark.scheduler.AccumulableInfo\n+import org.apache.spark.util.Utils\n+\n+\n+private[spark] case class AccumulatorMetadata(\n+    id: Long,\n+    name: Option[String],\n+    countFailedValues: Boolean) extends Serializable\n+\n+\n+abstract class NewAccumulator[IN, OUT] extends Serializable {\n+  private[spark] var metadata: AccumulatorMetadata = _\n+\n+  private[spark] def register(\n+      sc: SparkContext,\n+      id: Long = AccumulatorContext.newId(),\n+      name: Option[String] = None,\n+      countFailedValues: Boolean = false): Unit = {\n+    if (this.metadata != null) {\n+      throw new IllegalStateException(\"Cannot register an Accumulator twice.\")\n+    }\n+    this.metadata = AccumulatorMetadata(id, name, countFailedValues)\n+    AccumulatorContext.register(this)\n+    sc.cleaner.foreach(_.registerAccumulatorForCleanup(this))\n+  }\n+\n+  private[spark] def assertRegistered(): Unit = {\n+    if (metadata == null) {\n+      throw new IllegalStateException(\"Accumulator is not registered yet\")\n+    }\n+  }\n+\n+  def id: Long = {\n+    assertRegistered()\n+    metadata.id\n+  }\n+\n+  def initialize(): Unit = {}\n+\n+  def add(v: IN): Unit\n+\n+  def +=(v: IN): Unit = add(v)\n+\n+  def merge(other: NewAccumulator[IN, OUT]): Unit\n+\n+  def ++=(other: NewAccumulator[IN, OUT]): Unit = merge(other)\n+\n+  def value: OUT\n+\n+  private[spark] def toInfo(update: Option[Any], value: Option[Any]): AccumulableInfo = {\n+    assertRegistered()\n+    val isInternal = metadata.name.exists(_.startsWith(InternalAccumulator.METRICS_PREFIX))\n+    new AccumulableInfo(\n+      metadata.id, metadata.name, update, value, isInternal, metadata.countFailedValues)\n+  }\n+\n+  // Called by Java when serializing an object\n+  private def writeObject(out: ObjectOutputStream): Unit = Utils.tryOrIOException {\n+    assertRegistered()\n+    out.defaultWriteObject()\n+  }\n+\n+  // Called by Java when deserializing an object\n+  private def readObject(in: ObjectInputStream): Unit = Utils.tryOrIOException {\n+    in.defaultReadObject()\n+    initialize()\n+\n+    // Automatically register the accumulator when it is deserialized with the task closure.\n+    // This is for external accumulators and internal ones that do not represent task level\n+    // metrics, e.g. internal SQL metrics, which are per-operator.\n+    val taskContext = TaskContext.get()\n+    if (taskContext != null) {\n+      taskContext.registerAccumulator(this)\n+    }\n+  }\n+}\n+\n+object AccumulatorContext {\n+\n+  /**\n+   * This global map holds the original accumulator objects that are created on the driver.\n+   * It keeps weak references to these objects so that accumulators can be garbage-collected\n+   * once the RDDs and user-code that reference them are cleaned up.\n+   * TODO: Don't use a global map; these should be tied to a SparkContext (SPARK-13051).\n+   */\n+  @GuardedBy(\"AccumulatorContext\")\n+  private val originals = new java.util.HashMap[Long, jl.ref.WeakReference[NewAccumulator[_, _]]]\n+\n+  private[this] val nextId = new AtomicLong(0L)\n+\n+  /**\n+   * Return a globally unique ID for a new [[NewAccumulator]].\n+   * Note: Once you copy the [[NewAccumulator]] the ID is no longer unique.\n+   */\n+  def newId(): Long = nextId.getAndIncrement\n+\n+  /**\n+   * Register an [[NewAccumulator]] created on the driver such that it can be used on the executors.\n+   *\n+   * All accumulators registered here can later be used as a container for accumulating partial\n+   * values across multiple tasks. This is what [[org.apache.spark.scheduler.DAGScheduler]] does.\n+   * Note: if an accumulator is registered here, it should also be registered with the active\n+   * context cleaner for cleanup so as to avoid memory leaks.\n+   *\n+   * If an [[NewAccumulator]] with the same ID was already registered, this does nothing instead\n+   * of overwriting it. This happens when we copy accumulators, e.g. when we reconstruct\n+   * [[org.apache.spark.executor.TaskMetrics]] from accumulator updates.\n+   */\n+  def register(a: NewAccumulator[_, _]): Unit = synchronized {\n+    if (!originals.containsKey(a.id)) {\n+      originals.put(a.id, new jl.ref.WeakReference[NewAccumulator[_, _]](a))\n+    }\n+  }\n+\n+  /**\n+   * Unregister the [[NewAccumulator]] with the given ID, if any.\n+   */\n+  def remove(id: Long): Unit = synchronized {\n+    originals.remove(id)\n+  }\n+\n+  /**\n+   * Return the [[NewAccumulator]] registered with the given ID, if any.\n+   */\n+  def get(id: Long): Option[NewAccumulator[_, _]] = synchronized {\n+    Option(originals.get(id)).map { ref =>\n+      // Since we are storing weak references, we must check whether the underlying data is valid.\n+      val acc = ref.get\n+      if (acc eq null) {\n+        throw new IllegalAccessError(s\"Attempted to access garbage collected accumulator $id\")\n+      }\n+      acc\n+    }\n+  }\n+\n+  /**\n+   * Clear all registered [[NewAccumulator]]s. For testing only.\n+   */\n+  def clear(): Unit = synchronized {\n+    originals.clear()\n+  }\n+}\n+\n+\n+class IntAccumulator extends NewAccumulator[jl.Integer, jl.Integer] {\n+  @transient private[this] var _sum = 0\n+\n+  override def add(v: jl.Integer): Unit = {\n+    _sum += v\n+  }\n+\n+  override def merge(other: NewAccumulator[jl.Integer, jl.Integer]): Unit = other match {\n+    case o: IntAccumulator => _sum += o.sum\n+    case _ => throw new UnsupportedOperationException(\n+      s\"Cannot merge ${this.getClass.getName} with ${other.getClass.getName}\")\n+  }\n+\n+  override def value: jl.Integer = _sum\n+\n+  def sum: Int = _sum\n+\n+  def setValue(newValue: Int): Unit = _sum = newValue\n+}\n+\n+\n+class LongAccumulator extends NewAccumulator[jl.Long, jl.Long] {\n+  @transient private[this] var _sum = 0L\n+\n+  override def add(v: jl.Long): Unit = {\n+    _sum += v\n+  }\n+\n+  override def merge(other: NewAccumulator[jl.Long, jl.Long]): Unit = other match {\n+    case o: LongAccumulator => _sum += o.sum\n+    case _ => throw new UnsupportedOperationException(\n+      s\"Cannot merge ${this.getClass.getName} with ${other.getClass.getName}\")\n+  }\n+\n+  override def value: jl.Long = _sum\n+\n+  def sum: Long = _sum\n+\n+  def setValue(newValue: Long): Unit = _sum = newValue\n+}\n+\n+\n+class DoubleAccumulator extends NewAccumulator[jl.Double, jl.Double] {\n+  @transient private[this] var _sum = 0.0\n+\n+  override def add(v: jl.Double): Unit = {\n+    _sum += v\n+  }\n+\n+  override def merge(other: NewAccumulator[jl.Double, jl.Double]): Unit = other match {\n+    case o: DoubleAccumulator => _sum += o.sum\n+    case _ => throw new UnsupportedOperationException(\n+      s\"Cannot merge ${this.getClass.getName} with ${other.getClass.getName}\")\n+  }\n+\n+  override def value: jl.Double = _sum\n+\n+  def sum: Double = _sum\n+\n+  def setValue(newValue: Double): Unit = _sum = newValue\n+}\n+\n+\n+class AverageAccumulator extends NewAccumulator[jl.Double, jl.Double] {"
  }],
  "prId": 12612
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "cc @rxin , I didn't send the accumulator back for a serialization problem.\n\nBasically when we send accumulator from driver to executors, we don't want to send its current value(think about list accumulator, we definitely don't wanna send the current list to executors.).\nBut when we send accumulator from executors to driver, we do need to send the current value.\n\nOne possible solution is to have 2 local variables for each accumulator, one for driver, one for executors. But it's a lot of trouble when accumulators have complex intermedia type, e.g. average accumulator. So I end up with this apporach.\n",
    "commit": "124568b3eeb7e0a657b2fbe4f54bb85543b7ffa3",
    "createdAt": "2016-04-25T16:23:24Z",
    "diffHunk": "@@ -0,0 +1,356 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.{lang => jl}\n+import java.io.{ObjectInputStream, ObjectOutputStream}\n+import java.util.concurrent.atomic.AtomicLong\n+import javax.annotation.concurrent.GuardedBy\n+\n+import org.apache.spark.scheduler.AccumulableInfo\n+import org.apache.spark.util.Utils\n+\n+\n+private[spark] case class AccumulatorMetadata(\n+    id: Long,\n+    name: Option[String],\n+    countFailedValues: Boolean) extends Serializable\n+\n+trait UpdatedValue extends Serializable"
  }, {
    "author": {
      "login": "holdenk"
    },
    "body": "Another potential problem with sending Accumulators over the wire, with the proposed API from the JIRA, is that the Accumulators register them selves inside of readObject.\n",
    "commit": "124568b3eeb7e0a657b2fbe4f54bb85543b7ffa3",
    "createdAt": "2016-04-25T21:39:29Z",
    "diffHunk": "@@ -0,0 +1,356 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.{lang => jl}\n+import java.io.{ObjectInputStream, ObjectOutputStream}\n+import java.util.concurrent.atomic.AtomicLong\n+import javax.annotation.concurrent.GuardedBy\n+\n+import org.apache.spark.scheduler.AccumulableInfo\n+import org.apache.spark.util.Utils\n+\n+\n+private[spark] case class AccumulatorMetadata(\n+    id: Long,\n+    name: Option[String],\n+    countFailedValues: Boolean) extends Serializable\n+\n+trait UpdatedValue extends Serializable"
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "@cloud-fan  Why can't we send the current list? The current list as far as I understand will always be zero sized? We can just create a copy of the accumulator for sending to the executors.\n",
    "commit": "124568b3eeb7e0a657b2fbe4f54bb85543b7ffa3",
    "createdAt": "2016-04-25T21:43:37Z",
    "diffHunk": "@@ -0,0 +1,356 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.{lang => jl}\n+import java.io.{ObjectInputStream, ObjectOutputStream}\n+import java.util.concurrent.atomic.AtomicLong\n+import javax.annotation.concurrent.GuardedBy\n+\n+import org.apache.spark.scheduler.AccumulableInfo\n+import org.apache.spark.util.Utils\n+\n+\n+private[spark] case class AccumulatorMetadata(\n+    id: Long,\n+    name: Option[String],\n+    countFailedValues: Boolean) extends Serializable\n+\n+trait UpdatedValue extends Serializable"
  }, {
    "author": {
      "login": "holdenk"
    },
    "body": "If the accumulator was used in two separate tasks it could have built up some values from the first task in the driver before the second task. But always sending a zeroed copy to the executor would be an OK solution to that.\n",
    "commit": "124568b3eeb7e0a657b2fbe4f54bb85543b7ffa3",
    "createdAt": "2016-04-25T21:47:16Z",
    "diffHunk": "@@ -0,0 +1,356 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.{lang => jl}\n+import java.io.{ObjectInputStream, ObjectOutputStream}\n+import java.util.concurrent.atomic.AtomicLong\n+import javax.annotation.concurrent.GuardedBy\n+\n+import org.apache.spark.scheduler.AccumulableInfo\n+import org.apache.spark.util.Utils\n+\n+\n+private[spark] case class AccumulatorMetadata(\n+    id: Long,\n+    name: Option[String],\n+    countFailedValues: Boolean) extends Serializable\n+\n+trait UpdatedValue extends Serializable"
  }],
  "prId": 12612
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "i'd expose the specialized methods as public APIs, and just have them name the same thing as the generic methods so all users will automatically benefit from them.\n",
    "commit": "124568b3eeb7e0a657b2fbe4f54bb85543b7ffa3",
    "createdAt": "2016-04-26T00:49:27Z",
    "diffHunk": "@@ -0,0 +1,356 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.{lang => jl}\n+import java.io.{ObjectInputStream, ObjectOutputStream}\n+import java.util.concurrent.atomic.AtomicLong\n+import javax.annotation.concurrent.GuardedBy\n+\n+import org.apache.spark.scheduler.AccumulableInfo\n+import org.apache.spark.util.Utils\n+\n+\n+private[spark] case class AccumulatorMetadata(\n+    id: Long,\n+    name: Option[String],\n+    countFailedValues: Boolean) extends Serializable\n+\n+trait UpdatedValue extends Serializable\n+\n+private[spark] class UpdatedValueString(s: String) extends UpdatedValue {\n+  override def toString: String = s\n+}\n+\n+private[spark] case class AccumulatorUpdates(\n+    id: Long,\n+    name: Option[String],\n+    countFailedValues: Boolean,\n+    value: UpdatedValue) extends Serializable\n+\n+abstract class NewAccumulator[IN, OUT] extends Serializable {\n+  private[spark] var metadata: AccumulatorMetadata = _\n+  private[this] var atDriverSide = true\n+\n+  private[spark] def register(\n+      sc: SparkContext,\n+      name: Option[String] = None,\n+      countFailedValues: Boolean = false): Unit = {\n+    if (this.metadata != null) {\n+      throw new IllegalStateException(\"Cannot register an Accumulator twice.\")\n+    }\n+    this.metadata = AccumulatorMetadata(AccumulatorContext.newId(), name, countFailedValues)\n+    AccumulatorContext.register(this)\n+    sc.cleaner.foreach(_.registerAccumulatorForCleanup(this))\n+  }\n+\n+  def id: Long = {\n+    assert(metadata != null, \"Cannot get accumulator id with null metadata\")\n+    metadata.id\n+  }\n+\n+  def name: Option[String] = {\n+    assert(metadata != null, \"Cannot get accumulator name with null metadata\")\n+    metadata.name\n+  }\n+\n+  def countFailedValues: Boolean = {\n+    assert(metadata != null, \"Cannot get accumulator countFailedValues with null metadata\")\n+    metadata.countFailedValues\n+  }\n+\n+  private[spark] def toInfo(update: Option[Any], value: Option[Any]): AccumulableInfo = {\n+    val isInternal = name.exists(_.startsWith(InternalAccumulator.METRICS_PREFIX))\n+    new AccumulableInfo(id, name, update, value, isInternal, countFailedValues)\n+  }\n+\n+  final private[spark] def isAtDriverSide: Boolean = atDriverSide\n+\n+  final def isRegistered: Boolean =\n+    metadata != null && AccumulatorContext.originals.containsKey(metadata.id)\n+\n+  def initialize(): Unit = {}\n+\n+  def add(v: IN): Unit\n+\n+  def +=(v: IN): Unit = add(v)\n+\n+  def updatedValue: UpdatedValue\n+\n+  def isNoOp(updates: UpdatedValue): Boolean\n+\n+  def applyUpdates(updates: UpdatedValue): Unit\n+\n+  final def value: OUT = {\n+    if (atDriverSide) {\n+      localValue\n+    } else {\n+      throw new UnsupportedOperationException(\"Can't read accumulator value in task\")\n+    }\n+  }\n+\n+  def localValue: OUT\n+\n+  private[spark] def getUpdates: AccumulatorUpdates =\n+    AccumulatorUpdates(id, name, countFailedValues, updatedValue)\n+\n+  // Called by Java when serializing an object\n+  private def writeObject(out: ObjectOutputStream): Unit = Utils.tryOrIOException {\n+    if (atDriverSide && !isRegistered) {\n+      throw new IllegalStateException(\n+        \"Accumulator must be registered before serialize and send to executor\")\n+    }\n+    out.defaultWriteObject()\n+  }\n+\n+  // Called by Java when deserializing an object\n+  private def readObject(in: ObjectInputStream): Unit = Utils.tryOrIOException {\n+    in.defaultReadObject()\n+    initialize()\n+    atDriverSide = false\n+\n+    // Automatically register the accumulator when it is deserialized with the task closure.\n+    // This is for external accumulators and internal ones that do not represent task level\n+    // metrics, e.g. internal SQL metrics, which are per-operator.\n+    val taskContext = TaskContext.get()\n+    if (taskContext != null) {\n+      taskContext.registerAccumulator(this)\n+    }\n+  }\n+}\n+\n+\n+private[spark] object AccumulatorContext {\n+\n+  /**\n+   * This global map holds the original accumulator objects that are created on the driver.\n+   * It keeps weak references to these objects so that accumulators can be garbage-collected\n+   * once the RDDs and user-code that reference them are cleaned up.\n+   * TODO: Don't use a global map; these should be tied to a SparkContext (SPARK-13051).\n+   */\n+  @GuardedBy(\"AccumulatorContext\")\n+  val originals = new java.util.HashMap[Long, jl.ref.WeakReference[NewAccumulator[_, _]]]\n+\n+  private[this] val nextId = new AtomicLong(0L)\n+\n+  /**\n+   * Return a globally unique ID for a new [[Accumulator]].\n+   * Note: Once you copy the [[Accumulator]] the ID is no longer unique.\n+   */\n+  def newId(): Long = nextId.getAndIncrement\n+\n+  /**\n+   * Register an [[Accumulator]] created on the driver such that it can be used on the executors.\n+   *\n+   * All accumulators registered here can later be used as a container for accumulating partial\n+   * values across multiple tasks. This is what [[org.apache.spark.scheduler.DAGScheduler]] does.\n+   * Note: if an accumulator is registered here, it should also be registered with the active\n+   * context cleaner for cleanup so as to avoid memory leaks.\n+   *\n+   * If an [[Accumulator]] with the same ID was already registered, this does nothing instead\n+   * of overwriting it. We will never register same accumulator twice, this is just a sanity check.\n+   */\n+  def register(a: NewAccumulator[_, _]): Unit = synchronized {\n+    if (!originals.containsKey(a.id)) {\n+      originals.put(a.id, new jl.ref.WeakReference[NewAccumulator[_, _]](a))\n+    }\n+  }\n+\n+  /**\n+   * Unregister the [[Accumulator]] with the given ID, if any.\n+   */\n+  def remove(id: Long): Unit = synchronized {\n+    originals.remove(id)\n+  }\n+\n+  /**\n+   * Return the [[Accumulator]] registered with the given ID, if any.\n+   */\n+  def get(id: Long): Option[NewAccumulator[_, _]] = synchronized {\n+    Option(originals.get(id)).map { ref =>\n+      // Since we are storing weak references, we must check whether the underlying data is valid.\n+      val acc = ref.get\n+      if (acc eq null) {\n+        throw new IllegalAccessError(s\"Attempted to access garbage collected accumulator $id\")\n+      }\n+      acc\n+    }\n+  }\n+\n+  /**\n+   * Clear all registered [[Accumulator]]s. For testing only.\n+   */\n+  def clear(): Unit = synchronized {\n+    originals.clear()\n+  }\n+}\n+\n+\n+case class UpdatedLongValue(l: Long) extends UpdatedValue {\n+  override def toString: String = l.toString\n+}\n+\n+class LongAccumulator extends NewAccumulator[jl.Long, jl.Long] {\n+  @transient private[this] var _sum = 0L\n+\n+  override def updatedValue: UpdatedValue = new UpdatedLongValue(_sum)\n+\n+  override def add(v: jl.Long): Unit = _sum += v\n+\n+  private[spark] def unboxAdd(v: Long): Unit = _sum += v\n+\n+  private[spark] def unboxValue: Long = _sum\n+\n+  private[spark] def setValue(newValue: Long): Unit = _sum = newValue\n+\n+  override def isNoOp(updates: UpdatedValue): Boolean = {\n+    val v = updates.asInstanceOf[UpdatedLongValue].l\n+    v == 0\n+  }\n+\n+  override def applyUpdates(updates: UpdatedValue): Unit = {\n+    val v = updates.asInstanceOf[UpdatedLongValue].l\n+    _sum += v\n+  }\n+\n+  override def localValue: jl.Long = _sum\n+}\n+\n+\n+case class UpdatedDoubleValue(d: Double) extends UpdatedValue {\n+  override def toString: String = d.toString\n+}\n+\n+class DoubleAccumulator extends NewAccumulator[jl.Double, jl.Double] {\n+  @transient private[this] var _sum = 0.0\n+\n+  override def updatedValue: UpdatedValue = new UpdatedDoubleValue(_sum)\n+\n+  override def add(v: jl.Double): Unit = _sum += v\n+\n+  private[spark] def unboxAdd(v: Double): Unit = _sum += v"
  }],
  "prId": 12612
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "This is really a big problem...\n\nWe need some serialization hooks to support sending accumulator back from executors, and I tried 2 approaches but both failed:\n1. Add a writing hook, which resets the accumulator before send it from driver to executor. The problem is we can't just reset, the accumulator states should be kept at driver side. And the java serializing hook isn't flex enough to allow us do a copy or something. One possible workaround is to create an `AccumulatorWrapper` so that we can have full control of accumulator serialization. But this will complicate the hierarchy.\n2. Add a reading hook, which resets the accumlator after deserialization. Unfortunately it doesn't work when `Accumulator` is a base class. By the time `readObject` is called, child's fields are not initialized yet. Calling `reset` here is no-op, the values of child's fileds will be filled later.\n\nGenerally speaking, `writeObject` and `readObject` is not a good serialization hook. We'd either figure out some tricky to workaround it, or find out other better serialization hooks. (or do not send accumulators back)\n\n@rxin any ideas?\n",
    "commit": "124568b3eeb7e0a657b2fbe4f54bb85543b7ffa3",
    "createdAt": "2016-04-26T15:07:14Z",
    "diffHunk": "@@ -0,0 +1,333 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.{lang => jl}\n+import java.io.{ObjectInputStream, ObjectOutputStream}\n+import java.util.concurrent.atomic.AtomicLong\n+import javax.annotation.concurrent.GuardedBy\n+\n+import org.apache.spark.scheduler.AccumulableInfo\n+import org.apache.spark.util.Utils\n+\n+\n+private[spark] case class AccumulatorMetadata(\n+    id: Long,\n+    name: Option[String],\n+    countFailedValues: Boolean) extends Serializable\n+\n+\n+abstract class NewAccumulator[IN, OUT] extends Serializable {\n+  private[spark] var metadata: AccumulatorMetadata = _\n+  private[this] var atDriverSide = true\n+\n+  private[spark] def register(\n+      sc: SparkContext,\n+      name: Option[String] = None,\n+      countFailedValues: Boolean = false): Unit = {\n+    if (this.metadata != null) {\n+      throw new IllegalStateException(\"Cannot register an Accumulator twice.\")\n+    }\n+    this.metadata = AccumulatorMetadata(AccumulatorContext.newId(), name, countFailedValues)\n+    AccumulatorContext.register(this)\n+    sc.cleaner.foreach(_.registerAccumulatorForCleanup(this))\n+  }\n+\n+  final def isRegistered: Boolean =\n+    metadata != null && AccumulatorContext.originals.containsKey(metadata.id)\n+\n+  private def assertMetadataNotNull(): Unit = {\n+    if (metadata == null) {\n+      throw new IllegalAccessError(\"The metadata of this accumulator has not been assigned yet.\")\n+    }\n+  }\n+\n+  def id: Long = {\n+    assertMetadataNotNull()\n+    metadata.id\n+  }\n+\n+  def name: Option[String] = {\n+    assertMetadataNotNull()\n+    metadata.name\n+  }\n+\n+  def countFailedValues: Boolean = {\n+    assertMetadataNotNull()\n+    metadata.countFailedValues\n+  }\n+\n+  private[spark] def toInfo(update: Option[Any], value: Option[Any]): AccumulableInfo = {\n+    val isInternal = name.exists(_.startsWith(InternalAccumulator.METRICS_PREFIX))\n+    new AccumulableInfo(id, name, update, value, isInternal, countFailedValues)\n+  }\n+\n+  final private[spark] def isAtDriverSide: Boolean = atDriverSide\n+\n+  def copyAndReset(): NewAccumulator[IN, OUT]\n+\n+  def isZero(): Boolean\n+\n+  def add(v: IN): Unit\n+\n+  def +=(v: IN): Unit = add(v)\n+\n+  def merge(other: NewAccumulator[IN, OUT]): Unit\n+\n+  final def value: OUT = {\n+    if (atDriverSide) {\n+      localValue\n+    } else {\n+      throw new UnsupportedOperationException(\"Can't read accumulator value in task\")\n+    }\n+  }\n+\n+  def localValue: OUT\n+\n+  // Called by Java when serializing an object\n+  private def writeObject(out: ObjectOutputStream): Unit = Utils.tryOrIOException {\n+    if (atDriverSide) {\n+      if (!isRegistered) {\n+        throw new UnsupportedOperationException(\n+          \"Accumulator must be registered before send to executor\")\n+      }\n+      // TODO: this is wrong."
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "as discussed offline, writeReplace\n",
    "commit": "124568b3eeb7e0a657b2fbe4f54bb85543b7ffa3",
    "createdAt": "2016-04-27T07:52:31Z",
    "diffHunk": "@@ -0,0 +1,333 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.{lang => jl}\n+import java.io.{ObjectInputStream, ObjectOutputStream}\n+import java.util.concurrent.atomic.AtomicLong\n+import javax.annotation.concurrent.GuardedBy\n+\n+import org.apache.spark.scheduler.AccumulableInfo\n+import org.apache.spark.util.Utils\n+\n+\n+private[spark] case class AccumulatorMetadata(\n+    id: Long,\n+    name: Option[String],\n+    countFailedValues: Boolean) extends Serializable\n+\n+\n+abstract class NewAccumulator[IN, OUT] extends Serializable {\n+  private[spark] var metadata: AccumulatorMetadata = _\n+  private[this] var atDriverSide = true\n+\n+  private[spark] def register(\n+      sc: SparkContext,\n+      name: Option[String] = None,\n+      countFailedValues: Boolean = false): Unit = {\n+    if (this.metadata != null) {\n+      throw new IllegalStateException(\"Cannot register an Accumulator twice.\")\n+    }\n+    this.metadata = AccumulatorMetadata(AccumulatorContext.newId(), name, countFailedValues)\n+    AccumulatorContext.register(this)\n+    sc.cleaner.foreach(_.registerAccumulatorForCleanup(this))\n+  }\n+\n+  final def isRegistered: Boolean =\n+    metadata != null && AccumulatorContext.originals.containsKey(metadata.id)\n+\n+  private def assertMetadataNotNull(): Unit = {\n+    if (metadata == null) {\n+      throw new IllegalAccessError(\"The metadata of this accumulator has not been assigned yet.\")\n+    }\n+  }\n+\n+  def id: Long = {\n+    assertMetadataNotNull()\n+    metadata.id\n+  }\n+\n+  def name: Option[String] = {\n+    assertMetadataNotNull()\n+    metadata.name\n+  }\n+\n+  def countFailedValues: Boolean = {\n+    assertMetadataNotNull()\n+    metadata.countFailedValues\n+  }\n+\n+  private[spark] def toInfo(update: Option[Any], value: Option[Any]): AccumulableInfo = {\n+    val isInternal = name.exists(_.startsWith(InternalAccumulator.METRICS_PREFIX))\n+    new AccumulableInfo(id, name, update, value, isInternal, countFailedValues)\n+  }\n+\n+  final private[spark] def isAtDriverSide: Boolean = atDriverSide\n+\n+  def copyAndReset(): NewAccumulator[IN, OUT]\n+\n+  def isZero(): Boolean\n+\n+  def add(v: IN): Unit\n+\n+  def +=(v: IN): Unit = add(v)\n+\n+  def merge(other: NewAccumulator[IN, OUT]): Unit\n+\n+  final def value: OUT = {\n+    if (atDriverSide) {\n+      localValue\n+    } else {\n+      throw new UnsupportedOperationException(\"Can't read accumulator value in task\")\n+    }\n+  }\n+\n+  def localValue: OUT\n+\n+  // Called by Java when serializing an object\n+  private def writeObject(out: ObjectOutputStream): Unit = Utils.tryOrIOException {\n+    if (atDriverSide) {\n+      if (!isRegistered) {\n+        throw new UnsupportedOperationException(\n+          \"Accumulator must be registered before send to executor\")\n+      }\n+      // TODO: this is wrong."
  }],
  "prId": 12612
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "these should be javadoc of the methods, rather than in the classdoc\n",
    "commit": "124568b3eeb7e0a657b2fbe4f54bb85543b7ffa3",
    "createdAt": "2016-04-28T02:31:34Z",
    "diffHunk": "@@ -0,0 +1,356 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.{lang => jl}\n+import java.io.ObjectInputStream\n+import java.util.concurrent.atomic.AtomicLong\n+import javax.annotation.concurrent.GuardedBy\n+\n+import org.apache.spark.scheduler.AccumulableInfo\n+import org.apache.spark.util.Utils\n+\n+\n+private[spark] case class AccumulatorMetadata(\n+    id: Long,\n+    name: Option[String],\n+    countFailedValues: Boolean) extends Serializable\n+\n+\n+/**\n+ * The base class for accumulators, that can accumulate inputs of type `IN`, and produce output of\n+ * type `OUT`.  Implementations must define following methods:\n+ *  - isZero:       tell if this accumulator is zero value or not. e.g. for a counter accumulator,"
  }],
  "prId": 12612
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "i don't think you need this paragraph. it is pretty obvious from the interface.\n",
    "commit": "124568b3eeb7e0a657b2fbe4f54bb85543b7ffa3",
    "createdAt": "2016-04-28T02:32:11Z",
    "diffHunk": "@@ -0,0 +1,356 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.{lang => jl}\n+import java.io.ObjectInputStream\n+import java.util.concurrent.atomic.AtomicLong\n+import javax.annotation.concurrent.GuardedBy\n+\n+import org.apache.spark.scheduler.AccumulableInfo\n+import org.apache.spark.util.Utils\n+\n+\n+private[spark] case class AccumulatorMetadata(\n+    id: Long,\n+    name: Option[String],\n+    countFailedValues: Boolean) extends Serializable\n+\n+\n+/**\n+ * The base class for accumulators, that can accumulate inputs of type `IN`, and produce output of\n+ * type `OUT`.  Implementations must define following methods:\n+ *  - isZero:       tell if this accumulator is zero value or not. e.g. for a counter accumulator,\n+ *                  0 is zero value; for a list accumulator, Nil is zero value.\n+ *  - copyAndReset: create a new copy of this accumulator, which is zero value. i.e. call `isZero`\n+ *                  on the copy must return true.\n+ *  - add:          defines how to accumulate the inputs. e.g. it can be a simple `+=` for counter\n+ *                  accumulator\n+ *  - merge:        defines how to merge another accumulator of same type.\n+ *  - localValue:   defines how to produce the output by the current state of this accumulator.\n+ *\n+ * The implementations decide how to store intermediate values, e.g. a long field for a counter"
  }],
  "prId": 12612
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "do we need this method?\n",
    "commit": "124568b3eeb7e0a657b2fbe4f54bb85543b7ffa3",
    "createdAt": "2016-04-28T02:35:12Z",
    "diffHunk": "@@ -0,0 +1,356 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.{lang => jl}\n+import java.io.ObjectInputStream\n+import java.util.concurrent.atomic.AtomicLong\n+import javax.annotation.concurrent.GuardedBy\n+\n+import org.apache.spark.scheduler.AccumulableInfo\n+import org.apache.spark.util.Utils\n+\n+\n+private[spark] case class AccumulatorMetadata(\n+    id: Long,\n+    name: Option[String],\n+    countFailedValues: Boolean) extends Serializable\n+\n+\n+/**\n+ * The base class for accumulators, that can accumulate inputs of type `IN`, and produce output of\n+ * type `OUT`.  Implementations must define following methods:\n+ *  - isZero:       tell if this accumulator is zero value or not. e.g. for a counter accumulator,\n+ *                  0 is zero value; for a list accumulator, Nil is zero value.\n+ *  - copyAndReset: create a new copy of this accumulator, which is zero value. i.e. call `isZero`\n+ *                  on the copy must return true.\n+ *  - add:          defines how to accumulate the inputs. e.g. it can be a simple `+=` for counter\n+ *                  accumulator\n+ *  - merge:        defines how to merge another accumulator of same type.\n+ *  - localValue:   defines how to produce the output by the current state of this accumulator.\n+ *\n+ * The implementations decide how to store intermediate values, e.g. a long field for a counter\n+ * accumulator, a double and a long field for a average accumulator(storing the sum and count).\n+ */\n+abstract class NewAccumulator[IN, OUT] extends Serializable {\n+  private[spark] var metadata: AccumulatorMetadata = _\n+  private[this] var atDriverSide = true\n+\n+  private[spark] def register(\n+      sc: SparkContext,\n+      name: Option[String] = None,\n+      countFailedValues: Boolean = false): Unit = {\n+    if (this.metadata != null) {\n+      throw new IllegalStateException(\"Cannot register an Accumulator twice.\")\n+    }\n+    this.metadata = AccumulatorMetadata(AccumulatorContext.newId(), name, countFailedValues)\n+    AccumulatorContext.register(this)\n+    sc.cleaner.foreach(_.registerAccumulatorForCleanup(this))\n+  }\n+\n+  final def isRegistered: Boolean =\n+    metadata != null && AccumulatorContext.originals.containsKey(metadata.id)\n+\n+  private def assertMetadataNotNull(): Unit = {\n+    if (metadata == null) {\n+      throw new IllegalAccessError(\"The metadata of this accumulator has not been assigned yet.\")\n+    }\n+  }\n+\n+  final def id: Long = {\n+    assertMetadataNotNull()\n+    metadata.id\n+  }\n+\n+  final def name: Option[String] = {\n+    assertMetadataNotNull()\n+    metadata.name\n+  }\n+\n+  final def countFailedValues: Boolean = {\n+    assertMetadataNotNull()\n+    metadata.countFailedValues\n+  }\n+\n+  private[spark] def toInfo(update: Option[Any], value: Option[Any]): AccumulableInfo = {\n+    val isInternal = name.exists(_.startsWith(InternalAccumulator.METRICS_PREFIX))\n+    new AccumulableInfo(id, name, update, value, isInternal, countFailedValues)\n+  }\n+\n+  final private[spark] def isAtDriverSide: Boolean = atDriverSide\n+\n+  def isZero(): Boolean"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "It's needed in https://github.com/apache/spark/pull/12612/files#diff-6a9ff7fb74fd490a50462d45db2d5e11R1101.\n\nThe logic is related to some UI stuff, I haven't looked into it yet, so I just keep it.\n",
    "commit": "124568b3eeb7e0a657b2fbe4f54bb85543b7ffa3",
    "createdAt": "2016-04-28T03:26:45Z",
    "diffHunk": "@@ -0,0 +1,356 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.{lang => jl}\n+import java.io.ObjectInputStream\n+import java.util.concurrent.atomic.AtomicLong\n+import javax.annotation.concurrent.GuardedBy\n+\n+import org.apache.spark.scheduler.AccumulableInfo\n+import org.apache.spark.util.Utils\n+\n+\n+private[spark] case class AccumulatorMetadata(\n+    id: Long,\n+    name: Option[String],\n+    countFailedValues: Boolean) extends Serializable\n+\n+\n+/**\n+ * The base class for accumulators, that can accumulate inputs of type `IN`, and produce output of\n+ * type `OUT`.  Implementations must define following methods:\n+ *  - isZero:       tell if this accumulator is zero value or not. e.g. for a counter accumulator,\n+ *                  0 is zero value; for a list accumulator, Nil is zero value.\n+ *  - copyAndReset: create a new copy of this accumulator, which is zero value. i.e. call `isZero`\n+ *                  on the copy must return true.\n+ *  - add:          defines how to accumulate the inputs. e.g. it can be a simple `+=` for counter\n+ *                  accumulator\n+ *  - merge:        defines how to merge another accumulator of same type.\n+ *  - localValue:   defines how to produce the output by the current state of this accumulator.\n+ *\n+ * The implementations decide how to store intermediate values, e.g. a long field for a counter\n+ * accumulator, a double and a long field for a average accumulator(storing the sum and count).\n+ */\n+abstract class NewAccumulator[IN, OUT] extends Serializable {\n+  private[spark] var metadata: AccumulatorMetadata = _\n+  private[this] var atDriverSide = true\n+\n+  private[spark] def register(\n+      sc: SparkContext,\n+      name: Option[String] = None,\n+      countFailedValues: Boolean = false): Unit = {\n+    if (this.metadata != null) {\n+      throw new IllegalStateException(\"Cannot register an Accumulator twice.\")\n+    }\n+    this.metadata = AccumulatorMetadata(AccumulatorContext.newId(), name, countFailedValues)\n+    AccumulatorContext.register(this)\n+    sc.cleaner.foreach(_.registerAccumulatorForCleanup(this))\n+  }\n+\n+  final def isRegistered: Boolean =\n+    metadata != null && AccumulatorContext.originals.containsKey(metadata.id)\n+\n+  private def assertMetadataNotNull(): Unit = {\n+    if (metadata == null) {\n+      throw new IllegalAccessError(\"The metadata of this accumulator has not been assigned yet.\")\n+    }\n+  }\n+\n+  final def id: Long = {\n+    assertMetadataNotNull()\n+    metadata.id\n+  }\n+\n+  final def name: Option[String] = {\n+    assertMetadataNotNull()\n+    metadata.name\n+  }\n+\n+  final def countFailedValues: Boolean = {\n+    assertMetadataNotNull()\n+    metadata.countFailedValues\n+  }\n+\n+  private[spark] def toInfo(update: Option[Any], value: Option[Any]): AccumulableInfo = {\n+    val isInternal = name.exists(_.startsWith(InternalAccumulator.METRICS_PREFIX))\n+    new AccumulableInfo(id, name, update, value, isInternal, countFailedValues)\n+  }\n+\n+  final private[spark] def isAtDriverSide: Boolean = atDriverSide\n+\n+  def isZero(): Boolean"
  }],
  "prId": 12612
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "remove this - I'd rather keep the API minimal for now.\n",
    "commit": "124568b3eeb7e0a657b2fbe4f54bb85543b7ffa3",
    "createdAt": "2016-04-28T02:35:58Z",
    "diffHunk": "@@ -0,0 +1,356 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.{lang => jl}\n+import java.io.ObjectInputStream\n+import java.util.concurrent.atomic.AtomicLong\n+import javax.annotation.concurrent.GuardedBy\n+\n+import org.apache.spark.scheduler.AccumulableInfo\n+import org.apache.spark.util.Utils\n+\n+\n+private[spark] case class AccumulatorMetadata(\n+    id: Long,\n+    name: Option[String],\n+    countFailedValues: Boolean) extends Serializable\n+\n+\n+/**\n+ * The base class for accumulators, that can accumulate inputs of type `IN`, and produce output of\n+ * type `OUT`.  Implementations must define following methods:\n+ *  - isZero:       tell if this accumulator is zero value or not. e.g. for a counter accumulator,\n+ *                  0 is zero value; for a list accumulator, Nil is zero value.\n+ *  - copyAndReset: create a new copy of this accumulator, which is zero value. i.e. call `isZero`\n+ *                  on the copy must return true.\n+ *  - add:          defines how to accumulate the inputs. e.g. it can be a simple `+=` for counter\n+ *                  accumulator\n+ *  - merge:        defines how to merge another accumulator of same type.\n+ *  - localValue:   defines how to produce the output by the current state of this accumulator.\n+ *\n+ * The implementations decide how to store intermediate values, e.g. a long field for a counter\n+ * accumulator, a double and a long field for a average accumulator(storing the sum and count).\n+ */\n+abstract class NewAccumulator[IN, OUT] extends Serializable {\n+  private[spark] var metadata: AccumulatorMetadata = _\n+  private[this] var atDriverSide = true\n+\n+  private[spark] def register(\n+      sc: SparkContext,\n+      name: Option[String] = None,\n+      countFailedValues: Boolean = false): Unit = {\n+    if (this.metadata != null) {\n+      throw new IllegalStateException(\"Cannot register an Accumulator twice.\")\n+    }\n+    this.metadata = AccumulatorMetadata(AccumulatorContext.newId(), name, countFailedValues)\n+    AccumulatorContext.register(this)\n+    sc.cleaner.foreach(_.registerAccumulatorForCleanup(this))\n+  }\n+\n+  final def isRegistered: Boolean =\n+    metadata != null && AccumulatorContext.originals.containsKey(metadata.id)\n+\n+  private def assertMetadataNotNull(): Unit = {\n+    if (metadata == null) {\n+      throw new IllegalAccessError(\"The metadata of this accumulator has not been assigned yet.\")\n+    }\n+  }\n+\n+  final def id: Long = {\n+    assertMetadataNotNull()\n+    metadata.id\n+  }\n+\n+  final def name: Option[String] = {\n+    assertMetadataNotNull()\n+    metadata.name\n+  }\n+\n+  final def countFailedValues: Boolean = {\n+    assertMetadataNotNull()\n+    metadata.countFailedValues\n+  }\n+\n+  private[spark] def toInfo(update: Option[Any], value: Option[Any]): AccumulableInfo = {\n+    val isInternal = name.exists(_.startsWith(InternalAccumulator.METRICS_PREFIX))\n+    new AccumulableInfo(id, name, update, value, isInternal, countFailedValues)\n+  }\n+\n+  final private[spark] def isAtDriverSide: Boolean = atDriverSide\n+\n+  def isZero(): Boolean\n+\n+  def copyAndReset(): NewAccumulator[IN, OUT]\n+\n+  def add(v: IN): Unit\n+\n+  def +=(v: IN): Unit = add(v)"
  }],
  "prId": 12612
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "need to document that this merges in place\n",
    "commit": "124568b3eeb7e0a657b2fbe4f54bb85543b7ffa3",
    "createdAt": "2016-04-28T03:51:26Z",
    "diffHunk": "@@ -0,0 +1,356 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.{lang => jl}\n+import java.io.ObjectInputStream\n+import java.util.concurrent.atomic.AtomicLong\n+import javax.annotation.concurrent.GuardedBy\n+\n+import org.apache.spark.scheduler.AccumulableInfo\n+import org.apache.spark.util.Utils\n+\n+\n+private[spark] case class AccumulatorMetadata(\n+    id: Long,\n+    name: Option[String],\n+    countFailedValues: Boolean) extends Serializable\n+\n+\n+/**\n+ * The base class for accumulators, that can accumulate inputs of type `IN`, and produce output of\n+ * type `OUT`.  Implementations must define following methods:\n+ *  - isZero:       tell if this accumulator is zero value or not. e.g. for a counter accumulator,\n+ *                  0 is zero value; for a list accumulator, Nil is zero value.\n+ *  - copyAndReset: create a new copy of this accumulator, which is zero value. i.e. call `isZero`\n+ *                  on the copy must return true.\n+ *  - add:          defines how to accumulate the inputs. e.g. it can be a simple `+=` for counter\n+ *                  accumulator\n+ *  - merge:        defines how to merge another accumulator of same type.\n+ *  - localValue:   defines how to produce the output by the current state of this accumulator.\n+ *\n+ * The implementations decide how to store intermediate values, e.g. a long field for a counter\n+ * accumulator, a double and a long field for a average accumulator(storing the sum and count).\n+ */\n+abstract class NewAccumulator[IN, OUT] extends Serializable {\n+  private[spark] var metadata: AccumulatorMetadata = _\n+  private[this] var atDriverSide = true\n+\n+  private[spark] def register(\n+      sc: SparkContext,\n+      name: Option[String] = None,\n+      countFailedValues: Boolean = false): Unit = {\n+    if (this.metadata != null) {\n+      throw new IllegalStateException(\"Cannot register an Accumulator twice.\")\n+    }\n+    this.metadata = AccumulatorMetadata(AccumulatorContext.newId(), name, countFailedValues)\n+    AccumulatorContext.register(this)\n+    sc.cleaner.foreach(_.registerAccumulatorForCleanup(this))\n+  }\n+\n+  final def isRegistered: Boolean =\n+    metadata != null && AccumulatorContext.originals.containsKey(metadata.id)\n+\n+  private def assertMetadataNotNull(): Unit = {\n+    if (metadata == null) {\n+      throw new IllegalAccessError(\"The metadata of this accumulator has not been assigned yet.\")\n+    }\n+  }\n+\n+  final def id: Long = {\n+    assertMetadataNotNull()\n+    metadata.id\n+  }\n+\n+  final def name: Option[String] = {\n+    assertMetadataNotNull()\n+    metadata.name\n+  }\n+\n+  final def countFailedValues: Boolean = {\n+    assertMetadataNotNull()\n+    metadata.countFailedValues\n+  }\n+\n+  private[spark] def toInfo(update: Option[Any], value: Option[Any]): AccumulableInfo = {\n+    val isInternal = name.exists(_.startsWith(InternalAccumulator.METRICS_PREFIX))\n+    new AccumulableInfo(id, name, update, value, isInternal, countFailedValues)\n+  }\n+\n+  final private[spark] def isAtDriverSide: Boolean = atDriverSide\n+\n+  def isZero(): Boolean\n+\n+  def copyAndReset(): NewAccumulator[IN, OUT]\n+\n+  def add(v: IN): Unit\n+\n+  def +=(v: IN): Unit = add(v)\n+\n+  def merge(other: NewAccumulator[IN, OUT]): Unit"
  }],
  "prId": 12612
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "this should be private, however, this hook won't be called if it's private, not sure why, so I use `final protected` to work around it.\n",
    "commit": "124568b3eeb7e0a657b2fbe4f54bb85543b7ffa3",
    "createdAt": "2016-04-28T05:18:27Z",
    "diffHunk": "@@ -0,0 +1,391 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.{lang => jl}\n+import java.io.ObjectInputStream\n+import java.util.concurrent.atomic.AtomicLong\n+import javax.annotation.concurrent.GuardedBy\n+\n+import org.apache.spark.scheduler.AccumulableInfo\n+import org.apache.spark.util.Utils\n+\n+\n+private[spark] case class AccumulatorMetadata(\n+    id: Long,\n+    name: Option[String],\n+    countFailedValues: Boolean) extends Serializable\n+\n+\n+/**\n+ * The base class for accumulators, that can accumulate inputs of type `IN`, and produce output of\n+ * type `OUT`.\n+ */\n+abstract class NewAccumulator[IN, OUT] extends Serializable {\n+  private[spark] var metadata: AccumulatorMetadata = _\n+  private[this] var atDriverSide = true\n+\n+  private[spark] def register(\n+      sc: SparkContext,\n+      name: Option[String] = None,\n+      countFailedValues: Boolean = false): Unit = {\n+    if (this.metadata != null) {\n+      throw new IllegalStateException(\"Cannot register an Accumulator twice.\")\n+    }\n+    this.metadata = AccumulatorMetadata(AccumulatorContext.newId(), name, countFailedValues)\n+    AccumulatorContext.register(this)\n+    sc.cleaner.foreach(_.registerAccumulatorForCleanup(this))\n+  }\n+\n+  /**\n+   * Returns true if this accumulator has been registered.  Note that all accumulators must be\n+   * registered before ues, or it will throw exception.\n+   */\n+  final def isRegistered: Boolean =\n+    metadata != null && AccumulatorContext.originals.containsKey(metadata.id)\n+\n+  private def assertMetadataNotNull(): Unit = {\n+    if (metadata == null) {\n+      throw new IllegalAccessError(\"The metadata of this accumulator has not been assigned yet.\")\n+    }\n+  }\n+\n+  /**\n+   * Returns the id of this accumulator, can only be called after registration.\n+   */\n+  final def id: Long = {\n+    assertMetadataNotNull()\n+    metadata.id\n+  }\n+\n+  /**\n+   * Returns the name of this accumulator, can only be called after registration.\n+   */\n+  final def name: Option[String] = {\n+    assertMetadataNotNull()\n+    metadata.name\n+  }\n+\n+  /**\n+   * Whether to accumulate values from failed tasks. This is set to true for system and time\n+   * metrics like serialization time or bytes spilled, and false for things with absolute values\n+   * like number of input rows.  This should be used for internal metrics only.\n+   */\n+  private[spark] final def countFailedValues: Boolean = {\n+    assertMetadataNotNull()\n+    metadata.countFailedValues\n+  }\n+\n+  /**\n+   * Creates an [[AccumulableInfo]] representation of this [[NewAccumulator]] with the provided\n+   * values.\n+   */\n+  private[spark] def toInfo(update: Option[Any], value: Option[Any]): AccumulableInfo = {\n+    val isInternal = name.exists(_.startsWith(InternalAccumulator.METRICS_PREFIX))\n+    new AccumulableInfo(id, name, update, value, isInternal, countFailedValues)\n+  }\n+\n+  final private[spark] def isAtDriverSide: Boolean = atDriverSide\n+\n+  /**\n+   * Tells if this accumulator is zero value or not. e.g. for a counter accumulator, 0 is zero\n+   * value; for a list accumulator, Nil is zero value.\n+   */\n+  def isZero(): Boolean\n+\n+  /**\n+   * Creates a new copy of this accumulator, which is zero value. i.e. call `isZero` on the copy\n+   * must return true.\n+   */\n+  def copyAndReset(): NewAccumulator[IN, OUT]\n+\n+  /**\n+   * Takes the inputs and accumulates. e.g. it can be a simple `+=` for counter accumulator.\n+   */\n+  def add(v: IN): Unit\n+\n+  /**\n+   * Merges another same-type accumulator into this one and update its state, i.e. this should be\n+   * merge-in-place.\n+   */\n+  def merge(other: NewAccumulator[IN, OUT]): Unit\n+\n+  /**\n+   * Access this accumulator's current value; only allowed on driver.\n+   */\n+  final def value: OUT = {\n+    if (atDriverSide) {\n+      localValue\n+    } else {\n+      throw new UnsupportedOperationException(\"Can't read accumulator value in task\")\n+    }\n+  }\n+\n+  /**\n+   * Defines the current value of this accumulator.\n+   *\n+   * This is NOT the global value of the accumulator.  To get the global value after a\n+   * completed operation on the dataset, call `value`.\n+   */\n+  def localValue: OUT\n+\n+  // Called by Java when serializing an object\n+  final protected def writeReplace(): Any = {",
    "line": 148
  }],
  "prId": 12612
}, {
  "comments": [{
    "author": {
      "login": "MLnick"
    },
    "body": "@cloud-fan I'm getting intermittent, but regular, test failures in `ALSSuite` (not sure if there might be others, this just happens to be something I'm working on now).\n\ne.g.\n\n```\n[info] - exact rank-1 matrix *** FAILED *** (4 seconds, 397 milliseconds)\n[info]   org.apache.spark.SparkException: Job aborted due to stage failure: Failed to serialize task 74, not attempting to retry it. Exception during serialization: java.lang.UnsupportedOperationException: Accumulator must be registered before send to executor\n[info]   at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1448)\n[info]   at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1436)\n[info]   at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1435)\n[info]   at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n[info]   at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n[info]   at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1435)\n[info]   at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:809)\n[info]   at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:809)\n[info]   at scala.Option.foreach(Option.scala:257)\n[info]   at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:809)\n[info]   at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1657)\n[info]   at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1616)\n[info]   at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n[info]   at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n[info]   at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n[info]   at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873)\n[info]   at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936)\n[info]   at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:970)\n[info]   at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n[info]   at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n[info]   at org.apache.spark.rdd.RDD.withScope(RDD.scala:357)\n[info]   at org.apache.spark.rdd.RDD.reduce(RDD.scala:952)\n[info]   at org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$stats$1.apply(DoubleRDDFunctions.scala:42)\n[info]   at org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$stats$1.apply(DoubleRDDFunctions.scala:42)\n[info]   at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n[info]   at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n[info]   at org.apache.spark.rdd.RDD.withScope(RDD.scala:357)\n[info]   at org.apache.spark.rdd.DoubleRDDFunctions.stats(DoubleRDDFunctions.scala:41)\n[info]   at org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$mean$1.apply$mcD$sp(DoubleRDDFunctions.scala:47)\n[info]   at org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$mean$1.apply(DoubleRDDFunctions.scala:47)\n[info]   at org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$mean$1.apply(DoubleRDDFunctions.scala:47)\n[info]   at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n[info]   at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n[info]   at org.apache.spark.rdd.RDD.withScope(RDD.scala:357)\n[info]   at org.apache.spark.rdd.DoubleRDDFunctions.mean(DoubleRDDFunctions.scala:46)\n[info]   at org.apache.spark.ml.recommendation.ALSSuite.testALS(ALSSuite.scala:373)\n[info]   at org.apache.spark.ml.recommendation.ALSSuite$$anonfun$12.apply$mcV$sp(ALSSuite.scala:385)\n[info]   at org.apache.spark.ml.recommendation.ALSSuite$$anonfun$12.apply(ALSSuite.scala:383)\n[info]   at org.apache.spark.ml.recommendation.ALSSuite$$anonfun$12.apply(ALSSuite.scala:383)\n[info]   at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)\n[info]   at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)\n[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)\n[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)\n[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)\n[info]   at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)\n[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:56)\n[info]   at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)\n[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)\n[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)\n[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)\n[info]   at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)\n[info]   at org.scalatest.FunSuite.runTest(FunSuite.scala:1555)\n[info]   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)\n[info]   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)\n[info]   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)\n[info]   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)\n[info]   at scala.collection.immutable.List.foreach(List.scala:381)\n[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)\n[info]   at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)\n[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)\n[info]   at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)\n[info]   at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)\n[info]   at org.scalatest.Suite$class.run(Suite.scala:1424)\n[info]   at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)\n[info]   at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)\n[info]   at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)\n[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:545)\n[info]   at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)\n[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:28)\n[info]   at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)\n[info]   at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)\n[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:28)\n[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:357)\n[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:502)\n[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:296)\n[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:286)\n[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n[info]   at java.lang.Thread.run(Thread.java:745)\n```\n",
    "commit": "124568b3eeb7e0a657b2fbe4f54bb85543b7ffa3",
    "createdAt": "2016-04-28T11:52:06Z",
    "diffHunk": "@@ -0,0 +1,391 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.{lang => jl}\n+import java.io.ObjectInputStream\n+import java.util.concurrent.atomic.AtomicLong\n+import javax.annotation.concurrent.GuardedBy\n+\n+import org.apache.spark.scheduler.AccumulableInfo\n+import org.apache.spark.util.Utils\n+\n+\n+private[spark] case class AccumulatorMetadata(\n+    id: Long,\n+    name: Option[String],\n+    countFailedValues: Boolean) extends Serializable\n+\n+\n+/**\n+ * The base class for accumulators, that can accumulate inputs of type `IN`, and produce output of\n+ * type `OUT`.\n+ */\n+abstract class NewAccumulator[IN, OUT] extends Serializable {\n+  private[spark] var metadata: AccumulatorMetadata = _\n+  private[this] var atDriverSide = true\n+\n+  private[spark] def register(\n+      sc: SparkContext,\n+      name: Option[String] = None,\n+      countFailedValues: Boolean = false): Unit = {\n+    if (this.metadata != null) {\n+      throw new IllegalStateException(\"Cannot register an Accumulator twice.\")\n+    }\n+    this.metadata = AccumulatorMetadata(AccumulatorContext.newId(), name, countFailedValues)\n+    AccumulatorContext.register(this)\n+    sc.cleaner.foreach(_.registerAccumulatorForCleanup(this))\n+  }\n+\n+  /**\n+   * Returns true if this accumulator has been registered.  Note that all accumulators must be\n+   * registered before ues, or it will throw exception.\n+   */\n+  final def isRegistered: Boolean =\n+    metadata != null && AccumulatorContext.originals.containsKey(metadata.id)\n+\n+  private def assertMetadataNotNull(): Unit = {\n+    if (metadata == null) {\n+      throw new IllegalAccessError(\"The metadata of this accumulator has not been assigned yet.\")\n+    }\n+  }\n+\n+  /**\n+   * Returns the id of this accumulator, can only be called after registration.\n+   */\n+  final def id: Long = {\n+    assertMetadataNotNull()\n+    metadata.id\n+  }\n+\n+  /**\n+   * Returns the name of this accumulator, can only be called after registration.\n+   */\n+  final def name: Option[String] = {\n+    assertMetadataNotNull()\n+    metadata.name\n+  }\n+\n+  /**\n+   * Whether to accumulate values from failed tasks. This is set to true for system and time\n+   * metrics like serialization time or bytes spilled, and false for things with absolute values\n+   * like number of input rows.  This should be used for internal metrics only.\n+   */\n+  private[spark] final def countFailedValues: Boolean = {\n+    assertMetadataNotNull()\n+    metadata.countFailedValues\n+  }\n+\n+  /**\n+   * Creates an [[AccumulableInfo]] representation of this [[NewAccumulator]] with the provided\n+   * values.\n+   */\n+  private[spark] def toInfo(update: Option[Any], value: Option[Any]): AccumulableInfo = {\n+    val isInternal = name.exists(_.startsWith(InternalAccumulator.METRICS_PREFIX))\n+    new AccumulableInfo(id, name, update, value, isInternal, countFailedValues)\n+  }\n+\n+  final private[spark] def isAtDriverSide: Boolean = atDriverSide\n+\n+  /**\n+   * Tells if this accumulator is zero value or not. e.g. for a counter accumulator, 0 is zero\n+   * value; for a list accumulator, Nil is zero value.\n+   */\n+  def isZero(): Boolean\n+\n+  /**\n+   * Creates a new copy of this accumulator, which is zero value. i.e. call `isZero` on the copy\n+   * must return true.\n+   */\n+  def copyAndReset(): NewAccumulator[IN, OUT]\n+\n+  /**\n+   * Takes the inputs and accumulates. e.g. it can be a simple `+=` for counter accumulator.\n+   */\n+  def add(v: IN): Unit\n+\n+  /**\n+   * Merges another same-type accumulator into this one and update its state, i.e. this should be\n+   * merge-in-place.\n+   */\n+  def merge(other: NewAccumulator[IN, OUT]): Unit\n+\n+  /**\n+   * Access this accumulator's current value; only allowed on driver.\n+   */\n+  final def value: OUT = {\n+    if (atDriverSide) {\n+      localValue\n+    } else {\n+      throw new UnsupportedOperationException(\"Can't read accumulator value in task\")\n+    }\n+  }\n+\n+  /**\n+   * Defines the current value of this accumulator.\n+   *\n+   * This is NOT the global value of the accumulator.  To get the global value after a\n+   * completed operation on the dataset, call `value`.\n+   */\n+  def localValue: OUT\n+\n+  // Called by Java when serializing an object\n+  final protected def writeReplace(): Any = {\n+    if (atDriverSide) {\n+      if (!isRegistered) {\n+        throw new UnsupportedOperationException(\n+          \"Accumulator must be registered before send to executor\")",
    "line": 152
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "I also found some tests failed because of this indeterminately, looking into it. \n",
    "commit": "124568b3eeb7e0a657b2fbe4f54bb85543b7ffa3",
    "createdAt": "2016-04-28T12:38:18Z",
    "diffHunk": "@@ -0,0 +1,391 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.{lang => jl}\n+import java.io.ObjectInputStream\n+import java.util.concurrent.atomic.AtomicLong\n+import javax.annotation.concurrent.GuardedBy\n+\n+import org.apache.spark.scheduler.AccumulableInfo\n+import org.apache.spark.util.Utils\n+\n+\n+private[spark] case class AccumulatorMetadata(\n+    id: Long,\n+    name: Option[String],\n+    countFailedValues: Boolean) extends Serializable\n+\n+\n+/**\n+ * The base class for accumulators, that can accumulate inputs of type `IN`, and produce output of\n+ * type `OUT`.\n+ */\n+abstract class NewAccumulator[IN, OUT] extends Serializable {\n+  private[spark] var metadata: AccumulatorMetadata = _\n+  private[this] var atDriverSide = true\n+\n+  private[spark] def register(\n+      sc: SparkContext,\n+      name: Option[String] = None,\n+      countFailedValues: Boolean = false): Unit = {\n+    if (this.metadata != null) {\n+      throw new IllegalStateException(\"Cannot register an Accumulator twice.\")\n+    }\n+    this.metadata = AccumulatorMetadata(AccumulatorContext.newId(), name, countFailedValues)\n+    AccumulatorContext.register(this)\n+    sc.cleaner.foreach(_.registerAccumulatorForCleanup(this))\n+  }\n+\n+  /**\n+   * Returns true if this accumulator has been registered.  Note that all accumulators must be\n+   * registered before ues, or it will throw exception.\n+   */\n+  final def isRegistered: Boolean =\n+    metadata != null && AccumulatorContext.originals.containsKey(metadata.id)\n+\n+  private def assertMetadataNotNull(): Unit = {\n+    if (metadata == null) {\n+      throw new IllegalAccessError(\"The metadata of this accumulator has not been assigned yet.\")\n+    }\n+  }\n+\n+  /**\n+   * Returns the id of this accumulator, can only be called after registration.\n+   */\n+  final def id: Long = {\n+    assertMetadataNotNull()\n+    metadata.id\n+  }\n+\n+  /**\n+   * Returns the name of this accumulator, can only be called after registration.\n+   */\n+  final def name: Option[String] = {\n+    assertMetadataNotNull()\n+    metadata.name\n+  }\n+\n+  /**\n+   * Whether to accumulate values from failed tasks. This is set to true for system and time\n+   * metrics like serialization time or bytes spilled, and false for things with absolute values\n+   * like number of input rows.  This should be used for internal metrics only.\n+   */\n+  private[spark] final def countFailedValues: Boolean = {\n+    assertMetadataNotNull()\n+    metadata.countFailedValues\n+  }\n+\n+  /**\n+   * Creates an [[AccumulableInfo]] representation of this [[NewAccumulator]] with the provided\n+   * values.\n+   */\n+  private[spark] def toInfo(update: Option[Any], value: Option[Any]): AccumulableInfo = {\n+    val isInternal = name.exists(_.startsWith(InternalAccumulator.METRICS_PREFIX))\n+    new AccumulableInfo(id, name, update, value, isInternal, countFailedValues)\n+  }\n+\n+  final private[spark] def isAtDriverSide: Boolean = atDriverSide\n+\n+  /**\n+   * Tells if this accumulator is zero value or not. e.g. for a counter accumulator, 0 is zero\n+   * value; for a list accumulator, Nil is zero value.\n+   */\n+  def isZero(): Boolean\n+\n+  /**\n+   * Creates a new copy of this accumulator, which is zero value. i.e. call `isZero` on the copy\n+   * must return true.\n+   */\n+  def copyAndReset(): NewAccumulator[IN, OUT]\n+\n+  /**\n+   * Takes the inputs and accumulates. e.g. it can be a simple `+=` for counter accumulator.\n+   */\n+  def add(v: IN): Unit\n+\n+  /**\n+   * Merges another same-type accumulator into this one and update its state, i.e. this should be\n+   * merge-in-place.\n+   */\n+  def merge(other: NewAccumulator[IN, OUT]): Unit\n+\n+  /**\n+   * Access this accumulator's current value; only allowed on driver.\n+   */\n+  final def value: OUT = {\n+    if (atDriverSide) {\n+      localValue\n+    } else {\n+      throw new UnsupportedOperationException(\"Can't read accumulator value in task\")\n+    }\n+  }\n+\n+  /**\n+   * Defines the current value of this accumulator.\n+   *\n+   * This is NOT the global value of the accumulator.  To get the global value after a\n+   * completed operation on the dataset, call `value`.\n+   */\n+  def localValue: OUT\n+\n+  // Called by Java when serializing an object\n+  final protected def writeReplace(): Any = {\n+    if (atDriverSide) {\n+      if (!isRegistered) {\n+        throw new UnsupportedOperationException(\n+          \"Accumulator must be registered before send to executor\")",
    "line": 152
  }],
  "prId": 12612
}, {
  "comments": [{
    "author": {
      "login": "tedyu"
    },
    "body": "Why is this assignment needed ?\n",
    "commit": "124568b3eeb7e0a657b2fbe4f54bb85543b7ffa3",
    "createdAt": "2016-04-29T22:52:33Z",
    "diffHunk": "@@ -0,0 +1,391 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.{lang => jl}\n+import java.io.ObjectInputStream\n+import java.util.concurrent.atomic.AtomicLong\n+import javax.annotation.concurrent.GuardedBy\n+\n+import org.apache.spark.scheduler.AccumulableInfo\n+import org.apache.spark.util.Utils\n+\n+\n+private[spark] case class AccumulatorMetadata(\n+    id: Long,\n+    name: Option[String],\n+    countFailedValues: Boolean) extends Serializable\n+\n+\n+/**\n+ * The base class for accumulators, that can accumulate inputs of type `IN`, and produce output of\n+ * type `OUT`.\n+ */\n+abstract class NewAccumulator[IN, OUT] extends Serializable {\n+  private[spark] var metadata: AccumulatorMetadata = _\n+  private[this] var atDriverSide = true\n+\n+  private[spark] def register(\n+      sc: SparkContext,\n+      name: Option[String] = None,\n+      countFailedValues: Boolean = false): Unit = {\n+    if (this.metadata != null) {\n+      throw new IllegalStateException(\"Cannot register an Accumulator twice.\")\n+    }\n+    this.metadata = AccumulatorMetadata(AccumulatorContext.newId(), name, countFailedValues)\n+    AccumulatorContext.register(this)\n+    sc.cleaner.foreach(_.registerAccumulatorForCleanup(this))\n+  }\n+\n+  /**\n+   * Returns true if this accumulator has been registered.  Note that all accumulators must be\n+   * registered before ues, or it will throw exception.\n+   */\n+  final def isRegistered: Boolean =\n+    metadata != null && AccumulatorContext.originals.containsKey(metadata.id)\n+\n+  private def assertMetadataNotNull(): Unit = {\n+    if (metadata == null) {\n+      throw new IllegalAccessError(\"The metadata of this accumulator has not been assigned yet.\")\n+    }\n+  }\n+\n+  /**\n+   * Returns the id of this accumulator, can only be called after registration.\n+   */\n+  final def id: Long = {\n+    assertMetadataNotNull()\n+    metadata.id\n+  }\n+\n+  /**\n+   * Returns the name of this accumulator, can only be called after registration.\n+   */\n+  final def name: Option[String] = {\n+    assertMetadataNotNull()\n+    metadata.name\n+  }\n+\n+  /**\n+   * Whether to accumulate values from failed tasks. This is set to true for system and time\n+   * metrics like serialization time or bytes spilled, and false for things with absolute values\n+   * like number of input rows.  This should be used for internal metrics only.\n+   */\n+  private[spark] final def countFailedValues: Boolean = {\n+    assertMetadataNotNull()\n+    metadata.countFailedValues\n+  }\n+\n+  /**\n+   * Creates an [[AccumulableInfo]] representation of this [[NewAccumulator]] with the provided\n+   * values.\n+   */\n+  private[spark] def toInfo(update: Option[Any], value: Option[Any]): AccumulableInfo = {\n+    val isInternal = name.exists(_.startsWith(InternalAccumulator.METRICS_PREFIX))\n+    new AccumulableInfo(id, name, update, value, isInternal, countFailedValues)\n+  }\n+\n+  final private[spark] def isAtDriverSide: Boolean = atDriverSide\n+\n+  /**\n+   * Tells if this accumulator is zero value or not. e.g. for a counter accumulator, 0 is zero\n+   * value; for a list accumulator, Nil is zero value.\n+   */\n+  def isZero(): Boolean\n+\n+  /**\n+   * Creates a new copy of this accumulator, which is zero value. i.e. call `isZero` on the copy\n+   * must return true.\n+   */\n+  def copyAndReset(): NewAccumulator[IN, OUT]\n+\n+  /**\n+   * Takes the inputs and accumulates. e.g. it can be a simple `+=` for counter accumulator.\n+   */\n+  def add(v: IN): Unit\n+\n+  /**\n+   * Merges another same-type accumulator into this one and update its state, i.e. this should be\n+   * merge-in-place.\n+   */\n+  def merge(other: NewAccumulator[IN, OUT]): Unit\n+\n+  /**\n+   * Access this accumulator's current value; only allowed on driver.\n+   */\n+  final def value: OUT = {\n+    if (atDriverSide) {\n+      localValue\n+    } else {\n+      throw new UnsupportedOperationException(\"Can't read accumulator value in task\")\n+    }\n+  }\n+\n+  /**\n+   * Defines the current value of this accumulator.\n+   *\n+   * This is NOT the global value of the accumulator.  To get the global value after a\n+   * completed operation on the dataset, call `value`.\n+   */\n+  def localValue: OUT\n+\n+  // Called by Java when serializing an object\n+  final protected def writeReplace(): Any = {\n+    if (atDriverSide) {\n+      if (!isRegistered) {\n+        throw new UnsupportedOperationException(\n+          \"Accumulator must be registered before send to executor\")\n+      }\n+      val copy = copyAndReset()\n+      assert(copy.isZero(), \"copyAndReset must return a zero value copy\")\n+      copy.metadata = metadata\n+      copy\n+    } else {\n+      this\n+    }\n+  }\n+\n+  // Called by Java when deserializing an object\n+  private def readObject(in: ObjectInputStream): Unit = Utils.tryOrIOException {\n+    in.defaultReadObject()\n+    if (atDriverSide) {\n+      atDriverSide = false\n+\n+      // Automatically register the accumulator when it is deserialized with the task closure.\n+      // This is for external accumulators and internal ones that do not represent task level\n+      // metrics, e.g. internal SQL metrics, which are per-operator.\n+      val taskContext = TaskContext.get()\n+      if (taskContext != null) {\n+        taskContext.registerAccumulator(this)\n+      }\n+    } else {\n+      atDriverSide = true",
    "line": 177
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "When the accumulator is sent back from executor to driver, we should set the `atDriverSide` flag.\n",
    "commit": "124568b3eeb7e0a657b2fbe4f54bb85543b7ffa3",
    "createdAt": "2016-04-29T23:52:49Z",
    "diffHunk": "@@ -0,0 +1,391 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.{lang => jl}\n+import java.io.ObjectInputStream\n+import java.util.concurrent.atomic.AtomicLong\n+import javax.annotation.concurrent.GuardedBy\n+\n+import org.apache.spark.scheduler.AccumulableInfo\n+import org.apache.spark.util.Utils\n+\n+\n+private[spark] case class AccumulatorMetadata(\n+    id: Long,\n+    name: Option[String],\n+    countFailedValues: Boolean) extends Serializable\n+\n+\n+/**\n+ * The base class for accumulators, that can accumulate inputs of type `IN`, and produce output of\n+ * type `OUT`.\n+ */\n+abstract class NewAccumulator[IN, OUT] extends Serializable {\n+  private[spark] var metadata: AccumulatorMetadata = _\n+  private[this] var atDriverSide = true\n+\n+  private[spark] def register(\n+      sc: SparkContext,\n+      name: Option[String] = None,\n+      countFailedValues: Boolean = false): Unit = {\n+    if (this.metadata != null) {\n+      throw new IllegalStateException(\"Cannot register an Accumulator twice.\")\n+    }\n+    this.metadata = AccumulatorMetadata(AccumulatorContext.newId(), name, countFailedValues)\n+    AccumulatorContext.register(this)\n+    sc.cleaner.foreach(_.registerAccumulatorForCleanup(this))\n+  }\n+\n+  /**\n+   * Returns true if this accumulator has been registered.  Note that all accumulators must be\n+   * registered before ues, or it will throw exception.\n+   */\n+  final def isRegistered: Boolean =\n+    metadata != null && AccumulatorContext.originals.containsKey(metadata.id)\n+\n+  private def assertMetadataNotNull(): Unit = {\n+    if (metadata == null) {\n+      throw new IllegalAccessError(\"The metadata of this accumulator has not been assigned yet.\")\n+    }\n+  }\n+\n+  /**\n+   * Returns the id of this accumulator, can only be called after registration.\n+   */\n+  final def id: Long = {\n+    assertMetadataNotNull()\n+    metadata.id\n+  }\n+\n+  /**\n+   * Returns the name of this accumulator, can only be called after registration.\n+   */\n+  final def name: Option[String] = {\n+    assertMetadataNotNull()\n+    metadata.name\n+  }\n+\n+  /**\n+   * Whether to accumulate values from failed tasks. This is set to true for system and time\n+   * metrics like serialization time or bytes spilled, and false for things with absolute values\n+   * like number of input rows.  This should be used for internal metrics only.\n+   */\n+  private[spark] final def countFailedValues: Boolean = {\n+    assertMetadataNotNull()\n+    metadata.countFailedValues\n+  }\n+\n+  /**\n+   * Creates an [[AccumulableInfo]] representation of this [[NewAccumulator]] with the provided\n+   * values.\n+   */\n+  private[spark] def toInfo(update: Option[Any], value: Option[Any]): AccumulableInfo = {\n+    val isInternal = name.exists(_.startsWith(InternalAccumulator.METRICS_PREFIX))\n+    new AccumulableInfo(id, name, update, value, isInternal, countFailedValues)\n+  }\n+\n+  final private[spark] def isAtDriverSide: Boolean = atDriverSide\n+\n+  /**\n+   * Tells if this accumulator is zero value or not. e.g. for a counter accumulator, 0 is zero\n+   * value; for a list accumulator, Nil is zero value.\n+   */\n+  def isZero(): Boolean\n+\n+  /**\n+   * Creates a new copy of this accumulator, which is zero value. i.e. call `isZero` on the copy\n+   * must return true.\n+   */\n+  def copyAndReset(): NewAccumulator[IN, OUT]\n+\n+  /**\n+   * Takes the inputs and accumulates. e.g. it can be a simple `+=` for counter accumulator.\n+   */\n+  def add(v: IN): Unit\n+\n+  /**\n+   * Merges another same-type accumulator into this one and update its state, i.e. this should be\n+   * merge-in-place.\n+   */\n+  def merge(other: NewAccumulator[IN, OUT]): Unit\n+\n+  /**\n+   * Access this accumulator's current value; only allowed on driver.\n+   */\n+  final def value: OUT = {\n+    if (atDriverSide) {\n+      localValue\n+    } else {\n+      throw new UnsupportedOperationException(\"Can't read accumulator value in task\")\n+    }\n+  }\n+\n+  /**\n+   * Defines the current value of this accumulator.\n+   *\n+   * This is NOT the global value of the accumulator.  To get the global value after a\n+   * completed operation on the dataset, call `value`.\n+   */\n+  def localValue: OUT\n+\n+  // Called by Java when serializing an object\n+  final protected def writeReplace(): Any = {\n+    if (atDriverSide) {\n+      if (!isRegistered) {\n+        throw new UnsupportedOperationException(\n+          \"Accumulator must be registered before send to executor\")\n+      }\n+      val copy = copyAndReset()\n+      assert(copy.isZero(), \"copyAndReset must return a zero value copy\")\n+      copy.metadata = metadata\n+      copy\n+    } else {\n+      this\n+    }\n+  }\n+\n+  // Called by Java when deserializing an object\n+  private def readObject(in: ObjectInputStream): Unit = Utils.tryOrIOException {\n+    in.defaultReadObject()\n+    if (atDriverSide) {\n+      atDriverSide = false\n+\n+      // Automatically register the accumulator when it is deserialized with the task closure.\n+      // This is for external accumulators and internal ones that do not represent task level\n+      // metrics, e.g. internal SQL metrics, which are per-operator.\n+      val taskContext = TaskContext.get()\n+      if (taskContext != null) {\n+        taskContext.registerAccumulator(this)\n+      }\n+    } else {\n+      atDriverSide = true",
    "line": 177
  }],
  "prId": 12612
}]