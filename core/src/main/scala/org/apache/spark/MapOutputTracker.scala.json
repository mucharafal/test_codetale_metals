[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "`includes both a host and an executor id` is confusing. We can just say `task location strinng (please refer to TaskLocation)`",
    "commit": "9c1dc5538afce26c4e693e353d8d4ef4231bb78c",
    "createdAt": "2019-09-18T08:22:24Z",
    "diffHunk": "@@ -629,6 +645,35 @@ private[spark] class MapOutputTrackerMaster(\n     None\n   }\n \n+  /**\n+   * Return the locations where the Mapper(s) ran. The locations each includes both a host and an\n+   * executor id on that host.\n+   *\n+   * @param dep shuffle dependency object\n+   * @param startMapId the start map id\n+   * @param endMapId the end map id\n+   * @return a sequence of locations that each includes both a host and an executor id on that"
  }],
  "prId": 25295
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "We shouldn't add code that only for future use. For now we only need a single `mapId`, IIUC.",
    "commit": "9c1dc5538afce26c4e693e353d8d4ef4231bb78c",
    "createdAt": "2019-10-11T11:21:21Z",
    "diffHunk": "@@ -669,6 +685,34 @@ private[spark] class MapOutputTrackerMaster(\n     None\n   }\n \n+  /**\n+   * Return the locations where the Mapper(s) ran. The locations each includes both a host and an\n+   * executor id on that host.\n+   *\n+   * @param dep shuffle dependency object\n+   * @param startMapId the start map id\n+   * @param endMapId the end map id"
  }],
  "prId": 25295
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "Since we need to get all the shuffle status anyway, we can call the existing `convertMapStatuses`, and do an extra filter to only collect the blocks whose mapIndex is what we want.",
    "commit": "9c1dc5538afce26c4e693e353d8d4ef4231bb78c",
    "createdAt": "2019-10-11T11:28:25Z",
    "diffHunk": "@@ -749,6 +818,26 @@ private[spark] class MapOutputTrackerWorker(conf: SparkConf) extends MapOutputTr\n     }\n   }\n \n+  override def getMapSizesByExecutorId(\n+      shuffleId: Int,\n+      startPartition: Int,\n+      endPartition: Int,\n+      mapId: Int,\n+      useOldFetchProtocol: Boolean) : Iterator[(BlockManagerId, Seq[(BlockId, Long, Int)])] = {\n+    logDebug(s\"Fetching outputs for shuffle $shuffleId, mapId $mapId\" +\n+      s\"partitions $startPartition-$endPartition\")\n+    val statuses = getStatuses(shuffleId)",
    "line": 104
  }],
  "prId": 25295
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "The doc should be updated as we only get one location for one mapper now.",
    "commit": "9c1dc5538afce26c4e693e353d8d4ef4231bb78c",
    "createdAt": "2019-10-14T08:15:03Z",
    "diffHunk": "@@ -669,6 +685,31 @@ private[spark] class MapOutputTrackerMaster(\n     None\n   }\n \n+  /**\n+   * Return the locations where the Mapper(s) ran. The locations each includes both a host and an"
  }],
  "prId": 25295
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "nit:\r\n```\r\nval iter = statuses.iterator.zipWithIndex\r\nfor ((status, mapIndex) <- mapId.map(id => iter.filter(_._2 == id)).getOrElse(iter))\r\n```\r\n\r\nThen we can save a lot of duplicated code.",
    "commit": "9c1dc5538afce26c4e693e353d8d4ef4231bb78c",
    "createdAt": "2019-10-14T08:26:05Z",
    "diffHunk": "@@ -893,31 +979,59 @@ private[spark] object MapOutputTracker extends Logging {\n       startPartition: Int,\n       endPartition: Int,\n       statuses: Array[MapStatus],\n-      useOldFetchProtocol: Boolean): Iterator[(BlockManagerId, Seq[(BlockId, Long, Int)])] = {\n+      useOldFetchProtocol: Boolean,\n+      mapId : Option[Int] = None): Iterator[(BlockManagerId, Seq[(BlockId, Long, Int)])] = {\n     assert (statuses != null)\n     val splitsByAddress = new HashMap[BlockManagerId, ListBuffer[(BlockId, Long, Int)]]\n-    for ((status, mapIndex) <- statuses.iterator.zipWithIndex) {\n-      if (status == null) {\n-        val errorMessage = s\"Missing an output location for shuffle $shuffleId\"\n-        logError(errorMessage)\n-        throw new MetadataFetchFailedException(shuffleId, startPartition, errorMessage)\n-      } else {\n-        for (part <- startPartition until endPartition) {\n-          val size = status.getSizeForBlock(part)\n-          if (size != 0) {\n-            if (useOldFetchProtocol) {\n-              // While we use the old shuffle fetch protocol, we use mapIndex as mapId in the\n-              // ShuffleBlockId.\n-              splitsByAddress.getOrElseUpdate(status.location, ListBuffer()) +=\n-                ((ShuffleBlockId(shuffleId, mapIndex, part), size, mapIndex))\n-            } else {\n-              splitsByAddress.getOrElseUpdate(status.location, ListBuffer()) +=\n-                ((ShuffleBlockId(shuffleId, status.mapTaskId, part), size, mapIndex))\n+    mapId match {\n+      case (Some(mapId)) =>\n+        for ((status, mapIndex) <- statuses.iterator.zipWithIndex.filter(_._2 == mapId)) {"
  }],
  "prId": 25295
}]