[{
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "indent 2 more\n",
    "commit": "a1cec18651f5089701e3a517c2bc2a53ae801fbd",
    "createdAt": "2015-11-19T18:47:45Z",
    "diffHunk": "@@ -21,17 +21,222 @@ import java.io.File\n \n import scala.reflect.ClassTag\n \n+import org.apache.spark.CheckpointSuite._\n import org.apache.spark.rdd._\n import org.apache.spark.storage.{BlockId, StorageLevel, TestBlockId}\n import org.apache.spark.util.Utils\n \n+trait RDDCheckpointTester { self: SparkFunSuite =>\n+\n+  protected val partitioner = new HashPartitioner(2)\n+\n+  private def defaultCollectFunc[T](rdd: RDD[T]): Any = rdd.collect()\n+\n+  /** Implementations of this trait must implement this method */\n+  protected def sparkContext: SparkContext\n+\n+  /**\n+   * Test checkpointing of the RDD generated by the given operation. It tests whether the\n+   * serialized size of the RDD is reduce after checkpointing or not. This function should be called\n+   * on all RDDs that have a parent RDD (i.e., do not call on ParallelCollection, BlockRDD, etc.).\n+   *\n+   * @param op an operation to run on the RDD\n+   * @param reliableCheckpoint if true, use reliable checkpoints, otherwise use local checkpoints\n+   * @param collectFunc a function for collecting the values in the RDD, in case there are\n+   *                    non-comparable types like arrays that we want to convert to something that supports ==\n+   */\n+  protected def testRDD[U: ClassTag](\n+    op: (RDD[Int]) => RDD[U],\n+    reliableCheckpoint: Boolean,\n+    collectFunc: RDD[U] => Any = defaultCollectFunc[U] _): Unit = {"
  }],
  "prId": 9831
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "2 more\n",
    "commit": "a1cec18651f5089701e3a517c2bc2a53ae801fbd",
    "createdAt": "2015-11-19T18:50:43Z",
    "diffHunk": "@@ -21,17 +21,222 @@ import java.io.File\n \n import scala.reflect.ClassTag\n \n+import org.apache.spark.CheckpointSuite._\n import org.apache.spark.rdd._\n import org.apache.spark.storage.{BlockId, StorageLevel, TestBlockId}\n import org.apache.spark.util.Utils\n \n+trait RDDCheckpointTester { self: SparkFunSuite =>\n+\n+  protected val partitioner = new HashPartitioner(2)\n+\n+  private def defaultCollectFunc[T](rdd: RDD[T]): Any = rdd.collect()\n+\n+  /** Implementations of this trait must implement this method */\n+  protected def sparkContext: SparkContext\n+\n+  /**\n+   * Test checkpointing of the RDD generated by the given operation. It tests whether the\n+   * serialized size of the RDD is reduce after checkpointing or not. This function should be called\n+   * on all RDDs that have a parent RDD (i.e., do not call on ParallelCollection, BlockRDD, etc.).\n+   *\n+   * @param op an operation to run on the RDD\n+   * @param reliableCheckpoint if true, use reliable checkpoints, otherwise use local checkpoints\n+   * @param collectFunc a function for collecting the values in the RDD, in case there are\n+   *                    non-comparable types like arrays that we want to convert to something that supports ==\n+   */\n+  protected def testRDD[U: ClassTag](\n+    op: (RDD[Int]) => RDD[U],\n+    reliableCheckpoint: Boolean,\n+    collectFunc: RDD[U] => Any = defaultCollectFunc[U] _): Unit = {\n+    // Generate the final RDD using given RDD operation\n+    val baseRDD = generateFatRDD()\n+    val operatedRDD = op(baseRDD)\n+    val parentRDD = operatedRDD.dependencies.headOption.orNull\n+    val rddType = operatedRDD.getClass.getSimpleName\n+    val numPartitions = operatedRDD.partitions.length\n+\n+    // Force initialization of all the data structures in RDDs\n+    // Without this, serializing the RDD will give a wrong estimate of the size of the RDD\n+    initializeRdd(operatedRDD)\n+\n+    val partitionsBeforeCheckpoint = operatedRDD.partitions\n+\n+    // Find serialized sizes before and after the checkpoint\n+    logInfo(\"RDD before checkpoint: \" + operatedRDD + \"\\n\" + operatedRDD.toDebugString)\n+    val (rddSizeBeforeCheckpoint, partitionSizeBeforeCheckpoint) = getSerializedSizes(operatedRDD)\n+    checkpoint(operatedRDD, reliableCheckpoint)\n+    val result = collectFunc(operatedRDD)\n+    operatedRDD.collect() // force re-initialization of post-checkpoint lazy variables\n+    val (rddSizeAfterCheckpoint, partitionSizeAfterCheckpoint) = getSerializedSizes(operatedRDD)\n+    logInfo(\"RDD after checkpoint: \" + operatedRDD + \"\\n\" + operatedRDD.toDebugString)\n+\n+    // Test whether the checkpoint file has been created\n+    if (reliableCheckpoint) {\n+      assert(\n+        collectFunc(sparkContext.checkpointFile[U](operatedRDD.getCheckpointFile.get)) === result)\n+    }\n+\n+    // Test whether dependencies have been changed from its earlier parent RDD\n+    assert(operatedRDD.dependencies.head.rdd != parentRDD)\n+\n+    // Test whether the partitions have been changed from its earlier partitions\n+    assert(operatedRDD.partitions.toList != partitionsBeforeCheckpoint.toList)\n+\n+    // Test whether the partitions have been changed to the new Hadoop partitions\n+    assert(operatedRDD.partitions.toList === operatedRDD.checkpointData.get.getPartitions.toList)\n+\n+    // Test whether the number of partitions is same as before\n+    assert(operatedRDD.partitions.length === numPartitions)\n+\n+    // Test whether the data in the checkpointed RDD is same as original\n+    assert(collectFunc(operatedRDD) === result)\n+\n+    // Test whether serialized size of the RDD has reduced.\n+    logInfo(\"Size of \" + rddType +\n+      \" [\" + rddSizeBeforeCheckpoint + \" --> \" + rddSizeAfterCheckpoint + \"]\")\n+    assert(\n+      rddSizeAfterCheckpoint < rddSizeBeforeCheckpoint,\n+      \"Size of \" + rddType + \" did not reduce after checkpointing \" +\n+        \" [\" + rddSizeBeforeCheckpoint + \" --> \" + rddSizeAfterCheckpoint + \"]\"\n+    )\n+  }\n+\n+  /**\n+   * Test whether checkpointing of the parent of the generated RDD also\n+   * truncates the lineage or not. Some RDDs like CoGroupedRDD hold on to its parent\n+   * RDDs partitions. So even if the parent RDD is checkpointed and its partitions changed,\n+   * the generated RDD will remember the partitions and therefore potentially the whole lineage.\n+   * This function should be called only those RDD whose partitions refer to parent RDD's\n+   * partitions (i.e., do not call it on simple RDD like MappedRDD).\n+   *\n+   * @param op an operation to run on the RDD\n+   * @param reliableCheckpoint if true, use reliable checkpoints, otherwise use local checkpoints\n+   * @param collectFunc a function for collecting the values in the RDD, in case there are\n+   *                    non-comparable types like arrays that we want to convert to something\n+   *                    that supports ==\n+   */\n+  protected def testRDDPartitions[U: ClassTag](\n+    op: (RDD[Int]) => RDD[U],\n+    reliableCheckpoint: Boolean,\n+    collectFunc: RDD[U] => Any = defaultCollectFunc[U] _): Unit = {"
  }],
  "prId": 9831
}]