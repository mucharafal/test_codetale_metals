[{
  "comments": [{
    "author": {
      "login": "mateiz"
    },
    "body": "Code style is a little weird here and in other parts of the file -- don't leave spaces inside the parens, but do leave them around operators. So this should be:\n\n```\nassert(persisted.filter(_ == 1).count() == 5)\n```\n\nfor example.\n",
    "commit": "9ef7cb84a060e6618766f47695ae8e41cfb15246",
    "createdAt": "2014-03-03T05:25:31Z",
    "diffHunk": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.storage\n+\n+import org.scalatest.FunSuite\n+import org.apache.spark.{SharedSparkContext, SparkConf, LocalSparkContext, SparkContext}\n+\n+\n+class FlatmapIteratorSuite extends FunSuite with LocalSparkContext {\n+  /* Tests the ability of Spark to deal with user provided iterators from flatMap\n+   * calls, that may generate more data then available memory. In any\n+   * memory based persistance Spark will unroll the iterator into an ArrayBuffer\n+   * for caching, however in the case that the use defines DISK_ONLY persistance,\n+   * the iterator will be fed directly to the serializer and written to disk.\n+   *\n+   * This also tests the ObjectOutputStream reset rate. When serializing using the\n+   * Java serialization system, the serializer caches objects to prevent writing redundant\n+   * data, however that stops GC of those objects. By calling 'reset' you flush that\n+   * info from the serializer, and allow old objects to be GC'd\n+   */\n+  test(\"Flatmap Iterator to Disk\") {\n+    val sconf = new SparkConf().setMaster(\"local-cluster[1,1,512]\")\n+      .setAppName(\"iterator_to_disk_test\")\n+    sc = new SparkContext(sconf)\n+    try {\n+      val expand_size = 100\n+      val data = sc.parallelize( (1 to 5).toSeq ).\n+        flatMap( x => Stream.range(0, expand_size) )\n+      var persisted = data.persist(StorageLevel.DISK_ONLY)\n+      println(persisted.count())\n+      assert( persisted.count() == 500)\n+      assert( persisted.filter( _==1 ).count() == 5 )"
  }, {
    "author": {
      "login": "mateiz"
    },
    "body": "Also in ScalaTest, you can use `===` instead of `==` to get nicer error messages (e.g. \"expected 5 but got 4\" instead of \"assertion failed: false\").\n",
    "commit": "9ef7cb84a060e6618766f47695ae8e41cfb15246",
    "createdAt": "2014-03-03T05:26:10Z",
    "diffHunk": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.storage\n+\n+import org.scalatest.FunSuite\n+import org.apache.spark.{SharedSparkContext, SparkConf, LocalSparkContext, SparkContext}\n+\n+\n+class FlatmapIteratorSuite extends FunSuite with LocalSparkContext {\n+  /* Tests the ability of Spark to deal with user provided iterators from flatMap\n+   * calls, that may generate more data then available memory. In any\n+   * memory based persistance Spark will unroll the iterator into an ArrayBuffer\n+   * for caching, however in the case that the use defines DISK_ONLY persistance,\n+   * the iterator will be fed directly to the serializer and written to disk.\n+   *\n+   * This also tests the ObjectOutputStream reset rate. When serializing using the\n+   * Java serialization system, the serializer caches objects to prevent writing redundant\n+   * data, however that stops GC of those objects. By calling 'reset' you flush that\n+   * info from the serializer, and allow old objects to be GC'd\n+   */\n+  test(\"Flatmap Iterator to Disk\") {\n+    val sconf = new SparkConf().setMaster(\"local-cluster[1,1,512]\")\n+      .setAppName(\"iterator_to_disk_test\")\n+    sc = new SparkContext(sconf)\n+    try {\n+      val expand_size = 100\n+      val data = sc.parallelize( (1 to 5).toSeq ).\n+        flatMap( x => Stream.range(0, expand_size) )\n+      var persisted = data.persist(StorageLevel.DISK_ONLY)\n+      println(persisted.count())\n+      assert( persisted.count() == 500)\n+      assert( persisted.filter( _==1 ).count() == 5 )"
  }],
  "prId": 50
}, {
  "comments": [{
    "author": {
      "login": "mateiz"
    },
    "body": "Why are you catching OutOfMemoryError just letting the test suite fail? I don't think the test runner will easily recover from this. Maybe use a println or something if you want to leave a message saying this is an expected failure mode.\n",
    "commit": "9ef7cb84a060e6618766f47695ae8e41cfb15246",
    "createdAt": "2014-03-03T05:28:20Z",
    "diffHunk": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.storage\n+\n+import org.scalatest.FunSuite\n+import org.apache.spark.{SharedSparkContext, SparkConf, LocalSparkContext, SparkContext}\n+\n+\n+class FlatmapIteratorSuite extends FunSuite with LocalSparkContext {\n+  /* Tests the ability of Spark to deal with user provided iterators from flatMap\n+   * calls, that may generate more data then available memory. In any\n+   * memory based persistance Spark will unroll the iterator into an ArrayBuffer\n+   * for caching, however in the case that the use defines DISK_ONLY persistance,\n+   * the iterator will be fed directly to the serializer and written to disk.\n+   *\n+   * This also tests the ObjectOutputStream reset rate. When serializing using the\n+   * Java serialization system, the serializer caches objects to prevent writing redundant\n+   * data, however that stops GC of those objects. By calling 'reset' you flush that\n+   * info from the serializer, and allow old objects to be GC'd\n+   */\n+  test(\"Flatmap Iterator to Disk\") {\n+    val sconf = new SparkConf().setMaster(\"local-cluster[1,1,512]\")\n+      .setAppName(\"iterator_to_disk_test\")\n+    sc = new SparkContext(sconf)\n+    try {\n+      val expand_size = 100\n+      val data = sc.parallelize( (1 to 5).toSeq ).\n+        flatMap( x => Stream.range(0, expand_size) )\n+      var persisted = data.persist(StorageLevel.DISK_ONLY)\n+      println(persisted.count())\n+      assert( persisted.count() == 500)\n+      assert( persisted.filter( _==1 ).count() == 5 )\n+    } catch {\n+      case _ : OutOfMemoryError => assert(false)"
  }],
  "prId": 50
}]