[{
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "you can avoid sleeping by using [`cache.setTicker`](https://github.com/google/guava/wiki/CachesExplained#testing-timed-eviction)\n",
    "commit": "826587fab9e2976092c0ed54b385d4c8cb596fbf",
    "createdAt": "2015-11-04T17:17:27Z",
    "diffHunk": "@@ -150,4 +158,12 @@ class StorageStatusListenerSuite extends FunSuite {\n     listener.onUnpersistRDD(SparkListenerUnpersistRDD(1))\n     assert(listener.executorIdToStorageStatus(\"big\").numBlocks === 0)\n   }\n+  \n+  test(\"Killed Executor Entry removed after configurable time\") {\n+    val localtestconf = new SparkConf().set(StorageStatusListener.TIME_TO_EXPIRE_KILLED_EXECUTOR,\"5s\")\n+    val listener = new StorageStatusListener(localtestconf)\n+    listener.removedExecutorIdToStorageStatus.put(\"1\", new StorageStatus(null, 50))\n+    Thread.sleep(5500)"
  }, {
    "author": {
      "login": "archit279thakur"
    },
    "body": "For that we'll have to set an arbitrary ticker to the main cache, We would not want to set any arbitrary ticker to the original cache. Creating a new cache in the test would not be testing of our functionality and would be equivalent of testing just the Guava Cache's code. right? Please correct, if wrong.\n",
    "commit": "826587fab9e2976092c0ed54b385d4c8cb596fbf",
    "createdAt": "2015-11-05T07:11:58Z",
    "diffHunk": "@@ -150,4 +158,12 @@ class StorageStatusListenerSuite extends FunSuite {\n     listener.onUnpersistRDD(SparkListenerUnpersistRDD(1))\n     assert(listener.executorIdToStorageStatus(\"big\").numBlocks === 0)\n   }\n+  \n+  test(\"Killed Executor Entry removed after configurable time\") {\n+    val localtestconf = new SparkConf().set(StorageStatusListener.TIME_TO_EXPIRE_KILLED_EXECUTOR,\"5s\")\n+    val listener = new StorageStatusListener(localtestconf)\n+    listener.removedExecutorIdToStorageStatus.put(\"1\", new StorageStatus(null, 50))\n+    Thread.sleep(5500)"
  }, {
    "author": {
      "login": "squito"
    },
    "body": "I think you can do something in between -- StorageStatusListener can have a `private[storage]` constructor which takes the ticker, and the public one just defaults it to the system ticker.  Yes, you would not be testing _exactly_ the same behavior, but it tests the important parts.\n\nSleeping isn't the worst thing in this case -- often it leads to flaky tests, though I don't think that would be the case here.  Still, 5 seconds is awfully long for this test when it should take a tiny fraction of that, and it adds up over all the tests.\n",
    "commit": "826587fab9e2976092c0ed54b385d4c8cb596fbf",
    "createdAt": "2015-11-05T16:08:05Z",
    "diffHunk": "@@ -150,4 +158,12 @@ class StorageStatusListenerSuite extends FunSuite {\n     listener.onUnpersistRDD(SparkListenerUnpersistRDD(1))\n     assert(listener.executorIdToStorageStatus(\"big\").numBlocks === 0)\n   }\n+  \n+  test(\"Killed Executor Entry removed after configurable time\") {\n+    val localtestconf = new SparkConf().set(StorageStatusListener.TIME_TO_EXPIRE_KILLED_EXECUTOR,\"5s\")\n+    val listener = new StorageStatusListener(localtestconf)\n+    listener.removedExecutorIdToStorageStatus.put(\"1\", new StorageStatus(null, 50))\n+    Thread.sleep(5500)"
  }],
  "prId": 6263
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "spurious extra doc, I don't think you use `SparkConfSuite`, and the import of `SparkConf` is out of order\n",
    "commit": "826587fab9e2976092c0ed54b385d4c8cb596fbf",
    "createdAt": "2015-11-04T17:18:25Z",
    "diffHunk": "@@ -21,6 +21,13 @@ import org.scalatest.FunSuite\n import org.apache.spark.Success\n import org.apache.spark.executor.TaskMetrics\n import org.apache.spark.scheduler._\n+import org.apache.spark.SparkConfSuite\n+import org.apache.spark.SparkConf\n+\n+/**\n+ * Test the behavior of StorageStatusListener in response to all relevant events.\n+ */\n+"
  }],
  "prId": 6263
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "nit: import ordering & grouping.  org.scalatest & com.google go in the same group, above the org.apache.spark group\n",
    "commit": "826587fab9e2976092c0ed54b385d4c8cb596fbf",
    "createdAt": "2015-11-05T19:20:24Z",
    "diffHunk": "@@ -17,10 +17,16 @@\n \n package org.apache.spark.storage\n \n-import org.scalatest.FunSuite\n+import java.util.concurrent.TimeUnit\n+import java.util.concurrent.atomic.AtomicLong\n+\n+import org.apache.spark.SparkConf\n import org.apache.spark.Success\n import org.apache.spark.executor.TaskMetrics\n import org.apache.spark.scheduler._\n+import org.scalatest.FunSuite\n+\n+import com.google.common.base.Ticker"
  }, {
    "author": {
      "login": "archit279thakur"
    },
    "body": "Thanks for pointing it out. Corrected.\n",
    "commit": "826587fab9e2976092c0ed54b385d4c8cb596fbf",
    "createdAt": "2015-11-05T19:44:44Z",
    "diffHunk": "@@ -17,10 +17,16 @@\n \n package org.apache.spark.storage\n \n-import org.scalatest.FunSuite\n+import java.util.concurrent.TimeUnit\n+import java.util.concurrent.atomic.AtomicLong\n+\n+import org.apache.spark.SparkConf\n import org.apache.spark.Success\n import org.apache.spark.executor.TaskMetrics\n import org.apache.spark.scheduler._\n+import org.scalatest.FunSuite\n+\n+import com.google.common.base.Ticker"
  }],
  "prId": 6263
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "this is more complicated than it needs to be -- no need for an atomic (there is only one thread here) you can just use a long. also I'd check the `removedExecutorStorageStatusList` method, rather than the cache itself.\n\n``` scala\n    class MyTicker extends Ticker {\n      var t = 0L\n      override def read(): Long = t\n    }\n    val ticker = new MyTicker\n    val listener = new StorageStatusListener(localtestconf, ticker)\n    listener.removedExecutorIdToStorageStatus.put(\"1\", new StorageStatus(null, 50))\n    assert(listener.removedExecutorStorageStatusList.nonEmpty)\n    ticker.t = 5000000001L\n    assert(listener.removedExecutorStorageStatusList.isEmpty)\n```\n",
    "commit": "826587fab9e2976092c0ed54b385d4c8cb596fbf",
    "createdAt": "2015-11-05T20:19:06Z",
    "diffHunk": "@@ -150,4 +157,21 @@ class StorageStatusListenerSuite extends FunSuite {\n     listener.onUnpersistRDD(SparkListenerUnpersistRDD(1))\n     assert(listener.executorIdToStorageStatus(\"big\").numBlocks === 0)\n   }\n+  \n+  test(\"Killed Executor Entry removed after configurable time\") {\n+    val localtestconf = new SparkConf().set(StorageStatusListener.TIME_TO_EXPIRE_KILLED_EXECUTOR,\"5s\")\n+    val ticker = new Ticker {\n+      val nanos = new AtomicLong()\n+      def advance(time: Long, timeUnit: TimeUnit) = {\n+        nanos.addAndGet(timeUnit.toNanos(time))\n+      }\n+      override def read() = {\n+        nanos.getAndAdd(0)\n+      }\n+    }\n+    val listener = new StorageStatusListener(localtestconf, ticker)\n+    listener.removedExecutorIdToStorageStatus.put(\"1\", new StorageStatus(null, 50))\n+    ticker.advance(5, TimeUnit.SECONDS)\n+    assert(listener.removedExecutorIdToStorageStatus.asMap.get(\"1\") == null)"
  }],
  "prId": 6263
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "nit: line too long\n",
    "commit": "826587fab9e2976092c0ed54b385d4c8cb596fbf",
    "createdAt": "2015-11-05T20:26:17Z",
    "diffHunk": "@@ -150,4 +157,21 @@ class StorageStatusListenerSuite extends FunSuite {\n     listener.onUnpersistRDD(SparkListenerUnpersistRDD(1))\n     assert(listener.executorIdToStorageStatus(\"big\").numBlocks === 0)\n   }\n+  \n+  test(\"Killed Executor Entry removed after configurable time\") {\n+    val localtestconf = new SparkConf().set(StorageStatusListener.TIME_TO_EXPIRE_KILLED_EXECUTOR,\"5s\")"
  }],
  "prId": 6263
}]