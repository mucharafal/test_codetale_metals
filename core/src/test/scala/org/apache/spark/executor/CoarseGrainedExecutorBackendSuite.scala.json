[{
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "nit: `nio.Files.write` should work without scalastyle warning",
    "commit": "b9dacef1d7d47d19df300628d3841b3a13c03547",
    "createdAt": "2019-05-10T16:48:48Z",
    "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+\n+import java.io.{File, PrintWriter}\n+import java.net.URL\n+import java.nio.charset.StandardCharsets\n+import java.nio.file.{Files => JavaFiles}\n+import java.nio.file.attribute.PosixFilePermission.{OWNER_EXECUTE, OWNER_READ, OWNER_WRITE}\n+import java.util.EnumSet\n+\n+import com.google.common.io.Files\n+import org.json4s.JsonAST.{JArray, JObject, JString}\n+import org.json4s.JsonDSL._\n+import org.json4s.jackson.JsonMethods.{compact, render}\n+import org.mockito.Mockito.when\n+import org.scalatest.mockito.MockitoSugar\n+\n+import org.apache.spark._\n+import org.apache.spark.internal.config._\n+import org.apache.spark.rpc.RpcEnv\n+import org.apache.spark.serializer.JavaSerializer\n+import org.apache.spark.util.Utils\n+\n+class CoarseGrainedExecutorBackendSuite extends SparkFunSuite\n+    with LocalSparkContext with MockitoSugar {\n+\n+  // scalastyle:off println\n+  private def writeFileWithJson(dir: File, strToWrite: JArray): String = {\n+    val f1 = File.createTempFile(\"test-resource-parser1\", \"\", dir)\n+    val writer1 = new PrintWriter(f1)"
  }],
  "prId": 24406
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "it is useful to include `error` in the output message",
    "commit": "b9dacef1d7d47d19df300628d3841b3a13c03547",
    "createdAt": "2019-05-10T16:49:58Z",
    "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+\n+import java.io.{File, PrintWriter}\n+import java.net.URL\n+import java.nio.charset.StandardCharsets\n+import java.nio.file.{Files => JavaFiles}\n+import java.nio.file.attribute.PosixFilePermission.{OWNER_EXECUTE, OWNER_READ, OWNER_WRITE}\n+import java.util.EnumSet\n+\n+import com.google.common.io.Files\n+import org.json4s.JsonAST.{JArray, JObject, JString}\n+import org.json4s.JsonDSL._\n+import org.json4s.jackson.JsonMethods.{compact, render}\n+import org.mockito.Mockito.when\n+import org.scalatest.mockito.MockitoSugar\n+\n+import org.apache.spark._\n+import org.apache.spark.internal.config._\n+import org.apache.spark.rpc.RpcEnv\n+import org.apache.spark.serializer.JavaSerializer\n+import org.apache.spark.util.Utils\n+\n+class CoarseGrainedExecutorBackendSuite extends SparkFunSuite\n+    with LocalSparkContext with MockitoSugar {\n+\n+  // scalastyle:off println\n+  private def writeFileWithJson(dir: File, strToWrite: JArray): String = {\n+    val f1 = File.createTempFile(\"test-resource-parser1\", \"\", dir)\n+    val writer1 = new PrintWriter(f1)\n+    writer1.println(compact(render(strToWrite)))\n+    writer1.close()\n+    f1.getPath()\n+  }\n+  // scalastyle:on println\n+\n+  test(\"parsing no resources\") {\n+    val conf = new SparkConf\n+    conf.set(SPARK_TASK_RESOURCE_PREFIX + \"gpu\" + SPARK_RESOURCE_COUNT_POSTFIX, \"2\")\n+    val serializer = new JavaSerializer(conf)\n+    val env = createMockEnv(conf, serializer)\n+\n+    // we don't really use this, just need it to get at the parser function\n+    val backend = new CoarseGrainedExecutorBackend( env.rpcEnv, \"driverurl\", \"1\", \"host1\",\n+      4, Seq.empty[URL], env, None)\n+    withTempDir { tmpDir =>\n+      val testResourceArgs: JObject = (\"\" -> \"\")\n+      val ja = JArray(List(testResourceArgs))\n+      val f1 = writeFileWithJson(tmpDir, ja)\n+      var error = intercept[SparkException] {\n+        val parsedResources = backend.parseOrFindResources(Some(f1))\n+      }.getMessage()\n+\n+      assert(error.contains(\"Exception parsing the resources in\"))"
  }, {
    "author": {
      "login": "tgravescs"
    },
    "body": "I'll add it, but the unit test output prints both already if it doesn't match, for example:\r\n\r\n- executor resource found less than required *** FAILED ***\r\n  \"Resource: gpu, with addresses: 0,1 is less than what the user requested for count: 4, via spark.executor.resource.gpu.count\" did not contain \"is l than what the user requested for count\" (CoarseGrainedExecutorBackendSuite.scala:192)\r\n",
    "commit": "b9dacef1d7d47d19df300628d3841b3a13c03547",
    "createdAt": "2019-05-10T20:09:35Z",
    "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+\n+import java.io.{File, PrintWriter}\n+import java.net.URL\n+import java.nio.charset.StandardCharsets\n+import java.nio.file.{Files => JavaFiles}\n+import java.nio.file.attribute.PosixFilePermission.{OWNER_EXECUTE, OWNER_READ, OWNER_WRITE}\n+import java.util.EnumSet\n+\n+import com.google.common.io.Files\n+import org.json4s.JsonAST.{JArray, JObject, JString}\n+import org.json4s.JsonDSL._\n+import org.json4s.jackson.JsonMethods.{compact, render}\n+import org.mockito.Mockito.when\n+import org.scalatest.mockito.MockitoSugar\n+\n+import org.apache.spark._\n+import org.apache.spark.internal.config._\n+import org.apache.spark.rpc.RpcEnv\n+import org.apache.spark.serializer.JavaSerializer\n+import org.apache.spark.util.Utils\n+\n+class CoarseGrainedExecutorBackendSuite extends SparkFunSuite\n+    with LocalSparkContext with MockitoSugar {\n+\n+  // scalastyle:off println\n+  private def writeFileWithJson(dir: File, strToWrite: JArray): String = {\n+    val f1 = File.createTempFile(\"test-resource-parser1\", \"\", dir)\n+    val writer1 = new PrintWriter(f1)\n+    writer1.println(compact(render(strToWrite)))\n+    writer1.close()\n+    f1.getPath()\n+  }\n+  // scalastyle:on println\n+\n+  test(\"parsing no resources\") {\n+    val conf = new SparkConf\n+    conf.set(SPARK_TASK_RESOURCE_PREFIX + \"gpu\" + SPARK_RESOURCE_COUNT_POSTFIX, \"2\")\n+    val serializer = new JavaSerializer(conf)\n+    val env = createMockEnv(conf, serializer)\n+\n+    // we don't really use this, just need it to get at the parser function\n+    val backend = new CoarseGrainedExecutorBackend( env.rpcEnv, \"driverurl\", \"1\", \"host1\",\n+      4, Seq.empty[URL], env, None)\n+    withTempDir { tmpDir =>\n+      val testResourceArgs: JObject = (\"\" -> \"\")\n+      val ja = JArray(List(testResourceArgs))\n+      val f1 = writeFileWithJson(tmpDir, ja)\n+      var error = intercept[SparkException] {\n+        val parsedResources = backend.parseOrFindResources(Some(f1))\n+      }.getMessage()\n+\n+      assert(error.contains(\"Exception parsing the resources in\"))"
  }],
  "prId": 24406
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`Seq(\"0\", \"1\")` should work with `org.json4s.JsonDSL._`",
    "commit": "b9dacef1d7d47d19df300628d3841b3a13c03547",
    "createdAt": "2019-05-10T16:53:52Z",
    "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+\n+import java.io.{File, PrintWriter}\n+import java.net.URL\n+import java.nio.charset.StandardCharsets\n+import java.nio.file.{Files => JavaFiles}\n+import java.nio.file.attribute.PosixFilePermission.{OWNER_EXECUTE, OWNER_READ, OWNER_WRITE}\n+import java.util.EnumSet\n+\n+import com.google.common.io.Files\n+import org.json4s.JsonAST.{JArray, JObject, JString}\n+import org.json4s.JsonDSL._\n+import org.json4s.jackson.JsonMethods.{compact, render}\n+import org.mockito.Mockito.when\n+import org.scalatest.mockito.MockitoSugar\n+\n+import org.apache.spark._\n+import org.apache.spark.internal.config._\n+import org.apache.spark.rpc.RpcEnv\n+import org.apache.spark.serializer.JavaSerializer\n+import org.apache.spark.util.Utils\n+\n+class CoarseGrainedExecutorBackendSuite extends SparkFunSuite\n+    with LocalSparkContext with MockitoSugar {\n+\n+  // scalastyle:off println\n+  private def writeFileWithJson(dir: File, strToWrite: JArray): String = {\n+    val f1 = File.createTempFile(\"test-resource-parser1\", \"\", dir)\n+    val writer1 = new PrintWriter(f1)\n+    writer1.println(compact(render(strToWrite)))\n+    writer1.close()\n+    f1.getPath()\n+  }\n+  // scalastyle:on println\n+\n+  test(\"parsing no resources\") {\n+    val conf = new SparkConf\n+    conf.set(SPARK_TASK_RESOURCE_PREFIX + \"gpu\" + SPARK_RESOURCE_COUNT_POSTFIX, \"2\")\n+    val serializer = new JavaSerializer(conf)\n+    val env = createMockEnv(conf, serializer)\n+\n+    // we don't really use this, just need it to get at the parser function\n+    val backend = new CoarseGrainedExecutorBackend( env.rpcEnv, \"driverurl\", \"1\", \"host1\",\n+      4, Seq.empty[URL], env, None)\n+    withTempDir { tmpDir =>\n+      val testResourceArgs: JObject = (\"\" -> \"\")\n+      val ja = JArray(List(testResourceArgs))\n+      val f1 = writeFileWithJson(tmpDir, ja)\n+      var error = intercept[SparkException] {\n+        val parsedResources = backend.parseOrFindResources(Some(f1))\n+      }.getMessage()\n+\n+      assert(error.contains(\"Exception parsing the resources in\"))\n+    }\n+  }\n+\n+  test(\"parsing one resources\") {\n+    val conf = new SparkConf\n+    conf.set(SPARK_EXECUTOR_RESOURCE_PREFIX + \"gpu\" + SPARK_RESOURCE_COUNT_POSTFIX, \"2\")\n+    conf.set(SPARK_TASK_RESOURCE_PREFIX + \"gpu\" + SPARK_RESOURCE_COUNT_POSTFIX, \"2\")\n+    val serializer = new JavaSerializer(conf)\n+    val env = createMockEnv(conf, serializer)\n+    // we don't really use this, just need it to get at the parser function\n+    val backend = new CoarseGrainedExecutorBackend( env.rpcEnv, \"driverurl\", \"1\", \"host1\",\n+      4, Seq.empty[URL], env, None)\n+    withTempDir { tmpDir =>\n+      val testResourceArgs =\n+        (\"name\" -> \"gpu\") ~\n+        (\"addresses\" -> JArray(Array(\"0\", \"1\").map(JString(_)).toList))\n+      val ja = JArray(List(testResourceArgs))\n+      val f1 = writeFileWithJson(tmpDir, ja)\n+      val parsedResources = backend.parseOrFindResources(Some(f1))\n+\n+      assert(parsedResources.size === 1)\n+      assert(parsedResources.get(\"gpu\").nonEmpty)\n+      assert(parsedResources.get(\"gpu\").get.name === \"gpu\")\n+      assert(parsedResources.get(\"gpu\").get.addresses.deep === Array(\"0\", \"1\").deep)\n+    }\n+  }\n+\n+  test(\"parsing multiple resources\") {\n+    val conf = new SparkConf\n+    conf.set(SPARK_EXECUTOR_RESOURCE_PREFIX + \"fpga\" + SPARK_RESOURCE_COUNT_POSTFIX, \"3\")\n+    conf.set(SPARK_TASK_RESOURCE_PREFIX + \"fpga\" + SPARK_RESOURCE_COUNT_POSTFIX, \"3\")\n+    conf.set(SPARK_EXECUTOR_RESOURCE_PREFIX + \"gpu\" + SPARK_RESOURCE_COUNT_POSTFIX, \"2\")\n+    conf.set(SPARK_TASK_RESOURCE_PREFIX + \"gpu\" + SPARK_RESOURCE_COUNT_POSTFIX, \"2\")\n+    val serializer = new JavaSerializer(conf)\n+    val env = createMockEnv(conf, serializer)\n+    // we don't really use this, just need it to get at the parser function\n+    val backend = new CoarseGrainedExecutorBackend( env.rpcEnv, \"driverurl\", \"1\", \"host1\",\n+      4, Seq.empty[URL], env, None)\n+\n+    withTempDir { tmpDir =>\n+      val gpuArgs =\n+        (\"name\" -> \"gpu\") ~\n+          (\"addresses\" -> JArray(Array(\"0\", \"1\").map(JString(_)).toList))"
  }, {
    "author": {
      "login": "tgravescs"
    },
    "body": "ah nice, that is much cleaner",
    "commit": "b9dacef1d7d47d19df300628d3841b3a13c03547",
    "createdAt": "2019-05-10T19:13:21Z",
    "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.executor\n+\n+\n+import java.io.{File, PrintWriter}\n+import java.net.URL\n+import java.nio.charset.StandardCharsets\n+import java.nio.file.{Files => JavaFiles}\n+import java.nio.file.attribute.PosixFilePermission.{OWNER_EXECUTE, OWNER_READ, OWNER_WRITE}\n+import java.util.EnumSet\n+\n+import com.google.common.io.Files\n+import org.json4s.JsonAST.{JArray, JObject, JString}\n+import org.json4s.JsonDSL._\n+import org.json4s.jackson.JsonMethods.{compact, render}\n+import org.mockito.Mockito.when\n+import org.scalatest.mockito.MockitoSugar\n+\n+import org.apache.spark._\n+import org.apache.spark.internal.config._\n+import org.apache.spark.rpc.RpcEnv\n+import org.apache.spark.serializer.JavaSerializer\n+import org.apache.spark.util.Utils\n+\n+class CoarseGrainedExecutorBackendSuite extends SparkFunSuite\n+    with LocalSparkContext with MockitoSugar {\n+\n+  // scalastyle:off println\n+  private def writeFileWithJson(dir: File, strToWrite: JArray): String = {\n+    val f1 = File.createTempFile(\"test-resource-parser1\", \"\", dir)\n+    val writer1 = new PrintWriter(f1)\n+    writer1.println(compact(render(strToWrite)))\n+    writer1.close()\n+    f1.getPath()\n+  }\n+  // scalastyle:on println\n+\n+  test(\"parsing no resources\") {\n+    val conf = new SparkConf\n+    conf.set(SPARK_TASK_RESOURCE_PREFIX + \"gpu\" + SPARK_RESOURCE_COUNT_POSTFIX, \"2\")\n+    val serializer = new JavaSerializer(conf)\n+    val env = createMockEnv(conf, serializer)\n+\n+    // we don't really use this, just need it to get at the parser function\n+    val backend = new CoarseGrainedExecutorBackend( env.rpcEnv, \"driverurl\", \"1\", \"host1\",\n+      4, Seq.empty[URL], env, None)\n+    withTempDir { tmpDir =>\n+      val testResourceArgs: JObject = (\"\" -> \"\")\n+      val ja = JArray(List(testResourceArgs))\n+      val f1 = writeFileWithJson(tmpDir, ja)\n+      var error = intercept[SparkException] {\n+        val parsedResources = backend.parseOrFindResources(Some(f1))\n+      }.getMessage()\n+\n+      assert(error.contains(\"Exception parsing the resources in\"))\n+    }\n+  }\n+\n+  test(\"parsing one resources\") {\n+    val conf = new SparkConf\n+    conf.set(SPARK_EXECUTOR_RESOURCE_PREFIX + \"gpu\" + SPARK_RESOURCE_COUNT_POSTFIX, \"2\")\n+    conf.set(SPARK_TASK_RESOURCE_PREFIX + \"gpu\" + SPARK_RESOURCE_COUNT_POSTFIX, \"2\")\n+    val serializer = new JavaSerializer(conf)\n+    val env = createMockEnv(conf, serializer)\n+    // we don't really use this, just need it to get at the parser function\n+    val backend = new CoarseGrainedExecutorBackend( env.rpcEnv, \"driverurl\", \"1\", \"host1\",\n+      4, Seq.empty[URL], env, None)\n+    withTempDir { tmpDir =>\n+      val testResourceArgs =\n+        (\"name\" -> \"gpu\") ~\n+        (\"addresses\" -> JArray(Array(\"0\", \"1\").map(JString(_)).toList))\n+      val ja = JArray(List(testResourceArgs))\n+      val f1 = writeFileWithJson(tmpDir, ja)\n+      val parsedResources = backend.parseOrFindResources(Some(f1))\n+\n+      assert(parsedResources.size === 1)\n+      assert(parsedResources.get(\"gpu\").nonEmpty)\n+      assert(parsedResources.get(\"gpu\").get.name === \"gpu\")\n+      assert(parsedResources.get(\"gpu\").get.addresses.deep === Array(\"0\", \"1\").deep)\n+    }\n+  }\n+\n+  test(\"parsing multiple resources\") {\n+    val conf = new SparkConf\n+    conf.set(SPARK_EXECUTOR_RESOURCE_PREFIX + \"fpga\" + SPARK_RESOURCE_COUNT_POSTFIX, \"3\")\n+    conf.set(SPARK_TASK_RESOURCE_PREFIX + \"fpga\" + SPARK_RESOURCE_COUNT_POSTFIX, \"3\")\n+    conf.set(SPARK_EXECUTOR_RESOURCE_PREFIX + \"gpu\" + SPARK_RESOURCE_COUNT_POSTFIX, \"2\")\n+    conf.set(SPARK_TASK_RESOURCE_PREFIX + \"gpu\" + SPARK_RESOURCE_COUNT_POSTFIX, \"2\")\n+    val serializer = new JavaSerializer(conf)\n+    val env = createMockEnv(conf, serializer)\n+    // we don't really use this, just need it to get at the parser function\n+    val backend = new CoarseGrainedExecutorBackend( env.rpcEnv, \"driverurl\", \"1\", \"host1\",\n+      4, Seq.empty[URL], env, None)\n+\n+    withTempDir { tmpDir =>\n+      val gpuArgs =\n+        (\"name\" -> \"gpu\") ~\n+          (\"addresses\" -> JArray(Array(\"0\", \"1\").map(JString(_)).toList))"
  }],
  "prId": 24406
}]