[{
  "comments": [{
    "author": {
      "login": "Ngone51"
    },
    "body": "`;` ?",
    "commit": "4664942f0509b8d34ff27ddc9427351ed836f663",
    "createdAt": "2018-05-28T10:47:56Z",
    "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.io\n+\n+import java.nio.ByteBuffer\n+import java.nio.channels.WritableByteChannel\n+\n+import scala.util.Random\n+\n+import org.mockito.Mockito.when\n+import org.scalatest.BeforeAndAfterEach\n+import org.scalatest.mockito.MockitoSugar\n+\n+import org.apache.spark.{SparkConf, SparkEnv, SparkFunSuite}\n+import org.apache.spark.internal.config\n+import org.apache.spark.util.io.ChunkedByteBuffer\n+\n+class ChunkedByteBufferFileRegionSuite extends SparkFunSuite with MockitoSugar\n+    with BeforeAndAfterEach {\n+\n+  override protected def beforeEach(): Unit = {\n+    super.beforeEach()\n+    val conf = new SparkConf()\n+    val env = mock[SparkEnv]\n+    SparkEnv.set(env)\n+    when(env.conf).thenReturn(conf)\n+  }\n+\n+  override protected def afterEach(): Unit = {\n+    SparkEnv.set(null)\n+  }\n+\n+  private def generateChunkByteBuffer(nChunks: Int, perChunk: Int): ChunkedByteBuffer = {\n+    val bytes = (0 until nChunks).map { chunkIdx =>\n+      val bb = ByteBuffer.allocate(perChunk)\n+      (0 until perChunk).foreach { idx =>\n+        bb.put((chunkIdx * perChunk + idx).toByte)\n+      }\n+      bb.position(0)\n+      bb\n+    }.toArray\n+    new ChunkedByteBuffer(bytes)\n+  }\n+\n+  test(\"transferTo can stop and resume correctly\") {\n+    SparkEnv.get.conf.set(config.BUFFER_WRITE_CHUNK_SIZE, 9L)\n+    val cbb = generateChunkByteBuffer(4, 10)\n+    val fileRegion = cbb.toNetty\n+\n+    val targetChannel = new LimitedWritableByteChannel(40)\n+\n+    var pos = 0L\n+    // write the fileregion to the channel, but with the transfer limited at various spots along\n+    // the way.\n+\n+    // limit to within the first chunk\n+    targetChannel.acceptNBytes = 5\n+    pos = fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 5)\n+\n+    // a little bit further within the first chunk\n+    targetChannel.acceptNBytes = 2\n+    pos += fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 7)\n+\n+    // past the first chunk, into the 2nd\n+    targetChannel.acceptNBytes = 6\n+    pos += fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 13)\n+\n+    // right to the end of the 2nd chunk\n+    targetChannel.acceptNBytes = 7\n+    pos += fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 20)\n+\n+    // rest of 2nd chunk, all of 3rd, some of 4th\n+    targetChannel.acceptNBytes = 15\n+    pos += fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 35)\n+\n+    // now till the end\n+    targetChannel.acceptNBytes = 5\n+    pos += fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 40)\n+\n+    // calling again at the end should be OK\n+    targetChannel.acceptNBytes = 20\n+    fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 40)\n+  }\n+\n+  test(s\"transfer to with random limits\") {\n+    val rng = new Random()\n+    val seed = System.currentTimeMillis()\n+    logInfo(s\"seed = $seed\")\n+    rng.setSeed(seed)\n+    val chunkSize = 1e4.toInt\n+    SparkEnv.get.conf.set(config.BUFFER_WRITE_CHUNK_SIZE, rng.nextInt(chunkSize).toLong)\n+\n+    val cbb = generateChunkByteBuffer(50, chunkSize)\n+    val fileRegion = cbb.toNetty\n+    val transferLimit = 1e5.toInt\n+    val targetChannel = new LimitedWritableByteChannel(transferLimit)\n+    while (targetChannel.pos < cbb.size) {\n+      val nextTransferSize = rng.nextInt(transferLimit)\n+      targetChannel.acceptNBytes = nextTransferSize\n+      fileRegion.transferTo(targetChannel, targetChannel.pos)\n+    }\n+    assert(0 === fileRegion.transferTo(targetChannel, targetChannel.pos))\n+  }\n+\n+  /**\n+   * This mocks a channel which only accepts a limited number of bytes at a time.  It also verifies\n+   * the written data matches our expectations as the data is received.\n+   * @param maxWriteSize\n+   */\n+  private class LimitedWritableByteChannel(maxWriteSize: Int) extends WritableByteChannel {\n+    val bytes = new Array[Byte](maxWriteSize)\n+    var acceptNBytes = 0\n+    var pos = 0\n+\n+    override def write(src: ByteBuffer): Int = {\n+      val origSrcPos = src.position()\n+      val length = math.min(acceptNBytes, src.remaining())\n+      src.get(bytes, 0, length)\n+      acceptNBytes -= length\n+      // verify we got the right data\n+      (0 until length).foreach { idx =>\n+        assert(bytes(idx) === (pos + idx).toByte, s\"; wrong data at ${pos + idx}\")",
    "line": 141
  }, {
    "author": {
      "login": "Ngone51"
    },
    "body": "`${pos + idx}` or `${idx}` ?",
    "commit": "4664942f0509b8d34ff27ddc9427351ed836f663",
    "createdAt": "2018-05-28T10:48:23Z",
    "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.io\n+\n+import java.nio.ByteBuffer\n+import java.nio.channels.WritableByteChannel\n+\n+import scala.util.Random\n+\n+import org.mockito.Mockito.when\n+import org.scalatest.BeforeAndAfterEach\n+import org.scalatest.mockito.MockitoSugar\n+\n+import org.apache.spark.{SparkConf, SparkEnv, SparkFunSuite}\n+import org.apache.spark.internal.config\n+import org.apache.spark.util.io.ChunkedByteBuffer\n+\n+class ChunkedByteBufferFileRegionSuite extends SparkFunSuite with MockitoSugar\n+    with BeforeAndAfterEach {\n+\n+  override protected def beforeEach(): Unit = {\n+    super.beforeEach()\n+    val conf = new SparkConf()\n+    val env = mock[SparkEnv]\n+    SparkEnv.set(env)\n+    when(env.conf).thenReturn(conf)\n+  }\n+\n+  override protected def afterEach(): Unit = {\n+    SparkEnv.set(null)\n+  }\n+\n+  private def generateChunkByteBuffer(nChunks: Int, perChunk: Int): ChunkedByteBuffer = {\n+    val bytes = (0 until nChunks).map { chunkIdx =>\n+      val bb = ByteBuffer.allocate(perChunk)\n+      (0 until perChunk).foreach { idx =>\n+        bb.put((chunkIdx * perChunk + idx).toByte)\n+      }\n+      bb.position(0)\n+      bb\n+    }.toArray\n+    new ChunkedByteBuffer(bytes)\n+  }\n+\n+  test(\"transferTo can stop and resume correctly\") {\n+    SparkEnv.get.conf.set(config.BUFFER_WRITE_CHUNK_SIZE, 9L)\n+    val cbb = generateChunkByteBuffer(4, 10)\n+    val fileRegion = cbb.toNetty\n+\n+    val targetChannel = new LimitedWritableByteChannel(40)\n+\n+    var pos = 0L\n+    // write the fileregion to the channel, but with the transfer limited at various spots along\n+    // the way.\n+\n+    // limit to within the first chunk\n+    targetChannel.acceptNBytes = 5\n+    pos = fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 5)\n+\n+    // a little bit further within the first chunk\n+    targetChannel.acceptNBytes = 2\n+    pos += fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 7)\n+\n+    // past the first chunk, into the 2nd\n+    targetChannel.acceptNBytes = 6\n+    pos += fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 13)\n+\n+    // right to the end of the 2nd chunk\n+    targetChannel.acceptNBytes = 7\n+    pos += fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 20)\n+\n+    // rest of 2nd chunk, all of 3rd, some of 4th\n+    targetChannel.acceptNBytes = 15\n+    pos += fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 35)\n+\n+    // now till the end\n+    targetChannel.acceptNBytes = 5\n+    pos += fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 40)\n+\n+    // calling again at the end should be OK\n+    targetChannel.acceptNBytes = 20\n+    fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 40)\n+  }\n+\n+  test(s\"transfer to with random limits\") {\n+    val rng = new Random()\n+    val seed = System.currentTimeMillis()\n+    logInfo(s\"seed = $seed\")\n+    rng.setSeed(seed)\n+    val chunkSize = 1e4.toInt\n+    SparkEnv.get.conf.set(config.BUFFER_WRITE_CHUNK_SIZE, rng.nextInt(chunkSize).toLong)\n+\n+    val cbb = generateChunkByteBuffer(50, chunkSize)\n+    val fileRegion = cbb.toNetty\n+    val transferLimit = 1e5.toInt\n+    val targetChannel = new LimitedWritableByteChannel(transferLimit)\n+    while (targetChannel.pos < cbb.size) {\n+      val nextTransferSize = rng.nextInt(transferLimit)\n+      targetChannel.acceptNBytes = nextTransferSize\n+      fileRegion.transferTo(targetChannel, targetChannel.pos)\n+    }\n+    assert(0 === fileRegion.transferTo(targetChannel, targetChannel.pos))\n+  }\n+\n+  /**\n+   * This mocks a channel which only accepts a limited number of bytes at a time.  It also verifies\n+   * the written data matches our expectations as the data is received.\n+   * @param maxWriteSize\n+   */\n+  private class LimitedWritableByteChannel(maxWriteSize: Int) extends WritableByteChannel {\n+    val bytes = new Array[Byte](maxWriteSize)\n+    var acceptNBytes = 0\n+    var pos = 0\n+\n+    override def write(src: ByteBuffer): Int = {\n+      val origSrcPos = src.position()\n+      val length = math.min(acceptNBytes, src.remaining())\n+      src.get(bytes, 0, length)\n+      acceptNBytes -= length\n+      // verify we got the right data\n+      (0 until length).foreach { idx =>\n+        assert(bytes(idx) === (pos + idx).toByte, s\"; wrong data at ${pos + idx}\")",
    "line": 141
  }, {
    "author": {
      "login": "squito"
    },
    "body": "';' because it separates the automatic portion of the error msg, making it easier to read IMO:\r\n```\r\n0 did not equal 1; wrong data at 0\r\n```\r\n\r\n`pos + idx` I think is more appropriate, its more helpful to know the position in the overall stream of data.",
    "commit": "4664942f0509b8d34ff27ddc9427351ed836f663",
    "createdAt": "2018-05-29T15:36:13Z",
    "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.io\n+\n+import java.nio.ByteBuffer\n+import java.nio.channels.WritableByteChannel\n+\n+import scala.util.Random\n+\n+import org.mockito.Mockito.when\n+import org.scalatest.BeforeAndAfterEach\n+import org.scalatest.mockito.MockitoSugar\n+\n+import org.apache.spark.{SparkConf, SparkEnv, SparkFunSuite}\n+import org.apache.spark.internal.config\n+import org.apache.spark.util.io.ChunkedByteBuffer\n+\n+class ChunkedByteBufferFileRegionSuite extends SparkFunSuite with MockitoSugar\n+    with BeforeAndAfterEach {\n+\n+  override protected def beforeEach(): Unit = {\n+    super.beforeEach()\n+    val conf = new SparkConf()\n+    val env = mock[SparkEnv]\n+    SparkEnv.set(env)\n+    when(env.conf).thenReturn(conf)\n+  }\n+\n+  override protected def afterEach(): Unit = {\n+    SparkEnv.set(null)\n+  }\n+\n+  private def generateChunkByteBuffer(nChunks: Int, perChunk: Int): ChunkedByteBuffer = {\n+    val bytes = (0 until nChunks).map { chunkIdx =>\n+      val bb = ByteBuffer.allocate(perChunk)\n+      (0 until perChunk).foreach { idx =>\n+        bb.put((chunkIdx * perChunk + idx).toByte)\n+      }\n+      bb.position(0)\n+      bb\n+    }.toArray\n+    new ChunkedByteBuffer(bytes)\n+  }\n+\n+  test(\"transferTo can stop and resume correctly\") {\n+    SparkEnv.get.conf.set(config.BUFFER_WRITE_CHUNK_SIZE, 9L)\n+    val cbb = generateChunkByteBuffer(4, 10)\n+    val fileRegion = cbb.toNetty\n+\n+    val targetChannel = new LimitedWritableByteChannel(40)\n+\n+    var pos = 0L\n+    // write the fileregion to the channel, but with the transfer limited at various spots along\n+    // the way.\n+\n+    // limit to within the first chunk\n+    targetChannel.acceptNBytes = 5\n+    pos = fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 5)\n+\n+    // a little bit further within the first chunk\n+    targetChannel.acceptNBytes = 2\n+    pos += fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 7)\n+\n+    // past the first chunk, into the 2nd\n+    targetChannel.acceptNBytes = 6\n+    pos += fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 13)\n+\n+    // right to the end of the 2nd chunk\n+    targetChannel.acceptNBytes = 7\n+    pos += fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 20)\n+\n+    // rest of 2nd chunk, all of 3rd, some of 4th\n+    targetChannel.acceptNBytes = 15\n+    pos += fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 35)\n+\n+    // now till the end\n+    targetChannel.acceptNBytes = 5\n+    pos += fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 40)\n+\n+    // calling again at the end should be OK\n+    targetChannel.acceptNBytes = 20\n+    fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 40)\n+  }\n+\n+  test(s\"transfer to with random limits\") {\n+    val rng = new Random()\n+    val seed = System.currentTimeMillis()\n+    logInfo(s\"seed = $seed\")\n+    rng.setSeed(seed)\n+    val chunkSize = 1e4.toInt\n+    SparkEnv.get.conf.set(config.BUFFER_WRITE_CHUNK_SIZE, rng.nextInt(chunkSize).toLong)\n+\n+    val cbb = generateChunkByteBuffer(50, chunkSize)\n+    val fileRegion = cbb.toNetty\n+    val transferLimit = 1e5.toInt\n+    val targetChannel = new LimitedWritableByteChannel(transferLimit)\n+    while (targetChannel.pos < cbb.size) {\n+      val nextTransferSize = rng.nextInt(transferLimit)\n+      targetChannel.acceptNBytes = nextTransferSize\n+      fileRegion.transferTo(targetChannel, targetChannel.pos)\n+    }\n+    assert(0 === fileRegion.transferTo(targetChannel, targetChannel.pos))\n+  }\n+\n+  /**\n+   * This mocks a channel which only accepts a limited number of bytes at a time.  It also verifies\n+   * the written data matches our expectations as the data is received.\n+   * @param maxWriteSize\n+   */\n+  private class LimitedWritableByteChannel(maxWriteSize: Int) extends WritableByteChannel {\n+    val bytes = new Array[Byte](maxWriteSize)\n+    var acceptNBytes = 0\n+    var pos = 0\n+\n+    override def write(src: ByteBuffer): Int = {\n+      val origSrcPos = src.position()\n+      val length = math.min(acceptNBytes, src.remaining())\n+      src.get(bytes, 0, length)\n+      acceptNBytes -= length\n+      // verify we got the right data\n+      (0 until length).foreach { idx =>\n+        assert(bytes(idx) === (pos + idx).toByte, s\"; wrong data at ${pos + idx}\")",
    "line": 141
  }, {
    "author": {
      "login": "Ngone51"
    },
    "body": "I see. Just because override the `bytes` array and the virtual `${pos + idx}` array position make me a little confused. Anyway, it is really a good designed test, especially for `data verify` part. ",
    "commit": "4664942f0509b8d34ff27ddc9427351ed836f663",
    "createdAt": "2018-05-30T01:54:52Z",
    "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.io\n+\n+import java.nio.ByteBuffer\n+import java.nio.channels.WritableByteChannel\n+\n+import scala.util.Random\n+\n+import org.mockito.Mockito.when\n+import org.scalatest.BeforeAndAfterEach\n+import org.scalatest.mockito.MockitoSugar\n+\n+import org.apache.spark.{SparkConf, SparkEnv, SparkFunSuite}\n+import org.apache.spark.internal.config\n+import org.apache.spark.util.io.ChunkedByteBuffer\n+\n+class ChunkedByteBufferFileRegionSuite extends SparkFunSuite with MockitoSugar\n+    with BeforeAndAfterEach {\n+\n+  override protected def beforeEach(): Unit = {\n+    super.beforeEach()\n+    val conf = new SparkConf()\n+    val env = mock[SparkEnv]\n+    SparkEnv.set(env)\n+    when(env.conf).thenReturn(conf)\n+  }\n+\n+  override protected def afterEach(): Unit = {\n+    SparkEnv.set(null)\n+  }\n+\n+  private def generateChunkByteBuffer(nChunks: Int, perChunk: Int): ChunkedByteBuffer = {\n+    val bytes = (0 until nChunks).map { chunkIdx =>\n+      val bb = ByteBuffer.allocate(perChunk)\n+      (0 until perChunk).foreach { idx =>\n+        bb.put((chunkIdx * perChunk + idx).toByte)\n+      }\n+      bb.position(0)\n+      bb\n+    }.toArray\n+    new ChunkedByteBuffer(bytes)\n+  }\n+\n+  test(\"transferTo can stop and resume correctly\") {\n+    SparkEnv.get.conf.set(config.BUFFER_WRITE_CHUNK_SIZE, 9L)\n+    val cbb = generateChunkByteBuffer(4, 10)\n+    val fileRegion = cbb.toNetty\n+\n+    val targetChannel = new LimitedWritableByteChannel(40)\n+\n+    var pos = 0L\n+    // write the fileregion to the channel, but with the transfer limited at various spots along\n+    // the way.\n+\n+    // limit to within the first chunk\n+    targetChannel.acceptNBytes = 5\n+    pos = fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 5)\n+\n+    // a little bit further within the first chunk\n+    targetChannel.acceptNBytes = 2\n+    pos += fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 7)\n+\n+    // past the first chunk, into the 2nd\n+    targetChannel.acceptNBytes = 6\n+    pos += fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 13)\n+\n+    // right to the end of the 2nd chunk\n+    targetChannel.acceptNBytes = 7\n+    pos += fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 20)\n+\n+    // rest of 2nd chunk, all of 3rd, some of 4th\n+    targetChannel.acceptNBytes = 15\n+    pos += fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 35)\n+\n+    // now till the end\n+    targetChannel.acceptNBytes = 5\n+    pos += fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 40)\n+\n+    // calling again at the end should be OK\n+    targetChannel.acceptNBytes = 20\n+    fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 40)\n+  }\n+\n+  test(s\"transfer to with random limits\") {\n+    val rng = new Random()\n+    val seed = System.currentTimeMillis()\n+    logInfo(s\"seed = $seed\")\n+    rng.setSeed(seed)\n+    val chunkSize = 1e4.toInt\n+    SparkEnv.get.conf.set(config.BUFFER_WRITE_CHUNK_SIZE, rng.nextInt(chunkSize).toLong)\n+\n+    val cbb = generateChunkByteBuffer(50, chunkSize)\n+    val fileRegion = cbb.toNetty\n+    val transferLimit = 1e5.toInt\n+    val targetChannel = new LimitedWritableByteChannel(transferLimit)\n+    while (targetChannel.pos < cbb.size) {\n+      val nextTransferSize = rng.nextInt(transferLimit)\n+      targetChannel.acceptNBytes = nextTransferSize\n+      fileRegion.transferTo(targetChannel, targetChannel.pos)\n+    }\n+    assert(0 === fileRegion.transferTo(targetChannel, targetChannel.pos))\n+  }\n+\n+  /**\n+   * This mocks a channel which only accepts a limited number of bytes at a time.  It also verifies\n+   * the written data matches our expectations as the data is received.\n+   * @param maxWriteSize\n+   */\n+  private class LimitedWritableByteChannel(maxWriteSize: Int) extends WritableByteChannel {\n+    val bytes = new Array[Byte](maxWriteSize)\n+    var acceptNBytes = 0\n+    var pos = 0\n+\n+    override def write(src: ByteBuffer): Int = {\n+      val origSrcPos = src.position()\n+      val length = math.min(acceptNBytes, src.remaining())\n+      src.get(bytes, 0, length)\n+      acceptNBytes -= length\n+      // verify we got the right data\n+      (0 until length).foreach { idx =>\n+        assert(bytes(idx) === (pos + idx).toByte, s\"; wrong data at ${pos + idx}\")",
    "line": 141
  }],
  "prId": 21440
}, {
  "comments": [{
    "author": {
      "login": "Ngone51"
    },
    "body": "nit: generateChunk**ed**ByteBuffer",
    "commit": "4664942f0509b8d34ff27ddc9427351ed836f663",
    "createdAt": "2018-05-28T10:52:56Z",
    "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.io\n+\n+import java.nio.ByteBuffer\n+import java.nio.channels.WritableByteChannel\n+\n+import scala.util.Random\n+\n+import org.mockito.Mockito.when\n+import org.scalatest.BeforeAndAfterEach\n+import org.scalatest.mockito.MockitoSugar\n+\n+import org.apache.spark.{SparkConf, SparkEnv, SparkFunSuite}\n+import org.apache.spark.internal.config\n+import org.apache.spark.util.io.ChunkedByteBuffer\n+\n+class ChunkedByteBufferFileRegionSuite extends SparkFunSuite with MockitoSugar\n+    with BeforeAndAfterEach {\n+\n+  override protected def beforeEach(): Unit = {\n+    super.beforeEach()\n+    val conf = new SparkConf()\n+    val env = mock[SparkEnv]\n+    SparkEnv.set(env)\n+    when(env.conf).thenReturn(conf)\n+  }\n+\n+  override protected def afterEach(): Unit = {\n+    SparkEnv.set(null)\n+  }\n+\n+  private def generateChunkByteBuffer(nChunks: Int, perChunk: Int): ChunkedByteBuffer = {"
  }],
  "prId": 21440
}, {
  "comments": [{
    "author": {
      "login": "Ngone51"
    },
    "body": "Also seems it is unused.",
    "commit": "4664942f0509b8d34ff27ddc9427351ed836f663",
    "createdAt": "2018-05-28T11:04:09Z",
    "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.io\n+\n+import java.nio.ByteBuffer\n+import java.nio.channels.WritableByteChannel\n+\n+import scala.util.Random\n+\n+import org.mockito.Mockito.when\n+import org.scalatest.BeforeAndAfterEach\n+import org.scalatest.mockito.MockitoSugar\n+\n+import org.apache.spark.{SparkConf, SparkEnv, SparkFunSuite}\n+import org.apache.spark.internal.config\n+import org.apache.spark.util.io.ChunkedByteBuffer\n+\n+class ChunkedByteBufferFileRegionSuite extends SparkFunSuite with MockitoSugar\n+    with BeforeAndAfterEach {\n+\n+  override protected def beforeEach(): Unit = {\n+    super.beforeEach()\n+    val conf = new SparkConf()\n+    val env = mock[SparkEnv]\n+    SparkEnv.set(env)\n+    when(env.conf).thenReturn(conf)\n+  }\n+\n+  override protected def afterEach(): Unit = {\n+    SparkEnv.set(null)\n+  }\n+\n+  private def generateChunkByteBuffer(nChunks: Int, perChunk: Int): ChunkedByteBuffer = {\n+    val bytes = (0 until nChunks).map { chunkIdx =>\n+      val bb = ByteBuffer.allocate(perChunk)\n+      (0 until perChunk).foreach { idx =>\n+        bb.put((chunkIdx * perChunk + idx).toByte)\n+      }\n+      bb.position(0)\n+      bb\n+    }.toArray\n+    new ChunkedByteBuffer(bytes)\n+  }\n+\n+  test(\"transferTo can stop and resume correctly\") {\n+    SparkEnv.get.conf.set(config.BUFFER_WRITE_CHUNK_SIZE, 9L)\n+    val cbb = generateChunkByteBuffer(4, 10)\n+    val fileRegion = cbb.toNetty\n+\n+    val targetChannel = new LimitedWritableByteChannel(40)\n+\n+    var pos = 0L\n+    // write the fileregion to the channel, but with the transfer limited at various spots along\n+    // the way.\n+\n+    // limit to within the first chunk\n+    targetChannel.acceptNBytes = 5\n+    pos = fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 5)\n+\n+    // a little bit further within the first chunk\n+    targetChannel.acceptNBytes = 2\n+    pos += fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 7)\n+\n+    // past the first chunk, into the 2nd\n+    targetChannel.acceptNBytes = 6\n+    pos += fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 13)\n+\n+    // right to the end of the 2nd chunk\n+    targetChannel.acceptNBytes = 7\n+    pos += fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 20)\n+\n+    // rest of 2nd chunk, all of 3rd, some of 4th\n+    targetChannel.acceptNBytes = 15\n+    pos += fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 35)\n+\n+    // now till the end\n+    targetChannel.acceptNBytes = 5\n+    pos += fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 40)\n+\n+    // calling again at the end should be OK\n+    targetChannel.acceptNBytes = 20\n+    fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 40)\n+  }\n+\n+  test(s\"transfer to with random limits\") {\n+    val rng = new Random()\n+    val seed = System.currentTimeMillis()\n+    logInfo(s\"seed = $seed\")\n+    rng.setSeed(seed)\n+    val chunkSize = 1e4.toInt\n+    SparkEnv.get.conf.set(config.BUFFER_WRITE_CHUNK_SIZE, rng.nextInt(chunkSize).toLong)\n+\n+    val cbb = generateChunkByteBuffer(50, chunkSize)\n+    val fileRegion = cbb.toNetty\n+    val transferLimit = 1e5.toInt\n+    val targetChannel = new LimitedWritableByteChannel(transferLimit)\n+    while (targetChannel.pos < cbb.size) {\n+      val nextTransferSize = rng.nextInt(transferLimit)\n+      targetChannel.acceptNBytes = nextTransferSize\n+      fileRegion.transferTo(targetChannel, targetChannel.pos)\n+    }\n+    assert(0 === fileRegion.transferTo(targetChannel, targetChannel.pos))\n+  }\n+\n+  /**\n+   * This mocks a channel which only accepts a limited number of bytes at a time.  It also verifies\n+   * the written data matches our expectations as the data is received.\n+   * @param maxWriteSize\n+   */\n+  private class LimitedWritableByteChannel(maxWriteSize: Int) extends WritableByteChannel {\n+    val bytes = new Array[Byte](maxWriteSize)\n+    var acceptNBytes = 0\n+    var pos = 0\n+\n+    override def write(src: ByteBuffer): Int = {\n+      val origSrcPos = src.position()"
  }],
  "prId": 21440
}, {
  "comments": [{
    "author": {
      "login": "Ngone51"
    },
    "body": "We override `bytes` array's previously written data ?",
    "commit": "4664942f0509b8d34ff27ddc9427351ed836f663",
    "createdAt": "2018-05-28T11:27:47Z",
    "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.io\n+\n+import java.nio.ByteBuffer\n+import java.nio.channels.WritableByteChannel\n+\n+import scala.util.Random\n+\n+import org.mockito.Mockito.when\n+import org.scalatest.BeforeAndAfterEach\n+import org.scalatest.mockito.MockitoSugar\n+\n+import org.apache.spark.{SparkConf, SparkEnv, SparkFunSuite}\n+import org.apache.spark.internal.config\n+import org.apache.spark.util.io.ChunkedByteBuffer\n+\n+class ChunkedByteBufferFileRegionSuite extends SparkFunSuite with MockitoSugar\n+    with BeforeAndAfterEach {\n+\n+  override protected def beforeEach(): Unit = {\n+    super.beforeEach()\n+    val conf = new SparkConf()\n+    val env = mock[SparkEnv]\n+    SparkEnv.set(env)\n+    when(env.conf).thenReturn(conf)\n+  }\n+\n+  override protected def afterEach(): Unit = {\n+    SparkEnv.set(null)\n+  }\n+\n+  private def generateChunkByteBuffer(nChunks: Int, perChunk: Int): ChunkedByteBuffer = {\n+    val bytes = (0 until nChunks).map { chunkIdx =>\n+      val bb = ByteBuffer.allocate(perChunk)\n+      (0 until perChunk).foreach { idx =>\n+        bb.put((chunkIdx * perChunk + idx).toByte)\n+      }\n+      bb.position(0)\n+      bb\n+    }.toArray\n+    new ChunkedByteBuffer(bytes)\n+  }\n+\n+  test(\"transferTo can stop and resume correctly\") {\n+    SparkEnv.get.conf.set(config.BUFFER_WRITE_CHUNK_SIZE, 9L)\n+    val cbb = generateChunkByteBuffer(4, 10)\n+    val fileRegion = cbb.toNetty\n+\n+    val targetChannel = new LimitedWritableByteChannel(40)\n+\n+    var pos = 0L\n+    // write the fileregion to the channel, but with the transfer limited at various spots along\n+    // the way.\n+\n+    // limit to within the first chunk\n+    targetChannel.acceptNBytes = 5\n+    pos = fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 5)\n+\n+    // a little bit further within the first chunk\n+    targetChannel.acceptNBytes = 2\n+    pos += fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 7)\n+\n+    // past the first chunk, into the 2nd\n+    targetChannel.acceptNBytes = 6\n+    pos += fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 13)\n+\n+    // right to the end of the 2nd chunk\n+    targetChannel.acceptNBytes = 7\n+    pos += fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 20)\n+\n+    // rest of 2nd chunk, all of 3rd, some of 4th\n+    targetChannel.acceptNBytes = 15\n+    pos += fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 35)\n+\n+    // now till the end\n+    targetChannel.acceptNBytes = 5\n+    pos += fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 40)\n+\n+    // calling again at the end should be OK\n+    targetChannel.acceptNBytes = 20\n+    fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 40)\n+  }\n+\n+  test(s\"transfer to with random limits\") {\n+    val rng = new Random()\n+    val seed = System.currentTimeMillis()\n+    logInfo(s\"seed = $seed\")\n+    rng.setSeed(seed)\n+    val chunkSize = 1e4.toInt\n+    SparkEnv.get.conf.set(config.BUFFER_WRITE_CHUNK_SIZE, rng.nextInt(chunkSize).toLong)\n+\n+    val cbb = generateChunkByteBuffer(50, chunkSize)\n+    val fileRegion = cbb.toNetty\n+    val transferLimit = 1e5.toInt\n+    val targetChannel = new LimitedWritableByteChannel(transferLimit)\n+    while (targetChannel.pos < cbb.size) {\n+      val nextTransferSize = rng.nextInt(transferLimit)\n+      targetChannel.acceptNBytes = nextTransferSize\n+      fileRegion.transferTo(targetChannel, targetChannel.pos)\n+    }\n+    assert(0 === fileRegion.transferTo(targetChannel, targetChannel.pos))\n+  }\n+\n+  /**\n+   * This mocks a channel which only accepts a limited number of bytes at a time.  It also verifies\n+   * the written data matches our expectations as the data is received.\n+   * @param maxWriteSize\n+   */\n+  private class LimitedWritableByteChannel(maxWriteSize: Int) extends WritableByteChannel {\n+    val bytes = new Array[Byte](maxWriteSize)\n+    var acceptNBytes = 0\n+    var pos = 0\n+\n+    override def write(src: ByteBuffer): Int = {\n+      val origSrcPos = src.position()\n+      val length = math.min(acceptNBytes, src.remaining())\n+      src.get(bytes, 0, length)"
  }, {
    "author": {
      "login": "squito"
    },
    "body": "yes, this is just test code, we're just checking the data that gets written is what we expect (which we know based on the absolute position).  Really, I could read just one byte at a time and check that is it the right data, but it seemed a little easier this way.",
    "commit": "4664942f0509b8d34ff27ddc9427351ed836f663",
    "createdAt": "2018-05-29T15:37:31Z",
    "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.io\n+\n+import java.nio.ByteBuffer\n+import java.nio.channels.WritableByteChannel\n+\n+import scala.util.Random\n+\n+import org.mockito.Mockito.when\n+import org.scalatest.BeforeAndAfterEach\n+import org.scalatest.mockito.MockitoSugar\n+\n+import org.apache.spark.{SparkConf, SparkEnv, SparkFunSuite}\n+import org.apache.spark.internal.config\n+import org.apache.spark.util.io.ChunkedByteBuffer\n+\n+class ChunkedByteBufferFileRegionSuite extends SparkFunSuite with MockitoSugar\n+    with BeforeAndAfterEach {\n+\n+  override protected def beforeEach(): Unit = {\n+    super.beforeEach()\n+    val conf = new SparkConf()\n+    val env = mock[SparkEnv]\n+    SparkEnv.set(env)\n+    when(env.conf).thenReturn(conf)\n+  }\n+\n+  override protected def afterEach(): Unit = {\n+    SparkEnv.set(null)\n+  }\n+\n+  private def generateChunkByteBuffer(nChunks: Int, perChunk: Int): ChunkedByteBuffer = {\n+    val bytes = (0 until nChunks).map { chunkIdx =>\n+      val bb = ByteBuffer.allocate(perChunk)\n+      (0 until perChunk).foreach { idx =>\n+        bb.put((chunkIdx * perChunk + idx).toByte)\n+      }\n+      bb.position(0)\n+      bb\n+    }.toArray\n+    new ChunkedByteBuffer(bytes)\n+  }\n+\n+  test(\"transferTo can stop and resume correctly\") {\n+    SparkEnv.get.conf.set(config.BUFFER_WRITE_CHUNK_SIZE, 9L)\n+    val cbb = generateChunkByteBuffer(4, 10)\n+    val fileRegion = cbb.toNetty\n+\n+    val targetChannel = new LimitedWritableByteChannel(40)\n+\n+    var pos = 0L\n+    // write the fileregion to the channel, but with the transfer limited at various spots along\n+    // the way.\n+\n+    // limit to within the first chunk\n+    targetChannel.acceptNBytes = 5\n+    pos = fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 5)\n+\n+    // a little bit further within the first chunk\n+    targetChannel.acceptNBytes = 2\n+    pos += fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 7)\n+\n+    // past the first chunk, into the 2nd\n+    targetChannel.acceptNBytes = 6\n+    pos += fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 13)\n+\n+    // right to the end of the 2nd chunk\n+    targetChannel.acceptNBytes = 7\n+    pos += fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 20)\n+\n+    // rest of 2nd chunk, all of 3rd, some of 4th\n+    targetChannel.acceptNBytes = 15\n+    pos += fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 35)\n+\n+    // now till the end\n+    targetChannel.acceptNBytes = 5\n+    pos += fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 40)\n+\n+    // calling again at the end should be OK\n+    targetChannel.acceptNBytes = 20\n+    fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 40)\n+  }\n+\n+  test(s\"transfer to with random limits\") {\n+    val rng = new Random()\n+    val seed = System.currentTimeMillis()\n+    logInfo(s\"seed = $seed\")\n+    rng.setSeed(seed)\n+    val chunkSize = 1e4.toInt\n+    SparkEnv.get.conf.set(config.BUFFER_WRITE_CHUNK_SIZE, rng.nextInt(chunkSize).toLong)\n+\n+    val cbb = generateChunkByteBuffer(50, chunkSize)\n+    val fileRegion = cbb.toNetty\n+    val transferLimit = 1e5.toInt\n+    val targetChannel = new LimitedWritableByteChannel(transferLimit)\n+    while (targetChannel.pos < cbb.size) {\n+      val nextTransferSize = rng.nextInt(transferLimit)\n+      targetChannel.acceptNBytes = nextTransferSize\n+      fileRegion.transferTo(targetChannel, targetChannel.pos)\n+    }\n+    assert(0 === fileRegion.transferTo(targetChannel, targetChannel.pos))\n+  }\n+\n+  /**\n+   * This mocks a channel which only accepts a limited number of bytes at a time.  It also verifies\n+   * the written data matches our expectations as the data is received.\n+   * @param maxWriteSize\n+   */\n+  private class LimitedWritableByteChannel(maxWriteSize: Int) extends WritableByteChannel {\n+    val bytes = new Array[Byte](maxWriteSize)\n+    var acceptNBytes = 0\n+    var pos = 0\n+\n+    override def write(src: ByteBuffer): Int = {\n+      val origSrcPos = src.position()\n+      val length = math.min(acceptNBytes, src.remaining())\n+      src.get(bytes, 0, length)"
  }],
  "prId": 21440
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "remove",
    "commit": "4664942f0509b8d34ff27ddc9427351ed836f663",
    "createdAt": "2018-06-27T23:16:29Z",
    "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.io\n+\n+import java.nio.ByteBuffer\n+import java.nio.channels.WritableByteChannel\n+\n+import scala.util.Random\n+\n+import org.mockito.Mockito.when\n+import org.scalatest.BeforeAndAfterEach\n+import org.scalatest.mockito.MockitoSugar\n+\n+import org.apache.spark.{SparkConf, SparkEnv, SparkFunSuite}\n+import org.apache.spark.internal.config\n+import org.apache.spark.util.io.ChunkedByteBuffer\n+\n+class ChunkedByteBufferFileRegionSuite extends SparkFunSuite with MockitoSugar\n+    with BeforeAndAfterEach {\n+\n+  override protected def beforeEach(): Unit = {\n+    super.beforeEach()\n+    val conf = new SparkConf()\n+    val env = mock[SparkEnv]\n+    SparkEnv.set(env)\n+    when(env.conf).thenReturn(conf)\n+  }\n+\n+  override protected def afterEach(): Unit = {\n+    SparkEnv.set(null)\n+  }\n+\n+  private def generateChunkedByteBuffer(nChunks: Int, perChunk: Int): ChunkedByteBuffer = {\n+    val bytes = (0 until nChunks).map { chunkIdx =>\n+      val bb = ByteBuffer.allocate(perChunk)\n+      (0 until perChunk).foreach { idx =>\n+        bb.put((chunkIdx * perChunk + idx).toByte)\n+      }\n+      bb.position(0)\n+      bb\n+    }.toArray\n+    new ChunkedByteBuffer(bytes)\n+  }\n+\n+  test(\"transferTo can stop and resume correctly\") {\n+    SparkEnv.get.conf.set(config.BUFFER_WRITE_CHUNK_SIZE, 9L)\n+    val cbb = generateChunkedByteBuffer(4, 10)\n+    val fileRegion = cbb.toNetty\n+\n+    val targetChannel = new LimitedWritableByteChannel(40)\n+\n+    var pos = 0L\n+    // write the fileregion to the channel, but with the transfer limited at various spots along\n+    // the way.\n+\n+    // limit to within the first chunk\n+    targetChannel.acceptNBytes = 5\n+    pos = fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 5)\n+\n+    // a little bit further within the first chunk\n+    targetChannel.acceptNBytes = 2\n+    pos += fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 7)\n+\n+    // past the first chunk, into the 2nd\n+    targetChannel.acceptNBytes = 6\n+    pos += fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 13)\n+\n+    // right to the end of the 2nd chunk\n+    targetChannel.acceptNBytes = 7\n+    pos += fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 20)\n+\n+    // rest of 2nd chunk, all of 3rd, some of 4th\n+    targetChannel.acceptNBytes = 15\n+    pos += fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 35)\n+\n+    // now till the end\n+    targetChannel.acceptNBytes = 5\n+    pos += fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 40)\n+\n+    // calling again at the end should be OK\n+    targetChannel.acceptNBytes = 20\n+    fileRegion.transferTo(targetChannel, pos)\n+    assert(targetChannel.pos === 40)\n+  }\n+\n+  test(s\"transfer to with random limits\") {\n+    val rng = new Random()\n+    val seed = System.currentTimeMillis()\n+    logInfo(s\"seed = $seed\")\n+    rng.setSeed(seed)\n+    val chunkSize = 1e4.toInt\n+    SparkEnv.get.conf.set(config.BUFFER_WRITE_CHUNK_SIZE, rng.nextInt(chunkSize).toLong)\n+\n+    val cbb = generateChunkedByteBuffer(50, chunkSize)\n+    val fileRegion = cbb.toNetty\n+    val transferLimit = 1e5.toInt\n+    val targetChannel = new LimitedWritableByteChannel(transferLimit)\n+    while (targetChannel.pos < cbb.size) {\n+      val nextTransferSize = rng.nextInt(transferLimit)\n+      targetChannel.acceptNBytes = nextTransferSize\n+      fileRegion.transferTo(targetChannel, targetChannel.pos)\n+    }\n+    assert(0 === fileRegion.transferTo(targetChannel, targetChannel.pos))\n+  }\n+\n+  /**\n+   * This mocks a channel which only accepts a limited number of bytes at a time.  It also verifies\n+   * the written data matches our expectations as the data is received.\n+   * @param maxWriteSize"
  }],
  "prId": 21440
}]