[{
  "comments": [{
    "author": {
      "login": "mateiz"
    },
    "body": "Split these guys onto multiple lines and make sure to follow the code style in terms of spaces around operators, etc. For example:\n\n```\nprivate def xmap(x: RDD[String], uc: UnserializableClass): RDD[String] = \n  x.map(y => uc.op(y))\n\nprivate def xmapWith(x: RDD[String], uc: UnserializableClass): RDD[String] =\n  x.mapWith(x => x.toString)((x,y) => x + uc.op(y))\n\n...\n```\n\nAlso, since this is in a test, there's no need to write `private`, and probably no need to start these things with `x`\n",
    "commit": "f4cafa0712be5011ff9dc7e5e6d27a7077095b28",
    "createdAt": "2014-04-04T21:23:48Z",
    "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.serializer;\n+\n+import java.io.NotSerializableException\n+\n+import org.scalatest.FunSuite\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.SparkException\n+import org.apache.spark.SharedSparkContext\n+\n+/* A trivial (but unserializable) container for trivial functions */\n+class UnserializableClass {\n+  def op[T](x: T) = x.toString\n+  \n+  def pred[T](x: T) = x.toString.length % 2 == 0\n+}\n+\n+class ProactiveClosureSerializationSuite extends FunSuite with SharedSparkContext {\n+\n+  def fixture = (sc.parallelize(0 until 1000).map(_.toString), new UnserializableClass)\n+\n+  test(\"throws expected serialization exceptions on actions\") {\n+    val (data, uc) = fixture\n+      \n+    val ex = intercept[SparkException] {\n+      data.map(uc.op(_)).count\n+    }\n+        \n+    assert(ex.getMessage.matches(\".*Task not serializable.*\"))\n+  }\n+\n+  // There is probably a cleaner way to eliminate boilerplate here, but we're\n+  // iterating over a map from transformation names to functions that perform that\n+  // transformation on a given RDD, creating one test case for each\n+  \n+  for (transformation <- \n+      Map(\"map\" -> xmap _, \"flatMap\" -> xflatMap _, \"filter\" -> xfilter _, \"mapWith\" -> xmapWith _,\n+          \"mapPartitions\" -> xmapPartitions _, \"mapPartitionsWithIndex\" -> xmapPartitionsWithIndex _,\n+          \"mapPartitionsWithContext\" -> xmapPartitionsWithContext _, \"filterWith\" -> xfilterWith _)) {\n+    val (name, xf) = transformation\n+    \n+    test(s\"$name transformations throw proactive serialization exceptions\") {\n+      val (data, uc) = fixture\n+      \n+      val ex = intercept[SparkException] {\n+        xf(data, uc)\n+      }\n+\n+      assert(ex.getMessage.matches(\".*Task not serializable.*\"), s\"RDD.$name doesn't proactively throw NotSerializableException\")\n+    }\n+  }\n+  \n+  private def xmap(x: RDD[String], uc: UnserializableClass): RDD[String] = x.map(y=>uc.op(y))\n+  private def xmapWith(x: RDD[String], uc: UnserializableClass): RDD[String] = x.mapWith(x => x.toString)((x,y)=>x + uc.op(y))\n+  private def xflatMap(x: RDD[String], uc: UnserializableClass): RDD[String] = x.flatMap(y=>Seq(uc.op(y)))\n+  private def xfilter(x: RDD[String], uc: UnserializableClass): RDD[String] = x.filter(y=>uc.pred(y))\n+  private def xfilterWith(x: RDD[String], uc: UnserializableClass): RDD[String] = x.filterWith(x => x.toString)((x,y)=>uc.pred(y))\n+  private def xmapPartitions(x: RDD[String], uc: UnserializableClass): RDD[String] = x.mapPartitions(_.map(y=>uc.op(y)))\n+  private def xmapPartitionsWithIndex(x: RDD[String], uc: UnserializableClass): RDD[String] = x.mapPartitionsWithIndex((_, it) => it.map(y=>uc.op(y)))\n+  private def xmapPartitionsWithContext(x: RDD[String], uc: UnserializableClass): RDD[String] = x.mapPartitionsWithContext((_, it) => it.map(y=>uc.op(y)))"
  }],
  "prId": 189
}]