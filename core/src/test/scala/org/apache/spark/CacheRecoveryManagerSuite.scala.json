[{
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: indentation",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-06T22:54:25Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.concurrent.{Future, Promise}\n+import scala.concurrent.ExecutionContext.Implicits.global\n+import scala.reflect.ClassTag\n+\n+import org.mockito.Mockito._\n+import org.scalatest.Matchers\n+import org.scalatest.mock.MockitoSugar\n+\n+import org.apache.spark.rpc._\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages._\n+\n+class CacheRecoveryManagerSuite extends SparkFunSuite with MockitoSugar with Matchers {\n+  val oneGB: Long = 1024L * 1024L * 1024L * 1024L\n+  val plentyOfMem = Map(BlockManagerId(\"1\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"2\", \"host\", 12, None) -> ((oneGB, oneGB)),"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "fixed",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-13T18:33:45Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.concurrent.{Future, Promise}\n+import scala.concurrent.ExecutionContext.Implicits.global\n+import scala.reflect.ClassTag\n+\n+import org.mockito.Mockito._\n+import org.scalatest.Matchers\n+import org.scalatest.mock.MockitoSugar\n+\n+import org.apache.spark.rpc._\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages._\n+\n+class CacheRecoveryManagerSuite extends SparkFunSuite with MockitoSugar with Matchers {\n+  val oneGB: Long = 1024L * 1024L * 1024L * 1024L\n+  val plentyOfMem = Map(BlockManagerId(\"1\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"2\", \"host\", 12, None) -> ((oneGB, oneGB)),"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "What is `GracefulShutdown`?",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-06T22:54:48Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.concurrent.{Future, Promise}\n+import scala.concurrent.ExecutionContext.Implicits.global\n+import scala.reflect.ClassTag\n+\n+import org.mockito.Mockito._\n+import org.scalatest.Matchers\n+import org.scalatest.mock.MockitoSugar\n+\n+import org.apache.spark.rpc._\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages._\n+\n+class CacheRecoveryManagerSuite extends SparkFunSuite with MockitoSugar with Matchers {\n+  val oneGB: Long = 1024L * 1024L * 1024L * 1024L\n+  val plentyOfMem = Map(BlockManagerId(\"1\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"2\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"3\", \"host\", 12, None) -> ((oneGB, oneGB)))\n+\n+  test(\"GracefulShutdown will take blocks until empty and then kill executor\") {"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "fixed",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-13T18:34:04Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.concurrent.{Future, Promise}\n+import scala.concurrent.ExecutionContext.Implicits.global\n+import scala.reflect.ClassTag\n+\n+import org.mockito.Mockito._\n+import org.scalatest.Matchers\n+import org.scalatest.mock.MockitoSugar\n+\n+import org.apache.spark.rpc._\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages._\n+\n+class CacheRecoveryManagerSuite extends SparkFunSuite with MockitoSugar with Matchers {\n+  val oneGB: Long = 1024L * 1024L * 1024L * 1024L\n+  val plentyOfMem = Map(BlockManagerId(\"1\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"2\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"3\", \"host\", 12, None) -> ((oneGB, oneGB)))\n+\n+  test(\"GracefulShutdown will take blocks until empty and then kill executor\") {"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Don't you need to stop these things to avoid leaking thread pools?",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-06T22:55:48Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.concurrent.{Future, Promise}\n+import scala.concurrent.ExecutionContext.Implicits.global\n+import scala.reflect.ClassTag\n+\n+import org.mockito.Mockito._\n+import org.scalatest.Matchers\n+import org.scalatest.mock.MockitoSugar\n+\n+import org.apache.spark.rpc._\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages._\n+\n+class CacheRecoveryManagerSuite extends SparkFunSuite with MockitoSugar with Matchers {\n+  val oneGB: Long = 1024L * 1024L * 1024L * 1024L\n+  val plentyOfMem = Map(BlockManagerId(\"1\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"2\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"3\", \"host\", 12, None) -> ((oneGB, oneGB)))\n+\n+  test(\"GracefulShutdown will take blocks until empty and then kill executor\") {\n+    val conf = new SparkConf()\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Seq(RDDBlockId(1, 1), RDDBlockId(2, 1))\n+    val bmme = FakeBMM(1, blocks.iterator, plentyOfMem)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "fixed",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-13T18:34:17Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.concurrent.{Future, Promise}\n+import scala.concurrent.ExecutionContext.Implicits.global\n+import scala.reflect.ClassTag\n+\n+import org.mockito.Mockito._\n+import org.scalatest.Matchers\n+import org.scalatest.mock.MockitoSugar\n+\n+import org.apache.spark.rpc._\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages._\n+\n+class CacheRecoveryManagerSuite extends SparkFunSuite with MockitoSugar with Matchers {\n+  val oneGB: Long = 1024L * 1024L * 1024L * 1024L\n+  val plentyOfMem = Map(BlockManagerId(\"1\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"2\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"3\", \"host\", 12, None) -> ((oneGB, oneGB)))\n+\n+  test(\"GracefulShutdown will take blocks until empty and then kill executor\") {\n+    val conf = new SparkConf()\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Seq(RDDBlockId(1, 1), RDDBlockId(2, 1))\n+    val bmme = FakeBMM(1, blocks.iterator, plentyOfMem)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`sleep` is a hacky way of doing this. If there isn't a more explicit way of doing it (e.g. by waiting on a future), I recommend using `eventually`.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-06T22:56:59Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.concurrent.{Future, Promise}\n+import scala.concurrent.ExecutionContext.Implicits.global\n+import scala.reflect.ClassTag\n+\n+import org.mockito.Mockito._\n+import org.scalatest.Matchers\n+import org.scalatest.mock.MockitoSugar\n+\n+import org.apache.spark.rpc._\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages._\n+\n+class CacheRecoveryManagerSuite extends SparkFunSuite with MockitoSugar with Matchers {\n+  val oneGB: Long = 1024L * 1024L * 1024L * 1024L\n+  val plentyOfMem = Map(BlockManagerId(\"1\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"2\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"3\", \"host\", 12, None) -> ((oneGB, oneGB)))\n+\n+  test(\"GracefulShutdown will take blocks until empty and then kill executor\") {\n+    val conf = new SparkConf()\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Seq(RDDBlockId(1, 1), RDDBlockId(2, 1))\n+    val bmme = FakeBMM(1, blocks.iterator, plentyOfMem)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)\n+\n+    when(eam.killExecutors(Seq(\"1\"))).thenReturn(Seq(\"1\"))\n+\n+    cacheRecoveryManager.startCacheRecovery(Seq(\"1\"))\n+    Thread.sleep(1000)"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "Replaced all of these with eventually",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-13T18:34:51Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.concurrent.{Future, Promise}\n+import scala.concurrent.ExecutionContext.Implicits.global\n+import scala.reflect.ClassTag\n+\n+import org.mockito.Mockito._\n+import org.scalatest.Matchers\n+import org.scalatest.mock.MockitoSugar\n+\n+import org.apache.spark.rpc._\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages._\n+\n+class CacheRecoveryManagerSuite extends SparkFunSuite with MockitoSugar with Matchers {\n+  val oneGB: Long = 1024L * 1024L * 1024L * 1024L\n+  val plentyOfMem = Map(BlockManagerId(\"1\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"2\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"3\", \"host\", 12, None) -> ((oneGB, oneGB)))\n+\n+  test(\"GracefulShutdown will take blocks until empty and then kill executor\") {\n+    val conf = new SparkConf()\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Seq(RDDBlockId(1, 1), RDDBlockId(2, 1))\n+    val bmme = FakeBMM(1, blocks.iterator, plentyOfMem)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)\n+\n+    when(eam.killExecutors(Seq(\"1\"))).thenReturn(Seq(\"1\"))\n+\n+    cacheRecoveryManager.startCacheRecovery(Seq(\"1\"))\n+    Thread.sleep(1000)"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: too many empty lines",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-06T22:57:07Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.concurrent.{Future, Promise}\n+import scala.concurrent.ExecutionContext.Implicits.global\n+import scala.reflect.ClassTag\n+\n+import org.mockito.Mockito._\n+import org.scalatest.Matchers\n+import org.scalatest.mock.MockitoSugar\n+\n+import org.apache.spark.rpc._\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages._\n+\n+class CacheRecoveryManagerSuite extends SparkFunSuite with MockitoSugar with Matchers {\n+  val oneGB: Long = 1024L * 1024L * 1024L * 1024L\n+  val plentyOfMem = Map(BlockManagerId(\"1\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"2\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"3\", \"host\", 12, None) -> ((oneGB, oneGB)))\n+\n+  test(\"GracefulShutdown will take blocks until empty and then kill executor\") {\n+    val conf = new SparkConf()\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Seq(RDDBlockId(1, 1), RDDBlockId(2, 1))\n+    val bmme = FakeBMM(1, blocks.iterator, plentyOfMem)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)\n+\n+    when(eam.killExecutors(Seq(\"1\"))).thenReturn(Seq(\"1\"))\n+\n+    cacheRecoveryManager.startCacheRecovery(Seq(\"1\"))\n+    Thread.sleep(1000)\n+    verify(eam).killExecutors(Seq(\"1\"))\n+\n+"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "fixed",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-13T18:35:10Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.concurrent.{Future, Promise}\n+import scala.concurrent.ExecutionContext.Implicits.global\n+import scala.reflect.ClassTag\n+\n+import org.mockito.Mockito._\n+import org.scalatest.Matchers\n+import org.scalatest.mock.MockitoSugar\n+\n+import org.apache.spark.rpc._\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages._\n+\n+class CacheRecoveryManagerSuite extends SparkFunSuite with MockitoSugar with Matchers {\n+  val oneGB: Long = 1024L * 1024L * 1024L * 1024L\n+  val plentyOfMem = Map(BlockManagerId(\"1\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"2\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"3\", \"host\", 12, None) -> ((oneGB, oneGB)))\n+\n+  test(\"GracefulShutdown will take blocks until empty and then kill executor\") {\n+    val conf = new SparkConf()\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Seq(RDDBlockId(1, 1), RDDBlockId(2, 1))\n+    val bmme = FakeBMM(1, blocks.iterator, plentyOfMem)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)\n+\n+    when(eam.killExecutors(Seq(\"1\"))).thenReturn(Seq(\"1\"))\n+\n+    cacheRecoveryManager.startCacheRecovery(Seq(\"1\"))\n+    Thread.sleep(1000)\n+    verify(eam).killExecutors(Seq(\"1\"))\n+\n+"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`DYN_ALLOCATION_CACHE_RECOVERY_TIMEOUT.key`",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-06T22:57:44Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.concurrent.{Future, Promise}\n+import scala.concurrent.ExecutionContext.Implicits.global\n+import scala.reflect.ClassTag\n+\n+import org.mockito.Mockito._\n+import org.scalatest.Matchers\n+import org.scalatest.mock.MockitoSugar\n+\n+import org.apache.spark.rpc._\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages._\n+\n+class CacheRecoveryManagerSuite extends SparkFunSuite with MockitoSugar with Matchers {\n+  val oneGB: Long = 1024L * 1024L * 1024L * 1024L\n+  val plentyOfMem = Map(BlockManagerId(\"1\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"2\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"3\", \"host\", 12, None) -> ((oneGB, oneGB)))\n+\n+  test(\"GracefulShutdown will take blocks until empty and then kill executor\") {\n+    val conf = new SparkConf()\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Seq(RDDBlockId(1, 1), RDDBlockId(2, 1))\n+    val bmme = FakeBMM(1, blocks.iterator, plentyOfMem)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)\n+\n+    when(eam.killExecutors(Seq(\"1\"))).thenReturn(Seq(\"1\"))\n+\n+    cacheRecoveryManager.startCacheRecovery(Seq(\"1\"))\n+    Thread.sleep(1000)\n+    verify(eam).killExecutors(Seq(\"1\"))\n+\n+\n+    bmme.replicated.get(\"1\").get shouldBe 2\n+  }\n+\n+  test(\"GracefulShutdown will kill executor if it takes too long to replicate\") {\n+    val conf = new SparkConf().set(\"spark.dynamicAllocation.cacheRecovery.timeout\", \"1s\")"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "fixed",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-13T18:35:16Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.concurrent.{Future, Promise}\n+import scala.concurrent.ExecutionContext.Implicits.global\n+import scala.reflect.ClassTag\n+\n+import org.mockito.Mockito._\n+import org.scalatest.Matchers\n+import org.scalatest.mock.MockitoSugar\n+\n+import org.apache.spark.rpc._\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages._\n+\n+class CacheRecoveryManagerSuite extends SparkFunSuite with MockitoSugar with Matchers {\n+  val oneGB: Long = 1024L * 1024L * 1024L * 1024L\n+  val plentyOfMem = Map(BlockManagerId(\"1\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"2\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"3\", \"host\", 12, None) -> ((oneGB, oneGB)))\n+\n+  test(\"GracefulShutdown will take blocks until empty and then kill executor\") {\n+    val conf = new SparkConf()\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Seq(RDDBlockId(1, 1), RDDBlockId(2, 1))\n+    val bmme = FakeBMM(1, blocks.iterator, plentyOfMem)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)\n+\n+    when(eam.killExecutors(Seq(\"1\"))).thenReturn(Seq(\"1\"))\n+\n+    cacheRecoveryManager.startCacheRecovery(Seq(\"1\"))\n+    Thread.sleep(1000)\n+    verify(eam).killExecutors(Seq(\"1\"))\n+\n+\n+    bmme.replicated.get(\"1\").get shouldBe 2\n+  }\n+\n+  test(\"GracefulShutdown will kill executor if it takes too long to replicate\") {\n+    val conf = new SparkConf().set(\"spark.dynamicAllocation.cacheRecovery.timeout\", \"1s\")"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Same comment.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-06T22:58:08Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.concurrent.{Future, Promise}\n+import scala.concurrent.ExecutionContext.Implicits.global\n+import scala.reflect.ClassTag\n+\n+import org.mockito.Mockito._\n+import org.scalatest.Matchers\n+import org.scalatest.mock.MockitoSugar\n+\n+import org.apache.spark.rpc._\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages._\n+\n+class CacheRecoveryManagerSuite extends SparkFunSuite with MockitoSugar with Matchers {\n+  val oneGB: Long = 1024L * 1024L * 1024L * 1024L\n+  val plentyOfMem = Map(BlockManagerId(\"1\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"2\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"3\", \"host\", 12, None) -> ((oneGB, oneGB)))\n+\n+  test(\"GracefulShutdown will take blocks until empty and then kill executor\") {\n+    val conf = new SparkConf()\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Seq(RDDBlockId(1, 1), RDDBlockId(2, 1))\n+    val bmme = FakeBMM(1, blocks.iterator, plentyOfMem)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)\n+\n+    when(eam.killExecutors(Seq(\"1\"))).thenReturn(Seq(\"1\"))\n+\n+    cacheRecoveryManager.startCacheRecovery(Seq(\"1\"))\n+    Thread.sleep(1000)\n+    verify(eam).killExecutors(Seq(\"1\"))\n+\n+\n+    bmme.replicated.get(\"1\").get shouldBe 2\n+  }\n+\n+  test(\"GracefulShutdown will kill executor if it takes too long to replicate\") {\n+    val conf = new SparkConf().set(\"spark.dynamicAllocation.cacheRecovery.timeout\", \"1s\")\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Set(RDDBlockId(1, 1), RDDBlockId(2, 1), RDDBlockId(3, 1), RDDBlockId(4, 1))\n+    val bmme = FakeBMM(600, blocks.iterator, plentyOfMem)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)\n+\n+    cacheRecoveryManager.startCacheRecovery(Seq(\"1\"))\n+    Thread.sleep(1010)"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "fixed",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-13T18:35:22Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.concurrent.{Future, Promise}\n+import scala.concurrent.ExecutionContext.Implicits.global\n+import scala.reflect.ClassTag\n+\n+import org.mockito.Mockito._\n+import org.scalatest.Matchers\n+import org.scalatest.mock.MockitoSugar\n+\n+import org.apache.spark.rpc._\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages._\n+\n+class CacheRecoveryManagerSuite extends SparkFunSuite with MockitoSugar with Matchers {\n+  val oneGB: Long = 1024L * 1024L * 1024L * 1024L\n+  val plentyOfMem = Map(BlockManagerId(\"1\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"2\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"3\", \"host\", 12, None) -> ((oneGB, oneGB)))\n+\n+  test(\"GracefulShutdown will take blocks until empty and then kill executor\") {\n+    val conf = new SparkConf()\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Seq(RDDBlockId(1, 1), RDDBlockId(2, 1))\n+    val bmme = FakeBMM(1, blocks.iterator, plentyOfMem)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)\n+\n+    when(eam.killExecutors(Seq(\"1\"))).thenReturn(Seq(\"1\"))\n+\n+    cacheRecoveryManager.startCacheRecovery(Seq(\"1\"))\n+    Thread.sleep(1000)\n+    verify(eam).killExecutors(Seq(\"1\"))\n+\n+\n+    bmme.replicated.get(\"1\").get shouldBe 2\n+  }\n+\n+  test(\"GracefulShutdown will kill executor if it takes too long to replicate\") {\n+    val conf = new SparkConf().set(\"spark.dynamicAllocation.cacheRecovery.timeout\", \"1s\")\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Set(RDDBlockId(1, 1), RDDBlockId(2, 1), RDDBlockId(3, 1), RDDBlockId(4, 1))\n+    val bmme = FakeBMM(600, blocks.iterator, plentyOfMem)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)\n+\n+    cacheRecoveryManager.startCacheRecovery(Seq(\"1\"))\n+    Thread.sleep(1010)"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Same.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-06T22:58:54Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.concurrent.{Future, Promise}\n+import scala.concurrent.ExecutionContext.Implicits.global\n+import scala.reflect.ClassTag\n+\n+import org.mockito.Mockito._\n+import org.scalatest.Matchers\n+import org.scalatest.mock.MockitoSugar\n+\n+import org.apache.spark.rpc._\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages._\n+\n+class CacheRecoveryManagerSuite extends SparkFunSuite with MockitoSugar with Matchers {\n+  val oneGB: Long = 1024L * 1024L * 1024L * 1024L\n+  val plentyOfMem = Map(BlockManagerId(\"1\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"2\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"3\", \"host\", 12, None) -> ((oneGB, oneGB)))\n+\n+  test(\"GracefulShutdown will take blocks until empty and then kill executor\") {\n+    val conf = new SparkConf()\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Seq(RDDBlockId(1, 1), RDDBlockId(2, 1))\n+    val bmme = FakeBMM(1, blocks.iterator, plentyOfMem)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)\n+\n+    when(eam.killExecutors(Seq(\"1\"))).thenReturn(Seq(\"1\"))\n+\n+    cacheRecoveryManager.startCacheRecovery(Seq(\"1\"))\n+    Thread.sleep(1000)\n+    verify(eam).killExecutors(Seq(\"1\"))\n+\n+\n+    bmme.replicated.get(\"1\").get shouldBe 2\n+  }\n+\n+  test(\"GracefulShutdown will kill executor if it takes too long to replicate\") {\n+    val conf = new SparkConf().set(\"spark.dynamicAllocation.cacheRecovery.timeout\", \"1s\")\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Set(RDDBlockId(1, 1), RDDBlockId(2, 1), RDDBlockId(3, 1), RDDBlockId(4, 1))\n+    val bmme = FakeBMM(600, blocks.iterator, plentyOfMem)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)\n+\n+    cacheRecoveryManager.startCacheRecovery(Seq(\"1\"))\n+    Thread.sleep(1010)\n+    verify(eam, times(1)).killExecutors(Seq(\"1\"))\n+    bmme.replicated.get(\"1\").get shouldBe 1\n+  }\n+\n+  test(\"shutdown timer will get cancelled if replication finishes\") {\n+    val conf = new SparkConf().set(\"spark.dynamicAllocation.cacheRecovery.timeout\", \"1s\")"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "fixed",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-13T18:35:32Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.concurrent.{Future, Promise}\n+import scala.concurrent.ExecutionContext.Implicits.global\n+import scala.reflect.ClassTag\n+\n+import org.mockito.Mockito._\n+import org.scalatest.Matchers\n+import org.scalatest.mock.MockitoSugar\n+\n+import org.apache.spark.rpc._\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages._\n+\n+class CacheRecoveryManagerSuite extends SparkFunSuite with MockitoSugar with Matchers {\n+  val oneGB: Long = 1024L * 1024L * 1024L * 1024L\n+  val plentyOfMem = Map(BlockManagerId(\"1\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"2\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"3\", \"host\", 12, None) -> ((oneGB, oneGB)))\n+\n+  test(\"GracefulShutdown will take blocks until empty and then kill executor\") {\n+    val conf = new SparkConf()\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Seq(RDDBlockId(1, 1), RDDBlockId(2, 1))\n+    val bmme = FakeBMM(1, blocks.iterator, plentyOfMem)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)\n+\n+    when(eam.killExecutors(Seq(\"1\"))).thenReturn(Seq(\"1\"))\n+\n+    cacheRecoveryManager.startCacheRecovery(Seq(\"1\"))\n+    Thread.sleep(1000)\n+    verify(eam).killExecutors(Seq(\"1\"))\n+\n+\n+    bmme.replicated.get(\"1\").get shouldBe 2\n+  }\n+\n+  test(\"GracefulShutdown will kill executor if it takes too long to replicate\") {\n+    val conf = new SparkConf().set(\"spark.dynamicAllocation.cacheRecovery.timeout\", \"1s\")\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Set(RDDBlockId(1, 1), RDDBlockId(2, 1), RDDBlockId(3, 1), RDDBlockId(4, 1))\n+    val bmme = FakeBMM(600, blocks.iterator, plentyOfMem)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)\n+\n+    cacheRecoveryManager.startCacheRecovery(Seq(\"1\"))\n+    Thread.sleep(1010)\n+    verify(eam, times(1)).killExecutors(Seq(\"1\"))\n+    bmme.replicated.get(\"1\").get shouldBe 1\n+  }\n+\n+  test(\"shutdown timer will get cancelled if replication finishes\") {\n+    val conf = new SparkConf().set(\"spark.dynamicAllocation.cacheRecovery.timeout\", \"1s\")"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Same comment.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-06T22:59:37Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.concurrent.{Future, Promise}\n+import scala.concurrent.ExecutionContext.Implicits.global\n+import scala.reflect.ClassTag\n+\n+import org.mockito.Mockito._\n+import org.scalatest.Matchers\n+import org.scalatest.mock.MockitoSugar\n+\n+import org.apache.spark.rpc._\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages._\n+\n+class CacheRecoveryManagerSuite extends SparkFunSuite with MockitoSugar with Matchers {\n+  val oneGB: Long = 1024L * 1024L * 1024L * 1024L\n+  val plentyOfMem = Map(BlockManagerId(\"1\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"2\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"3\", \"host\", 12, None) -> ((oneGB, oneGB)))\n+\n+  test(\"GracefulShutdown will take blocks until empty and then kill executor\") {\n+    val conf = new SparkConf()\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Seq(RDDBlockId(1, 1), RDDBlockId(2, 1))\n+    val bmme = FakeBMM(1, blocks.iterator, plentyOfMem)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)\n+\n+    when(eam.killExecutors(Seq(\"1\"))).thenReturn(Seq(\"1\"))\n+\n+    cacheRecoveryManager.startCacheRecovery(Seq(\"1\"))\n+    Thread.sleep(1000)\n+    verify(eam).killExecutors(Seq(\"1\"))\n+\n+\n+    bmme.replicated.get(\"1\").get shouldBe 2\n+  }\n+\n+  test(\"GracefulShutdown will kill executor if it takes too long to replicate\") {\n+    val conf = new SparkConf().set(\"spark.dynamicAllocation.cacheRecovery.timeout\", \"1s\")\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Set(RDDBlockId(1, 1), RDDBlockId(2, 1), RDDBlockId(3, 1), RDDBlockId(4, 1))\n+    val bmme = FakeBMM(600, blocks.iterator, plentyOfMem)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)\n+\n+    cacheRecoveryManager.startCacheRecovery(Seq(\"1\"))\n+    Thread.sleep(1010)\n+    verify(eam, times(1)).killExecutors(Seq(\"1\"))\n+    bmme.replicated.get(\"1\").get shouldBe 1\n+  }\n+\n+  test(\"shutdown timer will get cancelled if replication finishes\") {\n+    val conf = new SparkConf().set(\"spark.dynamicAllocation.cacheRecovery.timeout\", \"1s\")\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Set(RDDBlockId(1, 1))\n+    val bmme = FakeBMM(1, blocks.iterator, plentyOfMem)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)\n+\n+    cacheRecoveryManager.startCacheRecovery(Seq(\"1\"))\n+    Thread.sleep(1100)\n+    // should be killed once not twice\n+    verify(eam, times(1)).killExecutors(Seq(\"1\"))\n+  }\n+\n+\n+  test(\"Blocks won't replicate if we are running out of space\") {\n+    val conf = new SparkConf()\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Seq(RDDBlockId(1, 1), RDDBlockId(1, 1), RDDBlockId(1, 1), RDDBlockId(1, 1))\n+    val memStatus = Map(BlockManagerId(\"1\", \"host\", 12, None) -> ((2L, 1L)),\n+      BlockManagerId(\"2\", \"host\", 12, None) -> ((3L, 1L)),\n+      BlockManagerId(\"3\", \"host\", 12, None) -> ((4L, 1L)),\n+      BlockManagerId(\"4\", \"host\", 12, None) -> ((4L, 4L)))\n+    val bmme = FakeBMM(1, blocks.iterator, memStatus)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)\n+\n+    cacheRecoveryManager.startCacheRecovery(Seq(\"1\", \"2\", \"3\"))\n+    Thread.sleep(100)"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "fixed",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-13T18:35:39Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.concurrent.{Future, Promise}\n+import scala.concurrent.ExecutionContext.Implicits.global\n+import scala.reflect.ClassTag\n+\n+import org.mockito.Mockito._\n+import org.scalatest.Matchers\n+import org.scalatest.mock.MockitoSugar\n+\n+import org.apache.spark.rpc._\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages._\n+\n+class CacheRecoveryManagerSuite extends SparkFunSuite with MockitoSugar with Matchers {\n+  val oneGB: Long = 1024L * 1024L * 1024L * 1024L\n+  val plentyOfMem = Map(BlockManagerId(\"1\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"2\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"3\", \"host\", 12, None) -> ((oneGB, oneGB)))\n+\n+  test(\"GracefulShutdown will take blocks until empty and then kill executor\") {\n+    val conf = new SparkConf()\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Seq(RDDBlockId(1, 1), RDDBlockId(2, 1))\n+    val bmme = FakeBMM(1, blocks.iterator, plentyOfMem)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)\n+\n+    when(eam.killExecutors(Seq(\"1\"))).thenReturn(Seq(\"1\"))\n+\n+    cacheRecoveryManager.startCacheRecovery(Seq(\"1\"))\n+    Thread.sleep(1000)\n+    verify(eam).killExecutors(Seq(\"1\"))\n+\n+\n+    bmme.replicated.get(\"1\").get shouldBe 2\n+  }\n+\n+  test(\"GracefulShutdown will kill executor if it takes too long to replicate\") {\n+    val conf = new SparkConf().set(\"spark.dynamicAllocation.cacheRecovery.timeout\", \"1s\")\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Set(RDDBlockId(1, 1), RDDBlockId(2, 1), RDDBlockId(3, 1), RDDBlockId(4, 1))\n+    val bmme = FakeBMM(600, blocks.iterator, plentyOfMem)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)\n+\n+    cacheRecoveryManager.startCacheRecovery(Seq(\"1\"))\n+    Thread.sleep(1010)\n+    verify(eam, times(1)).killExecutors(Seq(\"1\"))\n+    bmme.replicated.get(\"1\").get shouldBe 1\n+  }\n+\n+  test(\"shutdown timer will get cancelled if replication finishes\") {\n+    val conf = new SparkConf().set(\"spark.dynamicAllocation.cacheRecovery.timeout\", \"1s\")\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Set(RDDBlockId(1, 1))\n+    val bmme = FakeBMM(1, blocks.iterator, plentyOfMem)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)\n+\n+    cacheRecoveryManager.startCacheRecovery(Seq(\"1\"))\n+    Thread.sleep(1100)\n+    // should be killed once not twice\n+    verify(eam, times(1)).killExecutors(Seq(\"1\"))\n+  }\n+\n+\n+  test(\"Blocks won't replicate if we are running out of space\") {\n+    val conf = new SparkConf()\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Seq(RDDBlockId(1, 1), RDDBlockId(1, 1), RDDBlockId(1, 1), RDDBlockId(1, 1))\n+    val memStatus = Map(BlockManagerId(\"1\", \"host\", 12, None) -> ((2L, 1L)),\n+      BlockManagerId(\"2\", \"host\", 12, None) -> ((3L, 1L)),\n+      BlockManagerId(\"3\", \"host\", 12, None) -> ((4L, 1L)),\n+      BlockManagerId(\"4\", \"host\", 12, None) -> ((4L, 4L)))\n+    val bmme = FakeBMM(1, blocks.iterator, memStatus)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)\n+\n+    cacheRecoveryManager.startCacheRecovery(Seq(\"1\", \"2\", \"3\"))\n+    Thread.sleep(100)"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Indentation.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-06T22:59:49Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.concurrent.{Future, Promise}\n+import scala.concurrent.ExecutionContext.Implicits.global\n+import scala.reflect.ClassTag\n+\n+import org.mockito.Mockito._\n+import org.scalatest.Matchers\n+import org.scalatest.mock.MockitoSugar\n+\n+import org.apache.spark.rpc._\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages._\n+\n+class CacheRecoveryManagerSuite extends SparkFunSuite with MockitoSugar with Matchers {\n+  val oneGB: Long = 1024L * 1024L * 1024L * 1024L\n+  val plentyOfMem = Map(BlockManagerId(\"1\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"2\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"3\", \"host\", 12, None) -> ((oneGB, oneGB)))\n+\n+  test(\"GracefulShutdown will take blocks until empty and then kill executor\") {\n+    val conf = new SparkConf()\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Seq(RDDBlockId(1, 1), RDDBlockId(2, 1))\n+    val bmme = FakeBMM(1, blocks.iterator, plentyOfMem)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)\n+\n+    when(eam.killExecutors(Seq(\"1\"))).thenReturn(Seq(\"1\"))\n+\n+    cacheRecoveryManager.startCacheRecovery(Seq(\"1\"))\n+    Thread.sleep(1000)\n+    verify(eam).killExecutors(Seq(\"1\"))\n+\n+\n+    bmme.replicated.get(\"1\").get shouldBe 2\n+  }\n+\n+  test(\"GracefulShutdown will kill executor if it takes too long to replicate\") {\n+    val conf = new SparkConf().set(\"spark.dynamicAllocation.cacheRecovery.timeout\", \"1s\")\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Set(RDDBlockId(1, 1), RDDBlockId(2, 1), RDDBlockId(3, 1), RDDBlockId(4, 1))\n+    val bmme = FakeBMM(600, blocks.iterator, plentyOfMem)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)\n+\n+    cacheRecoveryManager.startCacheRecovery(Seq(\"1\"))\n+    Thread.sleep(1010)\n+    verify(eam, times(1)).killExecutors(Seq(\"1\"))\n+    bmme.replicated.get(\"1\").get shouldBe 1\n+  }\n+\n+  test(\"shutdown timer will get cancelled if replication finishes\") {\n+    val conf = new SparkConf().set(\"spark.dynamicAllocation.cacheRecovery.timeout\", \"1s\")\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Set(RDDBlockId(1, 1))\n+    val bmme = FakeBMM(1, blocks.iterator, plentyOfMem)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)\n+\n+    cacheRecoveryManager.startCacheRecovery(Seq(\"1\"))\n+    Thread.sleep(1100)\n+    // should be killed once not twice\n+    verify(eam, times(1)).killExecutors(Seq(\"1\"))\n+  }\n+\n+\n+  test(\"Blocks won't replicate if we are running out of space\") {\n+    val conf = new SparkConf()\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Seq(RDDBlockId(1, 1), RDDBlockId(1, 1), RDDBlockId(1, 1), RDDBlockId(1, 1))\n+    val memStatus = Map(BlockManagerId(\"1\", \"host\", 12, None) -> ((2L, 1L)),\n+      BlockManagerId(\"2\", \"host\", 12, None) -> ((3L, 1L)),\n+      BlockManagerId(\"3\", \"host\", 12, None) -> ((4L, 1L)),\n+      BlockManagerId(\"4\", \"host\", 12, None) -> ((4L, 4L)))\n+    val bmme = FakeBMM(1, blocks.iterator, memStatus)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)\n+\n+    cacheRecoveryManager.startCacheRecovery(Seq(\"1\", \"2\", \"3\"))\n+    Thread.sleep(100)\n+    bmme.replicated.size shouldBe  2\n+  }\n+\n+  test(\"Blocks won't replicate if we are stopping all executors\") {\n+    val conf = new SparkConf()\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Seq(RDDBlockId(1, 1), RDDBlockId(1, 1), RDDBlockId(1, 1), RDDBlockId(1, 1))\n+    val memStatus = Map(BlockManagerId(\"1\", \"host\", 12, None) -> ((2L, 1L)),\n+                         BlockManagerId(\"2\", \"host\", 12, None) -> ((2L, 1L)),"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "fixed",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-13T18:35:47Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.concurrent.{Future, Promise}\n+import scala.concurrent.ExecutionContext.Implicits.global\n+import scala.reflect.ClassTag\n+\n+import org.mockito.Mockito._\n+import org.scalatest.Matchers\n+import org.scalatest.mock.MockitoSugar\n+\n+import org.apache.spark.rpc._\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages._\n+\n+class CacheRecoveryManagerSuite extends SparkFunSuite with MockitoSugar with Matchers {\n+  val oneGB: Long = 1024L * 1024L * 1024L * 1024L\n+  val plentyOfMem = Map(BlockManagerId(\"1\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"2\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"3\", \"host\", 12, None) -> ((oneGB, oneGB)))\n+\n+  test(\"GracefulShutdown will take blocks until empty and then kill executor\") {\n+    val conf = new SparkConf()\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Seq(RDDBlockId(1, 1), RDDBlockId(2, 1))\n+    val bmme = FakeBMM(1, blocks.iterator, plentyOfMem)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)\n+\n+    when(eam.killExecutors(Seq(\"1\"))).thenReturn(Seq(\"1\"))\n+\n+    cacheRecoveryManager.startCacheRecovery(Seq(\"1\"))\n+    Thread.sleep(1000)\n+    verify(eam).killExecutors(Seq(\"1\"))\n+\n+\n+    bmme.replicated.get(\"1\").get shouldBe 2\n+  }\n+\n+  test(\"GracefulShutdown will kill executor if it takes too long to replicate\") {\n+    val conf = new SparkConf().set(\"spark.dynamicAllocation.cacheRecovery.timeout\", \"1s\")\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Set(RDDBlockId(1, 1), RDDBlockId(2, 1), RDDBlockId(3, 1), RDDBlockId(4, 1))\n+    val bmme = FakeBMM(600, blocks.iterator, plentyOfMem)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)\n+\n+    cacheRecoveryManager.startCacheRecovery(Seq(\"1\"))\n+    Thread.sleep(1010)\n+    verify(eam, times(1)).killExecutors(Seq(\"1\"))\n+    bmme.replicated.get(\"1\").get shouldBe 1\n+  }\n+\n+  test(\"shutdown timer will get cancelled if replication finishes\") {\n+    val conf = new SparkConf().set(\"spark.dynamicAllocation.cacheRecovery.timeout\", \"1s\")\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Set(RDDBlockId(1, 1))\n+    val bmme = FakeBMM(1, blocks.iterator, plentyOfMem)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)\n+\n+    cacheRecoveryManager.startCacheRecovery(Seq(\"1\"))\n+    Thread.sleep(1100)\n+    // should be killed once not twice\n+    verify(eam, times(1)).killExecutors(Seq(\"1\"))\n+  }\n+\n+\n+  test(\"Blocks won't replicate if we are running out of space\") {\n+    val conf = new SparkConf()\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Seq(RDDBlockId(1, 1), RDDBlockId(1, 1), RDDBlockId(1, 1), RDDBlockId(1, 1))\n+    val memStatus = Map(BlockManagerId(\"1\", \"host\", 12, None) -> ((2L, 1L)),\n+      BlockManagerId(\"2\", \"host\", 12, None) -> ((3L, 1L)),\n+      BlockManagerId(\"3\", \"host\", 12, None) -> ((4L, 1L)),\n+      BlockManagerId(\"4\", \"host\", 12, None) -> ((4L, 4L)))\n+    val bmme = FakeBMM(1, blocks.iterator, memStatus)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)\n+\n+    cacheRecoveryManager.startCacheRecovery(Seq(\"1\", \"2\", \"3\"))\n+    Thread.sleep(100)\n+    bmme.replicated.size shouldBe  2\n+  }\n+\n+  test(\"Blocks won't replicate if we are stopping all executors\") {\n+    val conf = new SparkConf()\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Seq(RDDBlockId(1, 1), RDDBlockId(1, 1), RDDBlockId(1, 1), RDDBlockId(1, 1))\n+    val memStatus = Map(BlockManagerId(\"1\", \"host\", 12, None) -> ((2L, 1L)),\n+                         BlockManagerId(\"2\", \"host\", 12, None) -> ((2L, 1L)),"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Same.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-06T23:00:03Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.concurrent.{Future, Promise}\n+import scala.concurrent.ExecutionContext.Implicits.global\n+import scala.reflect.ClassTag\n+\n+import org.mockito.Mockito._\n+import org.scalatest.Matchers\n+import org.scalatest.mock.MockitoSugar\n+\n+import org.apache.spark.rpc._\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages._\n+\n+class CacheRecoveryManagerSuite extends SparkFunSuite with MockitoSugar with Matchers {\n+  val oneGB: Long = 1024L * 1024L * 1024L * 1024L\n+  val plentyOfMem = Map(BlockManagerId(\"1\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"2\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"3\", \"host\", 12, None) -> ((oneGB, oneGB)))\n+\n+  test(\"GracefulShutdown will take blocks until empty and then kill executor\") {\n+    val conf = new SparkConf()\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Seq(RDDBlockId(1, 1), RDDBlockId(2, 1))\n+    val bmme = FakeBMM(1, blocks.iterator, plentyOfMem)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)\n+\n+    when(eam.killExecutors(Seq(\"1\"))).thenReturn(Seq(\"1\"))\n+\n+    cacheRecoveryManager.startCacheRecovery(Seq(\"1\"))\n+    Thread.sleep(1000)\n+    verify(eam).killExecutors(Seq(\"1\"))\n+\n+\n+    bmme.replicated.get(\"1\").get shouldBe 2\n+  }\n+\n+  test(\"GracefulShutdown will kill executor if it takes too long to replicate\") {\n+    val conf = new SparkConf().set(\"spark.dynamicAllocation.cacheRecovery.timeout\", \"1s\")\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Set(RDDBlockId(1, 1), RDDBlockId(2, 1), RDDBlockId(3, 1), RDDBlockId(4, 1))\n+    val bmme = FakeBMM(600, blocks.iterator, plentyOfMem)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)\n+\n+    cacheRecoveryManager.startCacheRecovery(Seq(\"1\"))\n+    Thread.sleep(1010)\n+    verify(eam, times(1)).killExecutors(Seq(\"1\"))\n+    bmme.replicated.get(\"1\").get shouldBe 1\n+  }\n+\n+  test(\"shutdown timer will get cancelled if replication finishes\") {\n+    val conf = new SparkConf().set(\"spark.dynamicAllocation.cacheRecovery.timeout\", \"1s\")\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Set(RDDBlockId(1, 1))\n+    val bmme = FakeBMM(1, blocks.iterator, plentyOfMem)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)\n+\n+    cacheRecoveryManager.startCacheRecovery(Seq(\"1\"))\n+    Thread.sleep(1100)\n+    // should be killed once not twice\n+    verify(eam, times(1)).killExecutors(Seq(\"1\"))\n+  }\n+\n+\n+  test(\"Blocks won't replicate if we are running out of space\") {\n+    val conf = new SparkConf()\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Seq(RDDBlockId(1, 1), RDDBlockId(1, 1), RDDBlockId(1, 1), RDDBlockId(1, 1))\n+    val memStatus = Map(BlockManagerId(\"1\", \"host\", 12, None) -> ((2L, 1L)),\n+      BlockManagerId(\"2\", \"host\", 12, None) -> ((3L, 1L)),\n+      BlockManagerId(\"3\", \"host\", 12, None) -> ((4L, 1L)),\n+      BlockManagerId(\"4\", \"host\", 12, None) -> ((4L, 4L)))\n+    val bmme = FakeBMM(1, blocks.iterator, memStatus)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)\n+\n+    cacheRecoveryManager.startCacheRecovery(Seq(\"1\", \"2\", \"3\"))\n+    Thread.sleep(100)\n+    bmme.replicated.size shouldBe  2\n+  }\n+\n+  test(\"Blocks won't replicate if we are stopping all executors\") {\n+    val conf = new SparkConf()\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Seq(RDDBlockId(1, 1), RDDBlockId(1, 1), RDDBlockId(1, 1), RDDBlockId(1, 1))\n+    val memStatus = Map(BlockManagerId(\"1\", \"host\", 12, None) -> ((2L, 1L)),\n+                         BlockManagerId(\"2\", \"host\", 12, None) -> ((2L, 1L)),\n+                         BlockManagerId(\"3\", \"host\", 12, None) -> ((2L, 1L)),\n+                         BlockManagerId(\"4\", \"host\", 12, None) -> ((2L, 1L)))\n+    val bmme = FakeBMM(1, blocks.iterator, memStatus)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)\n+\n+    cacheRecoveryManager.startCacheRecovery(Seq(\"1\", \"2\", \"3\", \"4\"))\n+    Thread.sleep(100)"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "fixed",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-13T18:36:40Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import java.util.concurrent.ConcurrentHashMap\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.concurrent.{Future, Promise}\n+import scala.concurrent.ExecutionContext.Implicits.global\n+import scala.reflect.ClassTag\n+\n+import org.mockito.Mockito._\n+import org.scalatest.Matchers\n+import org.scalatest.mock.MockitoSugar\n+\n+import org.apache.spark.rpc._\n+import org.apache.spark.storage.{BlockId, BlockManagerId, RDDBlockId}\n+import org.apache.spark.storage.BlockManagerMessages._\n+\n+class CacheRecoveryManagerSuite extends SparkFunSuite with MockitoSugar with Matchers {\n+  val oneGB: Long = 1024L * 1024L * 1024L * 1024L\n+  val plentyOfMem = Map(BlockManagerId(\"1\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"2\", \"host\", 12, None) -> ((oneGB, oneGB)),\n+                        BlockManagerId(\"3\", \"host\", 12, None) -> ((oneGB, oneGB)))\n+\n+  test(\"GracefulShutdown will take blocks until empty and then kill executor\") {\n+    val conf = new SparkConf()\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Seq(RDDBlockId(1, 1), RDDBlockId(2, 1))\n+    val bmme = FakeBMM(1, blocks.iterator, plentyOfMem)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)\n+\n+    when(eam.killExecutors(Seq(\"1\"))).thenReturn(Seq(\"1\"))\n+\n+    cacheRecoveryManager.startCacheRecovery(Seq(\"1\"))\n+    Thread.sleep(1000)\n+    verify(eam).killExecutors(Seq(\"1\"))\n+\n+\n+    bmme.replicated.get(\"1\").get shouldBe 2\n+  }\n+\n+  test(\"GracefulShutdown will kill executor if it takes too long to replicate\") {\n+    val conf = new SparkConf().set(\"spark.dynamicAllocation.cacheRecovery.timeout\", \"1s\")\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Set(RDDBlockId(1, 1), RDDBlockId(2, 1), RDDBlockId(3, 1), RDDBlockId(4, 1))\n+    val bmme = FakeBMM(600, blocks.iterator, plentyOfMem)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)\n+\n+    cacheRecoveryManager.startCacheRecovery(Seq(\"1\"))\n+    Thread.sleep(1010)\n+    verify(eam, times(1)).killExecutors(Seq(\"1\"))\n+    bmme.replicated.get(\"1\").get shouldBe 1\n+  }\n+\n+  test(\"shutdown timer will get cancelled if replication finishes\") {\n+    val conf = new SparkConf().set(\"spark.dynamicAllocation.cacheRecovery.timeout\", \"1s\")\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Set(RDDBlockId(1, 1))\n+    val bmme = FakeBMM(1, blocks.iterator, plentyOfMem)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)\n+\n+    cacheRecoveryManager.startCacheRecovery(Seq(\"1\"))\n+    Thread.sleep(1100)\n+    // should be killed once not twice\n+    verify(eam, times(1)).killExecutors(Seq(\"1\"))\n+  }\n+\n+\n+  test(\"Blocks won't replicate if we are running out of space\") {\n+    val conf = new SparkConf()\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Seq(RDDBlockId(1, 1), RDDBlockId(1, 1), RDDBlockId(1, 1), RDDBlockId(1, 1))\n+    val memStatus = Map(BlockManagerId(\"1\", \"host\", 12, None) -> ((2L, 1L)),\n+      BlockManagerId(\"2\", \"host\", 12, None) -> ((3L, 1L)),\n+      BlockManagerId(\"3\", \"host\", 12, None) -> ((4L, 1L)),\n+      BlockManagerId(\"4\", \"host\", 12, None) -> ((4L, 4L)))\n+    val bmme = FakeBMM(1, blocks.iterator, memStatus)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)\n+\n+    cacheRecoveryManager.startCacheRecovery(Seq(\"1\", \"2\", \"3\"))\n+    Thread.sleep(100)\n+    bmme.replicated.size shouldBe  2\n+  }\n+\n+  test(\"Blocks won't replicate if we are stopping all executors\") {\n+    val conf = new SparkConf()\n+    val eam = mock[ExecutorAllocationManager]\n+    val blocks = Seq(RDDBlockId(1, 1), RDDBlockId(1, 1), RDDBlockId(1, 1), RDDBlockId(1, 1))\n+    val memStatus = Map(BlockManagerId(\"1\", \"host\", 12, None) -> ((2L, 1L)),\n+                         BlockManagerId(\"2\", \"host\", 12, None) -> ((2L, 1L)),\n+                         BlockManagerId(\"3\", \"host\", 12, None) -> ((2L, 1L)),\n+                         BlockManagerId(\"4\", \"host\", 12, None) -> ((2L, 1L)))\n+    val bmme = FakeBMM(1, blocks.iterator, memStatus)\n+    val bmmeRef = DummyRef(bmme)\n+    val cacheRecoveryManager = new CacheRecoveryManager(bmmeRef, eam, conf)\n+\n+    cacheRecoveryManager.startCacheRecovery(Seq(\"1\", \"2\", \"3\", \"4\"))\n+    Thread.sleep(100)"
  }],
  "prId": 19041
}]