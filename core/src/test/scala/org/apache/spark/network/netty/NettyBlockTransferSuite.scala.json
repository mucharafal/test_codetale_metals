[{
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "this test does pass on my machine, when I run it with a 16 gb heap, but didn't want to make it run on jenkins for now\n",
    "commit": "cd84c6996c5fd48f2f9f2855e7f3e7a75ba5865a",
    "createdAt": "2015-03-02T22:22:58Z",
    "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.network.netty\n+\n+import java.nio.ByteBuffer\n+import java.util.concurrent.TimeUnit\n+\n+import org.apache.commons.io.IOUtils\n+import org.apache.spark.network.BlockDataManager\n+import org.apache.spark.network.buffer._\n+import org.apache.spark.network.shuffle.BlockFetchingListener\n+import org.apache.spark.storage.{BlockId, StorageLevel, RDDBlockId, ShuffleBlockId}\n+import org.apache.spark.{Logging, SecurityManager, SparkConf}\n+import org.mockito.ArgumentCaptor\n+import org.mockito.{Matchers => MockitoMatchers}\n+import org.mockito.Mockito._\n+import org.scalatest.mock.MockitoSugar\n+import org.scalatest.{Matchers, FunSuite}\n+\n+import scala.concurrent.duration.{Duration, FiniteDuration}\n+import scala.concurrent.{Await, Promise}\n+\n+class NettyBlockTransferSuite extends FunSuite with Matchers with MockitoSugar with Logging {\n+\n+  val conf = new SparkConf()\n+    .set(\"spark.app.id\", \"app-id\")\n+  val securityManager = new SecurityManager(conf)\n+\n+\n+  def fetchBlock(buf: LargeByteBuffer): ManagedBuffer = {\n+    val blockManager = mock[BlockDataManager]\n+    val blockId = ShuffleBlockId(0, 1, 2)\n+    val blockBuffer = new NioManagedBuffer(buf)\n+    when(blockManager.getBlockData(blockId)).thenReturn(blockBuffer)\n+\n+    val from = new NettyBlockTransferService(conf, securityManager, numCores = 1)\n+    from.init(blockManager)\n+    val to = new NettyBlockTransferService(conf, securityManager, numCores = 1)\n+    to.init(blockManager)\n+\n+    try {\n+      val promise = Promise[ManagedBuffer]()\n+\n+      to.fetchBlocks(from.hostName, from.port, \"1\", Array(blockId.toString),\n+        new BlockFetchingListener {\n+          override def onBlockFetchFailure(blockId: String, exception: Throwable): Unit = {\n+            promise.failure(exception)\n+          }\n+\n+          override def onBlockFetchSuccess(blockId: String, data: ManagedBuffer): Unit = {\n+            promise.success(data.retain())\n+          }\n+        })\n+\n+      Await.ready(promise.future, FiniteDuration(100, TimeUnit.SECONDS))\n+      promise.future.value.get.get\n+    } finally {\n+      from.close()\n+      to.close()\n+    }\n+\n+  }\n+\n+  ignore(\"simple fetch\") {\n+    val blockString = \"Hello, world!\"\n+    val blockBuffer = LargeByteBufferHelper.asLargeByteBuffer(blockString.getBytes)\n+    val fetched = fetchBlock(blockBuffer)\n+\n+    IOUtils.toString(fetched.createInputStream()) should equal(blockString)\n+  }\n+\n+\n+  def uploadBlock(buf: LargeByteBuffer, rddId: Int, timeout: Long) {\n+\n+    val fromBlockManager = mock[BlockDataManager]\n+    val toBlockManager = mock[BlockDataManager]\n+    val blockId = RDDBlockId(rddId, rddId + 1)\n+    val blockBuffer = new NioManagedBuffer(buf)\n+    val level = StorageLevel.DISK_ONLY //doesn't matter\n+\n+    val from = new NettyBlockTransferService(conf, securityManager, numCores = 1)\n+    from.init(fromBlockManager)\n+    val to = new NettyBlockTransferService(conf, securityManager, numCores = 1)\n+    to.init(toBlockManager)\n+\n+    try {\n+      val uploadFuture = from.uploadBlock(to.hostName, to.port, \"exec-1\", blockId, blockBuffer, level)\n+      Await.result(uploadFuture, Duration.apply(timeout, TimeUnit.MILLISECONDS))\n+      val bufferCaptor = ArgumentCaptor.forClass(classOf[ManagedBuffer])\n+      verify(toBlockManager).putBlockData(MockitoMatchers.eq(blockId), bufferCaptor.capture(),\n+        MockitoMatchers.eq(level))\n+      val putBuffer = bufferCaptor.getValue()\n+      logTrace(\"begin checking buffer equivalence\")\n+      equivalentBuffers(blockBuffer, putBuffer)\n+      logTrace(\"finished checking buffer equivalence\")\n+    } finally {\n+      from.close()\n+      to.close()\n+    }\n+\n+  }\n+\n+  test(\"small one-part upload\") {\n+    val buf = LargeByteBufferHelper.asLargeByteBuffer(Array[Byte](0,1,2,3))\n+    uploadBlock(buf, 0, 100)\n+  }\n+\n+  test(\"small multi-part upload\") {\n+    val parts = (0 until 5).map{idx =>\n+      val arr = Array.tabulate[Byte](100){subIdx => (idx + subIdx).toByte}\n+      ByteBuffer.wrap(arr)\n+    }.toArray\n+    val buf = new WrappedLargeByteBuffer(parts)\n+    uploadBlock(buf, 1, 500)\n+  }\n+\n+  //just don't want to kill the test server\n+  ignore(\"giant upload\") {\n+    // pretty close to max size due to overhead from the rest of the msg\n+    val parts = (0 until 2).map{_ => ByteBuffer.allocate(Integer.MAX_VALUE - 200)}.toArray\n+    val buf = new WrappedLargeByteBuffer(parts)\n+    uploadBlock(buf, 2, 20 * 60 * 1000) // yup, takes this long ...",
    "line": 136
  }],
  "prId": 4857
}]