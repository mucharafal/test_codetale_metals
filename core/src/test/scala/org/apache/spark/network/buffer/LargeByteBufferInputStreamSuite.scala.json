[{
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "You could just say `len = 42L + Integer.MAX_VALUE`...\n",
    "commit": "3447bb995b53c4d93154328c7c7c06e08a5ec9b9",
    "createdAt": "2015-04-20T18:55:34Z",
    "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.network.buffer\n+\n+import java.io.{FileInputStream, FileOutputStream, OutputStream, File}\n+import java.nio.ByteBuffer\n+import java.nio.channels.FileChannel.MapMode\n+\n+import org.junit.Assert._\n+import org.scalatest.{FunSuite, Matchers}\n+\n+import org.apache.spark.network.buffer.{LargeByteBuffer, LargeByteBufferHelper, WrappedLargeByteBuffer}\n+\n+class LargeByteBufferInputStreamSuite extends FunSuite with Matchers {\n+\n+  test(\"read from large mapped file\") {\n+    val testFile = File.createTempFile(\"large-buffer-input-stream-test\",\".bin\")\n+    testFile.deleteOnExit()\n+\n+    val out: OutputStream = new FileOutputStream(testFile)\n+    val buffer: Array[Byte] = new Array[Byte](1 << 16)\n+    val len: Long = 3L << 30\n+    assertTrue(len > Integer.MAX_VALUE)"
  }],
  "prId": 5400
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "IIUC, this test is creating a humongous test file (> 2GB)... two things:\n- how long does it take for the test to run?\n- this file should probably be explicitly deleted in a `finally` block in this test (or using `BeforeAndAfter`), instead of relying on `deleteOnExit`. Just because I'd be wary of leaking 2GB files.\n",
    "commit": "3447bb995b53c4d93154328c7c7c06e08a5ec9b9",
    "createdAt": "2015-04-20T18:57:49Z",
    "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.network.buffer\n+\n+import java.io.{FileInputStream, FileOutputStream, OutputStream, File}\n+import java.nio.ByteBuffer\n+import java.nio.channels.FileChannel.MapMode\n+\n+import org.junit.Assert._\n+import org.scalatest.{FunSuite, Matchers}\n+\n+import org.apache.spark.network.buffer.{LargeByteBuffer, LargeByteBufferHelper, WrappedLargeByteBuffer}\n+\n+class LargeByteBufferInputStreamSuite extends FunSuite with Matchers {\n+\n+  test(\"read from large mapped file\") {\n+    val testFile = File.createTempFile(\"large-buffer-input-stream-test\",\".bin\")\n+    testFile.deleteOnExit()"
  }, {
    "author": {
      "login": "squito"
    },
    "body": "on my laptop, it takes 16 seconds to run.\n\nI'll move the delete into a `finally` block\n",
    "commit": "3447bb995b53c4d93154328c7c7c06e08a5ec9b9",
    "createdAt": "2015-05-29T18:58:45Z",
    "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.network.buffer\n+\n+import java.io.{FileInputStream, FileOutputStream, OutputStream, File}\n+import java.nio.ByteBuffer\n+import java.nio.channels.FileChannel.MapMode\n+\n+import org.junit.Assert._\n+import org.scalatest.{FunSuite, Matchers}\n+\n+import org.apache.spark.network.buffer.{LargeByteBuffer, LargeByteBufferHelper, WrappedLargeByteBuffer}\n+\n+class LargeByteBufferInputStreamSuite extends FunSuite with Matchers {\n+\n+  test(\"read from large mapped file\") {\n+    val testFile = File.createTempFile(\"large-buffer-input-stream-test\",\".bin\")\n+    testFile.deleteOnExit()"
  }],
  "prId": 5400
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: delete\n",
    "commit": "3447bb995b53c4d93154328c7c7c06e08a5ec9b9",
    "createdAt": "2015-04-20T19:00:17Z",
    "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.network.buffer\n+\n+import java.io.{FileInputStream, FileOutputStream, OutputStream, File}\n+import java.nio.ByteBuffer\n+import java.nio.channels.FileChannel.MapMode\n+\n+import org.junit.Assert._\n+import org.scalatest.{FunSuite, Matchers}\n+\n+import org.apache.spark.network.buffer.{LargeByteBuffer, LargeByteBufferHelper, WrappedLargeByteBuffer}\n+\n+class LargeByteBufferInputStreamSuite extends FunSuite with Matchers {\n+\n+  test(\"read from large mapped file\") {\n+    val testFile = File.createTempFile(\"large-buffer-input-stream-test\",\".bin\")\n+    testFile.deleteOnExit()\n+\n+    val out: OutputStream = new FileOutputStream(testFile)\n+    val buffer: Array[Byte] = new Array[Byte](1 << 16)\n+    val len: Long = 3L << 30\n+    assertTrue(len > Integer.MAX_VALUE)\n+    (0 until buffer.length).foreach { idx =>\n+      buffer(idx) = idx.toByte\n+    }\n+    (0 until (len / buffer.length).toInt).foreach { idx =>\n+      out.write(buffer)\n+    }\n+    out.close\n+\n+    val channel = new FileInputStream(testFile).getChannel\n+    val buf = LargeByteBufferHelper.mapFile(channel, MapMode.READ_ONLY, 0, len)\n+    val in = new LargeByteBufferInputStream(buf, dispose = true)\n+\n+    val read = new Array[Byte](buffer.length)\n+    (0 until (len / buffer.length).toInt).foreach { idx =>\n+      in.disposed should be (false)\n+      in.read(read) should be (read.length)\n+      (0 until buffer.length).foreach { arrIdx =>\n+        assertEquals(buffer(arrIdx), read(arrIdx))\n+      }\n+    }\n+    in.disposed should be (false)\n+    in.read(read) should be (-1)\n+    in.disposed should be (false)\n+    in.close()\n+    in.disposed should be (true)\n+  }\n+\n+  test(\"dispose on close\") {\n+    // don't need to read to the end -- dispose anytime we close\n+    val data = new Array[Byte](10)\n+    val in = new LargeByteBufferInputStream(LargeByteBufferHelper.asLargeByteBuffer(data),\n+      dispose = true)\n+    in.disposed should be (false)\n+    in.close()\n+    in.disposed should be (true)\n+  }\n+\n+  test(\"io stream roundtrip\") {\n+"
  }],
  "prId": 5400
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: `foreach { idx => ... }`\n",
    "commit": "3447bb995b53c4d93154328c7c7c06e08a5ec9b9",
    "createdAt": "2015-06-02T16:41:44Z",
    "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.network.buffer\n+\n+import java.io.{File, FileInputStream, FileOutputStream, OutputStream}\n+import java.nio.channels.FileChannel.MapMode\n+\n+import org.junit.Assert._\n+import org.scalatest.Matchers\n+\n+import org.apache.spark.SparkFunSuite\n+\n+class LargeByteBufferInputStreamSuite extends SparkFunSuite with Matchers {\n+\n+  test(\"read from large mapped file\") {\n+    val testFile = File.createTempFile(\"large-buffer-input-stream-test\", \".bin\")\n+\n+    try {\n+      val out: OutputStream = new FileOutputStream(testFile)\n+      val buffer: Array[Byte] = new Array[Byte](1 << 16)\n+      val len: Long = buffer.length.toLong + Integer.MAX_VALUE + 1\n+      (0 until buffer.length).foreach { idx =>\n+        buffer(idx) = idx.toByte\n+      }\n+      (0 until (len / buffer.length).toInt).foreach { idx =>\n+        out.write(buffer)\n+      }\n+      out.close\n+\n+      val channel = new FileInputStream(testFile).getChannel\n+      val buf = LargeByteBufferHelper.mapFile(channel, MapMode.READ_ONLY, 0, len)\n+      val in = new LargeByteBufferInputStream(buf, true)\n+\n+      val read = new Array[Byte](buffer.length)\n+      (0 until (len / buffer.length).toInt).foreach { idx =>\n+        in.disposed should be(false)\n+        in.read(read) should be(read.length)\n+        (0 until buffer.length).foreach { arrIdx =>\n+          assertEquals(buffer(arrIdx), read(arrIdx))\n+        }\n+      }\n+      in.disposed should be(false)\n+      in.read(read) should be(-1)\n+      in.disposed should be(false)\n+      in.close()\n+      in.disposed should be(true)\n+    } finally {\n+      testFile.delete()\n+    }\n+  }\n+\n+  test(\"dispose on close\") {\n+    // don't need to read to the end -- dispose anytime we close\n+    val data = new Array[Byte](10)\n+    val in = new LargeByteBufferInputStream(LargeByteBufferHelper.asLargeByteBuffer(data), true)\n+    in.disposed should be (false)\n+    in.close()\n+    in.disposed should be (true)\n+  }\n+\n+  test(\"io stream roundtrip\") {\n+    val out = new LargeByteBufferOutputStream(128)\n+    (0 until 200).foreach{idx => out.write(idx)}"
  }],
  "prId": 5400
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: `foreach { idx =>`\n",
    "commit": "3447bb995b53c4d93154328c7c7c06e08a5ec9b9",
    "createdAt": "2015-06-02T16:42:01Z",
    "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.network.buffer\n+\n+import java.io.{File, FileInputStream, FileOutputStream, OutputStream}\n+import java.nio.channels.FileChannel.MapMode\n+\n+import org.junit.Assert._\n+import org.scalatest.Matchers\n+\n+import org.apache.spark.SparkFunSuite\n+\n+class LargeByteBufferInputStreamSuite extends SparkFunSuite with Matchers {\n+\n+  test(\"read from large mapped file\") {\n+    val testFile = File.createTempFile(\"large-buffer-input-stream-test\", \".bin\")\n+\n+    try {\n+      val out: OutputStream = new FileOutputStream(testFile)\n+      val buffer: Array[Byte] = new Array[Byte](1 << 16)\n+      val len: Long = buffer.length.toLong + Integer.MAX_VALUE + 1\n+      (0 until buffer.length).foreach { idx =>\n+        buffer(idx) = idx.toByte\n+      }\n+      (0 until (len / buffer.length).toInt).foreach { idx =>\n+        out.write(buffer)\n+      }\n+      out.close\n+\n+      val channel = new FileInputStream(testFile).getChannel\n+      val buf = LargeByteBufferHelper.mapFile(channel, MapMode.READ_ONLY, 0, len)\n+      val in = new LargeByteBufferInputStream(buf, true)\n+\n+      val read = new Array[Byte](buffer.length)\n+      (0 until (len / buffer.length).toInt).foreach { idx =>\n+        in.disposed should be(false)\n+        in.read(read) should be(read.length)\n+        (0 until buffer.length).foreach { arrIdx =>\n+          assertEquals(buffer(arrIdx), read(arrIdx))\n+        }\n+      }\n+      in.disposed should be(false)\n+      in.read(read) should be(-1)\n+      in.disposed should be(false)\n+      in.close()\n+      in.disposed should be(true)\n+    } finally {\n+      testFile.delete()\n+    }\n+  }\n+\n+  test(\"dispose on close\") {\n+    // don't need to read to the end -- dispose anytime we close\n+    val data = new Array[Byte](10)\n+    val in = new LargeByteBufferInputStream(LargeByteBufferHelper.asLargeByteBuffer(data), true)\n+    in.disposed should be (false)\n+    in.close()\n+    in.disposed should be (true)\n+  }\n+\n+  test(\"io stream roundtrip\") {\n+    val out = new LargeByteBufferOutputStream(128)\n+    (0 until 200).foreach{idx => out.write(idx)}\n+    out.close()\n+\n+    val lb = out.largeBuffer(128)\n+    // just make sure that we test reading from multiple chunks\n+    lb.asInstanceOf[WrappedLargeByteBuffer].underlying.size should be > 1\n+\n+    val rawIn = new LargeByteBufferInputStream(lb)\n+    val arr = new Array[Byte](500)\n+    val nRead = rawIn.read(arr, 0, 500)\n+    nRead should be (200)\n+    (0 until 200).foreach{idx =>"
  }],
  "prId": 5400
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "You could avoid the `disposed` field if you used Mockito here (mock `LargeByteBuffer` and verify `buffer.dispose()` is called).\n",
    "commit": "3447bb995b53c4d93154328c7c7c06e08a5ec9b9",
    "createdAt": "2015-11-02T19:59:29Z",
    "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.network.buffer\n+\n+import java.io.{File, FileInputStream, FileOutputStream, OutputStream}\n+import java.nio.channels.FileChannel.MapMode\n+\n+import org.junit.Assert._\n+import org.scalatest.Matchers\n+\n+import org.apache.spark.SparkFunSuite\n+\n+class LargeByteBufferInputStreamSuite extends SparkFunSuite with Matchers {\n+\n+  test(\"read from large mapped file\") {\n+    val testFile = File.createTempFile(\"large-buffer-input-stream-test\", \".bin\")\n+\n+    try {\n+      val out: OutputStream = new FileOutputStream(testFile)\n+      val buffer: Array[Byte] = new Array[Byte](1 << 16)\n+      val len: Long = buffer.length.toLong + Integer.MAX_VALUE + 1\n+      (0 until buffer.length).foreach { idx =>\n+        buffer(idx) = idx.toByte\n+      }\n+      (0 until (len / buffer.length).toInt).foreach { idx =>\n+        out.write(buffer)\n+      }\n+      out.close\n+\n+      val channel = new FileInputStream(testFile).getChannel\n+      val buf = LargeByteBufferHelper.mapFile(channel, MapMode.READ_ONLY, 0, len)\n+      val in = new LargeByteBufferInputStream(buf, true)\n+\n+      val read = new Array[Byte](buffer.length)\n+      (0 until (len / buffer.length).toInt).foreach { idx =>\n+        in.disposed should be(false)\n+        in.read(read) should be(read.length)\n+        (0 until buffer.length).foreach { arrIdx =>\n+          assertEquals(buffer(arrIdx), read(arrIdx))\n+        }\n+      }\n+      in.disposed should be(false)\n+      in.read(read) should be(-1)\n+      in.disposed should be(false)\n+      in.close()\n+      in.disposed should be(true)\n+    } finally {\n+      testFile.delete()\n+    }\n+  }\n+\n+  test(\"dispose on close\") {\n+    // don't need to read to the end -- dispose anytime we close\n+    val data = new Array[Byte](10)\n+    val in = new LargeByteBufferInputStream(LargeByteBufferHelper.asLargeByteBuffer(data), true)\n+    in.disposed should be (false)"
  }],
  "prId": 5400
}]