[{
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "you can combine these conditions into one `if`:\r\n\r\n```scala\r\nif (!executorDir.exists() && !executorDir.mkdirs()) {\r\n```",
    "commit": "475d27817b6ef37eed9b7b39fee709427811f0d6",
    "createdAt": "2019-03-07T15:19:16Z",
    "diffHunk": "@@ -243,4 +244,59 @@ class WorkerSuite extends SparkFunSuite with Matchers with BeforeAndAfter {\n       ExecutorStateChanged(\"app1\", 0, ExecutorState.EXITED, None, None))\n     assert(cleanupCalled.get() == value)\n   }\n+\n+  test(\"removes our metadata of all executors registered for the given \" +\n+    \"application after WorkDirCleanup when config spark.shuffle.service.db.enabled=true\") {\n+    testWorkDirCleanupAndRemoveMetadataWithConfig(true)\n+  }\n+\n+  test(\"don't removes our metadata of all executors registered for the given \" +\n+    \"application after WorkDirCleanup when config spark.shuffle.service.db.enabled=false\") {\n+    testWorkDirCleanupAndRemoveMetadataWithConfig(false)\n+  }\n+\n+  private def testWorkDirCleanupAndRemoveMetadataWithConfig(value: Boolean) = {\n+    val conf = new SparkConf().set(\"spark.shuffle.service.db.enabled\", value.toString)\n+    conf.set(\"spark.worker.cleanup.appDataTtl\", 60.toString)\n+    conf.set(\"spark.shuffle.service.enabled\", \"true\")\n+\n+    val appId = \"app1\"\n+    val execId = \"exec1\"\n+    val cleanupCalled = new AtomicBoolean(false)\n+    when(shuffleService.applicationRemoved(any[String])).thenAnswer(new Answer[Unit] {\n+      override def answer(invocations: InvocationOnMock): Unit = {\n+        cleanupCalled.set(true)\n+      }\n+    })\n+    val externalShuffleServiceSupplier = new Supplier[ExternalShuffleService] {\n+      override def get: ExternalShuffleService = shuffleService\n+    }\n+    val worker = makeWorker(conf, externalShuffleServiceSupplier)\n+\n+    val workDir = Option(\"/tmp/work\").map(new File(_)).get\n+    try {\n+      workDir.mkdirs()\n+      if (!workDir.exists() || !workDir.isDirectory) {\n+        logError(\"Failed to create work directory \" + workDir)\n+      }\n+      assert(workDir.isDirectory)\n+    } catch {\n+      case e: Exception =>\n+        logError(\"Failed to create work directory \" + workDir, e)\n+    }\n+    // initialize workers\n+    worker.workDir = workDir\n+    // Create the executor's working directory\n+    val executorDir = new File(worker.workDir, appId + \"/\" + execId)\n+\n+    if (!executorDir.exists()) {\n+      if (!executorDir.mkdirs()) {"
  }],
  "prId": 23393
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "using sleep() in tests to wait for things isn't great -- it either leads to flakiness if you dont' sleep long enough and the test is occasionally slow, or if you make it super long, then it makes tests slow.\r\n\r\nIdeally there would be a condition variable you could wait on, but that's probably not worth it here.  Instead using scalatest's `eventually` works well, eg. like this:\r\n\r\nhttps://github.com/apache/spark/blob/315c95c39953f677220aebc4592ad434019005c0/core/src/test/scala/org/apache/spark/storage/BlockManagerSuite.scala#L279-L282",
    "commit": "475d27817b6ef37eed9b7b39fee709427811f0d6",
    "createdAt": "2019-03-07T15:22:16Z",
    "diffHunk": "@@ -243,4 +244,59 @@ class WorkerSuite extends SparkFunSuite with Matchers with BeforeAndAfter {\n       ExecutorStateChanged(\"app1\", 0, ExecutorState.EXITED, None, None))\n     assert(cleanupCalled.get() == value)\n   }\n+\n+  test(\"removes our metadata of all executors registered for the given \" +\n+    \"application after WorkDirCleanup when config spark.shuffle.service.db.enabled=true\") {\n+    testWorkDirCleanupAndRemoveMetadataWithConfig(true)\n+  }\n+\n+  test(\"don't removes our metadata of all executors registered for the given \" +\n+    \"application after WorkDirCleanup when config spark.shuffle.service.db.enabled=false\") {\n+    testWorkDirCleanupAndRemoveMetadataWithConfig(false)\n+  }\n+\n+  private def testWorkDirCleanupAndRemoveMetadataWithConfig(value: Boolean) = {\n+    val conf = new SparkConf().set(\"spark.shuffle.service.db.enabled\", value.toString)\n+    conf.set(\"spark.worker.cleanup.appDataTtl\", 60.toString)\n+    conf.set(\"spark.shuffle.service.enabled\", \"true\")\n+\n+    val appId = \"app1\"\n+    val execId = \"exec1\"\n+    val cleanupCalled = new AtomicBoolean(false)\n+    when(shuffleService.applicationRemoved(any[String])).thenAnswer(new Answer[Unit] {\n+      override def answer(invocations: InvocationOnMock): Unit = {\n+        cleanupCalled.set(true)\n+      }\n+    })\n+    val externalShuffleServiceSupplier = new Supplier[ExternalShuffleService] {\n+      override def get: ExternalShuffleService = shuffleService\n+    }\n+    val worker = makeWorker(conf, externalShuffleServiceSupplier)\n+\n+    val workDir = Option(\"/tmp/work\").map(new File(_)).get\n+    try {\n+      workDir.mkdirs()\n+      if (!workDir.exists() || !workDir.isDirectory) {\n+        logError(\"Failed to create work directory \" + workDir)\n+      }\n+      assert(workDir.isDirectory)\n+    } catch {\n+      case e: Exception =>\n+        logError(\"Failed to create work directory \" + workDir, e)\n+    }\n+    // initialize workers\n+    worker.workDir = workDir\n+    // Create the executor's working directory\n+    val executorDir = new File(worker.workDir, appId + \"/\" + execId)\n+\n+    if (!executorDir.exists()) {\n+      if (!executorDir.mkdirs()) {\n+        throw new IOException(\"Failed to create directory \" + executorDir)\n+      }\n+    }\n+    executorDir.setLastModified(System.currentTimeMillis - (1000 * 120))\n+    worker.receive(WorkDirCleanup)\n+    Thread.sleep(10)"
  }],
  "prId": 23393
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "to test the old behavior of `WorkDirCleanup`, you also want to assert that `!executorDir.exists()`, right?  (regardless of `value`)",
    "commit": "475d27817b6ef37eed9b7b39fee709427811f0d6",
    "createdAt": "2019-03-07T15:24:27Z",
    "diffHunk": "@@ -243,4 +244,59 @@ class WorkerSuite extends SparkFunSuite with Matchers with BeforeAndAfter {\n       ExecutorStateChanged(\"app1\", 0, ExecutorState.EXITED, None, None))\n     assert(cleanupCalled.get() == value)\n   }\n+\n+  test(\"removes our metadata of all executors registered for the given \" +\n+    \"application after WorkDirCleanup when config spark.shuffle.service.db.enabled=true\") {\n+    testWorkDirCleanupAndRemoveMetadataWithConfig(true)\n+  }\n+\n+  test(\"don't removes our metadata of all executors registered for the given \" +\n+    \"application after WorkDirCleanup when config spark.shuffle.service.db.enabled=false\") {\n+    testWorkDirCleanupAndRemoveMetadataWithConfig(false)\n+  }\n+\n+  private def testWorkDirCleanupAndRemoveMetadataWithConfig(value: Boolean) = {\n+    val conf = new SparkConf().set(\"spark.shuffle.service.db.enabled\", value.toString)\n+    conf.set(\"spark.worker.cleanup.appDataTtl\", 60.toString)\n+    conf.set(\"spark.shuffle.service.enabled\", \"true\")\n+\n+    val appId = \"app1\"\n+    val execId = \"exec1\"\n+    val cleanupCalled = new AtomicBoolean(false)\n+    when(shuffleService.applicationRemoved(any[String])).thenAnswer(new Answer[Unit] {\n+      override def answer(invocations: InvocationOnMock): Unit = {\n+        cleanupCalled.set(true)\n+      }\n+    })\n+    val externalShuffleServiceSupplier = new Supplier[ExternalShuffleService] {\n+      override def get: ExternalShuffleService = shuffleService\n+    }\n+    val worker = makeWorker(conf, externalShuffleServiceSupplier)\n+\n+    val workDir = Option(\"/tmp/work\").map(new File(_)).get\n+    try {\n+      workDir.mkdirs()\n+      if (!workDir.exists() || !workDir.isDirectory) {\n+        logError(\"Failed to create work directory \" + workDir)\n+      }\n+      assert(workDir.isDirectory)\n+    } catch {\n+      case e: Exception =>\n+        logError(\"Failed to create work directory \" + workDir, e)\n+    }\n+    // initialize workers\n+    worker.workDir = workDir\n+    // Create the executor's working directory\n+    val executorDir = new File(worker.workDir, appId + \"/\" + execId)\n+\n+    if (!executorDir.exists()) {\n+      if (!executorDir.mkdirs()) {\n+        throw new IOException(\"Failed to create directory \" + executorDir)\n+      }\n+    }\n+    executorDir.setLastModified(System.currentTimeMillis - (1000 * 120))\n+    worker.receive(WorkDirCleanup)\n+    Thread.sleep(10)\n+    assert(cleanupCalled.get() == value)"
  }],
  "prId": 23393
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "nit: rename `value` to something more meaningful, eg. `dbCleanupEnabled`",
    "commit": "475d27817b6ef37eed9b7b39fee709427811f0d6",
    "createdAt": "2019-03-07T15:24:56Z",
    "diffHunk": "@@ -243,4 +244,59 @@ class WorkerSuite extends SparkFunSuite with Matchers with BeforeAndAfter {\n       ExecutorStateChanged(\"app1\", 0, ExecutorState.EXITED, None, None))\n     assert(cleanupCalled.get() == value)\n   }\n+\n+  test(\"removes our metadata of all executors registered for the given \" +\n+    \"application after WorkDirCleanup when config spark.shuffle.service.db.enabled=true\") {\n+    testWorkDirCleanupAndRemoveMetadataWithConfig(true)\n+  }\n+\n+  test(\"don't removes our metadata of all executors registered for the given \" +\n+    \"application after WorkDirCleanup when config spark.shuffle.service.db.enabled=false\") {\n+    testWorkDirCleanupAndRemoveMetadataWithConfig(false)\n+  }\n+\n+  private def testWorkDirCleanupAndRemoveMetadataWithConfig(value: Boolean) = {"
  }],
  "prId": 23393
}, {
  "comments": [{
    "author": {
      "login": "attilapiros"
    },
    "body": "Is there a way to avoid this sleep? On my machine running the test via IntelliJ this 10 milliseconds was not enough.\r\n",
    "commit": "475d27817b6ef37eed9b7b39fee709427811f0d6",
    "createdAt": "2019-03-07T15:29:09Z",
    "diffHunk": "@@ -243,4 +244,59 @@ class WorkerSuite extends SparkFunSuite with Matchers with BeforeAndAfter {\n       ExecutorStateChanged(\"app1\", 0, ExecutorState.EXITED, None, None))\n     assert(cleanupCalled.get() == value)\n   }\n+\n+  test(\"removes our metadata of all executors registered for the given \" +\n+    \"application after WorkDirCleanup when config spark.shuffle.service.db.enabled=true\") {\n+    testWorkDirCleanupAndRemoveMetadataWithConfig(true)\n+  }\n+\n+  test(\"don't removes our metadata of all executors registered for the given \" +\n+    \"application after WorkDirCleanup when config spark.shuffle.service.db.enabled=false\") {\n+    testWorkDirCleanupAndRemoveMetadataWithConfig(false)\n+  }\n+\n+  private def testWorkDirCleanupAndRemoveMetadataWithConfig(value: Boolean) = {\n+    val conf = new SparkConf().set(\"spark.shuffle.service.db.enabled\", value.toString)\n+    conf.set(\"spark.worker.cleanup.appDataTtl\", 60.toString)\n+    conf.set(\"spark.shuffle.service.enabled\", \"true\")\n+\n+    val appId = \"app1\"\n+    val execId = \"exec1\"\n+    val cleanupCalled = new AtomicBoolean(false)\n+    when(shuffleService.applicationRemoved(any[String])).thenAnswer(new Answer[Unit] {\n+      override def answer(invocations: InvocationOnMock): Unit = {\n+        cleanupCalled.set(true)\n+      }\n+    })\n+    val externalShuffleServiceSupplier = new Supplier[ExternalShuffleService] {\n+      override def get: ExternalShuffleService = shuffleService\n+    }\n+    val worker = makeWorker(conf, externalShuffleServiceSupplier)\n+\n+    val workDir = Option(\"/tmp/work\").map(new File(_)).get\n+    try {\n+      workDir.mkdirs()\n+      if (!workDir.exists() || !workDir.isDirectory) {\n+        logError(\"Failed to create work directory \" + workDir)\n+      }\n+      assert(workDir.isDirectory)\n+    } catch {\n+      case e: Exception =>\n+        logError(\"Failed to create work directory \" + workDir, e)\n+    }\n+    // initialize workers\n+    worker.workDir = workDir\n+    // Create the executor's working directory\n+    val executorDir = new File(worker.workDir, appId + \"/\" + execId)\n+\n+    if (!executorDir.exists()) {\n+      if (!executorDir.mkdirs()) {\n+        throw new IOException(\"Failed to create directory \" + executorDir)\n+      }\n+    }\n+    executorDir.setLastModified(System.currentTimeMillis - (1000 * 120))\n+    worker.receive(WorkDirCleanup)\n+    Thread.sleep(10)"
  }, {
    "author": {
      "login": "attilapiros"
    },
    "body": "Maybe overkill but moving in the Worker extracting the cleanup functionality (the cleanup Future body) into a separate method visible for the test and calling that function directly instead of sending WorkDirCleanup would solve this issue. Otherwise I would a bit worried about having a flaky test depending on Jenkins workload. What is your opinion?",
    "commit": "475d27817b6ef37eed9b7b39fee709427811f0d6",
    "createdAt": "2019-03-07T15:47:34Z",
    "diffHunk": "@@ -243,4 +244,59 @@ class WorkerSuite extends SparkFunSuite with Matchers with BeforeAndAfter {\n       ExecutorStateChanged(\"app1\", 0, ExecutorState.EXITED, None, None))\n     assert(cleanupCalled.get() == value)\n   }\n+\n+  test(\"removes our metadata of all executors registered for the given \" +\n+    \"application after WorkDirCleanup when config spark.shuffle.service.db.enabled=true\") {\n+    testWorkDirCleanupAndRemoveMetadataWithConfig(true)\n+  }\n+\n+  test(\"don't removes our metadata of all executors registered for the given \" +\n+    \"application after WorkDirCleanup when config spark.shuffle.service.db.enabled=false\") {\n+    testWorkDirCleanupAndRemoveMetadataWithConfig(false)\n+  }\n+\n+  private def testWorkDirCleanupAndRemoveMetadataWithConfig(value: Boolean) = {\n+    val conf = new SparkConf().set(\"spark.shuffle.service.db.enabled\", value.toString)\n+    conf.set(\"spark.worker.cleanup.appDataTtl\", 60.toString)\n+    conf.set(\"spark.shuffle.service.enabled\", \"true\")\n+\n+    val appId = \"app1\"\n+    val execId = \"exec1\"\n+    val cleanupCalled = new AtomicBoolean(false)\n+    when(shuffleService.applicationRemoved(any[String])).thenAnswer(new Answer[Unit] {\n+      override def answer(invocations: InvocationOnMock): Unit = {\n+        cleanupCalled.set(true)\n+      }\n+    })\n+    val externalShuffleServiceSupplier = new Supplier[ExternalShuffleService] {\n+      override def get: ExternalShuffleService = shuffleService\n+    }\n+    val worker = makeWorker(conf, externalShuffleServiceSupplier)\n+\n+    val workDir = Option(\"/tmp/work\").map(new File(_)).get\n+    try {\n+      workDir.mkdirs()\n+      if (!workDir.exists() || !workDir.isDirectory) {\n+        logError(\"Failed to create work directory \" + workDir)\n+      }\n+      assert(workDir.isDirectory)\n+    } catch {\n+      case e: Exception =>\n+        logError(\"Failed to create work directory \" + workDir, e)\n+    }\n+    // initialize workers\n+    worker.workDir = workDir\n+    // Create the executor's working directory\n+    val executorDir = new File(worker.workDir, appId + \"/\" + execId)\n+\n+    if (!executorDir.exists()) {\n+      if (!executorDir.mkdirs()) {\n+        throw new IOException(\"Failed to create directory \" + executorDir)\n+      }\n+    }\n+    executorDir.setLastModified(System.currentTimeMillis - (1000 * 120))\n+    worker.receive(WorkDirCleanup)\n+    Thread.sleep(10)"
  }, {
    "author": {
      "login": "attilapiros"
    },
    "body": "I just read @squito's solution for this problem and I like that as it is less intrusive.",
    "commit": "475d27817b6ef37eed9b7b39fee709427811f0d6",
    "createdAt": "2019-03-07T15:51:10Z",
    "diffHunk": "@@ -243,4 +244,59 @@ class WorkerSuite extends SparkFunSuite with Matchers with BeforeAndAfter {\n       ExecutorStateChanged(\"app1\", 0, ExecutorState.EXITED, None, None))\n     assert(cleanupCalled.get() == value)\n   }\n+\n+  test(\"removes our metadata of all executors registered for the given \" +\n+    \"application after WorkDirCleanup when config spark.shuffle.service.db.enabled=true\") {\n+    testWorkDirCleanupAndRemoveMetadataWithConfig(true)\n+  }\n+\n+  test(\"don't removes our metadata of all executors registered for the given \" +\n+    \"application after WorkDirCleanup when config spark.shuffle.service.db.enabled=false\") {\n+    testWorkDirCleanupAndRemoveMetadataWithConfig(false)\n+  }\n+\n+  private def testWorkDirCleanupAndRemoveMetadataWithConfig(value: Boolean) = {\n+    val conf = new SparkConf().set(\"spark.shuffle.service.db.enabled\", value.toString)\n+    conf.set(\"spark.worker.cleanup.appDataTtl\", 60.toString)\n+    conf.set(\"spark.shuffle.service.enabled\", \"true\")\n+\n+    val appId = \"app1\"\n+    val execId = \"exec1\"\n+    val cleanupCalled = new AtomicBoolean(false)\n+    when(shuffleService.applicationRemoved(any[String])).thenAnswer(new Answer[Unit] {\n+      override def answer(invocations: InvocationOnMock): Unit = {\n+        cleanupCalled.set(true)\n+      }\n+    })\n+    val externalShuffleServiceSupplier = new Supplier[ExternalShuffleService] {\n+      override def get: ExternalShuffleService = shuffleService\n+    }\n+    val worker = makeWorker(conf, externalShuffleServiceSupplier)\n+\n+    val workDir = Option(\"/tmp/work\").map(new File(_)).get\n+    try {\n+      workDir.mkdirs()\n+      if (!workDir.exists() || !workDir.isDirectory) {\n+        logError(\"Failed to create work directory \" + workDir)\n+      }\n+      assert(workDir.isDirectory)\n+    } catch {\n+      case e: Exception =>\n+        logError(\"Failed to create work directory \" + workDir, e)\n+    }\n+    // initialize workers\n+    worker.workDir = workDir\n+    // Create the executor's working directory\n+    val executorDir = new File(worker.workDir, appId + \"/\" + execId)\n+\n+    if (!executorDir.exists()) {\n+      if (!executorDir.mkdirs()) {\n+        throw new IOException(\"Failed to create directory \" + executorDir)\n+      }\n+    }\n+    executorDir.setLastModified(System.currentTimeMillis - (1000 * 120))\n+    worker.receive(WorkDirCleanup)\n+    Thread.sleep(10)"
  }],
  "prId": 23393
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "minor -- as I mentioned below, both of these tests should still test dir cleanup, so maybe we can rename a little to something like\r\n\r\nWorkDirCleanup cleans app dirs and shuffle metadata when spark.shuffle.service.db.enabled=true\r\n\r\nWorkdDirCleanup cleans only app dirs when spark.shuffle.service.db.enabled=false",
    "commit": "475d27817b6ef37eed9b7b39fee709427811f0d6",
    "createdAt": "2019-03-07T15:30:38Z",
    "diffHunk": "@@ -243,4 +244,59 @@ class WorkerSuite extends SparkFunSuite with Matchers with BeforeAndAfter {\n       ExecutorStateChanged(\"app1\", 0, ExecutorState.EXITED, None, None))\n     assert(cleanupCalled.get() == value)\n   }\n+\n+  test(\"removes our metadata of all executors registered for the given \" +\n+    \"application after WorkDirCleanup when config spark.shuffle.service.db.enabled=true\") {\n+    testWorkDirCleanupAndRemoveMetadataWithConfig(true)\n+  }\n+\n+  test(\"don't removes our metadata of all executors registered for the given \" +\n+    \"application after WorkDirCleanup when config spark.shuffle.service.db.enabled=false\") {"
  }],
  "prId": 23393
}, {
  "comments": [{
    "author": {
      "login": "attilapiros"
    },
    "body": "Super Nit: 60.toString => \"60\"",
    "commit": "475d27817b6ef37eed9b7b39fee709427811f0d6",
    "createdAt": "2019-03-13T14:33:54Z",
    "diffHunk": "@@ -245,4 +249,59 @@ class WorkerSuite extends SparkFunSuite with Matchers with BeforeAndAfter {\n       ExecutorStateChanged(\"app1\", 0, ExecutorState.EXITED, None, None))\n     assert(cleanupCalled.get() == value)\n   }\n+\n+  test(\"WorkDirCleanup cleans app dirs and shuffle metadata when \" +\n+    \"spark.shuffle.service.db.enabled=true\") {\n+    testWorkDirCleanupAndRemoveMetadataWithConfig(true)\n+  }\n+\n+  test(\"WorkdDirCleanup cleans only app dirs when\" +\n+    \"spark.shuffle.service.db.enabled=false\") {\n+    testWorkDirCleanupAndRemoveMetadataWithConfig(false)\n+  }\n+\n+  private def testWorkDirCleanupAndRemoveMetadataWithConfig(dbCleanupEnabled: Boolean) = {\n+    val conf = new SparkConf().set(\"spark.shuffle.service.db.enabled\", dbCleanupEnabled.toString)\n+    conf.set(\"spark.worker.cleanup.appDataTtl\", 60.toString)"
  }],
  "prId": 23393
}, {
  "comments": [{
    "author": {
      "login": "attilapiros"
    },
    "body": "Nit: indent",
    "commit": "475d27817b6ef37eed9b7b39fee709427811f0d6",
    "createdAt": "2019-03-13T14:44:33Z",
    "diffHunk": "@@ -245,4 +249,59 @@ class WorkerSuite extends SparkFunSuite with Matchers with BeforeAndAfter {\n       ExecutorStateChanged(\"app1\", 0, ExecutorState.EXITED, None, None))\n     assert(cleanupCalled.get() == value)\n   }\n+\n+  test(\"WorkDirCleanup cleans app dirs and shuffle metadata when \" +\n+    \"spark.shuffle.service.db.enabled=true\") {\n+    testWorkDirCleanupAndRemoveMetadataWithConfig(true)\n+  }\n+\n+  test(\"WorkdDirCleanup cleans only app dirs when\" +\n+    \"spark.shuffle.service.db.enabled=false\") {\n+    testWorkDirCleanupAndRemoveMetadataWithConfig(false)\n+  }\n+\n+  private def testWorkDirCleanupAndRemoveMetadataWithConfig(dbCleanupEnabled: Boolean) = {\n+    val conf = new SparkConf().set(\"spark.shuffle.service.db.enabled\", dbCleanupEnabled.toString)\n+    conf.set(\"spark.worker.cleanup.appDataTtl\", 60.toString)\n+    conf.set(\"spark.shuffle.service.enabled\", \"true\")\n+\n+    val appId = \"app1\"\n+    val execId = \"exec1\"\n+    val cleanupCalled = new AtomicBoolean(false)\n+    when(shuffleService.applicationRemoved(any[String])).thenAnswer(new Answer[Unit] {\n+      override def answer(invocations: InvocationOnMock): Unit = {\n+        cleanupCalled.set(true)\n+      }\n+    })\n+    val externalShuffleServiceSupplier = new Supplier[ExternalShuffleService] {\n+      override def get: ExternalShuffleService = shuffleService\n+    }\n+    val worker = makeWorker(conf, externalShuffleServiceSupplier)\n+\n+    val workDir = Option(\"/tmp/work\").map(new File(_)).get\n+    try {\n+      workDir.mkdirs()\n+      if (!workDir.exists() || !workDir.isDirectory) {\n+        logError(\"Failed to create work directory \" + workDir)\n+      }\n+      assert(workDir.isDirectory)\n+    } catch {\n+      case e: Exception =>\n+        logError(\"Failed to create work directory \" + workDir, e)\n+    }\n+    // initialize workers\n+    worker.workDir = workDir\n+    // Create the executor's working directory\n+    val executorDir = new File(worker.workDir, appId + \"/\" + execId)\n+\n+    if (!executorDir.exists && !executorDir.mkdirs()) {\n+        throw new IOException(\"Failed to create directory \" + executorDir)"
  }],
  "prId": 23393
}, {
  "comments": [{
    "author": {
      "login": "attilapiros"
    },
    "body": "why this complexity? Is not it just something like:\r\n```\r\n    val workDir = new File(\"/tmp/work\")\r\n```\r\n",
    "commit": "475d27817b6ef37eed9b7b39fee709427811f0d6",
    "createdAt": "2019-03-13T15:32:52Z",
    "diffHunk": "@@ -245,4 +249,59 @@ class WorkerSuite extends SparkFunSuite with Matchers with BeforeAndAfter {\n       ExecutorStateChanged(\"app1\", 0, ExecutorState.EXITED, None, None))\n     assert(cleanupCalled.get() == value)\n   }\n+\n+  test(\"WorkDirCleanup cleans app dirs and shuffle metadata when \" +\n+    \"spark.shuffle.service.db.enabled=true\") {\n+    testWorkDirCleanupAndRemoveMetadataWithConfig(true)\n+  }\n+\n+  test(\"WorkdDirCleanup cleans only app dirs when\" +\n+    \"spark.shuffle.service.db.enabled=false\") {\n+    testWorkDirCleanupAndRemoveMetadataWithConfig(false)\n+  }\n+\n+  private def testWorkDirCleanupAndRemoveMetadataWithConfig(dbCleanupEnabled: Boolean) = {\n+    val conf = new SparkConf().set(\"spark.shuffle.service.db.enabled\", dbCleanupEnabled.toString)\n+    conf.set(\"spark.worker.cleanup.appDataTtl\", 60.toString)\n+    conf.set(\"spark.shuffle.service.enabled\", \"true\")\n+\n+    val appId = \"app1\"\n+    val execId = \"exec1\"\n+    val cleanupCalled = new AtomicBoolean(false)\n+    when(shuffleService.applicationRemoved(any[String])).thenAnswer(new Answer[Unit] {\n+      override def answer(invocations: InvocationOnMock): Unit = {\n+        cleanupCalled.set(true)\n+      }\n+    })\n+    val externalShuffleServiceSupplier = new Supplier[ExternalShuffleService] {\n+      override def get: ExternalShuffleService = shuffleService\n+    }\n+    val worker = makeWorker(conf, externalShuffleServiceSupplier)\n+\n+    val workDir = Option(\"/tmp/work\").map(new File(_)).get"
  }, {
    "author": {
      "login": "attilapiros"
    },
    "body": "Please use `org.apache.spark.util.Utils#createTempDir` which safely cleaned up after the test is executed.",
    "commit": "475d27817b6ef37eed9b7b39fee709427811f0d6",
    "createdAt": "2019-03-13T15:47:45Z",
    "diffHunk": "@@ -245,4 +249,59 @@ class WorkerSuite extends SparkFunSuite with Matchers with BeforeAndAfter {\n       ExecutorStateChanged(\"app1\", 0, ExecutorState.EXITED, None, None))\n     assert(cleanupCalled.get() == value)\n   }\n+\n+  test(\"WorkDirCleanup cleans app dirs and shuffle metadata when \" +\n+    \"spark.shuffle.service.db.enabled=true\") {\n+    testWorkDirCleanupAndRemoveMetadataWithConfig(true)\n+  }\n+\n+  test(\"WorkdDirCleanup cleans only app dirs when\" +\n+    \"spark.shuffle.service.db.enabled=false\") {\n+    testWorkDirCleanupAndRemoveMetadataWithConfig(false)\n+  }\n+\n+  private def testWorkDirCleanupAndRemoveMetadataWithConfig(dbCleanupEnabled: Boolean) = {\n+    val conf = new SparkConf().set(\"spark.shuffle.service.db.enabled\", dbCleanupEnabled.toString)\n+    conf.set(\"spark.worker.cleanup.appDataTtl\", 60.toString)\n+    conf.set(\"spark.shuffle.service.enabled\", \"true\")\n+\n+    val appId = \"app1\"\n+    val execId = \"exec1\"\n+    val cleanupCalled = new AtomicBoolean(false)\n+    when(shuffleService.applicationRemoved(any[String])).thenAnswer(new Answer[Unit] {\n+      override def answer(invocations: InvocationOnMock): Unit = {\n+        cleanupCalled.set(true)\n+      }\n+    })\n+    val externalShuffleServiceSupplier = new Supplier[ExternalShuffleService] {\n+      override def get: ExternalShuffleService = shuffleService\n+    }\n+    val worker = makeWorker(conf, externalShuffleServiceSupplier)\n+\n+    val workDir = Option(\"/tmp/work\").map(new File(_)).get"
  }],
  "prId": 23393
}]