[{
  "comments": [{
    "author": {
      "login": "sarutak"
    },
    "body": "Here and in any other places, when we put many parameters, we should write like as follows right?\n\n```\nnew ExecutorRunner(\n  \"appId\",\n  execId,\n  createAppDesc(),\n  4,\n  1234,\n  null,\n  \"workerId\",\n  \"host\",\n  123,\n  ...)\n```\n",
    "commit": "23977fb3bc590f58e9d4d44cfcce78ce0a49baca",
    "createdAt": "2015-07-29T09:27:28Z",
    "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io.File\n+import java.util.Date\n+\n+import org.apache.spark.deploy.master.{ApplicationInfo, DriverInfo, WorkerInfo}\n+import org.apache.spark.deploy.worker.{DriverRunner, ExecutorRunner}\n+import org.apache.spark.{SecurityManager, SparkConf}\n+\n+private[deploy] object DeployTestUtils {\n+  def createAppDesc(): ApplicationDescription = {\n+    val cmd = new Command(\"mainClass\", List(\"arg1\", \"arg2\"), Map(), Seq(), Seq(), Seq())\n+    new ApplicationDescription(\"name\", Some(4), 1234, cmd, \"appUiUrl\")\n+  }\n+\n+  def createAppInfo() : ApplicationInfo = {\n+    val appInfo = new ApplicationInfo(JsonConstants.appInfoStartTime,\n+      \"id\", createAppDesc(), JsonConstants.submitDate, null, Int.MaxValue)\n+    appInfo.endTime = JsonConstants.currTimeInMillis\n+    appInfo\n+  }\n+\n+  def createDriverCommand(): Command = new Command(\n+    \"org.apache.spark.FakeClass\", Seq(\"some arg --and-some options -g foo\"),\n+    Map((\"K1\", \"V1\"), (\"K2\", \"V2\")), Seq(\"cp1\", \"cp2\"), Seq(\"lp1\", \"lp2\"), Seq(\"-Dfoo\")\n+  )\n+\n+  def createDriverDesc(): DriverDescription =\n+    new DriverDescription(\"hdfs://some-dir/some.jar\", 100, 3, false, createDriverCommand())\n+\n+  def createDriverInfo(): DriverInfo = new DriverInfo(3, \"driver-3\",\n+    createDriverDesc(), new Date())\n+\n+  def createWorkerInfo(): WorkerInfo = {\n+    val workerInfo = new WorkerInfo(\"id\", \"host\", 8080, 4, 1234, null, 80, \"publicAddress\")\n+    workerInfo.lastHeartbeat = JsonConstants.currTimeInMillis\n+    workerInfo\n+  }\n+\n+  def createExecutorRunner(execId: Int): ExecutorRunner = {\n+    new ExecutorRunner(\"appId\", execId, createAppDesc(), 4, 1234, null, \"workerId\", \"host\", 123,"
  }, {
    "author": {
      "login": "CodingCat"
    },
    "body": "sure, addressing that\n",
    "commit": "23977fb3bc590f58e9d4d44cfcce78ce0a49baca",
    "createdAt": "2015-07-29T12:19:34Z",
    "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io.File\n+import java.util.Date\n+\n+import org.apache.spark.deploy.master.{ApplicationInfo, DriverInfo, WorkerInfo}\n+import org.apache.spark.deploy.worker.{DriverRunner, ExecutorRunner}\n+import org.apache.spark.{SecurityManager, SparkConf}\n+\n+private[deploy] object DeployTestUtils {\n+  def createAppDesc(): ApplicationDescription = {\n+    val cmd = new Command(\"mainClass\", List(\"arg1\", \"arg2\"), Map(), Seq(), Seq(), Seq())\n+    new ApplicationDescription(\"name\", Some(4), 1234, cmd, \"appUiUrl\")\n+  }\n+\n+  def createAppInfo() : ApplicationInfo = {\n+    val appInfo = new ApplicationInfo(JsonConstants.appInfoStartTime,\n+      \"id\", createAppDesc(), JsonConstants.submitDate, null, Int.MaxValue)\n+    appInfo.endTime = JsonConstants.currTimeInMillis\n+    appInfo\n+  }\n+\n+  def createDriverCommand(): Command = new Command(\n+    \"org.apache.spark.FakeClass\", Seq(\"some arg --and-some options -g foo\"),\n+    Map((\"K1\", \"V1\"), (\"K2\", \"V2\")), Seq(\"cp1\", \"cp2\"), Seq(\"lp1\", \"lp2\"), Seq(\"-Dfoo\")\n+  )\n+\n+  def createDriverDesc(): DriverDescription =\n+    new DriverDescription(\"hdfs://some-dir/some.jar\", 100, 3, false, createDriverCommand())\n+\n+  def createDriverInfo(): DriverInfo = new DriverInfo(3, \"driver-3\",\n+    createDriverDesc(), new Date())\n+\n+  def createWorkerInfo(): WorkerInfo = {\n+    val workerInfo = new WorkerInfo(\"id\", \"host\", 8080, 4, 1234, null, 80, \"publicAddress\")\n+    workerInfo.lastHeartbeat = JsonConstants.currTimeInMillis\n+    workerInfo\n+  }\n+\n+  def createExecutorRunner(execId: Int): ExecutorRunner = {\n+    new ExecutorRunner(\"appId\", execId, createAppDesc(), 4, 1234, null, \"workerId\", \"host\", 123,"
  }],
  "prId": 7714
}]