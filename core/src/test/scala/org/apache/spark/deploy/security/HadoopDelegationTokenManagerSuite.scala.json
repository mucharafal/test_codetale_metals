[{
  "comments": [{
    "author": {
      "login": "attilapiros"
    },
    "body": "There are other test methods where all the others DT providers are tested similar to this. \r\nLike `test(\"using deprecated configurations\")` you can extend them with this line as well (to keep the symmetry).",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-10-01T12:18:56Z",
    "diffHunk": "@@ -46,6 +47,7 @@ class HadoopDelegationTokenManagerSuite extends SparkFunSuite with Matchers {\n     delegationTokenManager.getServiceDelegationTokenProvider(\"hadoopfs\") should not be (None)\n     delegationTokenManager.getServiceDelegationTokenProvider(\"hbase\") should not be (None)\n     delegationTokenManager.getServiceDelegationTokenProvider(\"hive\") should not be (None)\n+    delegationTokenManager.getServiceDelegationTokenProvider(\"kafka\") should not be (None)"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Fixed.",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-10-02T19:31:25Z",
    "diffHunk": "@@ -46,6 +47,7 @@ class HadoopDelegationTokenManagerSuite extends SparkFunSuite with Matchers {\n     delegationTokenManager.getServiceDelegationTokenProvider(\"hadoopfs\") should not be (None)\n     delegationTokenManager.getServiceDelegationTokenProvider(\"hbase\") should not be (None)\n     delegationTokenManager.getServiceDelegationTokenProvider(\"hive\") should not be (None)\n+    delegationTokenManager.getServiceDelegationTokenProvider(\"kafka\") should not be (None)"
  }],
  "prId": 22598
}, {
  "comments": [{
    "author": {
      "login": "attilapiros"
    },
    "body": "I have seen this is follows the `Obtain tokens For HBase` but the using `creds.getAllTokens.size should be (0)` for both the positive and negative case (if I understand correctly) is quite strange. Can you somehow mock a relevant method (i.e. in TokenUtil) to get back something? ",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-10-01T12:23:42Z",
    "diffHunk": "@@ -111,6 +113,17 @@ class HadoopDelegationTokenManagerSuite extends SparkFunSuite with Matchers {\n     creds.getAllTokens.size should be (0)\n   }\n \n+  test(\"Obtain tokens For Kafka\") {\n+    val hadoopConf = new Configuration()\n+    sparkConf.set(KAFKA_DELEGATION_TOKEN_ENABLED, true)\n+\n+    val kafkaTokenProvider = new KafkaDelegationTokenProvider()\n+    val creds = new Credentials()\n+    kafkaTokenProvider.obtainDelegationTokens(hadoopConf, sparkConf, creds)\n+\n+    creds.getAllTokens.size should be (0)"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "I was thinking about this and don't see clean option to do it:\r\n* Adding kafka dependency to `HadoopDelegationTokenManagerSuite` is bad idea.\r\n* `KafkaMicroBatchSourceSuite` can't reach neither `HadoopDelegationTokenManager` nor `KafkaDelegationTokenProvider`.\r\n* KDC is not available in general.\r\n",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-10-01T13:29:40Z",
    "diffHunk": "@@ -111,6 +113,17 @@ class HadoopDelegationTokenManagerSuite extends SparkFunSuite with Matchers {\n     creds.getAllTokens.size should be (0)\n   }\n \n+  test(\"Obtain tokens For Kafka\") {\n+    val hadoopConf = new Configuration()\n+    sparkConf.set(KAFKA_DELEGATION_TOKEN_ENABLED, true)\n+\n+    val kafkaTokenProvider = new KafkaDelegationTokenProvider()\n+    val creds = new Credentials()\n+    kafkaTokenProvider.obtainDelegationTokens(hadoopConf, sparkConf, creds)\n+\n+    creds.getAllTokens.size should be (0)"
  }, {
    "author": {
      "login": "attilapiros"
    },
    "body": "I see. Unfortunately this way this test has not much value as in obtainDelegationTokens an exception is thrown when the loadclass is called with \"org.apache.spark.sql.kafka010.TokenUtil\". But as this is quite the same for HBase you can keep it. ",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-10-02T04:28:26Z",
    "diffHunk": "@@ -111,6 +113,17 @@ class HadoopDelegationTokenManagerSuite extends SparkFunSuite with Matchers {\n     creds.getAllTokens.size should be (0)\n   }\n \n+  test(\"Obtain tokens For Kafka\") {\n+    val hadoopConf = new Configuration()\n+    sparkConf.set(KAFKA_DELEGATION_TOKEN_ENABLED, true)\n+\n+    val kafkaTokenProvider = new KafkaDelegationTokenProvider()\n+    val creds = new Credentials()\n+    kafkaTokenProvider.obtainDelegationTokens(hadoopConf, sparkConf, creds)\n+\n+    creds.getAllTokens.size should be (0)"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "Agree with Attila that this is not helping much. If you can have better tests on the kafka connector side, and exercise the `KafkaDelegationTokenProvider` code from there, it would be better.",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-10-04T18:17:05Z",
    "diffHunk": "@@ -111,6 +113,17 @@ class HadoopDelegationTokenManagerSuite extends SparkFunSuite with Matchers {\n     creds.getAllTokens.size should be (0)\n   }\n \n+  test(\"Obtain tokens For Kafka\") {\n+    val hadoopConf = new Configuration()\n+    sparkConf.set(KAFKA_DELEGATION_TOKEN_ENABLED, true)\n+\n+    val kafkaTokenProvider = new KafkaDelegationTokenProvider()\n+    val creds = new Credentials()\n+    kafkaTokenProvider.obtainDelegationTokens(hadoopConf, sparkConf, creds)\n+\n+    creds.getAllTokens.size should be (0)"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Increased test coverage.",
    "commit": "a1228657a56d53ee2ff39232536e88223950d36a",
    "createdAt": "2018-10-08T12:56:11Z",
    "diffHunk": "@@ -111,6 +113,17 @@ class HadoopDelegationTokenManagerSuite extends SparkFunSuite with Matchers {\n     creds.getAllTokens.size should be (0)\n   }\n \n+  test(\"Obtain tokens For Kafka\") {\n+    val hadoopConf = new Configuration()\n+    sparkConf.set(KAFKA_DELEGATION_TOKEN_ENABLED, true)\n+\n+    val kafkaTokenProvider = new KafkaDelegationTokenProvider()\n+    val creds = new Credentials()\n+    kafkaTokenProvider.obtainDelegationTokens(hadoopConf, sparkConf, creds)\n+\n+    creds.getAllTokens.size should be (0)"
  }],
  "prId": 22598
}]