[{
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "This comment makes me think that trying to test the behavior of \"what data is available in a file currently being written\" is not a good test. Spark makes a best effort to get data available from the file when replaying, but past that, this test seems to be trying to enforce something that at times is out of Spark's control.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-26T01:01:46Z",
    "diffHunk": "@@ -0,0 +1,400 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.{File, FileOutputStream, IOException}\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import scala.collection.mutable\n+import scala.io.Source\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.{LocalSparkContext, SparkConf, SparkFunSuite}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.history.EventLogTestHelper._\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+\n+abstract class EventLogFileWritersSuite extends SparkFunSuite with LocalSparkContext\n+  with BeforeAndAfter {\n+\n+  protected val fileSystem = Utils.getHadoopFileSystem(\"/\",\n+    SparkHadoopUtil.get.newConfiguration(new SparkConf()))\n+  protected var testDir: File = _\n+  protected var testDirPath: Path = _\n+\n+  before {\n+    testDir = Utils.createTempDir(namePrefix = s\"event log\")\n+    testDir.deleteOnExit()\n+    testDirPath = new Path(testDir.getAbsolutePath())\n+  }\n+\n+  after {\n+    Utils.deleteRecursively(testDir)\n+  }\n+\n+  test(\"create EventLogFileWriter with enable/disable rolling\") {\n+    def buildWriterAndVerify(conf: SparkConf, expectedClazz: Class[_]): Unit = {\n+      val writer = EventLogFileWriter(\n+        getUniqueApplicationId, None, testDirPath.toUri, conf,\n+        SparkHadoopUtil.get.newConfiguration(conf))\n+      val writerClazz = writer.getClass\n+      assert(expectedClazz === writerClazz,\n+        s\"default file writer should be $expectedClazz, but $writerClazz\")\n+    }\n+\n+    val conf = new SparkConf\n+    conf.set(EVENT_LOG_ENABLED, true)\n+    conf.set(EVENT_LOG_DIR, testDir.toString)\n+\n+    // default config\n+    buildWriterAndVerify(conf, classOf[SingleEventLogFileWriter])\n+\n+    conf.set(EVENT_LOG_ENABLE_ROLLING, true)\n+    buildWriterAndVerify(conf, classOf[RollingEventLogFilesWriter])\n+\n+    conf.set(EVENT_LOG_ENABLE_ROLLING, false)\n+    buildWriterAndVerify(conf, classOf[SingleEventLogFileWriter])\n+  }\n+\n+  val allCodecs = Seq(None) ++\n+    CompressionCodec.ALL_COMPRESSION_CODECS.map(c => Some(CompressionCodec.getShortName(c)))\n+\n+  allCodecs.foreach { codecShortName =>\n+    test(s\"initialize, write, stop - with codec $codecShortName\") {\n+      val appId = getUniqueApplicationId\n+      val attemptId = None\n+\n+      val conf = getLoggingConf(testDirPath, codecShortName)\n+      val writer = createWriter(appId, attemptId, testDirPath.toUri, conf,\n+        SparkHadoopUtil.get.newConfiguration(conf))\n+\n+      writer.start()\n+\n+      // snappy stream throws exception on empty stream, so we should provide some data to test.\n+      val dummyData = Seq(\"dummy1\", \"dummy2\", \"dummy3\")\n+      dummyData.foreach(writer.writeEvent(_, flushLogger = true))\n+\n+      verifyWriteEventLogFile(appId, attemptId, testDirPath.toUri, codecShortName,\n+        isCompleted = false, dummyData)\n+\n+      writer.stop()\n+\n+      verifyWriteEventLogFile(appId, attemptId, testDirPath.toUri, codecShortName,\n+        isCompleted = true, dummyData)\n+    }\n+  }\n+\n+  test(\"spark.eventLog.compression.codec overrides spark.io.compression.codec\") {\n+    val conf = new SparkConf\n+    conf.set(EVENT_LOG_COMPRESS, true)\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+\n+    val appId = \"test\"\n+    val appAttemptId = None\n+\n+    // The default value is `spark.io.compression.codec`.\n+    val writer = createWriter(appId, appAttemptId, testDirPath.toUri, conf, hadoopConf)\n+    assert(writer.compressionCodecName.contains(\"lz4\"))\n+\n+    // `spark.eventLog.compression.codec` overrides `spark.io.compression.codec`.\n+    conf.set(EVENT_LOG_COMPRESSION_CODEC, \"zstd\")\n+    val writer2 = createWriter(appId, appAttemptId, testDirPath.toUri, conf, hadoopConf)\n+    assert(writer2.compressionCodecName.contains(\"zstd\"))\n+  }\n+\n+  protected def readLinesFromEventLogFile(log: Path, fs: FileSystem): List[String] = {\n+    val logDataStream = EventLogFileReader.openEventLog(log, fs)\n+    try {\n+      Source.fromInputStream(logDataStream).getLines().toList\n+    } finally {\n+      logDataStream.close()\n+    }\n+  }\n+\n+  protected def createWriter(\n+      appId: String,\n+      appAttemptId : Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter\n+\n+  protected def verifyWriteEventLogFile(\n+      appId: String,\n+      appAttemptId : Option[String],\n+      logBaseDir: URI,\n+      compressionCodecShortName: Option[String],\n+      isCompleted: Boolean,\n+      expectedLines: Seq[String] = Seq.empty): Unit\n+\n+  protected def skipVerifyEventLogFile(\n+      compressionCodecShortName: Option[String],\n+      isCompleted: Boolean): Boolean = {\n+    // Spark initializes LZ4BlockOutputStream with syncFlush=false, so we can't force"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "I guess SHS is on the same page (I'm not sure it can be remedied with different JVM), so if there's some issue here, SHS may have same issue as well. Maybe we would be better to make problematic compressions as unsupported? (LZ4BlockOutputStream might be OK as it's just a bit late on flushing desired output, but for zstd I have seen InputStream being stuck with reading which might occur to SHS as well.)",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-26T01:32:10Z",
    "diffHunk": "@@ -0,0 +1,400 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.{File, FileOutputStream, IOException}\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import scala.collection.mutable\n+import scala.io.Source\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.{LocalSparkContext, SparkConf, SparkFunSuite}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.history.EventLogTestHelper._\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+\n+abstract class EventLogFileWritersSuite extends SparkFunSuite with LocalSparkContext\n+  with BeforeAndAfter {\n+\n+  protected val fileSystem = Utils.getHadoopFileSystem(\"/\",\n+    SparkHadoopUtil.get.newConfiguration(new SparkConf()))\n+  protected var testDir: File = _\n+  protected var testDirPath: Path = _\n+\n+  before {\n+    testDir = Utils.createTempDir(namePrefix = s\"event log\")\n+    testDir.deleteOnExit()\n+    testDirPath = new Path(testDir.getAbsolutePath())\n+  }\n+\n+  after {\n+    Utils.deleteRecursively(testDir)\n+  }\n+\n+  test(\"create EventLogFileWriter with enable/disable rolling\") {\n+    def buildWriterAndVerify(conf: SparkConf, expectedClazz: Class[_]): Unit = {\n+      val writer = EventLogFileWriter(\n+        getUniqueApplicationId, None, testDirPath.toUri, conf,\n+        SparkHadoopUtil.get.newConfiguration(conf))\n+      val writerClazz = writer.getClass\n+      assert(expectedClazz === writerClazz,\n+        s\"default file writer should be $expectedClazz, but $writerClazz\")\n+    }\n+\n+    val conf = new SparkConf\n+    conf.set(EVENT_LOG_ENABLED, true)\n+    conf.set(EVENT_LOG_DIR, testDir.toString)\n+\n+    // default config\n+    buildWriterAndVerify(conf, classOf[SingleEventLogFileWriter])\n+\n+    conf.set(EVENT_LOG_ENABLE_ROLLING, true)\n+    buildWriterAndVerify(conf, classOf[RollingEventLogFilesWriter])\n+\n+    conf.set(EVENT_LOG_ENABLE_ROLLING, false)\n+    buildWriterAndVerify(conf, classOf[SingleEventLogFileWriter])\n+  }\n+\n+  val allCodecs = Seq(None) ++\n+    CompressionCodec.ALL_COMPRESSION_CODECS.map(c => Some(CompressionCodec.getShortName(c)))\n+\n+  allCodecs.foreach { codecShortName =>\n+    test(s\"initialize, write, stop - with codec $codecShortName\") {\n+      val appId = getUniqueApplicationId\n+      val attemptId = None\n+\n+      val conf = getLoggingConf(testDirPath, codecShortName)\n+      val writer = createWriter(appId, attemptId, testDirPath.toUri, conf,\n+        SparkHadoopUtil.get.newConfiguration(conf))\n+\n+      writer.start()\n+\n+      // snappy stream throws exception on empty stream, so we should provide some data to test.\n+      val dummyData = Seq(\"dummy1\", \"dummy2\", \"dummy3\")\n+      dummyData.foreach(writer.writeEvent(_, flushLogger = true))\n+\n+      verifyWriteEventLogFile(appId, attemptId, testDirPath.toUri, codecShortName,\n+        isCompleted = false, dummyData)\n+\n+      writer.stop()\n+\n+      verifyWriteEventLogFile(appId, attemptId, testDirPath.toUri, codecShortName,\n+        isCompleted = true, dummyData)\n+    }\n+  }\n+\n+  test(\"spark.eventLog.compression.codec overrides spark.io.compression.codec\") {\n+    val conf = new SparkConf\n+    conf.set(EVENT_LOG_COMPRESS, true)\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+\n+    val appId = \"test\"\n+    val appAttemptId = None\n+\n+    // The default value is `spark.io.compression.codec`.\n+    val writer = createWriter(appId, appAttemptId, testDirPath.toUri, conf, hadoopConf)\n+    assert(writer.compressionCodecName.contains(\"lz4\"))\n+\n+    // `spark.eventLog.compression.codec` overrides `spark.io.compression.codec`.\n+    conf.set(EVENT_LOG_COMPRESSION_CODEC, \"zstd\")\n+    val writer2 = createWriter(appId, appAttemptId, testDirPath.toUri, conf, hadoopConf)\n+    assert(writer2.compressionCodecName.contains(\"zstd\"))\n+  }\n+\n+  protected def readLinesFromEventLogFile(log: Path, fs: FileSystem): List[String] = {\n+    val logDataStream = EventLogFileReader.openEventLog(log, fs)\n+    try {\n+      Source.fromInputStream(logDataStream).getLines().toList\n+    } finally {\n+      logDataStream.close()\n+    }\n+  }\n+\n+  protected def createWriter(\n+      appId: String,\n+      appAttemptId : Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter\n+\n+  protected def verifyWriteEventLogFile(\n+      appId: String,\n+      appAttemptId : Option[String],\n+      logBaseDir: URI,\n+      compressionCodecShortName: Option[String],\n+      isCompleted: Boolean,\n+      expectedLines: Seq[String] = Seq.empty): Unit\n+\n+  protected def skipVerifyEventLogFile(\n+      compressionCodecShortName: Option[String],\n+      isCompleted: Boolean): Boolean = {\n+    // Spark initializes LZ4BlockOutputStream with syncFlush=false, so we can't force"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "> Maybe we would be better to make problematic compressions as unsupported?\r\n\r\nThat's not what I mean. The SHS does the best it can; what I'm saying is that these tests being added that try to read open files aren't really helpful since they depend on specific behavior of code that is outside of Spark's control - namely, compression codecs.\r\n\r\nSo the test really should only be asserting that data is available in the file after the file is closed; before that, there is no guarantee that data has actually made it to the file.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-26T17:50:23Z",
    "diffHunk": "@@ -0,0 +1,400 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.{File, FileOutputStream, IOException}\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import scala.collection.mutable\n+import scala.io.Source\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.{LocalSparkContext, SparkConf, SparkFunSuite}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.history.EventLogTestHelper._\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+\n+abstract class EventLogFileWritersSuite extends SparkFunSuite with LocalSparkContext\n+  with BeforeAndAfter {\n+\n+  protected val fileSystem = Utils.getHadoopFileSystem(\"/\",\n+    SparkHadoopUtil.get.newConfiguration(new SparkConf()))\n+  protected var testDir: File = _\n+  protected var testDirPath: Path = _\n+\n+  before {\n+    testDir = Utils.createTempDir(namePrefix = s\"event log\")\n+    testDir.deleteOnExit()\n+    testDirPath = new Path(testDir.getAbsolutePath())\n+  }\n+\n+  after {\n+    Utils.deleteRecursively(testDir)\n+  }\n+\n+  test(\"create EventLogFileWriter with enable/disable rolling\") {\n+    def buildWriterAndVerify(conf: SparkConf, expectedClazz: Class[_]): Unit = {\n+      val writer = EventLogFileWriter(\n+        getUniqueApplicationId, None, testDirPath.toUri, conf,\n+        SparkHadoopUtil.get.newConfiguration(conf))\n+      val writerClazz = writer.getClass\n+      assert(expectedClazz === writerClazz,\n+        s\"default file writer should be $expectedClazz, but $writerClazz\")\n+    }\n+\n+    val conf = new SparkConf\n+    conf.set(EVENT_LOG_ENABLED, true)\n+    conf.set(EVENT_LOG_DIR, testDir.toString)\n+\n+    // default config\n+    buildWriterAndVerify(conf, classOf[SingleEventLogFileWriter])\n+\n+    conf.set(EVENT_LOG_ENABLE_ROLLING, true)\n+    buildWriterAndVerify(conf, classOf[RollingEventLogFilesWriter])\n+\n+    conf.set(EVENT_LOG_ENABLE_ROLLING, false)\n+    buildWriterAndVerify(conf, classOf[SingleEventLogFileWriter])\n+  }\n+\n+  val allCodecs = Seq(None) ++\n+    CompressionCodec.ALL_COMPRESSION_CODECS.map(c => Some(CompressionCodec.getShortName(c)))\n+\n+  allCodecs.foreach { codecShortName =>\n+    test(s\"initialize, write, stop - with codec $codecShortName\") {\n+      val appId = getUniqueApplicationId\n+      val attemptId = None\n+\n+      val conf = getLoggingConf(testDirPath, codecShortName)\n+      val writer = createWriter(appId, attemptId, testDirPath.toUri, conf,\n+        SparkHadoopUtil.get.newConfiguration(conf))\n+\n+      writer.start()\n+\n+      // snappy stream throws exception on empty stream, so we should provide some data to test.\n+      val dummyData = Seq(\"dummy1\", \"dummy2\", \"dummy3\")\n+      dummyData.foreach(writer.writeEvent(_, flushLogger = true))\n+\n+      verifyWriteEventLogFile(appId, attemptId, testDirPath.toUri, codecShortName,\n+        isCompleted = false, dummyData)\n+\n+      writer.stop()\n+\n+      verifyWriteEventLogFile(appId, attemptId, testDirPath.toUri, codecShortName,\n+        isCompleted = true, dummyData)\n+    }\n+  }\n+\n+  test(\"spark.eventLog.compression.codec overrides spark.io.compression.codec\") {\n+    val conf = new SparkConf\n+    conf.set(EVENT_LOG_COMPRESS, true)\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+\n+    val appId = \"test\"\n+    val appAttemptId = None\n+\n+    // The default value is `spark.io.compression.codec`.\n+    val writer = createWriter(appId, appAttemptId, testDirPath.toUri, conf, hadoopConf)\n+    assert(writer.compressionCodecName.contains(\"lz4\"))\n+\n+    // `spark.eventLog.compression.codec` overrides `spark.io.compression.codec`.\n+    conf.set(EVENT_LOG_COMPRESSION_CODEC, \"zstd\")\n+    val writer2 = createWriter(appId, appAttemptId, testDirPath.toUri, conf, hadoopConf)\n+    assert(writer2.compressionCodecName.contains(\"zstd\"))\n+  }\n+\n+  protected def readLinesFromEventLogFile(log: Path, fs: FileSystem): List[String] = {\n+    val logDataStream = EventLogFileReader.openEventLog(log, fs)\n+    try {\n+      Source.fromInputStream(logDataStream).getLines().toList\n+    } finally {\n+      logDataStream.close()\n+    }\n+  }\n+\n+  protected def createWriter(\n+      appId: String,\n+      appAttemptId : Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter\n+\n+  protected def verifyWriteEventLogFile(\n+      appId: String,\n+      appAttemptId : Option[String],\n+      logBaseDir: URI,\n+      compressionCodecShortName: Option[String],\n+      isCompleted: Boolean,\n+      expectedLines: Seq[String] = Seq.empty): Unit\n+\n+  protected def skipVerifyEventLogFile(\n+      compressionCodecShortName: Option[String],\n+      isCompleted: Boolean): Boolean = {\n+    // Spark initializes LZ4BlockOutputStream with syncFlush=false, so we can't force"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "> SHS does the best it can; \r\n\r\nOK, that clarifies everything. Then no strong opinion to keep the test on reading event log file before it's closed. Will remove.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-26T20:16:41Z",
    "diffHunk": "@@ -0,0 +1,400 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.{File, FileOutputStream, IOException}\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import scala.collection.mutable\n+import scala.io.Source\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.{LocalSparkContext, SparkConf, SparkFunSuite}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.history.EventLogTestHelper._\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+\n+abstract class EventLogFileWritersSuite extends SparkFunSuite with LocalSparkContext\n+  with BeforeAndAfter {\n+\n+  protected val fileSystem = Utils.getHadoopFileSystem(\"/\",\n+    SparkHadoopUtil.get.newConfiguration(new SparkConf()))\n+  protected var testDir: File = _\n+  protected var testDirPath: Path = _\n+\n+  before {\n+    testDir = Utils.createTempDir(namePrefix = s\"event log\")\n+    testDir.deleteOnExit()\n+    testDirPath = new Path(testDir.getAbsolutePath())\n+  }\n+\n+  after {\n+    Utils.deleteRecursively(testDir)\n+  }\n+\n+  test(\"create EventLogFileWriter with enable/disable rolling\") {\n+    def buildWriterAndVerify(conf: SparkConf, expectedClazz: Class[_]): Unit = {\n+      val writer = EventLogFileWriter(\n+        getUniqueApplicationId, None, testDirPath.toUri, conf,\n+        SparkHadoopUtil.get.newConfiguration(conf))\n+      val writerClazz = writer.getClass\n+      assert(expectedClazz === writerClazz,\n+        s\"default file writer should be $expectedClazz, but $writerClazz\")\n+    }\n+\n+    val conf = new SparkConf\n+    conf.set(EVENT_LOG_ENABLED, true)\n+    conf.set(EVENT_LOG_DIR, testDir.toString)\n+\n+    // default config\n+    buildWriterAndVerify(conf, classOf[SingleEventLogFileWriter])\n+\n+    conf.set(EVENT_LOG_ENABLE_ROLLING, true)\n+    buildWriterAndVerify(conf, classOf[RollingEventLogFilesWriter])\n+\n+    conf.set(EVENT_LOG_ENABLE_ROLLING, false)\n+    buildWriterAndVerify(conf, classOf[SingleEventLogFileWriter])\n+  }\n+\n+  val allCodecs = Seq(None) ++\n+    CompressionCodec.ALL_COMPRESSION_CODECS.map(c => Some(CompressionCodec.getShortName(c)))\n+\n+  allCodecs.foreach { codecShortName =>\n+    test(s\"initialize, write, stop - with codec $codecShortName\") {\n+      val appId = getUniqueApplicationId\n+      val attemptId = None\n+\n+      val conf = getLoggingConf(testDirPath, codecShortName)\n+      val writer = createWriter(appId, attemptId, testDirPath.toUri, conf,\n+        SparkHadoopUtil.get.newConfiguration(conf))\n+\n+      writer.start()\n+\n+      // snappy stream throws exception on empty stream, so we should provide some data to test.\n+      val dummyData = Seq(\"dummy1\", \"dummy2\", \"dummy3\")\n+      dummyData.foreach(writer.writeEvent(_, flushLogger = true))\n+\n+      verifyWriteEventLogFile(appId, attemptId, testDirPath.toUri, codecShortName,\n+        isCompleted = false, dummyData)\n+\n+      writer.stop()\n+\n+      verifyWriteEventLogFile(appId, attemptId, testDirPath.toUri, codecShortName,\n+        isCompleted = true, dummyData)\n+    }\n+  }\n+\n+  test(\"spark.eventLog.compression.codec overrides spark.io.compression.codec\") {\n+    val conf = new SparkConf\n+    conf.set(EVENT_LOG_COMPRESS, true)\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+\n+    val appId = \"test\"\n+    val appAttemptId = None\n+\n+    // The default value is `spark.io.compression.codec`.\n+    val writer = createWriter(appId, appAttemptId, testDirPath.toUri, conf, hadoopConf)\n+    assert(writer.compressionCodecName.contains(\"lz4\"))\n+\n+    // `spark.eventLog.compression.codec` overrides `spark.io.compression.codec`.\n+    conf.set(EVENT_LOG_COMPRESSION_CODEC, \"zstd\")\n+    val writer2 = createWriter(appId, appAttemptId, testDirPath.toUri, conf, hadoopConf)\n+    assert(writer2.compressionCodecName.contains(\"zstd\"))\n+  }\n+\n+  protected def readLinesFromEventLogFile(log: Path, fs: FileSystem): List[String] = {\n+    val logDataStream = EventLogFileReader.openEventLog(log, fs)\n+    try {\n+      Source.fromInputStream(logDataStream).getLines().toList\n+    } finally {\n+      logDataStream.close()\n+    }\n+  }\n+\n+  protected def createWriter(\n+      appId: String,\n+      appAttemptId : Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter\n+\n+  protected def verifyWriteEventLogFile(\n+      appId: String,\n+      appAttemptId : Option[String],\n+      logBaseDir: URI,\n+      compressionCodecShortName: Option[String],\n+      isCompleted: Boolean,\n+      expectedLines: Seq[String] = Seq.empty): Unit\n+\n+  protected def skipVerifyEventLogFile(\n+      compressionCodecShortName: Option[String],\n+      isCompleted: Boolean): Boolean = {\n+    // Spark initializes LZ4BlockOutputStream with syncFlush=false, so we can't force"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "See comment about this in previous review.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-10-02T17:13:40Z",
    "diffHunk": "@@ -0,0 +1,373 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.{File, FileOutputStream, IOException}\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import scala.collection.mutable\n+import scala.io.Source\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.{LocalSparkContext, SparkConf, SparkFunSuite}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.history.EventLogTestHelper._\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+\n+abstract class EventLogFileWritersSuite extends SparkFunSuite with LocalSparkContext\n+  with BeforeAndAfter {\n+\n+  protected val fileSystem = Utils.getHadoopFileSystem(\"/\",\n+    SparkHadoopUtil.get.newConfiguration(new SparkConf()))\n+  protected var testDir: File = _\n+  protected var testDirPath: Path = _\n+\n+  before {\n+    testDir = Utils.createTempDir(namePrefix = s\"event log\")\n+    testDir.deleteOnExit()"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "\"..., but found $writerClazz\"\r\n\r\nBut really, just `assert(writerClazz === expectedClazz)` gives you the appropriate error message.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-10-02T17:14:18Z",
    "diffHunk": "@@ -0,0 +1,373 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.{File, FileOutputStream, IOException}\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import scala.collection.mutable\n+import scala.io.Source\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.{LocalSparkContext, SparkConf, SparkFunSuite}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.history.EventLogTestHelper._\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+\n+abstract class EventLogFileWritersSuite extends SparkFunSuite with LocalSparkContext\n+  with BeforeAndAfter {\n+\n+  protected val fileSystem = Utils.getHadoopFileSystem(\"/\",\n+    SparkHadoopUtil.get.newConfiguration(new SparkConf()))\n+  protected var testDir: File = _\n+  protected var testDirPath: Path = _\n+\n+  before {\n+    testDir = Utils.createTempDir(namePrefix = s\"event log\")\n+    testDir.deleteOnExit()\n+    testDirPath = new Path(testDir.getAbsolutePath())\n+  }\n+\n+  after {\n+    Utils.deleteRecursively(testDir)\n+  }\n+\n+  test(\"create EventLogFileWriter with enable/disable rolling\") {\n+    def buildWriterAndVerify(conf: SparkConf, expectedClazz: Class[_]): Unit = {\n+      val writer = EventLogFileWriter(\n+        getUniqueApplicationId, None, testDirPath.toUri, conf,\n+        SparkHadoopUtil.get.newConfiguration(conf))\n+      val writerClazz = writer.getClass\n+      assert(expectedClazz === writerClazz,\n+        s\"default file writer should be $expectedClazz, but $writerClazz\")"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Agreed. Will remove hint message.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-10-03T00:51:16Z",
    "diffHunk": "@@ -0,0 +1,373 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.{File, FileOutputStream, IOException}\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import scala.collection.mutable\n+import scala.io.Source\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.{LocalSparkContext, SparkConf, SparkFunSuite}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.history.EventLogTestHelper._\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+\n+abstract class EventLogFileWritersSuite extends SparkFunSuite with LocalSparkContext\n+  with BeforeAndAfter {\n+\n+  protected val fileSystem = Utils.getHadoopFileSystem(\"/\",\n+    SparkHadoopUtil.get.newConfiguration(new SparkConf()))\n+  protected var testDir: File = _\n+  protected var testDirPath: Path = _\n+\n+  before {\n+    testDir = Utils.createTempDir(namePrefix = s\"event log\")\n+    testDir.deleteOnExit()\n+    testDirPath = new Path(testDir.getAbsolutePath())\n+  }\n+\n+  after {\n+    Utils.deleteRecursively(testDir)\n+  }\n+\n+  test(\"create EventLogFileWriter with enable/disable rolling\") {\n+    def buildWriterAndVerify(conf: SparkConf, expectedClazz: Class[_]): Unit = {\n+      val writer = EventLogFileWriter(\n+        getUniqueApplicationId, None, testDirPath.toUri, conf,\n+        SparkHadoopUtil.get.newConfiguration(conf))\n+      val writerClazz = writer.getClass\n+      assert(expectedClazz === writerClazz,\n+        s\"default file writer should be $expectedClazz, but $writerClazz\")"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Very similar to code in the reader test. Could be in `EventLogTestHelper`.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-10-02T17:19:53Z",
    "diffHunk": "@@ -0,0 +1,373 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.{File, FileOutputStream, IOException}\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import scala.collection.mutable\n+import scala.io.Source\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.{LocalSparkContext, SparkConf, SparkFunSuite}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.history.EventLogTestHelper._\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+\n+abstract class EventLogFileWritersSuite extends SparkFunSuite with LocalSparkContext\n+  with BeforeAndAfter {\n+\n+  protected val fileSystem = Utils.getHadoopFileSystem(\"/\",\n+    SparkHadoopUtil.get.newConfiguration(new SparkConf()))\n+  protected var testDir: File = _\n+  protected var testDirPath: Path = _\n+\n+  before {\n+    testDir = Utils.createTempDir(namePrefix = s\"event log\")\n+    testDir.deleteOnExit()\n+    testDirPath = new Path(testDir.getAbsolutePath())\n+  }\n+\n+  after {\n+    Utils.deleteRecursively(testDir)\n+  }\n+\n+  test(\"create EventLogFileWriter with enable/disable rolling\") {\n+    def buildWriterAndVerify(conf: SparkConf, expectedClazz: Class[_]): Unit = {\n+      val writer = EventLogFileWriter(\n+        getUniqueApplicationId, None, testDirPath.toUri, conf,\n+        SparkHadoopUtil.get.newConfiguration(conf))\n+      val writerClazz = writer.getClass\n+      assert(expectedClazz === writerClazz,\n+        s\"default file writer should be $expectedClazz, but $writerClazz\")\n+    }\n+\n+    val conf = new SparkConf\n+    conf.set(EVENT_LOG_ENABLED, true)\n+    conf.set(EVENT_LOG_DIR, testDir.toString)\n+\n+    // default config\n+    buildWriterAndVerify(conf, classOf[SingleEventLogFileWriter])\n+\n+    conf.set(EVENT_LOG_ENABLE_ROLLING, true)\n+    buildWriterAndVerify(conf, classOf[RollingEventLogFilesWriter])\n+\n+    conf.set(EVENT_LOG_ENABLE_ROLLING, false)\n+    buildWriterAndVerify(conf, classOf[SingleEventLogFileWriter])\n+  }\n+\n+  val allCodecs = Seq(None) ++\n+    CompressionCodec.ALL_COMPRESSION_CODECS.map(c => Some(CompressionCodec.getShortName(c)))\n+\n+  allCodecs.foreach { codecShortName =>\n+    test(s\"initialize, write, stop - with codec $codecShortName\") {\n+      val appId = getUniqueApplicationId\n+      val attemptId = None\n+\n+      val conf = getLoggingConf(testDirPath, codecShortName)\n+      val writer = createWriter(appId, attemptId, testDirPath.toUri, conf,\n+        SparkHadoopUtil.get.newConfiguration(conf))\n+\n+      writer.start()\n+\n+      // snappy stream throws exception on empty stream, so we should provide some data to test.\n+      val dummyData = Seq(\"dummy1\", \"dummy2\", \"dummy3\")\n+      dummyData.foreach(writer.writeEvent(_, flushLogger = true))\n+\n+      writer.stop()\n+\n+      verifyWriteEventLogFile(appId, attemptId, testDirPath.toUri, codecShortName, dummyData)\n+    }\n+  }\n+\n+  test(\"spark.eventLog.compression.codec overrides spark.io.compression.codec\") {\n+    val conf = new SparkConf\n+    conf.set(EVENT_LOG_COMPRESS, true)\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+\n+    val appId = \"test\"\n+    val appAttemptId = None\n+\n+    // The default value is `spark.io.compression.codec`.\n+    val writer = createWriter(appId, appAttemptId, testDirPath.toUri, conf, hadoopConf)\n+    assert(writer.compressionCodecName.contains(\"lz4\"))\n+\n+    // `spark.eventLog.compression.codec` overrides `spark.io.compression.codec`.\n+    conf.set(EVENT_LOG_COMPRESSION_CODEC, \"zstd\")\n+    val writer2 = createWriter(appId, appAttemptId, testDirPath.toUri, conf, hadoopConf)\n+    assert(writer2.compressionCodecName.contains(\"zstd\"))\n+  }\n+\n+  protected def readLinesFromEventLogFile(log: Path, fs: FileSystem): List[String] = {\n+    val logDataStream = EventLogFileReader.openEventLog(log, fs)\n+    try {\n+      Source.fromInputStream(logDataStream).getLines().toList\n+    } finally {\n+      logDataStream.close()\n+    }\n+  }\n+\n+  protected def createWriter(\n+      appId: String,\n+      appAttemptId : Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter\n+\n+  /**\n+   * This should be called with \"closed\" event log file; No guarantee on reading event log file\n+   * which is being written, especially the file is compressed. SHS also does the best it can.\n+   */\n+  protected def verifyWriteEventLogFile(\n+      appId: String,\n+      appAttemptId : Option[String],\n+      logBaseDir: URI,\n+      compressionCodecShortName: Option[String],\n+      expectedLines: Seq[String] = Seq.empty): Unit\n+}\n+\n+class SingleEventLogFileWriterSuite extends EventLogFileWritersSuite {\n+\n+  test(\"Log overwriting\") {\n+    val appId = \"test\"\n+    val appAttemptId = None\n+    val logUri = SingleEventLogFileWriter.getLogPath(testDir.toURI, appId, appAttemptId)\n+\n+    val conf = getLoggingConf(testDirPath)\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+    val writer = createWriter(appId, appAttemptId, testDir.toURI, conf, hadoopConf)\n+\n+    val logPath = new Path(logUri).toUri.getPath\n+    writer.start()\n+\n+    val dummyData = Seq(\"dummy1\", \"dummy2\", \"dummy3\")\n+    dummyData.foreach(writer.writeEvent(_, flushLogger = true))\n+\n+    // Create file before writing the event log\n+    new FileOutputStream(new File(logPath)).close()\n+    // Expected IOException, since we haven't enabled log overwrite.\n+    intercept[IOException] { writer.stop() }\n+\n+    // Try again, but enable overwriting.\n+    conf.set(EVENT_LOG_OVERWRITE, true)\n+    val writer2 = createWriter(appId, appAttemptId, testDir.toURI, conf, hadoopConf)\n+    writer2.start()\n+    dummyData.foreach(writer2.writeEvent(_, flushLogger = true))\n+    writer2.stop()\n+  }\n+\n+  test(\"Event log name\") {\n+    val baseDirUri = Utils.resolveURI(\"/base-dir\")\n+    // without compression\n+    assert(s\"${baseDirUri.toString}/app1\" === SingleEventLogFileWriter.getLogPath(\n+      baseDirUri, \"app1\", None, None))\n+    // with compression\n+    assert(s\"${baseDirUri.toString}/app1.lzf\" ===\n+      SingleEventLogFileWriter.getLogPath(baseDirUri, \"app1\", None, Some(\"lzf\")))\n+    // illegal characters in app ID\n+    assert(s\"${baseDirUri.toString}/a-fine-mind_dollar_bills__1\" ===\n+      SingleEventLogFileWriter.getLogPath(baseDirUri,\n+        \"a fine:mind$dollar{bills}.1\", None, None))\n+    // illegal characters in app ID with compression\n+    assert(s\"${baseDirUri.toString}/a-fine-mind_dollar_bills__1.lz4\" ===\n+      SingleEventLogFileWriter.getLogPath(baseDirUri,\n+        \"a fine:mind$dollar{bills}.1\", None, Some(\"lz4\")))\n+  }\n+\n+  override protected def createWriter(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+  }\n+\n+  override protected def verifyWriteEventLogFile(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      compressionCodecShortName: Option[String],\n+      expectedLines: Seq[String]): Unit = {\n+    // read single event log file\n+    val logPath = SingleEventLogFileWriter.getLogPath(logBaseDir, appId, appAttemptId,\n+      compressionCodecShortName)\n+\n+    val finalLogPath = new Path(logPath)\n+    assert(fileSystem.exists(finalLogPath) && fileSystem.isFile(finalLogPath))\n+    assert(expectedLines === readLinesFromEventLogFile(finalLogPath, fileSystem))\n+  }\n+}\n+\n+class RollingEventLogFilesWriterSuite extends EventLogFileWritersSuite {\n+  import RollingEventLogFilesWriter._\n+\n+  test(\"Event log names\") {\n+    val baseDirUri = Utils.resolveURI(\"/base-dir\")\n+    val appId = \"app1\"\n+    val appAttemptId = None\n+\n+    // happy case with app ID\n+    val logDir = RollingEventLogFilesWriter.getAppEventLogDirPath(baseDirUri, appId, None)\n+    assert(s\"${baseDirUri.toString}/eventlog_v2_app1\" === logDir.toString)\n+\n+    // appstatus: inprogress or completed\n+    assert(s\"$logDir/appstatus_app1.inprogress\" ===\n+      RollingEventLogFilesWriter.getAppStatusFilePath(logDir, appId, appAttemptId,\n+        inProgress = true).toString)\n+    assert(s\"$logDir/appstatus_app1\" ===\n+      RollingEventLogFilesWriter.getAppStatusFilePath(logDir, appId, appAttemptId,\n+        inProgress = false).toString)\n+\n+    // without compression\n+    assert(s\"$logDir/events_1_app1\" ===\n+      RollingEventLogFilesWriter.getEventLogFilePath(logDir, appId, appAttemptId, 1, None).toString)\n+\n+    // with compression\n+    assert(s\"$logDir/events_1_app1.lzf\" ===\n+      RollingEventLogFilesWriter.getEventLogFilePath(logDir, appId, appAttemptId,\n+        1, Some(\"lzf\")).toString)\n+\n+    // illegal characters in app ID\n+    assert(s\"${baseDirUri.toString}/eventlog_v2_a-fine-mind_dollar_bills__1\" ===\n+      RollingEventLogFilesWriter.getAppEventLogDirPath(baseDirUri,\n+        \"a fine:mind$dollar{bills}.1\", None).toString)\n+  }\n+\n+  test(\"Log overwriting\") {\n+    val appId = \"test\"\n+    val appAttemptId = None\n+    val logDirPath = RollingEventLogFilesWriter.getAppEventLogDirPath(testDir.toURI, appId,\n+      appAttemptId)\n+\n+    val conf = getLoggingConf(testDirPath)\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+    val writer = createWriter(appId, appAttemptId, testDir.toURI, conf, hadoopConf)\n+\n+    val logPath = logDirPath.toUri.getPath\n+\n+    // Create file before writing the event log directory\n+    // it doesn't matter whether the existing one is file or directory\n+    new FileOutputStream(new File(logPath)).close()\n+\n+    // Expected IOException, since we haven't enabled log overwrite.\n+    // Note that the place IOException is thrown is different from single event log file.\n+    intercept[IOException] { writer.start() }\n+\n+    // Try again, but enable overwriting.\n+    conf.set(EVENT_LOG_OVERWRITE, true)\n+\n+    val writer2 = createWriter(appId, appAttemptId, testDir.toURI, conf, hadoopConf)\n+    writer2.start()\n+    val dummyData = Seq(\"dummy1\", \"dummy2\", \"dummy3\")\n+    dummyData.foreach(writer2.writeEvent(_, flushLogger = true))\n+    writer2.stop()\n+  }\n+\n+  allCodecs.foreach { codecShortName =>\n+    test(s\"rolling event log files - codec $codecShortName\") {\n+      def assertEventLogFilesSequence(\n+          eventLogFiles: Seq[FileStatus],\n+          expectedLastSequence: Int,\n+          expectedMaxSizeBytes: Long): Unit = {\n+        assert(eventLogFiles.forall(f => f.getLen < expectedMaxSizeBytes))\n+        assert((1 to expectedLastSequence) ===\n+          eventLogFiles.map(f => getIndex(f.getPath.getName)))\n+      }\n+\n+      val appId = getUniqueApplicationId\n+      val attemptId = None\n+\n+      val conf = getLoggingConf(testDirPath, codecShortName)\n+      conf.set(EVENT_LOG_ENABLE_ROLLING, true)\n+      conf.set(EVENT_LOG_ROLLED_EVENT_LOG_MAX_FILE_SIZE.key, \"1k\")\n+\n+      val writer = createWriter(appId, attemptId, testDirPath.toUri, conf,\n+        SparkHadoopUtil.get.newConfiguration(conf))\n+\n+      writer.start()\n+\n+      val dummyString = \"dummy\"\n+      val dummyStringBytesLen = dummyString.getBytes(StandardCharsets.UTF_8).length\n+      val expectedLines = mutable.ArrayBuffer[String]()\n+\n+      // write log more than 2k (intended to roll over to 3 files)\n+      val repeatCount = Math.floor((1024 * 2) / dummyStringBytesLen).toInt\n+      (0 until repeatCount).foreach { _ =>\n+        expectedLines.append(dummyString)\n+        writer.writeEvent(dummyString, flushLogger = true)\n+      }"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`.sortBy { fs => ... }`",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-10-02T17:21:12Z",
    "diffHunk": "@@ -0,0 +1,373 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.{File, FileOutputStream, IOException}\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import scala.collection.mutable\n+import scala.io.Source\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.{LocalSparkContext, SparkConf, SparkFunSuite}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.history.EventLogTestHelper._\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+\n+abstract class EventLogFileWritersSuite extends SparkFunSuite with LocalSparkContext\n+  with BeforeAndAfter {\n+\n+  protected val fileSystem = Utils.getHadoopFileSystem(\"/\",\n+    SparkHadoopUtil.get.newConfiguration(new SparkConf()))\n+  protected var testDir: File = _\n+  protected var testDirPath: Path = _\n+\n+  before {\n+    testDir = Utils.createTempDir(namePrefix = s\"event log\")\n+    testDir.deleteOnExit()\n+    testDirPath = new Path(testDir.getAbsolutePath())\n+  }\n+\n+  after {\n+    Utils.deleteRecursively(testDir)\n+  }\n+\n+  test(\"create EventLogFileWriter with enable/disable rolling\") {\n+    def buildWriterAndVerify(conf: SparkConf, expectedClazz: Class[_]): Unit = {\n+      val writer = EventLogFileWriter(\n+        getUniqueApplicationId, None, testDirPath.toUri, conf,\n+        SparkHadoopUtil.get.newConfiguration(conf))\n+      val writerClazz = writer.getClass\n+      assert(expectedClazz === writerClazz,\n+        s\"default file writer should be $expectedClazz, but $writerClazz\")\n+    }\n+\n+    val conf = new SparkConf\n+    conf.set(EVENT_LOG_ENABLED, true)\n+    conf.set(EVENT_LOG_DIR, testDir.toString)\n+\n+    // default config\n+    buildWriterAndVerify(conf, classOf[SingleEventLogFileWriter])\n+\n+    conf.set(EVENT_LOG_ENABLE_ROLLING, true)\n+    buildWriterAndVerify(conf, classOf[RollingEventLogFilesWriter])\n+\n+    conf.set(EVENT_LOG_ENABLE_ROLLING, false)\n+    buildWriterAndVerify(conf, classOf[SingleEventLogFileWriter])\n+  }\n+\n+  val allCodecs = Seq(None) ++\n+    CompressionCodec.ALL_COMPRESSION_CODECS.map(c => Some(CompressionCodec.getShortName(c)))\n+\n+  allCodecs.foreach { codecShortName =>\n+    test(s\"initialize, write, stop - with codec $codecShortName\") {\n+      val appId = getUniqueApplicationId\n+      val attemptId = None\n+\n+      val conf = getLoggingConf(testDirPath, codecShortName)\n+      val writer = createWriter(appId, attemptId, testDirPath.toUri, conf,\n+        SparkHadoopUtil.get.newConfiguration(conf))\n+\n+      writer.start()\n+\n+      // snappy stream throws exception on empty stream, so we should provide some data to test.\n+      val dummyData = Seq(\"dummy1\", \"dummy2\", \"dummy3\")\n+      dummyData.foreach(writer.writeEvent(_, flushLogger = true))\n+\n+      writer.stop()\n+\n+      verifyWriteEventLogFile(appId, attemptId, testDirPath.toUri, codecShortName, dummyData)\n+    }\n+  }\n+\n+  test(\"spark.eventLog.compression.codec overrides spark.io.compression.codec\") {\n+    val conf = new SparkConf\n+    conf.set(EVENT_LOG_COMPRESS, true)\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+\n+    val appId = \"test\"\n+    val appAttemptId = None\n+\n+    // The default value is `spark.io.compression.codec`.\n+    val writer = createWriter(appId, appAttemptId, testDirPath.toUri, conf, hadoopConf)\n+    assert(writer.compressionCodecName.contains(\"lz4\"))\n+\n+    // `spark.eventLog.compression.codec` overrides `spark.io.compression.codec`.\n+    conf.set(EVENT_LOG_COMPRESSION_CODEC, \"zstd\")\n+    val writer2 = createWriter(appId, appAttemptId, testDirPath.toUri, conf, hadoopConf)\n+    assert(writer2.compressionCodecName.contains(\"zstd\"))\n+  }\n+\n+  protected def readLinesFromEventLogFile(log: Path, fs: FileSystem): List[String] = {\n+    val logDataStream = EventLogFileReader.openEventLog(log, fs)\n+    try {\n+      Source.fromInputStream(logDataStream).getLines().toList\n+    } finally {\n+      logDataStream.close()\n+    }\n+  }\n+\n+  protected def createWriter(\n+      appId: String,\n+      appAttemptId : Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter\n+\n+  /**\n+   * This should be called with \"closed\" event log file; No guarantee on reading event log file\n+   * which is being written, especially the file is compressed. SHS also does the best it can.\n+   */\n+  protected def verifyWriteEventLogFile(\n+      appId: String,\n+      appAttemptId : Option[String],\n+      logBaseDir: URI,\n+      compressionCodecShortName: Option[String],\n+      expectedLines: Seq[String] = Seq.empty): Unit\n+}\n+\n+class SingleEventLogFileWriterSuite extends EventLogFileWritersSuite {\n+\n+  test(\"Log overwriting\") {\n+    val appId = \"test\"\n+    val appAttemptId = None\n+    val logUri = SingleEventLogFileWriter.getLogPath(testDir.toURI, appId, appAttemptId)\n+\n+    val conf = getLoggingConf(testDirPath)\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+    val writer = createWriter(appId, appAttemptId, testDir.toURI, conf, hadoopConf)\n+\n+    val logPath = new Path(logUri).toUri.getPath\n+    writer.start()\n+\n+    val dummyData = Seq(\"dummy1\", \"dummy2\", \"dummy3\")\n+    dummyData.foreach(writer.writeEvent(_, flushLogger = true))\n+\n+    // Create file before writing the event log\n+    new FileOutputStream(new File(logPath)).close()\n+    // Expected IOException, since we haven't enabled log overwrite.\n+    intercept[IOException] { writer.stop() }\n+\n+    // Try again, but enable overwriting.\n+    conf.set(EVENT_LOG_OVERWRITE, true)\n+    val writer2 = createWriter(appId, appAttemptId, testDir.toURI, conf, hadoopConf)\n+    writer2.start()\n+    dummyData.foreach(writer2.writeEvent(_, flushLogger = true))\n+    writer2.stop()\n+  }\n+\n+  test(\"Event log name\") {\n+    val baseDirUri = Utils.resolveURI(\"/base-dir\")\n+    // without compression\n+    assert(s\"${baseDirUri.toString}/app1\" === SingleEventLogFileWriter.getLogPath(\n+      baseDirUri, \"app1\", None, None))\n+    // with compression\n+    assert(s\"${baseDirUri.toString}/app1.lzf\" ===\n+      SingleEventLogFileWriter.getLogPath(baseDirUri, \"app1\", None, Some(\"lzf\")))\n+    // illegal characters in app ID\n+    assert(s\"${baseDirUri.toString}/a-fine-mind_dollar_bills__1\" ===\n+      SingleEventLogFileWriter.getLogPath(baseDirUri,\n+        \"a fine:mind$dollar{bills}.1\", None, None))\n+    // illegal characters in app ID with compression\n+    assert(s\"${baseDirUri.toString}/a-fine-mind_dollar_bills__1.lz4\" ===\n+      SingleEventLogFileWriter.getLogPath(baseDirUri,\n+        \"a fine:mind$dollar{bills}.1\", None, Some(\"lz4\")))\n+  }\n+\n+  override protected def createWriter(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+  }\n+\n+  override protected def verifyWriteEventLogFile(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      compressionCodecShortName: Option[String],\n+      expectedLines: Seq[String]): Unit = {\n+    // read single event log file\n+    val logPath = SingleEventLogFileWriter.getLogPath(logBaseDir, appId, appAttemptId,\n+      compressionCodecShortName)\n+\n+    val finalLogPath = new Path(logPath)\n+    assert(fileSystem.exists(finalLogPath) && fileSystem.isFile(finalLogPath))\n+    assert(expectedLines === readLinesFromEventLogFile(finalLogPath, fileSystem))\n+  }\n+}\n+\n+class RollingEventLogFilesWriterSuite extends EventLogFileWritersSuite {\n+  import RollingEventLogFilesWriter._\n+\n+  test(\"Event log names\") {\n+    val baseDirUri = Utils.resolveURI(\"/base-dir\")\n+    val appId = \"app1\"\n+    val appAttemptId = None\n+\n+    // happy case with app ID\n+    val logDir = RollingEventLogFilesWriter.getAppEventLogDirPath(baseDirUri, appId, None)\n+    assert(s\"${baseDirUri.toString}/eventlog_v2_app1\" === logDir.toString)\n+\n+    // appstatus: inprogress or completed\n+    assert(s\"$logDir/appstatus_app1.inprogress\" ===\n+      RollingEventLogFilesWriter.getAppStatusFilePath(logDir, appId, appAttemptId,\n+        inProgress = true).toString)\n+    assert(s\"$logDir/appstatus_app1\" ===\n+      RollingEventLogFilesWriter.getAppStatusFilePath(logDir, appId, appAttemptId,\n+        inProgress = false).toString)\n+\n+    // without compression\n+    assert(s\"$logDir/events_1_app1\" ===\n+      RollingEventLogFilesWriter.getEventLogFilePath(logDir, appId, appAttemptId, 1, None).toString)\n+\n+    // with compression\n+    assert(s\"$logDir/events_1_app1.lzf\" ===\n+      RollingEventLogFilesWriter.getEventLogFilePath(logDir, appId, appAttemptId,\n+        1, Some(\"lzf\")).toString)\n+\n+    // illegal characters in app ID\n+    assert(s\"${baseDirUri.toString}/eventlog_v2_a-fine-mind_dollar_bills__1\" ===\n+      RollingEventLogFilesWriter.getAppEventLogDirPath(baseDirUri,\n+        \"a fine:mind$dollar{bills}.1\", None).toString)\n+  }\n+\n+  test(\"Log overwriting\") {\n+    val appId = \"test\"\n+    val appAttemptId = None\n+    val logDirPath = RollingEventLogFilesWriter.getAppEventLogDirPath(testDir.toURI, appId,\n+      appAttemptId)\n+\n+    val conf = getLoggingConf(testDirPath)\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+    val writer = createWriter(appId, appAttemptId, testDir.toURI, conf, hadoopConf)\n+\n+    val logPath = logDirPath.toUri.getPath\n+\n+    // Create file before writing the event log directory\n+    // it doesn't matter whether the existing one is file or directory\n+    new FileOutputStream(new File(logPath)).close()\n+\n+    // Expected IOException, since we haven't enabled log overwrite.\n+    // Note that the place IOException is thrown is different from single event log file.\n+    intercept[IOException] { writer.start() }\n+\n+    // Try again, but enable overwriting.\n+    conf.set(EVENT_LOG_OVERWRITE, true)\n+\n+    val writer2 = createWriter(appId, appAttemptId, testDir.toURI, conf, hadoopConf)\n+    writer2.start()\n+    val dummyData = Seq(\"dummy1\", \"dummy2\", \"dummy3\")\n+    dummyData.foreach(writer2.writeEvent(_, flushLogger = true))\n+    writer2.stop()\n+  }\n+\n+  allCodecs.foreach { codecShortName =>\n+    test(s\"rolling event log files - codec $codecShortName\") {\n+      def assertEventLogFilesSequence(\n+          eventLogFiles: Seq[FileStatus],\n+          expectedLastSequence: Int,\n+          expectedMaxSizeBytes: Long): Unit = {\n+        assert(eventLogFiles.forall(f => f.getLen < expectedMaxSizeBytes))\n+        assert((1 to expectedLastSequence) ===\n+          eventLogFiles.map(f => getIndex(f.getPath.getName)))\n+      }\n+\n+      val appId = getUniqueApplicationId\n+      val attemptId = None\n+\n+      val conf = getLoggingConf(testDirPath, codecShortName)\n+      conf.set(EVENT_LOG_ENABLE_ROLLING, true)\n+      conf.set(EVENT_LOG_ROLLED_EVENT_LOG_MAX_FILE_SIZE.key, \"1k\")\n+\n+      val writer = createWriter(appId, attemptId, testDirPath.toUri, conf,\n+        SparkHadoopUtil.get.newConfiguration(conf))\n+\n+      writer.start()\n+\n+      val dummyString = \"dummy\"\n+      val dummyStringBytesLen = dummyString.getBytes(StandardCharsets.UTF_8).length\n+      val expectedLines = mutable.ArrayBuffer[String]()\n+\n+      // write log more than 2k (intended to roll over to 3 files)\n+      val repeatCount = Math.floor((1024 * 2) / dummyStringBytesLen).toInt\n+      (0 until repeatCount).foreach { _ =>\n+        expectedLines.append(dummyString)\n+        writer.writeEvent(dummyString, flushLogger = true)\n+      }\n+\n+      val logDirPath = getAppEventLogDirPath(testDirPath.toUri, appId, attemptId)\n+\n+      val eventLogFiles = listEventLogFiles(logDirPath)\n+      assertEventLogFilesSequence(eventLogFiles, 3, 1024 * 1024)\n+\n+      writer.stop()\n+\n+      val eventLogFiles2 = listEventLogFiles(logDirPath)\n+      assertEventLogFilesSequence(eventLogFiles2, 3, 1024 * 1024)\n+\n+      verifyWriteEventLogFile(appId, attemptId, testDirPath.toUri,\n+        codecShortName, expectedLines)\n+    }\n+  }\n+\n+  override protected def createWriter(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+  }\n+\n+  override protected def verifyWriteEventLogFile(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      compressionCodecShortName: Option[String],\n+      expectedLines: Seq[String]): Unit = {\n+    val logDirPath = getAppEventLogDirPath(logBaseDir, appId, appAttemptId)\n+\n+    assert(fileSystem.exists(logDirPath) && fileSystem.isDirectory(logDirPath))\n+\n+    val appStatusFile = getAppStatusFilePath(logDirPath, appId, appAttemptId, inProgress = false)\n+    assert(fileSystem.exists(appStatusFile) && fileSystem.isFile(appStatusFile))\n+\n+    val eventLogFiles = listEventLogFiles(logDirPath)\n+    val allLines = mutable.ArrayBuffer[String]()\n+    eventLogFiles.foreach { file =>\n+      allLines.appendAll(readLinesFromEventLogFile(file.getPath, fileSystem))\n+    }\n+\n+    assert(expectedLines === allLines)\n+  }\n+\n+  private def listEventLogFiles(logDirPath: Path): Seq[FileStatus] = {\n+    fileSystem.listStatus(logDirPath).filter(isEventLogFile)\n+      .sortBy(fs => getIndex(fs.getPath.getName))"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "Ngone51"
    },
    "body": "Hmmm, usually, I'd suggest to reuse those prefix variables here and below.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-10-03T15:11:14Z",
    "diffHunk": "@@ -0,0 +1,373 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.{File, FileOutputStream, IOException}\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import scala.collection.mutable\n+import scala.io.Source\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.{LocalSparkContext, SparkConf, SparkFunSuite}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.history.EventLogTestHelper._\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+\n+abstract class EventLogFileWritersSuite extends SparkFunSuite with LocalSparkContext\n+  with BeforeAndAfter {\n+\n+  protected val fileSystem = Utils.getHadoopFileSystem(\"/\",\n+    SparkHadoopUtil.get.newConfiguration(new SparkConf()))\n+  protected var testDir: File = _\n+  protected var testDirPath: Path = _\n+\n+  before {\n+    testDir = Utils.createTempDir(namePrefix = s\"event log\")\n+    testDir.deleteOnExit()\n+    testDirPath = new Path(testDir.getAbsolutePath())\n+  }\n+\n+  after {\n+    Utils.deleteRecursively(testDir)\n+  }\n+\n+  test(\"create EventLogFileWriter with enable/disable rolling\") {\n+    def buildWriterAndVerify(conf: SparkConf, expectedClazz: Class[_]): Unit = {\n+      val writer = EventLogFileWriter(\n+        getUniqueApplicationId, None, testDirPath.toUri, conf,\n+        SparkHadoopUtil.get.newConfiguration(conf))\n+      val writerClazz = writer.getClass\n+      assert(expectedClazz === writerClazz,\n+        s\"default file writer should be $expectedClazz, but $writerClazz\")\n+    }\n+\n+    val conf = new SparkConf\n+    conf.set(EVENT_LOG_ENABLED, true)\n+    conf.set(EVENT_LOG_DIR, testDir.toString)\n+\n+    // default config\n+    buildWriterAndVerify(conf, classOf[SingleEventLogFileWriter])\n+\n+    conf.set(EVENT_LOG_ENABLE_ROLLING, true)\n+    buildWriterAndVerify(conf, classOf[RollingEventLogFilesWriter])\n+\n+    conf.set(EVENT_LOG_ENABLE_ROLLING, false)\n+    buildWriterAndVerify(conf, classOf[SingleEventLogFileWriter])\n+  }\n+\n+  val allCodecs = Seq(None) ++\n+    CompressionCodec.ALL_COMPRESSION_CODECS.map(c => Some(CompressionCodec.getShortName(c)))\n+\n+  allCodecs.foreach { codecShortName =>\n+    test(s\"initialize, write, stop - with codec $codecShortName\") {\n+      val appId = getUniqueApplicationId\n+      val attemptId = None\n+\n+      val conf = getLoggingConf(testDirPath, codecShortName)\n+      val writer = createWriter(appId, attemptId, testDirPath.toUri, conf,\n+        SparkHadoopUtil.get.newConfiguration(conf))\n+\n+      writer.start()\n+\n+      // snappy stream throws exception on empty stream, so we should provide some data to test.\n+      val dummyData = Seq(\"dummy1\", \"dummy2\", \"dummy3\")\n+      dummyData.foreach(writer.writeEvent(_, flushLogger = true))\n+\n+      writer.stop()\n+\n+      verifyWriteEventLogFile(appId, attemptId, testDirPath.toUri, codecShortName, dummyData)\n+    }\n+  }\n+\n+  test(\"spark.eventLog.compression.codec overrides spark.io.compression.codec\") {\n+    val conf = new SparkConf\n+    conf.set(EVENT_LOG_COMPRESS, true)\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+\n+    val appId = \"test\"\n+    val appAttemptId = None\n+\n+    // The default value is `spark.io.compression.codec`.\n+    val writer = createWriter(appId, appAttemptId, testDirPath.toUri, conf, hadoopConf)\n+    assert(writer.compressionCodecName.contains(\"lz4\"))\n+\n+    // `spark.eventLog.compression.codec` overrides `spark.io.compression.codec`.\n+    conf.set(EVENT_LOG_COMPRESSION_CODEC, \"zstd\")\n+    val writer2 = createWriter(appId, appAttemptId, testDirPath.toUri, conf, hadoopConf)\n+    assert(writer2.compressionCodecName.contains(\"zstd\"))\n+  }\n+\n+  protected def readLinesFromEventLogFile(log: Path, fs: FileSystem): List[String] = {\n+    val logDataStream = EventLogFileReader.openEventLog(log, fs)\n+    try {\n+      Source.fromInputStream(logDataStream).getLines().toList\n+    } finally {\n+      logDataStream.close()\n+    }\n+  }\n+\n+  protected def createWriter(\n+      appId: String,\n+      appAttemptId : Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter\n+\n+  /**\n+   * This should be called with \"closed\" event log file; No guarantee on reading event log file\n+   * which is being written, especially the file is compressed. SHS also does the best it can.\n+   */\n+  protected def verifyWriteEventLogFile(\n+      appId: String,\n+      appAttemptId : Option[String],\n+      logBaseDir: URI,\n+      compressionCodecShortName: Option[String],\n+      expectedLines: Seq[String] = Seq.empty): Unit\n+}\n+\n+class SingleEventLogFileWriterSuite extends EventLogFileWritersSuite {\n+\n+  test(\"Log overwriting\") {\n+    val appId = \"test\"\n+    val appAttemptId = None\n+    val logUri = SingleEventLogFileWriter.getLogPath(testDir.toURI, appId, appAttemptId)\n+\n+    val conf = getLoggingConf(testDirPath)\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+    val writer = createWriter(appId, appAttemptId, testDir.toURI, conf, hadoopConf)\n+\n+    val logPath = new Path(logUri).toUri.getPath\n+    writer.start()\n+\n+    val dummyData = Seq(\"dummy1\", \"dummy2\", \"dummy3\")\n+    dummyData.foreach(writer.writeEvent(_, flushLogger = true))\n+\n+    // Create file before writing the event log\n+    new FileOutputStream(new File(logPath)).close()\n+    // Expected IOException, since we haven't enabled log overwrite.\n+    intercept[IOException] { writer.stop() }\n+\n+    // Try again, but enable overwriting.\n+    conf.set(EVENT_LOG_OVERWRITE, true)\n+    val writer2 = createWriter(appId, appAttemptId, testDir.toURI, conf, hadoopConf)\n+    writer2.start()\n+    dummyData.foreach(writer2.writeEvent(_, flushLogger = true))\n+    writer2.stop()\n+  }\n+\n+  test(\"Event log name\") {\n+    val baseDirUri = Utils.resolveURI(\"/base-dir\")\n+    // without compression\n+    assert(s\"${baseDirUri.toString}/app1\" === SingleEventLogFileWriter.getLogPath(\n+      baseDirUri, \"app1\", None, None))\n+    // with compression\n+    assert(s\"${baseDirUri.toString}/app1.lzf\" ===\n+      SingleEventLogFileWriter.getLogPath(baseDirUri, \"app1\", None, Some(\"lzf\")))\n+    // illegal characters in app ID\n+    assert(s\"${baseDirUri.toString}/a-fine-mind_dollar_bills__1\" ===\n+      SingleEventLogFileWriter.getLogPath(baseDirUri,\n+        \"a fine:mind$dollar{bills}.1\", None, None))\n+    // illegal characters in app ID with compression\n+    assert(s\"${baseDirUri.toString}/a-fine-mind_dollar_bills__1.lz4\" ===\n+      SingleEventLogFileWriter.getLogPath(baseDirUri,\n+        \"a fine:mind$dollar{bills}.1\", None, Some(\"lz4\")))\n+  }\n+\n+  override protected def createWriter(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+  }\n+\n+  override protected def verifyWriteEventLogFile(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      compressionCodecShortName: Option[String],\n+      expectedLines: Seq[String]): Unit = {\n+    // read single event log file\n+    val logPath = SingleEventLogFileWriter.getLogPath(logBaseDir, appId, appAttemptId,\n+      compressionCodecShortName)\n+\n+    val finalLogPath = new Path(logPath)\n+    assert(fileSystem.exists(finalLogPath) && fileSystem.isFile(finalLogPath))\n+    assert(expectedLines === readLinesFromEventLogFile(finalLogPath, fileSystem))\n+  }\n+}\n+\n+class RollingEventLogFilesWriterSuite extends EventLogFileWritersSuite {\n+  import RollingEventLogFilesWriter._\n+\n+  test(\"Event log names\") {\n+    val baseDirUri = Utils.resolveURI(\"/base-dir\")\n+    val appId = \"app1\"\n+    val appAttemptId = None\n+\n+    // happy case with app ID\n+    val logDir = RollingEventLogFilesWriter.getAppEventLogDirPath(baseDirUri, appId, None)\n+    assert(s\"${baseDirUri.toString}/eventlog_v2_app1\" === logDir.toString)"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "That sounds like as a preference as I've got review comments from other PRs that keep the full text in test. But, until someone jumps in I'm OK to make a change. I'll change it.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-10-03T20:52:51Z",
    "diffHunk": "@@ -0,0 +1,373 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.{File, FileOutputStream, IOException}\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import scala.collection.mutable\n+import scala.io.Source\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.{LocalSparkContext, SparkConf, SparkFunSuite}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.history.EventLogTestHelper._\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+\n+abstract class EventLogFileWritersSuite extends SparkFunSuite with LocalSparkContext\n+  with BeforeAndAfter {\n+\n+  protected val fileSystem = Utils.getHadoopFileSystem(\"/\",\n+    SparkHadoopUtil.get.newConfiguration(new SparkConf()))\n+  protected var testDir: File = _\n+  protected var testDirPath: Path = _\n+\n+  before {\n+    testDir = Utils.createTempDir(namePrefix = s\"event log\")\n+    testDir.deleteOnExit()\n+    testDirPath = new Path(testDir.getAbsolutePath())\n+  }\n+\n+  after {\n+    Utils.deleteRecursively(testDir)\n+  }\n+\n+  test(\"create EventLogFileWriter with enable/disable rolling\") {\n+    def buildWriterAndVerify(conf: SparkConf, expectedClazz: Class[_]): Unit = {\n+      val writer = EventLogFileWriter(\n+        getUniqueApplicationId, None, testDirPath.toUri, conf,\n+        SparkHadoopUtil.get.newConfiguration(conf))\n+      val writerClazz = writer.getClass\n+      assert(expectedClazz === writerClazz,\n+        s\"default file writer should be $expectedClazz, but $writerClazz\")\n+    }\n+\n+    val conf = new SparkConf\n+    conf.set(EVENT_LOG_ENABLED, true)\n+    conf.set(EVENT_LOG_DIR, testDir.toString)\n+\n+    // default config\n+    buildWriterAndVerify(conf, classOf[SingleEventLogFileWriter])\n+\n+    conf.set(EVENT_LOG_ENABLE_ROLLING, true)\n+    buildWriterAndVerify(conf, classOf[RollingEventLogFilesWriter])\n+\n+    conf.set(EVENT_LOG_ENABLE_ROLLING, false)\n+    buildWriterAndVerify(conf, classOf[SingleEventLogFileWriter])\n+  }\n+\n+  val allCodecs = Seq(None) ++\n+    CompressionCodec.ALL_COMPRESSION_CODECS.map(c => Some(CompressionCodec.getShortName(c)))\n+\n+  allCodecs.foreach { codecShortName =>\n+    test(s\"initialize, write, stop - with codec $codecShortName\") {\n+      val appId = getUniqueApplicationId\n+      val attemptId = None\n+\n+      val conf = getLoggingConf(testDirPath, codecShortName)\n+      val writer = createWriter(appId, attemptId, testDirPath.toUri, conf,\n+        SparkHadoopUtil.get.newConfiguration(conf))\n+\n+      writer.start()\n+\n+      // snappy stream throws exception on empty stream, so we should provide some data to test.\n+      val dummyData = Seq(\"dummy1\", \"dummy2\", \"dummy3\")\n+      dummyData.foreach(writer.writeEvent(_, flushLogger = true))\n+\n+      writer.stop()\n+\n+      verifyWriteEventLogFile(appId, attemptId, testDirPath.toUri, codecShortName, dummyData)\n+    }\n+  }\n+\n+  test(\"spark.eventLog.compression.codec overrides spark.io.compression.codec\") {\n+    val conf = new SparkConf\n+    conf.set(EVENT_LOG_COMPRESS, true)\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+\n+    val appId = \"test\"\n+    val appAttemptId = None\n+\n+    // The default value is `spark.io.compression.codec`.\n+    val writer = createWriter(appId, appAttemptId, testDirPath.toUri, conf, hadoopConf)\n+    assert(writer.compressionCodecName.contains(\"lz4\"))\n+\n+    // `spark.eventLog.compression.codec` overrides `spark.io.compression.codec`.\n+    conf.set(EVENT_LOG_COMPRESSION_CODEC, \"zstd\")\n+    val writer2 = createWriter(appId, appAttemptId, testDirPath.toUri, conf, hadoopConf)\n+    assert(writer2.compressionCodecName.contains(\"zstd\"))\n+  }\n+\n+  protected def readLinesFromEventLogFile(log: Path, fs: FileSystem): List[String] = {\n+    val logDataStream = EventLogFileReader.openEventLog(log, fs)\n+    try {\n+      Source.fromInputStream(logDataStream).getLines().toList\n+    } finally {\n+      logDataStream.close()\n+    }\n+  }\n+\n+  protected def createWriter(\n+      appId: String,\n+      appAttemptId : Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter\n+\n+  /**\n+   * This should be called with \"closed\" event log file; No guarantee on reading event log file\n+   * which is being written, especially the file is compressed. SHS also does the best it can.\n+   */\n+  protected def verifyWriteEventLogFile(\n+      appId: String,\n+      appAttemptId : Option[String],\n+      logBaseDir: URI,\n+      compressionCodecShortName: Option[String],\n+      expectedLines: Seq[String] = Seq.empty): Unit\n+}\n+\n+class SingleEventLogFileWriterSuite extends EventLogFileWritersSuite {\n+\n+  test(\"Log overwriting\") {\n+    val appId = \"test\"\n+    val appAttemptId = None\n+    val logUri = SingleEventLogFileWriter.getLogPath(testDir.toURI, appId, appAttemptId)\n+\n+    val conf = getLoggingConf(testDirPath)\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+    val writer = createWriter(appId, appAttemptId, testDir.toURI, conf, hadoopConf)\n+\n+    val logPath = new Path(logUri).toUri.getPath\n+    writer.start()\n+\n+    val dummyData = Seq(\"dummy1\", \"dummy2\", \"dummy3\")\n+    dummyData.foreach(writer.writeEvent(_, flushLogger = true))\n+\n+    // Create file before writing the event log\n+    new FileOutputStream(new File(logPath)).close()\n+    // Expected IOException, since we haven't enabled log overwrite.\n+    intercept[IOException] { writer.stop() }\n+\n+    // Try again, but enable overwriting.\n+    conf.set(EVENT_LOG_OVERWRITE, true)\n+    val writer2 = createWriter(appId, appAttemptId, testDir.toURI, conf, hadoopConf)\n+    writer2.start()\n+    dummyData.foreach(writer2.writeEvent(_, flushLogger = true))\n+    writer2.stop()\n+  }\n+\n+  test(\"Event log name\") {\n+    val baseDirUri = Utils.resolveURI(\"/base-dir\")\n+    // without compression\n+    assert(s\"${baseDirUri.toString}/app1\" === SingleEventLogFileWriter.getLogPath(\n+      baseDirUri, \"app1\", None, None))\n+    // with compression\n+    assert(s\"${baseDirUri.toString}/app1.lzf\" ===\n+      SingleEventLogFileWriter.getLogPath(baseDirUri, \"app1\", None, Some(\"lzf\")))\n+    // illegal characters in app ID\n+    assert(s\"${baseDirUri.toString}/a-fine-mind_dollar_bills__1\" ===\n+      SingleEventLogFileWriter.getLogPath(baseDirUri,\n+        \"a fine:mind$dollar{bills}.1\", None, None))\n+    // illegal characters in app ID with compression\n+    assert(s\"${baseDirUri.toString}/a-fine-mind_dollar_bills__1.lz4\" ===\n+      SingleEventLogFileWriter.getLogPath(baseDirUri,\n+        \"a fine:mind$dollar{bills}.1\", None, Some(\"lz4\")))\n+  }\n+\n+  override protected def createWriter(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+  }\n+\n+  override protected def verifyWriteEventLogFile(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      compressionCodecShortName: Option[String],\n+      expectedLines: Seq[String]): Unit = {\n+    // read single event log file\n+    val logPath = SingleEventLogFileWriter.getLogPath(logBaseDir, appId, appAttemptId,\n+      compressionCodecShortName)\n+\n+    val finalLogPath = new Path(logPath)\n+    assert(fileSystem.exists(finalLogPath) && fileSystem.isFile(finalLogPath))\n+    assert(expectedLines === readLinesFromEventLogFile(finalLogPath, fileSystem))\n+  }\n+}\n+\n+class RollingEventLogFilesWriterSuite extends EventLogFileWritersSuite {\n+  import RollingEventLogFilesWriter._\n+\n+  test(\"Event log names\") {\n+    val baseDirUri = Utils.resolveURI(\"/base-dir\")\n+    val appId = \"app1\"\n+    val appAttemptId = None\n+\n+    // happy case with app ID\n+    val logDir = RollingEventLogFilesWriter.getAppEventLogDirPath(baseDirUri, appId, None)\n+    assert(s\"${baseDirUri.toString}/eventlog_v2_app1\" === logDir.toString)"
  }, {
    "author": {
      "login": "Ngone51"
    },
    "body": "Thanks.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-10-08T15:55:07Z",
    "diffHunk": "@@ -0,0 +1,373 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.{File, FileOutputStream, IOException}\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import scala.collection.mutable\n+import scala.io.Source\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.{LocalSparkContext, SparkConf, SparkFunSuite}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.history.EventLogTestHelper._\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+\n+abstract class EventLogFileWritersSuite extends SparkFunSuite with LocalSparkContext\n+  with BeforeAndAfter {\n+\n+  protected val fileSystem = Utils.getHadoopFileSystem(\"/\",\n+    SparkHadoopUtil.get.newConfiguration(new SparkConf()))\n+  protected var testDir: File = _\n+  protected var testDirPath: Path = _\n+\n+  before {\n+    testDir = Utils.createTempDir(namePrefix = s\"event log\")\n+    testDir.deleteOnExit()\n+    testDirPath = new Path(testDir.getAbsolutePath())\n+  }\n+\n+  after {\n+    Utils.deleteRecursively(testDir)\n+  }\n+\n+  test(\"create EventLogFileWriter with enable/disable rolling\") {\n+    def buildWriterAndVerify(conf: SparkConf, expectedClazz: Class[_]): Unit = {\n+      val writer = EventLogFileWriter(\n+        getUniqueApplicationId, None, testDirPath.toUri, conf,\n+        SparkHadoopUtil.get.newConfiguration(conf))\n+      val writerClazz = writer.getClass\n+      assert(expectedClazz === writerClazz,\n+        s\"default file writer should be $expectedClazz, but $writerClazz\")\n+    }\n+\n+    val conf = new SparkConf\n+    conf.set(EVENT_LOG_ENABLED, true)\n+    conf.set(EVENT_LOG_DIR, testDir.toString)\n+\n+    // default config\n+    buildWriterAndVerify(conf, classOf[SingleEventLogFileWriter])\n+\n+    conf.set(EVENT_LOG_ENABLE_ROLLING, true)\n+    buildWriterAndVerify(conf, classOf[RollingEventLogFilesWriter])\n+\n+    conf.set(EVENT_LOG_ENABLE_ROLLING, false)\n+    buildWriterAndVerify(conf, classOf[SingleEventLogFileWriter])\n+  }\n+\n+  val allCodecs = Seq(None) ++\n+    CompressionCodec.ALL_COMPRESSION_CODECS.map(c => Some(CompressionCodec.getShortName(c)))\n+\n+  allCodecs.foreach { codecShortName =>\n+    test(s\"initialize, write, stop - with codec $codecShortName\") {\n+      val appId = getUniqueApplicationId\n+      val attemptId = None\n+\n+      val conf = getLoggingConf(testDirPath, codecShortName)\n+      val writer = createWriter(appId, attemptId, testDirPath.toUri, conf,\n+        SparkHadoopUtil.get.newConfiguration(conf))\n+\n+      writer.start()\n+\n+      // snappy stream throws exception on empty stream, so we should provide some data to test.\n+      val dummyData = Seq(\"dummy1\", \"dummy2\", \"dummy3\")\n+      dummyData.foreach(writer.writeEvent(_, flushLogger = true))\n+\n+      writer.stop()\n+\n+      verifyWriteEventLogFile(appId, attemptId, testDirPath.toUri, codecShortName, dummyData)\n+    }\n+  }\n+\n+  test(\"spark.eventLog.compression.codec overrides spark.io.compression.codec\") {\n+    val conf = new SparkConf\n+    conf.set(EVENT_LOG_COMPRESS, true)\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+\n+    val appId = \"test\"\n+    val appAttemptId = None\n+\n+    // The default value is `spark.io.compression.codec`.\n+    val writer = createWriter(appId, appAttemptId, testDirPath.toUri, conf, hadoopConf)\n+    assert(writer.compressionCodecName.contains(\"lz4\"))\n+\n+    // `spark.eventLog.compression.codec` overrides `spark.io.compression.codec`.\n+    conf.set(EVENT_LOG_COMPRESSION_CODEC, \"zstd\")\n+    val writer2 = createWriter(appId, appAttemptId, testDirPath.toUri, conf, hadoopConf)\n+    assert(writer2.compressionCodecName.contains(\"zstd\"))\n+  }\n+\n+  protected def readLinesFromEventLogFile(log: Path, fs: FileSystem): List[String] = {\n+    val logDataStream = EventLogFileReader.openEventLog(log, fs)\n+    try {\n+      Source.fromInputStream(logDataStream).getLines().toList\n+    } finally {\n+      logDataStream.close()\n+    }\n+  }\n+\n+  protected def createWriter(\n+      appId: String,\n+      appAttemptId : Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter\n+\n+  /**\n+   * This should be called with \"closed\" event log file; No guarantee on reading event log file\n+   * which is being written, especially the file is compressed. SHS also does the best it can.\n+   */\n+  protected def verifyWriteEventLogFile(\n+      appId: String,\n+      appAttemptId : Option[String],\n+      logBaseDir: URI,\n+      compressionCodecShortName: Option[String],\n+      expectedLines: Seq[String] = Seq.empty): Unit\n+}\n+\n+class SingleEventLogFileWriterSuite extends EventLogFileWritersSuite {\n+\n+  test(\"Log overwriting\") {\n+    val appId = \"test\"\n+    val appAttemptId = None\n+    val logUri = SingleEventLogFileWriter.getLogPath(testDir.toURI, appId, appAttemptId)\n+\n+    val conf = getLoggingConf(testDirPath)\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+    val writer = createWriter(appId, appAttemptId, testDir.toURI, conf, hadoopConf)\n+\n+    val logPath = new Path(logUri).toUri.getPath\n+    writer.start()\n+\n+    val dummyData = Seq(\"dummy1\", \"dummy2\", \"dummy3\")\n+    dummyData.foreach(writer.writeEvent(_, flushLogger = true))\n+\n+    // Create file before writing the event log\n+    new FileOutputStream(new File(logPath)).close()\n+    // Expected IOException, since we haven't enabled log overwrite.\n+    intercept[IOException] { writer.stop() }\n+\n+    // Try again, but enable overwriting.\n+    conf.set(EVENT_LOG_OVERWRITE, true)\n+    val writer2 = createWriter(appId, appAttemptId, testDir.toURI, conf, hadoopConf)\n+    writer2.start()\n+    dummyData.foreach(writer2.writeEvent(_, flushLogger = true))\n+    writer2.stop()\n+  }\n+\n+  test(\"Event log name\") {\n+    val baseDirUri = Utils.resolveURI(\"/base-dir\")\n+    // without compression\n+    assert(s\"${baseDirUri.toString}/app1\" === SingleEventLogFileWriter.getLogPath(\n+      baseDirUri, \"app1\", None, None))\n+    // with compression\n+    assert(s\"${baseDirUri.toString}/app1.lzf\" ===\n+      SingleEventLogFileWriter.getLogPath(baseDirUri, \"app1\", None, Some(\"lzf\")))\n+    // illegal characters in app ID\n+    assert(s\"${baseDirUri.toString}/a-fine-mind_dollar_bills__1\" ===\n+      SingleEventLogFileWriter.getLogPath(baseDirUri,\n+        \"a fine:mind$dollar{bills}.1\", None, None))\n+    // illegal characters in app ID with compression\n+    assert(s\"${baseDirUri.toString}/a-fine-mind_dollar_bills__1.lz4\" ===\n+      SingleEventLogFileWriter.getLogPath(baseDirUri,\n+        \"a fine:mind$dollar{bills}.1\", None, Some(\"lz4\")))\n+  }\n+\n+  override protected def createWriter(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+  }\n+\n+  override protected def verifyWriteEventLogFile(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      compressionCodecShortName: Option[String],\n+      expectedLines: Seq[String]): Unit = {\n+    // read single event log file\n+    val logPath = SingleEventLogFileWriter.getLogPath(logBaseDir, appId, appAttemptId,\n+      compressionCodecShortName)\n+\n+    val finalLogPath = new Path(logPath)\n+    assert(fileSystem.exists(finalLogPath) && fileSystem.isFile(finalLogPath))\n+    assert(expectedLines === readLinesFromEventLogFile(finalLogPath, fileSystem))\n+  }\n+}\n+\n+class RollingEventLogFilesWriterSuite extends EventLogFileWritersSuite {\n+  import RollingEventLogFilesWriter._\n+\n+  test(\"Event log names\") {\n+    val baseDirUri = Utils.resolveURI(\"/base-dir\")\n+    val appId = \"app1\"\n+    val appAttemptId = None\n+\n+    // happy case with app ID\n+    val logDir = RollingEventLogFilesWriter.getAppEventLogDirPath(baseDirUri, appId, None)\n+    assert(s\"${baseDirUri.toString}/eventlog_v2_app1\" === logDir.toString)"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "Ngone51"
    },
    "body": "Theoretically,  `f.getLen == expectedMaxSizeBytes` is also possible ? Since I remember that we do roll up log file when `curFileLen + curEventLen > maxLen`, right ?",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-10-03T15:18:52Z",
    "diffHunk": "@@ -0,0 +1,373 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.{File, FileOutputStream, IOException}\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import scala.collection.mutable\n+import scala.io.Source\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.{LocalSparkContext, SparkConf, SparkFunSuite}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.history.EventLogTestHelper._\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+\n+abstract class EventLogFileWritersSuite extends SparkFunSuite with LocalSparkContext\n+  with BeforeAndAfter {\n+\n+  protected val fileSystem = Utils.getHadoopFileSystem(\"/\",\n+    SparkHadoopUtil.get.newConfiguration(new SparkConf()))\n+  protected var testDir: File = _\n+  protected var testDirPath: Path = _\n+\n+  before {\n+    testDir = Utils.createTempDir(namePrefix = s\"event log\")\n+    testDir.deleteOnExit()\n+    testDirPath = new Path(testDir.getAbsolutePath())\n+  }\n+\n+  after {\n+    Utils.deleteRecursively(testDir)\n+  }\n+\n+  test(\"create EventLogFileWriter with enable/disable rolling\") {\n+    def buildWriterAndVerify(conf: SparkConf, expectedClazz: Class[_]): Unit = {\n+      val writer = EventLogFileWriter(\n+        getUniqueApplicationId, None, testDirPath.toUri, conf,\n+        SparkHadoopUtil.get.newConfiguration(conf))\n+      val writerClazz = writer.getClass\n+      assert(expectedClazz === writerClazz,\n+        s\"default file writer should be $expectedClazz, but $writerClazz\")\n+    }\n+\n+    val conf = new SparkConf\n+    conf.set(EVENT_LOG_ENABLED, true)\n+    conf.set(EVENT_LOG_DIR, testDir.toString)\n+\n+    // default config\n+    buildWriterAndVerify(conf, classOf[SingleEventLogFileWriter])\n+\n+    conf.set(EVENT_LOG_ENABLE_ROLLING, true)\n+    buildWriterAndVerify(conf, classOf[RollingEventLogFilesWriter])\n+\n+    conf.set(EVENT_LOG_ENABLE_ROLLING, false)\n+    buildWriterAndVerify(conf, classOf[SingleEventLogFileWriter])\n+  }\n+\n+  val allCodecs = Seq(None) ++\n+    CompressionCodec.ALL_COMPRESSION_CODECS.map(c => Some(CompressionCodec.getShortName(c)))\n+\n+  allCodecs.foreach { codecShortName =>\n+    test(s\"initialize, write, stop - with codec $codecShortName\") {\n+      val appId = getUniqueApplicationId\n+      val attemptId = None\n+\n+      val conf = getLoggingConf(testDirPath, codecShortName)\n+      val writer = createWriter(appId, attemptId, testDirPath.toUri, conf,\n+        SparkHadoopUtil.get.newConfiguration(conf))\n+\n+      writer.start()\n+\n+      // snappy stream throws exception on empty stream, so we should provide some data to test.\n+      val dummyData = Seq(\"dummy1\", \"dummy2\", \"dummy3\")\n+      dummyData.foreach(writer.writeEvent(_, flushLogger = true))\n+\n+      writer.stop()\n+\n+      verifyWriteEventLogFile(appId, attemptId, testDirPath.toUri, codecShortName, dummyData)\n+    }\n+  }\n+\n+  test(\"spark.eventLog.compression.codec overrides spark.io.compression.codec\") {\n+    val conf = new SparkConf\n+    conf.set(EVENT_LOG_COMPRESS, true)\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+\n+    val appId = \"test\"\n+    val appAttemptId = None\n+\n+    // The default value is `spark.io.compression.codec`.\n+    val writer = createWriter(appId, appAttemptId, testDirPath.toUri, conf, hadoopConf)\n+    assert(writer.compressionCodecName.contains(\"lz4\"))\n+\n+    // `spark.eventLog.compression.codec` overrides `spark.io.compression.codec`.\n+    conf.set(EVENT_LOG_COMPRESSION_CODEC, \"zstd\")\n+    val writer2 = createWriter(appId, appAttemptId, testDirPath.toUri, conf, hadoopConf)\n+    assert(writer2.compressionCodecName.contains(\"zstd\"))\n+  }\n+\n+  protected def readLinesFromEventLogFile(log: Path, fs: FileSystem): List[String] = {\n+    val logDataStream = EventLogFileReader.openEventLog(log, fs)\n+    try {\n+      Source.fromInputStream(logDataStream).getLines().toList\n+    } finally {\n+      logDataStream.close()\n+    }\n+  }\n+\n+  protected def createWriter(\n+      appId: String,\n+      appAttemptId : Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter\n+\n+  /**\n+   * This should be called with \"closed\" event log file; No guarantee on reading event log file\n+   * which is being written, especially the file is compressed. SHS also does the best it can.\n+   */\n+  protected def verifyWriteEventLogFile(\n+      appId: String,\n+      appAttemptId : Option[String],\n+      logBaseDir: URI,\n+      compressionCodecShortName: Option[String],\n+      expectedLines: Seq[String] = Seq.empty): Unit\n+}\n+\n+class SingleEventLogFileWriterSuite extends EventLogFileWritersSuite {\n+\n+  test(\"Log overwriting\") {\n+    val appId = \"test\"\n+    val appAttemptId = None\n+    val logUri = SingleEventLogFileWriter.getLogPath(testDir.toURI, appId, appAttemptId)\n+\n+    val conf = getLoggingConf(testDirPath)\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+    val writer = createWriter(appId, appAttemptId, testDir.toURI, conf, hadoopConf)\n+\n+    val logPath = new Path(logUri).toUri.getPath\n+    writer.start()\n+\n+    val dummyData = Seq(\"dummy1\", \"dummy2\", \"dummy3\")\n+    dummyData.foreach(writer.writeEvent(_, flushLogger = true))\n+\n+    // Create file before writing the event log\n+    new FileOutputStream(new File(logPath)).close()\n+    // Expected IOException, since we haven't enabled log overwrite.\n+    intercept[IOException] { writer.stop() }\n+\n+    // Try again, but enable overwriting.\n+    conf.set(EVENT_LOG_OVERWRITE, true)\n+    val writer2 = createWriter(appId, appAttemptId, testDir.toURI, conf, hadoopConf)\n+    writer2.start()\n+    dummyData.foreach(writer2.writeEvent(_, flushLogger = true))\n+    writer2.stop()\n+  }\n+\n+  test(\"Event log name\") {\n+    val baseDirUri = Utils.resolveURI(\"/base-dir\")\n+    // without compression\n+    assert(s\"${baseDirUri.toString}/app1\" === SingleEventLogFileWriter.getLogPath(\n+      baseDirUri, \"app1\", None, None))\n+    // with compression\n+    assert(s\"${baseDirUri.toString}/app1.lzf\" ===\n+      SingleEventLogFileWriter.getLogPath(baseDirUri, \"app1\", None, Some(\"lzf\")))\n+    // illegal characters in app ID\n+    assert(s\"${baseDirUri.toString}/a-fine-mind_dollar_bills__1\" ===\n+      SingleEventLogFileWriter.getLogPath(baseDirUri,\n+        \"a fine:mind$dollar{bills}.1\", None, None))\n+    // illegal characters in app ID with compression\n+    assert(s\"${baseDirUri.toString}/a-fine-mind_dollar_bills__1.lz4\" ===\n+      SingleEventLogFileWriter.getLogPath(baseDirUri,\n+        \"a fine:mind$dollar{bills}.1\", None, Some(\"lz4\")))\n+  }\n+\n+  override protected def createWriter(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+  }\n+\n+  override protected def verifyWriteEventLogFile(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      compressionCodecShortName: Option[String],\n+      expectedLines: Seq[String]): Unit = {\n+    // read single event log file\n+    val logPath = SingleEventLogFileWriter.getLogPath(logBaseDir, appId, appAttemptId,\n+      compressionCodecShortName)\n+\n+    val finalLogPath = new Path(logPath)\n+    assert(fileSystem.exists(finalLogPath) && fileSystem.isFile(finalLogPath))\n+    assert(expectedLines === readLinesFromEventLogFile(finalLogPath, fileSystem))\n+  }\n+}\n+\n+class RollingEventLogFilesWriterSuite extends EventLogFileWritersSuite {\n+  import RollingEventLogFilesWriter._\n+\n+  test(\"Event log names\") {\n+    val baseDirUri = Utils.resolveURI(\"/base-dir\")\n+    val appId = \"app1\"\n+    val appAttemptId = None\n+\n+    // happy case with app ID\n+    val logDir = RollingEventLogFilesWriter.getAppEventLogDirPath(baseDirUri, appId, None)\n+    assert(s\"${baseDirUri.toString}/eventlog_v2_app1\" === logDir.toString)\n+\n+    // appstatus: inprogress or completed\n+    assert(s\"$logDir/appstatus_app1.inprogress\" ===\n+      RollingEventLogFilesWriter.getAppStatusFilePath(logDir, appId, appAttemptId,\n+        inProgress = true).toString)\n+    assert(s\"$logDir/appstatus_app1\" ===\n+      RollingEventLogFilesWriter.getAppStatusFilePath(logDir, appId, appAttemptId,\n+        inProgress = false).toString)\n+\n+    // without compression\n+    assert(s\"$logDir/events_1_app1\" ===\n+      RollingEventLogFilesWriter.getEventLogFilePath(logDir, appId, appAttemptId, 1, None).toString)\n+\n+    // with compression\n+    assert(s\"$logDir/events_1_app1.lzf\" ===\n+      RollingEventLogFilesWriter.getEventLogFilePath(logDir, appId, appAttemptId,\n+        1, Some(\"lzf\")).toString)\n+\n+    // illegal characters in app ID\n+    assert(s\"${baseDirUri.toString}/eventlog_v2_a-fine-mind_dollar_bills__1\" ===\n+      RollingEventLogFilesWriter.getAppEventLogDirPath(baseDirUri,\n+        \"a fine:mind$dollar{bills}.1\", None).toString)\n+  }\n+\n+  test(\"Log overwriting\") {\n+    val appId = \"test\"\n+    val appAttemptId = None\n+    val logDirPath = RollingEventLogFilesWriter.getAppEventLogDirPath(testDir.toURI, appId,\n+      appAttemptId)\n+\n+    val conf = getLoggingConf(testDirPath)\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+    val writer = createWriter(appId, appAttemptId, testDir.toURI, conf, hadoopConf)\n+\n+    val logPath = logDirPath.toUri.getPath\n+\n+    // Create file before writing the event log directory\n+    // it doesn't matter whether the existing one is file or directory\n+    new FileOutputStream(new File(logPath)).close()\n+\n+    // Expected IOException, since we haven't enabled log overwrite.\n+    // Note that the place IOException is thrown is different from single event log file.\n+    intercept[IOException] { writer.start() }\n+\n+    // Try again, but enable overwriting.\n+    conf.set(EVENT_LOG_OVERWRITE, true)\n+\n+    val writer2 = createWriter(appId, appAttemptId, testDir.toURI, conf, hadoopConf)\n+    writer2.start()\n+    val dummyData = Seq(\"dummy1\", \"dummy2\", \"dummy3\")\n+    dummyData.foreach(writer2.writeEvent(_, flushLogger = true))\n+    writer2.stop()\n+  }\n+\n+  allCodecs.foreach { codecShortName =>\n+    test(s\"rolling event log files - codec $codecShortName\") {\n+      def assertEventLogFilesSequence(\n+          eventLogFiles: Seq[FileStatus],\n+          expectedLastSequence: Int,\n+          expectedMaxSizeBytes: Long): Unit = {\n+        assert(eventLogFiles.forall(f => f.getLen < expectedMaxSizeBytes))"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Nice finding. Will fix.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-10-03T20:54:15Z",
    "diffHunk": "@@ -0,0 +1,373 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.{File, FileOutputStream, IOException}\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import scala.collection.mutable\n+import scala.io.Source\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.{LocalSparkContext, SparkConf, SparkFunSuite}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.history.EventLogTestHelper._\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+\n+abstract class EventLogFileWritersSuite extends SparkFunSuite with LocalSparkContext\n+  with BeforeAndAfter {\n+\n+  protected val fileSystem = Utils.getHadoopFileSystem(\"/\",\n+    SparkHadoopUtil.get.newConfiguration(new SparkConf()))\n+  protected var testDir: File = _\n+  protected var testDirPath: Path = _\n+\n+  before {\n+    testDir = Utils.createTempDir(namePrefix = s\"event log\")\n+    testDir.deleteOnExit()\n+    testDirPath = new Path(testDir.getAbsolutePath())\n+  }\n+\n+  after {\n+    Utils.deleteRecursively(testDir)\n+  }\n+\n+  test(\"create EventLogFileWriter with enable/disable rolling\") {\n+    def buildWriterAndVerify(conf: SparkConf, expectedClazz: Class[_]): Unit = {\n+      val writer = EventLogFileWriter(\n+        getUniqueApplicationId, None, testDirPath.toUri, conf,\n+        SparkHadoopUtil.get.newConfiguration(conf))\n+      val writerClazz = writer.getClass\n+      assert(expectedClazz === writerClazz,\n+        s\"default file writer should be $expectedClazz, but $writerClazz\")\n+    }\n+\n+    val conf = new SparkConf\n+    conf.set(EVENT_LOG_ENABLED, true)\n+    conf.set(EVENT_LOG_DIR, testDir.toString)\n+\n+    // default config\n+    buildWriterAndVerify(conf, classOf[SingleEventLogFileWriter])\n+\n+    conf.set(EVENT_LOG_ENABLE_ROLLING, true)\n+    buildWriterAndVerify(conf, classOf[RollingEventLogFilesWriter])\n+\n+    conf.set(EVENT_LOG_ENABLE_ROLLING, false)\n+    buildWriterAndVerify(conf, classOf[SingleEventLogFileWriter])\n+  }\n+\n+  val allCodecs = Seq(None) ++\n+    CompressionCodec.ALL_COMPRESSION_CODECS.map(c => Some(CompressionCodec.getShortName(c)))\n+\n+  allCodecs.foreach { codecShortName =>\n+    test(s\"initialize, write, stop - with codec $codecShortName\") {\n+      val appId = getUniqueApplicationId\n+      val attemptId = None\n+\n+      val conf = getLoggingConf(testDirPath, codecShortName)\n+      val writer = createWriter(appId, attemptId, testDirPath.toUri, conf,\n+        SparkHadoopUtil.get.newConfiguration(conf))\n+\n+      writer.start()\n+\n+      // snappy stream throws exception on empty stream, so we should provide some data to test.\n+      val dummyData = Seq(\"dummy1\", \"dummy2\", \"dummy3\")\n+      dummyData.foreach(writer.writeEvent(_, flushLogger = true))\n+\n+      writer.stop()\n+\n+      verifyWriteEventLogFile(appId, attemptId, testDirPath.toUri, codecShortName, dummyData)\n+    }\n+  }\n+\n+  test(\"spark.eventLog.compression.codec overrides spark.io.compression.codec\") {\n+    val conf = new SparkConf\n+    conf.set(EVENT_LOG_COMPRESS, true)\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+\n+    val appId = \"test\"\n+    val appAttemptId = None\n+\n+    // The default value is `spark.io.compression.codec`.\n+    val writer = createWriter(appId, appAttemptId, testDirPath.toUri, conf, hadoopConf)\n+    assert(writer.compressionCodecName.contains(\"lz4\"))\n+\n+    // `spark.eventLog.compression.codec` overrides `spark.io.compression.codec`.\n+    conf.set(EVENT_LOG_COMPRESSION_CODEC, \"zstd\")\n+    val writer2 = createWriter(appId, appAttemptId, testDirPath.toUri, conf, hadoopConf)\n+    assert(writer2.compressionCodecName.contains(\"zstd\"))\n+  }\n+\n+  protected def readLinesFromEventLogFile(log: Path, fs: FileSystem): List[String] = {\n+    val logDataStream = EventLogFileReader.openEventLog(log, fs)\n+    try {\n+      Source.fromInputStream(logDataStream).getLines().toList\n+    } finally {\n+      logDataStream.close()\n+    }\n+  }\n+\n+  protected def createWriter(\n+      appId: String,\n+      appAttemptId : Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter\n+\n+  /**\n+   * This should be called with \"closed\" event log file; No guarantee on reading event log file\n+   * which is being written, especially the file is compressed. SHS also does the best it can.\n+   */\n+  protected def verifyWriteEventLogFile(\n+      appId: String,\n+      appAttemptId : Option[String],\n+      logBaseDir: URI,\n+      compressionCodecShortName: Option[String],\n+      expectedLines: Seq[String] = Seq.empty): Unit\n+}\n+\n+class SingleEventLogFileWriterSuite extends EventLogFileWritersSuite {\n+\n+  test(\"Log overwriting\") {\n+    val appId = \"test\"\n+    val appAttemptId = None\n+    val logUri = SingleEventLogFileWriter.getLogPath(testDir.toURI, appId, appAttemptId)\n+\n+    val conf = getLoggingConf(testDirPath)\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+    val writer = createWriter(appId, appAttemptId, testDir.toURI, conf, hadoopConf)\n+\n+    val logPath = new Path(logUri).toUri.getPath\n+    writer.start()\n+\n+    val dummyData = Seq(\"dummy1\", \"dummy2\", \"dummy3\")\n+    dummyData.foreach(writer.writeEvent(_, flushLogger = true))\n+\n+    // Create file before writing the event log\n+    new FileOutputStream(new File(logPath)).close()\n+    // Expected IOException, since we haven't enabled log overwrite.\n+    intercept[IOException] { writer.stop() }\n+\n+    // Try again, but enable overwriting.\n+    conf.set(EVENT_LOG_OVERWRITE, true)\n+    val writer2 = createWriter(appId, appAttemptId, testDir.toURI, conf, hadoopConf)\n+    writer2.start()\n+    dummyData.foreach(writer2.writeEvent(_, flushLogger = true))\n+    writer2.stop()\n+  }\n+\n+  test(\"Event log name\") {\n+    val baseDirUri = Utils.resolveURI(\"/base-dir\")\n+    // without compression\n+    assert(s\"${baseDirUri.toString}/app1\" === SingleEventLogFileWriter.getLogPath(\n+      baseDirUri, \"app1\", None, None))\n+    // with compression\n+    assert(s\"${baseDirUri.toString}/app1.lzf\" ===\n+      SingleEventLogFileWriter.getLogPath(baseDirUri, \"app1\", None, Some(\"lzf\")))\n+    // illegal characters in app ID\n+    assert(s\"${baseDirUri.toString}/a-fine-mind_dollar_bills__1\" ===\n+      SingleEventLogFileWriter.getLogPath(baseDirUri,\n+        \"a fine:mind$dollar{bills}.1\", None, None))\n+    // illegal characters in app ID with compression\n+    assert(s\"${baseDirUri.toString}/a-fine-mind_dollar_bills__1.lz4\" ===\n+      SingleEventLogFileWriter.getLogPath(baseDirUri,\n+        \"a fine:mind$dollar{bills}.1\", None, Some(\"lz4\")))\n+  }\n+\n+  override protected def createWriter(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+  }\n+\n+  override protected def verifyWriteEventLogFile(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      compressionCodecShortName: Option[String],\n+      expectedLines: Seq[String]): Unit = {\n+    // read single event log file\n+    val logPath = SingleEventLogFileWriter.getLogPath(logBaseDir, appId, appAttemptId,\n+      compressionCodecShortName)\n+\n+    val finalLogPath = new Path(logPath)\n+    assert(fileSystem.exists(finalLogPath) && fileSystem.isFile(finalLogPath))\n+    assert(expectedLines === readLinesFromEventLogFile(finalLogPath, fileSystem))\n+  }\n+}\n+\n+class RollingEventLogFilesWriterSuite extends EventLogFileWritersSuite {\n+  import RollingEventLogFilesWriter._\n+\n+  test(\"Event log names\") {\n+    val baseDirUri = Utils.resolveURI(\"/base-dir\")\n+    val appId = \"app1\"\n+    val appAttemptId = None\n+\n+    // happy case with app ID\n+    val logDir = RollingEventLogFilesWriter.getAppEventLogDirPath(baseDirUri, appId, None)\n+    assert(s\"${baseDirUri.toString}/eventlog_v2_app1\" === logDir.toString)\n+\n+    // appstatus: inprogress or completed\n+    assert(s\"$logDir/appstatus_app1.inprogress\" ===\n+      RollingEventLogFilesWriter.getAppStatusFilePath(logDir, appId, appAttemptId,\n+        inProgress = true).toString)\n+    assert(s\"$logDir/appstatus_app1\" ===\n+      RollingEventLogFilesWriter.getAppStatusFilePath(logDir, appId, appAttemptId,\n+        inProgress = false).toString)\n+\n+    // without compression\n+    assert(s\"$logDir/events_1_app1\" ===\n+      RollingEventLogFilesWriter.getEventLogFilePath(logDir, appId, appAttemptId, 1, None).toString)\n+\n+    // with compression\n+    assert(s\"$logDir/events_1_app1.lzf\" ===\n+      RollingEventLogFilesWriter.getEventLogFilePath(logDir, appId, appAttemptId,\n+        1, Some(\"lzf\")).toString)\n+\n+    // illegal characters in app ID\n+    assert(s\"${baseDirUri.toString}/eventlog_v2_a-fine-mind_dollar_bills__1\" ===\n+      RollingEventLogFilesWriter.getAppEventLogDirPath(baseDirUri,\n+        \"a fine:mind$dollar{bills}.1\", None).toString)\n+  }\n+\n+  test(\"Log overwriting\") {\n+    val appId = \"test\"\n+    val appAttemptId = None\n+    val logDirPath = RollingEventLogFilesWriter.getAppEventLogDirPath(testDir.toURI, appId,\n+      appAttemptId)\n+\n+    val conf = getLoggingConf(testDirPath)\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+    val writer = createWriter(appId, appAttemptId, testDir.toURI, conf, hadoopConf)\n+\n+    val logPath = logDirPath.toUri.getPath\n+\n+    // Create file before writing the event log directory\n+    // it doesn't matter whether the existing one is file or directory\n+    new FileOutputStream(new File(logPath)).close()\n+\n+    // Expected IOException, since we haven't enabled log overwrite.\n+    // Note that the place IOException is thrown is different from single event log file.\n+    intercept[IOException] { writer.start() }\n+\n+    // Try again, but enable overwriting.\n+    conf.set(EVENT_LOG_OVERWRITE, true)\n+\n+    val writer2 = createWriter(appId, appAttemptId, testDir.toURI, conf, hadoopConf)\n+    writer2.start()\n+    val dummyData = Seq(\"dummy1\", \"dummy2\", \"dummy3\")\n+    dummyData.foreach(writer2.writeEvent(_, flushLogger = true))\n+    writer2.stop()\n+  }\n+\n+  allCodecs.foreach { codecShortName =>\n+    test(s\"rolling event log files - codec $codecShortName\") {\n+      def assertEventLogFilesSequence(\n+          eventLogFiles: Seq[FileStatus],\n+          expectedLastSequence: Int,\n+          expectedMaxSizeBytes: Long): Unit = {\n+        assert(eventLogFiles.forall(f => f.getLen < expectedMaxSizeBytes))"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "Ngone51"
    },
    "body": "Again, `Sequence` or `index` ?",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-10-03T15:19:25Z",
    "diffHunk": "@@ -0,0 +1,373 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history\n+\n+import java.io.{File, FileOutputStream, IOException}\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+\n+import scala.collection.mutable\n+import scala.io.Source\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.{LocalSparkContext, SparkConf, SparkFunSuite}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.history.EventLogTestHelper._\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.util.Utils\n+\n+\n+abstract class EventLogFileWritersSuite extends SparkFunSuite with LocalSparkContext\n+  with BeforeAndAfter {\n+\n+  protected val fileSystem = Utils.getHadoopFileSystem(\"/\",\n+    SparkHadoopUtil.get.newConfiguration(new SparkConf()))\n+  protected var testDir: File = _\n+  protected var testDirPath: Path = _\n+\n+  before {\n+    testDir = Utils.createTempDir(namePrefix = s\"event log\")\n+    testDir.deleteOnExit()\n+    testDirPath = new Path(testDir.getAbsolutePath())\n+  }\n+\n+  after {\n+    Utils.deleteRecursively(testDir)\n+  }\n+\n+  test(\"create EventLogFileWriter with enable/disable rolling\") {\n+    def buildWriterAndVerify(conf: SparkConf, expectedClazz: Class[_]): Unit = {\n+      val writer = EventLogFileWriter(\n+        getUniqueApplicationId, None, testDirPath.toUri, conf,\n+        SparkHadoopUtil.get.newConfiguration(conf))\n+      val writerClazz = writer.getClass\n+      assert(expectedClazz === writerClazz,\n+        s\"default file writer should be $expectedClazz, but $writerClazz\")\n+    }\n+\n+    val conf = new SparkConf\n+    conf.set(EVENT_LOG_ENABLED, true)\n+    conf.set(EVENT_LOG_DIR, testDir.toString)\n+\n+    // default config\n+    buildWriterAndVerify(conf, classOf[SingleEventLogFileWriter])\n+\n+    conf.set(EVENT_LOG_ENABLE_ROLLING, true)\n+    buildWriterAndVerify(conf, classOf[RollingEventLogFilesWriter])\n+\n+    conf.set(EVENT_LOG_ENABLE_ROLLING, false)\n+    buildWriterAndVerify(conf, classOf[SingleEventLogFileWriter])\n+  }\n+\n+  val allCodecs = Seq(None) ++\n+    CompressionCodec.ALL_COMPRESSION_CODECS.map(c => Some(CompressionCodec.getShortName(c)))\n+\n+  allCodecs.foreach { codecShortName =>\n+    test(s\"initialize, write, stop - with codec $codecShortName\") {\n+      val appId = getUniqueApplicationId\n+      val attemptId = None\n+\n+      val conf = getLoggingConf(testDirPath, codecShortName)\n+      val writer = createWriter(appId, attemptId, testDirPath.toUri, conf,\n+        SparkHadoopUtil.get.newConfiguration(conf))\n+\n+      writer.start()\n+\n+      // snappy stream throws exception on empty stream, so we should provide some data to test.\n+      val dummyData = Seq(\"dummy1\", \"dummy2\", \"dummy3\")\n+      dummyData.foreach(writer.writeEvent(_, flushLogger = true))\n+\n+      writer.stop()\n+\n+      verifyWriteEventLogFile(appId, attemptId, testDirPath.toUri, codecShortName, dummyData)\n+    }\n+  }\n+\n+  test(\"spark.eventLog.compression.codec overrides spark.io.compression.codec\") {\n+    val conf = new SparkConf\n+    conf.set(EVENT_LOG_COMPRESS, true)\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+\n+    val appId = \"test\"\n+    val appAttemptId = None\n+\n+    // The default value is `spark.io.compression.codec`.\n+    val writer = createWriter(appId, appAttemptId, testDirPath.toUri, conf, hadoopConf)\n+    assert(writer.compressionCodecName.contains(\"lz4\"))\n+\n+    // `spark.eventLog.compression.codec` overrides `spark.io.compression.codec`.\n+    conf.set(EVENT_LOG_COMPRESSION_CODEC, \"zstd\")\n+    val writer2 = createWriter(appId, appAttemptId, testDirPath.toUri, conf, hadoopConf)\n+    assert(writer2.compressionCodecName.contains(\"zstd\"))\n+  }\n+\n+  protected def readLinesFromEventLogFile(log: Path, fs: FileSystem): List[String] = {\n+    val logDataStream = EventLogFileReader.openEventLog(log, fs)\n+    try {\n+      Source.fromInputStream(logDataStream).getLines().toList\n+    } finally {\n+      logDataStream.close()\n+    }\n+  }\n+\n+  protected def createWriter(\n+      appId: String,\n+      appAttemptId : Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter\n+\n+  /**\n+   * This should be called with \"closed\" event log file; No guarantee on reading event log file\n+   * which is being written, especially the file is compressed. SHS also does the best it can.\n+   */\n+  protected def verifyWriteEventLogFile(\n+      appId: String,\n+      appAttemptId : Option[String],\n+      logBaseDir: URI,\n+      compressionCodecShortName: Option[String],\n+      expectedLines: Seq[String] = Seq.empty): Unit\n+}\n+\n+class SingleEventLogFileWriterSuite extends EventLogFileWritersSuite {\n+\n+  test(\"Log overwriting\") {\n+    val appId = \"test\"\n+    val appAttemptId = None\n+    val logUri = SingleEventLogFileWriter.getLogPath(testDir.toURI, appId, appAttemptId)\n+\n+    val conf = getLoggingConf(testDirPath)\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+    val writer = createWriter(appId, appAttemptId, testDir.toURI, conf, hadoopConf)\n+\n+    val logPath = new Path(logUri).toUri.getPath\n+    writer.start()\n+\n+    val dummyData = Seq(\"dummy1\", \"dummy2\", \"dummy3\")\n+    dummyData.foreach(writer.writeEvent(_, flushLogger = true))\n+\n+    // Create file before writing the event log\n+    new FileOutputStream(new File(logPath)).close()\n+    // Expected IOException, since we haven't enabled log overwrite.\n+    intercept[IOException] { writer.stop() }\n+\n+    // Try again, but enable overwriting.\n+    conf.set(EVENT_LOG_OVERWRITE, true)\n+    val writer2 = createWriter(appId, appAttemptId, testDir.toURI, conf, hadoopConf)\n+    writer2.start()\n+    dummyData.foreach(writer2.writeEvent(_, flushLogger = true))\n+    writer2.stop()\n+  }\n+\n+  test(\"Event log name\") {\n+    val baseDirUri = Utils.resolveURI(\"/base-dir\")\n+    // without compression\n+    assert(s\"${baseDirUri.toString}/app1\" === SingleEventLogFileWriter.getLogPath(\n+      baseDirUri, \"app1\", None, None))\n+    // with compression\n+    assert(s\"${baseDirUri.toString}/app1.lzf\" ===\n+      SingleEventLogFileWriter.getLogPath(baseDirUri, \"app1\", None, Some(\"lzf\")))\n+    // illegal characters in app ID\n+    assert(s\"${baseDirUri.toString}/a-fine-mind_dollar_bills__1\" ===\n+      SingleEventLogFileWriter.getLogPath(baseDirUri,\n+        \"a fine:mind$dollar{bills}.1\", None, None))\n+    // illegal characters in app ID with compression\n+    assert(s\"${baseDirUri.toString}/a-fine-mind_dollar_bills__1.lz4\" ===\n+      SingleEventLogFileWriter.getLogPath(baseDirUri,\n+        \"a fine:mind$dollar{bills}.1\", None, Some(\"lz4\")))\n+  }\n+\n+  override protected def createWriter(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+  }\n+\n+  override protected def verifyWriteEventLogFile(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      compressionCodecShortName: Option[String],\n+      expectedLines: Seq[String]): Unit = {\n+    // read single event log file\n+    val logPath = SingleEventLogFileWriter.getLogPath(logBaseDir, appId, appAttemptId,\n+      compressionCodecShortName)\n+\n+    val finalLogPath = new Path(logPath)\n+    assert(fileSystem.exists(finalLogPath) && fileSystem.isFile(finalLogPath))\n+    assert(expectedLines === readLinesFromEventLogFile(finalLogPath, fileSystem))\n+  }\n+}\n+\n+class RollingEventLogFilesWriterSuite extends EventLogFileWritersSuite {\n+  import RollingEventLogFilesWriter._\n+\n+  test(\"Event log names\") {\n+    val baseDirUri = Utils.resolveURI(\"/base-dir\")\n+    val appId = \"app1\"\n+    val appAttemptId = None\n+\n+    // happy case with app ID\n+    val logDir = RollingEventLogFilesWriter.getAppEventLogDirPath(baseDirUri, appId, None)\n+    assert(s\"${baseDirUri.toString}/eventlog_v2_app1\" === logDir.toString)\n+\n+    // appstatus: inprogress or completed\n+    assert(s\"$logDir/appstatus_app1.inprogress\" ===\n+      RollingEventLogFilesWriter.getAppStatusFilePath(logDir, appId, appAttemptId,\n+        inProgress = true).toString)\n+    assert(s\"$logDir/appstatus_app1\" ===\n+      RollingEventLogFilesWriter.getAppStatusFilePath(logDir, appId, appAttemptId,\n+        inProgress = false).toString)\n+\n+    // without compression\n+    assert(s\"$logDir/events_1_app1\" ===\n+      RollingEventLogFilesWriter.getEventLogFilePath(logDir, appId, appAttemptId, 1, None).toString)\n+\n+    // with compression\n+    assert(s\"$logDir/events_1_app1.lzf\" ===\n+      RollingEventLogFilesWriter.getEventLogFilePath(logDir, appId, appAttemptId,\n+        1, Some(\"lzf\")).toString)\n+\n+    // illegal characters in app ID\n+    assert(s\"${baseDirUri.toString}/eventlog_v2_a-fine-mind_dollar_bills__1\" ===\n+      RollingEventLogFilesWriter.getAppEventLogDirPath(baseDirUri,\n+        \"a fine:mind$dollar{bills}.1\", None).toString)\n+  }\n+\n+  test(\"Log overwriting\") {\n+    val appId = \"test\"\n+    val appAttemptId = None\n+    val logDirPath = RollingEventLogFilesWriter.getAppEventLogDirPath(testDir.toURI, appId,\n+      appAttemptId)\n+\n+    val conf = getLoggingConf(testDirPath)\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+    val writer = createWriter(appId, appAttemptId, testDir.toURI, conf, hadoopConf)\n+\n+    val logPath = logDirPath.toUri.getPath\n+\n+    // Create file before writing the event log directory\n+    // it doesn't matter whether the existing one is file or directory\n+    new FileOutputStream(new File(logPath)).close()\n+\n+    // Expected IOException, since we haven't enabled log overwrite.\n+    // Note that the place IOException is thrown is different from single event log file.\n+    intercept[IOException] { writer.start() }\n+\n+    // Try again, but enable overwriting.\n+    conf.set(EVENT_LOG_OVERWRITE, true)\n+\n+    val writer2 = createWriter(appId, appAttemptId, testDir.toURI, conf, hadoopConf)\n+    writer2.start()\n+    val dummyData = Seq(\"dummy1\", \"dummy2\", \"dummy3\")\n+    dummyData.foreach(writer2.writeEvent(_, flushLogger = true))\n+    writer2.stop()\n+  }\n+\n+  allCodecs.foreach { codecShortName =>\n+    test(s\"rolling event log files - codec $codecShortName\") {\n+      def assertEventLogFilesSequence("
  }],
  "prId": 25670
}]