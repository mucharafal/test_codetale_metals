[{
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "One line if you can? to establish this is a key-value pair?\n",
    "commit": "8adc8ba412e9a02af6750b052a0c9acb81c442a0",
    "createdAt": "2015-04-14T10:11:18Z",
    "diffHunk": "@@ -65,16 +66,22 @@ class LogUrlsStandaloneSuite extends FunSuite with LocalSparkContext {\n         new MySparkConf().setAll(getAll)\n       }\n     }\n-    val conf = new MySparkConf()\n+    val conf = new MySparkConf().set(\n+      \"spark.extraListeners\",\n+      \"org.apache.spark.deploy.SaveExecutorInfo\")"
  }],
  "prId": 5417
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Can this use the new find method above or did I miss something?\n",
    "commit": "8adc8ba412e9a02af6750b052a0c9acb81c442a0",
    "createdAt": "2015-04-14T10:11:38Z",
    "diffHunk": "@@ -65,16 +66,22 @@ class LogUrlsStandaloneSuite extends FunSuite with LocalSparkContext {\n         new MySparkConf().setAll(getAll)\n       }\n     }\n-    val conf = new MySparkConf()\n+    val conf = new MySparkConf().set(\n+      \"spark.extraListeners\",\n+      \"org.apache.spark.deploy.SaveExecutorInfo\")\n     sc = new SparkContext(\"local-cluster[2,1,512]\", \"test\", conf)\n \n-    val listener = new SaveExecutorInfo\n-    sc.addSparkListener(listener)\n-\n     // Trigger a job so that executors get added\n     sc.parallelize(1 to 100, 4).map(_.toString).count()\n \n     assert(sc.listenerBus.waitUntilEmpty(WAIT_TIMEOUT_MILLIS))\n+    val listenerOpt = sc.listenerBus.listeners.find {"
  }],
  "prId": 5417
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "should be `===`\n",
    "commit": "8adc8ba412e9a02af6750b052a0c9acb81c442a0",
    "createdAt": "2015-04-14T20:57:59Z",
    "diffHunk": "@@ -65,16 +66,17 @@ class LogUrlsStandaloneSuite extends FunSuite with LocalSparkContext {\n         new MySparkConf().setAll(getAll)\n       }\n     }\n-    val conf = new MySparkConf()\n+    val conf = new MySparkConf().set(\n+      \"spark.extraListeners\", classOf[SaveExecutorInfo].getName)\n     sc = new SparkContext(\"local-cluster[2,1,512]\", \"test\", conf)\n \n-    val listener = new SaveExecutorInfo\n-    sc.addSparkListener(listener)\n-\n     // Trigger a job so that executors get added\n     sc.parallelize(1 to 100, 4).map(_.toString).count()\n \n     assert(sc.listenerBus.waitUntilEmpty(WAIT_TIMEOUT_MILLIS))\n+    val listeners = sc.listenerBus.findListenersByClass[SaveExecutorInfo]\n+    assert(listeners.size == 1)",
    "line": 25
  }],
  "prId": 5417
}]