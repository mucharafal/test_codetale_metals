[{
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "typo, missing an 'l in `blockHandler`",
    "commit": "475d27817b6ef37eed9b7b39fee709427811f0d6",
    "createdAt": "2019-03-04T21:24:39Z",
    "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import com.google.common.io.{CharStreams, Closeables, Files}\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark.{SecurityManager, ShuffleSuite, SparkConf, SparkException}\n+import org.apache.spark.network.shuffle.{ExternalShuffleBlockHandler, ExternalShuffleBlockResolver}\n+import org.apache.spark.network.shuffle.protocol.ExecutorShuffleInfo\n+import org.apache.spark.network.util.JavaUtils\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * This suite gets BlockData when the ExternalShuffleService is restarted\n+ * with #spark.shuffle.service.db.enabled = true or false\n+ * Note that failures in this suite may arise when#spark.shuffle.service.db.enabled = false\n+ */\n+class ExternalShuffleServiceDbSuite extends ShuffleSuite with BeforeAndAfterAll {\n+  val sortBlock0 = \"Hello!\"\n+  val sortBlock1 = \"World!\"\n+  val SORT_MANAGER = \"org.apache.spark.shuffle.sort.SortShuffleManager\"\n+\n+  var sparkConf: SparkConf = _\n+  var dataContext: TestShuffleDataContext = _\n+\n+  var securityManager: SecurityManager = _\n+  var externalShuffleService: ExternalShuffleService = _\n+  var bockHandler: ExternalShuffleBlockHandler = _\n+  var blockResolver: ExternalShuffleBlockResolver = _\n+\n+  override def beforeAll() {\n+    super.beforeAll()\n+    sparkConf = new SparkConf()\n+    sparkConf.set(\"spark.shuffle.service.enabled\", \"true\")\n+    sparkConf.set(\"spark.local.dir\", System.getProperty(\"java.io.tmpdir\"))\n+    Utils.loadDefaultSparkProperties(sparkConf, null)\n+    securityManager = new SecurityManager(sparkConf)\n+\n+    dataContext = new TestShuffleDataContext(2, 5)\n+    dataContext.create()\n+    // Write some sort data.\n+    dataContext.insertSortShuffleData(0, 0,\n+      Array[Array[Byte]](sortBlock0.getBytes(StandardCharsets.UTF_8),\n+        sortBlock1.getBytes(StandardCharsets.UTF_8)))\n+    registerExecutor()\n+  }\n+\n+  override def afterAll() {\n+    try {\n+      dataContext.cleanup()\n+    } finally {\n+      super.afterAll()\n+    }\n+  }\n+\n+  def registerExecutor(): Unit = {\n+    sparkConf.set(\"spark.shuffle.service.db.enabled\", \"true\")\n+    externalShuffleService = new ExternalShuffleService(sparkConf, securityManager)\n+\n+    // external Shuffle Service start\n+    externalShuffleService.start()\n+    bockHandler = externalShuffleService.getBlockHandler\n+    blockResolver = bockHandler.getBlockResolver\n+    blockResolver.registerExecutor(\"app0\", \"exec0\", dataContext.createExecutorInfo(SORT_MANAGER))\n+    blockResolver.closeForTest()\n+    // external Shuffle Service stop\n+    externalShuffleService.stop()\n+  }\n+\n+  // This test getBlockData will be passed when the external shuffle service is restarted.\n+  test(\"restart External Shuffle Service With InitRegisteredExecutorsDB\") {\n+    sparkConf.set(\"spark.shuffle.service.db.enabled\", \"true\")\n+    externalShuffleService = new ExternalShuffleService(sparkConf, securityManager)\n+    // externalShuffleService restart\n+    externalShuffleService.start()\n+    bockHandler = externalShuffleService.getBlockHandler"
  }, {
    "author": {
      "login": "weixiuli"
    },
    "body": "done",
    "commit": "475d27817b6ef37eed9b7b39fee709427811f0d6",
    "createdAt": "2019-03-06T10:49:42Z",
    "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import com.google.common.io.{CharStreams, Closeables, Files}\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark.{SecurityManager, ShuffleSuite, SparkConf, SparkException}\n+import org.apache.spark.network.shuffle.{ExternalShuffleBlockHandler, ExternalShuffleBlockResolver}\n+import org.apache.spark.network.shuffle.protocol.ExecutorShuffleInfo\n+import org.apache.spark.network.util.JavaUtils\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * This suite gets BlockData when the ExternalShuffleService is restarted\n+ * with #spark.shuffle.service.db.enabled = true or false\n+ * Note that failures in this suite may arise when#spark.shuffle.service.db.enabled = false\n+ */\n+class ExternalShuffleServiceDbSuite extends ShuffleSuite with BeforeAndAfterAll {\n+  val sortBlock0 = \"Hello!\"\n+  val sortBlock1 = \"World!\"\n+  val SORT_MANAGER = \"org.apache.spark.shuffle.sort.SortShuffleManager\"\n+\n+  var sparkConf: SparkConf = _\n+  var dataContext: TestShuffleDataContext = _\n+\n+  var securityManager: SecurityManager = _\n+  var externalShuffleService: ExternalShuffleService = _\n+  var bockHandler: ExternalShuffleBlockHandler = _\n+  var blockResolver: ExternalShuffleBlockResolver = _\n+\n+  override def beforeAll() {\n+    super.beforeAll()\n+    sparkConf = new SparkConf()\n+    sparkConf.set(\"spark.shuffle.service.enabled\", \"true\")\n+    sparkConf.set(\"spark.local.dir\", System.getProperty(\"java.io.tmpdir\"))\n+    Utils.loadDefaultSparkProperties(sparkConf, null)\n+    securityManager = new SecurityManager(sparkConf)\n+\n+    dataContext = new TestShuffleDataContext(2, 5)\n+    dataContext.create()\n+    // Write some sort data.\n+    dataContext.insertSortShuffleData(0, 0,\n+      Array[Array[Byte]](sortBlock0.getBytes(StandardCharsets.UTF_8),\n+        sortBlock1.getBytes(StandardCharsets.UTF_8)))\n+    registerExecutor()\n+  }\n+\n+  override def afterAll() {\n+    try {\n+      dataContext.cleanup()\n+    } finally {\n+      super.afterAll()\n+    }\n+  }\n+\n+  def registerExecutor(): Unit = {\n+    sparkConf.set(\"spark.shuffle.service.db.enabled\", \"true\")\n+    externalShuffleService = new ExternalShuffleService(sparkConf, securityManager)\n+\n+    // external Shuffle Service start\n+    externalShuffleService.start()\n+    bockHandler = externalShuffleService.getBlockHandler\n+    blockResolver = bockHandler.getBlockResolver\n+    blockResolver.registerExecutor(\"app0\", \"exec0\", dataContext.createExecutorInfo(SORT_MANAGER))\n+    blockResolver.closeForTest()\n+    // external Shuffle Service stop\n+    externalShuffleService.stop()\n+  }\n+\n+  // This test getBlockData will be passed when the external shuffle service is restarted.\n+  test(\"restart External Shuffle Service With InitRegisteredExecutorsDB\") {\n+    sparkConf.set(\"spark.shuffle.service.db.enabled\", \"true\")\n+    externalShuffleService = new ExternalShuffleService(sparkConf, securityManager)\n+    // externalShuffleService restart\n+    externalShuffleService.start()\n+    bockHandler = externalShuffleService.getBlockHandler"
  }],
  "prId": 23393
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "I don't think you need to add `closeForTest()` and call that here.  You can close the `blockHandler` and re-create the `externalShuffleService` (closer to what you want to test, actually)",
    "commit": "475d27817b6ef37eed9b7b39fee709427811f0d6",
    "createdAt": "2019-03-04T21:32:51Z",
    "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import com.google.common.io.{CharStreams, Closeables, Files}\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark.{SecurityManager, ShuffleSuite, SparkConf, SparkException}\n+import org.apache.spark.network.shuffle.{ExternalShuffleBlockHandler, ExternalShuffleBlockResolver}\n+import org.apache.spark.network.shuffle.protocol.ExecutorShuffleInfo\n+import org.apache.spark.network.util.JavaUtils\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * This suite gets BlockData when the ExternalShuffleService is restarted\n+ * with #spark.shuffle.service.db.enabled = true or false\n+ * Note that failures in this suite may arise when#spark.shuffle.service.db.enabled = false\n+ */\n+class ExternalShuffleServiceDbSuite extends ShuffleSuite with BeforeAndAfterAll {\n+  val sortBlock0 = \"Hello!\"\n+  val sortBlock1 = \"World!\"\n+  val SORT_MANAGER = \"org.apache.spark.shuffle.sort.SortShuffleManager\"\n+\n+  var sparkConf: SparkConf = _\n+  var dataContext: TestShuffleDataContext = _\n+\n+  var securityManager: SecurityManager = _\n+  var externalShuffleService: ExternalShuffleService = _\n+  var bockHandler: ExternalShuffleBlockHandler = _\n+  var blockResolver: ExternalShuffleBlockResolver = _\n+\n+  override def beforeAll() {\n+    super.beforeAll()\n+    sparkConf = new SparkConf()\n+    sparkConf.set(\"spark.shuffle.service.enabled\", \"true\")\n+    sparkConf.set(\"spark.local.dir\", System.getProperty(\"java.io.tmpdir\"))\n+    Utils.loadDefaultSparkProperties(sparkConf, null)\n+    securityManager = new SecurityManager(sparkConf)\n+\n+    dataContext = new TestShuffleDataContext(2, 5)\n+    dataContext.create()\n+    // Write some sort data.\n+    dataContext.insertSortShuffleData(0, 0,\n+      Array[Array[Byte]](sortBlock0.getBytes(StandardCharsets.UTF_8),\n+        sortBlock1.getBytes(StandardCharsets.UTF_8)))\n+    registerExecutor()\n+  }\n+\n+  override def afterAll() {\n+    try {\n+      dataContext.cleanup()\n+    } finally {\n+      super.afterAll()\n+    }\n+  }\n+\n+  def registerExecutor(): Unit = {\n+    sparkConf.set(\"spark.shuffle.service.db.enabled\", \"true\")\n+    externalShuffleService = new ExternalShuffleService(sparkConf, securityManager)\n+\n+    // external Shuffle Service start\n+    externalShuffleService.start()\n+    bockHandler = externalShuffleService.getBlockHandler\n+    blockResolver = bockHandler.getBlockResolver\n+    blockResolver.registerExecutor(\"app0\", \"exec0\", dataContext.createExecutorInfo(SORT_MANAGER))\n+    blockResolver.closeForTest()"
  }, {
    "author": {
      "login": "weixiuli"
    },
    "body": "closeForTest() needs to be added ,it closes  LevelDB  timely to make sure LevelDB can be used again next time ,if we don't add it, there will be thrown an  exception  when  we use  LevelDB again ( eg: re-create the externalShuffleService).",
    "commit": "475d27817b6ef37eed9b7b39fee709427811f0d6",
    "createdAt": "2019-03-05T06:15:31Z",
    "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import com.google.common.io.{CharStreams, Closeables, Files}\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark.{SecurityManager, ShuffleSuite, SparkConf, SparkException}\n+import org.apache.spark.network.shuffle.{ExternalShuffleBlockHandler, ExternalShuffleBlockResolver}\n+import org.apache.spark.network.shuffle.protocol.ExecutorShuffleInfo\n+import org.apache.spark.network.util.JavaUtils\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * This suite gets BlockData when the ExternalShuffleService is restarted\n+ * with #spark.shuffle.service.db.enabled = true or false\n+ * Note that failures in this suite may arise when#spark.shuffle.service.db.enabled = false\n+ */\n+class ExternalShuffleServiceDbSuite extends ShuffleSuite with BeforeAndAfterAll {\n+  val sortBlock0 = \"Hello!\"\n+  val sortBlock1 = \"World!\"\n+  val SORT_MANAGER = \"org.apache.spark.shuffle.sort.SortShuffleManager\"\n+\n+  var sparkConf: SparkConf = _\n+  var dataContext: TestShuffleDataContext = _\n+\n+  var securityManager: SecurityManager = _\n+  var externalShuffleService: ExternalShuffleService = _\n+  var bockHandler: ExternalShuffleBlockHandler = _\n+  var blockResolver: ExternalShuffleBlockResolver = _\n+\n+  override def beforeAll() {\n+    super.beforeAll()\n+    sparkConf = new SparkConf()\n+    sparkConf.set(\"spark.shuffle.service.enabled\", \"true\")\n+    sparkConf.set(\"spark.local.dir\", System.getProperty(\"java.io.tmpdir\"))\n+    Utils.loadDefaultSparkProperties(sparkConf, null)\n+    securityManager = new SecurityManager(sparkConf)\n+\n+    dataContext = new TestShuffleDataContext(2, 5)\n+    dataContext.create()\n+    // Write some sort data.\n+    dataContext.insertSortShuffleData(0, 0,\n+      Array[Array[Byte]](sortBlock0.getBytes(StandardCharsets.UTF_8),\n+        sortBlock1.getBytes(StandardCharsets.UTF_8)))\n+    registerExecutor()\n+  }\n+\n+  override def afterAll() {\n+    try {\n+      dataContext.cleanup()\n+    } finally {\n+      super.afterAll()\n+    }\n+  }\n+\n+  def registerExecutor(): Unit = {\n+    sparkConf.set(\"spark.shuffle.service.db.enabled\", \"true\")\n+    externalShuffleService = new ExternalShuffleService(sparkConf, securityManager)\n+\n+    // external Shuffle Service start\n+    externalShuffleService.start()\n+    bockHandler = externalShuffleService.getBlockHandler\n+    blockResolver = bockHandler.getBlockResolver\n+    blockResolver.registerExecutor(\"app0\", \"exec0\", dataContext.createExecutorInfo(SORT_MANAGER))\n+    blockResolver.closeForTest()"
  }, {
    "author": {
      "login": "squito"
    },
    "body": "I meant you can call `blockHandler.close()`, which is public, and which will call `blockResolver.close()` immediately.",
    "commit": "475d27817b6ef37eed9b7b39fee709427811f0d6",
    "createdAt": "2019-03-06T04:15:24Z",
    "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import com.google.common.io.{CharStreams, Closeables, Files}\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark.{SecurityManager, ShuffleSuite, SparkConf, SparkException}\n+import org.apache.spark.network.shuffle.{ExternalShuffleBlockHandler, ExternalShuffleBlockResolver}\n+import org.apache.spark.network.shuffle.protocol.ExecutorShuffleInfo\n+import org.apache.spark.network.util.JavaUtils\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * This suite gets BlockData when the ExternalShuffleService is restarted\n+ * with #spark.shuffle.service.db.enabled = true or false\n+ * Note that failures in this suite may arise when#spark.shuffle.service.db.enabled = false\n+ */\n+class ExternalShuffleServiceDbSuite extends ShuffleSuite with BeforeAndAfterAll {\n+  val sortBlock0 = \"Hello!\"\n+  val sortBlock1 = \"World!\"\n+  val SORT_MANAGER = \"org.apache.spark.shuffle.sort.SortShuffleManager\"\n+\n+  var sparkConf: SparkConf = _\n+  var dataContext: TestShuffleDataContext = _\n+\n+  var securityManager: SecurityManager = _\n+  var externalShuffleService: ExternalShuffleService = _\n+  var bockHandler: ExternalShuffleBlockHandler = _\n+  var blockResolver: ExternalShuffleBlockResolver = _\n+\n+  override def beforeAll() {\n+    super.beforeAll()\n+    sparkConf = new SparkConf()\n+    sparkConf.set(\"spark.shuffle.service.enabled\", \"true\")\n+    sparkConf.set(\"spark.local.dir\", System.getProperty(\"java.io.tmpdir\"))\n+    Utils.loadDefaultSparkProperties(sparkConf, null)\n+    securityManager = new SecurityManager(sparkConf)\n+\n+    dataContext = new TestShuffleDataContext(2, 5)\n+    dataContext.create()\n+    // Write some sort data.\n+    dataContext.insertSortShuffleData(0, 0,\n+      Array[Array[Byte]](sortBlock0.getBytes(StandardCharsets.UTF_8),\n+        sortBlock1.getBytes(StandardCharsets.UTF_8)))\n+    registerExecutor()\n+  }\n+\n+  override def afterAll() {\n+    try {\n+      dataContext.cleanup()\n+    } finally {\n+      super.afterAll()\n+    }\n+  }\n+\n+  def registerExecutor(): Unit = {\n+    sparkConf.set(\"spark.shuffle.service.db.enabled\", \"true\")\n+    externalShuffleService = new ExternalShuffleService(sparkConf, securityManager)\n+\n+    // external Shuffle Service start\n+    externalShuffleService.start()\n+    bockHandler = externalShuffleService.getBlockHandler\n+    blockResolver = bockHandler.getBlockResolver\n+    blockResolver.registerExecutor(\"app0\", \"exec0\", dataContext.createExecutorInfo(SORT_MANAGER))\n+    blockResolver.closeForTest()"
  }, {
    "author": {
      "login": "weixiuli"
    },
    "body": "ok,i see.",
    "commit": "475d27817b6ef37eed9b7b39fee709427811f0d6",
    "createdAt": "2019-03-06T05:44:32Z",
    "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import com.google.common.io.{CharStreams, Closeables, Files}\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark.{SecurityManager, ShuffleSuite, SparkConf, SparkException}\n+import org.apache.spark.network.shuffle.{ExternalShuffleBlockHandler, ExternalShuffleBlockResolver}\n+import org.apache.spark.network.shuffle.protocol.ExecutorShuffleInfo\n+import org.apache.spark.network.util.JavaUtils\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * This suite gets BlockData when the ExternalShuffleService is restarted\n+ * with #spark.shuffle.service.db.enabled = true or false\n+ * Note that failures in this suite may arise when#spark.shuffle.service.db.enabled = false\n+ */\n+class ExternalShuffleServiceDbSuite extends ShuffleSuite with BeforeAndAfterAll {\n+  val sortBlock0 = \"Hello!\"\n+  val sortBlock1 = \"World!\"\n+  val SORT_MANAGER = \"org.apache.spark.shuffle.sort.SortShuffleManager\"\n+\n+  var sparkConf: SparkConf = _\n+  var dataContext: TestShuffleDataContext = _\n+\n+  var securityManager: SecurityManager = _\n+  var externalShuffleService: ExternalShuffleService = _\n+  var bockHandler: ExternalShuffleBlockHandler = _\n+  var blockResolver: ExternalShuffleBlockResolver = _\n+\n+  override def beforeAll() {\n+    super.beforeAll()\n+    sparkConf = new SparkConf()\n+    sparkConf.set(\"spark.shuffle.service.enabled\", \"true\")\n+    sparkConf.set(\"spark.local.dir\", System.getProperty(\"java.io.tmpdir\"))\n+    Utils.loadDefaultSparkProperties(sparkConf, null)\n+    securityManager = new SecurityManager(sparkConf)\n+\n+    dataContext = new TestShuffleDataContext(2, 5)\n+    dataContext.create()\n+    // Write some sort data.\n+    dataContext.insertSortShuffleData(0, 0,\n+      Array[Array[Byte]](sortBlock0.getBytes(StandardCharsets.UTF_8),\n+        sortBlock1.getBytes(StandardCharsets.UTF_8)))\n+    registerExecutor()\n+  }\n+\n+  override def afterAll() {\n+    try {\n+      dataContext.cleanup()\n+    } finally {\n+      super.afterAll()\n+    }\n+  }\n+\n+  def registerExecutor(): Unit = {\n+    sparkConf.set(\"spark.shuffle.service.db.enabled\", \"true\")\n+    externalShuffleService = new ExternalShuffleService(sparkConf, securityManager)\n+\n+    // external Shuffle Service start\n+    externalShuffleService.start()\n+    bockHandler = externalShuffleService.getBlockHandler\n+    blockResolver = bockHandler.getBlockResolver\n+    blockResolver.registerExecutor(\"app0\", \"exec0\", dataContext.createExecutorInfo(SORT_MANAGER))\n+    blockResolver.closeForTest()"
  }],
  "prId": 23393
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "I don't think those comments add anything.\r\n\r\nshould the close() / stop() be in the `afterAll` (or a `finally`)?",
    "commit": "475d27817b6ef37eed9b7b39fee709427811f0d6",
    "createdAt": "2019-03-04T21:38:20Z",
    "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import com.google.common.io.{CharStreams, Closeables, Files}\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark.{SecurityManager, ShuffleSuite, SparkConf, SparkException}\n+import org.apache.spark.network.shuffle.{ExternalShuffleBlockHandler, ExternalShuffleBlockResolver}\n+import org.apache.spark.network.shuffle.protocol.ExecutorShuffleInfo\n+import org.apache.spark.network.util.JavaUtils\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * This suite gets BlockData when the ExternalShuffleService is restarted\n+ * with #spark.shuffle.service.db.enabled = true or false\n+ * Note that failures in this suite may arise when#spark.shuffle.service.db.enabled = false\n+ */\n+class ExternalShuffleServiceDbSuite extends ShuffleSuite with BeforeAndAfterAll {\n+  val sortBlock0 = \"Hello!\"\n+  val sortBlock1 = \"World!\"\n+  val SORT_MANAGER = \"org.apache.spark.shuffle.sort.SortShuffleManager\"\n+\n+  var sparkConf: SparkConf = _\n+  var dataContext: TestShuffleDataContext = _\n+\n+  var securityManager: SecurityManager = _\n+  var externalShuffleService: ExternalShuffleService = _\n+  var bockHandler: ExternalShuffleBlockHandler = _\n+  var blockResolver: ExternalShuffleBlockResolver = _\n+\n+  override def beforeAll() {\n+    super.beforeAll()\n+    sparkConf = new SparkConf()\n+    sparkConf.set(\"spark.shuffle.service.enabled\", \"true\")\n+    sparkConf.set(\"spark.local.dir\", System.getProperty(\"java.io.tmpdir\"))\n+    Utils.loadDefaultSparkProperties(sparkConf, null)\n+    securityManager = new SecurityManager(sparkConf)\n+\n+    dataContext = new TestShuffleDataContext(2, 5)\n+    dataContext.create()\n+    // Write some sort data.\n+    dataContext.insertSortShuffleData(0, 0,\n+      Array[Array[Byte]](sortBlock0.getBytes(StandardCharsets.UTF_8),\n+        sortBlock1.getBytes(StandardCharsets.UTF_8)))\n+    registerExecutor()\n+  }\n+\n+  override def afterAll() {\n+    try {\n+      dataContext.cleanup()\n+    } finally {\n+      super.afterAll()\n+    }\n+  }\n+\n+  def registerExecutor(): Unit = {\n+    sparkConf.set(\"spark.shuffle.service.db.enabled\", \"true\")\n+    externalShuffleService = new ExternalShuffleService(sparkConf, securityManager)\n+\n+    // external Shuffle Service start\n+    externalShuffleService.start()\n+    bockHandler = externalShuffleService.getBlockHandler\n+    blockResolver = bockHandler.getBlockResolver\n+    blockResolver.registerExecutor(\"app0\", \"exec0\", dataContext.createExecutorInfo(SORT_MANAGER))\n+    blockResolver.closeForTest()\n+    // external Shuffle Service stop\n+    externalShuffleService.stop()\n+  }\n+\n+  // This test getBlockData will be passed when the external shuffle service is restarted.\n+  test(\"restart External Shuffle Service With InitRegisteredExecutorsDB\") {\n+    sparkConf.set(\"spark.shuffle.service.db.enabled\", \"true\")\n+    externalShuffleService = new ExternalShuffleService(sparkConf, securityManager)\n+    // externalShuffleService restart\n+    externalShuffleService.start()\n+    bockHandler = externalShuffleService.getBlockHandler\n+    blockResolver = bockHandler.getBlockResolver\n+\n+    val block0Stream = blockResolver.getBlockData(\"app0\", \"exec0\", 0, 0, 0).createInputStream\n+    val block0 = CharStreams.toString(new InputStreamReader(block0Stream, StandardCharsets.UTF_8))\n+    block0Stream.close()\n+    assert(sortBlock0 == block0)\n+    // pass\n+    blockResolver.closeForTest()\n+    // externalShuffleService stop\n+    externalShuffleService.stop()"
  }, {
    "author": {
      "login": "weixiuli"
    },
    "body": "done",
    "commit": "475d27817b6ef37eed9b7b39fee709427811f0d6",
    "createdAt": "2019-03-06T10:45:57Z",
    "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import com.google.common.io.{CharStreams, Closeables, Files}\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark.{SecurityManager, ShuffleSuite, SparkConf, SparkException}\n+import org.apache.spark.network.shuffle.{ExternalShuffleBlockHandler, ExternalShuffleBlockResolver}\n+import org.apache.spark.network.shuffle.protocol.ExecutorShuffleInfo\n+import org.apache.spark.network.util.JavaUtils\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * This suite gets BlockData when the ExternalShuffleService is restarted\n+ * with #spark.shuffle.service.db.enabled = true or false\n+ * Note that failures in this suite may arise when#spark.shuffle.service.db.enabled = false\n+ */\n+class ExternalShuffleServiceDbSuite extends ShuffleSuite with BeforeAndAfterAll {\n+  val sortBlock0 = \"Hello!\"\n+  val sortBlock1 = \"World!\"\n+  val SORT_MANAGER = \"org.apache.spark.shuffle.sort.SortShuffleManager\"\n+\n+  var sparkConf: SparkConf = _\n+  var dataContext: TestShuffleDataContext = _\n+\n+  var securityManager: SecurityManager = _\n+  var externalShuffleService: ExternalShuffleService = _\n+  var bockHandler: ExternalShuffleBlockHandler = _\n+  var blockResolver: ExternalShuffleBlockResolver = _\n+\n+  override def beforeAll() {\n+    super.beforeAll()\n+    sparkConf = new SparkConf()\n+    sparkConf.set(\"spark.shuffle.service.enabled\", \"true\")\n+    sparkConf.set(\"spark.local.dir\", System.getProperty(\"java.io.tmpdir\"))\n+    Utils.loadDefaultSparkProperties(sparkConf, null)\n+    securityManager = new SecurityManager(sparkConf)\n+\n+    dataContext = new TestShuffleDataContext(2, 5)\n+    dataContext.create()\n+    // Write some sort data.\n+    dataContext.insertSortShuffleData(0, 0,\n+      Array[Array[Byte]](sortBlock0.getBytes(StandardCharsets.UTF_8),\n+        sortBlock1.getBytes(StandardCharsets.UTF_8)))\n+    registerExecutor()\n+  }\n+\n+  override def afterAll() {\n+    try {\n+      dataContext.cleanup()\n+    } finally {\n+      super.afterAll()\n+    }\n+  }\n+\n+  def registerExecutor(): Unit = {\n+    sparkConf.set(\"spark.shuffle.service.db.enabled\", \"true\")\n+    externalShuffleService = new ExternalShuffleService(sparkConf, securityManager)\n+\n+    // external Shuffle Service start\n+    externalShuffleService.start()\n+    bockHandler = externalShuffleService.getBlockHandler\n+    blockResolver = bockHandler.getBlockResolver\n+    blockResolver.registerExecutor(\"app0\", \"exec0\", dataContext.createExecutorInfo(SORT_MANAGER))\n+    blockResolver.closeForTest()\n+    // external Shuffle Service stop\n+    externalShuffleService.stop()\n+  }\n+\n+  // This test getBlockData will be passed when the external shuffle service is restarted.\n+  test(\"restart External Shuffle Service With InitRegisteredExecutorsDB\") {\n+    sparkConf.set(\"spark.shuffle.service.db.enabled\", \"true\")\n+    externalShuffleService = new ExternalShuffleService(sparkConf, securityManager)\n+    // externalShuffleService restart\n+    externalShuffleService.start()\n+    bockHandler = externalShuffleService.getBlockHandler\n+    blockResolver = bockHandler.getBlockResolver\n+\n+    val block0Stream = blockResolver.getBlockData(\"app0\", \"exec0\", 0, 0, 0).createInputStream\n+    val block0 = CharStreams.toString(new InputStreamReader(block0Stream, StandardCharsets.UTF_8))\n+    block0Stream.close()\n+    assert(sortBlock0 == block0)\n+    // pass\n+    blockResolver.closeForTest()\n+    // externalShuffleService stop\n+    externalShuffleService.stop()"
  }, {
    "author": {
      "login": "weixiuli"
    },
    "body": "done",
    "commit": "475d27817b6ef37eed9b7b39fee709427811f0d6",
    "createdAt": "2019-03-06T10:50:13Z",
    "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import com.google.common.io.{CharStreams, Closeables, Files}\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark.{SecurityManager, ShuffleSuite, SparkConf, SparkException}\n+import org.apache.spark.network.shuffle.{ExternalShuffleBlockHandler, ExternalShuffleBlockResolver}\n+import org.apache.spark.network.shuffle.protocol.ExecutorShuffleInfo\n+import org.apache.spark.network.util.JavaUtils\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * This suite gets BlockData when the ExternalShuffleService is restarted\n+ * with #spark.shuffle.service.db.enabled = true or false\n+ * Note that failures in this suite may arise when#spark.shuffle.service.db.enabled = false\n+ */\n+class ExternalShuffleServiceDbSuite extends ShuffleSuite with BeforeAndAfterAll {\n+  val sortBlock0 = \"Hello!\"\n+  val sortBlock1 = \"World!\"\n+  val SORT_MANAGER = \"org.apache.spark.shuffle.sort.SortShuffleManager\"\n+\n+  var sparkConf: SparkConf = _\n+  var dataContext: TestShuffleDataContext = _\n+\n+  var securityManager: SecurityManager = _\n+  var externalShuffleService: ExternalShuffleService = _\n+  var bockHandler: ExternalShuffleBlockHandler = _\n+  var blockResolver: ExternalShuffleBlockResolver = _\n+\n+  override def beforeAll() {\n+    super.beforeAll()\n+    sparkConf = new SparkConf()\n+    sparkConf.set(\"spark.shuffle.service.enabled\", \"true\")\n+    sparkConf.set(\"spark.local.dir\", System.getProperty(\"java.io.tmpdir\"))\n+    Utils.loadDefaultSparkProperties(sparkConf, null)\n+    securityManager = new SecurityManager(sparkConf)\n+\n+    dataContext = new TestShuffleDataContext(2, 5)\n+    dataContext.create()\n+    // Write some sort data.\n+    dataContext.insertSortShuffleData(0, 0,\n+      Array[Array[Byte]](sortBlock0.getBytes(StandardCharsets.UTF_8),\n+        sortBlock1.getBytes(StandardCharsets.UTF_8)))\n+    registerExecutor()\n+  }\n+\n+  override def afterAll() {\n+    try {\n+      dataContext.cleanup()\n+    } finally {\n+      super.afterAll()\n+    }\n+  }\n+\n+  def registerExecutor(): Unit = {\n+    sparkConf.set(\"spark.shuffle.service.db.enabled\", \"true\")\n+    externalShuffleService = new ExternalShuffleService(sparkConf, securityManager)\n+\n+    // external Shuffle Service start\n+    externalShuffleService.start()\n+    bockHandler = externalShuffleService.getBlockHandler\n+    blockResolver = bockHandler.getBlockResolver\n+    blockResolver.registerExecutor(\"app0\", \"exec0\", dataContext.createExecutorInfo(SORT_MANAGER))\n+    blockResolver.closeForTest()\n+    // external Shuffle Service stop\n+    externalShuffleService.stop()\n+  }\n+\n+  // This test getBlockData will be passed when the external shuffle service is restarted.\n+  test(\"restart External Shuffle Service With InitRegisteredExecutorsDB\") {\n+    sparkConf.set(\"spark.shuffle.service.db.enabled\", \"true\")\n+    externalShuffleService = new ExternalShuffleService(sparkConf, securityManager)\n+    // externalShuffleService restart\n+    externalShuffleService.start()\n+    bockHandler = externalShuffleService.getBlockHandler\n+    blockResolver = bockHandler.getBlockResolver\n+\n+    val block0Stream = blockResolver.getBlockData(\"app0\", \"exec0\", 0, 0, 0).createInputStream\n+    val block0 = CharStreams.toString(new InputStreamReader(block0Stream, StandardCharsets.UTF_8))\n+    block0Stream.close()\n+    assert(sortBlock0 == block0)\n+    // pass\n+    blockResolver.closeForTest()\n+    // externalShuffleService stop\n+    externalShuffleService.stop()"
  }],
  "prId": 23393
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "I'd reword the test name & comment here a bit.  Maybe for test-name, \"Recover shuffle data with spark.shuffle.service.db.enabled=true after shuffle service restart\"\r\n\r\nand the comment should say something more like \"The beforeAll ensures the shuffle data was already written, and then the shuffle service was stopped.  Here we restart the shuffle service and make we can read the shuffle data\"",
    "commit": "475d27817b6ef37eed9b7b39fee709427811f0d6",
    "createdAt": "2019-03-04T21:42:14Z",
    "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import com.google.common.io.{CharStreams, Closeables, Files}\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark.{SecurityManager, ShuffleSuite, SparkConf, SparkException}\n+import org.apache.spark.network.shuffle.{ExternalShuffleBlockHandler, ExternalShuffleBlockResolver}\n+import org.apache.spark.network.shuffle.protocol.ExecutorShuffleInfo\n+import org.apache.spark.network.util.JavaUtils\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * This suite gets BlockData when the ExternalShuffleService is restarted\n+ * with #spark.shuffle.service.db.enabled = true or false\n+ * Note that failures in this suite may arise when#spark.shuffle.service.db.enabled = false\n+ */\n+class ExternalShuffleServiceDbSuite extends ShuffleSuite with BeforeAndAfterAll {\n+  val sortBlock0 = \"Hello!\"\n+  val sortBlock1 = \"World!\"\n+  val SORT_MANAGER = \"org.apache.spark.shuffle.sort.SortShuffleManager\"\n+\n+  var sparkConf: SparkConf = _\n+  var dataContext: TestShuffleDataContext = _\n+\n+  var securityManager: SecurityManager = _\n+  var externalShuffleService: ExternalShuffleService = _\n+  var bockHandler: ExternalShuffleBlockHandler = _\n+  var blockResolver: ExternalShuffleBlockResolver = _\n+\n+  override def beforeAll() {\n+    super.beforeAll()\n+    sparkConf = new SparkConf()\n+    sparkConf.set(\"spark.shuffle.service.enabled\", \"true\")\n+    sparkConf.set(\"spark.local.dir\", System.getProperty(\"java.io.tmpdir\"))\n+    Utils.loadDefaultSparkProperties(sparkConf, null)\n+    securityManager = new SecurityManager(sparkConf)\n+\n+    dataContext = new TestShuffleDataContext(2, 5)\n+    dataContext.create()\n+    // Write some sort data.\n+    dataContext.insertSortShuffleData(0, 0,\n+      Array[Array[Byte]](sortBlock0.getBytes(StandardCharsets.UTF_8),\n+        sortBlock1.getBytes(StandardCharsets.UTF_8)))\n+    registerExecutor()\n+  }\n+\n+  override def afterAll() {\n+    try {\n+      dataContext.cleanup()\n+    } finally {\n+      super.afterAll()\n+    }\n+  }\n+\n+  def registerExecutor(): Unit = {\n+    sparkConf.set(\"spark.shuffle.service.db.enabled\", \"true\")\n+    externalShuffleService = new ExternalShuffleService(sparkConf, securityManager)\n+\n+    // external Shuffle Service start\n+    externalShuffleService.start()\n+    bockHandler = externalShuffleService.getBlockHandler\n+    blockResolver = bockHandler.getBlockResolver\n+    blockResolver.registerExecutor(\"app0\", \"exec0\", dataContext.createExecutorInfo(SORT_MANAGER))\n+    blockResolver.closeForTest()\n+    // external Shuffle Service stop\n+    externalShuffleService.stop()\n+  }\n+\n+  // This test getBlockData will be passed when the external shuffle service is restarted.\n+  test(\"restart External Shuffle Service With InitRegisteredExecutorsDB\") {"
  }, {
    "author": {
      "login": "weixiuli"
    },
    "body": "done ,thanks your.",
    "commit": "475d27817b6ef37eed9b7b39fee709427811f0d6",
    "createdAt": "2019-03-06T10:51:06Z",
    "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import com.google.common.io.{CharStreams, Closeables, Files}\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark.{SecurityManager, ShuffleSuite, SparkConf, SparkException}\n+import org.apache.spark.network.shuffle.{ExternalShuffleBlockHandler, ExternalShuffleBlockResolver}\n+import org.apache.spark.network.shuffle.protocol.ExecutorShuffleInfo\n+import org.apache.spark.network.util.JavaUtils\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * This suite gets BlockData when the ExternalShuffleService is restarted\n+ * with #spark.shuffle.service.db.enabled = true or false\n+ * Note that failures in this suite may arise when#spark.shuffle.service.db.enabled = false\n+ */\n+class ExternalShuffleServiceDbSuite extends ShuffleSuite with BeforeAndAfterAll {\n+  val sortBlock0 = \"Hello!\"\n+  val sortBlock1 = \"World!\"\n+  val SORT_MANAGER = \"org.apache.spark.shuffle.sort.SortShuffleManager\"\n+\n+  var sparkConf: SparkConf = _\n+  var dataContext: TestShuffleDataContext = _\n+\n+  var securityManager: SecurityManager = _\n+  var externalShuffleService: ExternalShuffleService = _\n+  var bockHandler: ExternalShuffleBlockHandler = _\n+  var blockResolver: ExternalShuffleBlockResolver = _\n+\n+  override def beforeAll() {\n+    super.beforeAll()\n+    sparkConf = new SparkConf()\n+    sparkConf.set(\"spark.shuffle.service.enabled\", \"true\")\n+    sparkConf.set(\"spark.local.dir\", System.getProperty(\"java.io.tmpdir\"))\n+    Utils.loadDefaultSparkProperties(sparkConf, null)\n+    securityManager = new SecurityManager(sparkConf)\n+\n+    dataContext = new TestShuffleDataContext(2, 5)\n+    dataContext.create()\n+    // Write some sort data.\n+    dataContext.insertSortShuffleData(0, 0,\n+      Array[Array[Byte]](sortBlock0.getBytes(StandardCharsets.UTF_8),\n+        sortBlock1.getBytes(StandardCharsets.UTF_8)))\n+    registerExecutor()\n+  }\n+\n+  override def afterAll() {\n+    try {\n+      dataContext.cleanup()\n+    } finally {\n+      super.afterAll()\n+    }\n+  }\n+\n+  def registerExecutor(): Unit = {\n+    sparkConf.set(\"spark.shuffle.service.db.enabled\", \"true\")\n+    externalShuffleService = new ExternalShuffleService(sparkConf, securityManager)\n+\n+    // external Shuffle Service start\n+    externalShuffleService.start()\n+    bockHandler = externalShuffleService.getBlockHandler\n+    blockResolver = bockHandler.getBlockResolver\n+    blockResolver.registerExecutor(\"app0\", \"exec0\", dataContext.createExecutorInfo(SORT_MANAGER))\n+    blockResolver.closeForTest()\n+    // external Shuffle Service stop\n+    externalShuffleService.stop()\n+  }\n+\n+  // This test getBlockData will be passed when the external shuffle service is restarted.\n+  test(\"restart External Shuffle Service With InitRegisteredExecutorsDB\") {"
  }],
  "prId": 23393
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "same here",
    "commit": "475d27817b6ef37eed9b7b39fee709427811f0d6",
    "createdAt": "2019-03-04T21:43:04Z",
    "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import com.google.common.io.{CharStreams, Closeables, Files}\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark.{SecurityManager, ShuffleSuite, SparkConf, SparkException}\n+import org.apache.spark.network.shuffle.{ExternalShuffleBlockHandler, ExternalShuffleBlockResolver}\n+import org.apache.spark.network.shuffle.protocol.ExecutorShuffleInfo\n+import org.apache.spark.network.util.JavaUtils\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * This suite gets BlockData when the ExternalShuffleService is restarted\n+ * with #spark.shuffle.service.db.enabled = true or false\n+ * Note that failures in this suite may arise when#spark.shuffle.service.db.enabled = false\n+ */\n+class ExternalShuffleServiceDbSuite extends ShuffleSuite with BeforeAndAfterAll {\n+  val sortBlock0 = \"Hello!\"\n+  val sortBlock1 = \"World!\"\n+  val SORT_MANAGER = \"org.apache.spark.shuffle.sort.SortShuffleManager\"\n+\n+  var sparkConf: SparkConf = _\n+  var dataContext: TestShuffleDataContext = _\n+\n+  var securityManager: SecurityManager = _\n+  var externalShuffleService: ExternalShuffleService = _\n+  var bockHandler: ExternalShuffleBlockHandler = _\n+  var blockResolver: ExternalShuffleBlockResolver = _\n+\n+  override def beforeAll() {\n+    super.beforeAll()\n+    sparkConf = new SparkConf()\n+    sparkConf.set(\"spark.shuffle.service.enabled\", \"true\")\n+    sparkConf.set(\"spark.local.dir\", System.getProperty(\"java.io.tmpdir\"))\n+    Utils.loadDefaultSparkProperties(sparkConf, null)\n+    securityManager = new SecurityManager(sparkConf)\n+\n+    dataContext = new TestShuffleDataContext(2, 5)\n+    dataContext.create()\n+    // Write some sort data.\n+    dataContext.insertSortShuffleData(0, 0,\n+      Array[Array[Byte]](sortBlock0.getBytes(StandardCharsets.UTF_8),\n+        sortBlock1.getBytes(StandardCharsets.UTF_8)))\n+    registerExecutor()\n+  }\n+\n+  override def afterAll() {\n+    try {\n+      dataContext.cleanup()\n+    } finally {\n+      super.afterAll()\n+    }\n+  }\n+\n+  def registerExecutor(): Unit = {\n+    sparkConf.set(\"spark.shuffle.service.db.enabled\", \"true\")\n+    externalShuffleService = new ExternalShuffleService(sparkConf, securityManager)\n+\n+    // external Shuffle Service start\n+    externalShuffleService.start()\n+    bockHandler = externalShuffleService.getBlockHandler\n+    blockResolver = bockHandler.getBlockResolver\n+    blockResolver.registerExecutor(\"app0\", \"exec0\", dataContext.createExecutorInfo(SORT_MANAGER))\n+    blockResolver.closeForTest()\n+    // external Shuffle Service stop\n+    externalShuffleService.stop()\n+  }\n+\n+  // This test getBlockData will be passed when the external shuffle service is restarted.\n+  test(\"restart External Shuffle Service With InitRegisteredExecutorsDB\") {\n+    sparkConf.set(\"spark.shuffle.service.db.enabled\", \"true\")\n+    externalShuffleService = new ExternalShuffleService(sparkConf, securityManager)\n+    // externalShuffleService restart\n+    externalShuffleService.start()\n+    bockHandler = externalShuffleService.getBlockHandler\n+    blockResolver = bockHandler.getBlockResolver\n+\n+    val block0Stream = blockResolver.getBlockData(\"app0\", \"exec0\", 0, 0, 0).createInputStream\n+    val block0 = CharStreams.toString(new InputStreamReader(block0Stream, StandardCharsets.UTF_8))\n+    block0Stream.close()\n+    assert(sortBlock0 == block0)\n+    // pass\n+    blockResolver.closeForTest()\n+    // externalShuffleService stop\n+    externalShuffleService.stop()\n+\n+  }\n+\n+  // This test getBlockData will't be passed when the external shuffle service is restarted.\n+  test(\"restart External Shuffle Service Without InitRegisteredExecutorsDB\") {\n+    sparkConf.set(\"spark.shuffle.service.db.enabled\", \"false\")\n+    externalShuffleService = new ExternalShuffleService(sparkConf, securityManager)\n+    // externalShuffleService restart\n+    externalShuffleService.start()\n+    bockHandler = externalShuffleService.getBlockHandler\n+    blockResolver = bockHandler.getBlockResolver\n+\n+    val error = intercept[RuntimeException] {\n+      val block0Stream = blockResolver.getBlockData(\"app0\", \"exec0\", 0, 0, 0).createInputStream\n+      val block0 = CharStreams.toString(new InputStreamReader(block0Stream, StandardCharsets.UTF_8))\n+      block0Stream.close()\n+      assert(sortBlock0 == block0)\n+    }.getMessage\n+\n+    assert(error.contains(\"not registered\"))\n+    blockResolver.closeForTest()\n+    // externalShuffleService stop\n+    externalShuffleService.stop()"
  }, {
    "author": {
      "login": "weixiuli"
    },
    "body": "done",
    "commit": "475d27817b6ef37eed9b7b39fee709427811f0d6",
    "createdAt": "2019-03-06T10:51:15Z",
    "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import com.google.common.io.{CharStreams, Closeables, Files}\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark.{SecurityManager, ShuffleSuite, SparkConf, SparkException}\n+import org.apache.spark.network.shuffle.{ExternalShuffleBlockHandler, ExternalShuffleBlockResolver}\n+import org.apache.spark.network.shuffle.protocol.ExecutorShuffleInfo\n+import org.apache.spark.network.util.JavaUtils\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * This suite gets BlockData when the ExternalShuffleService is restarted\n+ * with #spark.shuffle.service.db.enabled = true or false\n+ * Note that failures in this suite may arise when#spark.shuffle.service.db.enabled = false\n+ */\n+class ExternalShuffleServiceDbSuite extends ShuffleSuite with BeforeAndAfterAll {\n+  val sortBlock0 = \"Hello!\"\n+  val sortBlock1 = \"World!\"\n+  val SORT_MANAGER = \"org.apache.spark.shuffle.sort.SortShuffleManager\"\n+\n+  var sparkConf: SparkConf = _\n+  var dataContext: TestShuffleDataContext = _\n+\n+  var securityManager: SecurityManager = _\n+  var externalShuffleService: ExternalShuffleService = _\n+  var bockHandler: ExternalShuffleBlockHandler = _\n+  var blockResolver: ExternalShuffleBlockResolver = _\n+\n+  override def beforeAll() {\n+    super.beforeAll()\n+    sparkConf = new SparkConf()\n+    sparkConf.set(\"spark.shuffle.service.enabled\", \"true\")\n+    sparkConf.set(\"spark.local.dir\", System.getProperty(\"java.io.tmpdir\"))\n+    Utils.loadDefaultSparkProperties(sparkConf, null)\n+    securityManager = new SecurityManager(sparkConf)\n+\n+    dataContext = new TestShuffleDataContext(2, 5)\n+    dataContext.create()\n+    // Write some sort data.\n+    dataContext.insertSortShuffleData(0, 0,\n+      Array[Array[Byte]](sortBlock0.getBytes(StandardCharsets.UTF_8),\n+        sortBlock1.getBytes(StandardCharsets.UTF_8)))\n+    registerExecutor()\n+  }\n+\n+  override def afterAll() {\n+    try {\n+      dataContext.cleanup()\n+    } finally {\n+      super.afterAll()\n+    }\n+  }\n+\n+  def registerExecutor(): Unit = {\n+    sparkConf.set(\"spark.shuffle.service.db.enabled\", \"true\")\n+    externalShuffleService = new ExternalShuffleService(sparkConf, securityManager)\n+\n+    // external Shuffle Service start\n+    externalShuffleService.start()\n+    bockHandler = externalShuffleService.getBlockHandler\n+    blockResolver = bockHandler.getBlockResolver\n+    blockResolver.registerExecutor(\"app0\", \"exec0\", dataContext.createExecutorInfo(SORT_MANAGER))\n+    blockResolver.closeForTest()\n+    // external Shuffle Service stop\n+    externalShuffleService.stop()\n+  }\n+\n+  // This test getBlockData will be passed when the external shuffle service is restarted.\n+  test(\"restart External Shuffle Service With InitRegisteredExecutorsDB\") {\n+    sparkConf.set(\"spark.shuffle.service.db.enabled\", \"true\")\n+    externalShuffleService = new ExternalShuffleService(sparkConf, securityManager)\n+    // externalShuffleService restart\n+    externalShuffleService.start()\n+    bockHandler = externalShuffleService.getBlockHandler\n+    blockResolver = bockHandler.getBlockResolver\n+\n+    val block0Stream = blockResolver.getBlockData(\"app0\", \"exec0\", 0, 0, 0).createInputStream\n+    val block0 = CharStreams.toString(new InputStreamReader(block0Stream, StandardCharsets.UTF_8))\n+    block0Stream.close()\n+    assert(sortBlock0 == block0)\n+    // pass\n+    blockResolver.closeForTest()\n+    // externalShuffleService stop\n+    externalShuffleService.stop()\n+\n+  }\n+\n+  // This test getBlockData will't be passed when the external shuffle service is restarted.\n+  test(\"restart External Shuffle Service Without InitRegisteredExecutorsDB\") {\n+    sparkConf.set(\"spark.shuffle.service.db.enabled\", \"false\")\n+    externalShuffleService = new ExternalShuffleService(sparkConf, securityManager)\n+    // externalShuffleService restart\n+    externalShuffleService.start()\n+    bockHandler = externalShuffleService.getBlockHandler\n+    blockResolver = bockHandler.getBlockResolver\n+\n+    val error = intercept[RuntimeException] {\n+      val block0Stream = blockResolver.getBlockData(\"app0\", \"exec0\", 0, 0, 0).createInputStream\n+      val block0 = CharStreams.toString(new InputStreamReader(block0Stream, StandardCharsets.UTF_8))\n+      block0Stream.close()\n+      assert(sortBlock0 == block0)\n+    }.getMessage\n+\n+    assert(error.contains(\"not registered\"))\n+    blockResolver.closeForTest()\n+    // externalShuffleService stop\n+    externalShuffleService.stop()"
  }],
  "prId": 23393
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "typo: will not?  but as above, I'd reword the test name and comment",
    "commit": "475d27817b6ef37eed9b7b39fee709427811f0d6",
    "createdAt": "2019-03-04T21:43:45Z",
    "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import com.google.common.io.{CharStreams, Closeables, Files}\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark.{SecurityManager, ShuffleSuite, SparkConf, SparkException}\n+import org.apache.spark.network.shuffle.{ExternalShuffleBlockHandler, ExternalShuffleBlockResolver}\n+import org.apache.spark.network.shuffle.protocol.ExecutorShuffleInfo\n+import org.apache.spark.network.util.JavaUtils\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * This suite gets BlockData when the ExternalShuffleService is restarted\n+ * with #spark.shuffle.service.db.enabled = true or false\n+ * Note that failures in this suite may arise when#spark.shuffle.service.db.enabled = false\n+ */\n+class ExternalShuffleServiceDbSuite extends ShuffleSuite with BeforeAndAfterAll {\n+  val sortBlock0 = \"Hello!\"\n+  val sortBlock1 = \"World!\"\n+  val SORT_MANAGER = \"org.apache.spark.shuffle.sort.SortShuffleManager\"\n+\n+  var sparkConf: SparkConf = _\n+  var dataContext: TestShuffleDataContext = _\n+\n+  var securityManager: SecurityManager = _\n+  var externalShuffleService: ExternalShuffleService = _\n+  var bockHandler: ExternalShuffleBlockHandler = _\n+  var blockResolver: ExternalShuffleBlockResolver = _\n+\n+  override def beforeAll() {\n+    super.beforeAll()\n+    sparkConf = new SparkConf()\n+    sparkConf.set(\"spark.shuffle.service.enabled\", \"true\")\n+    sparkConf.set(\"spark.local.dir\", System.getProperty(\"java.io.tmpdir\"))\n+    Utils.loadDefaultSparkProperties(sparkConf, null)\n+    securityManager = new SecurityManager(sparkConf)\n+\n+    dataContext = new TestShuffleDataContext(2, 5)\n+    dataContext.create()\n+    // Write some sort data.\n+    dataContext.insertSortShuffleData(0, 0,\n+      Array[Array[Byte]](sortBlock0.getBytes(StandardCharsets.UTF_8),\n+        sortBlock1.getBytes(StandardCharsets.UTF_8)))\n+    registerExecutor()\n+  }\n+\n+  override def afterAll() {\n+    try {\n+      dataContext.cleanup()\n+    } finally {\n+      super.afterAll()\n+    }\n+  }\n+\n+  def registerExecutor(): Unit = {\n+    sparkConf.set(\"spark.shuffle.service.db.enabled\", \"true\")\n+    externalShuffleService = new ExternalShuffleService(sparkConf, securityManager)\n+\n+    // external Shuffle Service start\n+    externalShuffleService.start()\n+    bockHandler = externalShuffleService.getBlockHandler\n+    blockResolver = bockHandler.getBlockResolver\n+    blockResolver.registerExecutor(\"app0\", \"exec0\", dataContext.createExecutorInfo(SORT_MANAGER))\n+    blockResolver.closeForTest()\n+    // external Shuffle Service stop\n+    externalShuffleService.stop()\n+  }\n+\n+  // This test getBlockData will be passed when the external shuffle service is restarted.\n+  test(\"restart External Shuffle Service With InitRegisteredExecutorsDB\") {\n+    sparkConf.set(\"spark.shuffle.service.db.enabled\", \"true\")\n+    externalShuffleService = new ExternalShuffleService(sparkConf, securityManager)\n+    // externalShuffleService restart\n+    externalShuffleService.start()\n+    bockHandler = externalShuffleService.getBlockHandler\n+    blockResolver = bockHandler.getBlockResolver\n+\n+    val block0Stream = blockResolver.getBlockData(\"app0\", \"exec0\", 0, 0, 0).createInputStream\n+    val block0 = CharStreams.toString(new InputStreamReader(block0Stream, StandardCharsets.UTF_8))\n+    block0Stream.close()\n+    assert(sortBlock0 == block0)\n+    // pass\n+    blockResolver.closeForTest()\n+    // externalShuffleService stop\n+    externalShuffleService.stop()\n+\n+  }\n+\n+  // This test getBlockData will't be passed when the external shuffle service is restarted.\n+  test(\"restart External Shuffle Service Without InitRegisteredExecutorsDB\") {"
  }, {
    "author": {
      "login": "weixiuli"
    },
    "body": "done",
    "commit": "475d27817b6ef37eed9b7b39fee709427811f0d6",
    "createdAt": "2019-03-06T10:51:29Z",
    "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import com.google.common.io.{CharStreams, Closeables, Files}\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark.{SecurityManager, ShuffleSuite, SparkConf, SparkException}\n+import org.apache.spark.network.shuffle.{ExternalShuffleBlockHandler, ExternalShuffleBlockResolver}\n+import org.apache.spark.network.shuffle.protocol.ExecutorShuffleInfo\n+import org.apache.spark.network.util.JavaUtils\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * This suite gets BlockData when the ExternalShuffleService is restarted\n+ * with #spark.shuffle.service.db.enabled = true or false\n+ * Note that failures in this suite may arise when#spark.shuffle.service.db.enabled = false\n+ */\n+class ExternalShuffleServiceDbSuite extends ShuffleSuite with BeforeAndAfterAll {\n+  val sortBlock0 = \"Hello!\"\n+  val sortBlock1 = \"World!\"\n+  val SORT_MANAGER = \"org.apache.spark.shuffle.sort.SortShuffleManager\"\n+\n+  var sparkConf: SparkConf = _\n+  var dataContext: TestShuffleDataContext = _\n+\n+  var securityManager: SecurityManager = _\n+  var externalShuffleService: ExternalShuffleService = _\n+  var bockHandler: ExternalShuffleBlockHandler = _\n+  var blockResolver: ExternalShuffleBlockResolver = _\n+\n+  override def beforeAll() {\n+    super.beforeAll()\n+    sparkConf = new SparkConf()\n+    sparkConf.set(\"spark.shuffle.service.enabled\", \"true\")\n+    sparkConf.set(\"spark.local.dir\", System.getProperty(\"java.io.tmpdir\"))\n+    Utils.loadDefaultSparkProperties(sparkConf, null)\n+    securityManager = new SecurityManager(sparkConf)\n+\n+    dataContext = new TestShuffleDataContext(2, 5)\n+    dataContext.create()\n+    // Write some sort data.\n+    dataContext.insertSortShuffleData(0, 0,\n+      Array[Array[Byte]](sortBlock0.getBytes(StandardCharsets.UTF_8),\n+        sortBlock1.getBytes(StandardCharsets.UTF_8)))\n+    registerExecutor()\n+  }\n+\n+  override def afterAll() {\n+    try {\n+      dataContext.cleanup()\n+    } finally {\n+      super.afterAll()\n+    }\n+  }\n+\n+  def registerExecutor(): Unit = {\n+    sparkConf.set(\"spark.shuffle.service.db.enabled\", \"true\")\n+    externalShuffleService = new ExternalShuffleService(sparkConf, securityManager)\n+\n+    // external Shuffle Service start\n+    externalShuffleService.start()\n+    bockHandler = externalShuffleService.getBlockHandler\n+    blockResolver = bockHandler.getBlockResolver\n+    blockResolver.registerExecutor(\"app0\", \"exec0\", dataContext.createExecutorInfo(SORT_MANAGER))\n+    blockResolver.closeForTest()\n+    // external Shuffle Service stop\n+    externalShuffleService.stop()\n+  }\n+\n+  // This test getBlockData will be passed when the external shuffle service is restarted.\n+  test(\"restart External Shuffle Service With InitRegisteredExecutorsDB\") {\n+    sparkConf.set(\"spark.shuffle.service.db.enabled\", \"true\")\n+    externalShuffleService = new ExternalShuffleService(sparkConf, securityManager)\n+    // externalShuffleService restart\n+    externalShuffleService.start()\n+    bockHandler = externalShuffleService.getBlockHandler\n+    blockResolver = bockHandler.getBlockResolver\n+\n+    val block0Stream = blockResolver.getBlockData(\"app0\", \"exec0\", 0, 0, 0).createInputStream\n+    val block0 = CharStreams.toString(new InputStreamReader(block0Stream, StandardCharsets.UTF_8))\n+    block0Stream.close()\n+    assert(sortBlock0 == block0)\n+    // pass\n+    blockResolver.closeForTest()\n+    // externalShuffleService stop\n+    externalShuffleService.stop()\n+\n+  }\n+\n+  // This test getBlockData will't be passed when the external shuffle service is restarted.\n+  test(\"restart External Shuffle Service Without InitRegisteredExecutorsDB\") {"
  }],
  "prId": 23393
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "comment appears to be cutoff.\r\n\r\nI don't love that this is copied from elsewhere -- can you just add the network-common as a test dependency to core, like this:\r\n\r\nhttps://github.com/apache/spark/blob/827d3718770e2c65821a1f28b889743ba872b7cf/core/pom.xml#L364-L370",
    "commit": "475d27817b6ef37eed9b7b39fee709427811f0d6",
    "createdAt": "2019-03-04T21:46:57Z",
    "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import com.google.common.io.{CharStreams, Closeables, Files}\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark.{SecurityManager, ShuffleSuite, SparkConf, SparkException}\n+import org.apache.spark.network.shuffle.{ExternalShuffleBlockHandler, ExternalShuffleBlockResolver}\n+import org.apache.spark.network.shuffle.protocol.ExecutorShuffleInfo\n+import org.apache.spark.network.util.JavaUtils\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * This suite gets BlockData when the ExternalShuffleService is restarted\n+ * with #spark.shuffle.service.db.enabled = true or false\n+ * Note that failures in this suite may arise when#spark.shuffle.service.db.enabled = false\n+ */\n+class ExternalShuffleServiceDbSuite extends ShuffleSuite with BeforeAndAfterAll {\n+  val sortBlock0 = \"Hello!\"\n+  val sortBlock1 = \"World!\"\n+  val SORT_MANAGER = \"org.apache.spark.shuffle.sort.SortShuffleManager\"\n+\n+  var sparkConf: SparkConf = _\n+  var dataContext: TestShuffleDataContext = _\n+\n+  var securityManager: SecurityManager = _\n+  var externalShuffleService: ExternalShuffleService = _\n+  var bockHandler: ExternalShuffleBlockHandler = _\n+  var blockResolver: ExternalShuffleBlockResolver = _\n+\n+  override def beforeAll() {\n+    super.beforeAll()\n+    sparkConf = new SparkConf()\n+    sparkConf.set(\"spark.shuffle.service.enabled\", \"true\")\n+    sparkConf.set(\"spark.local.dir\", System.getProperty(\"java.io.tmpdir\"))\n+    Utils.loadDefaultSparkProperties(sparkConf, null)\n+    securityManager = new SecurityManager(sparkConf)\n+\n+    dataContext = new TestShuffleDataContext(2, 5)\n+    dataContext.create()\n+    // Write some sort data.\n+    dataContext.insertSortShuffleData(0, 0,\n+      Array[Array[Byte]](sortBlock0.getBytes(StandardCharsets.UTF_8),\n+        sortBlock1.getBytes(StandardCharsets.UTF_8)))\n+    registerExecutor()\n+  }\n+\n+  override def afterAll() {\n+    try {\n+      dataContext.cleanup()\n+    } finally {\n+      super.afterAll()\n+    }\n+  }\n+\n+  def registerExecutor(): Unit = {\n+    sparkConf.set(\"spark.shuffle.service.db.enabled\", \"true\")\n+    externalShuffleService = new ExternalShuffleService(sparkConf, securityManager)\n+\n+    // external Shuffle Service start\n+    externalShuffleService.start()\n+    bockHandler = externalShuffleService.getBlockHandler\n+    blockResolver = bockHandler.getBlockResolver\n+    blockResolver.registerExecutor(\"app0\", \"exec0\", dataContext.createExecutorInfo(SORT_MANAGER))\n+    blockResolver.closeForTest()\n+    // external Shuffle Service stop\n+    externalShuffleService.stop()\n+  }\n+\n+  // This test getBlockData will be passed when the external shuffle service is restarted.\n+  test(\"restart External Shuffle Service With InitRegisteredExecutorsDB\") {\n+    sparkConf.set(\"spark.shuffle.service.db.enabled\", \"true\")\n+    externalShuffleService = new ExternalShuffleService(sparkConf, securityManager)\n+    // externalShuffleService restart\n+    externalShuffleService.start()\n+    bockHandler = externalShuffleService.getBlockHandler\n+    blockResolver = bockHandler.getBlockResolver\n+\n+    val block0Stream = blockResolver.getBlockData(\"app0\", \"exec0\", 0, 0, 0).createInputStream\n+    val block0 = CharStreams.toString(new InputStreamReader(block0Stream, StandardCharsets.UTF_8))\n+    block0Stream.close()\n+    assert(sortBlock0 == block0)\n+    // pass\n+    blockResolver.closeForTest()\n+    // externalShuffleService stop\n+    externalShuffleService.stop()\n+\n+  }\n+\n+  // This test getBlockData will't be passed when the external shuffle service is restarted.\n+  test(\"restart External Shuffle Service Without InitRegisteredExecutorsDB\") {\n+    sparkConf.set(\"spark.shuffle.service.db.enabled\", \"false\")\n+    externalShuffleService = new ExternalShuffleService(sparkConf, securityManager)\n+    // externalShuffleService restart\n+    externalShuffleService.start()\n+    bockHandler = externalShuffleService.getBlockHandler\n+    blockResolver = bockHandler.getBlockResolver\n+\n+    val error = intercept[RuntimeException] {\n+      val block0Stream = blockResolver.getBlockData(\"app0\", \"exec0\", 0, 0, 0).createInputStream\n+      val block0 = CharStreams.toString(new InputStreamReader(block0Stream, StandardCharsets.UTF_8))\n+      block0Stream.close()\n+      assert(sortBlock0 == block0)\n+    }.getMessage\n+\n+    assert(error.contains(\"not registered\"))\n+    blockResolver.closeForTest()\n+    // externalShuffleService stop\n+    externalShuffleService.stop()\n+  }\n+\n+  /**\n+   * Manages some sort-shuffle data, including the creation\n+   * and cleanup of directories that can be read by the"
  }],
  "prId": 23393
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "this block is not indented correctly",
    "commit": "475d27817b6ef37eed9b7b39fee709427811f0d6",
    "createdAt": "2019-03-06T16:55:06Z",
    "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import com.google.common.io.CharStreams\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark.{SecurityManager, ShuffleSuite, SparkConf}\n+import org.apache.spark.network.shuffle.{ExternalShuffleBlockHandler, ExternalShuffleBlockResolver}\n+import org.apache.spark.network.shuffle.TestShuffleDataContext\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * This suite gets BlockData when the ExternalShuffleService is restarted\n+ * with #spark.shuffle.service.db.enabled = true or false\n+ * Note that failures in this suite may arise when#spark.shuffle.service.db.enabled = false\n+ */\n+class ExternalShuffleServiceDbSuite extends ShuffleSuite with BeforeAndAfterAll {\n+  val sortBlock0 = \"Hello!\"\n+  val sortBlock1 = \"World!\"\n+  val SORT_MANAGER = \"org.apache.spark.shuffle.sort.SortShuffleManager\"\n+\n+  var sparkConf: SparkConf = _\n+  var dataContext: TestShuffleDataContext = _\n+\n+  var securityManager: SecurityManager = _\n+  var externalShuffleService: ExternalShuffleService = _\n+  var blockHandler: ExternalShuffleBlockHandler = _\n+  var blockResolver: ExternalShuffleBlockResolver = _\n+\n+  override def beforeAll() {\n+    super.beforeAll()\n+    sparkConf = new SparkConf()\n+    sparkConf.set(\"spark.shuffle.service.enabled\", \"true\")\n+    sparkConf.set(\"spark.local.dir\", System.getProperty(\"java.io.tmpdir\"))\n+    Utils.loadDefaultSparkProperties(sparkConf, null)\n+    securityManager = new SecurityManager(sparkConf)\n+\n+    dataContext = new TestShuffleDataContext(2, 5)\n+    dataContext.create()\n+    // Write some sort data.\n+    dataContext.insertSortShuffleData(0, 0,\n+      Array[Array[Byte]](sortBlock0.getBytes(StandardCharsets.UTF_8),\n+        sortBlock1.getBytes(StandardCharsets.UTF_8)))\n+    registerExecutor()\n+  }\n+\n+  override def afterAll() {\n+    try {\n+      dataContext.cleanup()\n+    } finally {\n+      super.afterAll()\n+    }\n+  }\n+\n+  def registerExecutor(): Unit = {\n+    try {\n+      sparkConf.set(\"spark.shuffle.service.db.enabled\", \"true\")\n+      externalShuffleService = new ExternalShuffleService(sparkConf, securityManager)\n+\n+      // external Shuffle Service start\n+      externalShuffleService.start()\n+      blockHandler = externalShuffleService.getBlockHandler\n+      blockResolver = blockHandler.getBlockResolver\n+      blockResolver.registerExecutor(\"app0\", \"exec0\", dataContext.createExecutorInfo(SORT_MANAGER))\n+    } finally {\n+      blockHandler.close()\n+      // external Shuffle Service stop\n+      externalShuffleService.stop()\n+    }\n+  }\n+\n+  // The beforeAll ensures the shuffle data was already written, and then\n+  // the shuffle service was stopped. Here we restart the shuffle service\n+  // and make we can read the shuffle data\n+  test(\"Recover shuffle data with spark.shuffle.service.db.enabled=true after \" +\n+    \"shuffle service restart\") {\n+    try {\n+      sparkConf.set(\"spark.shuffle.service.db.enabled\", \"true\")\n+      externalShuffleService = new ExternalShuffleService(sparkConf, securityManager)\n+      // externalShuffleService restart\n+      externalShuffleService.start()\n+      blockHandler = externalShuffleService.getBlockHandler\n+      blockResolver = blockHandler.getBlockResolver\n+\n+      val block0Stream = blockResolver.getBlockData(\"app0\", \"exec0\", 0, 0, 0).createInputStream\n+      val block0 = CharStreams.toString(new InputStreamReader(block0Stream, StandardCharsets.UTF_8))\n+      block0Stream.close()\n+      assert(sortBlock0 == block0)\n+      // pass\n+    } finally {\n+      blockHandler.close()\n+      // externalShuffleService stop\n+      externalShuffleService.stop()\n+    }\n+\n+  }\n+\n+  // The beforeAll ensures the shuffle data was already written, and then\n+  // the shuffle service was stopped. Here we restart the shuffle service ,\n+  // but we can't read the shuffle data\n+  test(\"Can't recover shuffle data with spark.shuffle.service.db.enabled=false after\" +\n+    \" shuffle service restart\") {\n+    try {\n+      sparkConf.set(\"spark.shuffle.service.db.enabled\", \"false\")\n+      externalShuffleService = new ExternalShuffleService(sparkConf, securityManager)\n+      // externalShuffleService restart\n+      externalShuffleService.start()\n+      blockHandler = externalShuffleService.getBlockHandler\n+      blockResolver = blockHandler.getBlockResolver\n+\n+    val error = intercept[RuntimeException] {\n+      val block0Stream = blockResolver.getBlockData(\"app0\", \"exec0\", 0, 0, 0).createInputStream\n+      val block0 = CharStreams.toString(new InputStreamReader(block0Stream, StandardCharsets.UTF_8))\n+      block0Stream.close()\n+      assert(sortBlock0 == block0)\n+    }.getMessage"
  }],
  "prId": 23393
}, {
  "comments": [{
    "author": {
      "login": "attilapiros"
    },
    "body": "indentation?",
    "commit": "475d27817b6ef37eed9b7b39fee709427811f0d6",
    "createdAt": "2019-03-06T16:55:54Z",
    "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import com.google.common.io.CharStreams\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark.{SecurityManager, ShuffleSuite, SparkConf}\n+import org.apache.spark.network.shuffle.{ExternalShuffleBlockHandler, ExternalShuffleBlockResolver}\n+import org.apache.spark.network.shuffle.TestShuffleDataContext\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * This suite gets BlockData when the ExternalShuffleService is restarted\n+ * with #spark.shuffle.service.db.enabled = true or false\n+ * Note that failures in this suite may arise when#spark.shuffle.service.db.enabled = false\n+ */\n+class ExternalShuffleServiceDbSuite extends ShuffleSuite with BeforeAndAfterAll {\n+  val sortBlock0 = \"Hello!\"\n+  val sortBlock1 = \"World!\"\n+  val SORT_MANAGER = \"org.apache.spark.shuffle.sort.SortShuffleManager\"\n+\n+  var sparkConf: SparkConf = _\n+  var dataContext: TestShuffleDataContext = _\n+\n+  var securityManager: SecurityManager = _\n+  var externalShuffleService: ExternalShuffleService = _\n+  var blockHandler: ExternalShuffleBlockHandler = _\n+  var blockResolver: ExternalShuffleBlockResolver = _\n+\n+  override def beforeAll() {\n+    super.beforeAll()\n+    sparkConf = new SparkConf()\n+    sparkConf.set(\"spark.shuffle.service.enabled\", \"true\")\n+    sparkConf.set(\"spark.local.dir\", System.getProperty(\"java.io.tmpdir\"))\n+    Utils.loadDefaultSparkProperties(sparkConf, null)\n+    securityManager = new SecurityManager(sparkConf)\n+\n+    dataContext = new TestShuffleDataContext(2, 5)\n+    dataContext.create()\n+    // Write some sort data.\n+    dataContext.insertSortShuffleData(0, 0,\n+      Array[Array[Byte]](sortBlock0.getBytes(StandardCharsets.UTF_8),\n+        sortBlock1.getBytes(StandardCharsets.UTF_8)))\n+    registerExecutor()\n+  }\n+\n+  override def afterAll() {\n+    try {\n+      dataContext.cleanup()\n+    } finally {\n+      super.afterAll()\n+    }\n+  }\n+\n+  def registerExecutor(): Unit = {\n+    try {\n+      sparkConf.set(\"spark.shuffle.service.db.enabled\", \"true\")\n+      externalShuffleService = new ExternalShuffleService(sparkConf, securityManager)\n+\n+      // external Shuffle Service start\n+      externalShuffleService.start()\n+      blockHandler = externalShuffleService.getBlockHandler\n+      blockResolver = blockHandler.getBlockResolver\n+      blockResolver.registerExecutor(\"app0\", \"exec0\", dataContext.createExecutorInfo(SORT_MANAGER))\n+    } finally {\n+      blockHandler.close()\n+      // external Shuffle Service stop\n+      externalShuffleService.stop()\n+    }\n+  }\n+\n+  // The beforeAll ensures the shuffle data was already written, and then\n+  // the shuffle service was stopped. Here we restart the shuffle service\n+  // and make we can read the shuffle data\n+  test(\"Recover shuffle data with spark.shuffle.service.db.enabled=true after \" +\n+    \"shuffle service restart\") {\n+    try {\n+      sparkConf.set(\"spark.shuffle.service.db.enabled\", \"true\")\n+      externalShuffleService = new ExternalShuffleService(sparkConf, securityManager)\n+      // externalShuffleService restart\n+      externalShuffleService.start()\n+      blockHandler = externalShuffleService.getBlockHandler\n+      blockResolver = blockHandler.getBlockResolver\n+\n+      val block0Stream = blockResolver.getBlockData(\"app0\", \"exec0\", 0, 0, 0).createInputStream\n+      val block0 = CharStreams.toString(new InputStreamReader(block0Stream, StandardCharsets.UTF_8))\n+      block0Stream.close()\n+      assert(sortBlock0 == block0)\n+      // pass\n+    } finally {\n+      blockHandler.close()\n+      // externalShuffleService stop\n+      externalShuffleService.stop()\n+    }\n+\n+  }\n+\n+  // The beforeAll ensures the shuffle data was already written, and then\n+  // the shuffle service was stopped. Here we restart the shuffle service ,\n+  // but we can't read the shuffle data\n+  test(\"Can't recover shuffle data with spark.shuffle.service.db.enabled=false after\" +\n+    \" shuffle service restart\") {\n+    try {\n+      sparkConf.set(\"spark.shuffle.service.db.enabled\", \"false\")\n+      externalShuffleService = new ExternalShuffleService(sparkConf, securityManager)\n+      // externalShuffleService restart\n+      externalShuffleService.start()\n+      blockHandler = externalShuffleService.getBlockHandler\n+      blockResolver = blockHandler.getBlockResolver\n+\n+    val error = intercept[RuntimeException] {"
  }, {
    "author": {
      "login": "weixiuli"
    },
    "body": "done",
    "commit": "475d27817b6ef37eed9b7b39fee709427811f0d6",
    "createdAt": "2019-03-07T13:17:43Z",
    "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import com.google.common.io.CharStreams\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark.{SecurityManager, ShuffleSuite, SparkConf}\n+import org.apache.spark.network.shuffle.{ExternalShuffleBlockHandler, ExternalShuffleBlockResolver}\n+import org.apache.spark.network.shuffle.TestShuffleDataContext\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * This suite gets BlockData when the ExternalShuffleService is restarted\n+ * with #spark.shuffle.service.db.enabled = true or false\n+ * Note that failures in this suite may arise when#spark.shuffle.service.db.enabled = false\n+ */\n+class ExternalShuffleServiceDbSuite extends ShuffleSuite with BeforeAndAfterAll {\n+  val sortBlock0 = \"Hello!\"\n+  val sortBlock1 = \"World!\"\n+  val SORT_MANAGER = \"org.apache.spark.shuffle.sort.SortShuffleManager\"\n+\n+  var sparkConf: SparkConf = _\n+  var dataContext: TestShuffleDataContext = _\n+\n+  var securityManager: SecurityManager = _\n+  var externalShuffleService: ExternalShuffleService = _\n+  var blockHandler: ExternalShuffleBlockHandler = _\n+  var blockResolver: ExternalShuffleBlockResolver = _\n+\n+  override def beforeAll() {\n+    super.beforeAll()\n+    sparkConf = new SparkConf()\n+    sparkConf.set(\"spark.shuffle.service.enabled\", \"true\")\n+    sparkConf.set(\"spark.local.dir\", System.getProperty(\"java.io.tmpdir\"))\n+    Utils.loadDefaultSparkProperties(sparkConf, null)\n+    securityManager = new SecurityManager(sparkConf)\n+\n+    dataContext = new TestShuffleDataContext(2, 5)\n+    dataContext.create()\n+    // Write some sort data.\n+    dataContext.insertSortShuffleData(0, 0,\n+      Array[Array[Byte]](sortBlock0.getBytes(StandardCharsets.UTF_8),\n+        sortBlock1.getBytes(StandardCharsets.UTF_8)))\n+    registerExecutor()\n+  }\n+\n+  override def afterAll() {\n+    try {\n+      dataContext.cleanup()\n+    } finally {\n+      super.afterAll()\n+    }\n+  }\n+\n+  def registerExecutor(): Unit = {\n+    try {\n+      sparkConf.set(\"spark.shuffle.service.db.enabled\", \"true\")\n+      externalShuffleService = new ExternalShuffleService(sparkConf, securityManager)\n+\n+      // external Shuffle Service start\n+      externalShuffleService.start()\n+      blockHandler = externalShuffleService.getBlockHandler\n+      blockResolver = blockHandler.getBlockResolver\n+      blockResolver.registerExecutor(\"app0\", \"exec0\", dataContext.createExecutorInfo(SORT_MANAGER))\n+    } finally {\n+      blockHandler.close()\n+      // external Shuffle Service stop\n+      externalShuffleService.stop()\n+    }\n+  }\n+\n+  // The beforeAll ensures the shuffle data was already written, and then\n+  // the shuffle service was stopped. Here we restart the shuffle service\n+  // and make we can read the shuffle data\n+  test(\"Recover shuffle data with spark.shuffle.service.db.enabled=true after \" +\n+    \"shuffle service restart\") {\n+    try {\n+      sparkConf.set(\"spark.shuffle.service.db.enabled\", \"true\")\n+      externalShuffleService = new ExternalShuffleService(sparkConf, securityManager)\n+      // externalShuffleService restart\n+      externalShuffleService.start()\n+      blockHandler = externalShuffleService.getBlockHandler\n+      blockResolver = blockHandler.getBlockResolver\n+\n+      val block0Stream = blockResolver.getBlockData(\"app0\", \"exec0\", 0, 0, 0).createInputStream\n+      val block0 = CharStreams.toString(new InputStreamReader(block0Stream, StandardCharsets.UTF_8))\n+      block0Stream.close()\n+      assert(sortBlock0 == block0)\n+      // pass\n+    } finally {\n+      blockHandler.close()\n+      // externalShuffleService stop\n+      externalShuffleService.stop()\n+    }\n+\n+  }\n+\n+  // The beforeAll ensures the shuffle data was already written, and then\n+  // the shuffle service was stopped. Here we restart the shuffle service ,\n+  // but we can't read the shuffle data\n+  test(\"Can't recover shuffle data with spark.shuffle.service.db.enabled=false after\" +\n+    \" shuffle service restart\") {\n+    try {\n+      sparkConf.set(\"spark.shuffle.service.db.enabled\", \"false\")\n+      externalShuffleService = new ExternalShuffleService(sparkConf, securityManager)\n+      // externalShuffleService restart\n+      externalShuffleService.start()\n+      blockHandler = externalShuffleService.getBlockHandler\n+      blockResolver = blockHandler.getBlockResolver\n+\n+    val error = intercept[RuntimeException] {"
  }],
  "prId": 23393
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "actually, sorry one important thing -- why does this extend `ShuffleSuite`?  Its not really changing the behavior that `ShuffleSuite` is designed for",
    "commit": "475d27817b6ef37eed9b7b39fee709427811f0d6",
    "createdAt": "2019-03-06T17:04:28Z",
    "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import com.google.common.io.CharStreams\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark.{SecurityManager, ShuffleSuite, SparkConf}\n+import org.apache.spark.network.shuffle.{ExternalShuffleBlockHandler, ExternalShuffleBlockResolver}\n+import org.apache.spark.network.shuffle.TestShuffleDataContext\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * This suite gets BlockData when the ExternalShuffleService is restarted\n+ * with #spark.shuffle.service.db.enabled = true or false\n+ * Note that failures in this suite may arise when#spark.shuffle.service.db.enabled = false\n+ */\n+class ExternalShuffleServiceDbSuite extends ShuffleSuite with BeforeAndAfterAll {"
  }, {
    "author": {
      "login": "weixiuli"
    },
    "body": "sorry, I've replaced   it  with  `SparkFunSuite`.",
    "commit": "475d27817b6ef37eed9b7b39fee709427811f0d6",
    "createdAt": "2019-03-07T13:25:55Z",
    "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import com.google.common.io.CharStreams\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark.{SecurityManager, ShuffleSuite, SparkConf}\n+import org.apache.spark.network.shuffle.{ExternalShuffleBlockHandler, ExternalShuffleBlockResolver}\n+import org.apache.spark.network.shuffle.TestShuffleDataContext\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * This suite gets BlockData when the ExternalShuffleService is restarted\n+ * with #spark.shuffle.service.db.enabled = true or false\n+ * Note that failures in this suite may arise when#spark.shuffle.service.db.enabled = false\n+ */\n+class ExternalShuffleServiceDbSuite extends ShuffleSuite with BeforeAndAfterAll {"
  }],
  "prId": 23393
}, {
  "comments": [{
    "author": {
      "login": "attilapiros"
    },
    "body": "This assert is not needed.",
    "commit": "475d27817b6ef37eed9b7b39fee709427811f0d6",
    "createdAt": "2019-03-07T14:46:32Z",
    "diffHunk": "@@ -0,0 +1,145 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import com.google.common.io.CharStreams\n+\n+import org.apache.spark.{SecurityManager, SparkConf, SparkFunSuite}\n+import org.apache.spark.network.shuffle.{ExternalShuffleBlockHandler, ExternalShuffleBlockResolver}\n+import org.apache.spark.network.shuffle.TestShuffleDataContext\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * This suite gets BlockData when the ExternalShuffleService is restarted\n+ * with #spark.shuffle.service.db.enabled = true or false\n+ * Note that failures in this suite may arise when#spark.shuffle.service.db.enabled = false\n+ */\n+class ExternalShuffleServiceDbSuite extends SparkFunSuite {\n+  val sortBlock0 = \"Hello!\"\n+  val sortBlock1 = \"World!\"\n+  val SORT_MANAGER = \"org.apache.spark.shuffle.sort.SortShuffleManager\"\n+\n+  var sparkConf: SparkConf = _\n+  var dataContext: TestShuffleDataContext = _\n+\n+  var securityManager: SecurityManager = _\n+  var externalShuffleService: ExternalShuffleService = _\n+  var blockHandler: ExternalShuffleBlockHandler = _\n+  var blockResolver: ExternalShuffleBlockResolver = _\n+\n+  override def beforeAll() {\n+    super.beforeAll()\n+    sparkConf = new SparkConf()\n+    sparkConf.set(\"spark.shuffle.service.enabled\", \"true\")\n+    sparkConf.set(\"spark.local.dir\", System.getProperty(\"java.io.tmpdir\"))\n+    Utils.loadDefaultSparkProperties(sparkConf, null)\n+    securityManager = new SecurityManager(sparkConf)\n+\n+    dataContext = new TestShuffleDataContext(2, 5)\n+    dataContext.create()\n+    // Write some sort data.\n+    dataContext.insertSortShuffleData(0, 0,\n+      Array[Array[Byte]](sortBlock0.getBytes(StandardCharsets.UTF_8),\n+        sortBlock1.getBytes(StandardCharsets.UTF_8)))\n+    registerExecutor()\n+  }\n+\n+  override def afterAll() {\n+    try {\n+      dataContext.cleanup()\n+    } finally {\n+      super.afterAll()\n+    }\n+  }\n+\n+  def registerExecutor(): Unit = {\n+    try {\n+      sparkConf.set(\"spark.shuffle.service.db.enabled\", \"true\")\n+      externalShuffleService = new ExternalShuffleService(sparkConf, securityManager)\n+\n+      // external Shuffle Service start\n+      externalShuffleService.start()\n+      blockHandler = externalShuffleService.getBlockHandler\n+      blockResolver = blockHandler.getBlockResolver\n+      blockResolver.registerExecutor(\"app0\", \"exec0\", dataContext.createExecutorInfo(SORT_MANAGER))\n+    } finally {\n+      blockHandler.close()\n+      // external Shuffle Service stop\n+      externalShuffleService.stop()\n+    }\n+  }\n+\n+  // The beforeAll ensures the shuffle data was already written, and then\n+  // the shuffle service was stopped. Here we restart the shuffle service\n+  // and make we can read the shuffle data\n+  test(\"Recover shuffle data with spark.shuffle.service.db.enabled=true after \" +\n+    \"shuffle service restart\") {\n+    try {\n+      sparkConf.set(\"spark.shuffle.service.db.enabled\", \"true\")\n+      externalShuffleService = new ExternalShuffleService(sparkConf, securityManager)\n+      // externalShuffleService restart\n+      externalShuffleService.start()\n+      blockHandler = externalShuffleService.getBlockHandler\n+      blockResolver = blockHandler.getBlockResolver\n+\n+      val block0Stream = blockResolver.getBlockData(\"app0\", \"exec0\", 0, 0, 0).createInputStream\n+      val block0 = CharStreams.toString(new InputStreamReader(block0Stream, StandardCharsets.UTF_8))\n+      block0Stream.close()\n+      assert(sortBlock0 == block0)\n+      // pass\n+    } finally {\n+      blockHandler.close()\n+      // externalShuffleService stop\n+      externalShuffleService.stop()\n+    }\n+\n+  }\n+\n+  // The beforeAll ensures the shuffle data was already written, and then\n+  // the shuffle service was stopped. Here we restart the shuffle service ,\n+  // but we can't read the shuffle data\n+  test(\"Can't recover shuffle data with spark.shuffle.service.db.enabled=false after\" +\n+    \" shuffle service restart\") {\n+    try {\n+      sparkConf.set(\"spark.shuffle.service.db.enabled\", \"false\")\n+      externalShuffleService = new ExternalShuffleService(sparkConf, securityManager)\n+      // externalShuffleService restart\n+      externalShuffleService.start()\n+      blockHandler = externalShuffleService.getBlockHandler\n+      blockResolver = blockHandler.getBlockResolver\n+\n+      val error = intercept[RuntimeException] {\n+        val block0Stream = blockResolver.getBlockData(\"app0\", \"exec0\", 0, 0, 0)\n+          .createInputStream\n+        val block0 = CharStreams.toString(new InputStreamReader(block0Stream,\n+          StandardCharsets.UTF_8))\n+        block0Stream.close()\n+        assert(sortBlock0 == block0)"
  }],
  "prId": 23393
}]