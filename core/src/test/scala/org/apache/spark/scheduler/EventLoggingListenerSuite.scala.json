[{
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "added -> removed\r\n\r\nthough for that matter -- I'd just remove the doc comments on all these teeny helper methods",
    "commit": "571285beace1a0c1df92d9f5127828ed8955c93f",
    "createdAt": "2018-05-23T18:45:47Z",
    "diffHunk": "@@ -251,6 +261,233 @@ class EventLoggingListenerSuite extends SparkFunSuite with LocalSparkContext wit\n     }\n   }\n \n+  /**\n+   * Test executor metrics update logging functionality. This checks that a\n+   * SparkListenerExecutorMetricsUpdate event is added to the Spark history\n+   * log if one of the executor metrics is larger than any previously\n+   * recorded value for the metric, per executor per stage. The task metrics\n+   * should not be added.\n+   */\n+  private def testExecutorMetricsUpdateEventLogging() {\n+    val conf = getLoggingConf(testDirPath, None)\n+    val logName = \"executorMetricsUpdated-test\"\n+    val eventLogger = new EventLoggingListener(logName, None, testDirPath.toUri(), conf)\n+    val listenerBus = new LiveListenerBus(conf)\n+\n+    // expected ExecutorMetricsUpdate, for the given stage id and executor id\n+    val expectedMetricsEvents: Map[(Int, String), SparkListenerExecutorMetricsUpdate] =\n+      Map(\n+        ((0, \"1\"),\n+          createExecutorMetricsUpdateEvent(1,\n+            new ExecutorMetrics(-1L, 5000L, 50L, 50L, 20L, 50L, 10L, 100L, 30L, 70L, 20L))),\n+        ((0, \"2\"),\n+          createExecutorMetricsUpdateEvent(2,\n+            new ExecutorMetrics(-1L, 7000L, 70L, 50L, 20L, 10L, 10L, 50L, 30L, 80L, 40L))),\n+        ((1, \"1\"),\n+          createExecutorMetricsUpdateEvent(1,\n+            new ExecutorMetrics(-1L, 7000L, 70L, 50L, 30L, 60L, 30L, 80L, 55L, 50L, 0L))),\n+        ((1, \"2\"),\n+          createExecutorMetricsUpdateEvent(2,\n+            new ExecutorMetrics(-1L, 7000L, 70L, 50L, 40L, 10L, 30L, 50L, 60L, 40L, 40L))))\n+\n+    // Events to post.\n+    val events = Array(\n+      SparkListenerApplicationStart(\"executionMetrics\", None,\n+        1L, \"update\", None),\n+      createExecutorAddedEvent(1),\n+      createExecutorAddedEvent(2),\n+      createStageSubmittedEvent(0),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(10L, 4000L, 50L, 20L, 0L, 40L, 0L, 60L, 0L, 70L, 20L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(10L, 1500L, 50L, 20L, 0L, 0L, 0L, 20L, 0L, 70L, 0L)),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(15L, 4000L, 50L, 50L, 0L, 50L, 0L, 100L, 0L, 70L, 20L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(15L, 2000L, 50L, 10L, 0L, 10L, 0L, 30L, 0L, 70L, 0L)),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(20L, 2000L, 40L, 50L, 0L, 40L, 10L, 90L, 10L, 50L, 0L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(20L, 3500L, 50L, 15L, 0L, 10L, 10L, 35L, 10L, 80L, 0L)),\n+      createStageSubmittedEvent(1),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(25L, 5000L, 30L, 50L, 20L, 30L, 10L, 80L, 30L, 50L, 0L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(25L, 7000L, 70L, 50L, 20L, 0L, 10L, 50L, 30L, 10L, 40L)),\n+      createStageCompletedEvent(0),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(30L, 6000L, 70L, 20L, 30L, 10L, 0L, 30L, 30L, 30L, 0L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(30L, 5500L, 30L, 20L, 40L, 10L, 0L, 30L, 40L, 40L, 20L)),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(35L, 7000L, 70L, 5L, 25L, 60L, 30L, 65L, 55L, 30L, 0L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(35L, 5500L, 40L, 25L, 30L, 10L, 30L, 35L, 60L, 0L, 20L)),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(40L, 5500L, 70L, 15L, 20L, 55L, 20L, 70L, 40L, 20L, 0L)),\n+      createExecutorRemovedEvent(1),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(40L, 4000L, 20L, 25L, 30L, 10L, 30L, 35L, 60L, 0L, 0L)),\n+      createStageCompletedEvent(1),\n+      SparkListenerApplicationEnd(1000L))\n+\n+    // play the events for the event logger\n+    eventLogger.start()\n+    listenerBus.start(Mockito.mock(classOf[SparkContext]), Mockito.mock(classOf[MetricsSystem]))\n+    listenerBus.addToEventLogQueue(eventLogger)\n+    events.foreach(event => listenerBus.post(event))\n+    listenerBus.stop()\n+    eventLogger.stop()\n+\n+    // Verify the log file contains the expected events.\n+    // Posted events should be logged, except for ExecutorMetricsUpdate events -- these\n+    // are consolidated, and the peak values for each stage are logged at stage end.\n+    val logData = EventLoggingListener.openEventLog(new Path(eventLogger.logPath), fileSystem)\n+    try {\n+      val lines = readLines(logData)\n+      val logStart = SparkListenerLogStart(SPARK_VERSION)\n+      assert(lines.size === 14)\n+      assert(lines(0).contains(\"SparkListenerLogStart\"))\n+      assert(lines(1).contains(\"SparkListenerApplicationStart\"))\n+      assert(JsonProtocol.sparkEventFromJson(parse(lines(0))) === logStart)\n+      var i = 1\n+      events.foreach {event =>\n+        event match {\n+          case metricsUpdate: SparkListenerExecutorMetricsUpdate =>\n+          case stageCompleted: SparkListenerStageCompleted =>\n+            for (j <- 1 to 2) {\n+              checkExecutorMetricsUpdate(lines(i), stageCompleted.stageInfo.stageId,\n+                expectedMetricsEvents)\n+                i += 1\n+             }\n+            checkEvent(lines(i), event)\n+            i += 1\n+        case _ =>\n+          checkEvent(lines(i), event)\n+          i += 1\n+        }\n+      }\n+    } finally {\n+      logData.close()\n+    }\n+  }\n+\n+  /** Create a stage submitted event for the specified stage Id. */\n+  private def createStageSubmittedEvent(stageId: Int) = {\n+    SparkListenerStageSubmitted(new StageInfo(stageId, 0, stageId.toString, 0,\n+      Seq.empty, Seq.empty, \"details\"))\n+  }\n+\n+  /** Create a stage completed event for the specified stage Id. */\n+  private def createStageCompletedEvent(stageId: Int) = {\n+    SparkListenerStageCompleted(new StageInfo(stageId, 0, stageId.toString, 0,\n+      Seq.empty, Seq.empty, \"details\"))\n+  }\n+\n+  /** Create an executor added event for the specified executor Id. */\n+  private def createExecutorAddedEvent(executorId: Int) = {\n+    SparkListenerExecutorAdded(0L, executorId.toString, new ExecutorInfo(\"host1\", 1, Map.empty))\n+  }\n+\n+  /** Create an executor added event for the specified executor Id. */"
  }, {
    "author": {
      "login": "edwinalu"
    },
    "body": "I'll remove -- they are pretty self-explanatory.",
    "commit": "571285beace1a0c1df92d9f5127828ed8955c93f",
    "createdAt": "2018-05-25T20:57:32Z",
    "diffHunk": "@@ -251,6 +261,233 @@ class EventLoggingListenerSuite extends SparkFunSuite with LocalSparkContext wit\n     }\n   }\n \n+  /**\n+   * Test executor metrics update logging functionality. This checks that a\n+   * SparkListenerExecutorMetricsUpdate event is added to the Spark history\n+   * log if one of the executor metrics is larger than any previously\n+   * recorded value for the metric, per executor per stage. The task metrics\n+   * should not be added.\n+   */\n+  private def testExecutorMetricsUpdateEventLogging() {\n+    val conf = getLoggingConf(testDirPath, None)\n+    val logName = \"executorMetricsUpdated-test\"\n+    val eventLogger = new EventLoggingListener(logName, None, testDirPath.toUri(), conf)\n+    val listenerBus = new LiveListenerBus(conf)\n+\n+    // expected ExecutorMetricsUpdate, for the given stage id and executor id\n+    val expectedMetricsEvents: Map[(Int, String), SparkListenerExecutorMetricsUpdate] =\n+      Map(\n+        ((0, \"1\"),\n+          createExecutorMetricsUpdateEvent(1,\n+            new ExecutorMetrics(-1L, 5000L, 50L, 50L, 20L, 50L, 10L, 100L, 30L, 70L, 20L))),\n+        ((0, \"2\"),\n+          createExecutorMetricsUpdateEvent(2,\n+            new ExecutorMetrics(-1L, 7000L, 70L, 50L, 20L, 10L, 10L, 50L, 30L, 80L, 40L))),\n+        ((1, \"1\"),\n+          createExecutorMetricsUpdateEvent(1,\n+            new ExecutorMetrics(-1L, 7000L, 70L, 50L, 30L, 60L, 30L, 80L, 55L, 50L, 0L))),\n+        ((1, \"2\"),\n+          createExecutorMetricsUpdateEvent(2,\n+            new ExecutorMetrics(-1L, 7000L, 70L, 50L, 40L, 10L, 30L, 50L, 60L, 40L, 40L))))\n+\n+    // Events to post.\n+    val events = Array(\n+      SparkListenerApplicationStart(\"executionMetrics\", None,\n+        1L, \"update\", None),\n+      createExecutorAddedEvent(1),\n+      createExecutorAddedEvent(2),\n+      createStageSubmittedEvent(0),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(10L, 4000L, 50L, 20L, 0L, 40L, 0L, 60L, 0L, 70L, 20L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(10L, 1500L, 50L, 20L, 0L, 0L, 0L, 20L, 0L, 70L, 0L)),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(15L, 4000L, 50L, 50L, 0L, 50L, 0L, 100L, 0L, 70L, 20L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(15L, 2000L, 50L, 10L, 0L, 10L, 0L, 30L, 0L, 70L, 0L)),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(20L, 2000L, 40L, 50L, 0L, 40L, 10L, 90L, 10L, 50L, 0L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(20L, 3500L, 50L, 15L, 0L, 10L, 10L, 35L, 10L, 80L, 0L)),\n+      createStageSubmittedEvent(1),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(25L, 5000L, 30L, 50L, 20L, 30L, 10L, 80L, 30L, 50L, 0L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(25L, 7000L, 70L, 50L, 20L, 0L, 10L, 50L, 30L, 10L, 40L)),\n+      createStageCompletedEvent(0),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(30L, 6000L, 70L, 20L, 30L, 10L, 0L, 30L, 30L, 30L, 0L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(30L, 5500L, 30L, 20L, 40L, 10L, 0L, 30L, 40L, 40L, 20L)),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(35L, 7000L, 70L, 5L, 25L, 60L, 30L, 65L, 55L, 30L, 0L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(35L, 5500L, 40L, 25L, 30L, 10L, 30L, 35L, 60L, 0L, 20L)),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(40L, 5500L, 70L, 15L, 20L, 55L, 20L, 70L, 40L, 20L, 0L)),\n+      createExecutorRemovedEvent(1),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(40L, 4000L, 20L, 25L, 30L, 10L, 30L, 35L, 60L, 0L, 0L)),\n+      createStageCompletedEvent(1),\n+      SparkListenerApplicationEnd(1000L))\n+\n+    // play the events for the event logger\n+    eventLogger.start()\n+    listenerBus.start(Mockito.mock(classOf[SparkContext]), Mockito.mock(classOf[MetricsSystem]))\n+    listenerBus.addToEventLogQueue(eventLogger)\n+    events.foreach(event => listenerBus.post(event))\n+    listenerBus.stop()\n+    eventLogger.stop()\n+\n+    // Verify the log file contains the expected events.\n+    // Posted events should be logged, except for ExecutorMetricsUpdate events -- these\n+    // are consolidated, and the peak values for each stage are logged at stage end.\n+    val logData = EventLoggingListener.openEventLog(new Path(eventLogger.logPath), fileSystem)\n+    try {\n+      val lines = readLines(logData)\n+      val logStart = SparkListenerLogStart(SPARK_VERSION)\n+      assert(lines.size === 14)\n+      assert(lines(0).contains(\"SparkListenerLogStart\"))\n+      assert(lines(1).contains(\"SparkListenerApplicationStart\"))\n+      assert(JsonProtocol.sparkEventFromJson(parse(lines(0))) === logStart)\n+      var i = 1\n+      events.foreach {event =>\n+        event match {\n+          case metricsUpdate: SparkListenerExecutorMetricsUpdate =>\n+          case stageCompleted: SparkListenerStageCompleted =>\n+            for (j <- 1 to 2) {\n+              checkExecutorMetricsUpdate(lines(i), stageCompleted.stageInfo.stageId,\n+                expectedMetricsEvents)\n+                i += 1\n+             }\n+            checkEvent(lines(i), event)\n+            i += 1\n+        case _ =>\n+          checkEvent(lines(i), event)\n+          i += 1\n+        }\n+      }\n+    } finally {\n+      logData.close()\n+    }\n+  }\n+\n+  /** Create a stage submitted event for the specified stage Id. */\n+  private def createStageSubmittedEvent(stageId: Int) = {\n+    SparkListenerStageSubmitted(new StageInfo(stageId, 0, stageId.toString, 0,\n+      Seq.empty, Seq.empty, \"details\"))\n+  }\n+\n+  /** Create a stage completed event for the specified stage Id. */\n+  private def createStageCompletedEvent(stageId: Int) = {\n+    SparkListenerStageCompleted(new StageInfo(stageId, 0, stageId.toString, 0,\n+      Seq.empty, Seq.empty, \"details\"))\n+  }\n+\n+  /** Create an executor added event for the specified executor Id. */\n+  private def createExecutorAddedEvent(executorId: Int) = {\n+    SparkListenerExecutorAdded(0L, executorId.toString, new ExecutorInfo(\"host1\", 1, Map.empty))\n+  }\n+\n+  /** Create an executor added event for the specified executor Id. */"
  }],
  "prId": 21221
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "I found this pretty confusing at first.  I suggest renaming `i` to `logIdx` and including a comment about the `j` loop.  Also we tend to use `(1 to 2).foreach`.  eg.\r\n\r\n```scala\r\n// just before the SparkListenerStageCompleted gets logged, we expect to get a \r\n// SparkListenerExecutorMetricsUpdate for each executor\r\n(1 to 2).foreach { _ =>\r\n  checkExecutorMetricsUpdate(lines(logIdx), stageCompleted.stageInfo.stageId,\r\n    expectedMetricsEvents)\r\n  logIdx += 1\r\n}\r\n// also check that we get the expected SparkListenerStageCompleted\r\ncheckEvent(lines(logIdx), event)\r\nlogIdx += 1\r\n```",
    "commit": "571285beace1a0c1df92d9f5127828ed8955c93f",
    "createdAt": "2018-05-23T19:00:10Z",
    "diffHunk": "@@ -251,6 +261,233 @@ class EventLoggingListenerSuite extends SparkFunSuite with LocalSparkContext wit\n     }\n   }\n \n+  /**\n+   * Test executor metrics update logging functionality. This checks that a\n+   * SparkListenerExecutorMetricsUpdate event is added to the Spark history\n+   * log if one of the executor metrics is larger than any previously\n+   * recorded value for the metric, per executor per stage. The task metrics\n+   * should not be added.\n+   */\n+  private def testExecutorMetricsUpdateEventLogging() {\n+    val conf = getLoggingConf(testDirPath, None)\n+    val logName = \"executorMetricsUpdated-test\"\n+    val eventLogger = new EventLoggingListener(logName, None, testDirPath.toUri(), conf)\n+    val listenerBus = new LiveListenerBus(conf)\n+\n+    // expected ExecutorMetricsUpdate, for the given stage id and executor id\n+    val expectedMetricsEvents: Map[(Int, String), SparkListenerExecutorMetricsUpdate] =\n+      Map(\n+        ((0, \"1\"),\n+          createExecutorMetricsUpdateEvent(1,\n+            new ExecutorMetrics(-1L, 5000L, 50L, 50L, 20L, 50L, 10L, 100L, 30L, 70L, 20L))),\n+        ((0, \"2\"),\n+          createExecutorMetricsUpdateEvent(2,\n+            new ExecutorMetrics(-1L, 7000L, 70L, 50L, 20L, 10L, 10L, 50L, 30L, 80L, 40L))),\n+        ((1, \"1\"),\n+          createExecutorMetricsUpdateEvent(1,\n+            new ExecutorMetrics(-1L, 7000L, 70L, 50L, 30L, 60L, 30L, 80L, 55L, 50L, 0L))),\n+        ((1, \"2\"),\n+          createExecutorMetricsUpdateEvent(2,\n+            new ExecutorMetrics(-1L, 7000L, 70L, 50L, 40L, 10L, 30L, 50L, 60L, 40L, 40L))))\n+\n+    // Events to post.\n+    val events = Array(\n+      SparkListenerApplicationStart(\"executionMetrics\", None,\n+        1L, \"update\", None),\n+      createExecutorAddedEvent(1),\n+      createExecutorAddedEvent(2),\n+      createStageSubmittedEvent(0),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(10L, 4000L, 50L, 20L, 0L, 40L, 0L, 60L, 0L, 70L, 20L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(10L, 1500L, 50L, 20L, 0L, 0L, 0L, 20L, 0L, 70L, 0L)),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(15L, 4000L, 50L, 50L, 0L, 50L, 0L, 100L, 0L, 70L, 20L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(15L, 2000L, 50L, 10L, 0L, 10L, 0L, 30L, 0L, 70L, 0L)),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(20L, 2000L, 40L, 50L, 0L, 40L, 10L, 90L, 10L, 50L, 0L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(20L, 3500L, 50L, 15L, 0L, 10L, 10L, 35L, 10L, 80L, 0L)),\n+      createStageSubmittedEvent(1),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(25L, 5000L, 30L, 50L, 20L, 30L, 10L, 80L, 30L, 50L, 0L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(25L, 7000L, 70L, 50L, 20L, 0L, 10L, 50L, 30L, 10L, 40L)),\n+      createStageCompletedEvent(0),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(30L, 6000L, 70L, 20L, 30L, 10L, 0L, 30L, 30L, 30L, 0L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(30L, 5500L, 30L, 20L, 40L, 10L, 0L, 30L, 40L, 40L, 20L)),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(35L, 7000L, 70L, 5L, 25L, 60L, 30L, 65L, 55L, 30L, 0L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(35L, 5500L, 40L, 25L, 30L, 10L, 30L, 35L, 60L, 0L, 20L)),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(40L, 5500L, 70L, 15L, 20L, 55L, 20L, 70L, 40L, 20L, 0L)),\n+      createExecutorRemovedEvent(1),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(40L, 4000L, 20L, 25L, 30L, 10L, 30L, 35L, 60L, 0L, 0L)),\n+      createStageCompletedEvent(1),\n+      SparkListenerApplicationEnd(1000L))\n+\n+    // play the events for the event logger\n+    eventLogger.start()\n+    listenerBus.start(Mockito.mock(classOf[SparkContext]), Mockito.mock(classOf[MetricsSystem]))\n+    listenerBus.addToEventLogQueue(eventLogger)\n+    events.foreach(event => listenerBus.post(event))\n+    listenerBus.stop()\n+    eventLogger.stop()\n+\n+    // Verify the log file contains the expected events.\n+    // Posted events should be logged, except for ExecutorMetricsUpdate events -- these\n+    // are consolidated, and the peak values for each stage are logged at stage end.\n+    val logData = EventLoggingListener.openEventLog(new Path(eventLogger.logPath), fileSystem)\n+    try {\n+      val lines = readLines(logData)\n+      val logStart = SparkListenerLogStart(SPARK_VERSION)\n+      assert(lines.size === 14)\n+      assert(lines(0).contains(\"SparkListenerLogStart\"))\n+      assert(lines(1).contains(\"SparkListenerApplicationStart\"))\n+      assert(JsonProtocol.sparkEventFromJson(parse(lines(0))) === logStart)\n+      var i = 1\n+      events.foreach {event =>\n+        event match {\n+          case metricsUpdate: SparkListenerExecutorMetricsUpdate =>\n+          case stageCompleted: SparkListenerStageCompleted =>\n+            for (j <- 1 to 2) {\n+              checkExecutorMetricsUpdate(lines(i), stageCompleted.stageInfo.stageId,\n+                expectedMetricsEvents)\n+                i += 1\n+             }\n+            checkEvent(lines(i), event)\n+            i += 1"
  }, {
    "author": {
      "login": "edwinalu"
    },
    "body": "Changed for both.",
    "commit": "571285beace1a0c1df92d9f5127828ed8955c93f",
    "createdAt": "2018-05-25T20:59:43Z",
    "diffHunk": "@@ -251,6 +261,233 @@ class EventLoggingListenerSuite extends SparkFunSuite with LocalSparkContext wit\n     }\n   }\n \n+  /**\n+   * Test executor metrics update logging functionality. This checks that a\n+   * SparkListenerExecutorMetricsUpdate event is added to the Spark history\n+   * log if one of the executor metrics is larger than any previously\n+   * recorded value for the metric, per executor per stage. The task metrics\n+   * should not be added.\n+   */\n+  private def testExecutorMetricsUpdateEventLogging() {\n+    val conf = getLoggingConf(testDirPath, None)\n+    val logName = \"executorMetricsUpdated-test\"\n+    val eventLogger = new EventLoggingListener(logName, None, testDirPath.toUri(), conf)\n+    val listenerBus = new LiveListenerBus(conf)\n+\n+    // expected ExecutorMetricsUpdate, for the given stage id and executor id\n+    val expectedMetricsEvents: Map[(Int, String), SparkListenerExecutorMetricsUpdate] =\n+      Map(\n+        ((0, \"1\"),\n+          createExecutorMetricsUpdateEvent(1,\n+            new ExecutorMetrics(-1L, 5000L, 50L, 50L, 20L, 50L, 10L, 100L, 30L, 70L, 20L))),\n+        ((0, \"2\"),\n+          createExecutorMetricsUpdateEvent(2,\n+            new ExecutorMetrics(-1L, 7000L, 70L, 50L, 20L, 10L, 10L, 50L, 30L, 80L, 40L))),\n+        ((1, \"1\"),\n+          createExecutorMetricsUpdateEvent(1,\n+            new ExecutorMetrics(-1L, 7000L, 70L, 50L, 30L, 60L, 30L, 80L, 55L, 50L, 0L))),\n+        ((1, \"2\"),\n+          createExecutorMetricsUpdateEvent(2,\n+            new ExecutorMetrics(-1L, 7000L, 70L, 50L, 40L, 10L, 30L, 50L, 60L, 40L, 40L))))\n+\n+    // Events to post.\n+    val events = Array(\n+      SparkListenerApplicationStart(\"executionMetrics\", None,\n+        1L, \"update\", None),\n+      createExecutorAddedEvent(1),\n+      createExecutorAddedEvent(2),\n+      createStageSubmittedEvent(0),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(10L, 4000L, 50L, 20L, 0L, 40L, 0L, 60L, 0L, 70L, 20L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(10L, 1500L, 50L, 20L, 0L, 0L, 0L, 20L, 0L, 70L, 0L)),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(15L, 4000L, 50L, 50L, 0L, 50L, 0L, 100L, 0L, 70L, 20L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(15L, 2000L, 50L, 10L, 0L, 10L, 0L, 30L, 0L, 70L, 0L)),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(20L, 2000L, 40L, 50L, 0L, 40L, 10L, 90L, 10L, 50L, 0L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(20L, 3500L, 50L, 15L, 0L, 10L, 10L, 35L, 10L, 80L, 0L)),\n+      createStageSubmittedEvent(1),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(25L, 5000L, 30L, 50L, 20L, 30L, 10L, 80L, 30L, 50L, 0L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(25L, 7000L, 70L, 50L, 20L, 0L, 10L, 50L, 30L, 10L, 40L)),\n+      createStageCompletedEvent(0),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(30L, 6000L, 70L, 20L, 30L, 10L, 0L, 30L, 30L, 30L, 0L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(30L, 5500L, 30L, 20L, 40L, 10L, 0L, 30L, 40L, 40L, 20L)),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(35L, 7000L, 70L, 5L, 25L, 60L, 30L, 65L, 55L, 30L, 0L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(35L, 5500L, 40L, 25L, 30L, 10L, 30L, 35L, 60L, 0L, 20L)),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(40L, 5500L, 70L, 15L, 20L, 55L, 20L, 70L, 40L, 20L, 0L)),\n+      createExecutorRemovedEvent(1),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(40L, 4000L, 20L, 25L, 30L, 10L, 30L, 35L, 60L, 0L, 0L)),\n+      createStageCompletedEvent(1),\n+      SparkListenerApplicationEnd(1000L))\n+\n+    // play the events for the event logger\n+    eventLogger.start()\n+    listenerBus.start(Mockito.mock(classOf[SparkContext]), Mockito.mock(classOf[MetricsSystem]))\n+    listenerBus.addToEventLogQueue(eventLogger)\n+    events.foreach(event => listenerBus.post(event))\n+    listenerBus.stop()\n+    eventLogger.stop()\n+\n+    // Verify the log file contains the expected events.\n+    // Posted events should be logged, except for ExecutorMetricsUpdate events -- these\n+    // are consolidated, and the peak values for each stage are logged at stage end.\n+    val logData = EventLoggingListener.openEventLog(new Path(eventLogger.logPath), fileSystem)\n+    try {\n+      val lines = readLines(logData)\n+      val logStart = SparkListenerLogStart(SPARK_VERSION)\n+      assert(lines.size === 14)\n+      assert(lines(0).contains(\"SparkListenerLogStart\"))\n+      assert(lines(1).contains(\"SparkListenerApplicationStart\"))\n+      assert(JsonProtocol.sparkEventFromJson(parse(lines(0))) === logStart)\n+      var i = 1\n+      events.foreach {event =>\n+        event match {\n+          case metricsUpdate: SparkListenerExecutorMetricsUpdate =>\n+          case stageCompleted: SparkListenerStageCompleted =>\n+            for (j <- 1 to 2) {\n+              checkExecutorMetricsUpdate(lines(i), stageCompleted.stageInfo.stageId,\n+                expectedMetricsEvents)\n+                i += 1\n+             }\n+            checkEvent(lines(i), event)\n+            i += 1"
  }],
  "prId": 21221
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "you can pull `JsonProtocol.sparkEventFromJson(parse(line))` out to avoid repeating, along with the type comparison.\r\n\r\n```scala\r\nval parsed = JsonProtocol.sparkEventFromJson(parse(line))\r\nassert(parsed.getClass === event.getClass)\r\nevent match {\r\n ...\r\n```\r\n\r\n(also `assertTypeError` does something else entirely: http://doc.scalatest.org/2.2.6/#org.scalatest.Assertions)",
    "commit": "571285beace1a0c1df92d9f5127828ed8955c93f",
    "createdAt": "2018-05-23T19:01:05Z",
    "diffHunk": "@@ -251,6 +261,233 @@ class EventLoggingListenerSuite extends SparkFunSuite with LocalSparkContext wit\n     }\n   }\n \n+  /**\n+   * Test executor metrics update logging functionality. This checks that a\n+   * SparkListenerExecutorMetricsUpdate event is added to the Spark history\n+   * log if one of the executor metrics is larger than any previously\n+   * recorded value for the metric, per executor per stage. The task metrics\n+   * should not be added.\n+   */\n+  private def testExecutorMetricsUpdateEventLogging() {\n+    val conf = getLoggingConf(testDirPath, None)\n+    val logName = \"executorMetricsUpdated-test\"\n+    val eventLogger = new EventLoggingListener(logName, None, testDirPath.toUri(), conf)\n+    val listenerBus = new LiveListenerBus(conf)\n+\n+    // expected ExecutorMetricsUpdate, for the given stage id and executor id\n+    val expectedMetricsEvents: Map[(Int, String), SparkListenerExecutorMetricsUpdate] =\n+      Map(\n+        ((0, \"1\"),\n+          createExecutorMetricsUpdateEvent(1,\n+            new ExecutorMetrics(-1L, 5000L, 50L, 50L, 20L, 50L, 10L, 100L, 30L, 70L, 20L))),\n+        ((0, \"2\"),\n+          createExecutorMetricsUpdateEvent(2,\n+            new ExecutorMetrics(-1L, 7000L, 70L, 50L, 20L, 10L, 10L, 50L, 30L, 80L, 40L))),\n+        ((1, \"1\"),\n+          createExecutorMetricsUpdateEvent(1,\n+            new ExecutorMetrics(-1L, 7000L, 70L, 50L, 30L, 60L, 30L, 80L, 55L, 50L, 0L))),\n+        ((1, \"2\"),\n+          createExecutorMetricsUpdateEvent(2,\n+            new ExecutorMetrics(-1L, 7000L, 70L, 50L, 40L, 10L, 30L, 50L, 60L, 40L, 40L))))\n+\n+    // Events to post.\n+    val events = Array(\n+      SparkListenerApplicationStart(\"executionMetrics\", None,\n+        1L, \"update\", None),\n+      createExecutorAddedEvent(1),\n+      createExecutorAddedEvent(2),\n+      createStageSubmittedEvent(0),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(10L, 4000L, 50L, 20L, 0L, 40L, 0L, 60L, 0L, 70L, 20L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(10L, 1500L, 50L, 20L, 0L, 0L, 0L, 20L, 0L, 70L, 0L)),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(15L, 4000L, 50L, 50L, 0L, 50L, 0L, 100L, 0L, 70L, 20L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(15L, 2000L, 50L, 10L, 0L, 10L, 0L, 30L, 0L, 70L, 0L)),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(20L, 2000L, 40L, 50L, 0L, 40L, 10L, 90L, 10L, 50L, 0L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(20L, 3500L, 50L, 15L, 0L, 10L, 10L, 35L, 10L, 80L, 0L)),\n+      createStageSubmittedEvent(1),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(25L, 5000L, 30L, 50L, 20L, 30L, 10L, 80L, 30L, 50L, 0L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(25L, 7000L, 70L, 50L, 20L, 0L, 10L, 50L, 30L, 10L, 40L)),\n+      createStageCompletedEvent(0),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(30L, 6000L, 70L, 20L, 30L, 10L, 0L, 30L, 30L, 30L, 0L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(30L, 5500L, 30L, 20L, 40L, 10L, 0L, 30L, 40L, 40L, 20L)),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(35L, 7000L, 70L, 5L, 25L, 60L, 30L, 65L, 55L, 30L, 0L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(35L, 5500L, 40L, 25L, 30L, 10L, 30L, 35L, 60L, 0L, 20L)),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(40L, 5500L, 70L, 15L, 20L, 55L, 20L, 70L, 40L, 20L, 0L)),\n+      createExecutorRemovedEvent(1),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(40L, 4000L, 20L, 25L, 30L, 10L, 30L, 35L, 60L, 0L, 0L)),\n+      createStageCompletedEvent(1),\n+      SparkListenerApplicationEnd(1000L))\n+\n+    // play the events for the event logger\n+    eventLogger.start()\n+    listenerBus.start(Mockito.mock(classOf[SparkContext]), Mockito.mock(classOf[MetricsSystem]))\n+    listenerBus.addToEventLogQueue(eventLogger)\n+    events.foreach(event => listenerBus.post(event))\n+    listenerBus.stop()\n+    eventLogger.stop()\n+\n+    // Verify the log file contains the expected events.\n+    // Posted events should be logged, except for ExecutorMetricsUpdate events -- these\n+    // are consolidated, and the peak values for each stage are logged at stage end.\n+    val logData = EventLoggingListener.openEventLog(new Path(eventLogger.logPath), fileSystem)\n+    try {\n+      val lines = readLines(logData)\n+      val logStart = SparkListenerLogStart(SPARK_VERSION)\n+      assert(lines.size === 14)\n+      assert(lines(0).contains(\"SparkListenerLogStart\"))\n+      assert(lines(1).contains(\"SparkListenerApplicationStart\"))\n+      assert(JsonProtocol.sparkEventFromJson(parse(lines(0))) === logStart)\n+      var i = 1\n+      events.foreach {event =>\n+        event match {\n+          case metricsUpdate: SparkListenerExecutorMetricsUpdate =>\n+          case stageCompleted: SparkListenerStageCompleted =>\n+            for (j <- 1 to 2) {\n+              checkExecutorMetricsUpdate(lines(i), stageCompleted.stageInfo.stageId,\n+                expectedMetricsEvents)\n+                i += 1\n+             }\n+            checkEvent(lines(i), event)\n+            i += 1\n+        case _ =>\n+          checkEvent(lines(i), event)\n+          i += 1\n+        }\n+      }\n+    } finally {\n+      logData.close()\n+    }\n+  }\n+\n+  /** Create a stage submitted event for the specified stage Id. */\n+  private def createStageSubmittedEvent(stageId: Int) = {\n+    SparkListenerStageSubmitted(new StageInfo(stageId, 0, stageId.toString, 0,\n+      Seq.empty, Seq.empty, \"details\"))\n+  }\n+\n+  /** Create a stage completed event for the specified stage Id. */\n+  private def createStageCompletedEvent(stageId: Int) = {\n+    SparkListenerStageCompleted(new StageInfo(stageId, 0, stageId.toString, 0,\n+      Seq.empty, Seq.empty, \"details\"))\n+  }\n+\n+  /** Create an executor added event for the specified executor Id. */\n+  private def createExecutorAddedEvent(executorId: Int) = {\n+    SparkListenerExecutorAdded(0L, executorId.toString, new ExecutorInfo(\"host1\", 1, Map.empty))\n+  }\n+\n+  /** Create an executor added event for the specified executor Id. */\n+  private def createExecutorRemovedEvent(executorId: Int) = {\n+    SparkListenerExecutorRemoved(0L, executorId.toString, \"test\")\n+  }\n+\n+  /** Create an executor metrics update event, with the specified executor metrics values. */\n+  private def createExecutorMetricsUpdateEvent(\n+      executorId: Int,\n+      executorMetrics: ExecutorMetrics): SparkListenerExecutorMetricsUpdate = {\n+    val taskMetrics = TaskMetrics.empty\n+    taskMetrics.incDiskBytesSpilled(111)\n+    taskMetrics.incMemoryBytesSpilled(222)\n+    val accum = Array((333L, 1, 1, taskMetrics.accumulators().map(AccumulatorSuite.makeInfo)))\n+    SparkListenerExecutorMetricsUpdate(executorId.toString, accum, Some(executorMetrics))\n+  }\n+\n+  /** Check that the two ExecutorMetrics match */\n+  private def checkExecutorMetrics(\n+      executorMetrics1: Option[ExecutorMetrics],\n+      executorMetrics2: Option[ExecutorMetrics]) = {\n+    (executorMetrics1, executorMetrics2) match {\n+      case (Some(e1), Some(e2)) =>\n+        assert(e1.timestamp === e2.timestamp)\n+        assert(e1.jvmUsedHeapMemory === e2.jvmUsedHeapMemory)\n+        assert(e1.jvmUsedNonHeapMemory === e2.jvmUsedNonHeapMemory)\n+        assert(e1.onHeapExecutionMemory === e2.onHeapExecutionMemory)\n+        assert(e1.offHeapExecutionMemory === e2.offHeapExecutionMemory)\n+        assert(e1.onHeapStorageMemory === e2.onHeapStorageMemory)\n+        assert(e1.offHeapStorageMemory === e2.offHeapStorageMemory)\n+        assert(e1.onHeapUnifiedMemory === e2.onHeapUnifiedMemory)\n+        assert(e1.offHeapUnifiedMemory === e2.offHeapUnifiedMemory)\n+        assert(e1.directMemory === e2.directMemory)\n+        assert(e1.mappedMemory === e2.mappedMemory)\n+      case (None, None) =>\n+      case _ =>\n+        assert(false)\n+    }\n+  }\n+\n+  /** Check that the Spark history log line matches the expected event. */\n+  private def checkEvent(line: String, event: SparkListenerEvent): Unit = {\n+    assert(line.contains(event.getClass.toString.split(\"\\\\.\").last))\n+    event match {\n+      case executorMetrics: SparkListenerExecutorMetricsUpdate =>\n+        JsonProtocol.sparkEventFromJson(parse(line)) match {"
  }, {
    "author": {
      "login": "edwinalu"
    },
    "body": "Thanks, modified. ",
    "commit": "571285beace1a0c1df92d9f5127828ed8955c93f",
    "createdAt": "2018-05-25T21:21:31Z",
    "diffHunk": "@@ -251,6 +261,233 @@ class EventLoggingListenerSuite extends SparkFunSuite with LocalSparkContext wit\n     }\n   }\n \n+  /**\n+   * Test executor metrics update logging functionality. This checks that a\n+   * SparkListenerExecutorMetricsUpdate event is added to the Spark history\n+   * log if one of the executor metrics is larger than any previously\n+   * recorded value for the metric, per executor per stage. The task metrics\n+   * should not be added.\n+   */\n+  private def testExecutorMetricsUpdateEventLogging() {\n+    val conf = getLoggingConf(testDirPath, None)\n+    val logName = \"executorMetricsUpdated-test\"\n+    val eventLogger = new EventLoggingListener(logName, None, testDirPath.toUri(), conf)\n+    val listenerBus = new LiveListenerBus(conf)\n+\n+    // expected ExecutorMetricsUpdate, for the given stage id and executor id\n+    val expectedMetricsEvents: Map[(Int, String), SparkListenerExecutorMetricsUpdate] =\n+      Map(\n+        ((0, \"1\"),\n+          createExecutorMetricsUpdateEvent(1,\n+            new ExecutorMetrics(-1L, 5000L, 50L, 50L, 20L, 50L, 10L, 100L, 30L, 70L, 20L))),\n+        ((0, \"2\"),\n+          createExecutorMetricsUpdateEvent(2,\n+            new ExecutorMetrics(-1L, 7000L, 70L, 50L, 20L, 10L, 10L, 50L, 30L, 80L, 40L))),\n+        ((1, \"1\"),\n+          createExecutorMetricsUpdateEvent(1,\n+            new ExecutorMetrics(-1L, 7000L, 70L, 50L, 30L, 60L, 30L, 80L, 55L, 50L, 0L))),\n+        ((1, \"2\"),\n+          createExecutorMetricsUpdateEvent(2,\n+            new ExecutorMetrics(-1L, 7000L, 70L, 50L, 40L, 10L, 30L, 50L, 60L, 40L, 40L))))\n+\n+    // Events to post.\n+    val events = Array(\n+      SparkListenerApplicationStart(\"executionMetrics\", None,\n+        1L, \"update\", None),\n+      createExecutorAddedEvent(1),\n+      createExecutorAddedEvent(2),\n+      createStageSubmittedEvent(0),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(10L, 4000L, 50L, 20L, 0L, 40L, 0L, 60L, 0L, 70L, 20L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(10L, 1500L, 50L, 20L, 0L, 0L, 0L, 20L, 0L, 70L, 0L)),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(15L, 4000L, 50L, 50L, 0L, 50L, 0L, 100L, 0L, 70L, 20L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(15L, 2000L, 50L, 10L, 0L, 10L, 0L, 30L, 0L, 70L, 0L)),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(20L, 2000L, 40L, 50L, 0L, 40L, 10L, 90L, 10L, 50L, 0L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(20L, 3500L, 50L, 15L, 0L, 10L, 10L, 35L, 10L, 80L, 0L)),\n+      createStageSubmittedEvent(1),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(25L, 5000L, 30L, 50L, 20L, 30L, 10L, 80L, 30L, 50L, 0L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(25L, 7000L, 70L, 50L, 20L, 0L, 10L, 50L, 30L, 10L, 40L)),\n+      createStageCompletedEvent(0),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(30L, 6000L, 70L, 20L, 30L, 10L, 0L, 30L, 30L, 30L, 0L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(30L, 5500L, 30L, 20L, 40L, 10L, 0L, 30L, 40L, 40L, 20L)),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(35L, 7000L, 70L, 5L, 25L, 60L, 30L, 65L, 55L, 30L, 0L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(35L, 5500L, 40L, 25L, 30L, 10L, 30L, 35L, 60L, 0L, 20L)),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(40L, 5500L, 70L, 15L, 20L, 55L, 20L, 70L, 40L, 20L, 0L)),\n+      createExecutorRemovedEvent(1),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(40L, 4000L, 20L, 25L, 30L, 10L, 30L, 35L, 60L, 0L, 0L)),\n+      createStageCompletedEvent(1),\n+      SparkListenerApplicationEnd(1000L))\n+\n+    // play the events for the event logger\n+    eventLogger.start()\n+    listenerBus.start(Mockito.mock(classOf[SparkContext]), Mockito.mock(classOf[MetricsSystem]))\n+    listenerBus.addToEventLogQueue(eventLogger)\n+    events.foreach(event => listenerBus.post(event))\n+    listenerBus.stop()\n+    eventLogger.stop()\n+\n+    // Verify the log file contains the expected events.\n+    // Posted events should be logged, except for ExecutorMetricsUpdate events -- these\n+    // are consolidated, and the peak values for each stage are logged at stage end.\n+    val logData = EventLoggingListener.openEventLog(new Path(eventLogger.logPath), fileSystem)\n+    try {\n+      val lines = readLines(logData)\n+      val logStart = SparkListenerLogStart(SPARK_VERSION)\n+      assert(lines.size === 14)\n+      assert(lines(0).contains(\"SparkListenerLogStart\"))\n+      assert(lines(1).contains(\"SparkListenerApplicationStart\"))\n+      assert(JsonProtocol.sparkEventFromJson(parse(lines(0))) === logStart)\n+      var i = 1\n+      events.foreach {event =>\n+        event match {\n+          case metricsUpdate: SparkListenerExecutorMetricsUpdate =>\n+          case stageCompleted: SparkListenerStageCompleted =>\n+            for (j <- 1 to 2) {\n+              checkExecutorMetricsUpdate(lines(i), stageCompleted.stageInfo.stageId,\n+                expectedMetricsEvents)\n+                i += 1\n+             }\n+            checkEvent(lines(i), event)\n+            i += 1\n+        case _ =>\n+          checkEvent(lines(i), event)\n+          i += 1\n+        }\n+      }\n+    } finally {\n+      logData.close()\n+    }\n+  }\n+\n+  /** Create a stage submitted event for the specified stage Id. */\n+  private def createStageSubmittedEvent(stageId: Int) = {\n+    SparkListenerStageSubmitted(new StageInfo(stageId, 0, stageId.toString, 0,\n+      Seq.empty, Seq.empty, \"details\"))\n+  }\n+\n+  /** Create a stage completed event for the specified stage Id. */\n+  private def createStageCompletedEvent(stageId: Int) = {\n+    SparkListenerStageCompleted(new StageInfo(stageId, 0, stageId.toString, 0,\n+      Seq.empty, Seq.empty, \"details\"))\n+  }\n+\n+  /** Create an executor added event for the specified executor Id. */\n+  private def createExecutorAddedEvent(executorId: Int) = {\n+    SparkListenerExecutorAdded(0L, executorId.toString, new ExecutorInfo(\"host1\", 1, Map.empty))\n+  }\n+\n+  /** Create an executor added event for the specified executor Id. */\n+  private def createExecutorRemovedEvent(executorId: Int) = {\n+    SparkListenerExecutorRemoved(0L, executorId.toString, \"test\")\n+  }\n+\n+  /** Create an executor metrics update event, with the specified executor metrics values. */\n+  private def createExecutorMetricsUpdateEvent(\n+      executorId: Int,\n+      executorMetrics: ExecutorMetrics): SparkListenerExecutorMetricsUpdate = {\n+    val taskMetrics = TaskMetrics.empty\n+    taskMetrics.incDiskBytesSpilled(111)\n+    taskMetrics.incMemoryBytesSpilled(222)\n+    val accum = Array((333L, 1, 1, taskMetrics.accumulators().map(AccumulatorSuite.makeInfo)))\n+    SparkListenerExecutorMetricsUpdate(executorId.toString, accum, Some(executorMetrics))\n+  }\n+\n+  /** Check that the two ExecutorMetrics match */\n+  private def checkExecutorMetrics(\n+      executorMetrics1: Option[ExecutorMetrics],\n+      executorMetrics2: Option[ExecutorMetrics]) = {\n+    (executorMetrics1, executorMetrics2) match {\n+      case (Some(e1), Some(e2)) =>\n+        assert(e1.timestamp === e2.timestamp)\n+        assert(e1.jvmUsedHeapMemory === e2.jvmUsedHeapMemory)\n+        assert(e1.jvmUsedNonHeapMemory === e2.jvmUsedNonHeapMemory)\n+        assert(e1.onHeapExecutionMemory === e2.onHeapExecutionMemory)\n+        assert(e1.offHeapExecutionMemory === e2.offHeapExecutionMemory)\n+        assert(e1.onHeapStorageMemory === e2.onHeapStorageMemory)\n+        assert(e1.offHeapStorageMemory === e2.offHeapStorageMemory)\n+        assert(e1.onHeapUnifiedMemory === e2.onHeapUnifiedMemory)\n+        assert(e1.offHeapUnifiedMemory === e2.offHeapUnifiedMemory)\n+        assert(e1.directMemory === e2.directMemory)\n+        assert(e1.mappedMemory === e2.mappedMemory)\n+      case (None, None) =>\n+      case _ =>\n+        assert(false)\n+    }\n+  }\n+\n+  /** Check that the Spark history log line matches the expected event. */\n+  private def checkEvent(line: String, event: SparkListenerEvent): Unit = {\n+    assert(line.contains(event.getClass.toString.split(\"\\\\.\").last))\n+    event match {\n+      case executorMetrics: SparkListenerExecutorMetricsUpdate =>\n+        JsonProtocol.sparkEventFromJson(parse(line)) match {"
  }],
  "prId": 21221
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "you're never using this w/ `SparkListenerExecutorMetricsUpdate`, right?",
    "commit": "571285beace1a0c1df92d9f5127828ed8955c93f",
    "createdAt": "2018-05-23T19:04:12Z",
    "diffHunk": "@@ -251,6 +261,233 @@ class EventLoggingListenerSuite extends SparkFunSuite with LocalSparkContext wit\n     }\n   }\n \n+  /**\n+   * Test executor metrics update logging functionality. This checks that a\n+   * SparkListenerExecutorMetricsUpdate event is added to the Spark history\n+   * log if one of the executor metrics is larger than any previously\n+   * recorded value for the metric, per executor per stage. The task metrics\n+   * should not be added.\n+   */\n+  private def testExecutorMetricsUpdateEventLogging() {\n+    val conf = getLoggingConf(testDirPath, None)\n+    val logName = \"executorMetricsUpdated-test\"\n+    val eventLogger = new EventLoggingListener(logName, None, testDirPath.toUri(), conf)\n+    val listenerBus = new LiveListenerBus(conf)\n+\n+    // expected ExecutorMetricsUpdate, for the given stage id and executor id\n+    val expectedMetricsEvents: Map[(Int, String), SparkListenerExecutorMetricsUpdate] =\n+      Map(\n+        ((0, \"1\"),\n+          createExecutorMetricsUpdateEvent(1,\n+            new ExecutorMetrics(-1L, 5000L, 50L, 50L, 20L, 50L, 10L, 100L, 30L, 70L, 20L))),\n+        ((0, \"2\"),\n+          createExecutorMetricsUpdateEvent(2,\n+            new ExecutorMetrics(-1L, 7000L, 70L, 50L, 20L, 10L, 10L, 50L, 30L, 80L, 40L))),\n+        ((1, \"1\"),\n+          createExecutorMetricsUpdateEvent(1,\n+            new ExecutorMetrics(-1L, 7000L, 70L, 50L, 30L, 60L, 30L, 80L, 55L, 50L, 0L))),\n+        ((1, \"2\"),\n+          createExecutorMetricsUpdateEvent(2,\n+            new ExecutorMetrics(-1L, 7000L, 70L, 50L, 40L, 10L, 30L, 50L, 60L, 40L, 40L))))\n+\n+    // Events to post.\n+    val events = Array(\n+      SparkListenerApplicationStart(\"executionMetrics\", None,\n+        1L, \"update\", None),\n+      createExecutorAddedEvent(1),\n+      createExecutorAddedEvent(2),\n+      createStageSubmittedEvent(0),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(10L, 4000L, 50L, 20L, 0L, 40L, 0L, 60L, 0L, 70L, 20L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(10L, 1500L, 50L, 20L, 0L, 0L, 0L, 20L, 0L, 70L, 0L)),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(15L, 4000L, 50L, 50L, 0L, 50L, 0L, 100L, 0L, 70L, 20L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(15L, 2000L, 50L, 10L, 0L, 10L, 0L, 30L, 0L, 70L, 0L)),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(20L, 2000L, 40L, 50L, 0L, 40L, 10L, 90L, 10L, 50L, 0L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(20L, 3500L, 50L, 15L, 0L, 10L, 10L, 35L, 10L, 80L, 0L)),\n+      createStageSubmittedEvent(1),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(25L, 5000L, 30L, 50L, 20L, 30L, 10L, 80L, 30L, 50L, 0L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(25L, 7000L, 70L, 50L, 20L, 0L, 10L, 50L, 30L, 10L, 40L)),\n+      createStageCompletedEvent(0),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(30L, 6000L, 70L, 20L, 30L, 10L, 0L, 30L, 30L, 30L, 0L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(30L, 5500L, 30L, 20L, 40L, 10L, 0L, 30L, 40L, 40L, 20L)),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(35L, 7000L, 70L, 5L, 25L, 60L, 30L, 65L, 55L, 30L, 0L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(35L, 5500L, 40L, 25L, 30L, 10L, 30L, 35L, 60L, 0L, 20L)),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(40L, 5500L, 70L, 15L, 20L, 55L, 20L, 70L, 40L, 20L, 0L)),\n+      createExecutorRemovedEvent(1),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(40L, 4000L, 20L, 25L, 30L, 10L, 30L, 35L, 60L, 0L, 0L)),\n+      createStageCompletedEvent(1),\n+      SparkListenerApplicationEnd(1000L))\n+\n+    // play the events for the event logger\n+    eventLogger.start()\n+    listenerBus.start(Mockito.mock(classOf[SparkContext]), Mockito.mock(classOf[MetricsSystem]))\n+    listenerBus.addToEventLogQueue(eventLogger)\n+    events.foreach(event => listenerBus.post(event))\n+    listenerBus.stop()\n+    eventLogger.stop()\n+\n+    // Verify the log file contains the expected events.\n+    // Posted events should be logged, except for ExecutorMetricsUpdate events -- these\n+    // are consolidated, and the peak values for each stage are logged at stage end.\n+    val logData = EventLoggingListener.openEventLog(new Path(eventLogger.logPath), fileSystem)\n+    try {\n+      val lines = readLines(logData)\n+      val logStart = SparkListenerLogStart(SPARK_VERSION)\n+      assert(lines.size === 14)\n+      assert(lines(0).contains(\"SparkListenerLogStart\"))\n+      assert(lines(1).contains(\"SparkListenerApplicationStart\"))\n+      assert(JsonProtocol.sparkEventFromJson(parse(lines(0))) === logStart)\n+      var i = 1\n+      events.foreach {event =>\n+        event match {\n+          case metricsUpdate: SparkListenerExecutorMetricsUpdate =>\n+          case stageCompleted: SparkListenerStageCompleted =>\n+            for (j <- 1 to 2) {\n+              checkExecutorMetricsUpdate(lines(i), stageCompleted.stageInfo.stageId,\n+                expectedMetricsEvents)\n+                i += 1\n+             }\n+            checkEvent(lines(i), event)\n+            i += 1\n+        case _ =>\n+          checkEvent(lines(i), event)\n+          i += 1\n+        }\n+      }\n+    } finally {\n+      logData.close()\n+    }\n+  }\n+\n+  /** Create a stage submitted event for the specified stage Id. */\n+  private def createStageSubmittedEvent(stageId: Int) = {\n+    SparkListenerStageSubmitted(new StageInfo(stageId, 0, stageId.toString, 0,\n+      Seq.empty, Seq.empty, \"details\"))\n+  }\n+\n+  /** Create a stage completed event for the specified stage Id. */\n+  private def createStageCompletedEvent(stageId: Int) = {\n+    SparkListenerStageCompleted(new StageInfo(stageId, 0, stageId.toString, 0,\n+      Seq.empty, Seq.empty, \"details\"))\n+  }\n+\n+  /** Create an executor added event for the specified executor Id. */\n+  private def createExecutorAddedEvent(executorId: Int) = {\n+    SparkListenerExecutorAdded(0L, executorId.toString, new ExecutorInfo(\"host1\", 1, Map.empty))\n+  }\n+\n+  /** Create an executor added event for the specified executor Id. */\n+  private def createExecutorRemovedEvent(executorId: Int) = {\n+    SparkListenerExecutorRemoved(0L, executorId.toString, \"test\")\n+  }\n+\n+  /** Create an executor metrics update event, with the specified executor metrics values. */\n+  private def createExecutorMetricsUpdateEvent(\n+      executorId: Int,\n+      executorMetrics: ExecutorMetrics): SparkListenerExecutorMetricsUpdate = {\n+    val taskMetrics = TaskMetrics.empty\n+    taskMetrics.incDiskBytesSpilled(111)\n+    taskMetrics.incMemoryBytesSpilled(222)\n+    val accum = Array((333L, 1, 1, taskMetrics.accumulators().map(AccumulatorSuite.makeInfo)))\n+    SparkListenerExecutorMetricsUpdate(executorId.toString, accum, Some(executorMetrics))\n+  }\n+\n+  /** Check that the two ExecutorMetrics match */\n+  private def checkExecutorMetrics(\n+      executorMetrics1: Option[ExecutorMetrics],\n+      executorMetrics2: Option[ExecutorMetrics]) = {\n+    (executorMetrics1, executorMetrics2) match {\n+      case (Some(e1), Some(e2)) =>\n+        assert(e1.timestamp === e2.timestamp)\n+        assert(e1.jvmUsedHeapMemory === e2.jvmUsedHeapMemory)\n+        assert(e1.jvmUsedNonHeapMemory === e2.jvmUsedNonHeapMemory)\n+        assert(e1.onHeapExecutionMemory === e2.onHeapExecutionMemory)\n+        assert(e1.offHeapExecutionMemory === e2.offHeapExecutionMemory)\n+        assert(e1.onHeapStorageMemory === e2.onHeapStorageMemory)\n+        assert(e1.offHeapStorageMemory === e2.offHeapStorageMemory)\n+        assert(e1.onHeapUnifiedMemory === e2.onHeapUnifiedMemory)\n+        assert(e1.offHeapUnifiedMemory === e2.offHeapUnifiedMemory)\n+        assert(e1.directMemory === e2.directMemory)\n+        assert(e1.mappedMemory === e2.mappedMemory)\n+      case (None, None) =>\n+      case _ =>\n+        assert(false)\n+    }\n+  }\n+\n+  /** Check that the Spark history log line matches the expected event. */\n+  private def checkEvent(line: String, event: SparkListenerEvent): Unit = {\n+    assert(line.contains(event.getClass.toString.split(\"\\\\.\").last))\n+    event match {\n+      case executorMetrics: SparkListenerExecutorMetricsUpdate =>"
  }, {
    "author": {
      "login": "edwinalu"
    },
    "body": "Nope, with the change in design to logging the executor metrics updates at stage end, this part is skipped -- I'll remove this.",
    "commit": "571285beace1a0c1df92d9f5127828ed8955c93f",
    "createdAt": "2018-05-25T21:19:17Z",
    "diffHunk": "@@ -251,6 +261,233 @@ class EventLoggingListenerSuite extends SparkFunSuite with LocalSparkContext wit\n     }\n   }\n \n+  /**\n+   * Test executor metrics update logging functionality. This checks that a\n+   * SparkListenerExecutorMetricsUpdate event is added to the Spark history\n+   * log if one of the executor metrics is larger than any previously\n+   * recorded value for the metric, per executor per stage. The task metrics\n+   * should not be added.\n+   */\n+  private def testExecutorMetricsUpdateEventLogging() {\n+    val conf = getLoggingConf(testDirPath, None)\n+    val logName = \"executorMetricsUpdated-test\"\n+    val eventLogger = new EventLoggingListener(logName, None, testDirPath.toUri(), conf)\n+    val listenerBus = new LiveListenerBus(conf)\n+\n+    // expected ExecutorMetricsUpdate, for the given stage id and executor id\n+    val expectedMetricsEvents: Map[(Int, String), SparkListenerExecutorMetricsUpdate] =\n+      Map(\n+        ((0, \"1\"),\n+          createExecutorMetricsUpdateEvent(1,\n+            new ExecutorMetrics(-1L, 5000L, 50L, 50L, 20L, 50L, 10L, 100L, 30L, 70L, 20L))),\n+        ((0, \"2\"),\n+          createExecutorMetricsUpdateEvent(2,\n+            new ExecutorMetrics(-1L, 7000L, 70L, 50L, 20L, 10L, 10L, 50L, 30L, 80L, 40L))),\n+        ((1, \"1\"),\n+          createExecutorMetricsUpdateEvent(1,\n+            new ExecutorMetrics(-1L, 7000L, 70L, 50L, 30L, 60L, 30L, 80L, 55L, 50L, 0L))),\n+        ((1, \"2\"),\n+          createExecutorMetricsUpdateEvent(2,\n+            new ExecutorMetrics(-1L, 7000L, 70L, 50L, 40L, 10L, 30L, 50L, 60L, 40L, 40L))))\n+\n+    // Events to post.\n+    val events = Array(\n+      SparkListenerApplicationStart(\"executionMetrics\", None,\n+        1L, \"update\", None),\n+      createExecutorAddedEvent(1),\n+      createExecutorAddedEvent(2),\n+      createStageSubmittedEvent(0),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(10L, 4000L, 50L, 20L, 0L, 40L, 0L, 60L, 0L, 70L, 20L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(10L, 1500L, 50L, 20L, 0L, 0L, 0L, 20L, 0L, 70L, 0L)),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(15L, 4000L, 50L, 50L, 0L, 50L, 0L, 100L, 0L, 70L, 20L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(15L, 2000L, 50L, 10L, 0L, 10L, 0L, 30L, 0L, 70L, 0L)),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(20L, 2000L, 40L, 50L, 0L, 40L, 10L, 90L, 10L, 50L, 0L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(20L, 3500L, 50L, 15L, 0L, 10L, 10L, 35L, 10L, 80L, 0L)),\n+      createStageSubmittedEvent(1),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(25L, 5000L, 30L, 50L, 20L, 30L, 10L, 80L, 30L, 50L, 0L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(25L, 7000L, 70L, 50L, 20L, 0L, 10L, 50L, 30L, 10L, 40L)),\n+      createStageCompletedEvent(0),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(30L, 6000L, 70L, 20L, 30L, 10L, 0L, 30L, 30L, 30L, 0L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(30L, 5500L, 30L, 20L, 40L, 10L, 0L, 30L, 40L, 40L, 20L)),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(35L, 7000L, 70L, 5L, 25L, 60L, 30L, 65L, 55L, 30L, 0L)),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(35L, 5500L, 40L, 25L, 30L, 10L, 30L, 35L, 60L, 0L, 20L)),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(40L, 5500L, 70L, 15L, 20L, 55L, 20L, 70L, 40L, 20L, 0L)),\n+      createExecutorRemovedEvent(1),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(40L, 4000L, 20L, 25L, 30L, 10L, 30L, 35L, 60L, 0L, 0L)),\n+      createStageCompletedEvent(1),\n+      SparkListenerApplicationEnd(1000L))\n+\n+    // play the events for the event logger\n+    eventLogger.start()\n+    listenerBus.start(Mockito.mock(classOf[SparkContext]), Mockito.mock(classOf[MetricsSystem]))\n+    listenerBus.addToEventLogQueue(eventLogger)\n+    events.foreach(event => listenerBus.post(event))\n+    listenerBus.stop()\n+    eventLogger.stop()\n+\n+    // Verify the log file contains the expected events.\n+    // Posted events should be logged, except for ExecutorMetricsUpdate events -- these\n+    // are consolidated, and the peak values for each stage are logged at stage end.\n+    val logData = EventLoggingListener.openEventLog(new Path(eventLogger.logPath), fileSystem)\n+    try {\n+      val lines = readLines(logData)\n+      val logStart = SparkListenerLogStart(SPARK_VERSION)\n+      assert(lines.size === 14)\n+      assert(lines(0).contains(\"SparkListenerLogStart\"))\n+      assert(lines(1).contains(\"SparkListenerApplicationStart\"))\n+      assert(JsonProtocol.sparkEventFromJson(parse(lines(0))) === logStart)\n+      var i = 1\n+      events.foreach {event =>\n+        event match {\n+          case metricsUpdate: SparkListenerExecutorMetricsUpdate =>\n+          case stageCompleted: SparkListenerStageCompleted =>\n+            for (j <- 1 to 2) {\n+              checkExecutorMetricsUpdate(lines(i), stageCompleted.stageInfo.stageId,\n+                expectedMetricsEvents)\n+                i += 1\n+             }\n+            checkEvent(lines(i), event)\n+            i += 1\n+        case _ =>\n+          checkEvent(lines(i), event)\n+          i += 1\n+        }\n+      }\n+    } finally {\n+      logData.close()\n+    }\n+  }\n+\n+  /** Create a stage submitted event for the specified stage Id. */\n+  private def createStageSubmittedEvent(stageId: Int) = {\n+    SparkListenerStageSubmitted(new StageInfo(stageId, 0, stageId.toString, 0,\n+      Seq.empty, Seq.empty, \"details\"))\n+  }\n+\n+  /** Create a stage completed event for the specified stage Id. */\n+  private def createStageCompletedEvent(stageId: Int) = {\n+    SparkListenerStageCompleted(new StageInfo(stageId, 0, stageId.toString, 0,\n+      Seq.empty, Seq.empty, \"details\"))\n+  }\n+\n+  /** Create an executor added event for the specified executor Id. */\n+  private def createExecutorAddedEvent(executorId: Int) = {\n+    SparkListenerExecutorAdded(0L, executorId.toString, new ExecutorInfo(\"host1\", 1, Map.empty))\n+  }\n+\n+  /** Create an executor added event for the specified executor Id. */\n+  private def createExecutorRemovedEvent(executorId: Int) = {\n+    SparkListenerExecutorRemoved(0L, executorId.toString, \"test\")\n+  }\n+\n+  /** Create an executor metrics update event, with the specified executor metrics values. */\n+  private def createExecutorMetricsUpdateEvent(\n+      executorId: Int,\n+      executorMetrics: ExecutorMetrics): SparkListenerExecutorMetricsUpdate = {\n+    val taskMetrics = TaskMetrics.empty\n+    taskMetrics.incDiskBytesSpilled(111)\n+    taskMetrics.incMemoryBytesSpilled(222)\n+    val accum = Array((333L, 1, 1, taskMetrics.accumulators().map(AccumulatorSuite.makeInfo)))\n+    SparkListenerExecutorMetricsUpdate(executorId.toString, accum, Some(executorMetrics))\n+  }\n+\n+  /** Check that the two ExecutorMetrics match */\n+  private def checkExecutorMetrics(\n+      executorMetrics1: Option[ExecutorMetrics],\n+      executorMetrics2: Option[ExecutorMetrics]) = {\n+    (executorMetrics1, executorMetrics2) match {\n+      case (Some(e1), Some(e2)) =>\n+        assert(e1.timestamp === e2.timestamp)\n+        assert(e1.jvmUsedHeapMemory === e2.jvmUsedHeapMemory)\n+        assert(e1.jvmUsedNonHeapMemory === e2.jvmUsedNonHeapMemory)\n+        assert(e1.onHeapExecutionMemory === e2.onHeapExecutionMemory)\n+        assert(e1.offHeapExecutionMemory === e2.offHeapExecutionMemory)\n+        assert(e1.onHeapStorageMemory === e2.onHeapStorageMemory)\n+        assert(e1.offHeapStorageMemory === e2.offHeapStorageMemory)\n+        assert(e1.onHeapUnifiedMemory === e2.onHeapUnifiedMemory)\n+        assert(e1.offHeapUnifiedMemory === e2.offHeapUnifiedMemory)\n+        assert(e1.directMemory === e2.directMemory)\n+        assert(e1.mappedMemory === e2.mappedMemory)\n+      case (None, None) =>\n+      case _ =>\n+        assert(false)\n+    }\n+  }\n+\n+  /** Check that the Spark history log line matches the expected event. */\n+  private def checkEvent(line: String, event: SparkListenerEvent): Unit = {\n+    assert(line.contains(event.getClass.toString.split(\"\\\\.\").last))\n+    event match {\n+      case executorMetrics: SparkListenerExecutorMetricsUpdate =>"
  }],
  "prId": 21221
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "it looks to me like you are not checking that you actually get everything in `expectedMetricsEvents` -- is that right?  can you add that check?",
    "commit": "571285beace1a0c1df92d9f5127828ed8955c93f",
    "createdAt": "2018-06-15T15:11:29Z",
    "diffHunk": "@@ -251,6 +261,222 @@ class EventLoggingListenerSuite extends SparkFunSuite with LocalSparkContext wit\n     }\n   }\n \n+  /**\n+   * Test executor metrics update logging functionality. This checks that a\n+   * SparkListenerExecutorMetricsUpdate event is added to the Spark history\n+   * log if one of the executor metrics is larger than any previously\n+   * recorded value for the metric, per executor per stage. The task metrics\n+   * should not be added.\n+   */\n+  private def testExecutorMetricsUpdateEventLogging() {\n+    val conf = getLoggingConf(testDirPath, None)\n+    val logName = \"executorMetricsUpdated-test\"\n+    val eventLogger = new EventLoggingListener(logName, None, testDirPath.toUri(), conf)\n+    val listenerBus = new LiveListenerBus(conf)\n+\n+    // expected ExecutorMetricsUpdate, for the given stage id and executor id\n+    val expectedMetricsEvents: Map[(Int, String), SparkListenerExecutorMetricsUpdate] ="
  }, {
    "author": {
      "login": "edwinalu"
    },
    "body": "I'll add a check.",
    "commit": "571285beace1a0c1df92d9f5127828ed8955c93f",
    "createdAt": "2018-06-16T03:07:34Z",
    "diffHunk": "@@ -251,6 +261,222 @@ class EventLoggingListenerSuite extends SparkFunSuite with LocalSparkContext wit\n     }\n   }\n \n+  /**\n+   * Test executor metrics update logging functionality. This checks that a\n+   * SparkListenerExecutorMetricsUpdate event is added to the Spark history\n+   * log if one of the executor metrics is larger than any previously\n+   * recorded value for the metric, per executor per stage. The task metrics\n+   * should not be added.\n+   */\n+  private def testExecutorMetricsUpdateEventLogging() {\n+    val conf = getLoggingConf(testDirPath, None)\n+    val logName = \"executorMetricsUpdated-test\"\n+    val eventLogger = new EventLoggingListener(logName, None, testDirPath.toUri(), conf)\n+    val listenerBus = new LiveListenerBus(conf)\n+\n+    // expected ExecutorMetricsUpdate, for the given stage id and executor id\n+    val expectedMetricsEvents: Map[(Int, String), SparkListenerExecutorMetricsUpdate] ="
  }],
  "prId": 21221
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "its hard to follow at a high level the purpose of what you're setting up here, so I'd drop in a few comments like\r\n\r\nreceive 3 metric updates from each executor with just stage 0 running, with different peak updates for each executor",
    "commit": "571285beace1a0c1df92d9f5127828ed8955c93f",
    "createdAt": "2018-06-15T15:14:41Z",
    "diffHunk": "@@ -251,6 +261,222 @@ class EventLoggingListenerSuite extends SparkFunSuite with LocalSparkContext wit\n     }\n   }\n \n+  /**\n+   * Test executor metrics update logging functionality. This checks that a\n+   * SparkListenerExecutorMetricsUpdate event is added to the Spark history\n+   * log if one of the executor metrics is larger than any previously\n+   * recorded value for the metric, per executor per stage. The task metrics\n+   * should not be added.\n+   */\n+  private def testExecutorMetricsUpdateEventLogging() {\n+    val conf = getLoggingConf(testDirPath, None)\n+    val logName = \"executorMetricsUpdated-test\"\n+    val eventLogger = new EventLoggingListener(logName, None, testDirPath.toUri(), conf)\n+    val listenerBus = new LiveListenerBus(conf)\n+\n+    // expected ExecutorMetricsUpdate, for the given stage id and executor id\n+    val expectedMetricsEvents: Map[(Int, String), SparkListenerExecutorMetricsUpdate] =\n+      Map(\n+        ((0, \"1\"),\n+          createExecutorMetricsUpdateEvent(1,\n+            new ExecutorMetrics(-1L,\n+              Array(5000L, 50L, 50L, 20L, 50L, 10L, 100L, 30L, 70L, 20L)))),\n+        ((0, \"2\"),\n+          createExecutorMetricsUpdateEvent(2,\n+            new ExecutorMetrics(-1L,\n+              Array(7000L, 70L, 50L, 20L, 10L, 10L, 50L, 30L, 80L, 40L)))),\n+        ((1, \"1\"),\n+          createExecutorMetricsUpdateEvent(1,\n+            new ExecutorMetrics(-1L,\n+              Array(7000L, 70L, 50L, 30L, 60L, 30L, 80L, 55L, 50L, 0L)))),\n+        ((1, \"2\"),\n+          createExecutorMetricsUpdateEvent(2,\n+            new ExecutorMetrics(-1L,\n+              Array(7000L, 70L, 50L, 40L, 10L, 30L, 50L, 60L, 40L, 40L)))))\n+\n+    // Events to post.\n+    val events = Array(\n+      SparkListenerApplicationStart(\"executionMetrics\", None,\n+        1L, \"update\", None),\n+      createExecutorAddedEvent(1),\n+      createExecutorAddedEvent(2),\n+      createStageSubmittedEvent(0),\n+      createExecutorMetricsUpdateEvent(1,"
  }, {
    "author": {
      "login": "edwinalu"
    },
    "body": "Adding comments.",
    "commit": "571285beace1a0c1df92d9f5127828ed8955c93f",
    "createdAt": "2018-06-16T03:07:07Z",
    "diffHunk": "@@ -251,6 +261,222 @@ class EventLoggingListenerSuite extends SparkFunSuite with LocalSparkContext wit\n     }\n   }\n \n+  /**\n+   * Test executor metrics update logging functionality. This checks that a\n+   * SparkListenerExecutorMetricsUpdate event is added to the Spark history\n+   * log if one of the executor metrics is larger than any previously\n+   * recorded value for the metric, per executor per stage. The task metrics\n+   * should not be added.\n+   */\n+  private def testExecutorMetricsUpdateEventLogging() {\n+    val conf = getLoggingConf(testDirPath, None)\n+    val logName = \"executorMetricsUpdated-test\"\n+    val eventLogger = new EventLoggingListener(logName, None, testDirPath.toUri(), conf)\n+    val listenerBus = new LiveListenerBus(conf)\n+\n+    // expected ExecutorMetricsUpdate, for the given stage id and executor id\n+    val expectedMetricsEvents: Map[(Int, String), SparkListenerExecutorMetricsUpdate] =\n+      Map(\n+        ((0, \"1\"),\n+          createExecutorMetricsUpdateEvent(1,\n+            new ExecutorMetrics(-1L,\n+              Array(5000L, 50L, 50L, 20L, 50L, 10L, 100L, 30L, 70L, 20L)))),\n+        ((0, \"2\"),\n+          createExecutorMetricsUpdateEvent(2,\n+            new ExecutorMetrics(-1L,\n+              Array(7000L, 70L, 50L, 20L, 10L, 10L, 50L, 30L, 80L, 40L)))),\n+        ((1, \"1\"),\n+          createExecutorMetricsUpdateEvent(1,\n+            new ExecutorMetrics(-1L,\n+              Array(7000L, 70L, 50L, 30L, 60L, 30L, 80L, 55L, 50L, 0L)))),\n+        ((1, \"2\"),\n+          createExecutorMetricsUpdateEvent(2,\n+            new ExecutorMetrics(-1L,\n+              Array(7000L, 70L, 50L, 40L, 10L, 30L, 50L, 60L, 40L, 40L)))))\n+\n+    // Events to post.\n+    val events = Array(\n+      SparkListenerApplicationStart(\"executionMetrics\", None,\n+        1L, \"update\", None),\n+      createExecutorAddedEvent(1),\n+      createExecutorAddedEvent(2),\n+      createStageSubmittedEvent(0),\n+      createExecutorMetricsUpdateEvent(1,"
  }],
  "prId": 21221
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "another comment:\r\n\r\nnow start stage 1, one more metric update for each executor, and a new peak for exec X\r\n\r\n(if there is a peak update, that is ...)",
    "commit": "571285beace1a0c1df92d9f5127828ed8955c93f",
    "createdAt": "2018-06-15T15:16:06Z",
    "diffHunk": "@@ -251,6 +261,222 @@ class EventLoggingListenerSuite extends SparkFunSuite with LocalSparkContext wit\n     }\n   }\n \n+  /**\n+   * Test executor metrics update logging functionality. This checks that a\n+   * SparkListenerExecutorMetricsUpdate event is added to the Spark history\n+   * log if one of the executor metrics is larger than any previously\n+   * recorded value for the metric, per executor per stage. The task metrics\n+   * should not be added.\n+   */\n+  private def testExecutorMetricsUpdateEventLogging() {\n+    val conf = getLoggingConf(testDirPath, None)\n+    val logName = \"executorMetricsUpdated-test\"\n+    val eventLogger = new EventLoggingListener(logName, None, testDirPath.toUri(), conf)\n+    val listenerBus = new LiveListenerBus(conf)\n+\n+    // expected ExecutorMetricsUpdate, for the given stage id and executor id\n+    val expectedMetricsEvents: Map[(Int, String), SparkListenerExecutorMetricsUpdate] =\n+      Map(\n+        ((0, \"1\"),\n+          createExecutorMetricsUpdateEvent(1,\n+            new ExecutorMetrics(-1L,\n+              Array(5000L, 50L, 50L, 20L, 50L, 10L, 100L, 30L, 70L, 20L)))),\n+        ((0, \"2\"),\n+          createExecutorMetricsUpdateEvent(2,\n+            new ExecutorMetrics(-1L,\n+              Array(7000L, 70L, 50L, 20L, 10L, 10L, 50L, 30L, 80L, 40L)))),\n+        ((1, \"1\"),\n+          createExecutorMetricsUpdateEvent(1,\n+            new ExecutorMetrics(-1L,\n+              Array(7000L, 70L, 50L, 30L, 60L, 30L, 80L, 55L, 50L, 0L)))),\n+        ((1, \"2\"),\n+          createExecutorMetricsUpdateEvent(2,\n+            new ExecutorMetrics(-1L,\n+              Array(7000L, 70L, 50L, 40L, 10L, 30L, 50L, 60L, 40L, 40L)))))\n+\n+    // Events to post.\n+    val events = Array(\n+      SparkListenerApplicationStart(\"executionMetrics\", None,\n+        1L, \"update\", None),\n+      createExecutorAddedEvent(1),\n+      createExecutorAddedEvent(2),\n+      createStageSubmittedEvent(0),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(10L,\n+          Array(4000L, 50L, 20L, 0L, 40L, 0L, 60L, 0L, 70L, 20L))),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(10L,\n+          Array(1500L, 50L, 20L, 0L, 0L, 0L, 20L, 0L, 70L, 0L))),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(15L,\n+          Array(4000L, 50L, 50L, 0L, 50L, 0L, 100L, 0L, 70L, 20L))),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(15L,\n+          Array(2000L, 50L, 10L, 0L, 10L, 0L, 30L, 0L, 70L, 0L))),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(20L,\n+          Array(2000L, 40L, 50L, 0L, 40L, 10L, 90L, 10L, 50L, 0L))),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(20L,\n+          Array(3500L, 50L, 15L, 0L, 10L, 10L, 35L, 10L, 80L, 0L))),\n+      createStageSubmittedEvent(1),"
  }],
  "prId": 21221
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "complete stage 0, and 3 more update for each executor with just stage 1 running, and another peak for exec Y",
    "commit": "571285beace1a0c1df92d9f5127828ed8955c93f",
    "createdAt": "2018-06-15T15:16:41Z",
    "diffHunk": "@@ -251,6 +261,222 @@ class EventLoggingListenerSuite extends SparkFunSuite with LocalSparkContext wit\n     }\n   }\n \n+  /**\n+   * Test executor metrics update logging functionality. This checks that a\n+   * SparkListenerExecutorMetricsUpdate event is added to the Spark history\n+   * log if one of the executor metrics is larger than any previously\n+   * recorded value for the metric, per executor per stage. The task metrics\n+   * should not be added.\n+   */\n+  private def testExecutorMetricsUpdateEventLogging() {\n+    val conf = getLoggingConf(testDirPath, None)\n+    val logName = \"executorMetricsUpdated-test\"\n+    val eventLogger = new EventLoggingListener(logName, None, testDirPath.toUri(), conf)\n+    val listenerBus = new LiveListenerBus(conf)\n+\n+    // expected ExecutorMetricsUpdate, for the given stage id and executor id\n+    val expectedMetricsEvents: Map[(Int, String), SparkListenerExecutorMetricsUpdate] =\n+      Map(\n+        ((0, \"1\"),\n+          createExecutorMetricsUpdateEvent(1,\n+            new ExecutorMetrics(-1L,\n+              Array(5000L, 50L, 50L, 20L, 50L, 10L, 100L, 30L, 70L, 20L)))),\n+        ((0, \"2\"),\n+          createExecutorMetricsUpdateEvent(2,\n+            new ExecutorMetrics(-1L,\n+              Array(7000L, 70L, 50L, 20L, 10L, 10L, 50L, 30L, 80L, 40L)))),\n+        ((1, \"1\"),\n+          createExecutorMetricsUpdateEvent(1,\n+            new ExecutorMetrics(-1L,\n+              Array(7000L, 70L, 50L, 30L, 60L, 30L, 80L, 55L, 50L, 0L)))),\n+        ((1, \"2\"),\n+          createExecutorMetricsUpdateEvent(2,\n+            new ExecutorMetrics(-1L,\n+              Array(7000L, 70L, 50L, 40L, 10L, 30L, 50L, 60L, 40L, 40L)))))\n+\n+    // Events to post.\n+    val events = Array(\n+      SparkListenerApplicationStart(\"executionMetrics\", None,\n+        1L, \"update\", None),\n+      createExecutorAddedEvent(1),\n+      createExecutorAddedEvent(2),\n+      createStageSubmittedEvent(0),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(10L,\n+          Array(4000L, 50L, 20L, 0L, 40L, 0L, 60L, 0L, 70L, 20L))),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(10L,\n+          Array(1500L, 50L, 20L, 0L, 0L, 0L, 20L, 0L, 70L, 0L))),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(15L,\n+          Array(4000L, 50L, 50L, 0L, 50L, 0L, 100L, 0L, 70L, 20L))),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(15L,\n+          Array(2000L, 50L, 10L, 0L, 10L, 0L, 30L, 0L, 70L, 0L))),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(20L,\n+          Array(2000L, 40L, 50L, 0L, 40L, 10L, 90L, 10L, 50L, 0L))),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(20L,\n+          Array(3500L, 50L, 15L, 0L, 10L, 10L, 35L, 10L, 80L, 0L))),\n+      createStageSubmittedEvent(1),\n+      createExecutorMetricsUpdateEvent(1,\n+        new ExecutorMetrics(25L,\n+          Array(5000L, 30L, 50L, 20L, 30L, 10L, 80L, 30L, 50L, 0L))),\n+      createExecutorMetricsUpdateEvent(2,\n+        new ExecutorMetrics(25L,\n+          Array(7000L, 70L, 50L, 20L, 0L, 10L, 50L, 30L, 10L, 40L))),\n+      createStageCompletedEvent(0),"
  }],
  "prId": 21221
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "i think you mean *executor* metrics, not *task* metrics, right?",
    "commit": "571285beace1a0c1df92d9f5127828ed8955c93f",
    "createdAt": "2018-06-27T20:12:23Z",
    "diffHunk": "@@ -251,6 +261,217 @@ class EventLoggingListenerSuite extends SparkFunSuite with LocalSparkContext wit\n     }\n   }\n \n+  /**\n+   * Test executor metrics update logging functionality. This checks that a\n+   * SparkListenerExecutorMetricsUpdate event is added to the Spark history\n+   * log if one of the executor metrics is larger than any previously\n+   * recorded value for the metric, per executor per stage. The task metrics"
  }, {
    "author": {
      "login": "edwinalu"
    },
    "body": "Woops, that was left over from when it was ExecutorMetricsUpdated.",
    "commit": "571285beace1a0c1df92d9f5127828ed8955c93f",
    "createdAt": "2018-06-28T01:07:28Z",
    "diffHunk": "@@ -251,6 +261,217 @@ class EventLoggingListenerSuite extends SparkFunSuite with LocalSparkContext wit\n     }\n   }\n \n+  /**\n+   * Test executor metrics update logging functionality. This checks that a\n+   * SparkListenerExecutorMetricsUpdate event is added to the Spark history\n+   * log if one of the executor metrics is larger than any previously\n+   * recorded value for the metric, per executor per stage. The task metrics"
  }],
  "prId": 21221
}, {
  "comments": [{
    "author": {
      "login": "rezasafi"
    },
    "body": "Are this comment and the one in line 322 correct? Shouldn't it say stage 1?",
    "commit": "571285beace1a0c1df92d9f5127828ed8955c93f",
    "createdAt": "2018-07-25T04:14:41Z",
    "diffHunk": "@@ -251,6 +261,215 @@ class EventLoggingListenerSuite extends SparkFunSuite with LocalSparkContext wit\n     }\n   }\n \n+  /**\n+   * Test stage executor metrics logging functionality. This checks that peak\n+   * values from SparkListenerExecutorMetricsUpdate events during a stage are\n+   * logged in a StageExecutorMetrics event for each executor at stage completion.\n+   */\n+  private def testStageExecutorMetricsEventLogging() {\n+    val conf = getLoggingConf(testDirPath, None)\n+    val logName = \"stageExecutorMetrics-test\"\n+    val eventLogger = new EventLoggingListener(logName, None, testDirPath.toUri(), conf)\n+    val listenerBus = new LiveListenerBus(conf)\n+\n+    // expected StageExecutorMetrics, for the given stage id and executor id\n+    val expectedMetricsEvents: Map[(Int, String), SparkListenerStageExecutorMetrics] =\n+      Map(\n+        ((0, \"1\"),\n+          new SparkListenerStageExecutorMetrics(\"1\", 0, 0,\n+              Array(5000L, 50L, 50L, 20L, 50L, 10L, 100L, 30L, 70L, 20L))),\n+        ((0, \"2\"),\n+          new SparkListenerStageExecutorMetrics(\"2\", 0, 0,\n+              Array(7000L, 70L, 50L, 20L, 10L, 10L, 50L, 30L, 80L, 40L))),\n+        ((1, \"1\"),\n+          new SparkListenerStageExecutorMetrics(\"1\", 1, 0,\n+              Array(7000L, 70L, 50L, 30L, 60L, 30L, 80L, 55L, 50L, 0L))),\n+        ((1, \"2\"),\n+          new SparkListenerStageExecutorMetrics(\"2\", 1, 0,\n+              Array(7000L, 70L, 50L, 40L, 10L, 30L, 50L, 60L, 40L, 40L))))\n+\n+    // Events to post.\n+    val events = Array(\n+      SparkListenerApplicationStart(\"executionMetrics\", None,\n+        1L, \"update\", None),\n+      createExecutorAddedEvent(1),\n+      createExecutorAddedEvent(2),\n+      createStageSubmittedEvent(0),\n+      // receive 3 metric updates from each executor with just stage 0 running,\n+      // with different peak updates for each executor\n+      createExecutorMetricsUpdateEvent(1,\n+          Array(4000L, 50L, 20L, 0L, 40L, 0L, 60L, 0L, 70L, 20L)),\n+      createExecutorMetricsUpdateEvent(2,\n+          Array(1500L, 50L, 20L, 0L, 0L, 0L, 20L, 0L, 70L, 0L)),\n+      // exec 1: new stage 0 peaks for metrics at indexes: 2, 4, 6\n+      createExecutorMetricsUpdateEvent(1,\n+          Array(4000L, 50L, 50L, 0L, 50L, 0L, 100L, 0L, 70L, 20L)),\n+      // exec 2: new stage 0 peaks for metrics at indexes: 0, 4, 6\n+      createExecutorMetricsUpdateEvent(2,\n+          Array(2000L, 50L, 10L, 0L, 10L, 0L, 30L, 0L, 70L, 0L)),\n+      // exec 1: new stage 0 peaks for metrics at indexes: 5, 7\n+      createExecutorMetricsUpdateEvent(1,\n+          Array(2000L, 40L, 50L, 0L, 40L, 10L, 90L, 10L, 50L, 0L)),\n+      // exec 2: new stage 0 peaks for metrics at indexes: 0, 5, 6, 7, 8\n+      createExecutorMetricsUpdateEvent(2,\n+          Array(3500L, 50L, 15L, 0L, 10L, 10L, 35L, 10L, 80L, 0L)),\n+      // now start stage 1, one more metric update for each executor, and new\n+      // peaks for some stage 1 metrics (as listed), initialize stage 1 peaks\n+      createStageSubmittedEvent(1),\n+      // exec 1: new stage 0 peaks for metrics at indexes: 0, 3, 7"
  }, {
    "author": {
      "login": "edwinalu"
    },
    "body": "Stage 0 is still running, and these are new peaks for that stage. It is also initializing all the stage 1 metric values, since these are the first executor metrics seen for stage 1 (I'll add this to the comments).",
    "commit": "571285beace1a0c1df92d9f5127828ed8955c93f",
    "createdAt": "2018-07-25T12:51:42Z",
    "diffHunk": "@@ -251,6 +261,215 @@ class EventLoggingListenerSuite extends SparkFunSuite with LocalSparkContext wit\n     }\n   }\n \n+  /**\n+   * Test stage executor metrics logging functionality. This checks that peak\n+   * values from SparkListenerExecutorMetricsUpdate events during a stage are\n+   * logged in a StageExecutorMetrics event for each executor at stage completion.\n+   */\n+  private def testStageExecutorMetricsEventLogging() {\n+    val conf = getLoggingConf(testDirPath, None)\n+    val logName = \"stageExecutorMetrics-test\"\n+    val eventLogger = new EventLoggingListener(logName, None, testDirPath.toUri(), conf)\n+    val listenerBus = new LiveListenerBus(conf)\n+\n+    // expected StageExecutorMetrics, for the given stage id and executor id\n+    val expectedMetricsEvents: Map[(Int, String), SparkListenerStageExecutorMetrics] =\n+      Map(\n+        ((0, \"1\"),\n+          new SparkListenerStageExecutorMetrics(\"1\", 0, 0,\n+              Array(5000L, 50L, 50L, 20L, 50L, 10L, 100L, 30L, 70L, 20L))),\n+        ((0, \"2\"),\n+          new SparkListenerStageExecutorMetrics(\"2\", 0, 0,\n+              Array(7000L, 70L, 50L, 20L, 10L, 10L, 50L, 30L, 80L, 40L))),\n+        ((1, \"1\"),\n+          new SparkListenerStageExecutorMetrics(\"1\", 1, 0,\n+              Array(7000L, 70L, 50L, 30L, 60L, 30L, 80L, 55L, 50L, 0L))),\n+        ((1, \"2\"),\n+          new SparkListenerStageExecutorMetrics(\"2\", 1, 0,\n+              Array(7000L, 70L, 50L, 40L, 10L, 30L, 50L, 60L, 40L, 40L))))\n+\n+    // Events to post.\n+    val events = Array(\n+      SparkListenerApplicationStart(\"executionMetrics\", None,\n+        1L, \"update\", None),\n+      createExecutorAddedEvent(1),\n+      createExecutorAddedEvent(2),\n+      createStageSubmittedEvent(0),\n+      // receive 3 metric updates from each executor with just stage 0 running,\n+      // with different peak updates for each executor\n+      createExecutorMetricsUpdateEvent(1,\n+          Array(4000L, 50L, 20L, 0L, 40L, 0L, 60L, 0L, 70L, 20L)),\n+      createExecutorMetricsUpdateEvent(2,\n+          Array(1500L, 50L, 20L, 0L, 0L, 0L, 20L, 0L, 70L, 0L)),\n+      // exec 1: new stage 0 peaks for metrics at indexes: 2, 4, 6\n+      createExecutorMetricsUpdateEvent(1,\n+          Array(4000L, 50L, 50L, 0L, 50L, 0L, 100L, 0L, 70L, 20L)),\n+      // exec 2: new stage 0 peaks for metrics at indexes: 0, 4, 6\n+      createExecutorMetricsUpdateEvent(2,\n+          Array(2000L, 50L, 10L, 0L, 10L, 0L, 30L, 0L, 70L, 0L)),\n+      // exec 1: new stage 0 peaks for metrics at indexes: 5, 7\n+      createExecutorMetricsUpdateEvent(1,\n+          Array(2000L, 40L, 50L, 0L, 40L, 10L, 90L, 10L, 50L, 0L)),\n+      // exec 2: new stage 0 peaks for metrics at indexes: 0, 5, 6, 7, 8\n+      createExecutorMetricsUpdateEvent(2,\n+          Array(3500L, 50L, 15L, 0L, 10L, 10L, 35L, 10L, 80L, 0L)),\n+      // now start stage 1, one more metric update for each executor, and new\n+      // peaks for some stage 1 metrics (as listed), initialize stage 1 peaks\n+      createStageSubmittedEvent(1),\n+      // exec 1: new stage 0 peaks for metrics at indexes: 0, 3, 7"
  }],
  "prId": 21221
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "super nit: I'd move these expectations after the events to post, it follows a bit more naturally.",
    "commit": "571285beace1a0c1df92d9f5127828ed8955c93f",
    "createdAt": "2018-08-01T19:14:21Z",
    "diffHunk": "@@ -251,6 +261,214 @@ class EventLoggingListenerSuite extends SparkFunSuite with LocalSparkContext wit\n     }\n   }\n \n+  /**\n+   * Test stage executor metrics logging functionality. This checks that peak\n+   * values from SparkListenerExecutorMetricsUpdate events during a stage are\n+   * logged in a StageExecutorMetrics event for each executor at stage completion.\n+   */\n+  private def testStageExecutorMetricsEventLogging() {\n+    val conf = getLoggingConf(testDirPath, None)\n+    val logName = \"stageExecutorMetrics-test\"\n+    val eventLogger = new EventLoggingListener(logName, None, testDirPath.toUri(), conf)\n+    val listenerBus = new LiveListenerBus(conf)\n+\n+    // expected StageExecutorMetrics, for the given stage id and executor id\n+    val expectedMetricsEvents: Map[(Int, String), SparkListenerStageExecutorMetrics] ="
  }, {
    "author": {
      "login": "edwinalu"
    },
    "body": "Moved to after the replay.",
    "commit": "571285beace1a0c1df92d9f5127828ed8955c93f",
    "createdAt": "2018-08-05T01:38:33Z",
    "diffHunk": "@@ -251,6 +261,214 @@ class EventLoggingListenerSuite extends SparkFunSuite with LocalSparkContext wit\n     }\n   }\n \n+  /**\n+   * Test stage executor metrics logging functionality. This checks that peak\n+   * values from SparkListenerExecutorMetricsUpdate events during a stage are\n+   * logged in a StageExecutorMetrics event for each executor at stage completion.\n+   */\n+  private def testStageExecutorMetricsEventLogging() {\n+    val conf = getLoggingConf(testDirPath, None)\n+    val logName = \"stageExecutorMetrics-test\"\n+    val eventLogger = new EventLoggingListener(logName, None, testDirPath.toUri(), conf)\n+    val listenerBus = new LiveListenerBus(conf)\n+\n+    // expected StageExecutorMetrics, for the given stage id and executor id\n+    val expectedMetricsEvents: Map[(Int, String), SparkListenerStageExecutorMetrics] ="
  }],
  "prId": 21221
}]