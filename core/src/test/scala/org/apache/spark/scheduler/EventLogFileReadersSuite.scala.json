[{
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`.map { c => ... }`",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-17T21:19:14Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io.{ByteArrayInputStream, ByteArrayOutputStream, File}\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+import java.util.zip.{ZipInputStream, ZipOutputStream}\n+\n+import scala.collection.mutable\n+\n+import com.google.common.io.{ByteStreams, Files}\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.{LocalSparkContext, SparkConf, SparkFunSuite}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.scheduler.EventLogTestHelper._\n+import org.apache.spark.util.Utils\n+\n+abstract class EventLogFileReadersSuite extends SparkFunSuite with LocalSparkContext\n+  with BeforeAndAfter with Logging {\n+\n+  protected val fileSystem = Utils.getHadoopFileSystem(\"/\",\n+    SparkHadoopUtil.get.newConfiguration(new SparkConf()))\n+  protected var testDir: File = _\n+  protected var testDirPath: Path = _\n+\n+  before {\n+    testDir = Utils.createTempDir(namePrefix = s\"event log\")\n+    testDir.deleteOnExit()\n+    testDirPath = new Path(testDir.getAbsolutePath())\n+  }\n+\n+  after {\n+    Utils.deleteRecursively(testDir)\n+  }\n+\n+  test(\"Retrieve EventLogFileReader correctly\") {\n+    def assertInstanceOfEventLogReader(\n+        expectedClazz: Option[Class[_ <: EventLogFileReader]],\n+        actual: Option[EventLogFileReader]): Unit = {\n+      if (expectedClazz.isEmpty) {\n+        assert(actual.isEmpty, s\"Expected no EventLogFileReader instance but was \" +\n+          s\"${actual.map(_.getClass).getOrElse(\"<None>\")}\")\n+      } else {\n+        assert(actual.isDefined, s\"Expected an EventLogFileReader instance but was empty\")\n+        assert(expectedClazz.get.isAssignableFrom(actual.get.getClass),\n+          s\"Expected ${expectedClazz.get} but was ${actual.get.getClass}\")\n+      }\n+    }\n+\n+    def testForPathWithoutSeq(\n+        path: Path,\n+        isFile: Boolean,\n+        expectedClazz: Option[Class[_ <: EventLogFileReader]]): Unit = {\n+      if (isFile) {\n+        Utils.tryWithResource(fileSystem.create(path)) { is =>\n+          is.writeInt(10)\n+        }\n+      } else {\n+        fileSystem.mkdirs(path)\n+      }\n+\n+      val reader = EventLogFileReader.getEventLogReader(fileSystem, path)\n+      assertInstanceOfEventLogReader(expectedClazz, reader)\n+      val reader2 = EventLogFileReader.getEventLogReader(fileSystem,\n+        fileSystem.getFileStatus(path))\n+      assertInstanceOfEventLogReader(expectedClazz, reader)\n+    }\n+\n+    // path with no last sequence - single event log\n+    val reader1 = EventLogFileReader.getEventLogReader(fileSystem, new Path(testDirPath, \"aaa\"),\n+      None)\n+    assertInstanceOfEventLogReader(Some(classOf[SingleFileEventLogFileReader]), Some(reader1))\n+\n+    // path with last sequence - rolling event log\n+    val reader2 = EventLogFileReader.getEventLogReader(fileSystem,\n+      new Path(testDirPath, \"eventlog_v2_aaa\"), Some(3))\n+    assertInstanceOfEventLogReader(Some(classOf[RollingEventLogFilesFileReader]), Some(reader2))\n+\n+    // path - file (both path and FileStatus)\n+    val eventLogFile = new Path(testDirPath, \"bbb\")\n+    testForPathWithoutSeq(eventLogFile, isFile = true, Some(classOf[SingleFileEventLogFileReader]))\n+\n+    // path - file starting with \".\"\n+    val invalidEventLogFile = new Path(testDirPath, \".bbb\")\n+    testForPathWithoutSeq(invalidEventLogFile, isFile = true, None)\n+\n+    // path - directory with \"eventlog_v2_\" prefix\n+    val eventLogDir = new Path(testDirPath, \"eventlog_v2_ccc\")\n+    testForPathWithoutSeq(eventLogDir, isFile = false,\n+      Some(classOf[RollingEventLogFilesFileReader]))\n+\n+    // path - directory with no \"eventlog_v2_\" prefix\n+    val invalidEventLogDir = new Path(testDirPath, \"ccc\")\n+    testForPathWithoutSeq(invalidEventLogDir, isFile = false, None)\n+  }\n+\n+  val allCodecs = Seq(None) ++\n+    CompressionCodec.ALL_COMPRESSION_CODECS.map(c => Some(CompressionCodec.getShortName(c)))"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Use `toSeq` after the `map` instead?",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-17T21:19:34Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io.{ByteArrayInputStream, ByteArrayOutputStream, File}\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+import java.util.zip.{ZipInputStream, ZipOutputStream}\n+\n+import scala.collection.mutable\n+\n+import com.google.common.io.{ByteStreams, Files}\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.{LocalSparkContext, SparkConf, SparkFunSuite}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.scheduler.EventLogTestHelper._\n+import org.apache.spark.util.Utils\n+\n+abstract class EventLogFileReadersSuite extends SparkFunSuite with LocalSparkContext\n+  with BeforeAndAfter with Logging {\n+\n+  protected val fileSystem = Utils.getHadoopFileSystem(\"/\",\n+    SparkHadoopUtil.get.newConfiguration(new SparkConf()))\n+  protected var testDir: File = _\n+  protected var testDirPath: Path = _\n+\n+  before {\n+    testDir = Utils.createTempDir(namePrefix = s\"event log\")\n+    testDir.deleteOnExit()\n+    testDirPath = new Path(testDir.getAbsolutePath())\n+  }\n+\n+  after {\n+    Utils.deleteRecursively(testDir)\n+  }\n+\n+  test(\"Retrieve EventLogFileReader correctly\") {\n+    def assertInstanceOfEventLogReader(\n+        expectedClazz: Option[Class[_ <: EventLogFileReader]],\n+        actual: Option[EventLogFileReader]): Unit = {\n+      if (expectedClazz.isEmpty) {\n+        assert(actual.isEmpty, s\"Expected no EventLogFileReader instance but was \" +\n+          s\"${actual.map(_.getClass).getOrElse(\"<None>\")}\")\n+      } else {\n+        assert(actual.isDefined, s\"Expected an EventLogFileReader instance but was empty\")\n+        assert(expectedClazz.get.isAssignableFrom(actual.get.getClass),\n+          s\"Expected ${expectedClazz.get} but was ${actual.get.getClass}\")\n+      }\n+    }\n+\n+    def testForPathWithoutSeq(\n+        path: Path,\n+        isFile: Boolean,\n+        expectedClazz: Option[Class[_ <: EventLogFileReader]]): Unit = {\n+      if (isFile) {\n+        Utils.tryWithResource(fileSystem.create(path)) { is =>\n+          is.writeInt(10)\n+        }\n+      } else {\n+        fileSystem.mkdirs(path)\n+      }\n+\n+      val reader = EventLogFileReader.getEventLogReader(fileSystem, path)\n+      assertInstanceOfEventLogReader(expectedClazz, reader)\n+      val reader2 = EventLogFileReader.getEventLogReader(fileSystem,\n+        fileSystem.getFileStatus(path))\n+      assertInstanceOfEventLogReader(expectedClazz, reader)\n+    }\n+\n+    // path with no last sequence - single event log\n+    val reader1 = EventLogFileReader.getEventLogReader(fileSystem, new Path(testDirPath, \"aaa\"),\n+      None)\n+    assertInstanceOfEventLogReader(Some(classOf[SingleFileEventLogFileReader]), Some(reader1))\n+\n+    // path with last sequence - rolling event log\n+    val reader2 = EventLogFileReader.getEventLogReader(fileSystem,\n+      new Path(testDirPath, \"eventlog_v2_aaa\"), Some(3))\n+    assertInstanceOfEventLogReader(Some(classOf[RollingEventLogFilesFileReader]), Some(reader2))\n+\n+    // path - file (both path and FileStatus)\n+    val eventLogFile = new Path(testDirPath, \"bbb\")\n+    testForPathWithoutSeq(eventLogFile, isFile = true, Some(classOf[SingleFileEventLogFileReader]))\n+\n+    // path - file starting with \".\"\n+    val invalidEventLogFile = new Path(testDirPath, \".bbb\")\n+    testForPathWithoutSeq(invalidEventLogFile, isFile = true, None)\n+\n+    // path - directory with \"eventlog_v2_\" prefix\n+    val eventLogDir = new Path(testDirPath, \"eventlog_v2_ccc\")\n+    testForPathWithoutSeq(eventLogDir, isFile = false,\n+      Some(classOf[RollingEventLogFilesFileReader]))\n+\n+    // path - directory with no \"eventlog_v2_\" prefix\n+    val invalidEventLogDir = new Path(testDirPath, \"ccc\")\n+    testForPathWithoutSeq(invalidEventLogDir, isFile = false, None)\n+  }\n+\n+  val allCodecs = Seq(None) ++"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "This is to append \"None\" into available codecs.",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-18T01:39:58Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io.{ByteArrayInputStream, ByteArrayOutputStream, File}\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+import java.util.zip.{ZipInputStream, ZipOutputStream}\n+\n+import scala.collection.mutable\n+\n+import com.google.common.io.{ByteStreams, Files}\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.{LocalSparkContext, SparkConf, SparkFunSuite}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.scheduler.EventLogTestHelper._\n+import org.apache.spark.util.Utils\n+\n+abstract class EventLogFileReadersSuite extends SparkFunSuite with LocalSparkContext\n+  with BeforeAndAfter with Logging {\n+\n+  protected val fileSystem = Utils.getHadoopFileSystem(\"/\",\n+    SparkHadoopUtil.get.newConfiguration(new SparkConf()))\n+  protected var testDir: File = _\n+  protected var testDirPath: Path = _\n+\n+  before {\n+    testDir = Utils.createTempDir(namePrefix = s\"event log\")\n+    testDir.deleteOnExit()\n+    testDirPath = new Path(testDir.getAbsolutePath())\n+  }\n+\n+  after {\n+    Utils.deleteRecursively(testDir)\n+  }\n+\n+  test(\"Retrieve EventLogFileReader correctly\") {\n+    def assertInstanceOfEventLogReader(\n+        expectedClazz: Option[Class[_ <: EventLogFileReader]],\n+        actual: Option[EventLogFileReader]): Unit = {\n+      if (expectedClazz.isEmpty) {\n+        assert(actual.isEmpty, s\"Expected no EventLogFileReader instance but was \" +\n+          s\"${actual.map(_.getClass).getOrElse(\"<None>\")}\")\n+      } else {\n+        assert(actual.isDefined, s\"Expected an EventLogFileReader instance but was empty\")\n+        assert(expectedClazz.get.isAssignableFrom(actual.get.getClass),\n+          s\"Expected ${expectedClazz.get} but was ${actual.get.getClass}\")\n+      }\n+    }\n+\n+    def testForPathWithoutSeq(\n+        path: Path,\n+        isFile: Boolean,\n+        expectedClazz: Option[Class[_ <: EventLogFileReader]]): Unit = {\n+      if (isFile) {\n+        Utils.tryWithResource(fileSystem.create(path)) { is =>\n+          is.writeInt(10)\n+        }\n+      } else {\n+        fileSystem.mkdirs(path)\n+      }\n+\n+      val reader = EventLogFileReader.getEventLogReader(fileSystem, path)\n+      assertInstanceOfEventLogReader(expectedClazz, reader)\n+      val reader2 = EventLogFileReader.getEventLogReader(fileSystem,\n+        fileSystem.getFileStatus(path))\n+      assertInstanceOfEventLogReader(expectedClazz, reader)\n+    }\n+\n+    // path with no last sequence - single event log\n+    val reader1 = EventLogFileReader.getEventLogReader(fileSystem, new Path(testDirPath, \"aaa\"),\n+      None)\n+    assertInstanceOfEventLogReader(Some(classOf[SingleFileEventLogFileReader]), Some(reader1))\n+\n+    // path with last sequence - rolling event log\n+    val reader2 = EventLogFileReader.getEventLogReader(fileSystem,\n+      new Path(testDirPath, \"eventlog_v2_aaa\"), Some(3))\n+    assertInstanceOfEventLogReader(Some(classOf[RollingEventLogFilesFileReader]), Some(reader2))\n+\n+    // path - file (both path and FileStatus)\n+    val eventLogFile = new Path(testDirPath, \"bbb\")\n+    testForPathWithoutSeq(eventLogFile, isFile = true, Some(classOf[SingleFileEventLogFileReader]))\n+\n+    // path - file starting with \".\"\n+    val invalidEventLogFile = new Path(testDirPath, \".bbb\")\n+    testForPathWithoutSeq(invalidEventLogFile, isFile = true, None)\n+\n+    // path - directory with \"eventlog_v2_\" prefix\n+    val eventLogDir = new Path(testDirPath, \"eventlog_v2_ccc\")\n+    testForPathWithoutSeq(eventLogDir, isFile = false,\n+      Some(classOf[RollingEventLogFilesFileReader]))\n+\n+    // path - directory with no \"eventlog_v2_\" prefix\n+    val invalidEventLogDir = new Path(testDirPath, \"ccc\")\n+    testForPathWithoutSeq(invalidEventLogDir, isFile = false, None)\n+  }\n+\n+  val allCodecs = Seq(None) ++"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "codec",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-17T21:20:01Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io.{ByteArrayInputStream, ByteArrayOutputStream, File}\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+import java.util.zip.{ZipInputStream, ZipOutputStream}\n+\n+import scala.collection.mutable\n+\n+import com.google.common.io.{ByteStreams, Files}\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.{LocalSparkContext, SparkConf, SparkFunSuite}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.scheduler.EventLogTestHelper._\n+import org.apache.spark.util.Utils\n+\n+abstract class EventLogFileReadersSuite extends SparkFunSuite with LocalSparkContext\n+  with BeforeAndAfter with Logging {\n+\n+  protected val fileSystem = Utils.getHadoopFileSystem(\"/\",\n+    SparkHadoopUtil.get.newConfiguration(new SparkConf()))\n+  protected var testDir: File = _\n+  protected var testDirPath: Path = _\n+\n+  before {\n+    testDir = Utils.createTempDir(namePrefix = s\"event log\")\n+    testDir.deleteOnExit()\n+    testDirPath = new Path(testDir.getAbsolutePath())\n+  }\n+\n+  after {\n+    Utils.deleteRecursively(testDir)\n+  }\n+\n+  test(\"Retrieve EventLogFileReader correctly\") {\n+    def assertInstanceOfEventLogReader(\n+        expectedClazz: Option[Class[_ <: EventLogFileReader]],\n+        actual: Option[EventLogFileReader]): Unit = {\n+      if (expectedClazz.isEmpty) {\n+        assert(actual.isEmpty, s\"Expected no EventLogFileReader instance but was \" +\n+          s\"${actual.map(_.getClass).getOrElse(\"<None>\")}\")\n+      } else {\n+        assert(actual.isDefined, s\"Expected an EventLogFileReader instance but was empty\")\n+        assert(expectedClazz.get.isAssignableFrom(actual.get.getClass),\n+          s\"Expected ${expectedClazz.get} but was ${actual.get.getClass}\")\n+      }\n+    }\n+\n+    def testForPathWithoutSeq(\n+        path: Path,\n+        isFile: Boolean,\n+        expectedClazz: Option[Class[_ <: EventLogFileReader]]): Unit = {\n+      if (isFile) {\n+        Utils.tryWithResource(fileSystem.create(path)) { is =>\n+          is.writeInt(10)\n+        }\n+      } else {\n+        fileSystem.mkdirs(path)\n+      }\n+\n+      val reader = EventLogFileReader.getEventLogReader(fileSystem, path)\n+      assertInstanceOfEventLogReader(expectedClazz, reader)\n+      val reader2 = EventLogFileReader.getEventLogReader(fileSystem,\n+        fileSystem.getFileStatus(path))\n+      assertInstanceOfEventLogReader(expectedClazz, reader)\n+    }\n+\n+    // path with no last sequence - single event log\n+    val reader1 = EventLogFileReader.getEventLogReader(fileSystem, new Path(testDirPath, \"aaa\"),\n+      None)\n+    assertInstanceOfEventLogReader(Some(classOf[SingleFileEventLogFileReader]), Some(reader1))\n+\n+    // path with last sequence - rolling event log\n+    val reader2 = EventLogFileReader.getEventLogReader(fileSystem,\n+      new Path(testDirPath, \"eventlog_v2_aaa\"), Some(3))\n+    assertInstanceOfEventLogReader(Some(classOf[RollingEventLogFilesFileReader]), Some(reader2))\n+\n+    // path - file (both path and FileStatus)\n+    val eventLogFile = new Path(testDirPath, \"bbb\")\n+    testForPathWithoutSeq(eventLogFile, isFile = true, Some(classOf[SingleFileEventLogFileReader]))\n+\n+    // path - file starting with \".\"\n+    val invalidEventLogFile = new Path(testDirPath, \".bbb\")\n+    testForPathWithoutSeq(invalidEventLogFile, isFile = true, None)\n+\n+    // path - directory with \"eventlog_v2_\" prefix\n+    val eventLogDir = new Path(testDirPath, \"eventlog_v2_ccc\")\n+    testForPathWithoutSeq(eventLogDir, isFile = false,\n+      Some(classOf[RollingEventLogFilesFileReader]))\n+\n+    // path - directory with no \"eventlog_v2_\" prefix\n+    val invalidEventLogDir = new Path(testDirPath, \"ccc\")\n+    testForPathWithoutSeq(invalidEventLogDir, isFile = false, None)\n+  }\n+\n+  val allCodecs = Seq(None) ++\n+    CompressionCodec.ALL_COMPRESSION_CODECS.map(c => Some(CompressionCodec.getShortName(c)))\n+\n+  allCodecs.foreach { codecShortName =>\n+    test(s\"get information, list event log files, zip log files - with code $codecShortName\") {"
  }],
  "prId": 25670
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Indent more",
    "commit": "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "createdAt": "2019-09-17T21:21:11Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import java.io.{ByteArrayInputStream, ByteArrayOutputStream, File}\n+import java.net.URI\n+import java.nio.charset.StandardCharsets\n+import java.util.zip.{ZipInputStream, ZipOutputStream}\n+\n+import scala.collection.mutable\n+\n+import com.google.common.io.{ByteStreams, Files}\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.{LocalSparkContext, SparkConf, SparkFunSuite}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+import org.apache.spark.io.CompressionCodec\n+import org.apache.spark.scheduler.EventLogTestHelper._\n+import org.apache.spark.util.Utils\n+\n+abstract class EventLogFileReadersSuite extends SparkFunSuite with LocalSparkContext\n+  with BeforeAndAfter with Logging {\n+\n+  protected val fileSystem = Utils.getHadoopFileSystem(\"/\",\n+    SparkHadoopUtil.get.newConfiguration(new SparkConf()))\n+  protected var testDir: File = _\n+  protected var testDirPath: Path = _\n+\n+  before {\n+    testDir = Utils.createTempDir(namePrefix = s\"event log\")\n+    testDir.deleteOnExit()\n+    testDirPath = new Path(testDir.getAbsolutePath())\n+  }\n+\n+  after {\n+    Utils.deleteRecursively(testDir)\n+  }\n+\n+  test(\"Retrieve EventLogFileReader correctly\") {\n+    def assertInstanceOfEventLogReader(\n+        expectedClazz: Option[Class[_ <: EventLogFileReader]],\n+        actual: Option[EventLogFileReader]): Unit = {\n+      if (expectedClazz.isEmpty) {\n+        assert(actual.isEmpty, s\"Expected no EventLogFileReader instance but was \" +\n+          s\"${actual.map(_.getClass).getOrElse(\"<None>\")}\")\n+      } else {\n+        assert(actual.isDefined, s\"Expected an EventLogFileReader instance but was empty\")\n+        assert(expectedClazz.get.isAssignableFrom(actual.get.getClass),\n+          s\"Expected ${expectedClazz.get} but was ${actual.get.getClass}\")\n+      }\n+    }\n+\n+    def testForPathWithoutSeq(\n+        path: Path,\n+        isFile: Boolean,\n+        expectedClazz: Option[Class[_ <: EventLogFileReader]]): Unit = {\n+      if (isFile) {\n+        Utils.tryWithResource(fileSystem.create(path)) { is =>\n+          is.writeInt(10)\n+        }\n+      } else {\n+        fileSystem.mkdirs(path)\n+      }\n+\n+      val reader = EventLogFileReader.getEventLogReader(fileSystem, path)\n+      assertInstanceOfEventLogReader(expectedClazz, reader)\n+      val reader2 = EventLogFileReader.getEventLogReader(fileSystem,\n+        fileSystem.getFileStatus(path))\n+      assertInstanceOfEventLogReader(expectedClazz, reader)\n+    }\n+\n+    // path with no last sequence - single event log\n+    val reader1 = EventLogFileReader.getEventLogReader(fileSystem, new Path(testDirPath, \"aaa\"),\n+      None)\n+    assertInstanceOfEventLogReader(Some(classOf[SingleFileEventLogFileReader]), Some(reader1))\n+\n+    // path with last sequence - rolling event log\n+    val reader2 = EventLogFileReader.getEventLogReader(fileSystem,\n+      new Path(testDirPath, \"eventlog_v2_aaa\"), Some(3))\n+    assertInstanceOfEventLogReader(Some(classOf[RollingEventLogFilesFileReader]), Some(reader2))\n+\n+    // path - file (both path and FileStatus)\n+    val eventLogFile = new Path(testDirPath, \"bbb\")\n+    testForPathWithoutSeq(eventLogFile, isFile = true, Some(classOf[SingleFileEventLogFileReader]))\n+\n+    // path - file starting with \".\"\n+    val invalidEventLogFile = new Path(testDirPath, \".bbb\")\n+    testForPathWithoutSeq(invalidEventLogFile, isFile = true, None)\n+\n+    // path - directory with \"eventlog_v2_\" prefix\n+    val eventLogDir = new Path(testDirPath, \"eventlog_v2_ccc\")\n+    testForPathWithoutSeq(eventLogDir, isFile = false,\n+      Some(classOf[RollingEventLogFilesFileReader]))\n+\n+    // path - directory with no \"eventlog_v2_\" prefix\n+    val invalidEventLogDir = new Path(testDirPath, \"ccc\")\n+    testForPathWithoutSeq(invalidEventLogDir, isFile = false, None)\n+  }\n+\n+  val allCodecs = Seq(None) ++\n+    CompressionCodec.ALL_COMPRESSION_CODECS.map(c => Some(CompressionCodec.getShortName(c)))\n+\n+  allCodecs.foreach { codecShortName =>\n+    test(s\"get information, list event log files, zip log files - with code $codecShortName\") {\n+      val appId = getUniqueApplicationId\n+      val attemptId = None\n+\n+      val conf = getLoggingConf(testDirPath, codecShortName)\n+      val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+\n+      val writer = createWriter(appId, attemptId, testDirPath.toUri, conf, hadoopConf)\n+      writer.start()\n+\n+      // The test for writing events into EventLogFileWriter is covered to its own test suite.\n+      val dummyData = Seq(\"dummy1\", \"dummy2\", \"dummy3\")\n+      dummyData.foreach(writer.writeEvent(_, flushLogger = true))\n+\n+      val logPathIncompleted = getCurrentLogPath(writer.logPath, isCompleted = false)\n+      val readerOpt = EventLogFileReader.getEventLogReader(fileSystem,\n+        new Path(logPathIncompleted))\n+      assertAppropriateReader(readerOpt)\n+      val reader = readerOpt.get\n+\n+      verifyReader(reader, new Path(logPathIncompleted), codecShortName, isCompleted = false)\n+\n+      writer.stop()\n+\n+      val logPathCompleted = getCurrentLogPath(writer.logPath, isCompleted = true)\n+      val readerOpt2 = EventLogFileReader.getEventLogReader(fileSystem, new Path(logPathCompleted))\n+      assertAppropriateReader(readerOpt2)\n+      val reader2 = readerOpt2.get\n+\n+      verifyReader(reader2, new Path(logPathCompleted), codecShortName, isCompleted = true)\n+    }\n+  }\n+\n+  protected def createWriter(\n+      appId: String,\n+      appAttemptId : Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter\n+\n+  protected def getCurrentLogPath(logPath: String, isCompleted: Boolean): String\n+\n+  protected def assertAppropriateReader(actualReader: Option[EventLogFileReader]): Unit\n+\n+  protected def verifyReader(\n+      reader: EventLogFileReader,\n+      logPath: Path,\n+      compressionCodecShortName: Option[String],\n+      isCompleted: Boolean): Unit\n+}\n+\n+class SingleFileEventLogFileReaderSuite extends EventLogFileReadersSuite {\n+  override protected def createWriter(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    new SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+  }\n+\n+  override protected def assertAppropriateReader(actualReader: Option[EventLogFileReader]): Unit = {\n+    assert(actualReader.isDefined, s\"Expected an EventLogReader instance but was empty\")\n+    assert(actualReader.get.isInstanceOf[SingleFileEventLogFileReader],\n+      s\"Expected SingleFileEventLogReader but was ${actualReader.get.getClass}\")\n+  }\n+\n+  override protected def getCurrentLogPath(logPath: String, isCompleted: Boolean): String = {\n+    if (!isCompleted) logPath + EventLogFileWriter.IN_PROGRESS else logPath\n+  }\n+\n+  override protected def verifyReader(\n+      reader: EventLogFileReader,\n+      logPath: Path,\n+      compressionCodecShortName: Option[String],\n+      isCompleted: Boolean): Unit = {\n+    val stats = fileSystem.getFileStatus(logPath)\n+\n+    assert(stats.isFile)\n+    assert(reader.rootPath === logPath)\n+    assert(reader.lastSequence.isEmpty)\n+    assert(reader.fileSizeForLastSequence === stats.getLen)\n+    assert(reader.completed === isCompleted)\n+    assert(reader.modificationTime === stats.getModificationTime)\n+    assert(reader.listEventLogFiles.length === 1)\n+    assert(reader.listEventLogFiles.map(_.getPath.toUri.getPath) ===\n+      Seq(logPath.toUri.getPath))\n+    assert(reader.compression === compressionCodecShortName)\n+    assert(reader.allSize === stats.getLen)\n+\n+    val underlyingStream = new ByteArrayOutputStream()\n+    Utils.tryWithResource(new ZipOutputStream(underlyingStream)) { os =>\n+      reader.zipEventLogFiles(os)\n+    }\n+\n+    Utils.tryWithResource(new ZipInputStream(\n+      new ByteArrayInputStream(underlyingStream.toByteArray))) { is =>\n+\n+      var entry = is.getNextEntry\n+      assert(entry != null)\n+      val actual = new String(ByteStreams.toByteArray(is), StandardCharsets.UTF_8)\n+      val expected = Files.toString(new File(logPath.toString), StandardCharsets.UTF_8)\n+      assert(actual === expected)\n+      assert(is.getNextEntry === null)\n+    }\n+  }\n+}\n+\n+class RollingEventLogFilesReaderSuite extends EventLogFileReadersSuite {\n+  allCodecs.foreach { codecShortName =>\n+    test(s\"rolling event log files - codec $codecShortName\") {\n+      val appId = getUniqueApplicationId\n+      val attemptId = None\n+\n+      val conf = getLoggingConf(testDirPath, codecShortName)\n+      conf.set(EVENT_LOG_ENABLE_ROLLING, true)\n+      conf.set(EVENT_LOG_ROLLED_EVENT_LOG_MAX_FILE_SIZE.key, \"1k\")\n+\n+      val writer = createWriter(appId, attemptId, testDirPath.toUri, conf,\n+        SparkHadoopUtil.get.newConfiguration(conf))\n+\n+      writer.start()\n+\n+      val dummyString = \"dummy\"\n+      val dummyStringBytesLen = dummyString.getBytes(StandardCharsets.UTF_8).length\n+      val expectedLines = mutable.ArrayBuffer[String]()\n+\n+      // write log more than 2k (intended to roll over to 3 files)\n+      val repeatCount = Math.floor((1024 * 2) / dummyStringBytesLen).toInt\n+      (0 until repeatCount).foreach { _ =>\n+        expectedLines.append(dummyString)\n+        writer.writeEvent(dummyString, flushLogger = true)\n+      }\n+\n+      val logPathIncompleted = getCurrentLogPath(writer.logPath, isCompleted = false)\n+      val readerOpt = EventLogFileReader.getEventLogReader(fileSystem,\n+        new Path(logPathIncompleted))\n+      verifyReader(readerOpt.get, new Path(logPathIncompleted), codecShortName, isCompleted = false)\n+      assert(readerOpt.get.listEventLogFiles.length === 3)\n+\n+      writer.stop()\n+\n+      val logPathCompleted = getCurrentLogPath(writer.logPath, isCompleted = true)\n+      val readerOpt2 = EventLogFileReader.getEventLogReader(fileSystem, new Path(logPathCompleted))\n+      verifyReader(readerOpt2.get, new Path(logPathCompleted), codecShortName, isCompleted = true)\n+      assert(readerOpt.get.listEventLogFiles.length === 3)\n+    }\n+  }\n+\n+  override protected def createWriter(\n+      appId: String,\n+      appAttemptId: Option[String],\n+      logBaseDir: URI,\n+      sparkConf: SparkConf,\n+      hadoopConf: Configuration): EventLogFileWriter = {\n+    new RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf)\n+  }\n+\n+  override protected def assertAppropriateReader(actualReader: Option[EventLogFileReader]): Unit = {\n+    assert(actualReader.isDefined, s\"Expected an EventLogReader instance but was empty\")\n+    assert(actualReader.get.isInstanceOf[RollingEventLogFilesFileReader],\n+      s\"Expected RollingEventLogFilesReader but was ${actualReader.get.getClass}\")\n+  }\n+\n+  override protected def getCurrentLogPath(logPath: String, isCompleted: Boolean): String = logPath\n+\n+  override protected def verifyReader(\n+      reader: EventLogFileReader,\n+      logPath: Path,\n+      compressionCodecShortName: Option[String],\n+      isCompleted: Boolean): Unit = {\n+    import RollingEventLogFilesWriter._\n+\n+    val stats = fileSystem.getFileStatus(logPath)\n+    assert(stats.isDirectory)\n+\n+    val statsInDir = fileSystem.listStatus(logPath)\n+    val eventFiles = statsInDir.filter(isEventLogFile).sortBy(s => getSequence(s.getPath.getName))\n+    assert(eventFiles.nonEmpty)\n+    val lastEventFile = eventFiles.last\n+    val allLen = eventFiles.map(_.getLen).sum\n+\n+    assert(reader.rootPath === logPath)\n+    assert(reader.lastSequence === Some(getSequence(lastEventFile.getPath.getName)))\n+    assert(reader.fileSizeForLastSequence === lastEventFile.getLen)\n+    assert(reader.completed === isCompleted)\n+    assert(reader.modificationTime === lastEventFile.getModificationTime)\n+    assert(reader.listEventLogFiles.length === eventFiles.length)\n+    assert(reader.listEventLogFiles.map(_.getPath) === eventFiles.map(_.getPath))\n+    assert(reader.compression === compressionCodecShortName)\n+    assert(reader.allSize === allLen)\n+\n+    val underlyingStream = new ByteArrayOutputStream()\n+    Utils.tryWithResource(new ZipOutputStream(underlyingStream)) { os =>\n+      reader.zipEventLogFiles(os)\n+    }\n+\n+    Utils.tryWithResource(new ZipInputStream(\n+      new ByteArrayInputStream(underlyingStream.toByteArray))) { is =>"
  }],
  "prId": 25670
}]