[{
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Hmmm, no. If you need access to this field, make it visible, or use `PrivateMethodTester`.\n",
    "commit": "57427f731a2d69c6536c1e7cd1d7af185f64e348",
    "createdAt": "2015-09-25T23:11:18Z",
    "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.mutable\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.{LocalSparkContext, SparkConf, SparkContext, SparkException, SparkFunSuite}\n+import org.apache.spark.scheduler.{SparkListenerTaskStart, SparkListener}\n+import org.apache.spark.util.{AkkaUtils, SerializableBuffer}\n+\n+class CoarseGrainedSchedulerBackendSuite extends SparkFunSuite with LocalSparkContext {\n+  import CoarseGrainedSchedulerBackendSuite._\n+\n+  test(\"serialized task larger than akka frame size\") {\n+    val conf = new SparkConf\n+    conf.set(\"spark.akka.frameSize\", \"1\")\n+    conf.set(\"spark.default.parallelism\", \"1\")\n+    sc = new SparkContext(\"local-cluster[2, 1, 1024]\", \"test\", conf)\n+    val frameSize = AkkaUtils.maxFrameSizeBytes(sc.conf)\n+    val buffer = new SerializableBuffer(java.nio.ByteBuffer.allocate(2 * frameSize))\n+    val larger = sc.parallelize(Seq(buffer))\n+    val thrown = intercept[SparkException] {\n+      larger.collect()\n+    }\n+    assert(thrown.getMessage.contains(\"using broadcast variables for large values\"))\n+    val smaller = sc.parallelize(1 to 4).collect()\n+    assert(smaller.size === 4)\n+  }\n+\n+  test(\"avoid scheduling tasks on preempted executors\") {\n+    // Initialize the SparkContext add register the listener\n+    val conf = new SparkConf()\n+    sc = new SparkContext(\"local-cluster[2, 1, 1024]\", \"test\", conf)\n+    val taskTracker = new TaskStateTracker\n+    sc.addSparkListener(taskTracker)\n+\n+    sc.parallelize(1 to 100).collect()\n+    taskTracker.taskSet.clear()\n+\n+    val backend = sc.schedulerBackend\n+    val executors = {\n+      val method = backend.getClass.getSuperclass.getDeclaredMethod("
  }, {
    "author": {
      "login": "jerryshao"
    },
    "body": "I'm not sure if `PrivateMethodTester` can be worked for superclass, I did tried before, but seems have some problems to find out the method. Seems if you don't accept reflection way, make it visible is the only solution.\n",
    "commit": "57427f731a2d69c6536c1e7cd1d7af185f64e348",
    "createdAt": "2015-09-25T23:36:23Z",
    "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.mutable\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.{LocalSparkContext, SparkConf, SparkContext, SparkException, SparkFunSuite}\n+import org.apache.spark.scheduler.{SparkListenerTaskStart, SparkListener}\n+import org.apache.spark.util.{AkkaUtils, SerializableBuffer}\n+\n+class CoarseGrainedSchedulerBackendSuite extends SparkFunSuite with LocalSparkContext {\n+  import CoarseGrainedSchedulerBackendSuite._\n+\n+  test(\"serialized task larger than akka frame size\") {\n+    val conf = new SparkConf\n+    conf.set(\"spark.akka.frameSize\", \"1\")\n+    conf.set(\"spark.default.parallelism\", \"1\")\n+    sc = new SparkContext(\"local-cluster[2, 1, 1024]\", \"test\", conf)\n+    val frameSize = AkkaUtils.maxFrameSizeBytes(sc.conf)\n+    val buffer = new SerializableBuffer(java.nio.ByteBuffer.allocate(2 * frameSize))\n+    val larger = sc.parallelize(Seq(buffer))\n+    val thrown = intercept[SparkException] {\n+      larger.collect()\n+    }\n+    assert(thrown.getMessage.contains(\"using broadcast variables for large values\"))\n+    val smaller = sc.parallelize(1 to 4).collect()\n+    assert(smaller.size === 4)\n+  }\n+\n+  test(\"avoid scheduling tasks on preempted executors\") {\n+    // Initialize the SparkContext add register the listener\n+    val conf = new SparkConf()\n+    sc = new SparkContext(\"local-cluster[2, 1, 1024]\", \"test\", conf)\n+    val taskTracker = new TaskStateTracker\n+    sc.addSparkListener(taskTracker)\n+\n+    sc.parallelize(1 to 100).collect()\n+    taskTracker.taskSet.clear()\n+\n+    val backend = sc.schedulerBackend\n+    val executors = {\n+      val method = backend.getClass.getSuperclass.getDeclaredMethod("
  }],
  "prId": 7786
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Don't use reflection here. Just make fields visible.\n",
    "commit": "57427f731a2d69c6536c1e7cd1d7af185f64e348",
    "createdAt": "2015-09-25T23:11:52Z",
    "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.mutable\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.{LocalSparkContext, SparkConf, SparkContext, SparkException, SparkFunSuite}\n+import org.apache.spark.scheduler.{SparkListenerTaskStart, SparkListener}\n+import org.apache.spark.util.{AkkaUtils, SerializableBuffer}\n+\n+class CoarseGrainedSchedulerBackendSuite extends SparkFunSuite with LocalSparkContext {\n+  import CoarseGrainedSchedulerBackendSuite._\n+\n+  test(\"serialized task larger than akka frame size\") {\n+    val conf = new SparkConf\n+    conf.set(\"spark.akka.frameSize\", \"1\")\n+    conf.set(\"spark.default.parallelism\", \"1\")\n+    sc = new SparkContext(\"local-cluster[2, 1, 1024]\", \"test\", conf)\n+    val frameSize = AkkaUtils.maxFrameSizeBytes(sc.conf)\n+    val buffer = new SerializableBuffer(java.nio.ByteBuffer.allocate(2 * frameSize))\n+    val larger = sc.parallelize(Seq(buffer))\n+    val thrown = intercept[SparkException] {\n+      larger.collect()\n+    }\n+    assert(thrown.getMessage.contains(\"using broadcast variables for large values\"))\n+    val smaller = sc.parallelize(1 to 4).collect()\n+    assert(smaller.size === 4)\n+  }\n+\n+  test(\"avoid scheduling tasks on preempted executors\") {\n+    // Initialize the SparkContext add register the listener\n+    val conf = new SparkConf()\n+    sc = new SparkContext(\"local-cluster[2, 1, 1024]\", \"test\", conf)\n+    val taskTracker = new TaskStateTracker\n+    sc.addSparkListener(taskTracker)\n+\n+    sc.parallelize(1 to 100).collect()\n+    taskTracker.taskSet.clear()\n+\n+    val backend = sc.schedulerBackend\n+    val executors = {\n+      val method = backend.getClass.getSuperclass.getDeclaredMethod(\n+        \"org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$$executorDataMap\"\n+      )\n+      method.setAccessible(true)\n+      method.invoke(backend).asInstanceOf[mutable.HashMap[String, ExecutorData]]\n+    }.keySet\n+\n+    // There should be two executors in running\n+    assert(executors.size === 2)\n+\n+    val preemptedExecutors = {\n+      val method = backend.getClass.getSuperclass.getDeclaredMethod(\"preemptedExecutorIDs\")"
  }],
  "prId": 7786
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Why not just declare this inside the class? No need for the `object`.\n",
    "commit": "57427f731a2d69c6536c1e7cd1d7af185f64e348",
    "createdAt": "2015-09-25T23:12:49Z",
    "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.mutable\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.{LocalSparkContext, SparkConf, SparkContext, SparkException, SparkFunSuite}\n+import org.apache.spark.scheduler.{SparkListenerTaskStart, SparkListener}\n+import org.apache.spark.util.{AkkaUtils, SerializableBuffer}\n+\n+class CoarseGrainedSchedulerBackendSuite extends SparkFunSuite with LocalSparkContext {\n+  import CoarseGrainedSchedulerBackendSuite._\n+\n+  test(\"serialized task larger than akka frame size\") {\n+    val conf = new SparkConf\n+    conf.set(\"spark.akka.frameSize\", \"1\")\n+    conf.set(\"spark.default.parallelism\", \"1\")\n+    sc = new SparkContext(\"local-cluster[2, 1, 1024]\", \"test\", conf)\n+    val frameSize = AkkaUtils.maxFrameSizeBytes(sc.conf)\n+    val buffer = new SerializableBuffer(java.nio.ByteBuffer.allocate(2 * frameSize))\n+    val larger = sc.parallelize(Seq(buffer))\n+    val thrown = intercept[SparkException] {\n+      larger.collect()\n+    }\n+    assert(thrown.getMessage.contains(\"using broadcast variables for large values\"))\n+    val smaller = sc.parallelize(1 to 4).collect()\n+    assert(smaller.size === 4)\n+  }\n+\n+  test(\"avoid scheduling tasks on preempted executors\") {\n+    // Initialize the SparkContext add register the listener\n+    val conf = new SparkConf()\n+    sc = new SparkContext(\"local-cluster[2, 1, 1024]\", \"test\", conf)\n+    val taskTracker = new TaskStateTracker\n+    sc.addSparkListener(taskTracker)\n+\n+    sc.parallelize(1 to 100).collect()\n+    taskTracker.taskSet.clear()\n+\n+    val backend = sc.schedulerBackend\n+    val executors = {\n+      val method = backend.getClass.getSuperclass.getDeclaredMethod(\n+        \"org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$$executorDataMap\"\n+      )\n+      method.setAccessible(true)\n+      method.invoke(backend).asInstanceOf[mutable.HashMap[String, ExecutorData]]\n+    }.keySet\n+\n+    // There should be two executors in running\n+    assert(executors.size === 2)\n+\n+    val preemptedExecutors = {\n+      val method = backend.getClass.getSuperclass.getDeclaredMethod(\"preemptedExecutorIDs\")\n+      method.setAccessible(true)\n+      method.invoke(backend).asInstanceOf[mutable.HashSet[String]]\n+    }\n+\n+    // There should be no preempted executors when initialized\n+    assert(preemptedExecutors.size === 0)\n+    sc.parallelize(1 to 100, 10).collect()\n+    assert(taskTracker.taskSet.size() === 10)\n+    assert(taskTracker.taskSet.values().asScala.toSet === Set(\"0\", \"1\"))\n+    taskTracker.taskSet.clear()\n+\n+    // Add executor \"1\" as a preempted executor\n+    preemptedExecutors.add(\"1\")\n+    assert(preemptedExecutors === Set(\"1\"))\n+    sc.parallelize(1 to 100, 10).collect()\n+    assert(taskTracker.taskSet.size() === 10)\n+    // All the tasks should be only scheduled on executor \"0\"\n+    assert(taskTracker.taskSet.values().asScala.toSet === Set(\"0\"))\n+    taskTracker.taskSet.clear()\n+\n+    // Clear the preempted executors\n+    preemptedExecutors.clear()\n+    assert(preemptedExecutors === Set.empty)\n+    sc.parallelize(1 to 100, 10).collect()\n+    assert(taskTracker.taskSet.size() === 10)\n+    // All the tasks will be scheduled on executor \"0\" and \"1\"\n+    assert(taskTracker.taskSet.values().asScala.toSet === Set(\"0\", \"1\"))\n+    taskTracker.taskSet.clear()\n+  }\n+}\n+\n+private object CoarseGrainedSchedulerBackendSuite {\n+  class TaskStateTracker extends SparkListener {"
  }],
  "prId": 7786
}]