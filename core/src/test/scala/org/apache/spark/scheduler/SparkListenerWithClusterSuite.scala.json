[{
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "Same \"no space before `:`\" style nit here, too, as long as you're updating things.\n",
    "commit": "946d2c52f0e1db32c4a041b2f62e8b0a71fd9fec",
    "createdAt": "2014-12-23T21:51:54Z",
    "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import org.apache.spark.scheduler.cluster.ExecutorInfo\n+import org.apache.spark.{SparkContext, LocalSparkContext}\n+\n+import org.scalatest.{FunSuite, BeforeAndAfter, BeforeAndAfterAll}\n+\n+import scala.collection.mutable\n+\n+/**\n+ * Unit tests for SparkListener that require a local cluster.\n+ */\n+class SparkListenerWithClusterSuite extends FunSuite with LocalSparkContext\n+  with BeforeAndAfter with BeforeAndAfterAll {\n+\n+  /** Length of time to wait while draining listener events. */\n+  val WAIT_TIMEOUT_MILLIS = 10000\n+\n+  before {\n+    sc = new SparkContext(\"local-cluster[2,1,512]\", \"SparkListenerSuite\")\n+  }\n+\n+  test(\"SparkListener sends executor added message\") {\n+    val listener = new SaveExecutorInfo\n+    sc.addSparkListener(listener)\n+\n+    val rdd1 = sc.parallelize(1 to 100, 4)\n+    val rdd2 = rdd1.map(_.toString)\n+    rdd2.setName(\"Target RDD\")\n+    rdd2.count()\n+\n+    assert(sc.listenerBus.waitUntilEmpty(WAIT_TIMEOUT_MILLIS))\n+    assert(listener.addedExecutorInfos.size == 2)\n+    assert(listener.addedExecutorInfos(\"0\").totalCores == 1)\n+    assert(listener.addedExecutorInfos(\"1\").totalCores == 1)\n+  }\n+\n+  private class SaveExecutorInfo extends SparkListener {\n+    val addedExecutorInfos = mutable.Map[String, ExecutorInfo]()\n+\n+    override def onExecutorAdded(executor : SparkListenerExecutorAdded) {"
  }],
  "prId": 3711
}]