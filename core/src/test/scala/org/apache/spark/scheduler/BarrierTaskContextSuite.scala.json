[{
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "should comment why we need local cluster",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-07-31T23:43:13Z",
    "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.util.Random\n+\n+import org.apache.spark._\n+\n+class BarrierTaskContextSuite extends SparkFunSuite with LocalSparkContext {\n+\n+  test(\"global sync by barrier() call\") {\n+    val conf = new SparkConf()\n+      .setMaster(\"local-cluster[4, 1, 1024]\")"
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "5ms seem too risky to me. Actually, 1 second is perhaps okay here.",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-07-31T23:47:35Z",
    "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.util.Random\n+\n+import org.apache.spark._\n+\n+class BarrierTaskContextSuite extends SparkFunSuite with LocalSparkContext {\n+\n+  test(\"global sync by barrier() call\") {\n+    val conf = new SparkConf()\n+      .setMaster(\"local-cluster[4, 1, 1024]\")\n+      .setAppName(\"test-cluster\")\n+    sc = new SparkContext(conf)\n+    val rdd = sc.makeRDD(1 to 10, 4)\n+    val rdd2 = rdd.barrier().mapPartitions { (it, context) =>\n+      // Sleep for a random time before global sync.\n+      Thread.sleep(Random.nextInt(1000))\n+      context.barrier()\n+      Seq(System.currentTimeMillis()).iterator\n+    }\n+\n+    val times = rdd2.collect()\n+    // All the tasks shall finish global sync within a short time slot.\n+    assert(times.max - times.min <= 5)"
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Why ignored? To create this scenario, we might need to create a new thread to call `context.barrier()` and then interrupt the thread.",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-07-31T23:50:02Z",
    "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.util.Random\n+\n+import org.apache.spark._\n+\n+class BarrierTaskContextSuite extends SparkFunSuite with LocalSparkContext {\n+\n+  test(\"global sync by barrier() call\") {\n+    val conf = new SparkConf()\n+      .setMaster(\"local-cluster[4, 1, 1024]\")\n+      .setAppName(\"test-cluster\")\n+    sc = new SparkContext(conf)\n+    val rdd = sc.makeRDD(1 to 10, 4)\n+    val rdd2 = rdd.barrier().mapPartitions { (it, context) =>\n+      // Sleep for a random time before global sync.\n+      Thread.sleep(Random.nextInt(1000))\n+      context.barrier()\n+      Seq(System.currentTimeMillis()).iterator\n+    }\n+\n+    val times = rdd2.collect()\n+    // All the tasks shall finish global sync within a short time slot.\n+    assert(times.max - times.min <= 5)\n+  }\n+\n+  test(\"support multiple barrier() call within a single task\") {\n+    val conf = new SparkConf()\n+      .setMaster(\"local-cluster[4, 1, 1024]\")\n+      .setAppName(\"test-cluster\")\n+    sc = new SparkContext(conf)\n+    val rdd = sc.makeRDD(1 to 10, 4)\n+    val rdd2 = rdd.barrier().mapPartitions { (it, context) =>\n+      // Sleep for a random time before global sync.\n+      Thread.sleep(Random.nextInt(1000))\n+      context.barrier()\n+      val time1 = System.currentTimeMillis()\n+      // Sleep for a random time between two global syncs.\n+      Thread.sleep(Random.nextInt(1000))\n+      context.barrier()\n+      val time2 = System.currentTimeMillis()\n+      Seq((time1, time2)).iterator\n+    }\n+\n+    val times = rdd2.collect()\n+    // All the tasks shall finish the first round of global sync within a short time slot.\n+    val times1 = times.map(_._1)\n+    assert(times1.max - times1.min <= 5)\n+\n+    // All the tasks shall finish the second round of global sync within a short time slot.\n+    val times2 = times.map(_._2)\n+    assert(times2.max - times2.min <= 5)\n+  }\n+\n+  test(\"throw exception on barrier() call timeout\") {\n+    val conf = new SparkConf()\n+      .set(\"spark.barrier.sync.timeout\", \"100\")\n+      .set(\"spark.test.noStageRetry\", \"true\")\n+      .setMaster(\"local-cluster[4, 1, 1024]\")\n+      .setAppName(\"test-cluster\")\n+    sc = new SparkContext(conf)\n+    val rdd = sc.makeRDD(1 to 10, 4)\n+    val rdd2 = rdd.barrier().mapPartitions { (it, context) =>\n+      // Task 3 shall sleep 200ms to ensure barrier() call timeout\n+      if (context.taskAttemptId() == 3) {\n+        Thread.sleep(200)\n+      }\n+      context.barrier()\n+      it\n+    }\n+\n+    val error = intercept[SparkException] {\n+      rdd2.collect()\n+    }.getMessage\n+    assert(error.contains(\"The coordinator didn't get all barrier sync requests\"))\n+    assert(error.contains(\"within 100 ms\"))\n+  }\n+\n+  test(\"throw exception if barrier() call doesn't happen on every task\") {\n+    val conf = new SparkConf()\n+      .set(\"spark.barrier.sync.timeout\", \"100\")\n+      .set(\"spark.test.noStageRetry\", \"true\")\n+      .setMaster(\"local-cluster[4, 1, 1024]\")\n+      .setAppName(\"test-cluster\")\n+    sc = new SparkContext(conf)\n+    val rdd = sc.makeRDD(1 to 10, 4)\n+    val rdd2 = rdd.barrier().mapPartitions { (it, context) =>\n+      if (context.taskAttemptId() != 0) {\n+        context.barrier()\n+      }\n+      it\n+    }\n+\n+    val error = intercept[SparkException] {\n+      rdd2.collect()\n+    }.getMessage\n+    assert(error.contains(\"The coordinator didn't get all barrier sync requests\"))\n+    assert(error.contains(\"within 100 ms\"))\n+  }\n+\n+  ignore(\"throw exception if barrier() call mismatched\") {"
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "seems can be simpler\r\n```\r\nif (context.taskAttemptId == 0) {\r\n  context.barrier()\r\n  context.barrier()\r\n} else {\r\n  context.barrier()\r\n}\r\n```",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-04T09:28:54Z",
    "diffHunk": "@@ -0,0 +1,148 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.util.Random\n+\n+import org.apache.spark._\n+\n+class BarrierTaskContextSuite extends SparkFunSuite with LocalSparkContext {\n+\n+  test(\"global sync by barrier() call\") {\n+    val conf = new SparkConf()\n+      // Init local cluster here so each barrier task runs in a separated process, thus `barrier()`\n+      // call is actually useful.\n+      .setMaster(\"local-cluster[4, 1, 1024]\")\n+      .setAppName(\"test-cluster\")\n+    sc = new SparkContext(conf)\n+    val rdd = sc.makeRDD(1 to 10, 4)\n+    val rdd2 = rdd.barrier().mapPartitions { (it, context) =>\n+      // Sleep for a random time before global sync.\n+      Thread.sleep(Random.nextInt(1000))\n+      context.barrier()\n+      Seq(System.currentTimeMillis()).iterator\n+    }\n+\n+    val times = rdd2.collect()\n+    // All the tasks shall finish global sync within a short time slot.\n+    assert(times.max - times.min <= 1000)\n+  }\n+\n+  test(\"support multiple barrier() call within a single task\") {\n+    val conf = new SparkConf()\n+      .setMaster(\"local-cluster[4, 1, 1024]\")\n+      .setAppName(\"test-cluster\")\n+    sc = new SparkContext(conf)\n+    val rdd = sc.makeRDD(1 to 10, 4)\n+    val rdd2 = rdd.barrier().mapPartitions { (it, context) =>\n+      // Sleep for a random time before global sync.\n+      Thread.sleep(Random.nextInt(1000))\n+      context.barrier()\n+      val time1 = System.currentTimeMillis()\n+      // Sleep for a random time between two global syncs.\n+      Thread.sleep(Random.nextInt(1000))\n+      context.barrier()\n+      val time2 = System.currentTimeMillis()\n+      Seq((time1, time2)).iterator\n+    }\n+\n+    val times = rdd2.collect()\n+    // All the tasks shall finish the first round of global sync within a short time slot.\n+    val times1 = times.map(_._1)\n+    assert(times1.max - times1.min <= 1000)\n+\n+    // All the tasks shall finish the second round of global sync within a short time slot.\n+    val times2 = times.map(_._2)\n+    assert(times2.max - times2.min <= 1000)\n+  }\n+\n+  test(\"throw exception on barrier() call timeout\") {\n+    val conf = new SparkConf()\n+      .set(\"spark.barrier.sync.timeout\", \"1\")\n+      .set(\"spark.test.noStageRetry\", \"true\")\n+      .setMaster(\"local-cluster[4, 1, 1024]\")\n+      .setAppName(\"test-cluster\")\n+    sc = new SparkContext(conf)\n+    val rdd = sc.makeRDD(1 to 10, 4)\n+    val rdd2 = rdd.barrier().mapPartitions { (it, context) =>\n+      // Task 3 shall sleep 2000ms to ensure barrier() call timeout\n+      if (context.taskAttemptId == 3) {\n+        Thread.sleep(2000)\n+      }\n+      context.barrier()\n+      it\n+    }\n+\n+    val error = intercept[SparkException] {\n+      rdd2.collect()\n+    }.getMessage\n+    assert(error.contains(\"The coordinator didn't get all barrier sync requests\"))\n+    assert(error.contains(\"within 1s\"))\n+  }\n+\n+  test(\"throw exception if barrier() call doesn't happen on every task\") {\n+    val conf = new SparkConf()\n+      .set(\"spark.barrier.sync.timeout\", \"1\")\n+      .set(\"spark.test.noStageRetry\", \"true\")\n+      .setMaster(\"local-cluster[4, 1, 1024]\")\n+      .setAppName(\"test-cluster\")\n+    sc = new SparkContext(conf)\n+    val rdd = sc.makeRDD(1 to 10, 4)\n+    val rdd2 = rdd.barrier().mapPartitions { (it, context) =>\n+      if (context.taskAttemptId != 0) {\n+        context.barrier()\n+      }\n+      it\n+    }\n+\n+    val error = intercept[SparkException] {\n+      rdd2.collect()\n+    }.getMessage\n+    assert(error.contains(\"The coordinator didn't get all barrier sync requests\"))\n+    assert(error.contains(\"within 1s\"))\n+  }\n+\n+  test(\"throw exception if the number of barrier() calls are not the same on every task\") {\n+    val conf = new SparkConf()\n+      .set(\"spark.barrier.sync.timeout\", \"1\")\n+      .set(\"spark.test.noStageRetry\", \"true\")\n+      .setMaster(\"local-cluster[4, 1, 1024]\")\n+      .setAppName(\"test-cluster\")\n+    sc = new SparkContext(conf)\n+    val rdd = sc.makeRDD(1 to 10, 4)\n+    val rdd2 = rdd.barrier().mapPartitions { (it, context) =>\n+      try {",
    "line": 129
  }, {
    "author": {
      "login": "jiangxb1987"
    },
    "body": "This actually shows one kind of wrong use case that accidentally skipped a `barrier()` call for a task. I updated the comment to show why we issue the problem in a less straight forward way.",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-04T14:52:59Z",
    "diffHunk": "@@ -0,0 +1,148 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.util.Random\n+\n+import org.apache.spark._\n+\n+class BarrierTaskContextSuite extends SparkFunSuite with LocalSparkContext {\n+\n+  test(\"global sync by barrier() call\") {\n+    val conf = new SparkConf()\n+      // Init local cluster here so each barrier task runs in a separated process, thus `barrier()`\n+      // call is actually useful.\n+      .setMaster(\"local-cluster[4, 1, 1024]\")\n+      .setAppName(\"test-cluster\")\n+    sc = new SparkContext(conf)\n+    val rdd = sc.makeRDD(1 to 10, 4)\n+    val rdd2 = rdd.barrier().mapPartitions { (it, context) =>\n+      // Sleep for a random time before global sync.\n+      Thread.sleep(Random.nextInt(1000))\n+      context.barrier()\n+      Seq(System.currentTimeMillis()).iterator\n+    }\n+\n+    val times = rdd2.collect()\n+    // All the tasks shall finish global sync within a short time slot.\n+    assert(times.max - times.min <= 1000)\n+  }\n+\n+  test(\"support multiple barrier() call within a single task\") {\n+    val conf = new SparkConf()\n+      .setMaster(\"local-cluster[4, 1, 1024]\")\n+      .setAppName(\"test-cluster\")\n+    sc = new SparkContext(conf)\n+    val rdd = sc.makeRDD(1 to 10, 4)\n+    val rdd2 = rdd.barrier().mapPartitions { (it, context) =>\n+      // Sleep for a random time before global sync.\n+      Thread.sleep(Random.nextInt(1000))\n+      context.barrier()\n+      val time1 = System.currentTimeMillis()\n+      // Sleep for a random time between two global syncs.\n+      Thread.sleep(Random.nextInt(1000))\n+      context.barrier()\n+      val time2 = System.currentTimeMillis()\n+      Seq((time1, time2)).iterator\n+    }\n+\n+    val times = rdd2.collect()\n+    // All the tasks shall finish the first round of global sync within a short time slot.\n+    val times1 = times.map(_._1)\n+    assert(times1.max - times1.min <= 1000)\n+\n+    // All the tasks shall finish the second round of global sync within a short time slot.\n+    val times2 = times.map(_._2)\n+    assert(times2.max - times2.min <= 1000)\n+  }\n+\n+  test(\"throw exception on barrier() call timeout\") {\n+    val conf = new SparkConf()\n+      .set(\"spark.barrier.sync.timeout\", \"1\")\n+      .set(\"spark.test.noStageRetry\", \"true\")\n+      .setMaster(\"local-cluster[4, 1, 1024]\")\n+      .setAppName(\"test-cluster\")\n+    sc = new SparkContext(conf)\n+    val rdd = sc.makeRDD(1 to 10, 4)\n+    val rdd2 = rdd.barrier().mapPartitions { (it, context) =>\n+      // Task 3 shall sleep 2000ms to ensure barrier() call timeout\n+      if (context.taskAttemptId == 3) {\n+        Thread.sleep(2000)\n+      }\n+      context.barrier()\n+      it\n+    }\n+\n+    val error = intercept[SparkException] {\n+      rdd2.collect()\n+    }.getMessage\n+    assert(error.contains(\"The coordinator didn't get all barrier sync requests\"))\n+    assert(error.contains(\"within 1s\"))\n+  }\n+\n+  test(\"throw exception if barrier() call doesn't happen on every task\") {\n+    val conf = new SparkConf()\n+      .set(\"spark.barrier.sync.timeout\", \"1\")\n+      .set(\"spark.test.noStageRetry\", \"true\")\n+      .setMaster(\"local-cluster[4, 1, 1024]\")\n+      .setAppName(\"test-cluster\")\n+    sc = new SparkContext(conf)\n+    val rdd = sc.makeRDD(1 to 10, 4)\n+    val rdd2 = rdd.barrier().mapPartitions { (it, context) =>\n+      if (context.taskAttemptId != 0) {\n+        context.barrier()\n+      }\n+      it\n+    }\n+\n+    val error = intercept[SparkException] {\n+      rdd2.collect()\n+    }.getMessage\n+    assert(error.contains(\"The coordinator didn't get all barrier sync requests\"))\n+    assert(error.contains(\"within 1s\"))\n+  }\n+\n+  test(\"throw exception if the number of barrier() calls are not the same on every task\") {\n+    val conf = new SparkConf()\n+      .set(\"spark.barrier.sync.timeout\", \"1\")\n+      .set(\"spark.test.noStageRetry\", \"true\")\n+      .setMaster(\"local-cluster[4, 1, 1024]\")\n+      .setAppName(\"test-cluster\")\n+    sc = new SparkContext(conf)\n+    val rdd = sc.makeRDD(1 to 10, 4)\n+    val rdd2 = rdd.barrier().mapPartitions { (it, context) =>\n+      try {",
    "line": 129
  }],
  "prId": 21898
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "the error message is updated: `within 1 seconds`",
    "commit": "1f71e6583f9f9f270d07323f15c731717e13518d",
    "createdAt": "2018-08-05T02:58:34Z",
    "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.util.Random\n+\n+import org.apache.spark._\n+\n+class BarrierTaskContextSuite extends SparkFunSuite with LocalSparkContext {\n+\n+  test(\"global sync by barrier() call\") {\n+    val conf = new SparkConf()\n+      // Init local cluster here so each barrier task runs in a separated process, thus `barrier()`\n+      // call is actually useful.\n+      .setMaster(\"local-cluster[4, 1, 1024]\")\n+      .setAppName(\"test-cluster\")\n+    sc = new SparkContext(conf)\n+    val rdd = sc.makeRDD(1 to 10, 4)\n+    val rdd2 = rdd.barrier().mapPartitions { (it, context) =>\n+      // Sleep for a random time before global sync.\n+      Thread.sleep(Random.nextInt(1000))\n+      context.barrier()\n+      Seq(System.currentTimeMillis()).iterator\n+    }\n+\n+    val times = rdd2.collect()\n+    // All the tasks shall finish global sync within a short time slot.\n+    assert(times.max - times.min <= 1000)\n+  }\n+\n+  test(\"support multiple barrier() call within a single task\") {\n+    val conf = new SparkConf()\n+      .setMaster(\"local-cluster[4, 1, 1024]\")\n+      .setAppName(\"test-cluster\")\n+    sc = new SparkContext(conf)\n+    val rdd = sc.makeRDD(1 to 10, 4)\n+    val rdd2 = rdd.barrier().mapPartitions { (it, context) =>\n+      // Sleep for a random time before global sync.\n+      Thread.sleep(Random.nextInt(1000))\n+      context.barrier()\n+      val time1 = System.currentTimeMillis()\n+      // Sleep for a random time between two global syncs.\n+      Thread.sleep(Random.nextInt(1000))\n+      context.barrier()\n+      val time2 = System.currentTimeMillis()\n+      Seq((time1, time2)).iterator\n+    }\n+\n+    val times = rdd2.collect()\n+    // All the tasks shall finish the first round of global sync within a short time slot.\n+    val times1 = times.map(_._1)\n+    assert(times1.max - times1.min <= 1000)\n+\n+    // All the tasks shall finish the second round of global sync within a short time slot.\n+    val times2 = times.map(_._2)\n+    assert(times2.max - times2.min <= 1000)\n+  }\n+\n+  test(\"throw exception on barrier() call timeout\") {\n+    val conf = new SparkConf()\n+      .set(\"spark.barrier.sync.timeout\", \"1\")\n+      .set(\"spark.test.noStageRetry\", \"true\")\n+      .setMaster(\"local-cluster[4, 1, 1024]\")\n+      .setAppName(\"test-cluster\")\n+    sc = new SparkContext(conf)\n+    val rdd = sc.makeRDD(1 to 10, 4)\n+    val rdd2 = rdd.barrier().mapPartitions { (it, context) =>\n+      // Task 3 shall sleep 2000ms to ensure barrier() call timeout\n+      if (context.taskAttemptId == 3) {\n+        Thread.sleep(2000)\n+      }\n+      context.barrier()\n+      it\n+    }\n+\n+    val error = intercept[SparkException] {\n+      rdd2.collect()\n+    }.getMessage\n+    assert(error.contains(\"The coordinator didn't get all barrier sync requests\"))\n+    assert(error.contains(\"within 1s\"))\n+  }\n+\n+  test(\"throw exception if barrier() call doesn't happen on every task\") {\n+    val conf = new SparkConf()\n+      .set(\"spark.barrier.sync.timeout\", \"1\")\n+      .set(\"spark.test.noStageRetry\", \"true\")\n+      .setMaster(\"local-cluster[4, 1, 1024]\")\n+      .setAppName(\"test-cluster\")\n+    sc = new SparkContext(conf)\n+    val rdd = sc.makeRDD(1 to 10, 4)\n+    val rdd2 = rdd.barrier().mapPartitions { (it, context) =>\n+      if (context.taskAttemptId != 0) {\n+        context.barrier()\n+      }\n+      it\n+    }\n+\n+    val error = intercept[SparkException] {\n+      rdd2.collect()\n+    }.getMessage\n+    assert(error.contains(\"The coordinator didn't get all barrier sync requests\"))\n+    assert(error.contains(\"within 1s\"))\n+  }\n+\n+  test(\"throw exception if the number of barrier() calls are not the same on every task\") {\n+    val conf = new SparkConf()\n+      .set(\"spark.barrier.sync.timeout\", \"1\")\n+      .set(\"spark.test.noStageRetry\", \"true\")\n+      .setMaster(\"local-cluster[4, 1, 1024]\")\n+      .setAppName(\"test-cluster\")\n+    sc = new SparkContext(conf)\n+    val rdd = sc.makeRDD(1 to 10, 4)\n+    val rdd2 = rdd.barrier().mapPartitions { (it, context) =>\n+      try {\n+        if (context.taskAttemptId == 0) {\n+          // Due to some non-obvious reason, the code can trigger an Exception and skip the\n+          // following statements within the try ... catch block, including the first barrier()\n+          // call.\n+          throw new SparkException(\"test\")\n+        }\n+        context.barrier()\n+      } catch {\n+        case e: Exception => // Do nothing\n+      }\n+      context.barrier()\n+      it\n+    }\n+\n+    val error = intercept[SparkException] {\n+      rdd2.collect()\n+    }.getMessage\n+    assert(error.contains(\"The coordinator didn't get all barrier sync requests\"))\n+    assert(error.contains(\"within 1s\"))"
  }],
  "prId": 21898
}]