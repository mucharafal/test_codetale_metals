[{
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "thanks a lot for creating the test cases\n",
    "commit": "98a974756ecf6c7e29a955010b440f4b8f116692",
    "createdAt": "2016-10-16T05:33:30Z",
    "diffHunk": "@@ -109,6 +109,72 @@ class TaskSchedulerImplSuite extends SparkFunSuite with LocalSparkContext with B\n     assert(!failedTaskSet)\n   }\n \n+  test(\"Scheduler balance the assignment to the worker with more free cores\") {",
    "line": 4
  }],
  "prId": 15218
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "Nit: remove this empty line.\n",
    "commit": "98a974756ecf6c7e29a955010b440f4b8f116692",
    "createdAt": "2016-10-16T22:58:55Z",
    "diffHunk": "@@ -109,6 +109,72 @@ class TaskSchedulerImplSuite extends SparkFunSuite with LocalSparkContext with B\n     assert(!failedTaskSet)\n   }\n \n+  test(\"Scheduler balance the assignment to the worker with more free cores\") {\n+    val taskScheduler = setupScheduler((\"spark.task.assigner\", classOf[BalancedAssigner].getName))\n+    val workerOffers = IndexedSeq(new WorkerOffer(\"executor0\", \"host0\", 2),\n+      new WorkerOffer(\"executor1\", \"host1\", 4))\n+    val selectedExecutorIds = {\n+      val taskSet = FakeTask.createTaskSet(2)\n+      taskScheduler.submitTasks(taskSet)\n+      val taskDescriptions = taskScheduler.resourceOffers(workerOffers).flatten\n+      assert(2 === taskDescriptions.length)\n+      taskDescriptions.map(_.executorId)\n+    }\n+    val count = selectedExecutorIds.count(_ == workerOffers(1).executorId)\n+    assert(count == 2)\n+    assert(!failedTaskSet)\n+  }\n+\n+  test(\"Scheduler balance the assignment across workers with same free cores\") {\n+    val taskScheduler = setupScheduler((\"spark.task.assigner\", classOf[BalancedAssigner].getName))\n+    val workerOffers = IndexedSeq(new WorkerOffer(\"executor0\", \"host0\", 2),\n+      new WorkerOffer(\"executor1\", \"host1\", 2))\n+    val selectedExecutorIds = {\n+      val taskSet = FakeTask.createTaskSet(2)\n+      taskScheduler.submitTasks(taskSet)\n+      val taskDescriptions = taskScheduler.resourceOffers(workerOffers).flatten\n+      assert(2 === taskDescriptions.length)\n+      taskDescriptions.map(_.executorId)\n+    }\n+    val count = selectedExecutorIds.count(_ == workerOffers(1).executorId)\n+    assert(count == 1)\n+    assert(!failedTaskSet)\n+  }\n+\n+  test(\"Scheduler packs the assignment to workers with less free cores\") {\n+    val taskScheduler = setupScheduler((\"spark.task.assigner\", classOf[PackedAssigner].getName))\n+    val workerOffers = IndexedSeq(new WorkerOffer(\"executor0\", \"host0\", 2),\n+      new WorkerOffer(\"executor1\", \"host1\", 4))\n+    val selectedExecutorIds = {\n+      val taskSet = FakeTask.createTaskSet(2)\n+      taskScheduler.submitTasks(taskSet)\n+      val taskDescriptions = taskScheduler.resourceOffers(workerOffers).flatten\n+      assert(2 === taskDescriptions.length)\n+      taskDescriptions.map(_.executorId)\n+    }\n+    val count = selectedExecutorIds.count(_ == workerOffers(0).executorId)\n+    assert(count == 2)\n+    assert(!failedTaskSet)\n+  }\n+\n+  test(\"Scheduler keeps packing the assignment to the same worker\") {\n+    val taskScheduler = setupScheduler((\"spark.task.assigner\", classOf[PackedAssigner].getName))\n+    val workerOffers = IndexedSeq(new WorkerOffer(\"executor0\", \"host0\", 4),\n+      new WorkerOffer(\"executor1\", \"host1\", 4))\n+    val selectedExecutorIds = {\n+      val taskSet = FakeTask.createTaskSet(4)\n+      taskScheduler.submitTasks(taskSet)\n+      val taskDescriptions = taskScheduler.resourceOffers(workerOffers).flatten\n+      assert(4 === taskDescriptions.length)\n+      taskDescriptions.map(_.executorId)\n+    }\n+",
    "line": 63
  }],
  "prId": 15218
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "Nit: remove this empty line.\n",
    "commit": "98a974756ecf6c7e29a955010b440f4b8f116692",
    "createdAt": "2016-10-16T22:59:06Z",
    "diffHunk": "@@ -109,6 +109,72 @@ class TaskSchedulerImplSuite extends SparkFunSuite with LocalSparkContext with B\n     assert(!failedTaskSet)\n   }\n \n+  test(\"Scheduler balance the assignment to the worker with more free cores\") {\n+    val taskScheduler = setupScheduler((\"spark.task.assigner\", classOf[BalancedAssigner].getName))\n+    val workerOffers = IndexedSeq(new WorkerOffer(\"executor0\", \"host0\", 2),\n+      new WorkerOffer(\"executor1\", \"host1\", 4))\n+    val selectedExecutorIds = {\n+      val taskSet = FakeTask.createTaskSet(2)\n+      taskScheduler.submitTasks(taskSet)\n+      val taskDescriptions = taskScheduler.resourceOffers(workerOffers).flatten\n+      assert(2 === taskDescriptions.length)\n+      taskDescriptions.map(_.executorId)\n+    }\n+    val count = selectedExecutorIds.count(_ == workerOffers(1).executorId)\n+    assert(count == 2)\n+    assert(!failedTaskSet)\n+  }\n+\n+  test(\"Scheduler balance the assignment across workers with same free cores\") {\n+    val taskScheduler = setupScheduler((\"spark.task.assigner\", classOf[BalancedAssigner].getName))\n+    val workerOffers = IndexedSeq(new WorkerOffer(\"executor0\", \"host0\", 2),\n+      new WorkerOffer(\"executor1\", \"host1\", 2))\n+    val selectedExecutorIds = {\n+      val taskSet = FakeTask.createTaskSet(2)\n+      taskScheduler.submitTasks(taskSet)\n+      val taskDescriptions = taskScheduler.resourceOffers(workerOffers).flatten\n+      assert(2 === taskDescriptions.length)\n+      taskDescriptions.map(_.executorId)\n+    }\n+    val count = selectedExecutorIds.count(_ == workerOffers(1).executorId)\n+    assert(count == 1)\n+    assert(!failedTaskSet)\n+  }\n+\n+  test(\"Scheduler packs the assignment to workers with less free cores\") {\n+    val taskScheduler = setupScheduler((\"spark.task.assigner\", classOf[PackedAssigner].getName))\n+    val workerOffers = IndexedSeq(new WorkerOffer(\"executor0\", \"host0\", 2),\n+      new WorkerOffer(\"executor1\", \"host1\", 4))\n+    val selectedExecutorIds = {\n+      val taskSet = FakeTask.createTaskSet(2)\n+      taskScheduler.submitTasks(taskSet)\n+      val taskDescriptions = taskScheduler.resourceOffers(workerOffers).flatten\n+      assert(2 === taskDescriptions.length)\n+      taskDescriptions.map(_.executorId)\n+    }\n+    val count = selectedExecutorIds.count(_ == workerOffers(0).executorId)\n+    assert(count == 2)\n+    assert(!failedTaskSet)\n+  }\n+\n+  test(\"Scheduler keeps packing the assignment to the same worker\") {\n+    val taskScheduler = setupScheduler((\"spark.task.assigner\", classOf[PackedAssigner].getName))\n+    val workerOffers = IndexedSeq(new WorkerOffer(\"executor0\", \"host0\", 4),\n+      new WorkerOffer(\"executor1\", \"host1\", 4))\n+    val selectedExecutorIds = {\n+      val taskSet = FakeTask.createTaskSet(4)\n+      taskScheduler.submitTasks(taskSet)\n+      val taskDescriptions = taskScheduler.resourceOffers(workerOffers).flatten\n+      assert(4 === taskDescriptions.length)\n+      taskDescriptions.map(_.executorId)\n+    }\n+\n+    val count = selectedExecutorIds.count(_ == workerOffers(0).executorId)\n+    assert(count == 4)\n+    assert(!failedTaskSet)\n+  }\n+\n+",
    "line": 69
  }],
  "prId": 15218
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "Nit: remove this empty line.\n",
    "commit": "98a974756ecf6c7e29a955010b440f4b8f116692",
    "createdAt": "2016-10-16T22:59:11Z",
    "diffHunk": "@@ -408,4 +474,5 @@ class TaskSchedulerImplSuite extends SparkFunSuite with LocalSparkContext with B\n     assert(thirdTaskDescs.size === 0)\n     assert(taskScheduler.getExecutorsAliveOnHost(\"host1\") === Some(Set(\"executor1\", \"executor3\")))\n   }\n+",
    "line": 77
  }],
  "prId": 15218
}]