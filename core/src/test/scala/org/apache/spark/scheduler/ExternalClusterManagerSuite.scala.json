[{
  "comments": [{
    "author": {
      "login": "tejasapatil"
    },
    "body": "Rename to `TestExternalClusterManager` or `DummyExternalClusterManager`\n",
    "commit": "674742029e28278b8ff038590464045f714b5790",
    "createdAt": "2016-03-24T05:40:34Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import org.apache.spark.{LocalSparkContext, SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.executor.TaskMetrics\n+import org.apache.spark.scheduler.SchedulingMode.SchedulingMode\n+import org.apache.spark.storage.BlockManagerId\n+\n+class ExternalClusterManagerSuite extends SparkFunSuite with LocalSparkContext\n+{\n+  test(\"launch of backend and scheduler\") {\n+    val conf = new SparkConf().setMaster(\"myclusterManager\").\n+        setAppName(\"testcm\").set(\"spark.driver.allowMultipleContexts\", \"true\")\n+    sc = new SparkContext(conf)\n+    // check if the scheduler components are created\n+    assert(sc.schedulerBackend.isInstanceOf[FakeSchedulerBackend])\n+    assert(sc.taskScheduler.isInstanceOf[FakeScheduler])\n+  }\n+}\n+\n+class CheckExternalClusterManager extends ExternalClusterManager {"
  }, {
    "author": {
      "login": "hbhanawat"
    },
    "body": "Done. \n",
    "commit": "674742029e28278b8ff038590464045f714b5790",
    "createdAt": "2016-03-24T12:26:59Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import org.apache.spark.{LocalSparkContext, SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.executor.TaskMetrics\n+import org.apache.spark.scheduler.SchedulingMode.SchedulingMode\n+import org.apache.spark.storage.BlockManagerId\n+\n+class ExternalClusterManagerSuite extends SparkFunSuite with LocalSparkContext\n+{\n+  test(\"launch of backend and scheduler\") {\n+    val conf = new SparkConf().setMaster(\"myclusterManager\").\n+        setAppName(\"testcm\").set(\"spark.driver.allowMultipleContexts\", \"true\")\n+    sc = new SparkContext(conf)\n+    // check if the scheduler components are created\n+    assert(sc.schedulerBackend.isInstanceOf[FakeSchedulerBackend])\n+    assert(sc.taskScheduler.isInstanceOf[FakeScheduler])\n+  }\n+}\n+\n+class CheckExternalClusterManager extends ExternalClusterManager {"
  }],
  "prId": 11723
}, {
  "comments": [{
    "author": {
      "login": "tejasapatil"
    },
    "body": "To keep this consistent with the external CM declared above, you could rename this to `TestTaskScheduler` or `DummyTaskScheduler`\n",
    "commit": "674742029e28278b8ff038590464045f714b5790",
    "createdAt": "2016-03-24T05:43:30Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import org.apache.spark.{LocalSparkContext, SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.executor.TaskMetrics\n+import org.apache.spark.scheduler.SchedulingMode.SchedulingMode\n+import org.apache.spark.storage.BlockManagerId\n+\n+class ExternalClusterManagerSuite extends SparkFunSuite with LocalSparkContext\n+{\n+  test(\"launch of backend and scheduler\") {\n+    val conf = new SparkConf().setMaster(\"myclusterManager\").\n+        setAppName(\"testcm\").set(\"spark.driver.allowMultipleContexts\", \"true\")\n+    sc = new SparkContext(conf)\n+    // check if the scheduler components are created\n+    assert(sc.schedulerBackend.isInstanceOf[FakeSchedulerBackend])\n+    assert(sc.taskScheduler.isInstanceOf[FakeScheduler])\n+  }\n+}\n+\n+class CheckExternalClusterManager extends ExternalClusterManager {\n+\n+  def canCreate(masterURL: String): Boolean = masterURL == \"myclusterManager\"\n+\n+  def createTaskScheduler(sc: SparkContext): TaskScheduler = new FakeScheduler\n+\n+  def createSchedulerBackend(sc: SparkContext, scheduler: TaskScheduler): SchedulerBackend =\n+    new FakeSchedulerBackend()\n+\n+  def initialize(scheduler: TaskScheduler, backend: SchedulerBackend): Unit = {}\n+\n+}\n+\n+class FakeScheduler extends TaskScheduler {"
  }],
  "prId": 11723
}, {
  "comments": [{
    "author": {
      "login": "tejasapatil"
    },
    "body": "Sorry I missed this in the last review comments yesterday. I thought that `FakeSchedulerBackend` was in this same file and you could rename it but now I see that its from some other place. \n\nWhile reading, it feels odd to have Fake\\* and then Dummy\\* test classes. I am not sure about the whats followed in Spark codebase. Couple options:\n- rename Dummy\\* classes => Fake_. Move all the Fake_ classes to a common test utils file for the module.\n- Instead of re-using `FakeSchedulerBackend` from another place, create a `FakeSchedulerBackend` here.\n",
    "commit": "674742029e28278b8ff038590464045f714b5790",
    "createdAt": "2016-03-24T15:43:52Z",
    "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import org.apache.spark.{LocalSparkContext, SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.scheduler.SchedulingMode.SchedulingMode\n+import org.apache.spark.storage.BlockManagerId\n+\n+class ExternalClusterManagerSuite extends SparkFunSuite with LocalSparkContext\n+{\n+  test(\"launch of backend and scheduler\") {\n+    val conf = new SparkConf().setMaster(\"myclusterManager\").\n+        setAppName(\"testcm\").set(\"spark.driver.allowMultipleContexts\", \"true\")\n+    sc = new SparkContext(conf)\n+    // check if the scheduler components are created\n+    assert(sc.schedulerBackend.isInstanceOf[FakeSchedulerBackend])"
  }, {
    "author": {
      "login": "hbhanawat"
    },
    "body": "I too  missed it completely.  \nI think it wasn't a great idea in the first place to use FakeSchedulerBackend of some other class from maintenance perspective. I am going ahead with your option 2.  \n",
    "commit": "674742029e28278b8ff038590464045f714b5790",
    "createdAt": "2016-03-24T18:28:42Z",
    "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import org.apache.spark.{LocalSparkContext, SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.scheduler.SchedulingMode.SchedulingMode\n+import org.apache.spark.storage.BlockManagerId\n+\n+class ExternalClusterManagerSuite extends SparkFunSuite with LocalSparkContext\n+{\n+  test(\"launch of backend and scheduler\") {\n+    val conf = new SparkConf().setMaster(\"myclusterManager\").\n+        setAppName(\"testcm\").set(\"spark.driver.allowMultipleContexts\", \"true\")\n+    sc = new SparkContext(conf)\n+    // check if the scheduler components are created\n+    assert(sc.schedulerBackend.isInstanceOf[FakeSchedulerBackend])"
  }],
  "prId": 11723
}, {
  "comments": [{
    "author": {
      "login": "tejasapatil"
    },
    "body": "`private`\n",
    "commit": "674742029e28278b8ff038590464045f714b5790",
    "createdAt": "2016-03-24T15:44:01Z",
    "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import org.apache.spark.{LocalSparkContext, SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.scheduler.SchedulingMode.SchedulingMode\n+import org.apache.spark.storage.BlockManagerId\n+\n+class ExternalClusterManagerSuite extends SparkFunSuite with LocalSparkContext\n+{\n+  test(\"launch of backend and scheduler\") {\n+    val conf = new SparkConf().setMaster(\"myclusterManager\").\n+        setAppName(\"testcm\").set(\"spark.driver.allowMultipleContexts\", \"true\")\n+    sc = new SparkContext(conf)\n+    // check if the scheduler components are created\n+    assert(sc.schedulerBackend.isInstanceOf[FakeSchedulerBackend])\n+    assert(sc.taskScheduler.isInstanceOf[DummyTaskScheduler])\n+  }\n+}\n+\n+class DummyExternalClusterManager extends ExternalClusterManager {"
  }, {
    "author": {
      "login": "hbhanawat"
    },
    "body": "Done. \n",
    "commit": "674742029e28278b8ff038590464045f714b5790",
    "createdAt": "2016-03-24T18:28:52Z",
    "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import org.apache.spark.{LocalSparkContext, SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.scheduler.SchedulingMode.SchedulingMode\n+import org.apache.spark.storage.BlockManagerId\n+\n+class ExternalClusterManagerSuite extends SparkFunSuite with LocalSparkContext\n+{\n+  test(\"launch of backend and scheduler\") {\n+    val conf = new SparkConf().setMaster(\"myclusterManager\").\n+        setAppName(\"testcm\").set(\"spark.driver.allowMultipleContexts\", \"true\")\n+    sc = new SparkContext(conf)\n+    // check if the scheduler components are created\n+    assert(sc.schedulerBackend.isInstanceOf[FakeSchedulerBackend])\n+    assert(sc.taskScheduler.isInstanceOf[DummyTaskScheduler])\n+  }\n+}\n+\n+class DummyExternalClusterManager extends ExternalClusterManager {"
  }],
  "prId": 11723
}, {
  "comments": [{
    "author": {
      "login": "tejasapatil"
    },
    "body": "`private`\n",
    "commit": "674742029e28278b8ff038590464045f714b5790",
    "createdAt": "2016-03-24T15:44:08Z",
    "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import org.apache.spark.{LocalSparkContext, SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.scheduler.SchedulingMode.SchedulingMode\n+import org.apache.spark.storage.BlockManagerId\n+\n+class ExternalClusterManagerSuite extends SparkFunSuite with LocalSparkContext\n+{\n+  test(\"launch of backend and scheduler\") {\n+    val conf = new SparkConf().setMaster(\"myclusterManager\").\n+        setAppName(\"testcm\").set(\"spark.driver.allowMultipleContexts\", \"true\")\n+    sc = new SparkContext(conf)\n+    // check if the scheduler components are created\n+    assert(sc.schedulerBackend.isInstanceOf[FakeSchedulerBackend])\n+    assert(sc.taskScheduler.isInstanceOf[DummyTaskScheduler])\n+  }\n+}\n+\n+class DummyExternalClusterManager extends ExternalClusterManager {\n+\n+  def canCreate(masterURL: String): Boolean = masterURL == \"myclusterManager\"\n+\n+  def createTaskScheduler(sc: SparkContext,\n+                          masterURL: String): TaskScheduler = new DummyTaskScheduler\n+\n+  def createSchedulerBackend(sc: SparkContext,\n+                             masterURL: String,\n+                             scheduler: TaskScheduler): SchedulerBackend =\n+    new FakeSchedulerBackend()\n+\n+  def initialize(scheduler: TaskScheduler, backend: SchedulerBackend): Unit = {}\n+\n+}\n+\n+class DummyTaskScheduler extends TaskScheduler {"
  }, {
    "author": {
      "login": "hbhanawat"
    },
    "body": "Done\n",
    "commit": "674742029e28278b8ff038590464045f714b5790",
    "createdAt": "2016-03-24T18:28:59Z",
    "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import org.apache.spark.{LocalSparkContext, SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.scheduler.SchedulingMode.SchedulingMode\n+import org.apache.spark.storage.BlockManagerId\n+\n+class ExternalClusterManagerSuite extends SparkFunSuite with LocalSparkContext\n+{\n+  test(\"launch of backend and scheduler\") {\n+    val conf = new SparkConf().setMaster(\"myclusterManager\").\n+        setAppName(\"testcm\").set(\"spark.driver.allowMultipleContexts\", \"true\")\n+    sc = new SparkContext(conf)\n+    // check if the scheduler components are created\n+    assert(sc.schedulerBackend.isInstanceOf[FakeSchedulerBackend])\n+    assert(sc.taskScheduler.isInstanceOf[DummyTaskScheduler])\n+  }\n+}\n+\n+class DummyExternalClusterManager extends ExternalClusterManager {\n+\n+  def canCreate(masterURL: String): Boolean = masterURL == \"myclusterManager\"\n+\n+  def createTaskScheduler(sc: SparkContext,\n+                          masterURL: String): TaskScheduler = new DummyTaskScheduler\n+\n+  def createSchedulerBackend(sc: SparkContext,\n+                             masterURL: String,\n+                             scheduler: TaskScheduler): SchedulerBackend =\n+    new FakeSchedulerBackend()\n+\n+  def initialize(scheduler: TaskScheduler, backend: SchedulerBackend): Unit = {}\n+\n+}\n+\n+class DummyTaskScheduler extends TaskScheduler {"
  }],
  "prId": 11723
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "add a Unit here for return type to be consistent\n",
    "commit": "674742029e28278b8ff038590464045f714b5790",
    "createdAt": "2016-04-15T05:43:47Z",
    "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import org.apache.spark.{LocalSparkContext, SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.scheduler.SchedulingMode.SchedulingMode\n+import org.apache.spark.storage.BlockManagerId\n+\n+class ExternalClusterManagerSuite extends SparkFunSuite with LocalSparkContext\n+{\n+  test(\"launch of backend and scheduler\") {\n+    val conf = new SparkConf().setMaster(\"myclusterManager\").\n+        setAppName(\"testcm\").set(\"spark.driver.allowMultipleContexts\", \"true\")\n+    sc = new SparkContext(conf)\n+    // check if the scheduler components are created\n+    assert(sc.schedulerBackend.isInstanceOf[DummySchedulerBackend])\n+    assert(sc.taskScheduler.isInstanceOf[DummyTaskScheduler])\n+  }\n+}\n+\n+private class DummyExternalClusterManager extends ExternalClusterManager {\n+\n+  def canCreate(masterURL: String): Boolean = masterURL == \"myclusterManager\"\n+\n+  def createTaskScheduler(sc: SparkContext,\n+                          masterURL: String): TaskScheduler = new DummyTaskScheduler\n+\n+  def createSchedulerBackend(sc: SparkContext,\n+                             masterURL: String,\n+                             scheduler: TaskScheduler): SchedulerBackend =\n+    new DummySchedulerBackend()\n+\n+  def initialize(scheduler: TaskScheduler, backend: SchedulerBackend): Unit = {}\n+\n+}\n+\n+private class DummySchedulerBackend extends SchedulerBackend {\n+  def start() {}\n+  def stop() {}\n+  def reviveOffers() {}\n+  def defaultParallelism(): Int = 1\n+}\n+\n+private class DummyTaskScheduler extends TaskScheduler {\n+  override def rootPool: Pool = null\n+  override def schedulingMode: SchedulingMode = SchedulingMode.NONE\n+  override def start(): Unit = {}\n+  override def stop(): Unit = {}\n+  override def submitTasks(taskSet: TaskSet): Unit = {}\n+  override def cancelTasks(stageId: Int, interruptThread: Boolean) {}"
  }],
  "prId": 11723
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "can you fix the indent for the functions?\n",
    "commit": "674742029e28278b8ff038590464045f714b5790",
    "createdAt": "2016-04-15T05:43:58Z",
    "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import org.apache.spark.{LocalSparkContext, SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.scheduler.SchedulingMode.SchedulingMode\n+import org.apache.spark.storage.BlockManagerId\n+\n+class ExternalClusterManagerSuite extends SparkFunSuite with LocalSparkContext\n+{\n+  test(\"launch of backend and scheduler\") {\n+    val conf = new SparkConf().setMaster(\"myclusterManager\").\n+        setAppName(\"testcm\").set(\"spark.driver.allowMultipleContexts\", \"true\")\n+    sc = new SparkContext(conf)\n+    // check if the scheduler components are created\n+    assert(sc.schedulerBackend.isInstanceOf[DummySchedulerBackend])\n+    assert(sc.taskScheduler.isInstanceOf[DummyTaskScheduler])\n+  }\n+}\n+\n+private class DummyExternalClusterManager extends ExternalClusterManager {\n+\n+  def canCreate(masterURL: String): Boolean = masterURL == \"myclusterManager\"\n+\n+  def createTaskScheduler(sc: SparkContext,",
    "line": 40
  }],
  "prId": 11723
}]