[{
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "The failure mode caused by this bug was an infinite loop, hence this timeout. The fact that the loop does not break after some maximum number of retries is a distinct bug which should be fixed as part of a separate patch.\n",
    "commit": "edbbf6fae97f67c5d9a309019514745cf35a2cbe",
    "createdAt": "2015-09-01T01:49:16Z",
    "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import org.apache.hadoop.mapred.{FileOutputCommitter, TaskAttemptContext}\n+import org.scalatest.concurrent.Timeouts\n+import org.scalatest.time.{Span, Seconds}\n+\n+import org.apache.spark.{SparkConf, SparkContext, LocalSparkContext, SparkFunSuite, TaskContext}\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Integration tests for the OutputCommitCoordinator.\n+ *\n+ * See also: [[OutputCommitCoordinatorSuite]] for unit tests that use mocks.\n+ */\n+class OutputCommitCoordinatorIntegrationSuite\n+  extends SparkFunSuite\n+  with LocalSparkContext\n+  with Timeouts {\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    val conf = new SparkConf()\n+      .set(\"master\", \"local[2,4]\")\n+      .set(\"spark.speculation\", \"true\")\n+      .set(\"spark.hadoop.mapred.output.committer.class\",\n+        classOf[ThrowExceptionOnFirstAttemptOutputCommitter].getCanonicalName)\n+    sc = new SparkContext(\"local[2, 4]\", \"test\", conf)\n+  }\n+\n+  test(\"exception thrown in OutputCommitter.commitTask()\") {\n+    // Regression test for SPARK-10381\n+    failAfter(Span(60, Seconds)) {",
    "line": 49
  }, {
    "author": {
      "login": "squito"
    },
    "body": "Any insight on what the other bug is?  Is it worth filing another minimal jira for it so we don't lose track of it? (I didn't find anything ... sorry if I just missed it.)\n\nI'm slightly concerned (perhaps just irrationally paranoid) that maybe we'll fix _that_ bug, then regress on this bug, but this unit test won't catch it.  this test doesn't have any asserts, it just checks that the job completes and doesn't throw exceptions (not that adding assertions would necessarily help us avoid that scenario either).  a minimal jira would at least let us add a note to take another look at this while fixing that issue.\n",
    "commit": "edbbf6fae97f67c5d9a309019514745cf35a2cbe",
    "createdAt": "2015-09-02T16:08:01Z",
    "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import org.apache.hadoop.mapred.{FileOutputCommitter, TaskAttemptContext}\n+import org.scalatest.concurrent.Timeouts\n+import org.scalatest.time.{Span, Seconds}\n+\n+import org.apache.spark.{SparkConf, SparkContext, LocalSparkContext, SparkFunSuite, TaskContext}\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Integration tests for the OutputCommitCoordinator.\n+ *\n+ * See also: [[OutputCommitCoordinatorSuite]] for unit tests that use mocks.\n+ */\n+class OutputCommitCoordinatorIntegrationSuite\n+  extends SparkFunSuite\n+  with LocalSparkContext\n+  with Timeouts {\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    val conf = new SparkConf()\n+      .set(\"master\", \"local[2,4]\")\n+      .set(\"spark.speculation\", \"true\")\n+      .set(\"spark.hadoop.mapred.output.committer.class\",\n+        classOf[ThrowExceptionOnFirstAttemptOutputCommitter].getCanonicalName)\n+    sc = new SparkContext(\"local[2, 4]\", \"test\", conf)\n+  }\n+\n+  test(\"exception thrown in OutputCommitter.commitTask()\") {\n+    // Regression test for SPARK-10381\n+    failAfter(Span(60, Seconds)) {",
    "line": 49
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "The other bug is due to the fact that DAGScheduler treats failures due to CommitDenied separately from other failures: they don't count towards the typical count of maximum task failures which can trigger a job failure. The correct fix is to add an upper-bound on the number of times that a commit can be denied as a last-ditch safety net to avoid infinite loop behavior.\n",
    "commit": "edbbf6fae97f67c5d9a309019514745cf35a2cbe",
    "createdAt": "2015-09-02T18:43:24Z",
    "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import org.apache.hadoop.mapred.{FileOutputCommitter, TaskAttemptContext}\n+import org.scalatest.concurrent.Timeouts\n+import org.scalatest.time.{Span, Seconds}\n+\n+import org.apache.spark.{SparkConf, SparkContext, LocalSparkContext, SparkFunSuite, TaskContext}\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Integration tests for the OutputCommitCoordinator.\n+ *\n+ * See also: [[OutputCommitCoordinatorSuite]] for unit tests that use mocks.\n+ */\n+class OutputCommitCoordinatorIntegrationSuite\n+  extends SparkFunSuite\n+  with LocalSparkContext\n+  with Timeouts {\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    val conf = new SparkConf()\n+      .set(\"master\", \"local[2,4]\")\n+      .set(\"spark.speculation\", \"true\")\n+      .set(\"spark.hadoop.mapred.output.committer.class\",\n+        classOf[ThrowExceptionOnFirstAttemptOutputCommitter].getCanonicalName)\n+    sc = new SparkContext(\"local[2, 4]\", \"test\", conf)\n+  }\n+\n+  test(\"exception thrown in OutputCommitter.commitTask()\") {\n+    // Regression test for SPARK-10381\n+    failAfter(Span(60, Seconds)) {",
    "line": 49
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "I will follow a followup JIRA.\n",
    "commit": "edbbf6fae97f67c5d9a309019514745cf35a2cbe",
    "createdAt": "2015-09-02T18:43:42Z",
    "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import org.apache.hadoop.mapred.{FileOutputCommitter, TaskAttemptContext}\n+import org.scalatest.concurrent.Timeouts\n+import org.scalatest.time.{Span, Seconds}\n+\n+import org.apache.spark.{SparkConf, SparkContext, LocalSparkContext, SparkFunSuite, TaskContext}\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Integration tests for the OutputCommitCoordinator.\n+ *\n+ * See also: [[OutputCommitCoordinatorSuite]] for unit tests that use mocks.\n+ */\n+class OutputCommitCoordinatorIntegrationSuite\n+  extends SparkFunSuite\n+  with LocalSparkContext\n+  with Timeouts {\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    val conf = new SparkConf()\n+      .set(\"master\", \"local[2,4]\")\n+      .set(\"spark.speculation\", \"true\")\n+      .set(\"spark.hadoop.mapred.output.committer.class\",\n+        classOf[ThrowExceptionOnFirstAttemptOutputCommitter].getCanonicalName)\n+    sc = new SparkContext(\"local[2, 4]\", \"test\", conf)\n+  }\n+\n+  test(\"exception thrown in OutputCommitter.commitTask()\") {\n+    // Regression test for SPARK-10381\n+    failAfter(Span(60, Seconds)) {",
    "line": 49
  }, {
    "author": {
      "login": "squito"
    },
    "body": "makes sense, thanks Josh!\n",
    "commit": "edbbf6fae97f67c5d9a309019514745cf35a2cbe",
    "createdAt": "2015-09-02T18:49:43Z",
    "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import org.apache.hadoop.mapred.{FileOutputCommitter, TaskAttemptContext}\n+import org.scalatest.concurrent.Timeouts\n+import org.scalatest.time.{Span, Seconds}\n+\n+import org.apache.spark.{SparkConf, SparkContext, LocalSparkContext, SparkFunSuite, TaskContext}\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Integration tests for the OutputCommitCoordinator.\n+ *\n+ * See also: [[OutputCommitCoordinatorSuite]] for unit tests that use mocks.\n+ */\n+class OutputCommitCoordinatorIntegrationSuite\n+  extends SparkFunSuite\n+  with LocalSparkContext\n+  with Timeouts {\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    val conf = new SparkConf()\n+      .set(\"master\", \"local[2,4]\")\n+      .set(\"spark.speculation\", \"true\")\n+      .set(\"spark.hadoop.mapred.output.committer.class\",\n+        classOf[ThrowExceptionOnFirstAttemptOutputCommitter].getCanonicalName)\n+    sc = new SparkContext(\"local[2, 4]\", \"test\", conf)\n+  }\n+\n+  test(\"exception thrown in OutputCommitter.commitTask()\") {\n+    // Regression test for SPARK-10381\n+    failAfter(Span(60, Seconds)) {",
    "line": 49
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "Filed https://issues.apache.org/jira/browse/SPARK-10607 for this\n",
    "commit": "edbbf6fae97f67c5d9a309019514745cf35a2cbe",
    "createdAt": "2015-09-15T01:58:31Z",
    "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import org.apache.hadoop.mapred.{FileOutputCommitter, TaskAttemptContext}\n+import org.scalatest.concurrent.Timeouts\n+import org.scalatest.time.{Span, Seconds}\n+\n+import org.apache.spark.{SparkConf, SparkContext, LocalSparkContext, SparkFunSuite, TaskContext}\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Integration tests for the OutputCommitCoordinator.\n+ *\n+ * See also: [[OutputCommitCoordinatorSuite]] for unit tests that use mocks.\n+ */\n+class OutputCommitCoordinatorIntegrationSuite\n+  extends SparkFunSuite\n+  with LocalSparkContext\n+  with Timeouts {\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    val conf = new SparkConf()\n+      .set(\"master\", \"local[2,4]\")\n+      .set(\"spark.speculation\", \"true\")\n+      .set(\"spark.hadoop.mapred.output.committer.class\",\n+        classOf[ThrowExceptionOnFirstAttemptOutputCommitter].getCanonicalName)\n+    sc = new SparkContext(\"local[2, 4]\", \"test\", conf)\n+  }\n+\n+  test(\"exception thrown in OutputCommitter.commitTask()\") {\n+    // Regression test for SPARK-10381\n+    failAfter(Span(60, Seconds)) {",
    "line": 49
  }],
  "prId": 8544
}]