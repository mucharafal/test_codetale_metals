[{
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Use config constants where possible.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-06T23:03:02Z",
    "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.util.Try\n+\n+import org.scalatest.{BeforeAndAfterEach, Matchers}\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite, TestUtils}\n+import org.apache.spark.network.TransportContext\n+import org.apache.spark.network.netty.SparkTransportConf\n+import org.apache.spark.network.server.TransportServer\n+import org.apache.spark.network.shuffle.ExternalShuffleBlockHandler\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage._\n+\n+class RecoverCachedDataSuite extends SparkFunSuite with Matchers with BeforeAndAfterEach {\n+\n+  var shuffleService: TransportServer = _\n+  var conf: SparkConf = _\n+  var sc: SparkContext = _\n+\n+  private def makeBaseConf() = new SparkConf()\n+    .setAppName(\"test\")\n+    .setMaster(\"local-cluster[4, 1, 512]\")\n+    .set(\"spark.executor.memory\", \"512m\")\n+    .set(\"spark.shuffle.service.enabled\", \"true\")\n+    .set(\"spark.dynamicAllocation.enabled\", \"true\")"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "fixed",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-18T20:03:08Z",
    "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.util.Try\n+\n+import org.scalatest.{BeforeAndAfterEach, Matchers}\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite, TestUtils}\n+import org.apache.spark.network.TransportContext\n+import org.apache.spark.network.netty.SparkTransportConf\n+import org.apache.spark.network.server.TransportServer\n+import org.apache.spark.network.shuffle.ExternalShuffleBlockHandler\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage._\n+\n+class RecoverCachedDataSuite extends SparkFunSuite with Matchers with BeforeAndAfterEach {\n+\n+  var shuffleService: TransportServer = _\n+  var conf: SparkConf = _\n+  var sc: SparkContext = _\n+\n+  private def makeBaseConf() = new SparkConf()\n+    .setAppName(\"test\")\n+    .setMaster(\"local-cluster[4, 1, 512]\")\n+    .set(\"spark.executor.memory\", \"512m\")\n+    .set(\"spark.shuffle.service.enabled\", \"true\")\n+    .set(\"spark.dynamicAllocation.enabled\", \"true\")"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Same comment.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-06T23:04:01Z",
    "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.util.Try\n+\n+import org.scalatest.{BeforeAndAfterEach, Matchers}\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite, TestUtils}\n+import org.apache.spark.network.TransportContext\n+import org.apache.spark.network.netty.SparkTransportConf\n+import org.apache.spark.network.server.TransportServer\n+import org.apache.spark.network.shuffle.ExternalShuffleBlockHandler\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage._\n+\n+class RecoverCachedDataSuite extends SparkFunSuite with Matchers with BeforeAndAfterEach {\n+\n+  var shuffleService: TransportServer = _\n+  var conf: SparkConf = _\n+  var sc: SparkContext = _\n+\n+  private def makeBaseConf() = new SparkConf()\n+    .setAppName(\"test\")\n+    .setMaster(\"local-cluster[4, 1, 512]\")\n+    .set(\"spark.executor.memory\", \"512m\")\n+    .set(\"spark.shuffle.service.enabled\", \"true\")\n+    .set(\"spark.dynamicAllocation.enabled\", \"true\")\n+    .set(\"spark.dynamicAllocation.cacheRecovery.enabled\", \"true\")\n+    .set(\"spark.dynamicAllocation.cachedExecutorIdleTimeout\", \"1s\")\n+    .set(\"spark.dynamicAllocation.executorIdleTimeout\", \"1s\")\n+    .set(\"spark.executor.instances\", \"1\")\n+    .set(\"spark.dynamicAllocation.initialExecutors\", \"4\")\n+    .set(\"spark.dynamicAllocation.minExecutors\", \"3\")\n+\n+  override def beforeEach(): Unit = {\n+    conf = makeBaseConf()\n+    val transportConf = SparkTransportConf.fromSparkConf(conf, \"shuffle\", numUsableCores = 4)\n+    val rpcHandler = new ExternalShuffleBlockHandler(transportConf, null)\n+    val transportContext = new TransportContext(transportConf, rpcHandler)\n+    shuffleService = transportContext.createServer()\n+    conf.set(\"spark.shuffle.service.port\", shuffleService.getPort.toString)\n+  }\n+\n+  override def afterEach(): Unit = {\n+    sc.stop()\n+    conf = null\n+    shuffleService.close()\n+  }\n+\n+  type BInfo = Map[BlockId, Map[BlockManagerId, BlockStatus]]\n+  private def getLocations(sc: SparkContext, rdd: RDD[_]): BInfo = {\n+    import scala.collection.breakOut\n+    val blockIds: Array[BlockId] = rdd.partitions.map(p => RDDBlockId(rdd.id, p.index))\n+    blockIds.map { id =>\n+      id -> Try(sc.env.blockManager.master.getBlockStatus(id)).getOrElse(Map.empty)\n+    }(breakOut)\n+  }\n+\n+  test(\"cached data is replicated before dynamic de-allocation\") {\n+    sc = new SparkContext(conf)\n+    TestUtils.waitUntilExecutorsUp(sc, 4, 60000)\n+\n+    val rdd = sc.parallelize(1 to 1000, 4).map(_ * 4).cache()\n+    rdd.reduce(_ + _) shouldBe 2002000\n+    sc.getExecutorIds().size shouldBe 4\n+    getLocations(sc, rdd).forall{ case (id, map) => map.nonEmpty } shouldBe true\n+\n+    Thread.sleep(3000)"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "fixed",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-18T20:03:32Z",
    "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.util.Try\n+\n+import org.scalatest.{BeforeAndAfterEach, Matchers}\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite, TestUtils}\n+import org.apache.spark.network.TransportContext\n+import org.apache.spark.network.netty.SparkTransportConf\n+import org.apache.spark.network.server.TransportServer\n+import org.apache.spark.network.shuffle.ExternalShuffleBlockHandler\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage._\n+\n+class RecoverCachedDataSuite extends SparkFunSuite with Matchers with BeforeAndAfterEach {\n+\n+  var shuffleService: TransportServer = _\n+  var conf: SparkConf = _\n+  var sc: SparkContext = _\n+\n+  private def makeBaseConf() = new SparkConf()\n+    .setAppName(\"test\")\n+    .setMaster(\"local-cluster[4, 1, 512]\")\n+    .set(\"spark.executor.memory\", \"512m\")\n+    .set(\"spark.shuffle.service.enabled\", \"true\")\n+    .set(\"spark.dynamicAllocation.enabled\", \"true\")\n+    .set(\"spark.dynamicAllocation.cacheRecovery.enabled\", \"true\")\n+    .set(\"spark.dynamicAllocation.cachedExecutorIdleTimeout\", \"1s\")\n+    .set(\"spark.dynamicAllocation.executorIdleTimeout\", \"1s\")\n+    .set(\"spark.executor.instances\", \"1\")\n+    .set(\"spark.dynamicAllocation.initialExecutors\", \"4\")\n+    .set(\"spark.dynamicAllocation.minExecutors\", \"3\")\n+\n+  override def beforeEach(): Unit = {\n+    conf = makeBaseConf()\n+    val transportConf = SparkTransportConf.fromSparkConf(conf, \"shuffle\", numUsableCores = 4)\n+    val rpcHandler = new ExternalShuffleBlockHandler(transportConf, null)\n+    val transportContext = new TransportContext(transportConf, rpcHandler)\n+    shuffleService = transportContext.createServer()\n+    conf.set(\"spark.shuffle.service.port\", shuffleService.getPort.toString)\n+  }\n+\n+  override def afterEach(): Unit = {\n+    sc.stop()\n+    conf = null\n+    shuffleService.close()\n+  }\n+\n+  type BInfo = Map[BlockId, Map[BlockManagerId, BlockStatus]]\n+  private def getLocations(sc: SparkContext, rdd: RDD[_]): BInfo = {\n+    import scala.collection.breakOut\n+    val blockIds: Array[BlockId] = rdd.partitions.map(p => RDDBlockId(rdd.id, p.index))\n+    blockIds.map { id =>\n+      id -> Try(sc.env.blockManager.master.getBlockStatus(id)).getOrElse(Map.empty)\n+    }(breakOut)\n+  }\n+\n+  test(\"cached data is replicated before dynamic de-allocation\") {\n+    sc = new SparkContext(conf)\n+    TestUtils.waitUntilExecutorsUp(sc, 4, 60000)\n+\n+    val rdd = sc.parallelize(1 to 1000, 4).map(_ * 4).cache()\n+    rdd.reduce(_ + _) shouldBe 2002000\n+    sc.getExecutorIds().size shouldBe 4\n+    getLocations(sc, rdd).forall{ case (id, map) => map.nonEmpty } shouldBe true\n+\n+    Thread.sleep(3000)"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Same comment.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-06T23:04:13Z",
    "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.util.Try\n+\n+import org.scalatest.{BeforeAndAfterEach, Matchers}\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite, TestUtils}\n+import org.apache.spark.network.TransportContext\n+import org.apache.spark.network.netty.SparkTransportConf\n+import org.apache.spark.network.server.TransportServer\n+import org.apache.spark.network.shuffle.ExternalShuffleBlockHandler\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage._\n+\n+class RecoverCachedDataSuite extends SparkFunSuite with Matchers with BeforeAndAfterEach {\n+\n+  var shuffleService: TransportServer = _\n+  var conf: SparkConf = _\n+  var sc: SparkContext = _\n+\n+  private def makeBaseConf() = new SparkConf()\n+    .setAppName(\"test\")\n+    .setMaster(\"local-cluster[4, 1, 512]\")\n+    .set(\"spark.executor.memory\", \"512m\")\n+    .set(\"spark.shuffle.service.enabled\", \"true\")\n+    .set(\"spark.dynamicAllocation.enabled\", \"true\")\n+    .set(\"spark.dynamicAllocation.cacheRecovery.enabled\", \"true\")\n+    .set(\"spark.dynamicAllocation.cachedExecutorIdleTimeout\", \"1s\")\n+    .set(\"spark.dynamicAllocation.executorIdleTimeout\", \"1s\")\n+    .set(\"spark.executor.instances\", \"1\")\n+    .set(\"spark.dynamicAllocation.initialExecutors\", \"4\")\n+    .set(\"spark.dynamicAllocation.minExecutors\", \"3\")\n+\n+  override def beforeEach(): Unit = {\n+    conf = makeBaseConf()\n+    val transportConf = SparkTransportConf.fromSparkConf(conf, \"shuffle\", numUsableCores = 4)\n+    val rpcHandler = new ExternalShuffleBlockHandler(transportConf, null)\n+    val transportContext = new TransportContext(transportConf, rpcHandler)\n+    shuffleService = transportContext.createServer()\n+    conf.set(\"spark.shuffle.service.port\", shuffleService.getPort.toString)\n+  }\n+\n+  override def afterEach(): Unit = {\n+    sc.stop()\n+    conf = null\n+    shuffleService.close()\n+  }\n+\n+  type BInfo = Map[BlockId, Map[BlockManagerId, BlockStatus]]\n+  private def getLocations(sc: SparkContext, rdd: RDD[_]): BInfo = {\n+    import scala.collection.breakOut\n+    val blockIds: Array[BlockId] = rdd.partitions.map(p => RDDBlockId(rdd.id, p.index))\n+    blockIds.map { id =>\n+      id -> Try(sc.env.blockManager.master.getBlockStatus(id)).getOrElse(Map.empty)\n+    }(breakOut)\n+  }\n+\n+  test(\"cached data is replicated before dynamic de-allocation\") {\n+    sc = new SparkContext(conf)\n+    TestUtils.waitUntilExecutorsUp(sc, 4, 60000)\n+\n+    val rdd = sc.parallelize(1 to 1000, 4).map(_ * 4).cache()\n+    rdd.reduce(_ + _) shouldBe 2002000\n+    sc.getExecutorIds().size shouldBe 4\n+    getLocations(sc, rdd).forall{ case (id, map) => map.nonEmpty } shouldBe true\n+\n+    Thread.sleep(3000)\n+    sc.getExecutorIds().size shouldBe 3\n+    getLocations(sc, rdd).forall{ case (id, map) => map.nonEmpty } shouldBe true\n+  }\n+\n+  test(\"dont fail if a bunch of executors are shut down at once\") {\n+    conf.set(\"spark.dynamicAllocation.minExecutors\", \"1\")\n+    sc = new SparkContext(conf)\n+    TestUtils.waitUntilExecutorsUp(sc, 2, 60000)\n+\n+    val rdd = sc.parallelize(1 to 1000, 4).map(_ * 4).cache()\n+    rdd.reduce(_ + _) shouldBe 2002000\n+    sc.getExecutorIds().size shouldBe 4\n+    getLocations(sc, rdd).forall{ case (id, map) => map.nonEmpty } shouldBe true\n+\n+    Thread.sleep(3000)"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "fixed",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-18T20:03:56Z",
    "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.util.Try\n+\n+import org.scalatest.{BeforeAndAfterEach, Matchers}\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite, TestUtils}\n+import org.apache.spark.network.TransportContext\n+import org.apache.spark.network.netty.SparkTransportConf\n+import org.apache.spark.network.server.TransportServer\n+import org.apache.spark.network.shuffle.ExternalShuffleBlockHandler\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage._\n+\n+class RecoverCachedDataSuite extends SparkFunSuite with Matchers with BeforeAndAfterEach {\n+\n+  var shuffleService: TransportServer = _\n+  var conf: SparkConf = _\n+  var sc: SparkContext = _\n+\n+  private def makeBaseConf() = new SparkConf()\n+    .setAppName(\"test\")\n+    .setMaster(\"local-cluster[4, 1, 512]\")\n+    .set(\"spark.executor.memory\", \"512m\")\n+    .set(\"spark.shuffle.service.enabled\", \"true\")\n+    .set(\"spark.dynamicAllocation.enabled\", \"true\")\n+    .set(\"spark.dynamicAllocation.cacheRecovery.enabled\", \"true\")\n+    .set(\"spark.dynamicAllocation.cachedExecutorIdleTimeout\", \"1s\")\n+    .set(\"spark.dynamicAllocation.executorIdleTimeout\", \"1s\")\n+    .set(\"spark.executor.instances\", \"1\")\n+    .set(\"spark.dynamicAllocation.initialExecutors\", \"4\")\n+    .set(\"spark.dynamicAllocation.minExecutors\", \"3\")\n+\n+  override def beforeEach(): Unit = {\n+    conf = makeBaseConf()\n+    val transportConf = SparkTransportConf.fromSparkConf(conf, \"shuffle\", numUsableCores = 4)\n+    val rpcHandler = new ExternalShuffleBlockHandler(transportConf, null)\n+    val transportContext = new TransportContext(transportConf, rpcHandler)\n+    shuffleService = transportContext.createServer()\n+    conf.set(\"spark.shuffle.service.port\", shuffleService.getPort.toString)\n+  }\n+\n+  override def afterEach(): Unit = {\n+    sc.stop()\n+    conf = null\n+    shuffleService.close()\n+  }\n+\n+  type BInfo = Map[BlockId, Map[BlockManagerId, BlockStatus]]\n+  private def getLocations(sc: SparkContext, rdd: RDD[_]): BInfo = {\n+    import scala.collection.breakOut\n+    val blockIds: Array[BlockId] = rdd.partitions.map(p => RDDBlockId(rdd.id, p.index))\n+    blockIds.map { id =>\n+      id -> Try(sc.env.blockManager.master.getBlockStatus(id)).getOrElse(Map.empty)\n+    }(breakOut)\n+  }\n+\n+  test(\"cached data is replicated before dynamic de-allocation\") {\n+    sc = new SparkContext(conf)\n+    TestUtils.waitUntilExecutorsUp(sc, 4, 60000)\n+\n+    val rdd = sc.parallelize(1 to 1000, 4).map(_ * 4).cache()\n+    rdd.reduce(_ + _) shouldBe 2002000\n+    sc.getExecutorIds().size shouldBe 4\n+    getLocations(sc, rdd).forall{ case (id, map) => map.nonEmpty } shouldBe true\n+\n+    Thread.sleep(3000)\n+    sc.getExecutorIds().size shouldBe 3\n+    getLocations(sc, rdd).forall{ case (id, map) => map.nonEmpty } shouldBe true\n+  }\n+\n+  test(\"dont fail if a bunch of executors are shut down at once\") {\n+    conf.set(\"spark.dynamicAllocation.minExecutors\", \"1\")\n+    sc = new SparkContext(conf)\n+    TestUtils.waitUntilExecutorsUp(sc, 2, 60000)\n+\n+    val rdd = sc.parallelize(1 to 1000, 4).map(_ * 4).cache()\n+    rdd.reduce(_ + _) shouldBe 2002000\n+    sc.getExecutorIds().size shouldBe 4\n+    getLocations(sc, rdd).forall{ case (id, map) => map.nonEmpty } shouldBe true\n+\n+    Thread.sleep(3000)"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Same comment.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-06T23:04:30Z",
    "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.util.Try\n+\n+import org.scalatest.{BeforeAndAfterEach, Matchers}\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite, TestUtils}\n+import org.apache.spark.network.TransportContext\n+import org.apache.spark.network.netty.SparkTransportConf\n+import org.apache.spark.network.server.TransportServer\n+import org.apache.spark.network.shuffle.ExternalShuffleBlockHandler\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage._\n+\n+class RecoverCachedDataSuite extends SparkFunSuite with Matchers with BeforeAndAfterEach {\n+\n+  var shuffleService: TransportServer = _\n+  var conf: SparkConf = _\n+  var sc: SparkContext = _\n+\n+  private def makeBaseConf() = new SparkConf()\n+    .setAppName(\"test\")\n+    .setMaster(\"local-cluster[4, 1, 512]\")\n+    .set(\"spark.executor.memory\", \"512m\")\n+    .set(\"spark.shuffle.service.enabled\", \"true\")\n+    .set(\"spark.dynamicAllocation.enabled\", \"true\")\n+    .set(\"spark.dynamicAllocation.cacheRecovery.enabled\", \"true\")\n+    .set(\"spark.dynamicAllocation.cachedExecutorIdleTimeout\", \"1s\")\n+    .set(\"spark.dynamicAllocation.executorIdleTimeout\", \"1s\")\n+    .set(\"spark.executor.instances\", \"1\")\n+    .set(\"spark.dynamicAllocation.initialExecutors\", \"4\")\n+    .set(\"spark.dynamicAllocation.minExecutors\", \"3\")\n+\n+  override def beforeEach(): Unit = {\n+    conf = makeBaseConf()\n+    val transportConf = SparkTransportConf.fromSparkConf(conf, \"shuffle\", numUsableCores = 4)\n+    val rpcHandler = new ExternalShuffleBlockHandler(transportConf, null)\n+    val transportContext = new TransportContext(transportConf, rpcHandler)\n+    shuffleService = transportContext.createServer()\n+    conf.set(\"spark.shuffle.service.port\", shuffleService.getPort.toString)\n+  }\n+\n+  override def afterEach(): Unit = {\n+    sc.stop()\n+    conf = null\n+    shuffleService.close()\n+  }\n+\n+  type BInfo = Map[BlockId, Map[BlockManagerId, BlockStatus]]\n+  private def getLocations(sc: SparkContext, rdd: RDD[_]): BInfo = {\n+    import scala.collection.breakOut\n+    val blockIds: Array[BlockId] = rdd.partitions.map(p => RDDBlockId(rdd.id, p.index))\n+    blockIds.map { id =>\n+      id -> Try(sc.env.blockManager.master.getBlockStatus(id)).getOrElse(Map.empty)\n+    }(breakOut)\n+  }\n+\n+  test(\"cached data is replicated before dynamic de-allocation\") {\n+    sc = new SparkContext(conf)\n+    TestUtils.waitUntilExecutorsUp(sc, 4, 60000)\n+\n+    val rdd = sc.parallelize(1 to 1000, 4).map(_ * 4).cache()\n+    rdd.reduce(_ + _) shouldBe 2002000\n+    sc.getExecutorIds().size shouldBe 4\n+    getLocations(sc, rdd).forall{ case (id, map) => map.nonEmpty } shouldBe true\n+\n+    Thread.sleep(3000)\n+    sc.getExecutorIds().size shouldBe 3\n+    getLocations(sc, rdd).forall{ case (id, map) => map.nonEmpty } shouldBe true\n+  }\n+\n+  test(\"dont fail if a bunch of executors are shut down at once\") {\n+    conf.set(\"spark.dynamicAllocation.minExecutors\", \"1\")\n+    sc = new SparkContext(conf)\n+    TestUtils.waitUntilExecutorsUp(sc, 2, 60000)\n+\n+    val rdd = sc.parallelize(1 to 1000, 4).map(_ * 4).cache()\n+    rdd.reduce(_ + _) shouldBe 2002000\n+    sc.getExecutorIds().size shouldBe 4\n+    getLocations(sc, rdd).forall{ case (id, map) => map.nonEmpty } shouldBe true\n+\n+    Thread.sleep(3000)\n+    sc.getExecutorIds().size shouldBe 1\n+    getLocations(sc, rdd).forall{ case (id, map) => map.nonEmpty } shouldBe true\n+  }\n+\n+  test(\"Executors should not accept new work while replicating away data before deallocation\") {\n+    conf.set(\"spark.dynamicAllocation.minExecutors\", \"1\")\n+\n+    sc = new SparkContext(conf)\n+    TestUtils.waitUntilExecutorsUp(sc, 4, 60000)\n+\n+    val rdd = sc.parallelize(1 to 100000, 4).map(_ * 4L).cache() // cache on all 4 executors\n+    rdd.reduce(_ + _) shouldBe 20000200000L // realize the cache\n+\n+    Thread.sleep(1102) // sleep long enough to trigger deallocation"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "fixed",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-18T20:04:03Z",
    "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.util.Try\n+\n+import org.scalatest.{BeforeAndAfterEach, Matchers}\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite, TestUtils}\n+import org.apache.spark.network.TransportContext\n+import org.apache.spark.network.netty.SparkTransportConf\n+import org.apache.spark.network.server.TransportServer\n+import org.apache.spark.network.shuffle.ExternalShuffleBlockHandler\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage._\n+\n+class RecoverCachedDataSuite extends SparkFunSuite with Matchers with BeforeAndAfterEach {\n+\n+  var shuffleService: TransportServer = _\n+  var conf: SparkConf = _\n+  var sc: SparkContext = _\n+\n+  private def makeBaseConf() = new SparkConf()\n+    .setAppName(\"test\")\n+    .setMaster(\"local-cluster[4, 1, 512]\")\n+    .set(\"spark.executor.memory\", \"512m\")\n+    .set(\"spark.shuffle.service.enabled\", \"true\")\n+    .set(\"spark.dynamicAllocation.enabled\", \"true\")\n+    .set(\"spark.dynamicAllocation.cacheRecovery.enabled\", \"true\")\n+    .set(\"spark.dynamicAllocation.cachedExecutorIdleTimeout\", \"1s\")\n+    .set(\"spark.dynamicAllocation.executorIdleTimeout\", \"1s\")\n+    .set(\"spark.executor.instances\", \"1\")\n+    .set(\"spark.dynamicAllocation.initialExecutors\", \"4\")\n+    .set(\"spark.dynamicAllocation.minExecutors\", \"3\")\n+\n+  override def beforeEach(): Unit = {\n+    conf = makeBaseConf()\n+    val transportConf = SparkTransportConf.fromSparkConf(conf, \"shuffle\", numUsableCores = 4)\n+    val rpcHandler = new ExternalShuffleBlockHandler(transportConf, null)\n+    val transportContext = new TransportContext(transportConf, rpcHandler)\n+    shuffleService = transportContext.createServer()\n+    conf.set(\"spark.shuffle.service.port\", shuffleService.getPort.toString)\n+  }\n+\n+  override def afterEach(): Unit = {\n+    sc.stop()\n+    conf = null\n+    shuffleService.close()\n+  }\n+\n+  type BInfo = Map[BlockId, Map[BlockManagerId, BlockStatus]]\n+  private def getLocations(sc: SparkContext, rdd: RDD[_]): BInfo = {\n+    import scala.collection.breakOut\n+    val blockIds: Array[BlockId] = rdd.partitions.map(p => RDDBlockId(rdd.id, p.index))\n+    blockIds.map { id =>\n+      id -> Try(sc.env.blockManager.master.getBlockStatus(id)).getOrElse(Map.empty)\n+    }(breakOut)\n+  }\n+\n+  test(\"cached data is replicated before dynamic de-allocation\") {\n+    sc = new SparkContext(conf)\n+    TestUtils.waitUntilExecutorsUp(sc, 4, 60000)\n+\n+    val rdd = sc.parallelize(1 to 1000, 4).map(_ * 4).cache()\n+    rdd.reduce(_ + _) shouldBe 2002000\n+    sc.getExecutorIds().size shouldBe 4\n+    getLocations(sc, rdd).forall{ case (id, map) => map.nonEmpty } shouldBe true\n+\n+    Thread.sleep(3000)\n+    sc.getExecutorIds().size shouldBe 3\n+    getLocations(sc, rdd).forall{ case (id, map) => map.nonEmpty } shouldBe true\n+  }\n+\n+  test(\"dont fail if a bunch of executors are shut down at once\") {\n+    conf.set(\"spark.dynamicAllocation.minExecutors\", \"1\")\n+    sc = new SparkContext(conf)\n+    TestUtils.waitUntilExecutorsUp(sc, 2, 60000)\n+\n+    val rdd = sc.parallelize(1 to 1000, 4).map(_ * 4).cache()\n+    rdd.reduce(_ + _) shouldBe 2002000\n+    sc.getExecutorIds().size shouldBe 4\n+    getLocations(sc, rdd).forall{ case (id, map) => map.nonEmpty } shouldBe true\n+\n+    Thread.sleep(3000)\n+    sc.getExecutorIds().size shouldBe 1\n+    getLocations(sc, rdd).forall{ case (id, map) => map.nonEmpty } shouldBe true\n+  }\n+\n+  test(\"Executors should not accept new work while replicating away data before deallocation\") {\n+    conf.set(\"spark.dynamicAllocation.minExecutors\", \"1\")\n+\n+    sc = new SparkContext(conf)\n+    TestUtils.waitUntilExecutorsUp(sc, 4, 60000)\n+\n+    val rdd = sc.parallelize(1 to 100000, 4).map(_ * 4L).cache() // cache on all 4 executors\n+    rdd.reduce(_ + _) shouldBe 20000200000L // realize the cache\n+\n+    Thread.sleep(1102) // sleep long enough to trigger deallocation"
  }],
  "prId": 19041
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Also this should be `CacheRecoveryManagerSuite`.",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-12T22:45:35Z",
    "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.util.Try\n+\n+import org.scalatest.{BeforeAndAfterEach, Matchers}\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite, TestUtils}\n+import org.apache.spark.network.TransportContext\n+import org.apache.spark.network.netty.SparkTransportConf\n+import org.apache.spark.network.server.TransportServer\n+import org.apache.spark.network.shuffle.ExternalShuffleBlockHandler\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage._\n+\n+class RecoverCachedDataSuite extends SparkFunSuite with Matchers with BeforeAndAfterEach {"
  }, {
    "author": {
      "login": "brad-kaiser"
    },
    "body": "Renamed to CacheRecoveryIntegrationSuite because I already have CacheReconveryManagerSuite. I would combine them, but they don't go together cleanly because one uses BeforeAndAfterEach",
    "commit": "03ed8a2f597a4d42566693a63c1860bd5a68d314",
    "createdAt": "2017-12-18T20:06:45Z",
    "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler\n+\n+import scala.util.Try\n+\n+import org.scalatest.{BeforeAndAfterEach, Matchers}\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite, TestUtils}\n+import org.apache.spark.network.TransportContext\n+import org.apache.spark.network.netty.SparkTransportConf\n+import org.apache.spark.network.server.TransportServer\n+import org.apache.spark.network.shuffle.ExternalShuffleBlockHandler\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage._\n+\n+class RecoverCachedDataSuite extends SparkFunSuite with Matchers with BeforeAndAfterEach {"
  }],
  "prId": 19041
}]