[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "`thus partial of the inputs should not get processed.` ->\r\n`It's very unlikely that Spark can process 10000 elements between JobCancelled is posted and task is really killed.`",
    "commit": "2c7d1c749ce54eada6e483cb64b8d32cb01e445d",
    "createdAt": "2018-04-06T10:49:38Z",
    "diffHunk": "@@ -349,36 +350,38 @@ class JobCancellationSuite extends SparkFunSuite with Matchers with BeforeAndAft\n       }\n     })\n \n-    val f = sc.parallelize(1 to 1000).map { i => (i, i) }\n+    // Explicitly disable interrupt task thread on cancelling tasks, so the task thread can only be\n+    // interrupted by `InterruptibleIterator`.\n+    sc.setLocalProperty(SparkContext.SPARK_JOB_INTERRUPT_ON_CANCEL, \"false\")\n+\n+    val f = sc.parallelize(1 to numElements).map { i => (i, i) }\n       .repartitionAndSortWithinPartitions(new HashPartitioner(1))\n       .mapPartitions { iter =>\n         taskStartedSemaphore.release()\n         iter\n       }.foreachAsync { x =>\n-        if (x._1 >= 10) {\n-          // This block of code is partially executed. It will be blocked when x._1 >= 10 and the\n-          // next iteration will be cancelled if the source iterator is interruptible. Then in this\n-          // case, the maximum num of increment would be 10(|1...10|)\n-          taskCancelledSemaphore.acquire()\n-        }\n+        // Block this code from being executed, until the job get cancelled. In this case, if the\n+        // source iterator is interruptible, the max number of increment should be under\n+        // `numElements`.\n+        taskCancelledSemaphore.acquire()\n         executionOfInterruptibleCounter.getAndIncrement()\n     }\n \n     taskStartedSemaphore.acquire()\n     // Job is cancelled when:\n     // 1. task in reduce stage has been started, guaranteed by previous line.\n-    // 2. task in reduce stage is blocked after processing at most 10 records as\n-    //    taskCancelledSemaphore is not released until cancelTasks event is posted\n-    // After job being cancelled, task in reduce stage will be cancelled and no more iteration are\n-    // executed.\n+    // 2. task in reduce stage is blocked as taskCancelledSemaphore is not released until\n+    //    JobCancelled event is posted.\n+    // After job being cancelled, task in reduce stage will be cancelled asynchronously, thus\n+    // partial of the inputs should not get processed."
  }],
  "prId": 20993
}, {
  "comments": [{
    "author": {
      "login": "advancedxy"
    },
    "body": "I  re-checked the `killTask` code. I believe there's still possibility(very unlikely) that the reduce task processes all the input elements before task is really killed, then we cannot observe the reduce task being interruptible.\r\n\r\nOne way to reduce possibility would be increasing the num of input elements. So I believe we should add comments in `val numElements = 10000` to make laters know that we choose `10000` for a reason.",
    "commit": "2c7d1c749ce54eada6e483cb64b8d32cb01e445d",
    "createdAt": "2018-04-06T15:29:44Z",
    "diffHunk": "@@ -349,36 +350,39 @@ class JobCancellationSuite extends SparkFunSuite with Matchers with BeforeAndAft\n       }\n     })\n \n-    val f = sc.parallelize(1 to 1000).map { i => (i, i) }\n+    // Explicitly disable interrupt task thread on cancelling tasks, so the task thread can only be\n+    // interrupted by `InterruptibleIterator`.\n+    sc.setLocalProperty(SparkContext.SPARK_JOB_INTERRUPT_ON_CANCEL, \"false\")\n+\n+    val f = sc.parallelize(1 to numElements).map { i => (i, i) }\n       .repartitionAndSortWithinPartitions(new HashPartitioner(1))\n       .mapPartitions { iter =>\n         taskStartedSemaphore.release()\n         iter\n       }.foreachAsync { x =>\n-        if (x._1 >= 10) {\n-          // This block of code is partially executed. It will be blocked when x._1 >= 10 and the\n-          // next iteration will be cancelled if the source iterator is interruptible. Then in this\n-          // case, the maximum num of increment would be 10(|1...10|)\n-          taskCancelledSemaphore.acquire()\n-        }\n+        // Block this code from being executed, until the job get cancelled. In this case, if the\n+        // source iterator is interruptible, the max number of increment should be under\n+        // `numElements`.\n+        taskCancelledSemaphore.acquire()\n         executionOfInterruptibleCounter.getAndIncrement()\n     }\n \n     taskStartedSemaphore.acquire()\n     // Job is cancelled when:\n     // 1. task in reduce stage has been started, guaranteed by previous line.\n-    // 2. task in reduce stage is blocked after processing at most 10 records as\n-    //    taskCancelledSemaphore is not released until cancelTasks event is posted\n-    // After job being cancelled, task in reduce stage will be cancelled and no more iteration are\n-    // executed.\n+    // 2. task in reduce stage is blocked as taskCancelledSemaphore is not released until\n+    //    JobCancelled event is posted.\n+    // After job being cancelled, task in reduce stage will be cancelled asynchronously, thus\n+    // partial of the inputs should not get processed (It's very unlikely that Spark can process\n+    // 10000 elements between JobCancelled is posted and task is really killed).",
    "line": 56
  }],
  "prId": 20993
}]