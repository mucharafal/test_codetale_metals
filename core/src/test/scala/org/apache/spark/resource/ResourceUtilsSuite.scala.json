[{
  "comments": [{
    "author": {
      "login": "Ngone51"
    },
    "body": "\"discovery script should be discoveryScriptGPU\"",
    "commit": "f3915856344f7b260397b5582d6738accf319619",
    "createdAt": "2019-06-14T15:41:54Z",
    "diffHunk": "@@ -0,0 +1,230 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.resource\n+\n+import java.io.File\n+import java.nio.file.{Files => JavaFiles}\n+\n+import org.apache.spark.{LocalSparkContext, SparkConf, SparkException, SparkFunSuite}\n+import org.apache.spark.TestUtils._\n+import org.apache.spark.internal.config._\n+import org.apache.spark.resource.ResourceUtils._\n+import org.apache.spark.resource.TestResourceIDs._\n+import org.apache.spark.util.Utils\n+\n+class ResourceUtilsSuite extends SparkFunSuite\n+    with LocalSparkContext {\n+\n+  test(\"ResourceID\") {\n+    val componentName = \"spark.test\"\n+    val resourceName = \"p100\"\n+    val id = ResourceID(componentName, resourceName)\n+    val confPrefix = s\"$componentName.resource.$resourceName.\"\n+    assert(id.confPrefix === confPrefix)\n+    assert(id.amountConf === s\"${confPrefix}amount\")\n+    assert(id.discoveryScriptConf === s\"${confPrefix}discoveryScript\")\n+    assert(id.vendorConf === s\"${confPrefix}vendor\")\n+  }\n+\n+  test(\"Resource discoverer no addresses errors\") {\n+    val conf = new SparkConf\n+    assume(!(Utils.isWindows))\n+    withTempDir { dir =>\n+      val scriptPath = createTempScriptWithExpectedOutput(dir, \"gpuDiscoverScript\",\n+        \"\"\"{\"name\": \"gpu\"}\"\"\")\n+      conf.set(EXECUTOR_GPU_ID.amountConf, \"2\")\n+      conf.set(EXECUTOR_GPU_ID.discoveryScriptConf, scriptPath)\n+\n+      val error = intercept[IllegalArgumentException] {\n+        getOrDiscoverAllResources(conf, SPARK_EXECUTOR_PREFIX, None)\n+      }.getMessage()\n+      assert(error.contains(\"Resource: gpu, with \" +\n+        \"addresses:  is less than what the user requested: 2\"))\n+    }\n+  }\n+\n+  test(\"Resource discoverer multiple resource types\") {\n+    val conf = new SparkConf\n+    assume(!(Utils.isWindows))\n+    withTempDir { dir =>\n+      val gpuDiscovery = createTempScriptWithExpectedOutput(dir, \"gpuDiscoveryScript\",\n+        \"\"\"{\"name\": \"gpu\", \"addresses\": [\"0\", \"1\"]}\"\"\")\n+      conf.set(EXECUTOR_GPU_ID.amountConf, \"2\")\n+      conf.set(EXECUTOR_GPU_ID.discoveryScriptConf, gpuDiscovery)\n+\n+      val fpgaDiscovery = createTempScriptWithExpectedOutput(dir, \"fpgDiscoverScript\",\n+        \"\"\"{\"name\": \"fpga\", \"addresses\": [\"f1\", \"f2\", \"f3\"]}\"\"\")\n+      conf.set(EXECUTOR_FPGA_ID.amountConf, \"2\")\n+      conf.set(EXECUTOR_FPGA_ID.discoveryScriptConf, fpgaDiscovery)\n+\n+      val resources = getOrDiscoverAllResources(conf, SPARK_EXECUTOR_PREFIX, None)\n+      assert(resources.size === 2)\n+      val gpuValue = resources.get(GPU)\n+      assert(gpuValue.nonEmpty, \"Should have a gpu entry\")\n+      assert(gpuValue.get.name == \"gpu\", \"name should be gpu\")\n+      assert(gpuValue.get.addresses.size == 2, \"Should have 2 indexes\")\n+      assert(gpuValue.get.addresses.deep == Array(\"0\", \"1\").deep, \"should have 0,1 entries\")\n+\n+      val fpgaValue = resources.get(FPGA)\n+      assert(fpgaValue.nonEmpty, \"Should have a gpu entry\")\n+      assert(fpgaValue.get.name == \"fpga\", \"name should be fpga\")\n+      assert(fpgaValue.get.addresses.size == 3, \"Should have 3 indexes\")\n+      assert(fpgaValue.get.addresses.deep == Array(\"f1\", \"f2\", \"f3\").deep,\n+        \"should have f1,f2,f3 entries\")\n+    }\n+  }\n+\n+  test(\"list resource ids\") {\n+    val conf = new SparkConf\n+    conf.set(DRIVER_GPU_ID.amountConf, \"2\")\n+    var resources = listResourceIds(conf, SPARK_DRIVER_PREFIX)\n+    assert(resources.size === 1, \"should only have GPU for resource\")\n+    assert(resources(0).resourceName == GPU, \"name should be gpu\")\n+\n+    conf.set(DRIVER_FPGA_ID.amountConf, \"2\")\n+    val resourcesMap = listResourceIds(conf, SPARK_DRIVER_PREFIX)\n+      .map{ rId => (rId.resourceName, 1)}.toMap\n+    assert(resourcesMap.size === 2, \"should only have GPU for resource\")\n+    assert(resourcesMap.get(GPU).nonEmpty, \"should have GPU\")\n+    assert(resourcesMap.get(FPGA).nonEmpty, \"should have FPGA\")\n+  }\n+\n+  test(\"parse resource request\") {\n+    val conf = new SparkConf\n+    conf.set(DRIVER_GPU_ID.amountConf, \"2\")\n+    var request = parseResourceRequest(conf, DRIVER_GPU_ID)\n+    assert(request.id.resourceName === GPU, \"should only have GPU for resource\")\n+    assert(request.amount === 2, \"GPU count should be 2\")\n+    assert(request.discoveryScript === None, \"discovery script should be empty\")\n+    assert(request.vendor === None, \"vendor should be empty\")\n+\n+    val vendor = \"nvidia.com\"\n+    val discoveryScript = \"discoveryScriptGPU\"\n+    conf.set(DRIVER_GPU_ID.discoveryScriptConf, discoveryScript)\n+    conf.set(DRIVER_GPU_ID.vendorConf, vendor)\n+    request = parseResourceRequest(conf, DRIVER_GPU_ID)\n+    assert(request.id.resourceName === GPU, \"should only have GPU for resource\")\n+    assert(request.amount === 2, \"GPU count should be 2\")\n+    assert(request.discoveryScript.get === discoveryScript, \"discovery script should be empty\")"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "changed to \"should get discovery script\". putting `discoveryScriptGPU` in results duplicate info.",
    "commit": "f3915856344f7b260397b5582d6738accf319619",
    "createdAt": "2019-06-18T15:17:07Z",
    "diffHunk": "@@ -0,0 +1,230 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.resource\n+\n+import java.io.File\n+import java.nio.file.{Files => JavaFiles}\n+\n+import org.apache.spark.{LocalSparkContext, SparkConf, SparkException, SparkFunSuite}\n+import org.apache.spark.TestUtils._\n+import org.apache.spark.internal.config._\n+import org.apache.spark.resource.ResourceUtils._\n+import org.apache.spark.resource.TestResourceIDs._\n+import org.apache.spark.util.Utils\n+\n+class ResourceUtilsSuite extends SparkFunSuite\n+    with LocalSparkContext {\n+\n+  test(\"ResourceID\") {\n+    val componentName = \"spark.test\"\n+    val resourceName = \"p100\"\n+    val id = ResourceID(componentName, resourceName)\n+    val confPrefix = s\"$componentName.resource.$resourceName.\"\n+    assert(id.confPrefix === confPrefix)\n+    assert(id.amountConf === s\"${confPrefix}amount\")\n+    assert(id.discoveryScriptConf === s\"${confPrefix}discoveryScript\")\n+    assert(id.vendorConf === s\"${confPrefix}vendor\")\n+  }\n+\n+  test(\"Resource discoverer no addresses errors\") {\n+    val conf = new SparkConf\n+    assume(!(Utils.isWindows))\n+    withTempDir { dir =>\n+      val scriptPath = createTempScriptWithExpectedOutput(dir, \"gpuDiscoverScript\",\n+        \"\"\"{\"name\": \"gpu\"}\"\"\")\n+      conf.set(EXECUTOR_GPU_ID.amountConf, \"2\")\n+      conf.set(EXECUTOR_GPU_ID.discoveryScriptConf, scriptPath)\n+\n+      val error = intercept[IllegalArgumentException] {\n+        getOrDiscoverAllResources(conf, SPARK_EXECUTOR_PREFIX, None)\n+      }.getMessage()\n+      assert(error.contains(\"Resource: gpu, with \" +\n+        \"addresses:  is less than what the user requested: 2\"))\n+    }\n+  }\n+\n+  test(\"Resource discoverer multiple resource types\") {\n+    val conf = new SparkConf\n+    assume(!(Utils.isWindows))\n+    withTempDir { dir =>\n+      val gpuDiscovery = createTempScriptWithExpectedOutput(dir, \"gpuDiscoveryScript\",\n+        \"\"\"{\"name\": \"gpu\", \"addresses\": [\"0\", \"1\"]}\"\"\")\n+      conf.set(EXECUTOR_GPU_ID.amountConf, \"2\")\n+      conf.set(EXECUTOR_GPU_ID.discoveryScriptConf, gpuDiscovery)\n+\n+      val fpgaDiscovery = createTempScriptWithExpectedOutput(dir, \"fpgDiscoverScript\",\n+        \"\"\"{\"name\": \"fpga\", \"addresses\": [\"f1\", \"f2\", \"f3\"]}\"\"\")\n+      conf.set(EXECUTOR_FPGA_ID.amountConf, \"2\")\n+      conf.set(EXECUTOR_FPGA_ID.discoveryScriptConf, fpgaDiscovery)\n+\n+      val resources = getOrDiscoverAllResources(conf, SPARK_EXECUTOR_PREFIX, None)\n+      assert(resources.size === 2)\n+      val gpuValue = resources.get(GPU)\n+      assert(gpuValue.nonEmpty, \"Should have a gpu entry\")\n+      assert(gpuValue.get.name == \"gpu\", \"name should be gpu\")\n+      assert(gpuValue.get.addresses.size == 2, \"Should have 2 indexes\")\n+      assert(gpuValue.get.addresses.deep == Array(\"0\", \"1\").deep, \"should have 0,1 entries\")\n+\n+      val fpgaValue = resources.get(FPGA)\n+      assert(fpgaValue.nonEmpty, \"Should have a gpu entry\")\n+      assert(fpgaValue.get.name == \"fpga\", \"name should be fpga\")\n+      assert(fpgaValue.get.addresses.size == 3, \"Should have 3 indexes\")\n+      assert(fpgaValue.get.addresses.deep == Array(\"f1\", \"f2\", \"f3\").deep,\n+        \"should have f1,f2,f3 entries\")\n+    }\n+  }\n+\n+  test(\"list resource ids\") {\n+    val conf = new SparkConf\n+    conf.set(DRIVER_GPU_ID.amountConf, \"2\")\n+    var resources = listResourceIds(conf, SPARK_DRIVER_PREFIX)\n+    assert(resources.size === 1, \"should only have GPU for resource\")\n+    assert(resources(0).resourceName == GPU, \"name should be gpu\")\n+\n+    conf.set(DRIVER_FPGA_ID.amountConf, \"2\")\n+    val resourcesMap = listResourceIds(conf, SPARK_DRIVER_PREFIX)\n+      .map{ rId => (rId.resourceName, 1)}.toMap\n+    assert(resourcesMap.size === 2, \"should only have GPU for resource\")\n+    assert(resourcesMap.get(GPU).nonEmpty, \"should have GPU\")\n+    assert(resourcesMap.get(FPGA).nonEmpty, \"should have FPGA\")\n+  }\n+\n+  test(\"parse resource request\") {\n+    val conf = new SparkConf\n+    conf.set(DRIVER_GPU_ID.amountConf, \"2\")\n+    var request = parseResourceRequest(conf, DRIVER_GPU_ID)\n+    assert(request.id.resourceName === GPU, \"should only have GPU for resource\")\n+    assert(request.amount === 2, \"GPU count should be 2\")\n+    assert(request.discoveryScript === None, \"discovery script should be empty\")\n+    assert(request.vendor === None, \"vendor should be empty\")\n+\n+    val vendor = \"nvidia.com\"\n+    val discoveryScript = \"discoveryScriptGPU\"\n+    conf.set(DRIVER_GPU_ID.discoveryScriptConf, discoveryScript)\n+    conf.set(DRIVER_GPU_ID.vendorConf, vendor)\n+    request = parseResourceRequest(conf, DRIVER_GPU_ID)\n+    assert(request.id.resourceName === GPU, \"should only have GPU for resource\")\n+    assert(request.amount === 2, \"GPU count should be 2\")\n+    assert(request.discoveryScript.get === discoveryScript, \"discovery script should be empty\")"
  }],
  "prId": 24856
}, {
  "comments": [{
    "author": {
      "login": "Ngone51"
    },
    "body": "\"vendor should be nvidia.com\"",
    "commit": "f3915856344f7b260397b5582d6738accf319619",
    "createdAt": "2019-06-14T15:42:21Z",
    "diffHunk": "@@ -0,0 +1,230 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.resource\n+\n+import java.io.File\n+import java.nio.file.{Files => JavaFiles}\n+\n+import org.apache.spark.{LocalSparkContext, SparkConf, SparkException, SparkFunSuite}\n+import org.apache.spark.TestUtils._\n+import org.apache.spark.internal.config._\n+import org.apache.spark.resource.ResourceUtils._\n+import org.apache.spark.resource.TestResourceIDs._\n+import org.apache.spark.util.Utils\n+\n+class ResourceUtilsSuite extends SparkFunSuite\n+    with LocalSparkContext {\n+\n+  test(\"ResourceID\") {\n+    val componentName = \"spark.test\"\n+    val resourceName = \"p100\"\n+    val id = ResourceID(componentName, resourceName)\n+    val confPrefix = s\"$componentName.resource.$resourceName.\"\n+    assert(id.confPrefix === confPrefix)\n+    assert(id.amountConf === s\"${confPrefix}amount\")\n+    assert(id.discoveryScriptConf === s\"${confPrefix}discoveryScript\")\n+    assert(id.vendorConf === s\"${confPrefix}vendor\")\n+  }\n+\n+  test(\"Resource discoverer no addresses errors\") {\n+    val conf = new SparkConf\n+    assume(!(Utils.isWindows))\n+    withTempDir { dir =>\n+      val scriptPath = createTempScriptWithExpectedOutput(dir, \"gpuDiscoverScript\",\n+        \"\"\"{\"name\": \"gpu\"}\"\"\")\n+      conf.set(EXECUTOR_GPU_ID.amountConf, \"2\")\n+      conf.set(EXECUTOR_GPU_ID.discoveryScriptConf, scriptPath)\n+\n+      val error = intercept[IllegalArgumentException] {\n+        getOrDiscoverAllResources(conf, SPARK_EXECUTOR_PREFIX, None)\n+      }.getMessage()\n+      assert(error.contains(\"Resource: gpu, with \" +\n+        \"addresses:  is less than what the user requested: 2\"))\n+    }\n+  }\n+\n+  test(\"Resource discoverer multiple resource types\") {\n+    val conf = new SparkConf\n+    assume(!(Utils.isWindows))\n+    withTempDir { dir =>\n+      val gpuDiscovery = createTempScriptWithExpectedOutput(dir, \"gpuDiscoveryScript\",\n+        \"\"\"{\"name\": \"gpu\", \"addresses\": [\"0\", \"1\"]}\"\"\")\n+      conf.set(EXECUTOR_GPU_ID.amountConf, \"2\")\n+      conf.set(EXECUTOR_GPU_ID.discoveryScriptConf, gpuDiscovery)\n+\n+      val fpgaDiscovery = createTempScriptWithExpectedOutput(dir, \"fpgDiscoverScript\",\n+        \"\"\"{\"name\": \"fpga\", \"addresses\": [\"f1\", \"f2\", \"f3\"]}\"\"\")\n+      conf.set(EXECUTOR_FPGA_ID.amountConf, \"2\")\n+      conf.set(EXECUTOR_FPGA_ID.discoveryScriptConf, fpgaDiscovery)\n+\n+      val resources = getOrDiscoverAllResources(conf, SPARK_EXECUTOR_PREFIX, None)\n+      assert(resources.size === 2)\n+      val gpuValue = resources.get(GPU)\n+      assert(gpuValue.nonEmpty, \"Should have a gpu entry\")\n+      assert(gpuValue.get.name == \"gpu\", \"name should be gpu\")\n+      assert(gpuValue.get.addresses.size == 2, \"Should have 2 indexes\")\n+      assert(gpuValue.get.addresses.deep == Array(\"0\", \"1\").deep, \"should have 0,1 entries\")\n+\n+      val fpgaValue = resources.get(FPGA)\n+      assert(fpgaValue.nonEmpty, \"Should have a gpu entry\")\n+      assert(fpgaValue.get.name == \"fpga\", \"name should be fpga\")\n+      assert(fpgaValue.get.addresses.size == 3, \"Should have 3 indexes\")\n+      assert(fpgaValue.get.addresses.deep == Array(\"f1\", \"f2\", \"f3\").deep,\n+        \"should have f1,f2,f3 entries\")\n+    }\n+  }\n+\n+  test(\"list resource ids\") {\n+    val conf = new SparkConf\n+    conf.set(DRIVER_GPU_ID.amountConf, \"2\")\n+    var resources = listResourceIds(conf, SPARK_DRIVER_PREFIX)\n+    assert(resources.size === 1, \"should only have GPU for resource\")\n+    assert(resources(0).resourceName == GPU, \"name should be gpu\")\n+\n+    conf.set(DRIVER_FPGA_ID.amountConf, \"2\")\n+    val resourcesMap = listResourceIds(conf, SPARK_DRIVER_PREFIX)\n+      .map{ rId => (rId.resourceName, 1)}.toMap\n+    assert(resourcesMap.size === 2, \"should only have GPU for resource\")\n+    assert(resourcesMap.get(GPU).nonEmpty, \"should have GPU\")\n+    assert(resourcesMap.get(FPGA).nonEmpty, \"should have FPGA\")\n+  }\n+\n+  test(\"parse resource request\") {\n+    val conf = new SparkConf\n+    conf.set(DRIVER_GPU_ID.amountConf, \"2\")\n+    var request = parseResourceRequest(conf, DRIVER_GPU_ID)\n+    assert(request.id.resourceName === GPU, \"should only have GPU for resource\")\n+    assert(request.amount === 2, \"GPU count should be 2\")\n+    assert(request.discoveryScript === None, \"discovery script should be empty\")\n+    assert(request.vendor === None, \"vendor should be empty\")\n+\n+    val vendor = \"nvidia.com\"\n+    val discoveryScript = \"discoveryScriptGPU\"\n+    conf.set(DRIVER_GPU_ID.discoveryScriptConf, discoveryScript)\n+    conf.set(DRIVER_GPU_ID.vendorConf, vendor)\n+    request = parseResourceRequest(conf, DRIVER_GPU_ID)\n+    assert(request.id.resourceName === GPU, \"should only have GPU for resource\")\n+    assert(request.amount === 2, \"GPU count should be 2\")\n+    assert(request.discoveryScript.get === discoveryScript, \"discovery script should be empty\")\n+    assert(request.vendor.get === vendor, \"vendor should be empty\")"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "updated.",
    "commit": "f3915856344f7b260397b5582d6738accf319619",
    "createdAt": "2019-06-18T15:17:27Z",
    "diffHunk": "@@ -0,0 +1,230 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.resource\n+\n+import java.io.File\n+import java.nio.file.{Files => JavaFiles}\n+\n+import org.apache.spark.{LocalSparkContext, SparkConf, SparkException, SparkFunSuite}\n+import org.apache.spark.TestUtils._\n+import org.apache.spark.internal.config._\n+import org.apache.spark.resource.ResourceUtils._\n+import org.apache.spark.resource.TestResourceIDs._\n+import org.apache.spark.util.Utils\n+\n+class ResourceUtilsSuite extends SparkFunSuite\n+    with LocalSparkContext {\n+\n+  test(\"ResourceID\") {\n+    val componentName = \"spark.test\"\n+    val resourceName = \"p100\"\n+    val id = ResourceID(componentName, resourceName)\n+    val confPrefix = s\"$componentName.resource.$resourceName.\"\n+    assert(id.confPrefix === confPrefix)\n+    assert(id.amountConf === s\"${confPrefix}amount\")\n+    assert(id.discoveryScriptConf === s\"${confPrefix}discoveryScript\")\n+    assert(id.vendorConf === s\"${confPrefix}vendor\")\n+  }\n+\n+  test(\"Resource discoverer no addresses errors\") {\n+    val conf = new SparkConf\n+    assume(!(Utils.isWindows))\n+    withTempDir { dir =>\n+      val scriptPath = createTempScriptWithExpectedOutput(dir, \"gpuDiscoverScript\",\n+        \"\"\"{\"name\": \"gpu\"}\"\"\")\n+      conf.set(EXECUTOR_GPU_ID.amountConf, \"2\")\n+      conf.set(EXECUTOR_GPU_ID.discoveryScriptConf, scriptPath)\n+\n+      val error = intercept[IllegalArgumentException] {\n+        getOrDiscoverAllResources(conf, SPARK_EXECUTOR_PREFIX, None)\n+      }.getMessage()\n+      assert(error.contains(\"Resource: gpu, with \" +\n+        \"addresses:  is less than what the user requested: 2\"))\n+    }\n+  }\n+\n+  test(\"Resource discoverer multiple resource types\") {\n+    val conf = new SparkConf\n+    assume(!(Utils.isWindows))\n+    withTempDir { dir =>\n+      val gpuDiscovery = createTempScriptWithExpectedOutput(dir, \"gpuDiscoveryScript\",\n+        \"\"\"{\"name\": \"gpu\", \"addresses\": [\"0\", \"1\"]}\"\"\")\n+      conf.set(EXECUTOR_GPU_ID.amountConf, \"2\")\n+      conf.set(EXECUTOR_GPU_ID.discoveryScriptConf, gpuDiscovery)\n+\n+      val fpgaDiscovery = createTempScriptWithExpectedOutput(dir, \"fpgDiscoverScript\",\n+        \"\"\"{\"name\": \"fpga\", \"addresses\": [\"f1\", \"f2\", \"f3\"]}\"\"\")\n+      conf.set(EXECUTOR_FPGA_ID.amountConf, \"2\")\n+      conf.set(EXECUTOR_FPGA_ID.discoveryScriptConf, fpgaDiscovery)\n+\n+      val resources = getOrDiscoverAllResources(conf, SPARK_EXECUTOR_PREFIX, None)\n+      assert(resources.size === 2)\n+      val gpuValue = resources.get(GPU)\n+      assert(gpuValue.nonEmpty, \"Should have a gpu entry\")\n+      assert(gpuValue.get.name == \"gpu\", \"name should be gpu\")\n+      assert(gpuValue.get.addresses.size == 2, \"Should have 2 indexes\")\n+      assert(gpuValue.get.addresses.deep == Array(\"0\", \"1\").deep, \"should have 0,1 entries\")\n+\n+      val fpgaValue = resources.get(FPGA)\n+      assert(fpgaValue.nonEmpty, \"Should have a gpu entry\")\n+      assert(fpgaValue.get.name == \"fpga\", \"name should be fpga\")\n+      assert(fpgaValue.get.addresses.size == 3, \"Should have 3 indexes\")\n+      assert(fpgaValue.get.addresses.deep == Array(\"f1\", \"f2\", \"f3\").deep,\n+        \"should have f1,f2,f3 entries\")\n+    }\n+  }\n+\n+  test(\"list resource ids\") {\n+    val conf = new SparkConf\n+    conf.set(DRIVER_GPU_ID.amountConf, \"2\")\n+    var resources = listResourceIds(conf, SPARK_DRIVER_PREFIX)\n+    assert(resources.size === 1, \"should only have GPU for resource\")\n+    assert(resources(0).resourceName == GPU, \"name should be gpu\")\n+\n+    conf.set(DRIVER_FPGA_ID.amountConf, \"2\")\n+    val resourcesMap = listResourceIds(conf, SPARK_DRIVER_PREFIX)\n+      .map{ rId => (rId.resourceName, 1)}.toMap\n+    assert(resourcesMap.size === 2, \"should only have GPU for resource\")\n+    assert(resourcesMap.get(GPU).nonEmpty, \"should have GPU\")\n+    assert(resourcesMap.get(FPGA).nonEmpty, \"should have FPGA\")\n+  }\n+\n+  test(\"parse resource request\") {\n+    val conf = new SparkConf\n+    conf.set(DRIVER_GPU_ID.amountConf, \"2\")\n+    var request = parseResourceRequest(conf, DRIVER_GPU_ID)\n+    assert(request.id.resourceName === GPU, \"should only have GPU for resource\")\n+    assert(request.amount === 2, \"GPU count should be 2\")\n+    assert(request.discoveryScript === None, \"discovery script should be empty\")\n+    assert(request.vendor === None, \"vendor should be empty\")\n+\n+    val vendor = \"nvidia.com\"\n+    val discoveryScript = \"discoveryScriptGPU\"\n+    conf.set(DRIVER_GPU_ID.discoveryScriptConf, discoveryScript)\n+    conf.set(DRIVER_GPU_ID.vendorConf, vendor)\n+    request = parseResourceRequest(conf, DRIVER_GPU_ID)\n+    assert(request.id.resourceName === GPU, \"should only have GPU for resource\")\n+    assert(request.amount === 2, \"GPU count should be 2\")\n+    assert(request.discoveryScript.get === discoveryScript, \"discovery script should be empty\")\n+    assert(request.vendor.get === vendor, \"vendor should be empty\")"
  }],
  "prId": 24856
}, {
  "comments": [{
    "author": {
      "login": "Ngone51"
    },
    "body": "case for the combination of resources file and discovery script ?",
    "commit": "f3915856344f7b260397b5582d6738accf319619",
    "createdAt": "2019-06-14T15:51:31Z",
    "diffHunk": "@@ -0,0 +1,230 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.resource\n+\n+import java.io.File\n+import java.nio.file.{Files => JavaFiles}\n+\n+import org.apache.spark.{LocalSparkContext, SparkConf, SparkException, SparkFunSuite}\n+import org.apache.spark.TestUtils._\n+import org.apache.spark.internal.config._\n+import org.apache.spark.resource.ResourceUtils._\n+import org.apache.spark.resource.TestResourceIDs._\n+import org.apache.spark.util.Utils\n+\n+class ResourceUtilsSuite extends SparkFunSuite",
    "line": 32
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "added one test. I will leave improving test coverage to a follow-up PR since the refactoring work is quite big now.",
    "commit": "f3915856344f7b260397b5582d6738accf319619",
    "createdAt": "2019-06-18T16:22:13Z",
    "diffHunk": "@@ -0,0 +1,230 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.resource\n+\n+import java.io.File\n+import java.nio.file.{Files => JavaFiles}\n+\n+import org.apache.spark.{LocalSparkContext, SparkConf, SparkException, SparkFunSuite}\n+import org.apache.spark.TestUtils._\n+import org.apache.spark.internal.config._\n+import org.apache.spark.resource.ResourceUtils._\n+import org.apache.spark.resource.TestResourceIDs._\n+import org.apache.spark.util.Utils\n+\n+class ResourceUtilsSuite extends SparkFunSuite",
    "line": 32
  }],
  "prId": 24856
}]