[{
  "comments": [{
    "author": {
      "login": "attilapiros"
    },
    "body": "Nit: space after ==\r\n",
    "commit": "32377da9ebf9baa5a5a2be9914de9f5639dbf612",
    "createdAt": "2019-03-01T16:05:36Z",
    "diffHunk": "@@ -1520,6 +1520,95 @@ class AppStatusListenerSuite extends SparkFunSuite with BeforeAndAfter {\n     }\n   }\n \n+  test(\"storage information on executor lost/down\") {\n+    val listener = new AppStatusListener(store, conf, true)\n+    val maxMemory = 42L\n+\n+    // Register a couple of block managers.\n+    val bm1 = BlockManagerId(\"1\", \"1.example.com\", 42)\n+    val bm2 = BlockManagerId(\"2\", \"2.example.com\", 84)\n+    Seq(bm1, bm2).foreach { bm =>\n+      listener.onExecutorAdded(SparkListenerExecutorAdded(1L, bm.executorId,\n+        new ExecutorInfo(bm.host, 1, Map.empty, Map.empty)))\n+      listener.onBlockManagerAdded(SparkListenerBlockManagerAdded(1L, bm, maxMemory))\n+    }\n+\n+    val rdd1b1 = RddBlock(1, 1, 1L, 2L)\n+    val rdd1b2 = RddBlock(1, 2, 3L, 4L)\n+    val level = StorageLevel.MEMORY_AND_DISK\n+\n+    // Submit a stage and make sure the RDDs are recorded.\n+    val rdd1Info = new RDDInfo(rdd1b1.rddId, \"rdd1\", 2, level, Nil)\n+    val stage = new StageInfo(1, 0, \"stage1\", 4, Seq(rdd1Info), Nil, \"details1\")\n+    listener.onStageSubmitted(SparkListenerStageSubmitted(stage, new Properties()))\n+\n+    // Add partition 1 replicated on two block managers.\n+    listener.onBlockUpdated(SparkListenerBlockUpdated(\n+      BlockUpdatedInfo(bm1, rdd1b1.blockId, level, rdd1b1.memSize, rdd1b1.diskSize)))\n+\n+    listener.onBlockUpdated(SparkListenerBlockUpdated(\n+      BlockUpdatedInfo(bm2, rdd1b1.blockId, level, rdd1b1.memSize, rdd1b1.diskSize)))\n+\n+    // Add a second partition only to bm 1.\n+    listener.onBlockUpdated(SparkListenerBlockUpdated(\n+      BlockUpdatedInfo(bm1, rdd1b2.blockId, level, rdd1b2.memSize, rdd1b2.diskSize)))\n+\n+    check[RDDStorageInfoWrapper](rdd1b1.rddId) { wrapper =>\n+      assert(wrapper.info.numCachedPartitions === 2L)\n+      assert(wrapper.info.memoryUsed === 2 * rdd1b1.memSize + rdd1b2.memSize)\n+      assert(wrapper.info.diskUsed === 2 * rdd1b1.diskSize + rdd1b2.diskSize)\n+      assert(wrapper.info.dataDistribution.get.size === 2L)\n+      assert(wrapper.info.partitions.get.size === 2L)\n+\n+      val dist = wrapper.info.dataDistribution.get.find(_.address == bm1.hostPort).get\n+      assert(dist.memoryUsed === rdd1b1.memSize + rdd1b2.memSize)\n+      assert(dist.diskUsed === rdd1b1.diskSize + rdd1b2.diskSize)\n+      assert(dist.memoryRemaining === maxMemory - dist.memoryUsed)\n+\n+      val part1 = wrapper.info.partitions.get.find(_.blockName === rdd1b1.blockId.name).get\n+      assert(part1.storageLevel === level.description)\n+      assert(part1.memoryUsed === 2 * rdd1b1.memSize)\n+      assert(part1.diskUsed === 2 * rdd1b1.diskSize)\n+      assert(part1.executors === Seq(bm1.executorId, bm2.executorId))\n+\n+      val part2 = wrapper.info.partitions.get.find(_.blockName === rdd1b2.blockId.name).get\n+      assert(part2.storageLevel === level.description)\n+      assert(part2.memoryUsed === rdd1b2.memSize)\n+      assert(part2.diskUsed === rdd1b2.diskSize)\n+      assert(part2.executors === Seq(bm1.executorId))\n+    }\n+\n+    check[ExecutorSummaryWrapper](bm1.executorId) { exec =>\n+      assert(exec.info.rddBlocks === 2L)\n+      assert(exec.info.memoryUsed === rdd1b1.memSize + rdd1b2.memSize)\n+      assert(exec.info.diskUsed === rdd1b1.diskSize + rdd1b2.diskSize)\n+    }\n+\n+    // Remove Executor 1.\n+    listener.onExecutorRemoved(createExecutorRemovedEvent(1))\n+\n+    // check that partition info now contains only details about what is remaining in bm2\n+    check[RDDStorageInfoWrapper](rdd1b1.rddId) { wrapper =>\n+      assert(wrapper.info.numCachedPartitions === 1L)\n+      assert(wrapper.info.memoryUsed === rdd1b1.memSize)\n+      assert(wrapper.info.diskUsed === rdd1b1.diskSize)\n+      assert(wrapper.info.dataDistribution.get.size === 1L)\n+      assert(wrapper.info.partitions.get.size ===1L)"
  }],
  "prId": 23920
}]