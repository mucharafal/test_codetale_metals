[{
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "any reason to do it this way, rather than using the TaskStart / TaskEnd events for a SparkListener?",
    "commit": "2e0329039435b7bc61ef0370490efe45ba8048c6",
    "createdAt": "2018-10-19T02:58:45Z",
    "diffHunk": "@@ -672,6 +674,55 @@ class SparkContextSuite extends SparkFunSuite with LocalSparkContext with Eventu\n       assert(sc.statusTracker.getExecutorInfos.map(_.numRunningTasks()).sum == 0)\n     }\n   }\n+\n+  test(\"cancel zombie tasks in a result stage when the job finishes\") {\n+    val conf = new SparkConf()\n+      .setMaster(\"local-cluster[1,2,1024]\")\n+      .setAppName(\"test-cluster\")\n+      .set(\"spark.ui.enabled\", \"false\")\n+      // Disable this so that if a task is running, we can make sure the executor will always send\n+      // task metrics via heartbeat to driver.\n+      .set(EXECUTOR_HEARTBEAT_DROP_ZERO_ACCUMULATOR_UPDATES.key, \"false\")\n+      // Set a short heartbeat interval to send SparkListenerExecutorMetricsUpdate fast\n+      .set(\"spark.executor.heartbeatInterval\", \"1s\")\n+    sc = new SparkContext(conf)\n+    sc.setLocalProperty(SparkContext.SPARK_JOB_INTERRUPT_ON_CANCEL, \"true\")\n+    @volatile var runningTaskIds: Seq[Long] = null\n+    val listener = new SparkListener {\n+      override def onExecutorMetricsUpdate(\n+          executorMetricsUpdate: SparkListenerExecutorMetricsUpdate): Unit = {\n+        if (executorMetricsUpdate.execId != SparkContext.DRIVER_IDENTIFIER) {\n+          runningTaskIds = executorMetricsUpdate.accumUpdates.map(_._1)\n+        }\n+      }\n+    }\n+    sc.addSparkListener(listener)\n+    sc.range(0, 2).groupBy((x: Long) => x % 2, 2).map { case (x, _) =>\n+      val context = org.apache.spark.TaskContext.get()\n+      if (context.stageAttemptNumber == 0) {\n+        if (context.partitionId == 0) {\n+          // Make the first task in the first stage attempt fail.\n+          throw new FetchFailedException(SparkEnv.get.blockManager.blockManagerId, 0, 0, 0,\n+            new java.io.IOException(\"fake\"))\n+        } else {\n+          // Make the second task in the first stage attempt sleep to generate a zombie task\n+          Thread.sleep(60000)\n+        }\n+      } else {\n+        // Make the second stage attempt successful.\n+      }\n+      x\n+    }.collect()\n+    sc.listenerBus.waitUntilEmpty(10000)\n+    // As executors will send the metrics of running tasks via heartbeat, we can use this to check\n+    // whether there is any running task.",
    "line": 56
  }, {
    "author": {
      "login": "zsxwing"
    },
    "body": "I prefer this way to make sure the executor did receive the kill command and interrupt the tasks.",
    "commit": "2e0329039435b7bc61ef0370490efe45ba8048c6",
    "createdAt": "2018-10-22T17:17:33Z",
    "diffHunk": "@@ -672,6 +674,55 @@ class SparkContextSuite extends SparkFunSuite with LocalSparkContext with Eventu\n       assert(sc.statusTracker.getExecutorInfos.map(_.numRunningTasks()).sum == 0)\n     }\n   }\n+\n+  test(\"cancel zombie tasks in a result stage when the job finishes\") {\n+    val conf = new SparkConf()\n+      .setMaster(\"local-cluster[1,2,1024]\")\n+      .setAppName(\"test-cluster\")\n+      .set(\"spark.ui.enabled\", \"false\")\n+      // Disable this so that if a task is running, we can make sure the executor will always send\n+      // task metrics via heartbeat to driver.\n+      .set(EXECUTOR_HEARTBEAT_DROP_ZERO_ACCUMULATOR_UPDATES.key, \"false\")\n+      // Set a short heartbeat interval to send SparkListenerExecutorMetricsUpdate fast\n+      .set(\"spark.executor.heartbeatInterval\", \"1s\")\n+    sc = new SparkContext(conf)\n+    sc.setLocalProperty(SparkContext.SPARK_JOB_INTERRUPT_ON_CANCEL, \"true\")\n+    @volatile var runningTaskIds: Seq[Long] = null\n+    val listener = new SparkListener {\n+      override def onExecutorMetricsUpdate(\n+          executorMetricsUpdate: SparkListenerExecutorMetricsUpdate): Unit = {\n+        if (executorMetricsUpdate.execId != SparkContext.DRIVER_IDENTIFIER) {\n+          runningTaskIds = executorMetricsUpdate.accumUpdates.map(_._1)\n+        }\n+      }\n+    }\n+    sc.addSparkListener(listener)\n+    sc.range(0, 2).groupBy((x: Long) => x % 2, 2).map { case (x, _) =>\n+      val context = org.apache.spark.TaskContext.get()\n+      if (context.stageAttemptNumber == 0) {\n+        if (context.partitionId == 0) {\n+          // Make the first task in the first stage attempt fail.\n+          throw new FetchFailedException(SparkEnv.get.blockManager.blockManagerId, 0, 0, 0,\n+            new java.io.IOException(\"fake\"))\n+        } else {\n+          // Make the second task in the first stage attempt sleep to generate a zombie task\n+          Thread.sleep(60000)\n+        }\n+      } else {\n+        // Make the second stage attempt successful.\n+      }\n+      x\n+    }.collect()\n+    sc.listenerBus.waitUntilEmpty(10000)\n+    // As executors will send the metrics of running tasks via heartbeat, we can use this to check\n+    // whether there is any running task.",
    "line": 56
  }],
  "prId": 22771
}]