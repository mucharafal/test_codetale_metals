[{
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "these tests are great, some cases in here I hadn't thought of, but I think we should add some more:\n\n1) using `rdd.filter`(I know its the same code path right now, but it should be there as a regression just in case it changes)\n2) something with `rdd.coalesce`, so there are a different number of tasks from partitions\n3) Some dag with more forks in it, eg.\n\n```\nval data = sc.parallelize(1 to 1e4.toInt, 20)\nval x = sc.accumulator(0, consistent = true)\nval y = sc.accumulator(0, consistent = true)\n\n// this rdd gets computed multiple times, sometimes with a coalesce, sometimes not, still only applies updates from x once\nval a = data.filter { i => if (i % 10 == 0) { x += 1; true} else false }\n\nval b = a.coalese(10)\nval c = b.map { i => y += i; i + 1 }\n\nassert(c.take(10).length == 10)\n\n// these updates to x also get counted\nval d = a.map { i => x += 10; i + 1 }\n\nval e = c.map { i => (i -> i) }.cogroup( d.map { i => y += i; (i -> i) } )\ne.count()\nassert(x.value == 11e3.toInt)\nassert(y.value == 10011000) // s = (1e3 * (1 + 1e3)) / 2; s * 2 * 10 + 1000\n```\n\n(something a bit simpler would work too.)\n\n4) concurrent jobs on a shared RDD w/ a consistent accumulator.  Perhaps it actually merits a special test on the internals to cover some interleavings?\n\nAlso can we use `local[2]` for the context?\n",
    "commit": "59db4af635fd53e2723d187b60dee6856af74df4",
    "createdAt": "2016-03-08T20:59:44Z",
    "diffHunk": "@@ -0,0 +1,166 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import scala.ref.WeakReference\n+\n+import org.scalatest.Matchers\n+\n+import org.apache.spark.scheduler._\n+\n+\n+class ConsistentAccumulatorSuite extends SparkFunSuite with Matchers with LocalSparkContext {\n+\n+  test(\"single partition\") {\n+    sc = new SparkContext(\"local\", \"test\")\n+    val acc : Accumulator[Int] = sc.accumulator(0, consistent = true)\n+\n+    val a = sc.parallelize(1 to 20, 1)\n+    val b = a.map{x => acc += x; x}\n+    b.cache()\n+    b.count()\n+    acc.value should be (210)\n+  }\n+\n+  test(\"map + cache + first + count\") {\n+    sc = new SparkContext(\"local\", \"test\")\n+    val acc : Accumulator[Int] = sc.accumulator(0, consistent = true)\n+\n+    val a = sc.parallelize(1 to 20, 10)\n+    val b = a.map{x => acc += x; x}\n+    b.cache()\n+    b.first()\n+    acc.value should be > (0)\n+    b.collect()\n+    acc.value should be (210)\n+  }\n+\n+  test (\"basic accumulation\"){\n+    sc = new SparkContext(\"local\", \"test\")\n+    val acc : Accumulator[Int] = sc.accumulator(0, consistent = true)\n+\n+    val d = sc.parallelize(1 to 20)\n+    d.map{x => acc += x}.count()\n+    acc.value should be (210)\n+\n+    val longAcc = sc.accumulator(0L, consistent = true)\n+    val maxInt = Integer.MAX_VALUE.toLong\n+    d.map{x => longAcc += maxInt + x; x}.count()\n+    longAcc.value should be (210L + maxInt * 20)\n+  }\n+\n+  test (\"basic accumulation flatMap\"){\n+    sc = new SparkContext(\"local\", \"test\")\n+    val acc : Accumulator[Int] = sc.accumulator(0, consistent = true)\n+\n+    val d = sc.parallelize(1 to 20)\n+    d.map{x => acc += x}.count()\n+    acc.value should be (210)\n+\n+    val longAcc = sc.accumulator(0L, consistent = true)\n+    val maxInt = Integer.MAX_VALUE.toLong\n+    val c = d.flatMap{x =>\n+      longAcc += maxInt + x\n+      if (x % 2 == 0) {\n+        Some(x)\n+      } else {\n+        None\n+      }\n+    }.count()\n+    longAcc.value should be (210L + maxInt * 20)\n+    c should be (10)\n+  }\n+\n+  test(\"map + map + count\") {\n+    sc = new SparkContext(\"local\", \"test\")\n+    val acc : Accumulator[Int] = sc.accumulator(0, consistent = true)\n+\n+    val a = sc.parallelize(1 to 20, 10)\n+    val b = a.map{x => acc += x; x}\n+    val c = b.map{x => acc += x; x}\n+    c.count()\n+    acc.value should be (420)\n+  }\n+\n+  test(\"first + count\") {\n+    sc = new SparkContext(\"local\", \"test\")\n+    val acc : Accumulator[Int] = sc.accumulator(0, consistent = true)\n+\n+    val a = sc.parallelize(1 to 20, 10)\n+    val b = a.map{x => acc += x; x}\n+    b.first()\n+    b.count()\n+    acc.value should be (210)\n+  }\n+\n+  test(\"map + count + count + map + count\") {\n+    sc = new SparkContext(\"local\", \"test\")\n+    val acc : Accumulator[Int] = sc.accumulator(0, consistent = true)\n+\n+    val a = sc.parallelize(1 to 20, 10)\n+    val b = a.map{x => acc += x; x}\n+    b.count()\n+    acc.value should be (210)\n+    b.count()\n+    acc.value should be (210)\n+    val c = b.map{x => acc += x; x}\n+    c.count()\n+    acc.value should be (420)\n+  }\n+\n+  test (\"map + toLocalIterator + count\"){\n+    sc = new SparkContext(\"local\", \"test\")\n+    val acc : Accumulator[Int] = sc.accumulator(0, consistent = true)\n+\n+    val a = sc.parallelize(1 to 100, 10)\n+    val b = a.map{x => acc += x; x}\n+    // This depends on toLocalIterators per-partition fetch behaviour\n+    b.toLocalIterator.take(2).toList\n+    acc.value should be > (0)\n+    b.count()\n+    acc.value should be (5050)\n+    b.count()\n+    acc.value should be (5050)\n+\n+    val c = b.map{x => acc += x; x}\n+    c.cache()\n+    c.toLocalIterator.take(2).toList\n+    acc.value should be > (5050)\n+    c.count()\n+    acc.value should be (10100)\n+  }"
  }, {
    "author": {
      "login": "holdenk"
    },
    "body": "Good points, I'll try and expand on the test coverage some over the next week.\n",
    "commit": "59db4af635fd53e2723d187b60dee6856af74df4",
    "createdAt": "2016-03-08T21:28:35Z",
    "diffHunk": "@@ -0,0 +1,166 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import scala.ref.WeakReference\n+\n+import org.scalatest.Matchers\n+\n+import org.apache.spark.scheduler._\n+\n+\n+class ConsistentAccumulatorSuite extends SparkFunSuite with Matchers with LocalSparkContext {\n+\n+  test(\"single partition\") {\n+    sc = new SparkContext(\"local\", \"test\")\n+    val acc : Accumulator[Int] = sc.accumulator(0, consistent = true)\n+\n+    val a = sc.parallelize(1 to 20, 1)\n+    val b = a.map{x => acc += x; x}\n+    b.cache()\n+    b.count()\n+    acc.value should be (210)\n+  }\n+\n+  test(\"map + cache + first + count\") {\n+    sc = new SparkContext(\"local\", \"test\")\n+    val acc : Accumulator[Int] = sc.accumulator(0, consistent = true)\n+\n+    val a = sc.parallelize(1 to 20, 10)\n+    val b = a.map{x => acc += x; x}\n+    b.cache()\n+    b.first()\n+    acc.value should be > (0)\n+    b.collect()\n+    acc.value should be (210)\n+  }\n+\n+  test (\"basic accumulation\"){\n+    sc = new SparkContext(\"local\", \"test\")\n+    val acc : Accumulator[Int] = sc.accumulator(0, consistent = true)\n+\n+    val d = sc.parallelize(1 to 20)\n+    d.map{x => acc += x}.count()\n+    acc.value should be (210)\n+\n+    val longAcc = sc.accumulator(0L, consistent = true)\n+    val maxInt = Integer.MAX_VALUE.toLong\n+    d.map{x => longAcc += maxInt + x; x}.count()\n+    longAcc.value should be (210L + maxInt * 20)\n+  }\n+\n+  test (\"basic accumulation flatMap\"){\n+    sc = new SparkContext(\"local\", \"test\")\n+    val acc : Accumulator[Int] = sc.accumulator(0, consistent = true)\n+\n+    val d = sc.parallelize(1 to 20)\n+    d.map{x => acc += x}.count()\n+    acc.value should be (210)\n+\n+    val longAcc = sc.accumulator(0L, consistent = true)\n+    val maxInt = Integer.MAX_VALUE.toLong\n+    val c = d.flatMap{x =>\n+      longAcc += maxInt + x\n+      if (x % 2 == 0) {\n+        Some(x)\n+      } else {\n+        None\n+      }\n+    }.count()\n+    longAcc.value should be (210L + maxInt * 20)\n+    c should be (10)\n+  }\n+\n+  test(\"map + map + count\") {\n+    sc = new SparkContext(\"local\", \"test\")\n+    val acc : Accumulator[Int] = sc.accumulator(0, consistent = true)\n+\n+    val a = sc.parallelize(1 to 20, 10)\n+    val b = a.map{x => acc += x; x}\n+    val c = b.map{x => acc += x; x}\n+    c.count()\n+    acc.value should be (420)\n+  }\n+\n+  test(\"first + count\") {\n+    sc = new SparkContext(\"local\", \"test\")\n+    val acc : Accumulator[Int] = sc.accumulator(0, consistent = true)\n+\n+    val a = sc.parallelize(1 to 20, 10)\n+    val b = a.map{x => acc += x; x}\n+    b.first()\n+    b.count()\n+    acc.value should be (210)\n+  }\n+\n+  test(\"map + count + count + map + count\") {\n+    sc = new SparkContext(\"local\", \"test\")\n+    val acc : Accumulator[Int] = sc.accumulator(0, consistent = true)\n+\n+    val a = sc.parallelize(1 to 20, 10)\n+    val b = a.map{x => acc += x; x}\n+    b.count()\n+    acc.value should be (210)\n+    b.count()\n+    acc.value should be (210)\n+    val c = b.map{x => acc += x; x}\n+    c.count()\n+    acc.value should be (420)\n+  }\n+\n+  test (\"map + toLocalIterator + count\"){\n+    sc = new SparkContext(\"local\", \"test\")\n+    val acc : Accumulator[Int] = sc.accumulator(0, consistent = true)\n+\n+    val a = sc.parallelize(1 to 100, 10)\n+    val b = a.map{x => acc += x; x}\n+    // This depends on toLocalIterators per-partition fetch behaviour\n+    b.toLocalIterator.take(2).toList\n+    acc.value should be > (0)\n+    b.count()\n+    acc.value should be (5050)\n+    b.count()\n+    acc.value should be (5050)\n+\n+    val c = b.map{x => acc += x; x}\n+    c.cache()\n+    c.toLocalIterator.take(2).toList\n+    acc.value should be > (5050)\n+    c.count()\n+    acc.value should be (10100)\n+  }"
  }],
  "prId": 11105
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "I think the coalesce test needs to do partial partition reading, of both the original partitions and the coalesced partitions, like so (this passes):\n\n``` scala\n  test(\"coalesce with partial partition reading\") {\n    // when coalescing, one task can completely read partitions of the input RDDs while not reading complete\n    // complete partitions of the post-coalesce RDD.  We make sure that the accumulator has consistent semantics\n    // in these cases.\n    sc = new SparkContext(\"local[2]\", \"test\")\n    val List(acc1, acc2, acc3) = 1.to(3).map(x => sc.accumulator(0, consistent = true)).toList\n    val a = sc.parallelize(1 to 20, 10)\n    val b = a.map{x => acc1 += x; acc2 += x; 2 * x}\n    val c = b.coalesce(2).map{x => acc1 += x; acc3 += x; x}\n    // we read all of partition 1 from RDD's a & b, and part of partition 2\n    // however, for RDD c, we don't read any of the partitions fully\n    // so we should get updates for 1 partition from b, and nothing from c\n    c.take(3) should be ((1 to 3).map(_*2).toArray)\n    acc1.value should be (3) // only elements 1 & 2 came from fully read partitions\n    acc2.value should be (3)\n    acc3.value should be (0)\n\n    // now we read a few more parts:\n    c.take(9) should be ((1 to 9).map(_*2).toArray)\n    acc1.value should be (36)\n    acc2.value should be (36)\n    acc3.value should be (0)\n\n\n    // a couple more, this time we read 1 of c's partitions fully\n    c.take(15) should be ((1 to 15).map(_*2).toArray)\n    acc1.value should be (215)\n    acc2.value should be (105)\n    acc3.value should be (110)\n\n    // and if we read the entire data, we get the entire value\n    c.count()\n    acc1.value should be (630)\n    acc2.value should be (210)\n    acc3.value should be (420)\n  }\n```\n",
    "commit": "59db4af635fd53e2723d187b60dee6856af74df4",
    "createdAt": "2016-03-16T21:04:32Z",
    "diffHunk": "@@ -0,0 +1,284 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import scala.ref.WeakReference\n+\n+import org.scalatest.Matchers\n+\n+import org.apache.spark.scheduler._\n+\n+\n+class ConsistentAccumulatorSuite extends SparkFunSuite with Matchers with LocalSparkContext {\n+  test(\"single partition\") {\n+    sc = new SparkContext(\"local[2]\", \"test\")\n+    val acc : Accumulator[Int] = sc.accumulator(0, consistent = true)\n+\n+    val a = sc.parallelize(1 to 20, 1)\n+    val b = a.map{x => acc += x; x}\n+    b.cache()\n+    b.count()\n+    acc.value should be (210)\n+  }\n+\n+  test(\"adding only the first element per partition should work even if partition is empty\") {\n+    sc = new SparkContext(\"local[2]\", \"test\")\n+    val acc: Accumulator[Int] = sc.accumulator(0, consistent = true)\n+    val a = sc.parallelize(1 to 20, 30)\n+    val b = a.mapPartitions{itr =>\n+      acc += 1\n+      itr\n+    }\n+    b.count()\n+    acc.value should be (30)\n+  }\n+\n+  test(\"shuffled (combineByKey)\") {\n+    sc = new SparkContext(\"local[2]\", \"test\")\n+    val a = sc.parallelize(1 to 40, 5)\n+    val buckets = 4\n+    val b = a.map{x => ((x % buckets), x)}\n+    val inputs = List(b, b.repartition(10), b.partitionBy(new HashPartitioner(5))).map(_.cache())\n+    val mapSideCombines = List(true, false)\n+    inputs.foreach{input =>\n+      mapSideCombines.foreach{mapSideCombine =>\n+        val accs = 1.to(4).map(x => sc.accumulator(0, consistent = true)).toList\n+        val raccs = 1.to(4).map(x => sc.accumulator(0, consistent = false)).toList\n+        val List(acc, acc1, acc2, acc3) = accs\n+        val List(racc, racc1, racc2, racc3) = raccs\n+        val c = input.combineByKey(\n+          (x: Int) => {acc1 += 1; acc += 1; racc1 += 1; racc += 1; x},\n+          {(a: Int, b: Int) => acc2 += 1; acc += 1; racc2 += 1; racc += 1; (a + b)},\n+          {(a: Int, b: Int) => acc3 += 1; acc += 1; racc3 += 1; racc += 1; (a + b)},\n+          new HashPartitioner(10),\n+          mapSideCombine)\n+        val d = input.combineByKey(\n+          (x: Int) => {acc1 += 1; acc += 1; x},\n+          {(a: Int, b: Int) => acc2 += 1; acc += 1; (a + b)},\n+          {(a: Int, b: Int) => acc3 += 1; acc += 1; (a + b)},\n+          new HashPartitioner(2),\n+          mapSideCombine)\n+        val e = d.map{x => acc += 1; x}\n+        c.count()\n+        // If our partitioner is known then we should only create\n+        // one combiner for each key value. Otherwise we should\n+        // create at least that many combiners.\n+        if (input.partitioner.isDefined) {\n+          acc1.value should be (buckets)\n+        } else {\n+          acc1.value should be >= (buckets)\n+        }\n+        if (input.partitioner.isDefined) {\n+          acc2.value should be > (0)\n+        } else if (mapSideCombine) {\n+          acc3.value should be > (0)\n+        } else {\n+          acc2.value should be > (0)\n+          acc3.value should be (0)\n+        }\n+        acc.value should be (acc1.value + acc2.value + acc3.value)\n+        val oldValues = accs.map(_.value)\n+        // For one action the consistent accumulators and regular should have the same value.\n+        accs.map(_.value) should be (raccs.map(_.value))\n+        c.count()\n+        accs.map(_.value) should be (oldValues)\n+        // Executing a second _different_ aggregation should count everything 2x\n+        d.count()\n+        accs.map(_.value) should be (oldValues.map(_ * 2))\n+        // Computing the mapped value on top and verify new changes are processed and old changes\n+        // are note double counted.\n+        val count = e.count()\n+        accs.tail.map(_.value) should be (oldValues.tail.map(_ * 2))\n+        acc.value should be (oldValues.head * 2 + count)\n+      }\n+    }\n+  }\n+\n+  test(\"map + cache + first + count\") {\n+    sc = new SparkContext(\"local[2]\", \"test\")\n+    val acc : Accumulator[Int] = sc.accumulator(0, consistent = true)\n+\n+    val a = sc.parallelize(1 to 20, 10)\n+    val b = a.map{x => acc += x; x}\n+    b.cache()\n+    b.first()\n+    acc.value should be > (0)\n+    b.collect()\n+    acc.value should be (210)\n+  }\n+\n+  test(\"coalesce acc on either side\") {\n+    sc = new SparkContext(\"local[2]\", \"test\")\n+    val List(acc1, acc2, acc3) = 1.to(3).map(x => sc.accumulator(0, consistent = true)).toList\n+    val a = sc.parallelize(1 to 20, 10)\n+    val b = a.map{x => acc1 += x; acc2 += x; 2 * x}\n+    val c = b.coalesce(2).map{x => acc1 += x; acc3 += x; x}\n+    c.count()\n+    acc1.value should be (630)\n+    acc2.value should be (210)\n+    acc3.value should be (420)\n+  }"
  }],
  "prId": 11105
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "nit: ranges are an exception to the usual style rules, the preferred idiom is `(1 to 4)`.\n\nAlso, spaces around `{`, so `inputs.foreach { input => ...` etc.\n",
    "commit": "59db4af635fd53e2723d187b60dee6856af74df4",
    "createdAt": "2016-03-16T21:58:58Z",
    "diffHunk": "@@ -0,0 +1,284 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import scala.ref.WeakReference\n+\n+import org.scalatest.Matchers\n+\n+import org.apache.spark.scheduler._\n+\n+\n+class ConsistentAccumulatorSuite extends SparkFunSuite with Matchers with LocalSparkContext {\n+  test(\"single partition\") {\n+    sc = new SparkContext(\"local[2]\", \"test\")\n+    val acc : Accumulator[Int] = sc.accumulator(0, consistent = true)\n+\n+    val a = sc.parallelize(1 to 20, 1)\n+    val b = a.map{x => acc += x; x}\n+    b.cache()\n+    b.count()\n+    acc.value should be (210)\n+  }\n+\n+  test(\"adding only the first element per partition should work even if partition is empty\") {\n+    sc = new SparkContext(\"local[2]\", \"test\")\n+    val acc: Accumulator[Int] = sc.accumulator(0, consistent = true)\n+    val a = sc.parallelize(1 to 20, 30)\n+    val b = a.mapPartitions{itr =>\n+      acc += 1\n+      itr\n+    }\n+    b.count()\n+    acc.value should be (30)\n+  }\n+\n+  test(\"shuffled (combineByKey)\") {\n+    sc = new SparkContext(\"local[2]\", \"test\")\n+    val a = sc.parallelize(1 to 40, 5)\n+    val buckets = 4\n+    val b = a.map{x => ((x % buckets), x)}\n+    val inputs = List(b, b.repartition(10), b.partitionBy(new HashPartitioner(5))).map(_.cache())\n+    val mapSideCombines = List(true, false)\n+    inputs.foreach{input =>\n+      mapSideCombines.foreach{mapSideCombine =>\n+        val accs = 1.to(4).map(x => sc.accumulator(0, consistent = true)).toList\n+        val raccs = 1.to(4).map(x => sc.accumulator(0, consistent = false)).toList"
  }],
  "prId": 11105
}]