[{
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "Why after `c.count()`, accs.map(_.value) still doesn't change?\n",
    "commit": "59db4af635fd53e2723d187b60dee6856af74df4",
    "createdAt": "2016-04-11T10:06:22Z",
    "diffHunk": "@@ -0,0 +1,361 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import scala.concurrent.ExecutionContext.Implicits.global\n+import scala.ref.WeakReference\n+\n+import org.scalatest.Matchers\n+\n+import org.apache.spark.scheduler._\n+\n+\n+class DataPropertyAccumulatorSuite extends SparkFunSuite with Matchers with LocalSparkContext {\n+  test(\"single partition\") {\n+    sc = new SparkContext(\"local[2]\", \"test\")\n+    val acc : Accumulator[Int] = sc.accumulator(0, dataProperty = true)\n+\n+    val a = sc.parallelize(1 to 20, 1)\n+    val b = a.map{x => acc += x; x}\n+    b.cache()\n+    b.count()\n+    acc.value should be (210)\n+  }\n+\n+  test(\"adding only the first element per partition should work even if partition is empty\") {\n+    sc = new SparkContext(\"local[2]\", \"test\")\n+    val acc: Accumulator[Int] = sc.accumulator(0, dataProperty = true)\n+    val a = sc.parallelize(1 to 20, 30)\n+    val b = a.mapPartitions{itr =>\n+      acc += 1\n+      itr\n+    }\n+    b.count()\n+    acc.value should be (30)\n+  }\n+\n+  test(\"shuffled (combineByKey)\") {\n+    sc = new SparkContext(\"local[2]\", \"test\")\n+    val a = sc.parallelize(1 to 40, 5)\n+    val buckets = 4\n+    val b = a.map{x => ((x % buckets), x)}\n+    val inputs = List(b, b.repartition(10), b.partitionBy(new HashPartitioner(5))).map(_.cache())\n+    val mapSideCombines = List(true, false)\n+    inputs.foreach { input =>\n+      mapSideCombines.foreach { mapSideCombine =>\n+        val accs = (1 to 4).map(x => sc.accumulator(0, dataProperty = true)).toList\n+        val raccs = (1 to 4).map(x => sc.accumulator(0, dataProperty = false)).toList\n+        val List(acc, acc1, acc2, acc3) = accs\n+        val List(racc, racc1, racc2, racc3) = raccs\n+        val c = input.combineByKey(\n+          (x: Int) => {acc1 += 1; acc += 1; racc1 += 1; racc += 1; x},\n+          {(a: Int, b: Int) => acc2 += 1; acc += 1; racc2 += 1; racc += 1; (a + b)},\n+          {(a: Int, b: Int) => acc3 += 1; acc += 1; racc3 += 1; racc += 1; (a + b)},\n+          new HashPartitioner(10),\n+          mapSideCombine)\n+        val d = input.combineByKey(\n+          (x: Int) => {acc1 += 1; acc += 1; x},\n+          {(a: Int, b: Int) => acc2 += 1; acc += 1; (a + b)},\n+          {(a: Int, b: Int) => acc3 += 1; acc += 1; (a + b)},\n+          new HashPartitioner(2),\n+          mapSideCombine)\n+        val e = d.map{x => acc += 1; x}\n+        c.count()\n+        // If our partitioner is known then we should only create\n+        // one combiner for each key value. Otherwise we should\n+        // create at least that many combiners.\n+        if (input.partitioner.isDefined) {\n+          acc1.value should be (buckets)\n+        } else {\n+          acc1.value should be >= (buckets)\n+        }\n+        if (input.partitioner.isDefined) {\n+          acc2.value should be > (0)\n+        } else if (mapSideCombine) {\n+          acc3.value should be > (0)\n+        } else {\n+          acc2.value should be > (0)\n+          acc3.value should be (0)\n+        }\n+        acc.value should be (acc1.value + acc2.value + acc3.value)\n+        val oldValues = accs.map(_.value)\n+        // For one action the data property accumulators and regular should have the same value.\n+        accs.map(_.value) should be (raccs.map(_.value))\n+        c.count()\n+        accs.map(_.value) should be (oldValues)",
    "line": 124
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "ok. I see.\n",
    "commit": "59db4af635fd53e2723d187b60dee6856af74df4",
    "createdAt": "2016-04-11T10:21:16Z",
    "diffHunk": "@@ -0,0 +1,361 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import scala.concurrent.ExecutionContext.Implicits.global\n+import scala.ref.WeakReference\n+\n+import org.scalatest.Matchers\n+\n+import org.apache.spark.scheduler._\n+\n+\n+class DataPropertyAccumulatorSuite extends SparkFunSuite with Matchers with LocalSparkContext {\n+  test(\"single partition\") {\n+    sc = new SparkContext(\"local[2]\", \"test\")\n+    val acc : Accumulator[Int] = sc.accumulator(0, dataProperty = true)\n+\n+    val a = sc.parallelize(1 to 20, 1)\n+    val b = a.map{x => acc += x; x}\n+    b.cache()\n+    b.count()\n+    acc.value should be (210)\n+  }\n+\n+  test(\"adding only the first element per partition should work even if partition is empty\") {\n+    sc = new SparkContext(\"local[2]\", \"test\")\n+    val acc: Accumulator[Int] = sc.accumulator(0, dataProperty = true)\n+    val a = sc.parallelize(1 to 20, 30)\n+    val b = a.mapPartitions{itr =>\n+      acc += 1\n+      itr\n+    }\n+    b.count()\n+    acc.value should be (30)\n+  }\n+\n+  test(\"shuffled (combineByKey)\") {\n+    sc = new SparkContext(\"local[2]\", \"test\")\n+    val a = sc.parallelize(1 to 40, 5)\n+    val buckets = 4\n+    val b = a.map{x => ((x % buckets), x)}\n+    val inputs = List(b, b.repartition(10), b.partitionBy(new HashPartitioner(5))).map(_.cache())\n+    val mapSideCombines = List(true, false)\n+    inputs.foreach { input =>\n+      mapSideCombines.foreach { mapSideCombine =>\n+        val accs = (1 to 4).map(x => sc.accumulator(0, dataProperty = true)).toList\n+        val raccs = (1 to 4).map(x => sc.accumulator(0, dataProperty = false)).toList\n+        val List(acc, acc1, acc2, acc3) = accs\n+        val List(racc, racc1, racc2, racc3) = raccs\n+        val c = input.combineByKey(\n+          (x: Int) => {acc1 += 1; acc += 1; racc1 += 1; racc += 1; x},\n+          {(a: Int, b: Int) => acc2 += 1; acc += 1; racc2 += 1; racc += 1; (a + b)},\n+          {(a: Int, b: Int) => acc3 += 1; acc += 1; racc3 += 1; racc += 1; (a + b)},\n+          new HashPartitioner(10),\n+          mapSideCombine)\n+        val d = input.combineByKey(\n+          (x: Int) => {acc1 += 1; acc += 1; x},\n+          {(a: Int, b: Int) => acc2 += 1; acc += 1; (a + b)},\n+          {(a: Int, b: Int) => acc3 += 1; acc += 1; (a + b)},\n+          new HashPartitioner(2),\n+          mapSideCombine)\n+        val e = d.map{x => acc += 1; x}\n+        c.count()\n+        // If our partitioner is known then we should only create\n+        // one combiner for each key value. Otherwise we should\n+        // create at least that many combiners.\n+        if (input.partitioner.isDefined) {\n+          acc1.value should be (buckets)\n+        } else {\n+          acc1.value should be >= (buckets)\n+        }\n+        if (input.partitioner.isDefined) {\n+          acc2.value should be > (0)\n+        } else if (mapSideCombine) {\n+          acc3.value should be > (0)\n+        } else {\n+          acc2.value should be > (0)\n+          acc3.value should be (0)\n+        }\n+        acc.value should be (acc1.value + acc2.value + acc3.value)\n+        val oldValues = accs.map(_.value)\n+        // For one action the data property accumulators and regular should have the same value.\n+        accs.map(_.value) should be (raccs.map(_.value))\n+        c.count()\n+        accs.map(_.value) should be (oldValues)",
    "line": 124
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "But actually the second `c.count()` will begin a new stage. I am curious why it doesn't cause the accumulators to be re-evaluated the second time.\n",
    "commit": "59db4af635fd53e2723d187b60dee6856af74df4",
    "createdAt": "2016-04-11T10:32:38Z",
    "diffHunk": "@@ -0,0 +1,361 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import scala.concurrent.ExecutionContext.Implicits.global\n+import scala.ref.WeakReference\n+\n+import org.scalatest.Matchers\n+\n+import org.apache.spark.scheduler._\n+\n+\n+class DataPropertyAccumulatorSuite extends SparkFunSuite with Matchers with LocalSparkContext {\n+  test(\"single partition\") {\n+    sc = new SparkContext(\"local[2]\", \"test\")\n+    val acc : Accumulator[Int] = sc.accumulator(0, dataProperty = true)\n+\n+    val a = sc.parallelize(1 to 20, 1)\n+    val b = a.map{x => acc += x; x}\n+    b.cache()\n+    b.count()\n+    acc.value should be (210)\n+  }\n+\n+  test(\"adding only the first element per partition should work even if partition is empty\") {\n+    sc = new SparkContext(\"local[2]\", \"test\")\n+    val acc: Accumulator[Int] = sc.accumulator(0, dataProperty = true)\n+    val a = sc.parallelize(1 to 20, 30)\n+    val b = a.mapPartitions{itr =>\n+      acc += 1\n+      itr\n+    }\n+    b.count()\n+    acc.value should be (30)\n+  }\n+\n+  test(\"shuffled (combineByKey)\") {\n+    sc = new SparkContext(\"local[2]\", \"test\")\n+    val a = sc.parallelize(1 to 40, 5)\n+    val buckets = 4\n+    val b = a.map{x => ((x % buckets), x)}\n+    val inputs = List(b, b.repartition(10), b.partitionBy(new HashPartitioner(5))).map(_.cache())\n+    val mapSideCombines = List(true, false)\n+    inputs.foreach { input =>\n+      mapSideCombines.foreach { mapSideCombine =>\n+        val accs = (1 to 4).map(x => sc.accumulator(0, dataProperty = true)).toList\n+        val raccs = (1 to 4).map(x => sc.accumulator(0, dataProperty = false)).toList\n+        val List(acc, acc1, acc2, acc3) = accs\n+        val List(racc, racc1, racc2, racc3) = raccs\n+        val c = input.combineByKey(\n+          (x: Int) => {acc1 += 1; acc += 1; racc1 += 1; racc += 1; x},\n+          {(a: Int, b: Int) => acc2 += 1; acc += 1; racc2 += 1; racc += 1; (a + b)},\n+          {(a: Int, b: Int) => acc3 += 1; acc += 1; racc3 += 1; racc += 1; (a + b)},\n+          new HashPartitioner(10),\n+          mapSideCombine)\n+        val d = input.combineByKey(\n+          (x: Int) => {acc1 += 1; acc += 1; x},\n+          {(a: Int, b: Int) => acc2 += 1; acc += 1; (a + b)},\n+          {(a: Int, b: Int) => acc3 += 1; acc += 1; (a + b)},\n+          new HashPartitioner(2),\n+          mapSideCombine)\n+        val e = d.map{x => acc += 1; x}\n+        c.count()\n+        // If our partitioner is known then we should only create\n+        // one combiner for each key value. Otherwise we should\n+        // create at least that many combiners.\n+        if (input.partitioner.isDefined) {\n+          acc1.value should be (buckets)\n+        } else {\n+          acc1.value should be >= (buckets)\n+        }\n+        if (input.partitioner.isDefined) {\n+          acc2.value should be > (0)\n+        } else if (mapSideCombine) {\n+          acc3.value should be > (0)\n+        } else {\n+          acc2.value should be > (0)\n+          acc3.value should be (0)\n+        }\n+        acc.value should be (acc1.value + acc2.value + acc3.value)\n+        val oldValues = accs.map(_.value)\n+        // For one action the data property accumulators and regular should have the same value.\n+        accs.map(_.value) should be (raccs.map(_.value))\n+        c.count()\n+        accs.map(_.value) should be (oldValues)",
    "line": 124
  }, {
    "author": {
      "login": "holdenk"
    },
    "body": "So for DataProperty accumulators they will only be evaluated once for each piece of data, regardless of which stage. The gating logic is mostly in `mergePending` which doesn't depend on stage IDs rather it uses the RDD IDs & Shuffle IDs.\n",
    "commit": "59db4af635fd53e2723d187b60dee6856af74df4",
    "createdAt": "2016-04-11T19:51:34Z",
    "diffHunk": "@@ -0,0 +1,361 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import scala.concurrent.ExecutionContext.Implicits.global\n+import scala.ref.WeakReference\n+\n+import org.scalatest.Matchers\n+\n+import org.apache.spark.scheduler._\n+\n+\n+class DataPropertyAccumulatorSuite extends SparkFunSuite with Matchers with LocalSparkContext {\n+  test(\"single partition\") {\n+    sc = new SparkContext(\"local[2]\", \"test\")\n+    val acc : Accumulator[Int] = sc.accumulator(0, dataProperty = true)\n+\n+    val a = sc.parallelize(1 to 20, 1)\n+    val b = a.map{x => acc += x; x}\n+    b.cache()\n+    b.count()\n+    acc.value should be (210)\n+  }\n+\n+  test(\"adding only the first element per partition should work even if partition is empty\") {\n+    sc = new SparkContext(\"local[2]\", \"test\")\n+    val acc: Accumulator[Int] = sc.accumulator(0, dataProperty = true)\n+    val a = sc.parallelize(1 to 20, 30)\n+    val b = a.mapPartitions{itr =>\n+      acc += 1\n+      itr\n+    }\n+    b.count()\n+    acc.value should be (30)\n+  }\n+\n+  test(\"shuffled (combineByKey)\") {\n+    sc = new SparkContext(\"local[2]\", \"test\")\n+    val a = sc.parallelize(1 to 40, 5)\n+    val buckets = 4\n+    val b = a.map{x => ((x % buckets), x)}\n+    val inputs = List(b, b.repartition(10), b.partitionBy(new HashPartitioner(5))).map(_.cache())\n+    val mapSideCombines = List(true, false)\n+    inputs.foreach { input =>\n+      mapSideCombines.foreach { mapSideCombine =>\n+        val accs = (1 to 4).map(x => sc.accumulator(0, dataProperty = true)).toList\n+        val raccs = (1 to 4).map(x => sc.accumulator(0, dataProperty = false)).toList\n+        val List(acc, acc1, acc2, acc3) = accs\n+        val List(racc, racc1, racc2, racc3) = raccs\n+        val c = input.combineByKey(\n+          (x: Int) => {acc1 += 1; acc += 1; racc1 += 1; racc += 1; x},\n+          {(a: Int, b: Int) => acc2 += 1; acc += 1; racc2 += 1; racc += 1; (a + b)},\n+          {(a: Int, b: Int) => acc3 += 1; acc += 1; racc3 += 1; racc += 1; (a + b)},\n+          new HashPartitioner(10),\n+          mapSideCombine)\n+        val d = input.combineByKey(\n+          (x: Int) => {acc1 += 1; acc += 1; x},\n+          {(a: Int, b: Int) => acc2 += 1; acc += 1; (a + b)},\n+          {(a: Int, b: Int) => acc3 += 1; acc += 1; (a + b)},\n+          new HashPartitioner(2),\n+          mapSideCombine)\n+        val e = d.map{x => acc += 1; x}\n+        c.count()\n+        // If our partitioner is known then we should only create\n+        // one combiner for each key value. Otherwise we should\n+        // create at least that many combiners.\n+        if (input.partitioner.isDefined) {\n+          acc1.value should be (buckets)\n+        } else {\n+          acc1.value should be >= (buckets)\n+        }\n+        if (input.partitioner.isDefined) {\n+          acc2.value should be > (0)\n+        } else if (mapSideCombine) {\n+          acc3.value should be > (0)\n+        } else {\n+          acc2.value should be > (0)\n+          acc3.value should be (0)\n+        }\n+        acc.value should be (acc1.value + acc2.value + acc3.value)\n+        val oldValues = accs.map(_.value)\n+        // For one action the data property accumulators and regular should have the same value.\n+        accs.map(_.value) should be (raccs.map(_.value))\n+        c.count()\n+        accs.map(_.value) should be (oldValues)",
    "line": 124
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "This is interesting. I tried to do the same behavior with different approach. I check RDD ID ahd Shuffle ID at DAGScheduler. Accumulators are not allowed to be updated on the same RDD ID and Shuffle ID. This simpler approach can achieve part of the function of this DataProperty accumulators.\n\nHowever, it can not handle some cases. For example, it will not pass all checks in the test `shuffled (combineByKey)`.  To put accumulators in shuffling aggregator looks like a very rare use case. As the aggregation will perform when ShuffleRDD is going to read shuffled output. It will not generate a new Shuffle ID and so that accumulator updates in shuffling aggregator will be counted again.\n\nAlthough it can't pass all test cases here, it should already cover most common use cases.\n",
    "commit": "59db4af635fd53e2723d187b60dee6856af74df4",
    "createdAt": "2016-04-12T14:00:14Z",
    "diffHunk": "@@ -0,0 +1,361 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import scala.concurrent.ExecutionContext.Implicits.global\n+import scala.ref.WeakReference\n+\n+import org.scalatest.Matchers\n+\n+import org.apache.spark.scheduler._\n+\n+\n+class DataPropertyAccumulatorSuite extends SparkFunSuite with Matchers with LocalSparkContext {\n+  test(\"single partition\") {\n+    sc = new SparkContext(\"local[2]\", \"test\")\n+    val acc : Accumulator[Int] = sc.accumulator(0, dataProperty = true)\n+\n+    val a = sc.parallelize(1 to 20, 1)\n+    val b = a.map{x => acc += x; x}\n+    b.cache()\n+    b.count()\n+    acc.value should be (210)\n+  }\n+\n+  test(\"adding only the first element per partition should work even if partition is empty\") {\n+    sc = new SparkContext(\"local[2]\", \"test\")\n+    val acc: Accumulator[Int] = sc.accumulator(0, dataProperty = true)\n+    val a = sc.parallelize(1 to 20, 30)\n+    val b = a.mapPartitions{itr =>\n+      acc += 1\n+      itr\n+    }\n+    b.count()\n+    acc.value should be (30)\n+  }\n+\n+  test(\"shuffled (combineByKey)\") {\n+    sc = new SparkContext(\"local[2]\", \"test\")\n+    val a = sc.parallelize(1 to 40, 5)\n+    val buckets = 4\n+    val b = a.map{x => ((x % buckets), x)}\n+    val inputs = List(b, b.repartition(10), b.partitionBy(new HashPartitioner(5))).map(_.cache())\n+    val mapSideCombines = List(true, false)\n+    inputs.foreach { input =>\n+      mapSideCombines.foreach { mapSideCombine =>\n+        val accs = (1 to 4).map(x => sc.accumulator(0, dataProperty = true)).toList\n+        val raccs = (1 to 4).map(x => sc.accumulator(0, dataProperty = false)).toList\n+        val List(acc, acc1, acc2, acc3) = accs\n+        val List(racc, racc1, racc2, racc3) = raccs\n+        val c = input.combineByKey(\n+          (x: Int) => {acc1 += 1; acc += 1; racc1 += 1; racc += 1; x},\n+          {(a: Int, b: Int) => acc2 += 1; acc += 1; racc2 += 1; racc += 1; (a + b)},\n+          {(a: Int, b: Int) => acc3 += 1; acc += 1; racc3 += 1; racc += 1; (a + b)},\n+          new HashPartitioner(10),\n+          mapSideCombine)\n+        val d = input.combineByKey(\n+          (x: Int) => {acc1 += 1; acc += 1; x},\n+          {(a: Int, b: Int) => acc2 += 1; acc += 1; (a + b)},\n+          {(a: Int, b: Int) => acc3 += 1; acc += 1; (a + b)},\n+          new HashPartitioner(2),\n+          mapSideCombine)\n+        val e = d.map{x => acc += 1; x}\n+        c.count()\n+        // If our partitioner is known then we should only create\n+        // one combiner for each key value. Otherwise we should\n+        // create at least that many combiners.\n+        if (input.partitioner.isDefined) {\n+          acc1.value should be (buckets)\n+        } else {\n+          acc1.value should be >= (buckets)\n+        }\n+        if (input.partitioner.isDefined) {\n+          acc2.value should be > (0)\n+        } else if (mapSideCombine) {\n+          acc3.value should be > (0)\n+        } else {\n+          acc2.value should be > (0)\n+          acc3.value should be (0)\n+        }\n+        acc.value should be (acc1.value + acc2.value + acc3.value)\n+        val oldValues = accs.map(_.value)\n+        // For one action the data property accumulators and regular should have the same value.\n+        accs.map(_.value) should be (raccs.map(_.value))\n+        c.count()\n+        accs.map(_.value) should be (oldValues)",
    "line": 124
  }, {
    "author": {
      "login": "squito"
    },
    "body": "@viirya sorry I am commenting really late here.  Do you still have your implementation?  It would be interesting to compare.\n\nIf I understand right, you are proposing that `TaskResult` would include which RDDs had been computed.  Then when the results are merged on the driver, you would know updates to count and which not to.   Because of corner cases w/ things like `rdd.take()` and `rdd.coalesce()`, I think you'd still need to handle some of the trickier details done here, eg. track whether a partition is fully read or not with iterator wrapping (and the performance effect it has).\n\nI'm pretty sure you'd also lose the ability to handle an accumulator that was shared across multiple rdds:\n\n``` scala\n\nval acc = ...\nval rddA = ...\nval rddB = rddA.map {...} // use acc in here\nrddB.take(1000)\nval rddC = rddB.map {...} // use acc here also\nrddC.count()\n```\n\nthe problem is, if you only get the final accumulator value in your task updates, you wont' know how much of that value to increment by.  You won't know which parts were already counted, and which ones weren't.  That example is somewhat contrived, but there are lots of trickier cases like concurrently executing jobs w/ shared RDDs etc.  A major goal is to provide very consistent semantics, and I think we need to make sure you get sane values out in all those cases.\n\nHonestly I can't think of a great use case for data-property accumulators inside a combineByKey.  All the uses I can think inside combine-by-key are really compute-property (eg., measuring how much map-side combining am I doing, say by having an accumulator track how many times the record size > X in the map-side).  And the data-property accumulators I'd use would be immediately after or before the combineByKey.  But if we can add it, its nice to have, since I'd hate to have weird restrictions on where you can use them.  probably there are good use cases which I can't think of at the moment.\n",
    "commit": "59db4af635fd53e2723d187b60dee6856af74df4",
    "createdAt": "2016-10-12T21:23:27Z",
    "diffHunk": "@@ -0,0 +1,361 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import scala.concurrent.ExecutionContext.Implicits.global\n+import scala.ref.WeakReference\n+\n+import org.scalatest.Matchers\n+\n+import org.apache.spark.scheduler._\n+\n+\n+class DataPropertyAccumulatorSuite extends SparkFunSuite with Matchers with LocalSparkContext {\n+  test(\"single partition\") {\n+    sc = new SparkContext(\"local[2]\", \"test\")\n+    val acc : Accumulator[Int] = sc.accumulator(0, dataProperty = true)\n+\n+    val a = sc.parallelize(1 to 20, 1)\n+    val b = a.map{x => acc += x; x}\n+    b.cache()\n+    b.count()\n+    acc.value should be (210)\n+  }\n+\n+  test(\"adding only the first element per partition should work even if partition is empty\") {\n+    sc = new SparkContext(\"local[2]\", \"test\")\n+    val acc: Accumulator[Int] = sc.accumulator(0, dataProperty = true)\n+    val a = sc.parallelize(1 to 20, 30)\n+    val b = a.mapPartitions{itr =>\n+      acc += 1\n+      itr\n+    }\n+    b.count()\n+    acc.value should be (30)\n+  }\n+\n+  test(\"shuffled (combineByKey)\") {\n+    sc = new SparkContext(\"local[2]\", \"test\")\n+    val a = sc.parallelize(1 to 40, 5)\n+    val buckets = 4\n+    val b = a.map{x => ((x % buckets), x)}\n+    val inputs = List(b, b.repartition(10), b.partitionBy(new HashPartitioner(5))).map(_.cache())\n+    val mapSideCombines = List(true, false)\n+    inputs.foreach { input =>\n+      mapSideCombines.foreach { mapSideCombine =>\n+        val accs = (1 to 4).map(x => sc.accumulator(0, dataProperty = true)).toList\n+        val raccs = (1 to 4).map(x => sc.accumulator(0, dataProperty = false)).toList\n+        val List(acc, acc1, acc2, acc3) = accs\n+        val List(racc, racc1, racc2, racc3) = raccs\n+        val c = input.combineByKey(\n+          (x: Int) => {acc1 += 1; acc += 1; racc1 += 1; racc += 1; x},\n+          {(a: Int, b: Int) => acc2 += 1; acc += 1; racc2 += 1; racc += 1; (a + b)},\n+          {(a: Int, b: Int) => acc3 += 1; acc += 1; racc3 += 1; racc += 1; (a + b)},\n+          new HashPartitioner(10),\n+          mapSideCombine)\n+        val d = input.combineByKey(\n+          (x: Int) => {acc1 += 1; acc += 1; x},\n+          {(a: Int, b: Int) => acc2 += 1; acc += 1; (a + b)},\n+          {(a: Int, b: Int) => acc3 += 1; acc += 1; (a + b)},\n+          new HashPartitioner(2),\n+          mapSideCombine)\n+        val e = d.map{x => acc += 1; x}\n+        c.count()\n+        // If our partitioner is known then we should only create\n+        // one combiner for each key value. Otherwise we should\n+        // create at least that many combiners.\n+        if (input.partitioner.isDefined) {\n+          acc1.value should be (buckets)\n+        } else {\n+          acc1.value should be >= (buckets)\n+        }\n+        if (input.partitioner.isDefined) {\n+          acc2.value should be > (0)\n+        } else if (mapSideCombine) {\n+          acc3.value should be > (0)\n+        } else {\n+          acc2.value should be > (0)\n+          acc3.value should be (0)\n+        }\n+        acc.value should be (acc1.value + acc2.value + acc3.value)\n+        val oldValues = accs.map(_.value)\n+        // For one action the data property accumulators and regular should have the same value.\n+        accs.map(_.value) should be (raccs.map(_.value))\n+        c.count()\n+        accs.map(_.value) should be (oldValues)",
    "line": 124
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "@squito That is a testing and playing implementation. Seems I don't push it to remote and I can not find it now.\n",
    "commit": "59db4af635fd53e2723d187b60dee6856af74df4",
    "createdAt": "2016-11-01T00:40:00Z",
    "diffHunk": "@@ -0,0 +1,361 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import scala.concurrent.ExecutionContext.Implicits.global\n+import scala.ref.WeakReference\n+\n+import org.scalatest.Matchers\n+\n+import org.apache.spark.scheduler._\n+\n+\n+class DataPropertyAccumulatorSuite extends SparkFunSuite with Matchers with LocalSparkContext {\n+  test(\"single partition\") {\n+    sc = new SparkContext(\"local[2]\", \"test\")\n+    val acc : Accumulator[Int] = sc.accumulator(0, dataProperty = true)\n+\n+    val a = sc.parallelize(1 to 20, 1)\n+    val b = a.map{x => acc += x; x}\n+    b.cache()\n+    b.count()\n+    acc.value should be (210)\n+  }\n+\n+  test(\"adding only the first element per partition should work even if partition is empty\") {\n+    sc = new SparkContext(\"local[2]\", \"test\")\n+    val acc: Accumulator[Int] = sc.accumulator(0, dataProperty = true)\n+    val a = sc.parallelize(1 to 20, 30)\n+    val b = a.mapPartitions{itr =>\n+      acc += 1\n+      itr\n+    }\n+    b.count()\n+    acc.value should be (30)\n+  }\n+\n+  test(\"shuffled (combineByKey)\") {\n+    sc = new SparkContext(\"local[2]\", \"test\")\n+    val a = sc.parallelize(1 to 40, 5)\n+    val buckets = 4\n+    val b = a.map{x => ((x % buckets), x)}\n+    val inputs = List(b, b.repartition(10), b.partitionBy(new HashPartitioner(5))).map(_.cache())\n+    val mapSideCombines = List(true, false)\n+    inputs.foreach { input =>\n+      mapSideCombines.foreach { mapSideCombine =>\n+        val accs = (1 to 4).map(x => sc.accumulator(0, dataProperty = true)).toList\n+        val raccs = (1 to 4).map(x => sc.accumulator(0, dataProperty = false)).toList\n+        val List(acc, acc1, acc2, acc3) = accs\n+        val List(racc, racc1, racc2, racc3) = raccs\n+        val c = input.combineByKey(\n+          (x: Int) => {acc1 += 1; acc += 1; racc1 += 1; racc += 1; x},\n+          {(a: Int, b: Int) => acc2 += 1; acc += 1; racc2 += 1; racc += 1; (a + b)},\n+          {(a: Int, b: Int) => acc3 += 1; acc += 1; racc3 += 1; racc += 1; (a + b)},\n+          new HashPartitioner(10),\n+          mapSideCombine)\n+        val d = input.combineByKey(\n+          (x: Int) => {acc1 += 1; acc += 1; x},\n+          {(a: Int, b: Int) => acc2 += 1; acc += 1; (a + b)},\n+          {(a: Int, b: Int) => acc3 += 1; acc += 1; (a + b)},\n+          new HashPartitioner(2),\n+          mapSideCombine)\n+        val e = d.map{x => acc += 1; x}\n+        c.count()\n+        // If our partitioner is known then we should only create\n+        // one combiner for each key value. Otherwise we should\n+        // create at least that many combiners.\n+        if (input.partitioner.isDefined) {\n+          acc1.value should be (buckets)\n+        } else {\n+          acc1.value should be >= (buckets)\n+        }\n+        if (input.partitioner.isDefined) {\n+          acc2.value should be > (0)\n+        } else if (mapSideCombine) {\n+          acc3.value should be > (0)\n+        } else {\n+          acc2.value should be > (0)\n+          acc3.value should be (0)\n+        }\n+        acc.value should be (acc1.value + acc2.value + acc3.value)\n+        val oldValues = accs.map(_.value)\n+        // For one action the data property accumulators and regular should have the same value.\n+        accs.map(_.value) should be (raccs.map(_.value))\n+        c.count()\n+        accs.map(_.value) should be (oldValues)",
    "line": 124
  }],
  "prId": 11105
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "related to the discussion about the iterator wrapping in ShuffledRDD -- this looks like the only test case that is related to the chaining of a shufflerdd plus another rdd.map, both with data accumulators.  Is this case sufficient?\n\nI think it is sufficient, though its not totally obvious to me, at least.  If it didn't work, than after the call to `d.count()`, when you later do `e.count()`, the updates from `e` would look like they were duplicate updates from `d` and get ignored.  does that sounds right?\n",
    "commit": "59db4af635fd53e2723d187b60dee6856af74df4",
    "createdAt": "2016-11-04T03:31:16Z",
    "diffHunk": "@@ -0,0 +1,383 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import scala.concurrent.ExecutionContext.Implicits.global\n+import scala.ref.WeakReference\n+\n+import org.scalatest.Matchers\n+\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{AccumulatorContext, AccumulatorMetadata, AccumulatorV2, LongAccumulator}\n+\n+\n+class DataPropertyAccumulatorSuite extends SparkFunSuite with Matchers with LocalSparkContext {\n+  test(\"two partition old and new\") {\n+    sc = new SparkContext(\"local[2]\", \"test\")\n+    val acc = sc.longAccumulator(dataProperty = true, \"l2\")\n+\n+    val a = sc.parallelize(1 to 20, 2)\n+    val b = a.map{x => acc.add(x); x}\n+    b.cache()\n+    b.count()\n+    acc.value should be (210)\n+  }\n+\n+  test(\"single partition\") {\n+    sc = new SparkContext(\"local[2]\", \"test\")\n+    val acc = sc.longAccumulator(dataProperty = true)\n+\n+    val a = sc.parallelize(1 to 20, 1)\n+    val b = a.map{x => acc.add(x); x}\n+    b.cache()\n+    b.count()\n+    acc.value should be (210)\n+  }\n+\n+  test(\"adding only the first element per partition should work even if partition is empty\") {\n+    sc = new SparkContext(\"local[2]\", \"test\")\n+    val acc = sc.longAccumulator(dataProperty = true)\n+    val a = sc.parallelize(1 to 20, 30)\n+    val b = a.mapPartitions{itr =>\n+      acc.add(1)\n+      itr\n+    }\n+    b.count()\n+    acc.value should be (30)\n+  }\n+\n+  test(\"shuffled (combineByKey)\") {\n+    sc = new SparkContext(\"local[2]\", \"test\")\n+    val a = sc.parallelize(1L to 40L, 5)\n+    val buckets = 4\n+    val b = a.map{x => ((x % buckets), x)}\n+    val inputs = List(b, b.repartition(10), b.partitionBy(new HashPartitioner(5))).map(_.cache())\n+    val mapSideCombines = List(true, false)\n+    inputs.foreach { input =>\n+      mapSideCombines.foreach { mapSideCombine =>\n+        val accs = (1 to 4).map(x => sc.longAccumulator(dataProperty = true)).toList\n+        val raccs = (1 to 4).map(x => sc.longAccumulator(dataProperty = false)).toList\n+        val List(acc, acc1, acc2, acc3) = accs\n+        val List(racc, racc1, racc2, racc3) = raccs\n+        val c = input.combineByKey(\n+          (x: Long) => {acc1.add(1); acc.add(1); racc1.add(1); racc.add(1); x},\n+          {(a: Long, b: Long) => acc2.add(1); acc.add(1); racc2.add(1); racc.add(1); (a + b)},\n+          {(a: Long, b: Long) => acc3.add(1); acc.add(1); racc3.add(1); racc.add(1); (a + b)},\n+          new HashPartitioner(10),\n+          mapSideCombine)\n+        val d = input.combineByKey(\n+          (x: Long) => {acc1.add(1); acc.add(1); x},\n+          {(a: Long, b: Long) => acc2.add(1); acc.add(1); (a + b)},\n+          {(a: Long, b: Long) => acc3.add(1); acc.add(1); (a + b)},\n+          new HashPartitioner(2),\n+          mapSideCombine)\n+        val e = d.map{x => acc.add(1); x}"
  }, {
    "author": {
      "login": "holdenk"
    },
    "body": "Well there are two ways it could fail, `e.count()` look like duplicates from d - or if the `insertAll` wasn't happening the updates from the read side of `d` would look like they were coming from `e` and we would get double counting.\n",
    "commit": "59db4af635fd53e2723d187b60dee6856af74df4",
    "createdAt": "2016-11-04T06:03:20Z",
    "diffHunk": "@@ -0,0 +1,383 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import scala.concurrent.ExecutionContext.Implicits.global\n+import scala.ref.WeakReference\n+\n+import org.scalatest.Matchers\n+\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{AccumulatorContext, AccumulatorMetadata, AccumulatorV2, LongAccumulator}\n+\n+\n+class DataPropertyAccumulatorSuite extends SparkFunSuite with Matchers with LocalSparkContext {\n+  test(\"two partition old and new\") {\n+    sc = new SparkContext(\"local[2]\", \"test\")\n+    val acc = sc.longAccumulator(dataProperty = true, \"l2\")\n+\n+    val a = sc.parallelize(1 to 20, 2)\n+    val b = a.map{x => acc.add(x); x}\n+    b.cache()\n+    b.count()\n+    acc.value should be (210)\n+  }\n+\n+  test(\"single partition\") {\n+    sc = new SparkContext(\"local[2]\", \"test\")\n+    val acc = sc.longAccumulator(dataProperty = true)\n+\n+    val a = sc.parallelize(1 to 20, 1)\n+    val b = a.map{x => acc.add(x); x}\n+    b.cache()\n+    b.count()\n+    acc.value should be (210)\n+  }\n+\n+  test(\"adding only the first element per partition should work even if partition is empty\") {\n+    sc = new SparkContext(\"local[2]\", \"test\")\n+    val acc = sc.longAccumulator(dataProperty = true)\n+    val a = sc.parallelize(1 to 20, 30)\n+    val b = a.mapPartitions{itr =>\n+      acc.add(1)\n+      itr\n+    }\n+    b.count()\n+    acc.value should be (30)\n+  }\n+\n+  test(\"shuffled (combineByKey)\") {\n+    sc = new SparkContext(\"local[2]\", \"test\")\n+    val a = sc.parallelize(1L to 40L, 5)\n+    val buckets = 4\n+    val b = a.map{x => ((x % buckets), x)}\n+    val inputs = List(b, b.repartition(10), b.partitionBy(new HashPartitioner(5))).map(_.cache())\n+    val mapSideCombines = List(true, false)\n+    inputs.foreach { input =>\n+      mapSideCombines.foreach { mapSideCombine =>\n+        val accs = (1 to 4).map(x => sc.longAccumulator(dataProperty = true)).toList\n+        val raccs = (1 to 4).map(x => sc.longAccumulator(dataProperty = false)).toList\n+        val List(acc, acc1, acc2, acc3) = accs\n+        val List(racc, racc1, racc2, racc3) = raccs\n+        val c = input.combineByKey(\n+          (x: Long) => {acc1.add(1); acc.add(1); racc1.add(1); racc.add(1); x},\n+          {(a: Long, b: Long) => acc2.add(1); acc.add(1); racc2.add(1); racc.add(1); (a + b)},\n+          {(a: Long, b: Long) => acc3.add(1); acc.add(1); racc3.add(1); racc.add(1); (a + b)},\n+          new HashPartitioner(10),\n+          mapSideCombine)\n+        val d = input.combineByKey(\n+          (x: Long) => {acc1.add(1); acc.add(1); x},\n+          {(a: Long, b: Long) => acc2.add(1); acc.add(1); (a + b)},\n+          {(a: Long, b: Long) => acc3.add(1); acc.add(1); (a + b)},\n+          new HashPartitioner(2),\n+          mapSideCombine)\n+        val e = d.map{x => acc.add(1); x}"
  }],
  "prId": 11105
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "A higher-level comment on these tests -- they are good at providing coverage over different cases, but they aren't always clear what is being tested and whether tests are redundant with each other.  For example, I'm not really sure if both \"map + count + count + map + count\" and \"map+map+count\" are necessary.  Its also *really* hard to follow \"shuffle (combineByKey)\", though the test itself is good at testing some complex conditions.",
    "commit": "59db4af635fd53e2723d187b60dee6856af74df4",
    "createdAt": "2016-12-02T18:14:49Z",
    "diffHunk": "@@ -0,0 +1,395 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import scala.concurrent.ExecutionContext.Implicits.global\n+import scala.ref.WeakReference\n+\n+import org.scalatest.Matchers\n+\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{AccumulatorContext, AccumulatorMetadata, AccumulatorV2, LongAccumulator}\n+\n+\n+class DataPropertyAccumulatorSuite extends SparkFunSuite with Matchers with LocalSparkContext {",
    "line": 29
  }, {
    "author": {
      "login": "holdenk"
    },
    "body": "That sounds like a good plan, I'll try and give the tests some more descriptive names (or where that isn't enough explain in comments some more about the functionality they are testing).",
    "commit": "59db4af635fd53e2723d187b60dee6856af74df4",
    "createdAt": "2016-12-03T11:56:30Z",
    "diffHunk": "@@ -0,0 +1,395 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark\n+\n+import scala.concurrent.ExecutionContext.Implicits.global\n+import scala.ref.WeakReference\n+\n+import org.scalatest.Matchers\n+\n+import org.apache.spark.scheduler._\n+import org.apache.spark.util.{AccumulatorContext, AccumulatorMetadata, AccumulatorV2, LongAccumulator}\n+\n+\n+class DataPropertyAccumulatorSuite extends SparkFunSuite with Matchers with LocalSparkContext {",
    "line": 29
  }],
  "prId": 11105
}]