[{
  "comments": [{
    "author": {
      "login": "liutang123"
    },
    "body": "```\r\ntestIgnoreEmptySplits(\r\n       data = Array.empty[Tuple2[String, String]],\r\n       actualPartitionNum = 1,\r\n       expectedPartitionNum = 0)\r\n```\r\n=>\r\n```\r\ntestIgnoreEmptySplits(\r\n       data = Array.empty[(String, String)],\r\n       actualPartitionNum = 1,\r\n       expectedPartitionNum = 0)\r\n```",
    "commit": "bcb3dbd178650f3c6bf54af32b0b1029b89286dd",
    "createdAt": "2017-10-16T11:41:54Z",
    "diffHunk": "@@ -549,9 +551,11 @@ class FileSuite extends SparkFunSuite with LocalSparkContext {\n       expectedPartitionNum = 2)\n   }\n \n-  test(\"spark.files.ignoreEmptySplits work correctly (new Hadoop API)\") {\n+  test(\"spark.hadoopRDD.ignoreEmptySplits work correctly (new Hadoop API)\") {\n     val conf = new SparkConf()\n-    conf.setAppName(\"test\").setMaster(\"local\").set(IGNORE_EMPTY_SPLITS, true)\n+      .setAppName(\"test\")\n+      .setMaster(\"local\")\n+      .set(HADOOP_RDD_IGNORE_EMPTY_SPLITS, true)\n     sc = new SparkContext(conf)\n \n     def testIgnoreEmptySplits(",
    "line": 22
  }],
  "prId": 19504
}]