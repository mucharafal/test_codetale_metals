[{
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "imports should be ordered alphabetically, so this should go after the org.apache.spark.scheduler ones\n",
    "commit": "54e6658d937f2da1c7ab9a2f8412e3a0cfc420b3",
    "createdAt": "2014-11-06T02:11:02Z",
    "diffHunk": "@@ -17,8 +17,11 @@\n \n package org.apache.spark.metrics\n \n+import org.apache.hadoop.io.{Text, LongWritable}\n+import org.apache.hadoop.mapreduce.lib.input.{TextInputFormat => NewTextInputFormat}\n import org.scalatest.FunSuite\n \n+import org.apache.spark.util.Utils"
  }],
  "prId": 3120
}, {
  "comments": [{
    "author": {
      "login": "kayousterhout"
    },
    "body": "Is one of these supposed to be bytesRead?\n",
    "commit": "54e6658d937f2da1c7ab9a2f8412e3a0cfc420b3",
    "createdAt": "2014-11-06T21:57:13Z",
    "diffHunk": "@@ -27,50 +30,92 @@ import scala.collection.mutable.ArrayBuffer\n import java.io.{FileWriter, PrintWriter, File}\n \n class InputMetricsSuite extends FunSuite with SharedSparkContext {\n-  test(\"input metrics when reading text file with single split\") {\n-    val file = new File(getClass.getSimpleName + \".txt\")\n-    val pw = new PrintWriter(new FileWriter(file))\n-    pw.println(\"some stuff\")\n-    pw.println(\"some other stuff\")\n-    pw.println(\"yet more stuff\")\n-    pw.println(\"too much stuff\")\n+\n+  @transient var tmpDir: File = _\n+  @transient var tmpFile: File = _\n+  @transient var tmpFilePath: String = _\n+\n+  override def beforeAll() {\n+    super.beforeAll()\n+\n+    tmpDir = Utils.createTempDir()\n+    val testTempDir = new File(tmpDir, \"test\")\n+    testTempDir.mkdir()\n+\n+    tmpFile = new File(testTempDir, getClass.getSimpleName + \".txt\")\n+    val pw = new PrintWriter(new FileWriter(tmpFile))\n+    for (x <- 1 to 1000000) {\n+      pw.println(\"s\")\n+    }\n     pw.close()\n-    file.deleteOnExit()\n \n-    val taskBytesRead = new ArrayBuffer[Long]()\n-    sc.addSparkListener(new SparkListener() {\n-      override def onTaskEnd(taskEnd: SparkListenerTaskEnd) {\n-        taskBytesRead += taskEnd.taskMetrics.inputMetrics.get.bytesRead\n-      }\n-    })\n-    sc.textFile(\"file://\" + file.getAbsolutePath, 2).count()\n+    // Path to tmpFile\n+    tmpFilePath = \"file://\" + tmpFile.getAbsolutePath\n+  }\n \n-    // Wait for task end events to come in\n-    sc.listenerBus.waitUntilEmpty(500)\n-    assert(taskBytesRead.length == 2)\n-    assert(taskBytesRead.sum >= file.length())\n+  override def afterAll() {\n+    super.afterAll()\n+    Utils.deleteRecursively(tmpDir)\n   }\n \n-  test(\"input metrics when reading text file with multiple splits\") {\n-    val file = new File(getClass.getSimpleName + \".txt\")\n-    val pw = new PrintWriter(new FileWriter(file))\n-    for (i <- 0 until 10000) {\n-      pw.println(\"some stuff\")\n+  test(\"input metrics for old hadoop with coalesce\") {\n+    val bytesRead = runAndReturnBytesRead {\n+      sc.textFile(tmpFilePath, 4).count()\n     }\n-    pw.close()\n-    file.deleteOnExit()\n+    val bytesRead2 = runAndReturnBytesRead {\n+      sc.textFile(tmpFilePath, 4).coalesce(2).count()\n+    }\n+    assert(bytesRead2 == bytesRead)\n+    assert(bytesRead2 >= tmpFile.length())\n+  }\n+\n+  test(\"input metrics with cache and coalesce\") {\n+    // prime the cache manager\n+    val rdd = sc.textFile(tmpFilePath, 4).cache()\n+    rdd.collect()\n+\n+    val bytesRead = runAndReturnBytesRead {\n+      rdd.count()\n+    }\n+    val bytesRead2 = runAndReturnBytesRead {\n+      rdd.coalesce(4).count()\n+    }\n+\n+    // for count and coelesce, the same bytes should be read.\n+    assert(bytesRead2 >= bytesRead2)"
  }, {
    "author": {
      "login": "ksakellis"
    },
    "body": "yes. my bad. \n",
    "commit": "54e6658d937f2da1c7ab9a2f8412e3a0cfc420b3",
    "createdAt": "2014-11-12T07:51:41Z",
    "diffHunk": "@@ -27,50 +30,92 @@ import scala.collection.mutable.ArrayBuffer\n import java.io.{FileWriter, PrintWriter, File}\n \n class InputMetricsSuite extends FunSuite with SharedSparkContext {\n-  test(\"input metrics when reading text file with single split\") {\n-    val file = new File(getClass.getSimpleName + \".txt\")\n-    val pw = new PrintWriter(new FileWriter(file))\n-    pw.println(\"some stuff\")\n-    pw.println(\"some other stuff\")\n-    pw.println(\"yet more stuff\")\n-    pw.println(\"too much stuff\")\n+\n+  @transient var tmpDir: File = _\n+  @transient var tmpFile: File = _\n+  @transient var tmpFilePath: String = _\n+\n+  override def beforeAll() {\n+    super.beforeAll()\n+\n+    tmpDir = Utils.createTempDir()\n+    val testTempDir = new File(tmpDir, \"test\")\n+    testTempDir.mkdir()\n+\n+    tmpFile = new File(testTempDir, getClass.getSimpleName + \".txt\")\n+    val pw = new PrintWriter(new FileWriter(tmpFile))\n+    for (x <- 1 to 1000000) {\n+      pw.println(\"s\")\n+    }\n     pw.close()\n-    file.deleteOnExit()\n \n-    val taskBytesRead = new ArrayBuffer[Long]()\n-    sc.addSparkListener(new SparkListener() {\n-      override def onTaskEnd(taskEnd: SparkListenerTaskEnd) {\n-        taskBytesRead += taskEnd.taskMetrics.inputMetrics.get.bytesRead\n-      }\n-    })\n-    sc.textFile(\"file://\" + file.getAbsolutePath, 2).count()\n+    // Path to tmpFile\n+    tmpFilePath = \"file://\" + tmpFile.getAbsolutePath\n+  }\n \n-    // Wait for task end events to come in\n-    sc.listenerBus.waitUntilEmpty(500)\n-    assert(taskBytesRead.length == 2)\n-    assert(taskBytesRead.sum >= file.length())\n+  override def afterAll() {\n+    super.afterAll()\n+    Utils.deleteRecursively(tmpDir)\n   }\n \n-  test(\"input metrics when reading text file with multiple splits\") {\n-    val file = new File(getClass.getSimpleName + \".txt\")\n-    val pw = new PrintWriter(new FileWriter(file))\n-    for (i <- 0 until 10000) {\n-      pw.println(\"some stuff\")\n+  test(\"input metrics for old hadoop with coalesce\") {\n+    val bytesRead = runAndReturnBytesRead {\n+      sc.textFile(tmpFilePath, 4).count()\n     }\n-    pw.close()\n-    file.deleteOnExit()\n+    val bytesRead2 = runAndReturnBytesRead {\n+      sc.textFile(tmpFilePath, 4).coalesce(2).count()\n+    }\n+    assert(bytesRead2 == bytesRead)\n+    assert(bytesRead2 >= tmpFile.length())\n+  }\n+\n+  test(\"input metrics with cache and coalesce\") {\n+    // prime the cache manager\n+    val rdd = sc.textFile(tmpFilePath, 4).cache()\n+    rdd.collect()\n+\n+    val bytesRead = runAndReturnBytesRead {\n+      rdd.count()\n+    }\n+    val bytesRead2 = runAndReturnBytesRead {\n+      rdd.coalesce(4).count()\n+    }\n+\n+    // for count and coelesce, the same bytes should be read.\n+    assert(bytesRead2 >= bytesRead2)"
  }],
  "prId": 3120
}, {
  "comments": [{
    "author": {
      "login": "kayousterhout"
    },
    "body": "Can you add a non-zero check here too?\n",
    "commit": "54e6658d937f2da1c7ab9a2f8412e3a0cfc420b3",
    "createdAt": "2014-11-06T22:10:00Z",
    "diffHunk": "@@ -27,50 +30,92 @@ import scala.collection.mutable.ArrayBuffer\n import java.io.{FileWriter, PrintWriter, File}\n \n class InputMetricsSuite extends FunSuite with SharedSparkContext {\n-  test(\"input metrics when reading text file with single split\") {\n-    val file = new File(getClass.getSimpleName + \".txt\")\n-    val pw = new PrintWriter(new FileWriter(file))\n-    pw.println(\"some stuff\")\n-    pw.println(\"some other stuff\")\n-    pw.println(\"yet more stuff\")\n-    pw.println(\"too much stuff\")\n+\n+  @transient var tmpDir: File = _\n+  @transient var tmpFile: File = _\n+  @transient var tmpFilePath: String = _\n+\n+  override def beforeAll() {\n+    super.beforeAll()\n+\n+    tmpDir = Utils.createTempDir()\n+    val testTempDir = new File(tmpDir, \"test\")\n+    testTempDir.mkdir()\n+\n+    tmpFile = new File(testTempDir, getClass.getSimpleName + \".txt\")\n+    val pw = new PrintWriter(new FileWriter(tmpFile))\n+    for (x <- 1 to 1000000) {\n+      pw.println(\"s\")\n+    }\n     pw.close()\n-    file.deleteOnExit()\n \n-    val taskBytesRead = new ArrayBuffer[Long]()\n-    sc.addSparkListener(new SparkListener() {\n-      override def onTaskEnd(taskEnd: SparkListenerTaskEnd) {\n-        taskBytesRead += taskEnd.taskMetrics.inputMetrics.get.bytesRead\n-      }\n-    })\n-    sc.textFile(\"file://\" + file.getAbsolutePath, 2).count()\n+    // Path to tmpFile\n+    tmpFilePath = \"file://\" + tmpFile.getAbsolutePath\n+  }\n \n-    // Wait for task end events to come in\n-    sc.listenerBus.waitUntilEmpty(500)\n-    assert(taskBytesRead.length == 2)\n-    assert(taskBytesRead.sum >= file.length())\n+  override def afterAll() {\n+    super.afterAll()\n+    Utils.deleteRecursively(tmpDir)\n   }\n \n-  test(\"input metrics when reading text file with multiple splits\") {\n-    val file = new File(getClass.getSimpleName + \".txt\")\n-    val pw = new PrintWriter(new FileWriter(file))\n-    for (i <- 0 until 10000) {\n-      pw.println(\"some stuff\")\n+  test(\"input metrics for old hadoop with coalesce\") {\n+    val bytesRead = runAndReturnBytesRead {\n+      sc.textFile(tmpFilePath, 4).count()\n     }\n-    pw.close()\n-    file.deleteOnExit()\n+    val bytesRead2 = runAndReturnBytesRead {\n+      sc.textFile(tmpFilePath, 4).coalesce(2).count()\n+    }\n+    assert(bytesRead2 == bytesRead)\n+    assert(bytesRead2 >= tmpFile.length())"
  }, {
    "author": {
      "login": "ksakellis"
    },
    "body": "Can you elaborate on this? What should we check is non-zero?\n",
    "commit": "54e6658d937f2da1c7ab9a2f8412e3a0cfc420b3",
    "createdAt": "2014-11-12T20:21:07Z",
    "diffHunk": "@@ -27,50 +30,92 @@ import scala.collection.mutable.ArrayBuffer\n import java.io.{FileWriter, PrintWriter, File}\n \n class InputMetricsSuite extends FunSuite with SharedSparkContext {\n-  test(\"input metrics when reading text file with single split\") {\n-    val file = new File(getClass.getSimpleName + \".txt\")\n-    val pw = new PrintWriter(new FileWriter(file))\n-    pw.println(\"some stuff\")\n-    pw.println(\"some other stuff\")\n-    pw.println(\"yet more stuff\")\n-    pw.println(\"too much stuff\")\n+\n+  @transient var tmpDir: File = _\n+  @transient var tmpFile: File = _\n+  @transient var tmpFilePath: String = _\n+\n+  override def beforeAll() {\n+    super.beforeAll()\n+\n+    tmpDir = Utils.createTempDir()\n+    val testTempDir = new File(tmpDir, \"test\")\n+    testTempDir.mkdir()\n+\n+    tmpFile = new File(testTempDir, getClass.getSimpleName + \".txt\")\n+    val pw = new PrintWriter(new FileWriter(tmpFile))\n+    for (x <- 1 to 1000000) {\n+      pw.println(\"s\")\n+    }\n     pw.close()\n-    file.deleteOnExit()\n \n-    val taskBytesRead = new ArrayBuffer[Long]()\n-    sc.addSparkListener(new SparkListener() {\n-      override def onTaskEnd(taskEnd: SparkListenerTaskEnd) {\n-        taskBytesRead += taskEnd.taskMetrics.inputMetrics.get.bytesRead\n-      }\n-    })\n-    sc.textFile(\"file://\" + file.getAbsolutePath, 2).count()\n+    // Path to tmpFile\n+    tmpFilePath = \"file://\" + tmpFile.getAbsolutePath\n+  }\n \n-    // Wait for task end events to come in\n-    sc.listenerBus.waitUntilEmpty(500)\n-    assert(taskBytesRead.length == 2)\n-    assert(taskBytesRead.sum >= file.length())\n+  override def afterAll() {\n+    super.afterAll()\n+    Utils.deleteRecursively(tmpDir)\n   }\n \n-  test(\"input metrics when reading text file with multiple splits\") {\n-    val file = new File(getClass.getSimpleName + \".txt\")\n-    val pw = new PrintWriter(new FileWriter(file))\n-    for (i <- 0 until 10000) {\n-      pw.println(\"some stuff\")\n+  test(\"input metrics for old hadoop with coalesce\") {\n+    val bytesRead = runAndReturnBytesRead {\n+      sc.textFile(tmpFilePath, 4).count()\n     }\n-    pw.close()\n-    file.deleteOnExit()\n+    val bytesRead2 = runAndReturnBytesRead {\n+      sc.textFile(tmpFilePath, 4).coalesce(2).count()\n+    }\n+    assert(bytesRead2 == bytesRead)\n+    assert(bytesRead2 >= tmpFile.length())"
  }, {
    "author": {
      "login": "kayousterhout"
    },
    "body": "bytesRead2 and bytesRead\n",
    "commit": "54e6658d937f2da1c7ab9a2f8412e3a0cfc420b3",
    "createdAt": "2014-11-12T22:22:56Z",
    "diffHunk": "@@ -27,50 +30,92 @@ import scala.collection.mutable.ArrayBuffer\n import java.io.{FileWriter, PrintWriter, File}\n \n class InputMetricsSuite extends FunSuite with SharedSparkContext {\n-  test(\"input metrics when reading text file with single split\") {\n-    val file = new File(getClass.getSimpleName + \".txt\")\n-    val pw = new PrintWriter(new FileWriter(file))\n-    pw.println(\"some stuff\")\n-    pw.println(\"some other stuff\")\n-    pw.println(\"yet more stuff\")\n-    pw.println(\"too much stuff\")\n+\n+  @transient var tmpDir: File = _\n+  @transient var tmpFile: File = _\n+  @transient var tmpFilePath: String = _\n+\n+  override def beforeAll() {\n+    super.beforeAll()\n+\n+    tmpDir = Utils.createTempDir()\n+    val testTempDir = new File(tmpDir, \"test\")\n+    testTempDir.mkdir()\n+\n+    tmpFile = new File(testTempDir, getClass.getSimpleName + \".txt\")\n+    val pw = new PrintWriter(new FileWriter(tmpFile))\n+    for (x <- 1 to 1000000) {\n+      pw.println(\"s\")\n+    }\n     pw.close()\n-    file.deleteOnExit()\n \n-    val taskBytesRead = new ArrayBuffer[Long]()\n-    sc.addSparkListener(new SparkListener() {\n-      override def onTaskEnd(taskEnd: SparkListenerTaskEnd) {\n-        taskBytesRead += taskEnd.taskMetrics.inputMetrics.get.bytesRead\n-      }\n-    })\n-    sc.textFile(\"file://\" + file.getAbsolutePath, 2).count()\n+    // Path to tmpFile\n+    tmpFilePath = \"file://\" + tmpFile.getAbsolutePath\n+  }\n \n-    // Wait for task end events to come in\n-    sc.listenerBus.waitUntilEmpty(500)\n-    assert(taskBytesRead.length == 2)\n-    assert(taskBytesRead.sum >= file.length())\n+  override def afterAll() {\n+    super.afterAll()\n+    Utils.deleteRecursively(tmpDir)\n   }\n \n-  test(\"input metrics when reading text file with multiple splits\") {\n-    val file = new File(getClass.getSimpleName + \".txt\")\n-    val pw = new PrintWriter(new FileWriter(file))\n-    for (i <- 0 until 10000) {\n-      pw.println(\"some stuff\")\n+  test(\"input metrics for old hadoop with coalesce\") {\n+    val bytesRead = runAndReturnBytesRead {\n+      sc.textFile(tmpFilePath, 4).count()\n     }\n-    pw.close()\n-    file.deleteOnExit()\n+    val bytesRead2 = runAndReturnBytesRead {\n+      sc.textFile(tmpFilePath, 4).coalesce(2).count()\n+    }\n+    assert(bytesRead2 == bytesRead)\n+    assert(bytesRead2 >= tmpFile.length())"
  }],
  "prId": 3120
}, {
  "comments": [{
    "author": {
      "login": "kayousterhout"
    },
    "body": "What's this doing?\n",
    "commit": "54e6658d937f2da1c7ab9a2f8412e3a0cfc420b3",
    "createdAt": "2014-11-06T22:10:22Z",
    "diffHunk": "@@ -27,50 +30,92 @@ import scala.collection.mutable.ArrayBuffer\n import java.io.{FileWriter, PrintWriter, File}\n \n class InputMetricsSuite extends FunSuite with SharedSparkContext {\n-  test(\"input metrics when reading text file with single split\") {\n-    val file = new File(getClass.getSimpleName + \".txt\")\n-    val pw = new PrintWriter(new FileWriter(file))\n-    pw.println(\"some stuff\")\n-    pw.println(\"some other stuff\")\n-    pw.println(\"yet more stuff\")\n-    pw.println(\"too much stuff\")\n+\n+  @transient var tmpDir: File = _\n+  @transient var tmpFile: File = _\n+  @transient var tmpFilePath: String = _\n+\n+  override def beforeAll() {\n+    super.beforeAll()\n+\n+    tmpDir = Utils.createTempDir()\n+    val testTempDir = new File(tmpDir, \"test\")\n+    testTempDir.mkdir()\n+\n+    tmpFile = new File(testTempDir, getClass.getSimpleName + \".txt\")\n+    val pw = new PrintWriter(new FileWriter(tmpFile))\n+    for (x <- 1 to 1000000) {\n+      pw.println(\"s\")\n+    }\n     pw.close()\n-    file.deleteOnExit()\n \n-    val taskBytesRead = new ArrayBuffer[Long]()\n-    sc.addSparkListener(new SparkListener() {\n-      override def onTaskEnd(taskEnd: SparkListenerTaskEnd) {\n-        taskBytesRead += taskEnd.taskMetrics.inputMetrics.get.bytesRead\n-      }\n-    })\n-    sc.textFile(\"file://\" + file.getAbsolutePath, 2).count()\n+    // Path to tmpFile\n+    tmpFilePath = \"file://\" + tmpFile.getAbsolutePath\n+  }\n \n-    // Wait for task end events to come in\n-    sc.listenerBus.waitUntilEmpty(500)\n-    assert(taskBytesRead.length == 2)\n-    assert(taskBytesRead.sum >= file.length())\n+  override def afterAll() {\n+    super.afterAll()\n+    Utils.deleteRecursively(tmpDir)\n   }\n \n-  test(\"input metrics when reading text file with multiple splits\") {\n-    val file = new File(getClass.getSimpleName + \".txt\")\n-    val pw = new PrintWriter(new FileWriter(file))\n-    for (i <- 0 until 10000) {\n-      pw.println(\"some stuff\")\n+  test(\"input metrics for old hadoop with coalesce\") {\n+    val bytesRead = runAndReturnBytesRead {\n+      sc.textFile(tmpFilePath, 4).count()\n     }\n-    pw.close()\n-    file.deleteOnExit()\n+    val bytesRead2 = runAndReturnBytesRead {\n+      sc.textFile(tmpFilePath, 4).coalesce(2).count()\n+    }\n+    assert(bytesRead2 == bytesRead)\n+    assert(bytesRead2 >= tmpFile.length())\n+  }\n+\n+  test(\"input metrics with cache and coalesce\") {\n+    // prime the cache manager\n+    val rdd = sc.textFile(tmpFilePath, 4).cache()\n+    rdd.collect()\n+\n+    val bytesRead = runAndReturnBytesRead {\n+      rdd.count()\n+    }\n+    val bytesRead2 = runAndReturnBytesRead {\n+      rdd.coalesce(4).count()\n+    }\n+\n+    // for count and coelesce, the same bytes should be read.\n+    assert(bytesRead2 >= bytesRead2)\n+  }\n \n+  test(\"input metrics for new Hadoop API with coalesce\") {\n+    val bytesRead = runAndReturnBytesRead {\n+      sc.newAPIHadoopFile(tmpFilePath, classOf[NewTextInputFormat], classOf[LongWritable],\n+        classOf[Text]).count()\n+    }\n+    val bytesRead2 = runAndReturnBytesRead {\n+      sc.newAPIHadoopFile(tmpFilePath, classOf[NewTextInputFormat], classOf[LongWritable],\n+        classOf[Text]).coalesce(5).count()\n+    }\n+    assert(bytesRead2 == bytesRead)\n+    assert(bytesRead >= tmpFile.length())\n+  }\n+\n+  test(\"input metrics when reading text file\") {\n+    val bytesRead = runAndReturnBytesRead {\n+      sc.textFile(tmpFilePath, 2).count()\n+    }\n+    assert(bytesRead >= tmpFile.length())\n+  }\n+\n+  private def runAndReturnBytesRead(job : => Unit): Long = {\n     val taskBytesRead = new ArrayBuffer[Long]()\n     sc.addSparkListener(new SparkListener() {\n       override def onTaskEnd(taskEnd: SparkListenerTaskEnd) {\n         taskBytesRead += taskEnd.taskMetrics.inputMetrics.get.bytesRead\n       }\n     })\n-    sc.textFile(\"file://\" + file.getAbsolutePath, 2).count()\n \n-    // Wait for task end events to come in\n+    job"
  }, {
    "author": {
      "login": "ksakellis"
    },
    "body": "This will run the job. The point of this method is to run the closure and wait until the metrics are reported and return them. This specific line is what will execute the closure.\n",
    "commit": "54e6658d937f2da1c7ab9a2f8412e3a0cfc420b3",
    "createdAt": "2014-11-12T20:22:16Z",
    "diffHunk": "@@ -27,50 +30,92 @@ import scala.collection.mutable.ArrayBuffer\n import java.io.{FileWriter, PrintWriter, File}\n \n class InputMetricsSuite extends FunSuite with SharedSparkContext {\n-  test(\"input metrics when reading text file with single split\") {\n-    val file = new File(getClass.getSimpleName + \".txt\")\n-    val pw = new PrintWriter(new FileWriter(file))\n-    pw.println(\"some stuff\")\n-    pw.println(\"some other stuff\")\n-    pw.println(\"yet more stuff\")\n-    pw.println(\"too much stuff\")\n+\n+  @transient var tmpDir: File = _\n+  @transient var tmpFile: File = _\n+  @transient var tmpFilePath: String = _\n+\n+  override def beforeAll() {\n+    super.beforeAll()\n+\n+    tmpDir = Utils.createTempDir()\n+    val testTempDir = new File(tmpDir, \"test\")\n+    testTempDir.mkdir()\n+\n+    tmpFile = new File(testTempDir, getClass.getSimpleName + \".txt\")\n+    val pw = new PrintWriter(new FileWriter(tmpFile))\n+    for (x <- 1 to 1000000) {\n+      pw.println(\"s\")\n+    }\n     pw.close()\n-    file.deleteOnExit()\n \n-    val taskBytesRead = new ArrayBuffer[Long]()\n-    sc.addSparkListener(new SparkListener() {\n-      override def onTaskEnd(taskEnd: SparkListenerTaskEnd) {\n-        taskBytesRead += taskEnd.taskMetrics.inputMetrics.get.bytesRead\n-      }\n-    })\n-    sc.textFile(\"file://\" + file.getAbsolutePath, 2).count()\n+    // Path to tmpFile\n+    tmpFilePath = \"file://\" + tmpFile.getAbsolutePath\n+  }\n \n-    // Wait for task end events to come in\n-    sc.listenerBus.waitUntilEmpty(500)\n-    assert(taskBytesRead.length == 2)\n-    assert(taskBytesRead.sum >= file.length())\n+  override def afterAll() {\n+    super.afterAll()\n+    Utils.deleteRecursively(tmpDir)\n   }\n \n-  test(\"input metrics when reading text file with multiple splits\") {\n-    val file = new File(getClass.getSimpleName + \".txt\")\n-    val pw = new PrintWriter(new FileWriter(file))\n-    for (i <- 0 until 10000) {\n-      pw.println(\"some stuff\")\n+  test(\"input metrics for old hadoop with coalesce\") {\n+    val bytesRead = runAndReturnBytesRead {\n+      sc.textFile(tmpFilePath, 4).count()\n     }\n-    pw.close()\n-    file.deleteOnExit()\n+    val bytesRead2 = runAndReturnBytesRead {\n+      sc.textFile(tmpFilePath, 4).coalesce(2).count()\n+    }\n+    assert(bytesRead2 == bytesRead)\n+    assert(bytesRead2 >= tmpFile.length())\n+  }\n+\n+  test(\"input metrics with cache and coalesce\") {\n+    // prime the cache manager\n+    val rdd = sc.textFile(tmpFilePath, 4).cache()\n+    rdd.collect()\n+\n+    val bytesRead = runAndReturnBytesRead {\n+      rdd.count()\n+    }\n+    val bytesRead2 = runAndReturnBytesRead {\n+      rdd.coalesce(4).count()\n+    }\n+\n+    // for count and coelesce, the same bytes should be read.\n+    assert(bytesRead2 >= bytesRead2)\n+  }\n \n+  test(\"input metrics for new Hadoop API with coalesce\") {\n+    val bytesRead = runAndReturnBytesRead {\n+      sc.newAPIHadoopFile(tmpFilePath, classOf[NewTextInputFormat], classOf[LongWritable],\n+        classOf[Text]).count()\n+    }\n+    val bytesRead2 = runAndReturnBytesRead {\n+      sc.newAPIHadoopFile(tmpFilePath, classOf[NewTextInputFormat], classOf[LongWritable],\n+        classOf[Text]).coalesce(5).count()\n+    }\n+    assert(bytesRead2 == bytesRead)\n+    assert(bytesRead >= tmpFile.length())\n+  }\n+\n+  test(\"input metrics when reading text file\") {\n+    val bytesRead = runAndReturnBytesRead {\n+      sc.textFile(tmpFilePath, 2).count()\n+    }\n+    assert(bytesRead >= tmpFile.length())\n+  }\n+\n+  private def runAndReturnBytesRead(job : => Unit): Long = {\n     val taskBytesRead = new ArrayBuffer[Long]()\n     sc.addSparkListener(new SparkListener() {\n       override def onTaskEnd(taskEnd: SparkListenerTaskEnd) {\n         taskBytesRead += taskEnd.taskMetrics.inputMetrics.get.bytesRead\n       }\n     })\n-    sc.textFile(\"file://\" + file.getAbsolutePath, 2).count()\n \n-    // Wait for task end events to come in\n+    job"
  }, {
    "author": {
      "login": "kayousterhout"
    },
    "body": "Ah sorry I see -- the style guide mandates using parentheses (i.e., \"job()\") for methods like this that have side-effects: http://docs.scala-lang.org/style/method-invocation.html\n",
    "commit": "54e6658d937f2da1c7ab9a2f8412e3a0cfc420b3",
    "createdAt": "2014-11-12T22:28:59Z",
    "diffHunk": "@@ -27,50 +30,92 @@ import scala.collection.mutable.ArrayBuffer\n import java.io.{FileWriter, PrintWriter, File}\n \n class InputMetricsSuite extends FunSuite with SharedSparkContext {\n-  test(\"input metrics when reading text file with single split\") {\n-    val file = new File(getClass.getSimpleName + \".txt\")\n-    val pw = new PrintWriter(new FileWriter(file))\n-    pw.println(\"some stuff\")\n-    pw.println(\"some other stuff\")\n-    pw.println(\"yet more stuff\")\n-    pw.println(\"too much stuff\")\n+\n+  @transient var tmpDir: File = _\n+  @transient var tmpFile: File = _\n+  @transient var tmpFilePath: String = _\n+\n+  override def beforeAll() {\n+    super.beforeAll()\n+\n+    tmpDir = Utils.createTempDir()\n+    val testTempDir = new File(tmpDir, \"test\")\n+    testTempDir.mkdir()\n+\n+    tmpFile = new File(testTempDir, getClass.getSimpleName + \".txt\")\n+    val pw = new PrintWriter(new FileWriter(tmpFile))\n+    for (x <- 1 to 1000000) {\n+      pw.println(\"s\")\n+    }\n     pw.close()\n-    file.deleteOnExit()\n \n-    val taskBytesRead = new ArrayBuffer[Long]()\n-    sc.addSparkListener(new SparkListener() {\n-      override def onTaskEnd(taskEnd: SparkListenerTaskEnd) {\n-        taskBytesRead += taskEnd.taskMetrics.inputMetrics.get.bytesRead\n-      }\n-    })\n-    sc.textFile(\"file://\" + file.getAbsolutePath, 2).count()\n+    // Path to tmpFile\n+    tmpFilePath = \"file://\" + tmpFile.getAbsolutePath\n+  }\n \n-    // Wait for task end events to come in\n-    sc.listenerBus.waitUntilEmpty(500)\n-    assert(taskBytesRead.length == 2)\n-    assert(taskBytesRead.sum >= file.length())\n+  override def afterAll() {\n+    super.afterAll()\n+    Utils.deleteRecursively(tmpDir)\n   }\n \n-  test(\"input metrics when reading text file with multiple splits\") {\n-    val file = new File(getClass.getSimpleName + \".txt\")\n-    val pw = new PrintWriter(new FileWriter(file))\n-    for (i <- 0 until 10000) {\n-      pw.println(\"some stuff\")\n+  test(\"input metrics for old hadoop with coalesce\") {\n+    val bytesRead = runAndReturnBytesRead {\n+      sc.textFile(tmpFilePath, 4).count()\n     }\n-    pw.close()\n-    file.deleteOnExit()\n+    val bytesRead2 = runAndReturnBytesRead {\n+      sc.textFile(tmpFilePath, 4).coalesce(2).count()\n+    }\n+    assert(bytesRead2 == bytesRead)\n+    assert(bytesRead2 >= tmpFile.length())\n+  }\n+\n+  test(\"input metrics with cache and coalesce\") {\n+    // prime the cache manager\n+    val rdd = sc.textFile(tmpFilePath, 4).cache()\n+    rdd.collect()\n+\n+    val bytesRead = runAndReturnBytesRead {\n+      rdd.count()\n+    }\n+    val bytesRead2 = runAndReturnBytesRead {\n+      rdd.coalesce(4).count()\n+    }\n+\n+    // for count and coelesce, the same bytes should be read.\n+    assert(bytesRead2 >= bytesRead2)\n+  }\n \n+  test(\"input metrics for new Hadoop API with coalesce\") {\n+    val bytesRead = runAndReturnBytesRead {\n+      sc.newAPIHadoopFile(tmpFilePath, classOf[NewTextInputFormat], classOf[LongWritable],\n+        classOf[Text]).count()\n+    }\n+    val bytesRead2 = runAndReturnBytesRead {\n+      sc.newAPIHadoopFile(tmpFilePath, classOf[NewTextInputFormat], classOf[LongWritable],\n+        classOf[Text]).coalesce(5).count()\n+    }\n+    assert(bytesRead2 == bytesRead)\n+    assert(bytesRead >= tmpFile.length())\n+  }\n+\n+  test(\"input metrics when reading text file\") {\n+    val bytesRead = runAndReturnBytesRead {\n+      sc.textFile(tmpFilePath, 2).count()\n+    }\n+    assert(bytesRead >= tmpFile.length())\n+  }\n+\n+  private def runAndReturnBytesRead(job : => Unit): Long = {\n     val taskBytesRead = new ArrayBuffer[Long]()\n     sc.addSparkListener(new SparkListener() {\n       override def onTaskEnd(taskEnd: SparkListenerTaskEnd) {\n         taskBytesRead += taskEnd.taskMetrics.inputMetrics.get.bytesRead\n       }\n     })\n-    sc.textFile(\"file://\" + file.getAbsolutePath, 2).count()\n \n-    // Wait for task end events to come in\n+    job"
  }, {
    "author": {
      "login": "ksakellis"
    },
    "body": "ok, ill add the parentheses so as to follow the style guide.\n",
    "commit": "54e6658d937f2da1c7ab9a2f8412e3a0cfc420b3",
    "createdAt": "2014-11-13T05:59:43Z",
    "diffHunk": "@@ -27,50 +30,92 @@ import scala.collection.mutable.ArrayBuffer\n import java.io.{FileWriter, PrintWriter, File}\n \n class InputMetricsSuite extends FunSuite with SharedSparkContext {\n-  test(\"input metrics when reading text file with single split\") {\n-    val file = new File(getClass.getSimpleName + \".txt\")\n-    val pw = new PrintWriter(new FileWriter(file))\n-    pw.println(\"some stuff\")\n-    pw.println(\"some other stuff\")\n-    pw.println(\"yet more stuff\")\n-    pw.println(\"too much stuff\")\n+\n+  @transient var tmpDir: File = _\n+  @transient var tmpFile: File = _\n+  @transient var tmpFilePath: String = _\n+\n+  override def beforeAll() {\n+    super.beforeAll()\n+\n+    tmpDir = Utils.createTempDir()\n+    val testTempDir = new File(tmpDir, \"test\")\n+    testTempDir.mkdir()\n+\n+    tmpFile = new File(testTempDir, getClass.getSimpleName + \".txt\")\n+    val pw = new PrintWriter(new FileWriter(tmpFile))\n+    for (x <- 1 to 1000000) {\n+      pw.println(\"s\")\n+    }\n     pw.close()\n-    file.deleteOnExit()\n \n-    val taskBytesRead = new ArrayBuffer[Long]()\n-    sc.addSparkListener(new SparkListener() {\n-      override def onTaskEnd(taskEnd: SparkListenerTaskEnd) {\n-        taskBytesRead += taskEnd.taskMetrics.inputMetrics.get.bytesRead\n-      }\n-    })\n-    sc.textFile(\"file://\" + file.getAbsolutePath, 2).count()\n+    // Path to tmpFile\n+    tmpFilePath = \"file://\" + tmpFile.getAbsolutePath\n+  }\n \n-    // Wait for task end events to come in\n-    sc.listenerBus.waitUntilEmpty(500)\n-    assert(taskBytesRead.length == 2)\n-    assert(taskBytesRead.sum >= file.length())\n+  override def afterAll() {\n+    super.afterAll()\n+    Utils.deleteRecursively(tmpDir)\n   }\n \n-  test(\"input metrics when reading text file with multiple splits\") {\n-    val file = new File(getClass.getSimpleName + \".txt\")\n-    val pw = new PrintWriter(new FileWriter(file))\n-    for (i <- 0 until 10000) {\n-      pw.println(\"some stuff\")\n+  test(\"input metrics for old hadoop with coalesce\") {\n+    val bytesRead = runAndReturnBytesRead {\n+      sc.textFile(tmpFilePath, 4).count()\n     }\n-    pw.close()\n-    file.deleteOnExit()\n+    val bytesRead2 = runAndReturnBytesRead {\n+      sc.textFile(tmpFilePath, 4).coalesce(2).count()\n+    }\n+    assert(bytesRead2 == bytesRead)\n+    assert(bytesRead2 >= tmpFile.length())\n+  }\n+\n+  test(\"input metrics with cache and coalesce\") {\n+    // prime the cache manager\n+    val rdd = sc.textFile(tmpFilePath, 4).cache()\n+    rdd.collect()\n+\n+    val bytesRead = runAndReturnBytesRead {\n+      rdd.count()\n+    }\n+    val bytesRead2 = runAndReturnBytesRead {\n+      rdd.coalesce(4).count()\n+    }\n+\n+    // for count and coelesce, the same bytes should be read.\n+    assert(bytesRead2 >= bytesRead2)\n+  }\n \n+  test(\"input metrics for new Hadoop API with coalesce\") {\n+    val bytesRead = runAndReturnBytesRead {\n+      sc.newAPIHadoopFile(tmpFilePath, classOf[NewTextInputFormat], classOf[LongWritable],\n+        classOf[Text]).count()\n+    }\n+    val bytesRead2 = runAndReturnBytesRead {\n+      sc.newAPIHadoopFile(tmpFilePath, classOf[NewTextInputFormat], classOf[LongWritable],\n+        classOf[Text]).coalesce(5).count()\n+    }\n+    assert(bytesRead2 == bytesRead)\n+    assert(bytesRead >= tmpFile.length())\n+  }\n+\n+  test(\"input metrics when reading text file\") {\n+    val bytesRead = runAndReturnBytesRead {\n+      sc.textFile(tmpFilePath, 2).count()\n+    }\n+    assert(bytesRead >= tmpFile.length())\n+  }\n+\n+  private def runAndReturnBytesRead(job : => Unit): Long = {\n     val taskBytesRead = new ArrayBuffer[Long]()\n     sc.addSparkListener(new SparkListener() {\n       override def onTaskEnd(taskEnd: SparkListenerTaskEnd) {\n         taskBytesRead += taskEnd.taskMetrics.inputMetrics.get.bytesRead\n       }\n     })\n-    sc.textFile(\"file://\" + file.getAbsolutePath, 2).count()\n \n-    // Wait for task end events to come in\n+    job"
  }],
  "prId": 3120
}]