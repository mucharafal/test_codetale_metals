[{
  "comments": [{
    "author": {
      "login": "tgravescs"
    },
    "body": "this answers my question about this.  I wasn't sure it made much sense but seems ok for reuse. Was wondering if you had a specific use case in mind but sounds like not.",
    "commit": "37ad680ec33ec6afac9d031897a549321e782d9c",
    "createdAt": "2019-10-29T14:02:29Z",
    "diffHunk": "@@ -0,0 +1,240 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.plugin\n+\n+import java.io.File\n+import java.nio.charset.StandardCharsets\n+import java.util.{Map => JMap}\n+\n+import scala.collection.JavaConverters._\n+import scala.concurrent.duration._\n+\n+import com.codahale.metrics.Gauge\n+import com.google.common.io.Files\n+import org.mockito.ArgumentMatchers.{any, eq => meq}\n+import org.mockito.Mockito.{mock, spy, verify, when}\n+import org.scalatest.BeforeAndAfterEach\n+import org.scalatest.concurrent.Eventually.{eventually, interval, timeout}\n+\n+import org.apache.spark.{ExecutorPlugin => _, _}\n+import org.apache.spark.api.plugin._\n+import org.apache.spark.internal.config._\n+import org.apache.spark.launcher.SparkLauncher\n+import org.apache.spark.util.Utils\n+\n+class PluginContainerSuite extends SparkFunSuite with BeforeAndAfterEach with LocalSparkContext {\n+\n+  override def afterEach(): Unit = {\n+    TestSparkPlugin.reset()\n+    super.afterEach()\n+  }\n+\n+  test(\"plugin initialization and communication\") {\n+    val conf = new SparkConf()\n+      .setAppName(getClass().getName())\n+      .set(SparkLauncher.SPARK_MASTER, \"local[1]\")\n+      .set(PLUGINS, Seq(classOf[TestSparkPlugin].getName()))\n+\n+    TestSparkPlugin.extraConf = Map(\"foo\" -> \"bar\", \"bar\" -> \"baz\").asJava\n+\n+    sc = new SparkContext(conf)\n+\n+    assert(TestSparkPlugin.driverPlugin != null)\n+    verify(TestSparkPlugin.driverPlugin).init(meq(sc), any())\n+\n+    assert(TestSparkPlugin.executorPlugin != null)\n+    verify(TestSparkPlugin.executorPlugin).init(any(), meq(TestSparkPlugin.extraConf))\n+\n+    assert(TestSparkPlugin.executorContext != null)\n+\n+    // One way messages don't block, so need to loop checking whether it arrives.\n+    TestSparkPlugin.executorContext.send(\"oneway\")\n+    eventually(timeout(10.seconds), interval(10.millis)) {\n+      verify(TestSparkPlugin.driverPlugin).receive(\"oneway\")\n+    }\n+\n+    assert(TestSparkPlugin.executorContext.ask(\"ask\") === \"reply\")\n+\n+    val err = intercept[Exception] {\n+      TestSparkPlugin.executorContext.ask(\"unknown message\")\n+    }\n+    assert(err.getMessage().contains(\"unknown message\"))\n+\n+    // It should be possible for the driver plugin to send a message to itself, even if that doesn't\n+    // make a whole lot of sense. It at least allows the same context class to be used on both",
    "line": 79
  }],
  "prId": 26170
}]