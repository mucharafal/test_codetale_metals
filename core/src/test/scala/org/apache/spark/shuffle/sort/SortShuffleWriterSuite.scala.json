[{
  "comments": [{
    "author": {
      "login": "attilapiros"
    },
    "body": "The `outputFile` can be deleted as only used in `afterEach`.",
    "commit": "05febb0d99c17dc8226dfd3d7013b17b696b2f6f",
    "createdAt": "2019-03-22T21:32:38Z",
    "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import java.io.File\n+\n+import org.mockito.Mockito._\n+import org.mockito.MockitoAnnotations\n+import org.scalatest.BeforeAndAfterEach\n+import org.apache.spark.{ShuffleDependency, _}\n+import org.apache.spark.memory.MemoryTestingUtils\n+import org.apache.spark.serializer.JavaSerializer\n+import org.apache.spark.shuffle.{BaseShuffleHandle, IndexShuffleBlockResolver}\n+import org.apache.spark.util.Utils\n+\n+class SortShuffleWriterSuite extends SparkFunSuite with BeforeAndAfterEach {\n+\n+  private val shuffleId = 0\n+  private val numMaps = 5\n+  private var outputFile: File = _"
  }],
  "prId": 24080
}, {
  "comments": [{
    "author": {
      "login": "attilapiros"
    },
    "body": "`sc` is never stopped this leads to resource leaks during test running.",
    "commit": "05febb0d99c17dc8226dfd3d7013b17b696b2f6f",
    "createdAt": "2019-03-22T21:33:15Z",
    "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import java.io.File\n+\n+import org.mockito.Mockito._\n+import org.mockito.MockitoAnnotations\n+import org.scalatest.BeforeAndAfterEach\n+import org.apache.spark.{ShuffleDependency, _}\n+import org.apache.spark.memory.MemoryTestingUtils\n+import org.apache.spark.serializer.JavaSerializer\n+import org.apache.spark.shuffle.{BaseShuffleHandle, IndexShuffleBlockResolver}\n+import org.apache.spark.util.Utils\n+\n+class SortShuffleWriterSuite extends SparkFunSuite with BeforeAndAfterEach {\n+\n+  private val shuffleId = 0\n+  private val numMaps = 5\n+  private var outputFile: File = _\n+  private val conf: SparkConf = new SparkConf(loadDefaults = false)\n+  private val sc = new SparkContext(\"local-cluster[1,1,1024]\", \"test\", conf)"
  }, {
    "author": {
      "login": "srowen"
    },
    "body": "@wangjiaochun for example can you used the `SharedSparkContext` trait?",
    "commit": "05febb0d99c17dc8226dfd3d7013b17b696b2f6f",
    "createdAt": "2019-03-25T14:28:31Z",
    "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import java.io.File\n+\n+import org.mockito.Mockito._\n+import org.mockito.MockitoAnnotations\n+import org.scalatest.BeforeAndAfterEach\n+import org.apache.spark.{ShuffleDependency, _}\n+import org.apache.spark.memory.MemoryTestingUtils\n+import org.apache.spark.serializer.JavaSerializer\n+import org.apache.spark.shuffle.{BaseShuffleHandle, IndexShuffleBlockResolver}\n+import org.apache.spark.util.Utils\n+\n+class SortShuffleWriterSuite extends SparkFunSuite with BeforeAndAfterEach {\n+\n+  private val shuffleId = 0\n+  private val numMaps = 5\n+  private var outputFile: File = _\n+  private val conf: SparkConf = new SparkConf(loadDefaults = false)\n+  private val sc = new SparkContext(\"local-cluster[1,1,1024]\", \"test\", conf)"
  }, {
    "author": {
      "login": "attilapiros"
    },
    "body": "The `SharedSparkContext` is a really good idea as if the `write with some records` test would be executed first and after that the `write empty iterator` I think the assert might fail in the second test as between the two tests the `shuffleWriteMetrics` are not cleaned up.",
    "commit": "05febb0d99c17dc8226dfd3d7013b17b696b2f6f",
    "createdAt": "2019-03-25T15:03:00Z",
    "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import java.io.File\n+\n+import org.mockito.Mockito._\n+import org.mockito.MockitoAnnotations\n+import org.scalatest.BeforeAndAfterEach\n+import org.apache.spark.{ShuffleDependency, _}\n+import org.apache.spark.memory.MemoryTestingUtils\n+import org.apache.spark.serializer.JavaSerializer\n+import org.apache.spark.shuffle.{BaseShuffleHandle, IndexShuffleBlockResolver}\n+import org.apache.spark.util.Utils\n+\n+class SortShuffleWriterSuite extends SparkFunSuite with BeforeAndAfterEach {\n+\n+  private val shuffleId = 0\n+  private val numMaps = 5\n+  private var outputFile: File = _\n+  private val conf: SparkConf = new SparkConf(loadDefaults = false)\n+  private val sc = new SparkContext(\"local-cluster[1,1,1024]\", \"test\", conf)"
  }, {
    "author": {
      "login": "wangjiaochun"
    },
    "body": "Thanks ,I will revise it again according to your opinion. @srowen  @attilapiros ",
    "commit": "05febb0d99c17dc8226dfd3d7013b17b696b2f6f",
    "createdAt": "2019-03-26T08:46:53Z",
    "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import java.io.File\n+\n+import org.mockito.Mockito._\n+import org.mockito.MockitoAnnotations\n+import org.scalatest.BeforeAndAfterEach\n+import org.apache.spark.{ShuffleDependency, _}\n+import org.apache.spark.memory.MemoryTestingUtils\n+import org.apache.spark.serializer.JavaSerializer\n+import org.apache.spark.shuffle.{BaseShuffleHandle, IndexShuffleBlockResolver}\n+import org.apache.spark.util.Utils\n+\n+class SortShuffleWriterSuite extends SparkFunSuite with BeforeAndAfterEach {\n+\n+  private val shuffleId = 0\n+  private val numMaps = 5\n+  private var outputFile: File = _\n+  private val conf: SparkConf = new SparkConf(loadDefaults = false)\n+  private val sc = new SparkContext(\"local-cluster[1,1,1024]\", \"test\", conf)"
  }],
  "prId": 24080
}, {
  "comments": [{
    "author": {
      "login": "attilapiros"
    },
    "body": "`shuffleBlockResolver` never stopped... ",
    "commit": "05febb0d99c17dc8226dfd3d7013b17b696b2f6f",
    "createdAt": "2019-03-22T21:34:09Z",
    "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import java.io.File\n+\n+import org.mockito.Mockito._\n+import org.mockito.MockitoAnnotations\n+import org.scalatest.BeforeAndAfterEach\n+import org.apache.spark.{ShuffleDependency, _}\n+import org.apache.spark.memory.MemoryTestingUtils\n+import org.apache.spark.serializer.JavaSerializer\n+import org.apache.spark.shuffle.{BaseShuffleHandle, IndexShuffleBlockResolver}\n+import org.apache.spark.util.Utils\n+\n+class SortShuffleWriterSuite extends SparkFunSuite with BeforeAndAfterEach {\n+\n+  private val shuffleId = 0\n+  private val numMaps = 5\n+  private var outputFile: File = _\n+  private val conf: SparkConf = new SparkConf(loadDefaults = false)\n+  private val sc = new SparkContext(\"local-cluster[1,1,1024]\", \"test\", conf)\n+  private var shuffleHandle: BaseShuffleHandle[Int, Int, Int] = _\n+  private val shuffleBlockResolver = new IndexShuffleBlockResolver(conf)",
    "line": 36
  }],
  "prId": 24080
}, {
  "comments": [{
    "author": {
      "login": "attilapiros"
    },
    "body": "delete the comment `// MapId` as you are already using named arguments it does not add additional value ",
    "commit": "05febb0d99c17dc8226dfd3d7013b17b696b2f6f",
    "createdAt": "2019-03-22T21:36:36Z",
    "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import java.io.File\n+\n+import org.mockito.Mockito._\n+import org.mockito.MockitoAnnotations\n+import org.scalatest.BeforeAndAfterEach\n+import org.apache.spark.{ShuffleDependency, _}\n+import org.apache.spark.memory.MemoryTestingUtils\n+import org.apache.spark.serializer.JavaSerializer\n+import org.apache.spark.shuffle.{BaseShuffleHandle, IndexShuffleBlockResolver}\n+import org.apache.spark.util.Utils\n+\n+class SortShuffleWriterSuite extends SparkFunSuite with BeforeAndAfterEach {\n+\n+  private val shuffleId = 0\n+  private val numMaps = 5\n+  private var outputFile: File = _\n+  private val conf: SparkConf = new SparkConf(loadDefaults = false)\n+  private val sc = new SparkContext(\"local-cluster[1,1,1024]\", \"test\", conf)\n+  private var shuffleHandle: BaseShuffleHandle[Int, Int, Int] = _\n+  private val shuffleBlockResolver = new IndexShuffleBlockResolver(conf)\n+  private val serializer = new JavaSerializer(conf)\n+  private val context = MemoryTestingUtils.fakeTaskContext(sc.env)\n+\n+  override def beforeEach(): Unit = {\n+    super.beforeEach()\n+    MockitoAnnotations.initMocks(this)\n+    val partitioner = new Partitioner() {\n+      def numPartitions = numMaps\n+      def getPartition(key: Any) = Utils.nonNegativeMod(key.hashCode, numPartitions)\n+    }\n+    shuffleHandle = {\n+      val dependency = mock(classOf[ShuffleDependency[Int, Int, Int]])\n+      when(dependency.partitioner).thenReturn(partitioner)\n+      when(dependency.serializer).thenReturn(serializer)\n+      when(dependency.aggregator).thenReturn(None)\n+      when(dependency.keyOrdering).thenReturn(None)\n+      new BaseShuffleHandle(shuffleId, numMaps = numMaps, dependency)\n+    }\n+  }\n+\n+  override def afterEach(): Unit = {\n+    try {\n+      if (outputFile != null) {\n+        Utils.deleteRecursively(outputFile)\n+      }\n+    } finally {\n+      super.afterEach()\n+    }\n+  }\n+\n+  test(\"write empty iterator\") {\n+    val writer = new SortShuffleWriter[Int, Int, Int](\n+      shuffleBlockResolver,\n+      shuffleHandle,\n+      mapId = 1, // MapId"
  }],
  "prId": 24080
}, {
  "comments": [{
    "author": {
      "login": "attilapiros"
    },
    "body": "Use named argument instead of this comment.",
    "commit": "05febb0d99c17dc8226dfd3d7013b17b696b2f6f",
    "createdAt": "2019-03-22T21:37:34Z",
    "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import java.io.File\n+\n+import org.mockito.Mockito._\n+import org.mockito.MockitoAnnotations\n+import org.scalatest.BeforeAndAfterEach\n+import org.apache.spark.{ShuffleDependency, _}\n+import org.apache.spark.memory.MemoryTestingUtils\n+import org.apache.spark.serializer.JavaSerializer\n+import org.apache.spark.shuffle.{BaseShuffleHandle, IndexShuffleBlockResolver}\n+import org.apache.spark.util.Utils\n+\n+class SortShuffleWriterSuite extends SparkFunSuite with BeforeAndAfterEach {\n+\n+  private val shuffleId = 0\n+  private val numMaps = 5\n+  private var outputFile: File = _\n+  private val conf: SparkConf = new SparkConf(loadDefaults = false)\n+  private val sc = new SparkContext(\"local-cluster[1,1,1024]\", \"test\", conf)\n+  private var shuffleHandle: BaseShuffleHandle[Int, Int, Int] = _\n+  private val shuffleBlockResolver = new IndexShuffleBlockResolver(conf)\n+  private val serializer = new JavaSerializer(conf)\n+  private val context = MemoryTestingUtils.fakeTaskContext(sc.env)\n+\n+  override def beforeEach(): Unit = {\n+    super.beforeEach()\n+    MockitoAnnotations.initMocks(this)\n+    val partitioner = new Partitioner() {\n+      def numPartitions = numMaps\n+      def getPartition(key: Any) = Utils.nonNegativeMod(key.hashCode, numPartitions)\n+    }\n+    shuffleHandle = {\n+      val dependency = mock(classOf[ShuffleDependency[Int, Int, Int]])\n+      when(dependency.partitioner).thenReturn(partitioner)\n+      when(dependency.serializer).thenReturn(serializer)\n+      when(dependency.aggregator).thenReturn(None)\n+      when(dependency.keyOrdering).thenReturn(None)\n+      new BaseShuffleHandle(shuffleId, numMaps = numMaps, dependency)\n+    }\n+  }\n+\n+  override def afterEach(): Unit = {\n+    try {\n+      if (outputFile != null) {\n+        Utils.deleteRecursively(outputFile)\n+      }\n+    } finally {\n+      super.afterEach()\n+    }\n+  }\n+\n+  test(\"write empty iterator\") {\n+    val writer = new SortShuffleWriter[Int, Int, Int](\n+      shuffleBlockResolver,\n+      shuffleHandle,\n+      mapId = 1, // MapId\n+      context\n+    )\n+    writer.write(Iterator.empty)\n+    writer.stop( /* success = */ true)"
  }],
  "prId": 24080
}, {
  "comments": [{
    "author": {
      "login": "attilapiros"
    },
    "body": "Everywhere:\r\n- use `===` in assert instead of `==`\r\n- use `!==` in assert instead of `!=`",
    "commit": "05febb0d99c17dc8226dfd3d7013b17b696b2f6f",
    "createdAt": "2019-03-22T21:39:10Z",
    "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import java.io.File\n+\n+import org.mockito.Mockito._\n+import org.mockito.MockitoAnnotations\n+import org.scalatest.BeforeAndAfterEach\n+import org.apache.spark.{ShuffleDependency, _}\n+import org.apache.spark.memory.MemoryTestingUtils\n+import org.apache.spark.serializer.JavaSerializer\n+import org.apache.spark.shuffle.{BaseShuffleHandle, IndexShuffleBlockResolver}\n+import org.apache.spark.util.Utils\n+\n+class SortShuffleWriterSuite extends SparkFunSuite with BeforeAndAfterEach {\n+\n+  private val shuffleId = 0\n+  private val numMaps = 5\n+  private var outputFile: File = _\n+  private val conf: SparkConf = new SparkConf(loadDefaults = false)\n+  private val sc = new SparkContext(\"local-cluster[1,1,1024]\", \"test\", conf)\n+  private var shuffleHandle: BaseShuffleHandle[Int, Int, Int] = _\n+  private val shuffleBlockResolver = new IndexShuffleBlockResolver(conf)\n+  private val serializer = new JavaSerializer(conf)\n+  private val context = MemoryTestingUtils.fakeTaskContext(sc.env)\n+\n+  override def beforeEach(): Unit = {\n+    super.beforeEach()\n+    MockitoAnnotations.initMocks(this)\n+    val partitioner = new Partitioner() {\n+      def numPartitions = numMaps\n+      def getPartition(key: Any) = Utils.nonNegativeMod(key.hashCode, numPartitions)\n+    }\n+    shuffleHandle = {\n+      val dependency = mock(classOf[ShuffleDependency[Int, Int, Int]])\n+      when(dependency.partitioner).thenReturn(partitioner)\n+      when(dependency.serializer).thenReturn(serializer)\n+      when(dependency.aggregator).thenReturn(None)\n+      when(dependency.keyOrdering).thenReturn(None)\n+      new BaseShuffleHandle(shuffleId, numMaps = numMaps, dependency)\n+    }\n+  }\n+\n+  override def afterEach(): Unit = {\n+    try {\n+      if (outputFile != null) {\n+        Utils.deleteRecursively(outputFile)\n+      }\n+    } finally {\n+      super.afterEach()\n+    }\n+  }\n+\n+  test(\"write empty iterator\") {\n+    val writer = new SortShuffleWriter[Int, Int, Int](\n+      shuffleBlockResolver,\n+      shuffleHandle,\n+      mapId = 1, // MapId\n+      context\n+    )\n+    writer.write(Iterator.empty)\n+    writer.stop( /* success = */ true)\n+    val dataFile = shuffleBlockResolver.getDataFile(shuffleId, 1)\n+    assert(!dataFile.exists())\n+    assert(dataFile.length() == 0)"
  }],
  "prId": 24080
}, {
  "comments": [{
    "author": {
      "login": "attilapiros"
    },
    "body": "Named argument.",
    "commit": "05febb0d99c17dc8226dfd3d7013b17b696b2f6f",
    "createdAt": "2019-03-22T21:39:36Z",
    "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import java.io.File\n+\n+import org.mockito.Mockito._\n+import org.mockito.MockitoAnnotations\n+import org.scalatest.BeforeAndAfterEach\n+import org.apache.spark.{ShuffleDependency, _}\n+import org.apache.spark.memory.MemoryTestingUtils\n+import org.apache.spark.serializer.JavaSerializer\n+import org.apache.spark.shuffle.{BaseShuffleHandle, IndexShuffleBlockResolver}\n+import org.apache.spark.util.Utils\n+\n+class SortShuffleWriterSuite extends SparkFunSuite with BeforeAndAfterEach {\n+\n+  private val shuffleId = 0\n+  private val numMaps = 5\n+  private var outputFile: File = _\n+  private val conf: SparkConf = new SparkConf(loadDefaults = false)\n+  private val sc = new SparkContext(\"local-cluster[1,1,1024]\", \"test\", conf)\n+  private var shuffleHandle: BaseShuffleHandle[Int, Int, Int] = _\n+  private val shuffleBlockResolver = new IndexShuffleBlockResolver(conf)\n+  private val serializer = new JavaSerializer(conf)\n+  private val context = MemoryTestingUtils.fakeTaskContext(sc.env)\n+\n+  override def beforeEach(): Unit = {\n+    super.beforeEach()\n+    MockitoAnnotations.initMocks(this)\n+    val partitioner = new Partitioner() {\n+      def numPartitions = numMaps\n+      def getPartition(key: Any) = Utils.nonNegativeMod(key.hashCode, numPartitions)\n+    }\n+    shuffleHandle = {\n+      val dependency = mock(classOf[ShuffleDependency[Int, Int, Int]])\n+      when(dependency.partitioner).thenReturn(partitioner)\n+      when(dependency.serializer).thenReturn(serializer)\n+      when(dependency.aggregator).thenReturn(None)\n+      when(dependency.keyOrdering).thenReturn(None)\n+      new BaseShuffleHandle(shuffleId, numMaps = numMaps, dependency)\n+    }\n+  }\n+\n+  override def afterEach(): Unit = {\n+    try {\n+      if (outputFile != null) {\n+        Utils.deleteRecursively(outputFile)\n+      }\n+    } finally {\n+      super.afterEach()\n+    }\n+  }\n+\n+  test(\"write empty iterator\") {\n+    val writer = new SortShuffleWriter[Int, Int, Int](\n+      shuffleBlockResolver,\n+      shuffleHandle,\n+      mapId = 1, // MapId\n+      context\n+    )\n+    writer.write(Iterator.empty)\n+    writer.stop( /* success = */ true)\n+    val dataFile = shuffleBlockResolver.getDataFile(shuffleId, 1)\n+    assert(!dataFile.exists())\n+    assert(dataFile.length() == 0)\n+    assert(context.taskMetrics().shuffleWriteMetrics.bytesWritten == 0)\n+    assert(context.taskMetrics().shuffleWriteMetrics.recordsWritten == 0)\n+\n+  }\n+\n+  test(\"write with some records\") {\n+    val records = Iterator((1, 2), (2, 3), (4, 4), (6, 5))\n+    val writer = new SortShuffleWriter[Int, Int, Int](\n+      shuffleBlockResolver,\n+      shuffleHandle,\n+      mapId = 2, // MapId\n+      context\n+    )\n+    writer.write(records)\n+    writer.stop( /* success = */ true)"
  }],
  "prId": 24080
}, {
  "comments": [{
    "author": {
      "login": "attilapiros"
    },
    "body": "Delete the comment.",
    "commit": "05febb0d99c17dc8226dfd3d7013b17b696b2f6f",
    "createdAt": "2019-03-22T21:39:53Z",
    "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import java.io.File\n+\n+import org.mockito.Mockito._\n+import org.mockito.MockitoAnnotations\n+import org.scalatest.BeforeAndAfterEach\n+import org.apache.spark.{ShuffleDependency, _}\n+import org.apache.spark.memory.MemoryTestingUtils\n+import org.apache.spark.serializer.JavaSerializer\n+import org.apache.spark.shuffle.{BaseShuffleHandle, IndexShuffleBlockResolver}\n+import org.apache.spark.util.Utils\n+\n+class SortShuffleWriterSuite extends SparkFunSuite with BeforeAndAfterEach {\n+\n+  private val shuffleId = 0\n+  private val numMaps = 5\n+  private var outputFile: File = _\n+  private val conf: SparkConf = new SparkConf(loadDefaults = false)\n+  private val sc = new SparkContext(\"local-cluster[1,1,1024]\", \"test\", conf)\n+  private var shuffleHandle: BaseShuffleHandle[Int, Int, Int] = _\n+  private val shuffleBlockResolver = new IndexShuffleBlockResolver(conf)\n+  private val serializer = new JavaSerializer(conf)\n+  private val context = MemoryTestingUtils.fakeTaskContext(sc.env)\n+\n+  override def beforeEach(): Unit = {\n+    super.beforeEach()\n+    MockitoAnnotations.initMocks(this)\n+    val partitioner = new Partitioner() {\n+      def numPartitions = numMaps\n+      def getPartition(key: Any) = Utils.nonNegativeMod(key.hashCode, numPartitions)\n+    }\n+    shuffleHandle = {\n+      val dependency = mock(classOf[ShuffleDependency[Int, Int, Int]])\n+      when(dependency.partitioner).thenReturn(partitioner)\n+      when(dependency.serializer).thenReturn(serializer)\n+      when(dependency.aggregator).thenReturn(None)\n+      when(dependency.keyOrdering).thenReturn(None)\n+      new BaseShuffleHandle(shuffleId, numMaps = numMaps, dependency)\n+    }\n+  }\n+\n+  override def afterEach(): Unit = {\n+    try {\n+      if (outputFile != null) {\n+        Utils.deleteRecursively(outputFile)\n+      }\n+    } finally {\n+      super.afterEach()\n+    }\n+  }\n+\n+  test(\"write empty iterator\") {\n+    val writer = new SortShuffleWriter[Int, Int, Int](\n+      shuffleBlockResolver,\n+      shuffleHandle,\n+      mapId = 1, // MapId\n+      context\n+    )\n+    writer.write(Iterator.empty)\n+    writer.stop( /* success = */ true)\n+    val dataFile = shuffleBlockResolver.getDataFile(shuffleId, 1)\n+    assert(!dataFile.exists())\n+    assert(dataFile.length() == 0)\n+    assert(context.taskMetrics().shuffleWriteMetrics.bytesWritten == 0)\n+    assert(context.taskMetrics().shuffleWriteMetrics.recordsWritten == 0)\n+\n+  }\n+\n+  test(\"write with some records\") {\n+    val records = Iterator((1, 2), (2, 3), (4, 4), (6, 5))\n+    val writer = new SortShuffleWriter[Int, Int, Int](\n+      shuffleBlockResolver,\n+      shuffleHandle,\n+      mapId = 2, // MapId"
  }, {
    "author": {
      "login": "wangjiaochun"
    },
    "body": "Thank you for your review @attilapiros ",
    "commit": "05febb0d99c17dc8226dfd3d7013b17b696b2f6f",
    "createdAt": "2019-03-25T06:25:09Z",
    "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import java.io.File\n+\n+import org.mockito.Mockito._\n+import org.mockito.MockitoAnnotations\n+import org.scalatest.BeforeAndAfterEach\n+import org.apache.spark.{ShuffleDependency, _}\n+import org.apache.spark.memory.MemoryTestingUtils\n+import org.apache.spark.serializer.JavaSerializer\n+import org.apache.spark.shuffle.{BaseShuffleHandle, IndexShuffleBlockResolver}\n+import org.apache.spark.util.Utils\n+\n+class SortShuffleWriterSuite extends SparkFunSuite with BeforeAndAfterEach {\n+\n+  private val shuffleId = 0\n+  private val numMaps = 5\n+  private var outputFile: File = _\n+  private val conf: SparkConf = new SparkConf(loadDefaults = false)\n+  private val sc = new SparkContext(\"local-cluster[1,1,1024]\", \"test\", conf)\n+  private var shuffleHandle: BaseShuffleHandle[Int, Int, Int] = _\n+  private val shuffleBlockResolver = new IndexShuffleBlockResolver(conf)\n+  private val serializer = new JavaSerializer(conf)\n+  private val context = MemoryTestingUtils.fakeTaskContext(sc.env)\n+\n+  override def beforeEach(): Unit = {\n+    super.beforeEach()\n+    MockitoAnnotations.initMocks(this)\n+    val partitioner = new Partitioner() {\n+      def numPartitions = numMaps\n+      def getPartition(key: Any) = Utils.nonNegativeMod(key.hashCode, numPartitions)\n+    }\n+    shuffleHandle = {\n+      val dependency = mock(classOf[ShuffleDependency[Int, Int, Int]])\n+      when(dependency.partitioner).thenReturn(partitioner)\n+      when(dependency.serializer).thenReturn(serializer)\n+      when(dependency.aggregator).thenReturn(None)\n+      when(dependency.keyOrdering).thenReturn(None)\n+      new BaseShuffleHandle(shuffleId, numMaps = numMaps, dependency)\n+    }\n+  }\n+\n+  override def afterEach(): Unit = {\n+    try {\n+      if (outputFile != null) {\n+        Utils.deleteRecursively(outputFile)\n+      }\n+    } finally {\n+      super.afterEach()\n+    }\n+  }\n+\n+  test(\"write empty iterator\") {\n+    val writer = new SortShuffleWriter[Int, Int, Int](\n+      shuffleBlockResolver,\n+      shuffleHandle,\n+      mapId = 1, // MapId\n+      context\n+    )\n+    writer.write(Iterator.empty)\n+    writer.stop( /* success = */ true)\n+    val dataFile = shuffleBlockResolver.getDataFile(shuffleId, 1)\n+    assert(!dataFile.exists())\n+    assert(dataFile.length() == 0)\n+    assert(context.taskMetrics().shuffleWriteMetrics.bytesWritten == 0)\n+    assert(context.taskMetrics().shuffleWriteMetrics.recordsWritten == 0)\n+\n+  }\n+\n+  test(\"write with some records\") {\n+    val records = Iterator((1, 2), (2, 3), (4, 4), (6, 5))\n+    val writer = new SortShuffleWriter[Int, Int, Int](\n+      shuffleBlockResolver,\n+      shuffleHandle,\n+      mapId = 2, // MapId"
  }],
  "prId": 24080
}, {
  "comments": [{
    "author": {
      "login": "kiszk"
    },
    "body": "Need blank line before this line for a separation.",
    "commit": "05febb0d99c17dc8226dfd3d7013b17b696b2f6f",
    "createdAt": "2019-03-25T03:41:06Z",
    "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import java.io.File\n+\n+import org.mockito.Mockito._\n+import org.mockito.MockitoAnnotations\n+import org.scalatest.BeforeAndAfterEach\n+import org.apache.spark.{ShuffleDependency, _}"
  }, {
    "author": {
      "login": "attilapiros"
    },
    "body": "And avoid using wildcard imports, unless you are importing more than 6 entities, or implicit methods as wildcard imports make the code less robust to external changes. \r\n\r\nHere, as I see, you need only 5 things:\r\n```\r\nimport org.apache.spark.{Partitioner, ShuffleDependency, SparkConf, SparkContext, SparkFunSuite}\r\n```",
    "commit": "05febb0d99c17dc8226dfd3d7013b17b696b2f6f",
    "createdAt": "2019-03-25T10:31:30Z",
    "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import java.io.File\n+\n+import org.mockito.Mockito._\n+import org.mockito.MockitoAnnotations\n+import org.scalatest.BeforeAndAfterEach\n+import org.apache.spark.{ShuffleDependency, _}"
  }],
  "prId": 24080
}, {
  "comments": [{
    "author": {
      "login": "kiszk"
    },
    "body": "Can we compare the certain expected value instead of `!= 0` in these three lines?",
    "commit": "05febb0d99c17dc8226dfd3d7013b17b696b2f6f",
    "createdAt": "2019-03-25T03:57:43Z",
    "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import java.io.File\n+\n+import org.mockito.Mockito._\n+import org.mockito.MockitoAnnotations\n+import org.scalatest.BeforeAndAfterEach\n+import org.apache.spark.{ShuffleDependency, _}\n+import org.apache.spark.memory.MemoryTestingUtils\n+import org.apache.spark.serializer.JavaSerializer\n+import org.apache.spark.shuffle.{BaseShuffleHandle, IndexShuffleBlockResolver}\n+import org.apache.spark.util.Utils\n+\n+class SortShuffleWriterSuite extends SparkFunSuite with BeforeAndAfterEach {\n+\n+  private val shuffleId = 0\n+  private val numMaps = 5\n+  private var outputFile: File = _\n+  private val conf: SparkConf = new SparkConf(loadDefaults = false)\n+  private val sc = new SparkContext(\"local-cluster[1,1,1024]\", \"test\", conf)\n+  private var shuffleHandle: BaseShuffleHandle[Int, Int, Int] = _\n+  private val shuffleBlockResolver = new IndexShuffleBlockResolver(conf)\n+  private val serializer = new JavaSerializer(conf)\n+  private val context = MemoryTestingUtils.fakeTaskContext(sc.env)\n+\n+  override def beforeEach(): Unit = {\n+    super.beforeEach()\n+    MockitoAnnotations.initMocks(this)\n+    val partitioner = new Partitioner() {\n+      def numPartitions = numMaps\n+      def getPartition(key: Any) = Utils.nonNegativeMod(key.hashCode, numPartitions)\n+    }\n+    shuffleHandle = {\n+      val dependency = mock(classOf[ShuffleDependency[Int, Int, Int]])\n+      when(dependency.partitioner).thenReturn(partitioner)\n+      when(dependency.serializer).thenReturn(serializer)\n+      when(dependency.aggregator).thenReturn(None)\n+      when(dependency.keyOrdering).thenReturn(None)\n+      new BaseShuffleHandle(shuffleId, numMaps = numMaps, dependency)\n+    }\n+  }\n+\n+  override def afterEach(): Unit = {\n+    try {\n+      if (outputFile != null) {\n+        Utils.deleteRecursively(outputFile)\n+      }\n+    } finally {\n+      super.afterEach()\n+    }\n+  }\n+\n+  test(\"write empty iterator\") {\n+    val writer = new SortShuffleWriter[Int, Int, Int](\n+      shuffleBlockResolver,\n+      shuffleHandle,\n+      mapId = 1, // MapId\n+      context\n+    )\n+    writer.write(Iterator.empty)\n+    writer.stop( /* success = */ true)\n+    val dataFile = shuffleBlockResolver.getDataFile(shuffleId, 1)\n+    assert(!dataFile.exists())\n+    assert(dataFile.length() == 0)\n+    assert(context.taskMetrics().shuffleWriteMetrics.bytesWritten == 0)\n+    assert(context.taskMetrics().shuffleWriteMetrics.recordsWritten == 0)\n+\n+  }\n+\n+  test(\"write with some records\") {\n+    val records = Iterator((1, 2), (2, 3), (4, 4), (6, 5))\n+    val writer = new SortShuffleWriter[Int, Int, Int](\n+      shuffleBlockResolver,\n+      shuffleHandle,\n+      mapId = 2, // MapId\n+      context\n+    )\n+    writer.write(records)\n+    writer.stop( /* success = */ true)\n+    val dataFile = shuffleBlockResolver.getDataFile(shuffleId, 2)\n+    assert(dataFile.exists())\n+    assert(dataFile.length() != 0)"
  }],
  "prId": 24080
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Also, use `===` or `!==` from scalatest",
    "commit": "05febb0d99c17dc8226dfd3d7013b17b696b2f6f",
    "createdAt": "2019-03-25T04:01:52Z",
    "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import java.io.File\n+\n+import org.mockito.Mockito._\n+import org.mockito.MockitoAnnotations\n+import org.scalatest.BeforeAndAfterEach\n+import org.apache.spark.{ShuffleDependency, _}\n+import org.apache.spark.memory.MemoryTestingUtils\n+import org.apache.spark.serializer.JavaSerializer\n+import org.apache.spark.shuffle.{BaseShuffleHandle, IndexShuffleBlockResolver}\n+import org.apache.spark.util.Utils\n+\n+class SortShuffleWriterSuite extends SparkFunSuite with BeforeAndAfterEach {\n+\n+  private val shuffleId = 0\n+  private val numMaps = 5\n+  private var outputFile: File = _\n+  private val conf: SparkConf = new SparkConf(loadDefaults = false)\n+  private val sc = new SparkContext(\"local-cluster[1,1,1024]\", \"test\", conf)\n+  private var shuffleHandle: BaseShuffleHandle[Int, Int, Int] = _\n+  private val shuffleBlockResolver = new IndexShuffleBlockResolver(conf)\n+  private val serializer = new JavaSerializer(conf)\n+  private val context = MemoryTestingUtils.fakeTaskContext(sc.env)\n+\n+  override def beforeEach(): Unit = {\n+    super.beforeEach()\n+    MockitoAnnotations.initMocks(this)\n+    val partitioner = new Partitioner() {\n+      def numPartitions = numMaps\n+      def getPartition(key: Any) = Utils.nonNegativeMod(key.hashCode, numPartitions)\n+    }\n+    shuffleHandle = {\n+      val dependency = mock(classOf[ShuffleDependency[Int, Int, Int]])\n+      when(dependency.partitioner).thenReturn(partitioner)\n+      when(dependency.serializer).thenReturn(serializer)\n+      when(dependency.aggregator).thenReturn(None)\n+      when(dependency.keyOrdering).thenReturn(None)\n+      new BaseShuffleHandle(shuffleId, numMaps = numMaps, dependency)\n+    }\n+  }\n+\n+  override def afterEach(): Unit = {\n+    try {\n+      if (outputFile != null) {\n+        Utils.deleteRecursively(outputFile)\n+      }\n+    } finally {\n+      super.afterEach()\n+    }\n+  }\n+\n+  test(\"write empty iterator\") {\n+    val writer = new SortShuffleWriter[Int, Int, Int](\n+      shuffleBlockResolver,\n+      shuffleHandle,\n+      mapId = 1, // MapId\n+      context\n+    )\n+    writer.write(Iterator.empty)\n+    writer.stop( /* success = */ true)\n+    val dataFile = shuffleBlockResolver.getDataFile(shuffleId, 1)\n+    assert(!dataFile.exists())\n+    assert(dataFile.length() == 0)\n+    assert(context.taskMetrics().shuffleWriteMetrics.bytesWritten == 0)\n+    assert(context.taskMetrics().shuffleWriteMetrics.recordsWritten == 0)\n+\n+  }\n+\n+  test(\"write with some records\") {\n+    val records = Iterator((1, 2), (2, 3), (4, 4), (6, 5))\n+    val writer = new SortShuffleWriter[Int, Int, Int](\n+      shuffleBlockResolver,\n+      shuffleHandle,\n+      mapId = 2, // MapId\n+      context\n+    )\n+    writer.write(records)\n+    writer.stop( /* success = */ true)\n+    val dataFile = shuffleBlockResolver.getDataFile(shuffleId, 2)\n+    assert(dataFile.exists())\n+    assert(dataFile.length() != 0)\n+    assert(context.taskMetrics().shuffleWriteMetrics.bytesWritten != 0)"
  }],
  "prId": 24080
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "We have test support code for managing a SparkContext and temp files; you can use those rather than recreate it",
    "commit": "05febb0d99c17dc8226dfd3d7013b17b696b2f6f",
    "createdAt": "2019-03-25T04:02:19Z",
    "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import java.io.File\n+\n+import org.mockito.Mockito._\n+import org.mockito.MockitoAnnotations\n+import org.scalatest.BeforeAndAfterEach\n+import org.apache.spark.{ShuffleDependency, _}\n+import org.apache.spark.memory.MemoryTestingUtils\n+import org.apache.spark.serializer.JavaSerializer\n+import org.apache.spark.shuffle.{BaseShuffleHandle, IndexShuffleBlockResolver}\n+import org.apache.spark.util.Utils\n+\n+class SortShuffleWriterSuite extends SparkFunSuite with BeforeAndAfterEach {\n+\n+  private val shuffleId = 0\n+  private val numMaps = 5\n+  private var outputFile: File = _\n+  private val conf: SparkConf = new SparkConf(loadDefaults = false)\n+  private val sc = new SparkContext(\"local-cluster[1,1,1024]\", \"test\", conf)\n+  private var shuffleHandle: BaseShuffleHandle[Int, Int, Int] = _\n+  private val shuffleBlockResolver = new IndexShuffleBlockResolver(conf)\n+  private val serializer = new JavaSerializer(conf)\n+  private val context = MemoryTestingUtils.fakeTaskContext(sc.env)\n+\n+  override def beforeEach(): Unit = {\n+    super.beforeEach()\n+    MockitoAnnotations.initMocks(this)\n+    val partitioner = new Partitioner() {\n+      def numPartitions = numMaps\n+      def getPartition(key: Any) = Utils.nonNegativeMod(key.hashCode, numPartitions)\n+    }\n+    shuffleHandle = {\n+      val dependency = mock(classOf[ShuffleDependency[Int, Int, Int]])\n+      when(dependency.partitioner).thenReturn(partitioner)\n+      when(dependency.serializer).thenReturn(serializer)\n+      when(dependency.aggregator).thenReturn(None)\n+      when(dependency.keyOrdering).thenReturn(None)\n+      new BaseShuffleHandle(shuffleId, numMaps = numMaps, dependency)\n+    }\n+  }\n+\n+  override def afterEach(): Unit = {\n+    try {\n+      if (outputFile != null) {\n+        Utils.deleteRecursively(outputFile)"
  }, {
    "author": {
      "login": "wangjiaochun"
    },
    "body": "Sorry, I didn't find it throughout the project，Can you provide an example for me？@srowen",
    "commit": "05febb0d99c17dc8226dfd3d7013b17b696b2f6f",
    "createdAt": "2019-03-25T06:28:49Z",
    "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import java.io.File\n+\n+import org.mockito.Mockito._\n+import org.mockito.MockitoAnnotations\n+import org.scalatest.BeforeAndAfterEach\n+import org.apache.spark.{ShuffleDependency, _}\n+import org.apache.spark.memory.MemoryTestingUtils\n+import org.apache.spark.serializer.JavaSerializer\n+import org.apache.spark.shuffle.{BaseShuffleHandle, IndexShuffleBlockResolver}\n+import org.apache.spark.util.Utils\n+\n+class SortShuffleWriterSuite extends SparkFunSuite with BeforeAndAfterEach {\n+\n+  private val shuffleId = 0\n+  private val numMaps = 5\n+  private var outputFile: File = _\n+  private val conf: SparkConf = new SparkConf(loadDefaults = false)\n+  private val sc = new SparkContext(\"local-cluster[1,1,1024]\", \"test\", conf)\n+  private var shuffleHandle: BaseShuffleHandle[Int, Int, Int] = _\n+  private val shuffleBlockResolver = new IndexShuffleBlockResolver(conf)\n+  private val serializer = new JavaSerializer(conf)\n+  private val context = MemoryTestingUtils.fakeTaskContext(sc.env)\n+\n+  override def beforeEach(): Unit = {\n+    super.beforeEach()\n+    MockitoAnnotations.initMocks(this)\n+    val partitioner = new Partitioner() {\n+      def numPartitions = numMaps\n+      def getPartition(key: Any) = Utils.nonNegativeMod(key.hashCode, numPartitions)\n+    }\n+    shuffleHandle = {\n+      val dependency = mock(classOf[ShuffleDependency[Int, Int, Int]])\n+      when(dependency.partitioner).thenReturn(partitioner)\n+      when(dependency.serializer).thenReturn(serializer)\n+      when(dependency.aggregator).thenReturn(None)\n+      when(dependency.keyOrdering).thenReturn(None)\n+      new BaseShuffleHandle(shuffleId, numMaps = numMaps, dependency)\n+    }\n+  }\n+\n+  override def afterEach(): Unit = {\n+    try {\n+      if (outputFile != null) {\n+        Utils.deleteRecursively(outputFile)"
  }, {
    "author": {
      "login": "attilapiros"
    },
    "body": "@wangjiaochun I think you are looking for `org.apache.spark.util.Utils#createTempDir` which registers the created directory for cleanup in a shutdown hook (deleting all contained files too recursively). \r\n\r\nBut here as I mentioned in my comment you do not need a temporary directory here. As I see in your latest commit you have already removed it.",
    "commit": "05febb0d99c17dc8226dfd3d7013b17b696b2f6f",
    "createdAt": "2019-03-25T10:22:39Z",
    "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import java.io.File\n+\n+import org.mockito.Mockito._\n+import org.mockito.MockitoAnnotations\n+import org.scalatest.BeforeAndAfterEach\n+import org.apache.spark.{ShuffleDependency, _}\n+import org.apache.spark.memory.MemoryTestingUtils\n+import org.apache.spark.serializer.JavaSerializer\n+import org.apache.spark.shuffle.{BaseShuffleHandle, IndexShuffleBlockResolver}\n+import org.apache.spark.util.Utils\n+\n+class SortShuffleWriterSuite extends SparkFunSuite with BeforeAndAfterEach {\n+\n+  private val shuffleId = 0\n+  private val numMaps = 5\n+  private var outputFile: File = _\n+  private val conf: SparkConf = new SparkConf(loadDefaults = false)\n+  private val sc = new SparkContext(\"local-cluster[1,1,1024]\", \"test\", conf)\n+  private var shuffleHandle: BaseShuffleHandle[Int, Int, Int] = _\n+  private val shuffleBlockResolver = new IndexShuffleBlockResolver(conf)\n+  private val serializer = new JavaSerializer(conf)\n+  private val context = MemoryTestingUtils.fakeTaskContext(sc.env)\n+\n+  override def beforeEach(): Unit = {\n+    super.beforeEach()\n+    MockitoAnnotations.initMocks(this)\n+    val partitioner = new Partitioner() {\n+      def numPartitions = numMaps\n+      def getPartition(key: Any) = Utils.nonNegativeMod(key.hashCode, numPartitions)\n+    }\n+    shuffleHandle = {\n+      val dependency = mock(classOf[ShuffleDependency[Int, Int, Int]])\n+      when(dependency.partitioner).thenReturn(partitioner)\n+      when(dependency.serializer).thenReturn(serializer)\n+      when(dependency.aggregator).thenReturn(None)\n+      when(dependency.keyOrdering).thenReturn(None)\n+      new BaseShuffleHandle(shuffleId, numMaps = numMaps, dependency)\n+    }\n+  }\n+\n+  override def afterEach(): Unit = {\n+    try {\n+      if (outputFile != null) {\n+        Utils.deleteRecursively(outputFile)"
  }],
  "prId": 24080
}, {
  "comments": [{
    "author": {
      "login": "attilapiros"
    },
    "body": "You are creating the SparkContext for your suite member val `sc` but you are closing it after each tests.\r\nSame for shuffleBlockResolver.\r\nSo I think you need `afterAll()` here instead of `afterEach()` and  in the finally you should call `super.afterAll()`.",
    "commit": "05febb0d99c17dc8226dfd3d7013b17b696b2f6f",
    "createdAt": "2019-03-25T10:45:22Z",
    "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import org.mockito.Mockito._\n+import org.mockito.MockitoAnnotations\n+import org.scalatest.BeforeAndAfterEach\n+\n+import org.apache.spark.{ShuffleDependency, _}\n+import org.apache.spark.memory.MemoryTestingUtils\n+import org.apache.spark.serializer.JavaSerializer\n+import org.apache.spark.shuffle.{BaseShuffleHandle, IndexShuffleBlockResolver}\n+import org.apache.spark.util.Utils\n+\n+class SortShuffleWriterSuite extends SparkFunSuite with BeforeAndAfterEach {\n+\n+  private val shuffleId = 0\n+  private val numMaps = 5\n+  private val conf: SparkConf = new SparkConf(loadDefaults = false)\n+  private val sc = new SparkContext(\"local-cluster[1,1,1024]\", \"test\", conf)\n+  private var shuffleHandle: BaseShuffleHandle[Int, Int, Int] = _\n+  private val shuffleBlockResolver = new IndexShuffleBlockResolver(conf)\n+  private val serializer = new JavaSerializer(conf)\n+  private val context = MemoryTestingUtils.fakeTaskContext(sc.env)\n+\n+  override def beforeEach(): Unit = {\n+    super.beforeEach()\n+    MockitoAnnotations.initMocks(this)\n+    val partitioner = new Partitioner() {\n+      def numPartitions = numMaps\n+      def getPartition(key: Any) = Utils.nonNegativeMod(key.hashCode, numPartitions)\n+    }\n+    shuffleHandle = {\n+      val dependency = mock(classOf[ShuffleDependency[Int, Int, Int]])\n+      when(dependency.partitioner).thenReturn(partitioner)\n+      when(dependency.serializer).thenReturn(serializer)\n+      when(dependency.aggregator).thenReturn(None)\n+      when(dependency.keyOrdering).thenReturn(None)\n+      new BaseShuffleHandle(shuffleId, numMaps = numMaps, dependency)\n+    }\n+  }\n+\n+  override def afterEach(): Unit = {"
  }, {
    "author": {
      "login": "attilapiros"
    },
    "body": "This is till suspicious for me as `shuffleBlockResolver` is initialised as a member val (and both tests uses it) but it is closed after each tests.  ",
    "commit": "05febb0d99c17dc8226dfd3d7013b17b696b2f6f",
    "createdAt": "2019-03-26T15:35:36Z",
    "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import org.mockito.Mockito._\n+import org.mockito.MockitoAnnotations\n+import org.scalatest.BeforeAndAfterEach\n+\n+import org.apache.spark.{ShuffleDependency, _}\n+import org.apache.spark.memory.MemoryTestingUtils\n+import org.apache.spark.serializer.JavaSerializer\n+import org.apache.spark.shuffle.{BaseShuffleHandle, IndexShuffleBlockResolver}\n+import org.apache.spark.util.Utils\n+\n+class SortShuffleWriterSuite extends SparkFunSuite with BeforeAndAfterEach {\n+\n+  private val shuffleId = 0\n+  private val numMaps = 5\n+  private val conf: SparkConf = new SparkConf(loadDefaults = false)\n+  private val sc = new SparkContext(\"local-cluster[1,1,1024]\", \"test\", conf)\n+  private var shuffleHandle: BaseShuffleHandle[Int, Int, Int] = _\n+  private val shuffleBlockResolver = new IndexShuffleBlockResolver(conf)\n+  private val serializer = new JavaSerializer(conf)\n+  private val context = MemoryTestingUtils.fakeTaskContext(sc.env)\n+\n+  override def beforeEach(): Unit = {\n+    super.beforeEach()\n+    MockitoAnnotations.initMocks(this)\n+    val partitioner = new Partitioner() {\n+      def numPartitions = numMaps\n+      def getPartition(key: Any) = Utils.nonNegativeMod(key.hashCode, numPartitions)\n+    }\n+    shuffleHandle = {\n+      val dependency = mock(classOf[ShuffleDependency[Int, Int, Int]])\n+      when(dependency.partitioner).thenReturn(partitioner)\n+      when(dependency.serializer).thenReturn(serializer)\n+      when(dependency.aggregator).thenReturn(None)\n+      when(dependency.keyOrdering).thenReturn(None)\n+      new BaseShuffleHandle(shuffleId, numMaps = numMaps, dependency)\n+    }\n+  }\n+\n+  override def afterEach(): Unit = {"
  }, {
    "author": {
      "login": "wangjiaochun"
    },
    "body": "the shuffleBlockResolver.stop() is actually an empty method and do nothing. But I think it's no need to call stop. Unit test IndexShuffleBlockResolverSuite also not call stop.",
    "commit": "05febb0d99c17dc8226dfd3d7013b17b696b2f6f",
    "createdAt": "2019-03-27T00:57:24Z",
    "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import org.mockito.Mockito._\n+import org.mockito.MockitoAnnotations\n+import org.scalatest.BeforeAndAfterEach\n+\n+import org.apache.spark.{ShuffleDependency, _}\n+import org.apache.spark.memory.MemoryTestingUtils\n+import org.apache.spark.serializer.JavaSerializer\n+import org.apache.spark.shuffle.{BaseShuffleHandle, IndexShuffleBlockResolver}\n+import org.apache.spark.util.Utils\n+\n+class SortShuffleWriterSuite extends SparkFunSuite with BeforeAndAfterEach {\n+\n+  private val shuffleId = 0\n+  private val numMaps = 5\n+  private val conf: SparkConf = new SparkConf(loadDefaults = false)\n+  private val sc = new SparkContext(\"local-cluster[1,1,1024]\", \"test\", conf)\n+  private var shuffleHandle: BaseShuffleHandle[Int, Int, Int] = _\n+  private val shuffleBlockResolver = new IndexShuffleBlockResolver(conf)\n+  private val serializer = new JavaSerializer(conf)\n+  private val context = MemoryTestingUtils.fakeTaskContext(sc.env)\n+\n+  override def beforeEach(): Unit = {\n+    super.beforeEach()\n+    MockitoAnnotations.initMocks(this)\n+    val partitioner = new Partitioner() {\n+      def numPartitions = numMaps\n+      def getPartition(key: Any) = Utils.nonNegativeMod(key.hashCode, numPartitions)\n+    }\n+    shuffleHandle = {\n+      val dependency = mock(classOf[ShuffleDependency[Int, Int, Int]])\n+      when(dependency.partitioner).thenReturn(partitioner)\n+      when(dependency.serializer).thenReturn(serializer)\n+      when(dependency.aggregator).thenReturn(None)\n+      when(dependency.keyOrdering).thenReturn(None)\n+      new BaseShuffleHandle(shuffleId, numMaps = numMaps, dependency)\n+    }\n+  }\n+\n+  override def afterEach(): Unit = {"
  }, {
    "author": {
      "login": "attilapiros"
    },
    "body": "I do not agree. As `IndexShuffleBlockResolver` can be closed so calling that close is our responsibility. If somebody adds some resources to this class he can rightfully assume the resource release will be handled correctly if he put it in the `IndexShuffleBlockResolver#close` method.\r\n\r\nAnyway my suggestion is quite simple just use `afterAll()` instead of `afterEach()`. \r\nI have done this change locally and tested it, passed successfully:\r\n\r\n```\r\n[info] SortShuffleWriterSuite:\r\n[info] - write empty iterator (103 milliseconds)\r\n[info] - write with some records (146 milliseconds)\r\n```\r\n ",
    "commit": "05febb0d99c17dc8226dfd3d7013b17b696b2f6f",
    "createdAt": "2019-03-27T09:43:12Z",
    "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import org.mockito.Mockito._\n+import org.mockito.MockitoAnnotations\n+import org.scalatest.BeforeAndAfterEach\n+\n+import org.apache.spark.{ShuffleDependency, _}\n+import org.apache.spark.memory.MemoryTestingUtils\n+import org.apache.spark.serializer.JavaSerializer\n+import org.apache.spark.shuffle.{BaseShuffleHandle, IndexShuffleBlockResolver}\n+import org.apache.spark.util.Utils\n+\n+class SortShuffleWriterSuite extends SparkFunSuite with BeforeAndAfterEach {\n+\n+  private val shuffleId = 0\n+  private val numMaps = 5\n+  private val conf: SparkConf = new SparkConf(loadDefaults = false)\n+  private val sc = new SparkContext(\"local-cluster[1,1,1024]\", \"test\", conf)\n+  private var shuffleHandle: BaseShuffleHandle[Int, Int, Int] = _\n+  private val shuffleBlockResolver = new IndexShuffleBlockResolver(conf)\n+  private val serializer = new JavaSerializer(conf)\n+  private val context = MemoryTestingUtils.fakeTaskContext(sc.env)\n+\n+  override def beforeEach(): Unit = {\n+    super.beforeEach()\n+    MockitoAnnotations.initMocks(this)\n+    val partitioner = new Partitioner() {\n+      def numPartitions = numMaps\n+      def getPartition(key: Any) = Utils.nonNegativeMod(key.hashCode, numPartitions)\n+    }\n+    shuffleHandle = {\n+      val dependency = mock(classOf[ShuffleDependency[Int, Int, Int]])\n+      when(dependency.partitioner).thenReturn(partitioner)\n+      when(dependency.serializer).thenReturn(serializer)\n+      when(dependency.aggregator).thenReturn(None)\n+      when(dependency.keyOrdering).thenReturn(None)\n+      new BaseShuffleHandle(shuffleId, numMaps = numMaps, dependency)\n+    }\n+  }\n+\n+  override def afterEach(): Unit = {"
  }],
  "prId": 24080
}, {
  "comments": [{
    "author": {
      "login": "attilapiros"
    },
    "body": "Delete the `dataFile` at the end of the test.",
    "commit": "05febb0d99c17dc8226dfd3d7013b17b696b2f6f",
    "createdAt": "2019-03-30T18:11:14Z",
    "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import org.mockito.Mockito._\n+import org.mockito.MockitoAnnotations\n+import org.scalatest.Matchers\n+\n+import org.apache.spark.{Partitioner, SharedSparkContext, ShuffleDependency, SparkFunSuite}\n+import org.apache.spark.memory.MemoryTestingUtils\n+import org.apache.spark.serializer.JavaSerializer\n+import org.apache.spark.shuffle.{BaseShuffleHandle, IndexShuffleBlockResolver}\n+import org.apache.spark.util.Utils\n+\n+class SortShuffleWriterSuite extends SparkFunSuite with SharedSparkContext with Matchers {\n+\n+  private val shuffleId = 0\n+  private val numMaps = 5\n+  private var shuffleHandle: BaseShuffleHandle[Int, Int, Int] = _\n+  private val shuffleBlockResolver = new IndexShuffleBlockResolver(conf)\n+  private val serializer = new JavaSerializer(conf)\n+\n+  override def beforeEach(): Unit = {\n+    super.beforeEach()\n+    MockitoAnnotations.initMocks(this)\n+    val partitioner = new Partitioner() {\n+      def numPartitions = numMaps\n+      def getPartition(key: Any) = Utils.nonNegativeMod(key.hashCode, numPartitions)\n+    }\n+    shuffleHandle = {\n+      val dependency = mock(classOf[ShuffleDependency[Int, Int, Int]])\n+      when(dependency.partitioner).thenReturn(partitioner)\n+      when(dependency.serializer).thenReturn(serializer)\n+      when(dependency.aggregator).thenReturn(None)\n+      when(dependency.keyOrdering).thenReturn(None)\n+      new BaseShuffleHandle(shuffleId, numMaps = numMaps, dependency)\n+    }\n+  }\n+\n+  override def afterAll(): Unit = {\n+    try {\n+      shuffleBlockResolver.stop()\n+    } finally {\n+      super.afterAll()\n+    }\n+  }\n+\n+  test(\"write empty iterator\") {\n+    val context = MemoryTestingUtils.fakeTaskContext(sc.env)\n+    val writer = new SortShuffleWriter[Int, Int, Int](\n+      shuffleBlockResolver,\n+      shuffleHandle,\n+      mapId = 1,\n+      context\n+    )\n+    writer.write(Iterator.empty)\n+    writer.stop(success = true)\n+    val dataFile = shuffleBlockResolver.getDataFile(shuffleId, 1)\n+    assert(!dataFile.exists())\n+    assert(dataFile.length() === 0)\n+    assert(context.taskMetrics().shuffleWriteMetrics.bytesWritten === 0)\n+    assert(context.taskMetrics().shuffleWriteMetrics.recordsWritten === 0)\n+\n+  }\n+\n+  test(\"write with some records\") {\n+    val context = MemoryTestingUtils.fakeTaskContext(sc.env)\n+    val records = Iterator((1, 2), (2, 3), (4, 4), (6, 5))\n+    val writer = new SortShuffleWriter[Int, Int, Int](\n+      shuffleBlockResolver,\n+      shuffleHandle,\n+      mapId = 2,\n+      context\n+    )\n+    writer.write(records)\n+    writer.stop(success = true)\n+    val dataFile = shuffleBlockResolver.getDataFile(shuffleId, 2)",
    "line": 90
  }, {
    "author": {
      "login": "attilapiros"
    },
    "body": "@wangjiaochun you are right that DiskBlockManager deletes the directory. \r\nSo please remove `Utils.deleteRecursively`.\r\n\r\nSo only the `Can we compare the certain expected value instead of != 0 in these three lines?` question left at https://github.com/apache/spark/pull/24080#discussion_r268482573",
    "commit": "05febb0d99c17dc8226dfd3d7013b17b696b2f6f",
    "createdAt": "2019-04-01T09:35:01Z",
    "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import org.mockito.Mockito._\n+import org.mockito.MockitoAnnotations\n+import org.scalatest.Matchers\n+\n+import org.apache.spark.{Partitioner, SharedSparkContext, ShuffleDependency, SparkFunSuite}\n+import org.apache.spark.memory.MemoryTestingUtils\n+import org.apache.spark.serializer.JavaSerializer\n+import org.apache.spark.shuffle.{BaseShuffleHandle, IndexShuffleBlockResolver}\n+import org.apache.spark.util.Utils\n+\n+class SortShuffleWriterSuite extends SparkFunSuite with SharedSparkContext with Matchers {\n+\n+  private val shuffleId = 0\n+  private val numMaps = 5\n+  private var shuffleHandle: BaseShuffleHandle[Int, Int, Int] = _\n+  private val shuffleBlockResolver = new IndexShuffleBlockResolver(conf)\n+  private val serializer = new JavaSerializer(conf)\n+\n+  override def beforeEach(): Unit = {\n+    super.beforeEach()\n+    MockitoAnnotations.initMocks(this)\n+    val partitioner = new Partitioner() {\n+      def numPartitions = numMaps\n+      def getPartition(key: Any) = Utils.nonNegativeMod(key.hashCode, numPartitions)\n+    }\n+    shuffleHandle = {\n+      val dependency = mock(classOf[ShuffleDependency[Int, Int, Int]])\n+      when(dependency.partitioner).thenReturn(partitioner)\n+      when(dependency.serializer).thenReturn(serializer)\n+      when(dependency.aggregator).thenReturn(None)\n+      when(dependency.keyOrdering).thenReturn(None)\n+      new BaseShuffleHandle(shuffleId, numMaps = numMaps, dependency)\n+    }\n+  }\n+\n+  override def afterAll(): Unit = {\n+    try {\n+      shuffleBlockResolver.stop()\n+    } finally {\n+      super.afterAll()\n+    }\n+  }\n+\n+  test(\"write empty iterator\") {\n+    val context = MemoryTestingUtils.fakeTaskContext(sc.env)\n+    val writer = new SortShuffleWriter[Int, Int, Int](\n+      shuffleBlockResolver,\n+      shuffleHandle,\n+      mapId = 1,\n+      context\n+    )\n+    writer.write(Iterator.empty)\n+    writer.stop(success = true)\n+    val dataFile = shuffleBlockResolver.getDataFile(shuffleId, 1)\n+    assert(!dataFile.exists())\n+    assert(dataFile.length() === 0)\n+    assert(context.taskMetrics().shuffleWriteMetrics.bytesWritten === 0)\n+    assert(context.taskMetrics().shuffleWriteMetrics.recordsWritten === 0)\n+\n+  }\n+\n+  test(\"write with some records\") {\n+    val context = MemoryTestingUtils.fakeTaskContext(sc.env)\n+    val records = Iterator((1, 2), (2, 3), (4, 4), (6, 5))\n+    val writer = new SortShuffleWriter[Int, Int, Int](\n+      shuffleBlockResolver,\n+      shuffleHandle,\n+      mapId = 2,\n+      context\n+    )\n+    writer.write(records)\n+    writer.stop(success = true)\n+    val dataFile = shuffleBlockResolver.getDataFile(shuffleId, 2)",
    "line": 90
  }],
  "prId": 24080
}, {
  "comments": [{
    "author": {
      "login": "attilapiros"
    },
    "body": "NIT: delete this empty line",
    "commit": "05febb0d99c17dc8226dfd3d7013b17b696b2f6f",
    "createdAt": "2019-03-30T18:11:41Z",
    "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import org.mockito.Mockito._\n+import org.mockito.MockitoAnnotations\n+import org.scalatest.Matchers\n+\n+import org.apache.spark.{Partitioner, SharedSparkContext, ShuffleDependency, SparkFunSuite}\n+import org.apache.spark.memory.MemoryTestingUtils\n+import org.apache.spark.serializer.JavaSerializer\n+import org.apache.spark.shuffle.{BaseShuffleHandle, IndexShuffleBlockResolver}\n+import org.apache.spark.util.Utils\n+\n+class SortShuffleWriterSuite extends SparkFunSuite with SharedSparkContext with Matchers {\n+\n+  private val shuffleId = 0\n+  private val numMaps = 5\n+  private var shuffleHandle: BaseShuffleHandle[Int, Int, Int] = _\n+  private val shuffleBlockResolver = new IndexShuffleBlockResolver(conf)\n+  private val serializer = new JavaSerializer(conf)\n+\n+  override def beforeEach(): Unit = {\n+    super.beforeEach()\n+    MockitoAnnotations.initMocks(this)\n+    val partitioner = new Partitioner() {\n+      def numPartitions = numMaps\n+      def getPartition(key: Any) = Utils.nonNegativeMod(key.hashCode, numPartitions)\n+    }\n+    shuffleHandle = {\n+      val dependency = mock(classOf[ShuffleDependency[Int, Int, Int]])\n+      when(dependency.partitioner).thenReturn(partitioner)\n+      when(dependency.serializer).thenReturn(serializer)\n+      when(dependency.aggregator).thenReturn(None)\n+      when(dependency.keyOrdering).thenReturn(None)\n+      new BaseShuffleHandle(shuffleId, numMaps = numMaps, dependency)\n+    }\n+  }\n+\n+  override def afterAll(): Unit = {\n+    try {\n+      shuffleBlockResolver.stop()\n+    } finally {\n+      super.afterAll()\n+    }\n+  }\n+\n+  test(\"write empty iterator\") {\n+    val context = MemoryTestingUtils.fakeTaskContext(sc.env)\n+    val writer = new SortShuffleWriter[Int, Int, Int](\n+      shuffleBlockResolver,\n+      shuffleHandle,\n+      mapId = 1,\n+      context\n+    )\n+    writer.write(Iterator.empty)\n+    writer.stop(success = true)\n+    val dataFile = shuffleBlockResolver.getDataFile(shuffleId, 1)\n+    assert(!dataFile.exists())\n+    assert(dataFile.length() === 0)\n+    assert(context.taskMetrics().shuffleWriteMetrics.bytesWritten === 0)\n+    assert(context.taskMetrics().shuffleWriteMetrics.recordsWritten === 0)\n+"
  }],
  "prId": 24080
}, {
  "comments": [{
    "author": {
      "login": "attilapiros"
    },
    "body": "revert expected val from this method, please\r\n",
    "commit": "05febb0d99c17dc8226dfd3d7013b17b696b2f6f",
    "createdAt": "2019-04-01T12:55:38Z",
    "diffHunk": "@@ -61,6 +62,7 @@ class SortShuffleWriterSuite extends SparkFunSuite with SharedSparkContext with\n   }\n \n   test(\"write empty iterator\") {\n+    val expected = 0"
  }],
  "prId": 24080
}, {
  "comments": [{
    "author": {
      "login": "attilapiros"
    },
    "body": "remove this line see, see next comment",
    "commit": "05febb0d99c17dc8226dfd3d7013b17b696b2f6f",
    "createdAt": "2019-04-01T13:02:48Z",
    "diffHunk": "@@ -72,12 +74,13 @@ class SortShuffleWriterSuite extends SparkFunSuite with SharedSparkContext with\n     writer.stop(success = true)\n     val dataFile = shuffleBlockResolver.getDataFile(shuffleId, 1)\n     assert(!dataFile.exists())\n-    assert(dataFile.length() === 0)\n-    assert(context.taskMetrics().shuffleWriteMetrics.bytesWritten === 0)\n-    assert(context.taskMetrics().shuffleWriteMetrics.recordsWritten === 0)\n+    assert(dataFile.length() === expected)\n+    assert(context.taskMetrics().shuffleWriteMetrics.bytesWritten === expected)\n+    assert(context.taskMetrics().shuffleWriteMetrics.recordsWritten === expected)\n   }\n \n   test(\"write with some records\") {\n+    val expected = 0"
  }],
  "prId": 24080
}, {
  "comments": [{
    "author": {
      "login": "attilapiros"
    },
    "body": "Use shuffle write metrics, like:\r\n\r\n```scala\r\n    val writeMetrics = context.taskMetrics().shuffleWriteMetrics\r\n    assert(dataFile.length() === writeMetrics.bytesWritten)\r\n    assert(context.taskMetrics().shuffleWriteMetrics.bytesWritten === writeMetrics.bytesWritten)\r\n    assert(context.taskMetrics().shuffleWriteMetrics.recordsWritten === writeMetrics.recordsWritten)\r\n```",
    "commit": "05febb0d99c17dc8226dfd3d7013b17b696b2f6f",
    "createdAt": "2019-04-01T13:17:49Z",
    "diffHunk": "@@ -90,9 +93,8 @@ class SortShuffleWriterSuite extends SparkFunSuite with SharedSparkContext with\n     writer.stop(success = true)\n     val dataFile = shuffleBlockResolver.getDataFile(shuffleId, 2)\n     assert(dataFile.exists())\n-    assert(dataFile.length() !== 0)\n-    assert(context.taskMetrics().shuffleWriteMetrics.bytesWritten !== 0)\n-    assert(context.taskMetrics().shuffleWriteMetrics.recordsWritten !== 0)\n-    Utils.deleteRecursively(dataFile)\n+    assert(dataFile.length() !== expected)"
  }, {
    "author": {
      "login": "kiszk"
    },
    "body": "`assert(dataFile.length() !== 0)` is too conservative. It would be great to compare `length()` with the specific value (e.g. as @attilapiros suggestion).",
    "commit": "05febb0d99c17dc8226dfd3d7013b17b696b2f6f",
    "createdAt": "2019-04-01T16:32:36Z",
    "diffHunk": "@@ -90,9 +93,8 @@ class SortShuffleWriterSuite extends SparkFunSuite with SharedSparkContext with\n     writer.stop(success = true)\n     val dataFile = shuffleBlockResolver.getDataFile(shuffleId, 2)\n     assert(dataFile.exists())\n-    assert(dataFile.length() !== 0)\n-    assert(context.taskMetrics().shuffleWriteMetrics.bytesWritten !== 0)\n-    assert(context.taskMetrics().shuffleWriteMetrics.recordsWritten !== 0)\n-    Utils.deleteRecursively(dataFile)\n+    assert(dataFile.length() !== expected)"
  }],
  "prId": 24080
}, {
  "comments": [{
    "author": {
      "login": "attilapiros"
    },
    "body": "Remove this line: `assert(dataFile.length() === writeMetrics.bytesWritten)`.\r\n\r\nAs `dataFile` does not exists, it only tests `java.io.File#length` behaviour on non-existing file and `writeMetrics.bytesWritten` is tested correctly in the next line.",
    "commit": "05febb0d99c17dc8226dfd3d7013b17b696b2f6f",
    "createdAt": "2019-04-04T14:20:52Z",
    "diffHunk": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import org.mockito.Mockito._\n+import org.mockito.MockitoAnnotations\n+import org.scalatest.Matchers\n+\n+import org.apache.spark.{Partitioner, SharedSparkContext, ShuffleDependency, SparkFunSuite}\n+import org.apache.spark.memory.MemoryTestingUtils\n+import org.apache.spark.serializer.JavaSerializer\n+import org.apache.spark.shuffle.{BaseShuffleHandle, IndexShuffleBlockResolver}\n+import org.apache.spark.util.Utils\n+\n+\n+class SortShuffleWriterSuite extends SparkFunSuite with SharedSparkContext with Matchers {\n+\n+  private val shuffleId = 0\n+  private val numMaps = 5\n+  private var shuffleHandle: BaseShuffleHandle[Int, Int, Int] = _\n+  private val shuffleBlockResolver = new IndexShuffleBlockResolver(conf)\n+  private val serializer = new JavaSerializer(conf)\n+\n+  override def beforeEach(): Unit = {\n+    super.beforeEach()\n+    MockitoAnnotations.initMocks(this)\n+    val partitioner = new Partitioner() {\n+      def numPartitions = numMaps\n+      def getPartition(key: Any) = Utils.nonNegativeMod(key.hashCode, numPartitions)\n+    }\n+    shuffleHandle = {\n+      val dependency = mock(classOf[ShuffleDependency[Int, Int, Int]])\n+      when(dependency.partitioner).thenReturn(partitioner)\n+      when(dependency.serializer).thenReturn(serializer)\n+      when(dependency.aggregator).thenReturn(None)\n+      when(dependency.keyOrdering).thenReturn(None)\n+      new BaseShuffleHandle(shuffleId, numMaps = numMaps, dependency)\n+    }\n+  }\n+\n+  override def afterAll(): Unit = {\n+    try {\n+      shuffleBlockResolver.stop()\n+    } finally {\n+      super.afterAll()\n+    }\n+  }\n+\n+  test(\"write empty iterator\") {\n+    val context = MemoryTestingUtils.fakeTaskContext(sc.env)\n+    val writer = new SortShuffleWriter[Int, Int, Int](\n+      shuffleBlockResolver,\n+      shuffleHandle,\n+      mapId = 1,\n+      context\n+    )\n+    writer.write(Iterator.empty)\n+    writer.stop(success = true)\n+    val dataFile = shuffleBlockResolver.getDataFile(shuffleId, 1)\n+    val writeMetrics = context.taskMetrics().shuffleWriteMetrics\n+    assert(!dataFile.exists())\n+    assert(dataFile.length() === writeMetrics.bytesWritten)"
  }],
  "prId": 24080
}, {
  "comments": [{
    "author": {
      "login": "attilapiros"
    },
    "body": "Nit: move ')' to the prev line",
    "commit": "05febb0d99c17dc8226dfd3d7013b17b696b2f6f",
    "createdAt": "2019-04-04T14:27:47Z",
    "diffHunk": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import org.mockito.Mockito._\n+import org.mockito.MockitoAnnotations\n+import org.scalatest.Matchers\n+\n+import org.apache.spark.{Partitioner, SharedSparkContext, ShuffleDependency, SparkFunSuite}\n+import org.apache.spark.memory.MemoryTestingUtils\n+import org.apache.spark.serializer.JavaSerializer\n+import org.apache.spark.shuffle.{BaseShuffleHandle, IndexShuffleBlockResolver}\n+import org.apache.spark.util.Utils\n+\n+\n+class SortShuffleWriterSuite extends SparkFunSuite with SharedSparkContext with Matchers {\n+\n+  private val shuffleId = 0\n+  private val numMaps = 5\n+  private var shuffleHandle: BaseShuffleHandle[Int, Int, Int] = _\n+  private val shuffleBlockResolver = new IndexShuffleBlockResolver(conf)\n+  private val serializer = new JavaSerializer(conf)\n+\n+  override def beforeEach(): Unit = {\n+    super.beforeEach()\n+    MockitoAnnotations.initMocks(this)\n+    val partitioner = new Partitioner() {\n+      def numPartitions = numMaps\n+      def getPartition(key: Any) = Utils.nonNegativeMod(key.hashCode, numPartitions)\n+    }\n+    shuffleHandle = {\n+      val dependency = mock(classOf[ShuffleDependency[Int, Int, Int]])\n+      when(dependency.partitioner).thenReturn(partitioner)\n+      when(dependency.serializer).thenReturn(serializer)\n+      when(dependency.aggregator).thenReturn(None)\n+      when(dependency.keyOrdering).thenReturn(None)\n+      new BaseShuffleHandle(shuffleId, numMaps = numMaps, dependency)\n+    }\n+  }\n+\n+  override def afterAll(): Unit = {\n+    try {\n+      shuffleBlockResolver.stop()\n+    } finally {\n+      super.afterAll()\n+    }\n+  }\n+\n+  test(\"write empty iterator\") {\n+    val context = MemoryTestingUtils.fakeTaskContext(sc.env)\n+    val writer = new SortShuffleWriter[Int, Int, Int](\n+      shuffleBlockResolver,\n+      shuffleHandle,\n+      mapId = 1,\n+      context\n+    )\n+    writer.write(Iterator.empty)\n+    writer.stop(success = true)\n+    val dataFile = shuffleBlockResolver.getDataFile(shuffleId, 1)\n+    val writeMetrics = context.taskMetrics().shuffleWriteMetrics\n+    assert(!dataFile.exists())\n+    assert(dataFile.length() === writeMetrics.bytesWritten)\n+    assert(writeMetrics.bytesWritten === 0)\n+    assert(writeMetrics.recordsWritten === 0)\n+  }\n+\n+  test(\"write with some records\") {\n+    val context = MemoryTestingUtils.fakeTaskContext(sc.env)\n+    val records = List[(Int, Int)]((1, 2), (2, 3), (4, 4), (6, 5))\n+    val writer = new SortShuffleWriter[Int, Int, Int](\n+      shuffleBlockResolver,\n+      shuffleHandle,\n+      mapId = 2,\n+      context\n+    )"
  }],
  "prId": 24080
}, {
  "comments": [{
    "author": {
      "login": "attilapiros"
    },
    "body": "Nit: move ')' to the prev line\r\n\r\n",
    "commit": "05febb0d99c17dc8226dfd3d7013b17b696b2f6f",
    "createdAt": "2019-04-04T14:28:07Z",
    "diffHunk": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.sort\n+\n+import org.mockito.Mockito._\n+import org.mockito.MockitoAnnotations\n+import org.scalatest.Matchers\n+\n+import org.apache.spark.{Partitioner, SharedSparkContext, ShuffleDependency, SparkFunSuite}\n+import org.apache.spark.memory.MemoryTestingUtils\n+import org.apache.spark.serializer.JavaSerializer\n+import org.apache.spark.shuffle.{BaseShuffleHandle, IndexShuffleBlockResolver}\n+import org.apache.spark.util.Utils\n+\n+\n+class SortShuffleWriterSuite extends SparkFunSuite with SharedSparkContext with Matchers {\n+\n+  private val shuffleId = 0\n+  private val numMaps = 5\n+  private var shuffleHandle: BaseShuffleHandle[Int, Int, Int] = _\n+  private val shuffleBlockResolver = new IndexShuffleBlockResolver(conf)\n+  private val serializer = new JavaSerializer(conf)\n+\n+  override def beforeEach(): Unit = {\n+    super.beforeEach()\n+    MockitoAnnotations.initMocks(this)\n+    val partitioner = new Partitioner() {\n+      def numPartitions = numMaps\n+      def getPartition(key: Any) = Utils.nonNegativeMod(key.hashCode, numPartitions)\n+    }\n+    shuffleHandle = {\n+      val dependency = mock(classOf[ShuffleDependency[Int, Int, Int]])\n+      when(dependency.partitioner).thenReturn(partitioner)\n+      when(dependency.serializer).thenReturn(serializer)\n+      when(dependency.aggregator).thenReturn(None)\n+      when(dependency.keyOrdering).thenReturn(None)\n+      new BaseShuffleHandle(shuffleId, numMaps = numMaps, dependency)\n+    }\n+  }\n+\n+  override def afterAll(): Unit = {\n+    try {\n+      shuffleBlockResolver.stop()\n+    } finally {\n+      super.afterAll()\n+    }\n+  }\n+\n+  test(\"write empty iterator\") {\n+    val context = MemoryTestingUtils.fakeTaskContext(sc.env)\n+    val writer = new SortShuffleWriter[Int, Int, Int](\n+      shuffleBlockResolver,\n+      shuffleHandle,\n+      mapId = 1,\n+      context\n+    )"
  }],
  "prId": 24080
}]