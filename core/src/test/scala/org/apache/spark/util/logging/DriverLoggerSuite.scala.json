[{
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "What exactly is this waiting for? Sleeping is generally the wrong way to wait for something.\r\n\r\nYou should also extend `LocalSparkContext`.\r\n\r\nIf you really need to wait, you should do it in `getSparkContext()` to avoid having to add the wait logic to every test.",
    "commit": "616bd01b5b23dc0ce0b50e95b7fd6684b1f7e9d5",
    "createdAt": "2018-10-09T20:26:06Z",
    "diffHunk": "@@ -0,0 +1,109 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.logging\n+\n+import java.io.{BufferedInputStream, FileInputStream}\n+\n+import org.apache.commons.io.FileUtils\n+\n+import org.apache.spark._\n+import org.apache.spark.{SparkContext, SparkFunSuite}\n+import org.apache.spark.internal.config._\n+import org.apache.spark.launcher.SparkLauncher\n+import org.apache.spark.network.util.JavaUtils\n+import org.apache.spark.util.Utils\n+\n+class DriverLoggerSuite extends SparkFunSuite {\n+\n+  test(\"driver logs are persisted\") {\n+    val sc = getSparkContext()\n+    // Wait for application to start"
  }],
  "prId": 22504
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "You should create a constant for `/tmp/hdfs_logs/` or read it from the SparkConf object.",
    "commit": "616bd01b5b23dc0ce0b50e95b7fd6684b1f7e9d5",
    "createdAt": "2018-10-09T20:29:10Z",
    "diffHunk": "@@ -0,0 +1,109 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.logging\n+\n+import java.io.{BufferedInputStream, FileInputStream}\n+\n+import org.apache.commons.io.FileUtils\n+\n+import org.apache.spark._\n+import org.apache.spark.{SparkContext, SparkFunSuite}\n+import org.apache.spark.internal.config._\n+import org.apache.spark.launcher.SparkLauncher\n+import org.apache.spark.network.util.JavaUtils\n+import org.apache.spark.util.Utils\n+\n+class DriverLoggerSuite extends SparkFunSuite {\n+\n+  test(\"driver logs are persisted\") {\n+    val sc = getSparkContext()\n+    // Wait for application to start\n+    Thread.sleep(1000)\n+\n+    val app_id = sc.applicationId\n+    // Run a simple spark application\n+    sc.parallelize(1 to 1000).count()\n+\n+    // Assert driver log file exists\n+    val rootDir = Utils.getLocalDir(sc.getConf)\n+    val driverLogsDir = FileUtils.getFile(rootDir, \"driver_logs\")\n+    assert(driverLogsDir.exists())\n+    val files = driverLogsDir.listFiles()\n+    assert(files.length === 1)\n+    assert(files(0).getName.equals(\"driver.log\"))\n+\n+    sc.stop()\n+    // On application end, file is moved to Hdfs (which is a local dir for this test)\n+    assert(!driverLogsDir.exists())\n+    val hdfsDir = FileUtils.getFile(\"/tmp/hdfs_logs/\", app_id)"
  }],
  "prId": 22504
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "I'm not a fan of code that relies on exact timing between threads for things to work.\r\n\r\nThis would be better if you drove the `HdfsAsyncWriter` class manually from the test, instead of indirectly through a `SparkContext`. The you can also control the log file flushing explicitly instead of the hack you have above.",
    "commit": "616bd01b5b23dc0ce0b50e95b7fd6684b1f7e9d5",
    "createdAt": "2018-10-09T20:33:29Z",
    "diffHunk": "@@ -0,0 +1,109 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.logging\n+\n+import java.io.{BufferedInputStream, FileInputStream}\n+\n+import org.apache.commons.io.FileUtils\n+\n+import org.apache.spark._\n+import org.apache.spark.{SparkContext, SparkFunSuite}\n+import org.apache.spark.internal.config._\n+import org.apache.spark.launcher.SparkLauncher\n+import org.apache.spark.network.util.JavaUtils\n+import org.apache.spark.util.Utils\n+\n+class DriverLoggerSuite extends SparkFunSuite {\n+\n+  test(\"driver logs are persisted\") {\n+    val sc = getSparkContext()\n+    // Wait for application to start\n+    Thread.sleep(1000)\n+\n+    val app_id = sc.applicationId\n+    // Run a simple spark application\n+    sc.parallelize(1 to 1000).count()\n+\n+    // Assert driver log file exists\n+    val rootDir = Utils.getLocalDir(sc.getConf)\n+    val driverLogsDir = FileUtils.getFile(rootDir, \"driver_logs\")\n+    assert(driverLogsDir.exists())\n+    val files = driverLogsDir.listFiles()\n+    assert(files.length === 1)\n+    assert(files(0).getName.equals(\"driver.log\"))\n+\n+    sc.stop()\n+    // On application end, file is moved to Hdfs (which is a local dir for this test)\n+    assert(!driverLogsDir.exists())\n+    val hdfsDir = FileUtils.getFile(\"/tmp/hdfs_logs/\", app_id)\n+    assert(hdfsDir.exists())\n+    val hdfsFiles = hdfsDir.listFiles()\n+    assert(hdfsFiles.length > 0)\n+    JavaUtils.deleteRecursively(hdfsDir)\n+    assert(!hdfsDir.exists())\n+  }\n+\n+  test(\"driver logs are synced to hdfs continuously\") {\n+    val sc = getSparkContext()\n+    // Wait for application to start\n+    Thread.sleep(1000)\n+\n+    val app_id = sc.applicationId\n+    // Run a simple spark application\n+    sc.parallelize(1 to 1000).count()\n+\n+    // Assert driver log file exists\n+    val rootDir = Utils.getLocalDir(sc.getConf)\n+    val driverLogsDir = FileUtils.getFile(rootDir, \"driver_logs\")\n+    assert(driverLogsDir.exists())\n+    val files = driverLogsDir.listFiles()\n+    assert(files.length === 1)\n+    assert(files(0).getName.equals(\"driver.log\"))\n+    for (i <- 1 to 1000) {\n+      logInfo(\"Log enough data to log file so that it can be flushed\")\n+    }\n+\n+    // After 5 secs, file contents are synced to Hdfs (which is a local dir for this test)\n+    Thread.sleep(6000)"
  }],
  "prId": 22504
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "I'm not a fan of what you did, exposing `SparkContext.driverLogger` for tests.\r\n\r\nIt should be possible to write this test without using `SparkContext` at all. Just create a `DriverLogger` and control it from the test.\r\n\r\nAlso, you did not address all of my previous feedback. e.g., the loop logging a bunch of things should not be needed. You should have some explicit action that causes the flush instead of relying on side effects of other calls.",
    "commit": "616bd01b5b23dc0ce0b50e95b7fd6684b1f7e9d5",
    "createdAt": "2018-10-15T20:59:49Z",
    "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.logging\n+\n+import java.io.{BufferedInputStream, FileInputStream}\n+\n+import org.apache.commons.io.FileUtils\n+\n+import org.apache.spark._\n+import org.apache.spark.{SparkContext, SparkFunSuite}\n+import org.apache.spark.internal.config._\n+import org.apache.spark.launcher.SparkLauncher\n+import org.apache.spark.network.util.JavaUtils\n+import org.apache.spark.util.Utils\n+\n+class DriverLoggerSuite extends SparkFunSuite with LocalSparkContext {\n+\n+  private val DRIVER_LOG_DIR_DEFAULT = \"/tmp/hdfs_logs\"\n+\n+  override def beforeAll(): Unit = {\n+    FileUtils.forceMkdir(FileUtils.getFile(DRIVER_LOG_DIR_DEFAULT))\n+  }\n+\n+  override def afterAll(): Unit = {\n+    JavaUtils.deleteRecursively(FileUtils.getFile(DRIVER_LOG_DIR_DEFAULT))\n+  }\n+\n+  test(\"driver logs are persisted\") {\n+    val sc = getSparkContext()\n+\n+    val app_id = sc.applicationId\n+    // Run a simple spark application\n+    sc.parallelize(1 to 1000).count()\n+\n+    // Assert driver log file exists\n+    val rootDir = Utils.getLocalDir(sc.getConf)\n+    val driverLogsDir = FileUtils.getFile(rootDir, \"driver_logs\")\n+    assert(driverLogsDir.exists())\n+    val files = driverLogsDir.listFiles()\n+    assert(files.length === 1)\n+    assert(files(0).getName.equals(\"driver.log\"))\n+\n+    sc.stop()\n+    // On application end, file is moved to Hdfs (which is a local dir for this test)\n+    assert(!driverLogsDir.exists())\n+    val hdfsDir = FileUtils.getFile(sc.getConf.get(DRIVER_LOG_DFS_DIR).get, app_id)\n+    assert(hdfsDir.exists())\n+    val hdfsFiles = hdfsDir.listFiles()\n+    assert(hdfsFiles.length > 0)\n+    JavaUtils.deleteRecursively(hdfsDir)\n+    assert(!hdfsDir.exists())\n+  }\n+\n+  test(\"driver logs are synced to hdfs continuously\") {\n+    val sc = getSparkContext()"
  }, {
    "author": {
      "login": "ankuriitg"
    },
    "body": "I changed the test to just check the final output, so it does not rely on loop logging anymore. That way, I don't need to expose `SparkContext.driverLogger` as well",
    "commit": "616bd01b5b23dc0ce0b50e95b7fd6684b1f7e9d5",
    "createdAt": "2018-10-15T22:58:18Z",
    "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.logging\n+\n+import java.io.{BufferedInputStream, FileInputStream}\n+\n+import org.apache.commons.io.FileUtils\n+\n+import org.apache.spark._\n+import org.apache.spark.{SparkContext, SparkFunSuite}\n+import org.apache.spark.internal.config._\n+import org.apache.spark.launcher.SparkLauncher\n+import org.apache.spark.network.util.JavaUtils\n+import org.apache.spark.util.Utils\n+\n+class DriverLoggerSuite extends SparkFunSuite with LocalSparkContext {\n+\n+  private val DRIVER_LOG_DIR_DEFAULT = \"/tmp/hdfs_logs\"\n+\n+  override def beforeAll(): Unit = {\n+    FileUtils.forceMkdir(FileUtils.getFile(DRIVER_LOG_DIR_DEFAULT))\n+  }\n+\n+  override def afterAll(): Unit = {\n+    JavaUtils.deleteRecursively(FileUtils.getFile(DRIVER_LOG_DIR_DEFAULT))\n+  }\n+\n+  test(\"driver logs are persisted\") {\n+    val sc = getSparkContext()\n+\n+    val app_id = sc.applicationId\n+    // Run a simple spark application\n+    sc.parallelize(1 to 1000).count()\n+\n+    // Assert driver log file exists\n+    val rootDir = Utils.getLocalDir(sc.getConf)\n+    val driverLogsDir = FileUtils.getFile(rootDir, \"driver_logs\")\n+    assert(driverLogsDir.exists())\n+    val files = driverLogsDir.listFiles()\n+    assert(files.length === 1)\n+    assert(files(0).getName.equals(\"driver.log\"))\n+\n+    sc.stop()\n+    // On application end, file is moved to Hdfs (which is a local dir for this test)\n+    assert(!driverLogsDir.exists())\n+    val hdfsDir = FileUtils.getFile(sc.getConf.get(DRIVER_LOG_DFS_DIR).get, app_id)\n+    assert(hdfsDir.exists())\n+    val hdfsFiles = hdfsDir.listFiles()\n+    assert(hdfsFiles.length > 0)\n+    JavaUtils.deleteRecursively(hdfsDir)\n+    assert(!hdfsDir.exists())\n+  }\n+\n+  test(\"driver logs are synced to hdfs continuously\") {\n+    val sc = getSparkContext()"
  }],
  "prId": 22504
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Don't do this. Tests run with a pre-configured temp directory, and should not write to `/tmp`.\r\n\r\nIf you need to create a temp directory, do so explicitly using the existing `Utils` API.",
    "commit": "616bd01b5b23dc0ce0b50e95b7fd6684b1f7e9d5",
    "createdAt": "2018-10-15T21:00:51Z",
    "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.logging\n+\n+import java.io.{BufferedInputStream, FileInputStream}\n+\n+import org.apache.commons.io.FileUtils\n+\n+import org.apache.spark._\n+import org.apache.spark.{SparkContext, SparkFunSuite}\n+import org.apache.spark.internal.config._\n+import org.apache.spark.launcher.SparkLauncher\n+import org.apache.spark.network.util.JavaUtils\n+import org.apache.spark.util.Utils\n+\n+class DriverLoggerSuite extends SparkFunSuite with LocalSparkContext {\n+\n+  private val DRIVER_LOG_DIR_DEFAULT = \"/tmp/hdfs_logs\"\n+\n+  override def beforeAll(): Unit = {\n+    FileUtils.forceMkdir(FileUtils.getFile(DRIVER_LOG_DIR_DEFAULT))\n+  }\n+\n+  override def afterAll(): Unit = {\n+    JavaUtils.deleteRecursively(FileUtils.getFile(DRIVER_LOG_DIR_DEFAULT))\n+  }\n+\n+  test(\"driver logs are persisted\") {\n+    val sc = getSparkContext()\n+\n+    val app_id = sc.applicationId\n+    // Run a simple spark application\n+    sc.parallelize(1 to 1000).count()\n+\n+    // Assert driver log file exists\n+    val rootDir = Utils.getLocalDir(sc.getConf)\n+    val driverLogsDir = FileUtils.getFile(rootDir, \"driver_logs\")\n+    assert(driverLogsDir.exists())\n+    val files = driverLogsDir.listFiles()\n+    assert(files.length === 1)\n+    assert(files(0).getName.equals(\"driver.log\"))\n+\n+    sc.stop()\n+    // On application end, file is moved to Hdfs (which is a local dir for this test)\n+    assert(!driverLogsDir.exists())\n+    val hdfsDir = FileUtils.getFile(sc.getConf.get(DRIVER_LOG_DFS_DIR).get, app_id)\n+    assert(hdfsDir.exists())\n+    val hdfsFiles = hdfsDir.listFiles()\n+    assert(hdfsFiles.length > 0)\n+    JavaUtils.deleteRecursively(hdfsDir)\n+    assert(!hdfsDir.exists())\n+  }\n+\n+  test(\"driver logs are synced to hdfs continuously\") {\n+    val sc = getSparkContext()\n+\n+    val app_id = sc.applicationId\n+    // Run a simple spark application\n+    sc.parallelize(1 to 1000).count()\n+\n+    // Assert driver log file exists\n+    val rootDir = Utils.getLocalDir(sc.getConf)\n+    val driverLogsDir = FileUtils.getFile(rootDir, \"driver_logs\")\n+    assert(driverLogsDir.exists())\n+    val files = driverLogsDir.listFiles()\n+    assert(files.length === 1)\n+    assert(files(0).getName.equals(\"driver.log\"))\n+    for (i <- 1 to 1000) {\n+      logInfo(\"Log enough data to log file so that it can be flushed\")\n+    }\n+\n+    // Sync the driver logs manually instead of waiting for scheduler\n+    sc._driverLogger.foreach(_.writer.foreach(_.run()))\n+    val hdfsDir = FileUtils.getFile(sc.getConf.get(DRIVER_LOG_DFS_DIR).get, app_id)\n+    assert(hdfsDir.exists())\n+    val hdfsFiles = hdfsDir.listFiles()\n+    assert(hdfsFiles.length > 0)\n+    val driverLogFile = hdfsFiles.filter(f => f.getName.equals(\"driver.log\")).head\n+    val hdfsIS = new BufferedInputStream(new FileInputStream(driverLogFile))\n+    assert(hdfsIS.available() > 0)\n+\n+    sc.stop()\n+    // Ensure that the local file is deleted on application end\n+    assert(!driverLogsDir.exists())\n+    JavaUtils.deleteRecursively(hdfsDir)\n+    assert(!hdfsDir.exists())\n+  }\n+\n+  private def getSparkContext(): SparkContext = {\n+    val conf = new SparkConf()\n+    conf.set(\"spark.local.dir\", \"/tmp\")"
  }],
  "prId": 22504
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "use a relative dir, not an absolute one, for testing data.  (what if a developer happened to have real data at /tmp/hdfs_logs before running this?)",
    "commit": "616bd01b5b23dc0ce0b50e95b7fd6684b1f7e9d5",
    "createdAt": "2018-10-18T17:30:12Z",
    "diffHunk": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.logging\n+\n+import java.io.{BufferedInputStream, FileInputStream}\n+\n+import org.apache.commons.io.FileUtils\n+\n+import org.apache.spark._\n+import org.apache.spark.{SparkContext, SparkFunSuite}\n+import org.apache.spark.internal.config._\n+import org.apache.spark.launcher.SparkLauncher\n+import org.apache.spark.network.util.JavaUtils\n+import org.apache.spark.util.Utils\n+\n+class DriverLoggerSuite extends SparkFunSuite with LocalSparkContext {\n+\n+  private val DRIVER_LOG_DIR_DEFAULT = \"/tmp/hdfs_logs\""
  }],
  "prId": 22504
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "comment is a bit misleading (or maybe out of date) -- its synced continuously, right?",
    "commit": "616bd01b5b23dc0ce0b50e95b7fd6684b1f7e9d5",
    "createdAt": "2018-10-18T17:31:35Z",
    "diffHunk": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.logging\n+\n+import java.io.{BufferedInputStream, FileInputStream}\n+\n+import org.apache.commons.io.FileUtils\n+\n+import org.apache.spark._\n+import org.apache.spark.{SparkContext, SparkFunSuite}\n+import org.apache.spark.internal.config._\n+import org.apache.spark.launcher.SparkLauncher\n+import org.apache.spark.network.util.JavaUtils\n+import org.apache.spark.util.Utils\n+\n+class DriverLoggerSuite extends SparkFunSuite with LocalSparkContext {\n+\n+  private val DRIVER_LOG_DIR_DEFAULT = \"/tmp/hdfs_logs\"\n+\n+  override def beforeAll(): Unit = {\n+    FileUtils.forceMkdir(FileUtils.getFile(DRIVER_LOG_DIR_DEFAULT))\n+  }\n+\n+  override def afterAll(): Unit = {\n+    JavaUtils.deleteRecursively(FileUtils.getFile(DRIVER_LOG_DIR_DEFAULT))\n+  }\n+\n+  test(\"driver logs are persisted locally and synced to hdfs\") {\n+    val sc = getSparkContext()\n+\n+    val app_id = sc.applicationId\n+    // Run a simple spark application\n+    sc.parallelize(1 to 1000).count()\n+\n+    // Assert driver log file exists\n+    val rootDir = Utils.getLocalDir(sc.getConf)\n+    val driverLogsDir = FileUtils.getFile(rootDir, \"driver_logs\")\n+    assert(driverLogsDir.exists())\n+    val files = driverLogsDir.listFiles()\n+    assert(files.length === 1)\n+    assert(files(0).getName.equals(\"driver.log\"))\n+\n+    sc.stop()\n+    // On application end, file is moved to Hdfs (which is a local dir for this test)"
  }],
  "prId": 22504
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "s/hdfs/dfs in all this file.",
    "commit": "616bd01b5b23dc0ce0b50e95b7fd6684b1f7e9d5",
    "createdAt": "2018-10-18T21:01:36Z",
    "diffHunk": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.logging\n+\n+import java.io.{BufferedInputStream, File, FileInputStream}\n+\n+import org.apache.commons.io.FileUtils\n+\n+import org.apache.spark._\n+import org.apache.spark.{SparkContext, SparkFunSuite}\n+import org.apache.spark.internal.config._\n+import org.apache.spark.launcher.SparkLauncher\n+import org.apache.spark.network.util.JavaUtils\n+import org.apache.spark.util.Utils\n+\n+class DriverLoggerSuite extends SparkFunSuite with LocalSparkContext {\n+\n+  private var rootHdfsDir : File = _"
  }],
  "prId": 22504
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "It's good practice to call `super.beforeAll` and `super.afterAll`. But here you don't need these; `createTempDir` cleans things up automatically for you.",
    "commit": "616bd01b5b23dc0ce0b50e95b7fd6684b1f7e9d5",
    "createdAt": "2018-10-18T21:13:29Z",
    "diffHunk": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.logging\n+\n+import java.io.{BufferedInputStream, File, FileInputStream}\n+\n+import org.apache.commons.io.FileUtils\n+\n+import org.apache.spark._\n+import org.apache.spark.{SparkContext, SparkFunSuite}\n+import org.apache.spark.internal.config._\n+import org.apache.spark.launcher.SparkLauncher\n+import org.apache.spark.network.util.JavaUtils\n+import org.apache.spark.util.Utils\n+\n+class DriverLoggerSuite extends SparkFunSuite with LocalSparkContext {\n+\n+  private var rootHdfsDir : File = _\n+\n+  override def beforeAll(): Unit = {\n+    rootHdfsDir = Utils.createTempDir(namePrefix = \"hdfs_logs\")"
  }],
  "prId": 22504
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "This line and the next are unnecessary.",
    "commit": "616bd01b5b23dc0ce0b50e95b7fd6684b1f7e9d5",
    "createdAt": "2018-10-18T21:14:11Z",
    "diffHunk": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.logging\n+\n+import java.io.{BufferedInputStream, File, FileInputStream}\n+\n+import org.apache.commons.io.FileUtils\n+\n+import org.apache.spark._\n+import org.apache.spark.{SparkContext, SparkFunSuite}\n+import org.apache.spark.internal.config._\n+import org.apache.spark.launcher.SparkLauncher\n+import org.apache.spark.network.util.JavaUtils\n+import org.apache.spark.util.Utils\n+\n+class DriverLoggerSuite extends SparkFunSuite with LocalSparkContext {\n+\n+  private var rootHdfsDir : File = _\n+\n+  override def beforeAll(): Unit = {\n+    rootHdfsDir = Utils.createTempDir(namePrefix = \"hdfs_logs\")\n+  }\n+\n+  override def afterAll(): Unit = {\n+    JavaUtils.deleteRecursively(rootHdfsDir)\n+  }\n+\n+  test(\"driver logs are persisted locally and synced to hdfs\") {\n+    val sc = getSparkContext()\n+\n+    val app_id = sc.applicationId\n+    // Run a simple spark application\n+    sc.parallelize(1 to 1000).count()\n+\n+    // Assert driver log file exists\n+    val rootDir = Utils.getLocalDir(sc.getConf)\n+    val driverLogsDir = FileUtils.getFile(rootDir, \"driver_logs\")\n+    assert(driverLogsDir.exists())\n+    val files = driverLogsDir.listFiles()\n+    assert(files.length === 1)\n+    assert(files(0).getName.equals(\"driver.log\"))\n+\n+    sc.stop()\n+    // File is continuously synced to Hdfs (which is a local dir for this test)\n+    assert(!driverLogsDir.exists())\n+    val hdfsDir = FileUtils.getFile(sc.getConf.get(DRIVER_LOG_DFS_DIR).get, app_id)\n+    assert(hdfsDir.exists())\n+    val hdfsFiles = hdfsDir.listFiles()\n+    assert(hdfsFiles.length > 0)\n+    val driverLogFile = hdfsFiles.filter(f => f.getName.equals(\"driver.log\")).head\n+    val hdfsIS = new BufferedInputStream(new FileInputStream(driverLogFile))\n+    assert(hdfsIS.available() > 0)\n+    JavaUtils.deleteRecursively(hdfsDir)"
  }],
  "prId": 22504
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Comment is unrelated to any of the code around it.",
    "commit": "616bd01b5b23dc0ce0b50e95b7fd6684b1f7e9d5",
    "createdAt": "2018-10-18T21:14:56Z",
    "diffHunk": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.logging\n+\n+import java.io.{BufferedInputStream, File, FileInputStream}\n+\n+import org.apache.commons.io.FileUtils\n+\n+import org.apache.spark._\n+import org.apache.spark.{SparkContext, SparkFunSuite}\n+import org.apache.spark.internal.config._\n+import org.apache.spark.launcher.SparkLauncher\n+import org.apache.spark.network.util.JavaUtils\n+import org.apache.spark.util.Utils\n+\n+class DriverLoggerSuite extends SparkFunSuite with LocalSparkContext {\n+\n+  private var rootHdfsDir : File = _\n+\n+  override def beforeAll(): Unit = {\n+    rootHdfsDir = Utils.createTempDir(namePrefix = \"hdfs_logs\")\n+  }\n+\n+  override def afterAll(): Unit = {\n+    JavaUtils.deleteRecursively(rootHdfsDir)\n+  }\n+\n+  test(\"driver logs are persisted locally and synced to hdfs\") {\n+    val sc = getSparkContext()\n+\n+    val app_id = sc.applicationId\n+    // Run a simple spark application\n+    sc.parallelize(1 to 1000).count()\n+\n+    // Assert driver log file exists\n+    val rootDir = Utils.getLocalDir(sc.getConf)\n+    val driverLogsDir = FileUtils.getFile(rootDir, \"driver_logs\")\n+    assert(driverLogsDir.exists())\n+    val files = driverLogsDir.listFiles()\n+    assert(files.length === 1)\n+    assert(files(0).getName.equals(\"driver.log\"))\n+\n+    sc.stop()\n+    // File is continuously synced to Hdfs (which is a local dir for this test)"
  }],
  "prId": 22504
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "All the checks below are basically:\r\n\r\n```\r\nhdfsFiles.exists { f => f.getName() == DRIVER_LOG_FILE && f.length() > 0 }\r\n```",
    "commit": "616bd01b5b23dc0ce0b50e95b7fd6684b1f7e9d5",
    "createdAt": "2018-10-18T21:17:00Z",
    "diffHunk": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.logging\n+\n+import java.io.{BufferedInputStream, File, FileInputStream}\n+\n+import org.apache.commons.io.FileUtils\n+\n+import org.apache.spark._\n+import org.apache.spark.{SparkContext, SparkFunSuite}\n+import org.apache.spark.internal.config._\n+import org.apache.spark.launcher.SparkLauncher\n+import org.apache.spark.network.util.JavaUtils\n+import org.apache.spark.util.Utils\n+\n+class DriverLoggerSuite extends SparkFunSuite with LocalSparkContext {\n+\n+  private var rootHdfsDir : File = _\n+\n+  override def beforeAll(): Unit = {\n+    rootHdfsDir = Utils.createTempDir(namePrefix = \"hdfs_logs\")\n+  }\n+\n+  override def afterAll(): Unit = {\n+    JavaUtils.deleteRecursively(rootHdfsDir)\n+  }\n+\n+  test(\"driver logs are persisted locally and synced to hdfs\") {\n+    val sc = getSparkContext()\n+\n+    val app_id = sc.applicationId\n+    // Run a simple spark application\n+    sc.parallelize(1 to 1000).count()\n+\n+    // Assert driver log file exists\n+    val rootDir = Utils.getLocalDir(sc.getConf)\n+    val driverLogsDir = FileUtils.getFile(rootDir, \"driver_logs\")\n+    assert(driverLogsDir.exists())\n+    val files = driverLogsDir.listFiles()\n+    assert(files.length === 1)\n+    assert(files(0).getName.equals(\"driver.log\"))\n+\n+    sc.stop()\n+    // File is continuously synced to Hdfs (which is a local dir for this test)\n+    assert(!driverLogsDir.exists())\n+    val hdfsDir = FileUtils.getFile(sc.getConf.get(DRIVER_LOG_DFS_DIR).get, app_id)\n+    assert(hdfsDir.exists())\n+    val hdfsFiles = hdfsDir.listFiles()"
  }],
  "prId": 22504
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "space before `{`\r\n\r\nYou also need to assert the condition...",
    "commit": "616bd01b5b23dc0ce0b50e95b7fd6684b1f7e9d5",
    "createdAt": "2018-10-19T18:27:18Z",
    "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util.logging\n+\n+import java.io.{BufferedInputStream, File, FileInputStream}\n+\n+import org.apache.commons.io.FileUtils\n+\n+import org.apache.spark._\n+import org.apache.spark.{SparkContext, SparkFunSuite}\n+import org.apache.spark.internal.config._\n+import org.apache.spark.launcher.SparkLauncher\n+import org.apache.spark.network.util.JavaUtils\n+import org.apache.spark.util.Utils\n+\n+class DriverLoggerSuite extends SparkFunSuite with LocalSparkContext {\n+\n+  private var rootDfsDir : File = _\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    rootDfsDir = Utils.createTempDir(namePrefix = \"dfs_logs\")\n+  }\n+\n+  override def afterAll(): Unit = {\n+    super.afterAll()\n+    JavaUtils.deleteRecursively(rootDfsDir)\n+  }\n+\n+  test(\"driver logs are persisted locally and synced to dfs\") {\n+    val sc = getSparkContext()\n+\n+    val app_id = sc.applicationId\n+    // Run a simple spark application\n+    sc.parallelize(1 to 1000).count()\n+\n+    // Assert driver log file exists\n+    val rootDir = Utils.getLocalDir(sc.getConf)\n+    val driverLogsDir = FileUtils.getFile(rootDir, DriverLogger.DRIVER_LOG_DIR)\n+    assert(driverLogsDir.exists())\n+    val files = driverLogsDir.listFiles()\n+    assert(files.length === 1)\n+    assert(files(0).getName.equals(DriverLogger.DRIVER_LOG_FILE))\n+\n+    sc.stop()\n+    assert(!driverLogsDir.exists())\n+    val dfsDir = FileUtils.getFile(sc.getConf.get(DRIVER_LOG_DFS_DIR).get, app_id)\n+    assert(dfsDir.exists())\n+    val dfsFiles = dfsDir.listFiles()\n+    dfsFiles.exists{ f => f.getName().equals(DriverLogger.DRIVER_LOG_FILE) && f.length() > 0 }"
  }],
  "prId": 22504
}]