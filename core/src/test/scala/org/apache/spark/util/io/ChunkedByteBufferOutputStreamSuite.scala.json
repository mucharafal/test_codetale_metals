[{
  "comments": [{
    "author": {
      "login": "uncleGen"
    },
    "body": "Discovery starting.\r\n```\r\nDiscovery completed in 42 seconds, 124 milliseconds.\r\nRun starting. Expected test count is: 9\r\nChunkedByteBufferOutputStreamSuite:\r\n- empty output\r\n- write a single byte\r\n- write a single near boundary\r\n- write a single at boundary\r\n- single chunk output\r\n- single chunk output at boundary size\r\n- multiple chunk output\r\n- multiple chunk output at boundary size\r\n- negative chunk size\r\nRun completed in 42 seconds, 700 milliseconds.\r\nTotal number of tests run: 9\r\nSuites: completed 2, aborted 0\r\nTests: succeeded 9, failed 0, canceled 0, ignored 0, pending 0\r\nAll tests passed.\r\n```",
    "commit": "6b892af78c0ce47352d8f2be1bb765aafaff363a",
    "createdAt": "2016-12-13T06:27:47Z",
    "diffHunk": "@@ -119,4 +119,21 @@ class ChunkedByteBufferOutputStreamSuite extends SparkFunSuite {\n     assert(arrays(1).toSeq === ref.slice(10, 20))\n     assert(arrays(2).toSeq === ref.slice(20, 30))\n   }\n+\n+  test(\"negative chunk size\") {\n+    val ref = new Array[Byte](8 * 1024 * 1024 + 10)\n+    Random.nextBytes(ref)\n+    val o = new ChunkedByteBufferOutputStream(-10, ByteBuffer.allocate)\n+    o.write(ref)\n+    o.close()\n+    val arrays = o.toChunkedByteBuffer.getChunks().map(_.array())\n+    assert(arrays.length === 3)\n+    assert(arrays(0).length === 4 * 1024 * 1024)\n+    assert(arrays(1).length === 4 * 1024 * 1024)\n+    assert(arrays(2).length === 10 )\n+\n+    assert(arrays(0).toSeq === ref.slice(0, 4 * 1024 * 1024))\n+    assert(arrays(1).toSeq === ref.slice(4 * 1024 * 1024, 8 * 1024 * 1024))\n+    assert(arrays(2).toSeq === ref.slice(8 * 1024 * 1024, 8 * 1024 * 1024 + 10))\n+  }\n }"
  }],
  "prId": 15915
}]