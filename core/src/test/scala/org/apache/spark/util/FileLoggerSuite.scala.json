[{
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "You spelled this `John_Valjohn` but the correct spelling is `Hugh_Jackman`.\n",
    "commit": "288383735c86f0898fdeb29dc644e67509c4de57",
    "createdAt": "2014-04-30T06:03:49Z",
    "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util\n+\n+import java.io.IOException\n+\n+import scala.io.Source\n+import scala.util.Try\n+\n+import org.apache.hadoop.fs.Path\n+import org.scalatest.{BeforeAndAfter, FunSuite}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.io.CompressionCodec\n+\n+/**\n+ * Test writing files through the FileLogger.\n+ */\n+class FileLoggerSuite extends FunSuite with BeforeAndAfter {\n+  private val fileSystem = Utils.getHadoopFileSystem(\"/\")\n+  private val allCompressionCodecs = Seq[String](\n+    \"org.apache.spark.io.LZFCompressionCodec\",\n+    \"org.apache.spark.io.SnappyCompressionCodec\"\n+  )\n+  private val logDir = \"/tmp/test-file-logger\"\n+  private val logDirPath = new Path(logDir)\n+\n+  after {\n+    Try { fileSystem.delete(logDirPath, true) }\n+  }\n+\n+  test(\"Simple logging\") {\n+    testSingleFile()\n+  }\n+\n+  test (\"Simple logging with compression\") {\n+    allCompressionCodecs.foreach { codec =>\n+      testSingleFile(Some(codec))\n+    }\n+  }\n+\n+  test(\"Logging multiple files\") {\n+    testMultipleFiles()\n+  }\n+\n+  test(\"Logging multiple files with compression\") {\n+    allCompressionCodecs.foreach { codec =>\n+      testMultipleFiles(Some(codec))\n+    }\n+  }\n+\n+  test(\"Logging when directory already exists\") {\n+    // Create the logging directory multiple times\n+    new FileLogger(logDir, new SparkConf, overwrite = true)\n+    new FileLogger(logDir, new SparkConf, overwrite = true)\n+    new FileLogger(logDir, new SparkConf, overwrite = true)\n+\n+    // If overwrite is not enabled, an exception should be thrown\n+    intercept[IOException] { new FileLogger(logDir, new SparkConf, overwrite = false) }\n+  }\n+\n+\n+  /* ----------------- *\n+   * Actual test logic *\n+   * ----------------- */\n+\n+  /**\n+   * Test logging to a single file.\n+   */\n+  private def testSingleFile(codecName: Option[String] = None) {\n+    val conf = getLoggingConf(codecName)\n+    val codec = codecName.map { c => CompressionCodec.createCodec(conf) }\n+    val logger =\n+      if (codecName.isDefined) {\n+        new FileLogger(logDir, conf, compress = true)\n+      } else {\n+        new FileLogger(logDir, conf)\n+      }\n+    assert(fileSystem.exists(logDirPath))\n+    assert(fileSystem.getFileStatus(logDirPath).isDir)\n+    assert(fileSystem.listStatus(logDirPath).size === 0)\n+\n+    logger.newFile()\n+    val files = fileSystem.listStatus(logDirPath)\n+    assert(files.size === 1)\n+    val firstFile = files.head\n+    val firstFilePath = firstFile.getPath\n+\n+    logger.log(\"hello\")\n+    logger.flush()\n+    assert(readFileContent(firstFilePath, codec) === \"hello\")\n+\n+    logger.log(\" world\")\n+    logger.close()\n+    assert(readFileContent(firstFilePath, codec) === \"hello world\")\n+  }\n+\n+  /**\n+   * Test logging to multiple files.\n+   */\n+  private def testMultipleFiles(codecName: Option[String] = None) {\n+    val conf = getLoggingConf(codecName)\n+    val codec = codecName.map { c => CompressionCodec.createCodec(conf) }\n+    val logger =\n+      if (codecName.isDefined) {\n+        new FileLogger(logDir, conf, compress = true)\n+      } else {\n+        new FileLogger(logDir, conf)\n+      }\n+\n+    logger.newFile(\"Jean_Valjean\")\n+    logger.logLine(\"Who am I?\")\n+    logger.logLine(\"Destiny?\")\n+    logger.newFile(\"John_Valjohn\")"
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "Oops, my bad. It's hard to get this one right.\n",
    "commit": "288383735c86f0898fdeb29dc644e67509c4de57",
    "createdAt": "2014-04-30T06:34:17Z",
    "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util\n+\n+import java.io.IOException\n+\n+import scala.io.Source\n+import scala.util.Try\n+\n+import org.apache.hadoop.fs.Path\n+import org.scalatest.{BeforeAndAfter, FunSuite}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.io.CompressionCodec\n+\n+/**\n+ * Test writing files through the FileLogger.\n+ */\n+class FileLoggerSuite extends FunSuite with BeforeAndAfter {\n+  private val fileSystem = Utils.getHadoopFileSystem(\"/\")\n+  private val allCompressionCodecs = Seq[String](\n+    \"org.apache.spark.io.LZFCompressionCodec\",\n+    \"org.apache.spark.io.SnappyCompressionCodec\"\n+  )\n+  private val logDir = \"/tmp/test-file-logger\"\n+  private val logDirPath = new Path(logDir)\n+\n+  after {\n+    Try { fileSystem.delete(logDirPath, true) }\n+  }\n+\n+  test(\"Simple logging\") {\n+    testSingleFile()\n+  }\n+\n+  test (\"Simple logging with compression\") {\n+    allCompressionCodecs.foreach { codec =>\n+      testSingleFile(Some(codec))\n+    }\n+  }\n+\n+  test(\"Logging multiple files\") {\n+    testMultipleFiles()\n+  }\n+\n+  test(\"Logging multiple files with compression\") {\n+    allCompressionCodecs.foreach { codec =>\n+      testMultipleFiles(Some(codec))\n+    }\n+  }\n+\n+  test(\"Logging when directory already exists\") {\n+    // Create the logging directory multiple times\n+    new FileLogger(logDir, new SparkConf, overwrite = true)\n+    new FileLogger(logDir, new SparkConf, overwrite = true)\n+    new FileLogger(logDir, new SparkConf, overwrite = true)\n+\n+    // If overwrite is not enabled, an exception should be thrown\n+    intercept[IOException] { new FileLogger(logDir, new SparkConf, overwrite = false) }\n+  }\n+\n+\n+  /* ----------------- *\n+   * Actual test logic *\n+   * ----------------- */\n+\n+  /**\n+   * Test logging to a single file.\n+   */\n+  private def testSingleFile(codecName: Option[String] = None) {\n+    val conf = getLoggingConf(codecName)\n+    val codec = codecName.map { c => CompressionCodec.createCodec(conf) }\n+    val logger =\n+      if (codecName.isDefined) {\n+        new FileLogger(logDir, conf, compress = true)\n+      } else {\n+        new FileLogger(logDir, conf)\n+      }\n+    assert(fileSystem.exists(logDirPath))\n+    assert(fileSystem.getFileStatus(logDirPath).isDir)\n+    assert(fileSystem.listStatus(logDirPath).size === 0)\n+\n+    logger.newFile()\n+    val files = fileSystem.listStatus(logDirPath)\n+    assert(files.size === 1)\n+    val firstFile = files.head\n+    val firstFilePath = firstFile.getPath\n+\n+    logger.log(\"hello\")\n+    logger.flush()\n+    assert(readFileContent(firstFilePath, codec) === \"hello\")\n+\n+    logger.log(\" world\")\n+    logger.close()\n+    assert(readFileContent(firstFilePath, codec) === \"hello world\")\n+  }\n+\n+  /**\n+   * Test logging to multiple files.\n+   */\n+  private def testMultipleFiles(codecName: Option[String] = None) {\n+    val conf = getLoggingConf(codecName)\n+    val codec = codecName.map { c => CompressionCodec.createCodec(conf) }\n+    val logger =\n+      if (codecName.isDefined) {\n+        new FileLogger(logDir, conf, compress = true)\n+      } else {\n+        new FileLogger(logDir, conf)\n+      }\n+\n+    logger.newFile(\"Jean_Valjean\")\n+    logger.logLine(\"Who am I?\")\n+    logger.logLine(\"Destiny?\")\n+    logger.newFile(\"John_Valjohn\")"
  }, {
    "author": {
      "login": "pwendell"
    },
    "body": "You could also have chosen `Wolverine`\n",
    "commit": "288383735c86f0898fdeb29dc644e67509c4de57",
    "createdAt": "2014-04-30T20:00:19Z",
    "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.util\n+\n+import java.io.IOException\n+\n+import scala.io.Source\n+import scala.util.Try\n+\n+import org.apache.hadoop.fs.Path\n+import org.scalatest.{BeforeAndAfter, FunSuite}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.io.CompressionCodec\n+\n+/**\n+ * Test writing files through the FileLogger.\n+ */\n+class FileLoggerSuite extends FunSuite with BeforeAndAfter {\n+  private val fileSystem = Utils.getHadoopFileSystem(\"/\")\n+  private val allCompressionCodecs = Seq[String](\n+    \"org.apache.spark.io.LZFCompressionCodec\",\n+    \"org.apache.spark.io.SnappyCompressionCodec\"\n+  )\n+  private val logDir = \"/tmp/test-file-logger\"\n+  private val logDirPath = new Path(logDir)\n+\n+  after {\n+    Try { fileSystem.delete(logDirPath, true) }\n+  }\n+\n+  test(\"Simple logging\") {\n+    testSingleFile()\n+  }\n+\n+  test (\"Simple logging with compression\") {\n+    allCompressionCodecs.foreach { codec =>\n+      testSingleFile(Some(codec))\n+    }\n+  }\n+\n+  test(\"Logging multiple files\") {\n+    testMultipleFiles()\n+  }\n+\n+  test(\"Logging multiple files with compression\") {\n+    allCompressionCodecs.foreach { codec =>\n+      testMultipleFiles(Some(codec))\n+    }\n+  }\n+\n+  test(\"Logging when directory already exists\") {\n+    // Create the logging directory multiple times\n+    new FileLogger(logDir, new SparkConf, overwrite = true)\n+    new FileLogger(logDir, new SparkConf, overwrite = true)\n+    new FileLogger(logDir, new SparkConf, overwrite = true)\n+\n+    // If overwrite is not enabled, an exception should be thrown\n+    intercept[IOException] { new FileLogger(logDir, new SparkConf, overwrite = false) }\n+  }\n+\n+\n+  /* ----------------- *\n+   * Actual test logic *\n+   * ----------------- */\n+\n+  /**\n+   * Test logging to a single file.\n+   */\n+  private def testSingleFile(codecName: Option[String] = None) {\n+    val conf = getLoggingConf(codecName)\n+    val codec = codecName.map { c => CompressionCodec.createCodec(conf) }\n+    val logger =\n+      if (codecName.isDefined) {\n+        new FileLogger(logDir, conf, compress = true)\n+      } else {\n+        new FileLogger(logDir, conf)\n+      }\n+    assert(fileSystem.exists(logDirPath))\n+    assert(fileSystem.getFileStatus(logDirPath).isDir)\n+    assert(fileSystem.listStatus(logDirPath).size === 0)\n+\n+    logger.newFile()\n+    val files = fileSystem.listStatus(logDirPath)\n+    assert(files.size === 1)\n+    val firstFile = files.head\n+    val firstFilePath = firstFile.getPath\n+\n+    logger.log(\"hello\")\n+    logger.flush()\n+    assert(readFileContent(firstFilePath, codec) === \"hello\")\n+\n+    logger.log(\" world\")\n+    logger.close()\n+    assert(readFileContent(firstFilePath, codec) === \"hello world\")\n+  }\n+\n+  /**\n+   * Test logging to multiple files.\n+   */\n+  private def testMultipleFiles(codecName: Option[String] = None) {\n+    val conf = getLoggingConf(codecName)\n+    val codec = codecName.map { c => CompressionCodec.createCodec(conf) }\n+    val logger =\n+      if (codecName.isDefined) {\n+        new FileLogger(logDir, conf, compress = true)\n+      } else {\n+        new FileLogger(logDir, conf)\n+      }\n+\n+    logger.newFile(\"Jean_Valjean\")\n+    logger.logLine(\"Who am I?\")\n+    logger.logLine(\"Destiny?\")\n+    logger.newFile(\"John_Valjohn\")"
  }],
  "prId": 591
}]