[{
  "comments": [{
    "author": {
      "login": "tnachen"
    },
    "body": "Remove",
    "commit": "8584913d51c1f68c8f9d3cdf142947cd35cdfcc2",
    "createdAt": "2016-11-29T17:43:00Z",
    "diffHunk": "@@ -0,0 +1,236 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.kubernetes\n+\n+import java.io.File\n+import java.util.Date\n+import java.util.concurrent.atomic.AtomicLong\n+\n+import io.fabric8.kubernetes.client.{ConfigBuilder, DefaultKubernetesClient, KubernetesClient}\n+import io.fabric8.kubernetes.api.model.{PodBuilder, ServiceBuilder}\n+import io.fabric8.kubernetes.client.dsl.LogWatch\n+import org.apache.spark.deploy.Command\n+import org.apache.spark.deploy.kubernetes.ClientArguments\n+import org.apache.spark.{io, _}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+\n+import collection.JavaConverters._\n+import org.apache.spark.util.Utils\n+\n+import scala.util.Random\n+\n+private[spark] object KubernetesClusterScheduler {\n+  def defaultNameSpace = \"default\"\n+  def defaultServiceAccountName = \"default\"\n+}\n+\n+/**\n+  * This is a simple extension to ClusterScheduler\n+  * */\n+private[spark] class KubernetesClusterScheduler(conf: SparkConf)\n+    extends Logging {\n+  private val DEFAULT_SUPERVISE = false\n+  private val DEFAULT_MEMORY = Utils.DEFAULT_DRIVER_MEM_MB // mb\n+  private val DEFAULT_CORES = 1.0\n+\n+  logInfo(\"Created KubernetesClusterScheduler instance\")\n+\n+  var client = setupKubernetesClient()\n+  val driverName = s\"spark-driver-${Random.alphanumeric take 5 mkString(\"\")}\".toLowerCase()\n+  val svcName = s\"spark-svc-${Random.alphanumeric take 5 mkString(\"\")}\".toLowerCase()\n+  val nameSpace = conf.get(\n+    \"spark.kubernetes.namespace\",\n+    KubernetesClusterScheduler.defaultNameSpace)\n+  val serviceAccountName = conf.get(\n+    \"spark.kubernetes.serviceAccountName\",\n+    KubernetesClusterScheduler.defaultServiceAccountName)\n+\n+  // Anything that should either not be passed to driver config in the cluster, or\n+  // that is going to be explicitly managed as command argument to the driver pod\n+  val confBlackList = scala.collection.Set(\n+    \"spark.master\",\n+    \"spark.app.name\",\n+    \"spark.submit.deployMode\",\n+    \"spark.executor.jar\",\n+    \"spark.dynamicAllocation.enabled\",\n+    \"spark.shuffle.service.enabled\")\n+\n+  def start(args: ClientArguments): Unit = {\n+    startDriver(client, args)\n+  }\n+\n+  def stop(): Unit = {\n+    client.pods().inNamespace(nameSpace).withName(driverName).delete()\n+    client\n+      .services()\n+      .inNamespace(nameSpace)\n+      .withName(svcName)\n+      .delete()\n+  }\n+\n+  def startDriver(client: KubernetesClient,\n+                  args: ClientArguments): Unit = {\n+    logInfo(\"Starting spark driver on kubernetes cluster\")\n+    val driverDescription = buildDriverDescription(args)\n+\n+    // image needs to support shim scripts \"/opt/driver.sh\" and \"/opt/executor.sh\"\n+    val sparkImage = conf.getOption(\"spark.kubernetes.sparkImage\").getOrElse {\n+      // TODO: this needs to default to some standard Apache Spark image\n+      throw new SparkException(\"Spark image not set. Please configure spark.kubernetes.sparkImage\")\n+    }\n+\n+    // This is the URL of the client jar.\n+    val clientJarUri = args.userJar\n+\n+    // This is the kubernetes master we're launching on.\n+    val kubernetesHost = \"k8s://\" + client.getMasterUrl().getHost()\n+    logInfo(\"Using as kubernetes-master: \" + kubernetesHost.toString())\n+\n+    val submitArgs = scala.collection.mutable.ArrayBuffer.empty[String]\n+    submitArgs ++= Vector(\n+      clientJarUri,\n+      s\"--class=${args.userClass}\",\n+      s\"--master=$kubernetesHost\",\n+      s\"--executor-memory=${driverDescription.mem}\",\n+      s\"--conf spark.executor.jar=$clientJarUri\")\n+\n+    submitArgs ++= conf.getAll.filter { case (name, _) => !confBlackList.contains(name) }\n+      .map { case (name, value) => s\"--conf ${name}=${value}\" }\n+\n+    if (conf.getBoolean(\"spark.dynamicAllocation.enabled\", false)) {\n+      submitArgs ++= Vector(\n+        \"--conf spark.dynamicAllocation.enabled=true\",\n+        \"--conf spark.shuffle.service.enabled=true\")\n+    }\n+\n+    // these have to come at end of arg list\n+    submitArgs ++= Vector(\"/opt/spark/kubernetes/client.jar\",\n+      args.userArgs.mkString(\" \"))\n+\n+    val labelMap = Map(\"type\" -> \"spark-driver\")\n+    val pod = new PodBuilder()\n+      .withNewMetadata()\n+      .withLabels(labelMap.asJava)\n+      .withName(driverName)\n+      .endMetadata()\n+      .withNewSpec()\n+      .withRestartPolicy(\"OnFailure\")\n+      .withServiceAccount(serviceAccountName)\n+      .addNewContainer()\n+      .withName(\"spark-driver\")\n+      .withImage(sparkImage)\n+      .withImagePullPolicy(\"Always\")\n+      .withCommand(s\"/opt/driver.sh\")\n+      .withArgs(submitArgs :_*)\n+      .endContainer()\n+      .endSpec()\n+      .build()\n+    client.pods().inNamespace(nameSpace).withName(driverName).create(pod)\n+\n+    var svc = new ServiceBuilder()\n+      .withNewMetadata()\n+      .withLabels(labelMap.asJava)\n+      .withName(svcName)\n+      .endMetadata()\n+      .withNewSpec()\n+      .addNewPort()\n+      .withPort(4040)\n+      .withNewTargetPort()\n+      .withIntVal(4040)\n+      .endTargetPort()\n+      .endPort()\n+      .withSelector(labelMap.asJava)\n+      .withType(\"LoadBalancer\")\n+      .endSpec()\n+      .build()\n+\n+    client\n+      .services()\n+      .inNamespace(nameSpace)\n+      .withName(svcName)\n+      .create(svc)\n+\n+//    try {",
    "line": 169
  }],
  "prId": 16061
}, {
  "comments": [{
    "author": {
      "login": "mccheah"
    },
    "body": "I think if we do this here in cluster mode we will shut ourselves down and risk not actually finishing the `stop()` method. It's also possible that the `SparkContext` object and its associated components like the scheduler can be stopped, but the JVM still is doing other things afterwards. Therefore I don't think we should be deleting the driver pod here.",
    "commit": "8584913d51c1f68c8f9d3cdf142947cd35cdfcc2",
    "createdAt": "2016-11-30T20:49:08Z",
    "diffHunk": "@@ -0,0 +1,236 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.kubernetes\n+\n+import java.io.File\n+import java.util.Date\n+import java.util.concurrent.atomic.AtomicLong\n+\n+import io.fabric8.kubernetes.client.{ConfigBuilder, DefaultKubernetesClient, KubernetesClient}\n+import io.fabric8.kubernetes.api.model.{PodBuilder, ServiceBuilder}\n+import io.fabric8.kubernetes.client.dsl.LogWatch\n+import org.apache.spark.deploy.Command\n+import org.apache.spark.deploy.kubernetes.ClientArguments\n+import org.apache.spark.{io, _}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config._\n+\n+import collection.JavaConverters._\n+import org.apache.spark.util.Utils\n+\n+import scala.util.Random\n+\n+private[spark] object KubernetesClusterScheduler {\n+  def defaultNameSpace = \"default\"\n+  def defaultServiceAccountName = \"default\"\n+}\n+\n+/**\n+  * This is a simple extension to ClusterScheduler\n+  * */\n+private[spark] class KubernetesClusterScheduler(conf: SparkConf)\n+    extends Logging {\n+  private val DEFAULT_SUPERVISE = false\n+  private val DEFAULT_MEMORY = Utils.DEFAULT_DRIVER_MEM_MB // mb\n+  private val DEFAULT_CORES = 1.0\n+\n+  logInfo(\"Created KubernetesClusterScheduler instance\")\n+\n+  var client = setupKubernetesClient()\n+  val driverName = s\"spark-driver-${Random.alphanumeric take 5 mkString(\"\")}\".toLowerCase()\n+  val svcName = s\"spark-svc-${Random.alphanumeric take 5 mkString(\"\")}\".toLowerCase()\n+  val nameSpace = conf.get(\n+    \"spark.kubernetes.namespace\",\n+    KubernetesClusterScheduler.defaultNameSpace)\n+  val serviceAccountName = conf.get(\n+    \"spark.kubernetes.serviceAccountName\",\n+    KubernetesClusterScheduler.defaultServiceAccountName)\n+\n+  // Anything that should either not be passed to driver config in the cluster, or\n+  // that is going to be explicitly managed as command argument to the driver pod\n+  val confBlackList = scala.collection.Set(\n+    \"spark.master\",\n+    \"spark.app.name\",\n+    \"spark.submit.deployMode\",\n+    \"spark.executor.jar\",\n+    \"spark.dynamicAllocation.enabled\",\n+    \"spark.shuffle.service.enabled\")\n+\n+  def start(args: ClientArguments): Unit = {\n+    startDriver(client, args)\n+  }\n+\n+  def stop(): Unit = {\n+    client.pods().inNamespace(nameSpace).withName(driverName).delete()",
    "line": 79
  }],
  "prId": 16061
}]