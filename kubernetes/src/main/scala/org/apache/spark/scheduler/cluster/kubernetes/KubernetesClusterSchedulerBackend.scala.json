[{
  "comments": [{
    "author": {
      "login": "tnachen"
    },
    "body": "This shouldn't happen, assert instead",
    "commit": "8584913d51c1f68c8f9d3cdf142947cd35cdfcc2",
    "createdAt": "2016-11-29T17:42:14Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.kubernetes\n+\n+import collection.JavaConverters._\n+import io.fabric8.kubernetes.api.model.PodBuilder\n+import io.fabric8.kubernetes.api.model.extensions.JobBuilder\n+import io.fabric8.kubernetes.client.{ConfigBuilder, DefaultKubernetesClient}\n+import org.apache.spark.internal.config._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster._\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages._\n+import org.apache.spark.{SparkConf, SparkContext, SparkException}\n+import org.apache.spark.rpc.RpcEndpointAddress\n+import org.apache.spark.scheduler.TaskSchedulerImpl\n+import org.apache.spark.util.Utils\n+\n+import scala.collection.mutable\n+import scala.util.Random\n+import scala.concurrent.Future\n+\n+private[spark] class KubernetesClusterSchedulerBackend(\n+                                                  scheduler: TaskSchedulerImpl,\n+                                                  sc: SparkContext)\n+  extends CoarseGrainedSchedulerBackend(scheduler, sc.env.rpcEnv) {\n+\n+  val client = new DefaultKubernetesClient()\n+\n+  val DEFAULT_NUMBER_EXECUTORS = 2\n+  val sparkExecutorName = s\"spark-executor-${Random.alphanumeric take 5 mkString(\"\")}\".toLowerCase()\n+\n+  // TODO: do these need mutex guarding?\n+  // key is executor id, value is pod name\n+  var executorToPod = mutable.Map.empty[String, String] // active executors\n+  var shutdownToPod = mutable.Map.empty[String, String] // pending shutdown\n+  var executorID = 0\n+\n+  val sparkImage = conf.get(\"spark.kubernetes.sparkImage\")\n+  val clientJarUri = conf.get(\"spark.executor.jar\")\n+  val ns = conf.get(\n+    \"spark.kubernetes.namespace\",\n+    KubernetesClusterScheduler.defaultNameSpace)\n+  val dynamicExecutors = Utils.isDynamicAllocationEnabled(conf)\n+\n+  // executor back-ends take their configuration this way\n+  if (dynamicExecutors) {\n+    conf.setExecutorEnv(\"spark.dynamicAllocation.enabled\", \"true\")\n+    conf.setExecutorEnv(\"spark.shuffle.service.enabled\", \"true\")\n+  }\n+\n+  override def start(): Unit = {\n+    super.start()\n+    createExecutorPods(getInitialTargetExecutorNumber(sc.getConf))\n+  }\n+\n+  override def stop(): Unit = {\n+    // Kill all executor pods indiscriminately\n+    killExecutorPods(executorToPod.toVector)\n+    killExecutorPods(shutdownToPod.toVector)\n+    super.stop()\n+  }\n+\n+  // Dynamic allocation interfaces\n+  override def doRequestTotalExecutors(requestedTotal: Int): Future[Boolean] = {\n+    logInfo(s\"Received doRequestTotalExecutors: $requestedTotal\")\n+    val n = executorToPod.size\n+    val delta = requestedTotal - n\n+    if (delta > 0) {\n+      logInfo(s\"Adding $delta new executors\")\n+      createExecutorPods(delta)\n+    } else if (delta < 0) {\n+      val d = -delta",
    "line": 87
  }, {
    "author": {
      "login": "erikerlandson"
    },
    "body": "Delta (as currently computed) can become negative, whenever the requested total starts to decrease.\r\n\r\nHowever, this logic warrants a total overhaul, since it was designed around the assumption that `doRequestTotalExecutors` is symmetric, in the sense of having to handle a decreasing total analogously to an increasing total.  But responsibilities for executor shutdown vs spin-up aren't symmetric.  I might file a small doc PR to clarify that point.",
    "commit": "8584913d51c1f68c8f9d3cdf142947cd35cdfcc2",
    "createdAt": "2016-11-29T19:34:44Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.kubernetes\n+\n+import collection.JavaConverters._\n+import io.fabric8.kubernetes.api.model.PodBuilder\n+import io.fabric8.kubernetes.api.model.extensions.JobBuilder\n+import io.fabric8.kubernetes.client.{ConfigBuilder, DefaultKubernetesClient}\n+import org.apache.spark.internal.config._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster._\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages._\n+import org.apache.spark.{SparkConf, SparkContext, SparkException}\n+import org.apache.spark.rpc.RpcEndpointAddress\n+import org.apache.spark.scheduler.TaskSchedulerImpl\n+import org.apache.spark.util.Utils\n+\n+import scala.collection.mutable\n+import scala.util.Random\n+import scala.concurrent.Future\n+\n+private[spark] class KubernetesClusterSchedulerBackend(\n+                                                  scheduler: TaskSchedulerImpl,\n+                                                  sc: SparkContext)\n+  extends CoarseGrainedSchedulerBackend(scheduler, sc.env.rpcEnv) {\n+\n+  val client = new DefaultKubernetesClient()\n+\n+  val DEFAULT_NUMBER_EXECUTORS = 2\n+  val sparkExecutorName = s\"spark-executor-${Random.alphanumeric take 5 mkString(\"\")}\".toLowerCase()\n+\n+  // TODO: do these need mutex guarding?\n+  // key is executor id, value is pod name\n+  var executorToPod = mutable.Map.empty[String, String] // active executors\n+  var shutdownToPod = mutable.Map.empty[String, String] // pending shutdown\n+  var executorID = 0\n+\n+  val sparkImage = conf.get(\"spark.kubernetes.sparkImage\")\n+  val clientJarUri = conf.get(\"spark.executor.jar\")\n+  val ns = conf.get(\n+    \"spark.kubernetes.namespace\",\n+    KubernetesClusterScheduler.defaultNameSpace)\n+  val dynamicExecutors = Utils.isDynamicAllocationEnabled(conf)\n+\n+  // executor back-ends take their configuration this way\n+  if (dynamicExecutors) {\n+    conf.setExecutorEnv(\"spark.dynamicAllocation.enabled\", \"true\")\n+    conf.setExecutorEnv(\"spark.shuffle.service.enabled\", \"true\")\n+  }\n+\n+  override def start(): Unit = {\n+    super.start()\n+    createExecutorPods(getInitialTargetExecutorNumber(sc.getConf))\n+  }\n+\n+  override def stop(): Unit = {\n+    // Kill all executor pods indiscriminately\n+    killExecutorPods(executorToPod.toVector)\n+    killExecutorPods(shutdownToPod.toVector)\n+    super.stop()\n+  }\n+\n+  // Dynamic allocation interfaces\n+  override def doRequestTotalExecutors(requestedTotal: Int): Future[Boolean] = {\n+    logInfo(s\"Received doRequestTotalExecutors: $requestedTotal\")\n+    val n = executorToPod.size\n+    val delta = requestedTotal - n\n+    if (delta > 0) {\n+      logInfo(s\"Adding $delta new executors\")\n+      createExecutorPods(delta)\n+    } else if (delta < 0) {\n+      val d = -delta",
    "line": 87
  }, {
    "author": {
      "login": "foxish"
    },
    "body": "We also need to add a method to find the not-ready pods in the system from the earlier scaling event, before we decide to create a new one. We had a discussion about this [here](https://github.com/foxish/spark/pull/4#issuecomment-260722475). I think relying on people to set resource requests/limits, or writing admission controllers correctly is one part of the solution, but we should also have an additional safeguard to ensure we're not flooding the system with pod creation requests.",
    "commit": "8584913d51c1f68c8f9d3cdf142947cd35cdfcc2",
    "createdAt": "2016-11-30T22:17:34Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.kubernetes\n+\n+import collection.JavaConverters._\n+import io.fabric8.kubernetes.api.model.PodBuilder\n+import io.fabric8.kubernetes.api.model.extensions.JobBuilder\n+import io.fabric8.kubernetes.client.{ConfigBuilder, DefaultKubernetesClient}\n+import org.apache.spark.internal.config._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster._\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages._\n+import org.apache.spark.{SparkConf, SparkContext, SparkException}\n+import org.apache.spark.rpc.RpcEndpointAddress\n+import org.apache.spark.scheduler.TaskSchedulerImpl\n+import org.apache.spark.util.Utils\n+\n+import scala.collection.mutable\n+import scala.util.Random\n+import scala.concurrent.Future\n+\n+private[spark] class KubernetesClusterSchedulerBackend(\n+                                                  scheduler: TaskSchedulerImpl,\n+                                                  sc: SparkContext)\n+  extends CoarseGrainedSchedulerBackend(scheduler, sc.env.rpcEnv) {\n+\n+  val client = new DefaultKubernetesClient()\n+\n+  val DEFAULT_NUMBER_EXECUTORS = 2\n+  val sparkExecutorName = s\"spark-executor-${Random.alphanumeric take 5 mkString(\"\")}\".toLowerCase()\n+\n+  // TODO: do these need mutex guarding?\n+  // key is executor id, value is pod name\n+  var executorToPod = mutable.Map.empty[String, String] // active executors\n+  var shutdownToPod = mutable.Map.empty[String, String] // pending shutdown\n+  var executorID = 0\n+\n+  val sparkImage = conf.get(\"spark.kubernetes.sparkImage\")\n+  val clientJarUri = conf.get(\"spark.executor.jar\")\n+  val ns = conf.get(\n+    \"spark.kubernetes.namespace\",\n+    KubernetesClusterScheduler.defaultNameSpace)\n+  val dynamicExecutors = Utils.isDynamicAllocationEnabled(conf)\n+\n+  // executor back-ends take their configuration this way\n+  if (dynamicExecutors) {\n+    conf.setExecutorEnv(\"spark.dynamicAllocation.enabled\", \"true\")\n+    conf.setExecutorEnv(\"spark.shuffle.service.enabled\", \"true\")\n+  }\n+\n+  override def start(): Unit = {\n+    super.start()\n+    createExecutorPods(getInitialTargetExecutorNumber(sc.getConf))\n+  }\n+\n+  override def stop(): Unit = {\n+    // Kill all executor pods indiscriminately\n+    killExecutorPods(executorToPod.toVector)\n+    killExecutorPods(shutdownToPod.toVector)\n+    super.stop()\n+  }\n+\n+  // Dynamic allocation interfaces\n+  override def doRequestTotalExecutors(requestedTotal: Int): Future[Boolean] = {\n+    logInfo(s\"Received doRequestTotalExecutors: $requestedTotal\")\n+    val n = executorToPod.size\n+    val delta = requestedTotal - n\n+    if (delta > 0) {\n+      logInfo(s\"Adding $delta new executors\")\n+      createExecutorPods(delta)\n+    } else if (delta < 0) {\n+      val d = -delta",
    "line": 87
  }, {
    "author": {
      "login": "erikerlandson"
    },
    "body": "I had started in on keeping track of new pods until they stop saying \"pending\", however I think this may be a good use case for a `PodWatcher` ",
    "commit": "8584913d51c1f68c8f9d3cdf142947cd35cdfcc2",
    "createdAt": "2016-11-30T22:23:40Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.kubernetes\n+\n+import collection.JavaConverters._\n+import io.fabric8.kubernetes.api.model.PodBuilder\n+import io.fabric8.kubernetes.api.model.extensions.JobBuilder\n+import io.fabric8.kubernetes.client.{ConfigBuilder, DefaultKubernetesClient}\n+import org.apache.spark.internal.config._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster._\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages._\n+import org.apache.spark.{SparkConf, SparkContext, SparkException}\n+import org.apache.spark.rpc.RpcEndpointAddress\n+import org.apache.spark.scheduler.TaskSchedulerImpl\n+import org.apache.spark.util.Utils\n+\n+import scala.collection.mutable\n+import scala.util.Random\n+import scala.concurrent.Future\n+\n+private[spark] class KubernetesClusterSchedulerBackend(\n+                                                  scheduler: TaskSchedulerImpl,\n+                                                  sc: SparkContext)\n+  extends CoarseGrainedSchedulerBackend(scheduler, sc.env.rpcEnv) {\n+\n+  val client = new DefaultKubernetesClient()\n+\n+  val DEFAULT_NUMBER_EXECUTORS = 2\n+  val sparkExecutorName = s\"spark-executor-${Random.alphanumeric take 5 mkString(\"\")}\".toLowerCase()\n+\n+  // TODO: do these need mutex guarding?\n+  // key is executor id, value is pod name\n+  var executorToPod = mutable.Map.empty[String, String] // active executors\n+  var shutdownToPod = mutable.Map.empty[String, String] // pending shutdown\n+  var executorID = 0\n+\n+  val sparkImage = conf.get(\"spark.kubernetes.sparkImage\")\n+  val clientJarUri = conf.get(\"spark.executor.jar\")\n+  val ns = conf.get(\n+    \"spark.kubernetes.namespace\",\n+    KubernetesClusterScheduler.defaultNameSpace)\n+  val dynamicExecutors = Utils.isDynamicAllocationEnabled(conf)\n+\n+  // executor back-ends take their configuration this way\n+  if (dynamicExecutors) {\n+    conf.setExecutorEnv(\"spark.dynamicAllocation.enabled\", \"true\")\n+    conf.setExecutorEnv(\"spark.shuffle.service.enabled\", \"true\")\n+  }\n+\n+  override def start(): Unit = {\n+    super.start()\n+    createExecutorPods(getInitialTargetExecutorNumber(sc.getConf))\n+  }\n+\n+  override def stop(): Unit = {\n+    // Kill all executor pods indiscriminately\n+    killExecutorPods(executorToPod.toVector)\n+    killExecutorPods(shutdownToPod.toVector)\n+    super.stop()\n+  }\n+\n+  // Dynamic allocation interfaces\n+  override def doRequestTotalExecutors(requestedTotal: Int): Future[Boolean] = {\n+    logInfo(s\"Received doRequestTotalExecutors: $requestedTotal\")\n+    val n = executorToPod.size\n+    val delta = requestedTotal - n\n+    if (delta > 0) {\n+      logInfo(s\"Adding $delta new executors\")\n+      createExecutorPods(delta)\n+    } else if (delta < 0) {\n+      val d = -delta",
    "line": 87
  }],
  "prId": 16061
}, {
  "comments": [{
    "author": {
      "login": "tnachen"
    },
    "body": "Fix the formatting to conform with Spark style",
    "commit": "8584913d51c1f68c8f9d3cdf142947cd35cdfcc2",
    "createdAt": "2016-11-29T17:42:38Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.kubernetes\n+\n+import collection.JavaConverters._\n+import io.fabric8.kubernetes.api.model.PodBuilder\n+import io.fabric8.kubernetes.api.model.extensions.JobBuilder\n+import io.fabric8.kubernetes.client.{ConfigBuilder, DefaultKubernetesClient}\n+import org.apache.spark.internal.config._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster._\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages._\n+import org.apache.spark.{SparkConf, SparkContext, SparkException}\n+import org.apache.spark.rpc.RpcEndpointAddress\n+import org.apache.spark.scheduler.TaskSchedulerImpl\n+import org.apache.spark.util.Utils\n+\n+import scala.collection.mutable\n+import scala.util.Random\n+import scala.concurrent.Future\n+\n+private[spark] class KubernetesClusterSchedulerBackend(\n+                                                  scheduler: TaskSchedulerImpl,",
    "line": 38
  }],
  "prId": 16061
}, {
  "comments": [{
    "author": {
      "login": "mccheah"
    },
    "body": "Another approach that we could take is to put the command in the Dockerfile and supply the container with environment variables to configure the runtime behavior. A few benefits to this:\r\n- Transparency: Instead of needing to inspect the shim scripts to discover the specific execution behavior, the behavior is well-defined in the Dockerfile.\r\n- Immutability: The run command does not need to be assembled by the client code every time.\r\n- Decoupling configuration from logic: The Docker containers are only configured with properties, as opposed to needing to specify both logic (running the shim script) and configuration.",
    "commit": "8584913d51c1f68c8f9d3cdf142947cd35cdfcc2",
    "createdAt": "2016-11-30T20:26:09Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.kubernetes\n+\n+import collection.JavaConverters._\n+import io.fabric8.kubernetes.api.model.PodBuilder\n+import io.fabric8.kubernetes.api.model.extensions.JobBuilder\n+import io.fabric8.kubernetes.client.{ConfigBuilder, DefaultKubernetesClient}\n+import org.apache.spark.internal.config._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster._\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages._\n+import org.apache.spark.{SparkConf, SparkContext, SparkException}\n+import org.apache.spark.rpc.RpcEndpointAddress\n+import org.apache.spark.scheduler.TaskSchedulerImpl\n+import org.apache.spark.util.Utils\n+\n+import scala.collection.mutable\n+import scala.util.Random\n+import scala.concurrent.Future\n+\n+private[spark] class KubernetesClusterSchedulerBackend(\n+                                                  scheduler: TaskSchedulerImpl,\n+                                                  sc: SparkContext)\n+  extends CoarseGrainedSchedulerBackend(scheduler, sc.env.rpcEnv) {\n+\n+  val client = new DefaultKubernetesClient()\n+\n+  val DEFAULT_NUMBER_EXECUTORS = 2\n+  val sparkExecutorName = s\"spark-executor-${Random.alphanumeric take 5 mkString(\"\")}\".toLowerCase()\n+\n+  // TODO: do these need mutex guarding?\n+  // key is executor id, value is pod name\n+  var executorToPod = mutable.Map.empty[String, String] // active executors\n+  var shutdownToPod = mutable.Map.empty[String, String] // pending shutdown\n+  var executorID = 0\n+\n+  val sparkImage = conf.get(\"spark.kubernetes.sparkImage\")\n+  val clientJarUri = conf.get(\"spark.executor.jar\")\n+  val ns = conf.get(\n+    \"spark.kubernetes.namespace\",\n+    KubernetesClusterScheduler.defaultNameSpace)\n+  val dynamicExecutors = Utils.isDynamicAllocationEnabled(conf)\n+\n+  // executor back-ends take their configuration this way\n+  if (dynamicExecutors) {\n+    conf.setExecutorEnv(\"spark.dynamicAllocation.enabled\", \"true\")\n+    conf.setExecutorEnv(\"spark.shuffle.service.enabled\", \"true\")\n+  }\n+\n+  override def start(): Unit = {\n+    super.start()\n+    createExecutorPods(getInitialTargetExecutorNumber(sc.getConf))\n+  }\n+\n+  override def stop(): Unit = {\n+    // Kill all executor pods indiscriminately\n+    killExecutorPods(executorToPod.toVector)\n+    killExecutorPods(shutdownToPod.toVector)\n+    super.stop()\n+  }\n+\n+  // Dynamic allocation interfaces\n+  override def doRequestTotalExecutors(requestedTotal: Int): Future[Boolean] = {\n+    logInfo(s\"Received doRequestTotalExecutors: $requestedTotal\")\n+    val n = executorToPod.size\n+    val delta = requestedTotal - n\n+    if (delta > 0) {\n+      logInfo(s\"Adding $delta new executors\")\n+      createExecutorPods(delta)\n+    } else if (delta < 0) {\n+      val d = -delta\n+      val idle = executorToPod.toVector.filter { case (id, _) => !scheduler.isExecutorBusy(id) }\n+      if (idle.length > 0) {\n+        logInfo(s\"Shutting down ${idle.length} idle executors\")\n+        shutdownExecutors(idle.take(d))\n+      }\n+      val r = math.max(0, d - idle.length)\n+      if (r > 0) {\n+        logInfo(s\"Shutting down $r non-idle executors\")\n+        shutdownExecutors(executorToPod.toVector.slice(n - r, n))\n+      }\n+    }\n+    // TODO: are there meaningful failure modes here?\n+    Future.successful(true)\n+  }\n+\n+  override def doKillExecutors(executorIds: Seq[String]): Future[Boolean] = {\n+    logInfo(s\"Received doKillExecutors\")\n+    killExecutorPods(executorIds.map { id => (id, executorToPod(id)) })\n+    Future.successful(true)\n+  }\n+\n+  private def createExecutorPods(n: Int) {\n+    for (i <- 1 to n) {\n+      executorID += 1\n+      executorToPod += ((executorID.toString, createExecutorPod(executorID)))\n+    }\n+  }\n+\n+  def shutdownExecutors(idPodPairs: Seq[(String, String)]) {\n+    val active = getExecutorIds.toSet\n+\n+    // Check for any finished shutting down and kill the pods\n+    val shutdown = shutdownToPod.toVector.filter { case (e, _) => !active.contains(e) }\n+    killExecutorPods(shutdown)\n+\n+    // Now request shutdown for the new ones.\n+    // Move them from executor list to list pending shutdown\n+    for ((id, pod) <- idPodPairs) {\n+      try {\n+        // TODO: 'ask' returns a future - can it be used to check eventual success?\n+        Option(driverEndpoint).foreach(_.ask[Boolean](RemoveExecutor(id, ExecutorKilled)))\n+        executorToPod -= id\n+        shutdownToPod += ((id, pod))\n+      } catch {\n+        case e: Exception => logError(s\"Error shutting down executor $id\", e)\n+      }\n+    }\n+  }\n+\n+  private def killExecutorPods(idPodPairs: Seq[(String, String)]) {\n+    for ((id, pod) <- idPodPairs) {\n+      try {\n+        client.pods().inNamespace(ns).withName(pod).delete()\n+        executorToPod -= id\n+        shutdownToPod -= id\n+      } catch {\n+        case e: Exception => logError(s\"Error killing executor pod $pod\", e)\n+      }\n+    }\n+  }\n+\n+  def getInitialTargetExecutorNumber(conf: SparkConf,\n+                                     numExecutors: Int =\n+                                     DEFAULT_NUMBER_EXECUTORS): Int = {\n+    if (dynamicExecutors) {\n+      val minNumExecutors = conf.get(DYN_ALLOCATION_MIN_EXECUTORS)\n+      val initialNumExecutors =\n+        Utils.getDynamicAllocationInitialExecutors(conf)\n+      val maxNumExecutors = conf.get(DYN_ALLOCATION_MAX_EXECUTORS)\n+      require(\n+        initialNumExecutors >= minNumExecutors && initialNumExecutors <= maxNumExecutors,\n+        s\"initial executor number $initialNumExecutors must between min executor number \" +\n+          s\"$minNumExecutors and max executor number $maxNumExecutors\")\n+\n+      initialNumExecutors\n+    } else {\n+      conf.get(EXECUTOR_INSTANCES).getOrElse(numExecutors)\n+    }\n+  }\n+\n+  def createExecutorPod(executorNum: Int): String = {\n+    // create a single k8s executor pod.\n+    val labelMap = Map(\"type\" -> \"spark-executor\")\n+    val podName = s\"$sparkExecutorName-$executorNum\"\n+\n+    val submitArgs = mutable.ArrayBuffer.empty[String]\n+\n+    if (conf.getBoolean(\"spark.dynamicAllocation.enabled\", false)) {\n+      submitArgs ++= Vector(\n+        \"dynamic-executors\")\n+    }\n+\n+    submitArgs ++= Vector(\"org.apache.spark.executor.CoarseGrainedExecutorBackend\",\n+      \"--driver-url\", s\"$driverURL\",\n+      \"--executor-id\", s\"$executorNum\",\n+      \"--hostname\", \"localhost\",\n+      \"--app-id\", \"1\", // TODO: change app-id per application and pass from driver.\n+      \"--cores\", \"1\")\n+\n+    var pod = new PodBuilder()\n+      .withNewMetadata()\n+      .withLabels(labelMap.asJava)\n+      .withName(podName)\n+      .endMetadata()\n+      .withNewSpec()\n+      .withRestartPolicy(\"OnFailure\")\n+\n+      .addNewContainer().withName(\"spark-executor\").withImage(sparkImage)\n+      .withImagePullPolicy(\"IfNotPresent\")\n+      .withCommand(\"/opt/executor.sh\")",
    "line": 197
  }, {
    "author": {
      "login": "erikerlandson"
    },
    "body": "I think if we can just execute the container and pass config via something along the lines of `SPARK_MASTER_OPTS` (is there `SPARK_DRIVER_OPTS` ?), that could also implicitly handle the case of running a custom user container.  It might shift documentation of conventions to which env vars to expect, but if we can use standard spark env-vars that would be idiomatic.",
    "commit": "8584913d51c1f68c8f9d3cdf142947cd35cdfcc2",
    "createdAt": "2016-11-30T21:49:39Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.kubernetes\n+\n+import collection.JavaConverters._\n+import io.fabric8.kubernetes.api.model.PodBuilder\n+import io.fabric8.kubernetes.api.model.extensions.JobBuilder\n+import io.fabric8.kubernetes.client.{ConfigBuilder, DefaultKubernetesClient}\n+import org.apache.spark.internal.config._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster._\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages._\n+import org.apache.spark.{SparkConf, SparkContext, SparkException}\n+import org.apache.spark.rpc.RpcEndpointAddress\n+import org.apache.spark.scheduler.TaskSchedulerImpl\n+import org.apache.spark.util.Utils\n+\n+import scala.collection.mutable\n+import scala.util.Random\n+import scala.concurrent.Future\n+\n+private[spark] class KubernetesClusterSchedulerBackend(\n+                                                  scheduler: TaskSchedulerImpl,\n+                                                  sc: SparkContext)\n+  extends CoarseGrainedSchedulerBackend(scheduler, sc.env.rpcEnv) {\n+\n+  val client = new DefaultKubernetesClient()\n+\n+  val DEFAULT_NUMBER_EXECUTORS = 2\n+  val sparkExecutorName = s\"spark-executor-${Random.alphanumeric take 5 mkString(\"\")}\".toLowerCase()\n+\n+  // TODO: do these need mutex guarding?\n+  // key is executor id, value is pod name\n+  var executorToPod = mutable.Map.empty[String, String] // active executors\n+  var shutdownToPod = mutable.Map.empty[String, String] // pending shutdown\n+  var executorID = 0\n+\n+  val sparkImage = conf.get(\"spark.kubernetes.sparkImage\")\n+  val clientJarUri = conf.get(\"spark.executor.jar\")\n+  val ns = conf.get(\n+    \"spark.kubernetes.namespace\",\n+    KubernetesClusterScheduler.defaultNameSpace)\n+  val dynamicExecutors = Utils.isDynamicAllocationEnabled(conf)\n+\n+  // executor back-ends take their configuration this way\n+  if (dynamicExecutors) {\n+    conf.setExecutorEnv(\"spark.dynamicAllocation.enabled\", \"true\")\n+    conf.setExecutorEnv(\"spark.shuffle.service.enabled\", \"true\")\n+  }\n+\n+  override def start(): Unit = {\n+    super.start()\n+    createExecutorPods(getInitialTargetExecutorNumber(sc.getConf))\n+  }\n+\n+  override def stop(): Unit = {\n+    // Kill all executor pods indiscriminately\n+    killExecutorPods(executorToPod.toVector)\n+    killExecutorPods(shutdownToPod.toVector)\n+    super.stop()\n+  }\n+\n+  // Dynamic allocation interfaces\n+  override def doRequestTotalExecutors(requestedTotal: Int): Future[Boolean] = {\n+    logInfo(s\"Received doRequestTotalExecutors: $requestedTotal\")\n+    val n = executorToPod.size\n+    val delta = requestedTotal - n\n+    if (delta > 0) {\n+      logInfo(s\"Adding $delta new executors\")\n+      createExecutorPods(delta)\n+    } else if (delta < 0) {\n+      val d = -delta\n+      val idle = executorToPod.toVector.filter { case (id, _) => !scheduler.isExecutorBusy(id) }\n+      if (idle.length > 0) {\n+        logInfo(s\"Shutting down ${idle.length} idle executors\")\n+        shutdownExecutors(idle.take(d))\n+      }\n+      val r = math.max(0, d - idle.length)\n+      if (r > 0) {\n+        logInfo(s\"Shutting down $r non-idle executors\")\n+        shutdownExecutors(executorToPod.toVector.slice(n - r, n))\n+      }\n+    }\n+    // TODO: are there meaningful failure modes here?\n+    Future.successful(true)\n+  }\n+\n+  override def doKillExecutors(executorIds: Seq[String]): Future[Boolean] = {\n+    logInfo(s\"Received doKillExecutors\")\n+    killExecutorPods(executorIds.map { id => (id, executorToPod(id)) })\n+    Future.successful(true)\n+  }\n+\n+  private def createExecutorPods(n: Int) {\n+    for (i <- 1 to n) {\n+      executorID += 1\n+      executorToPod += ((executorID.toString, createExecutorPod(executorID)))\n+    }\n+  }\n+\n+  def shutdownExecutors(idPodPairs: Seq[(String, String)]) {\n+    val active = getExecutorIds.toSet\n+\n+    // Check for any finished shutting down and kill the pods\n+    val shutdown = shutdownToPod.toVector.filter { case (e, _) => !active.contains(e) }\n+    killExecutorPods(shutdown)\n+\n+    // Now request shutdown for the new ones.\n+    // Move them from executor list to list pending shutdown\n+    for ((id, pod) <- idPodPairs) {\n+      try {\n+        // TODO: 'ask' returns a future - can it be used to check eventual success?\n+        Option(driverEndpoint).foreach(_.ask[Boolean](RemoveExecutor(id, ExecutorKilled)))\n+        executorToPod -= id\n+        shutdownToPod += ((id, pod))\n+      } catch {\n+        case e: Exception => logError(s\"Error shutting down executor $id\", e)\n+      }\n+    }\n+  }\n+\n+  private def killExecutorPods(idPodPairs: Seq[(String, String)]) {\n+    for ((id, pod) <- idPodPairs) {\n+      try {\n+        client.pods().inNamespace(ns).withName(pod).delete()\n+        executorToPod -= id\n+        shutdownToPod -= id\n+      } catch {\n+        case e: Exception => logError(s\"Error killing executor pod $pod\", e)\n+      }\n+    }\n+  }\n+\n+  def getInitialTargetExecutorNumber(conf: SparkConf,\n+                                     numExecutors: Int =\n+                                     DEFAULT_NUMBER_EXECUTORS): Int = {\n+    if (dynamicExecutors) {\n+      val minNumExecutors = conf.get(DYN_ALLOCATION_MIN_EXECUTORS)\n+      val initialNumExecutors =\n+        Utils.getDynamicAllocationInitialExecutors(conf)\n+      val maxNumExecutors = conf.get(DYN_ALLOCATION_MAX_EXECUTORS)\n+      require(\n+        initialNumExecutors >= minNumExecutors && initialNumExecutors <= maxNumExecutors,\n+        s\"initial executor number $initialNumExecutors must between min executor number \" +\n+          s\"$minNumExecutors and max executor number $maxNumExecutors\")\n+\n+      initialNumExecutors\n+    } else {\n+      conf.get(EXECUTOR_INSTANCES).getOrElse(numExecutors)\n+    }\n+  }\n+\n+  def createExecutorPod(executorNum: Int): String = {\n+    // create a single k8s executor pod.\n+    val labelMap = Map(\"type\" -> \"spark-executor\")\n+    val podName = s\"$sparkExecutorName-$executorNum\"\n+\n+    val submitArgs = mutable.ArrayBuffer.empty[String]\n+\n+    if (conf.getBoolean(\"spark.dynamicAllocation.enabled\", false)) {\n+      submitArgs ++= Vector(\n+        \"dynamic-executors\")\n+    }\n+\n+    submitArgs ++= Vector(\"org.apache.spark.executor.CoarseGrainedExecutorBackend\",\n+      \"--driver-url\", s\"$driverURL\",\n+      \"--executor-id\", s\"$executorNum\",\n+      \"--hostname\", \"localhost\",\n+      \"--app-id\", \"1\", // TODO: change app-id per application and pass from driver.\n+      \"--cores\", \"1\")\n+\n+    var pod = new PodBuilder()\n+      .withNewMetadata()\n+      .withLabels(labelMap.asJava)\n+      .withName(podName)\n+      .endMetadata()\n+      .withNewSpec()\n+      .withRestartPolicy(\"OnFailure\")\n+\n+      .addNewContainer().withName(\"spark-executor\").withImage(sparkImage)\n+      .withImagePullPolicy(\"IfNotPresent\")\n+      .withCommand(\"/opt/executor.sh\")",
    "line": 197
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "That makes sense - essentially the Dockerfile has this line:\r\n\r\n`CMD exec ${JAVA_HOME}/bin/java -Xmx$SPARK_EXECUTOR_MEMORY org.apache.spark... --executor-id $SPARK_EXECUTOR_ID ...` etc.\r\n\r\nAnd then we set these environment variables on the container.",
    "commit": "8584913d51c1f68c8f9d3cdf142947cd35cdfcc2",
    "createdAt": "2016-11-30T22:01:20Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.kubernetes\n+\n+import collection.JavaConverters._\n+import io.fabric8.kubernetes.api.model.PodBuilder\n+import io.fabric8.kubernetes.api.model.extensions.JobBuilder\n+import io.fabric8.kubernetes.client.{ConfigBuilder, DefaultKubernetesClient}\n+import org.apache.spark.internal.config._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster._\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages._\n+import org.apache.spark.{SparkConf, SparkContext, SparkException}\n+import org.apache.spark.rpc.RpcEndpointAddress\n+import org.apache.spark.scheduler.TaskSchedulerImpl\n+import org.apache.spark.util.Utils\n+\n+import scala.collection.mutable\n+import scala.util.Random\n+import scala.concurrent.Future\n+\n+private[spark] class KubernetesClusterSchedulerBackend(\n+                                                  scheduler: TaskSchedulerImpl,\n+                                                  sc: SparkContext)\n+  extends CoarseGrainedSchedulerBackend(scheduler, sc.env.rpcEnv) {\n+\n+  val client = new DefaultKubernetesClient()\n+\n+  val DEFAULT_NUMBER_EXECUTORS = 2\n+  val sparkExecutorName = s\"spark-executor-${Random.alphanumeric take 5 mkString(\"\")}\".toLowerCase()\n+\n+  // TODO: do these need mutex guarding?\n+  // key is executor id, value is pod name\n+  var executorToPod = mutable.Map.empty[String, String] // active executors\n+  var shutdownToPod = mutable.Map.empty[String, String] // pending shutdown\n+  var executorID = 0\n+\n+  val sparkImage = conf.get(\"spark.kubernetes.sparkImage\")\n+  val clientJarUri = conf.get(\"spark.executor.jar\")\n+  val ns = conf.get(\n+    \"spark.kubernetes.namespace\",\n+    KubernetesClusterScheduler.defaultNameSpace)\n+  val dynamicExecutors = Utils.isDynamicAllocationEnabled(conf)\n+\n+  // executor back-ends take their configuration this way\n+  if (dynamicExecutors) {\n+    conf.setExecutorEnv(\"spark.dynamicAllocation.enabled\", \"true\")\n+    conf.setExecutorEnv(\"spark.shuffle.service.enabled\", \"true\")\n+  }\n+\n+  override def start(): Unit = {\n+    super.start()\n+    createExecutorPods(getInitialTargetExecutorNumber(sc.getConf))\n+  }\n+\n+  override def stop(): Unit = {\n+    // Kill all executor pods indiscriminately\n+    killExecutorPods(executorToPod.toVector)\n+    killExecutorPods(shutdownToPod.toVector)\n+    super.stop()\n+  }\n+\n+  // Dynamic allocation interfaces\n+  override def doRequestTotalExecutors(requestedTotal: Int): Future[Boolean] = {\n+    logInfo(s\"Received doRequestTotalExecutors: $requestedTotal\")\n+    val n = executorToPod.size\n+    val delta = requestedTotal - n\n+    if (delta > 0) {\n+      logInfo(s\"Adding $delta new executors\")\n+      createExecutorPods(delta)\n+    } else if (delta < 0) {\n+      val d = -delta\n+      val idle = executorToPod.toVector.filter { case (id, _) => !scheduler.isExecutorBusy(id) }\n+      if (idle.length > 0) {\n+        logInfo(s\"Shutting down ${idle.length} idle executors\")\n+        shutdownExecutors(idle.take(d))\n+      }\n+      val r = math.max(0, d - idle.length)\n+      if (r > 0) {\n+        logInfo(s\"Shutting down $r non-idle executors\")\n+        shutdownExecutors(executorToPod.toVector.slice(n - r, n))\n+      }\n+    }\n+    // TODO: are there meaningful failure modes here?\n+    Future.successful(true)\n+  }\n+\n+  override def doKillExecutors(executorIds: Seq[String]): Future[Boolean] = {\n+    logInfo(s\"Received doKillExecutors\")\n+    killExecutorPods(executorIds.map { id => (id, executorToPod(id)) })\n+    Future.successful(true)\n+  }\n+\n+  private def createExecutorPods(n: Int) {\n+    for (i <- 1 to n) {\n+      executorID += 1\n+      executorToPod += ((executorID.toString, createExecutorPod(executorID)))\n+    }\n+  }\n+\n+  def shutdownExecutors(idPodPairs: Seq[(String, String)]) {\n+    val active = getExecutorIds.toSet\n+\n+    // Check for any finished shutting down and kill the pods\n+    val shutdown = shutdownToPod.toVector.filter { case (e, _) => !active.contains(e) }\n+    killExecutorPods(shutdown)\n+\n+    // Now request shutdown for the new ones.\n+    // Move them from executor list to list pending shutdown\n+    for ((id, pod) <- idPodPairs) {\n+      try {\n+        // TODO: 'ask' returns a future - can it be used to check eventual success?\n+        Option(driverEndpoint).foreach(_.ask[Boolean](RemoveExecutor(id, ExecutorKilled)))\n+        executorToPod -= id\n+        shutdownToPod += ((id, pod))\n+      } catch {\n+        case e: Exception => logError(s\"Error shutting down executor $id\", e)\n+      }\n+    }\n+  }\n+\n+  private def killExecutorPods(idPodPairs: Seq[(String, String)]) {\n+    for ((id, pod) <- idPodPairs) {\n+      try {\n+        client.pods().inNamespace(ns).withName(pod).delete()\n+        executorToPod -= id\n+        shutdownToPod -= id\n+      } catch {\n+        case e: Exception => logError(s\"Error killing executor pod $pod\", e)\n+      }\n+    }\n+  }\n+\n+  def getInitialTargetExecutorNumber(conf: SparkConf,\n+                                     numExecutors: Int =\n+                                     DEFAULT_NUMBER_EXECUTORS): Int = {\n+    if (dynamicExecutors) {\n+      val minNumExecutors = conf.get(DYN_ALLOCATION_MIN_EXECUTORS)\n+      val initialNumExecutors =\n+        Utils.getDynamicAllocationInitialExecutors(conf)\n+      val maxNumExecutors = conf.get(DYN_ALLOCATION_MAX_EXECUTORS)\n+      require(\n+        initialNumExecutors >= minNumExecutors && initialNumExecutors <= maxNumExecutors,\n+        s\"initial executor number $initialNumExecutors must between min executor number \" +\n+          s\"$minNumExecutors and max executor number $maxNumExecutors\")\n+\n+      initialNumExecutors\n+    } else {\n+      conf.get(EXECUTOR_INSTANCES).getOrElse(numExecutors)\n+    }\n+  }\n+\n+  def createExecutorPod(executorNum: Int): String = {\n+    // create a single k8s executor pod.\n+    val labelMap = Map(\"type\" -> \"spark-executor\")\n+    val podName = s\"$sparkExecutorName-$executorNum\"\n+\n+    val submitArgs = mutable.ArrayBuffer.empty[String]\n+\n+    if (conf.getBoolean(\"spark.dynamicAllocation.enabled\", false)) {\n+      submitArgs ++= Vector(\n+        \"dynamic-executors\")\n+    }\n+\n+    submitArgs ++= Vector(\"org.apache.spark.executor.CoarseGrainedExecutorBackend\",\n+      \"--driver-url\", s\"$driverURL\",\n+      \"--executor-id\", s\"$executorNum\",\n+      \"--hostname\", \"localhost\",\n+      \"--app-id\", \"1\", // TODO: change app-id per application and pass from driver.\n+      \"--cores\", \"1\")\n+\n+    var pod = new PodBuilder()\n+      .withNewMetadata()\n+      .withLabels(labelMap.asJava)\n+      .withName(podName)\n+      .endMetadata()\n+      .withNewSpec()\n+      .withRestartPolicy(\"OnFailure\")\n+\n+      .addNewContainer().withName(\"spark-executor\").withImage(sparkImage)\n+      .withImagePullPolicy(\"IfNotPresent\")\n+      .withCommand(\"/opt/executor.sh\")",
    "line": 197
  }, {
    "author": {
      "login": "erikerlandson"
    },
    "body": "IIUC, you don't even need to throw the `-Xmx` mem flags; the driver, executor, etc are supposed to honor those (and the `*_CORES`), aren't they?    We've been using `spark-class` to run the executor backends.   And I think just `spark-submit` (client mode) for the driver.   Although I think it all eventually bakes down to a jvm call.",
    "commit": "8584913d51c1f68c8f9d3cdf142947cd35cdfcc2",
    "createdAt": "2016-11-30T22:28:58Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.kubernetes\n+\n+import collection.JavaConverters._\n+import io.fabric8.kubernetes.api.model.PodBuilder\n+import io.fabric8.kubernetes.api.model.extensions.JobBuilder\n+import io.fabric8.kubernetes.client.{ConfigBuilder, DefaultKubernetesClient}\n+import org.apache.spark.internal.config._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster._\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages._\n+import org.apache.spark.{SparkConf, SparkContext, SparkException}\n+import org.apache.spark.rpc.RpcEndpointAddress\n+import org.apache.spark.scheduler.TaskSchedulerImpl\n+import org.apache.spark.util.Utils\n+\n+import scala.collection.mutable\n+import scala.util.Random\n+import scala.concurrent.Future\n+\n+private[spark] class KubernetesClusterSchedulerBackend(\n+                                                  scheduler: TaskSchedulerImpl,\n+                                                  sc: SparkContext)\n+  extends CoarseGrainedSchedulerBackend(scheduler, sc.env.rpcEnv) {\n+\n+  val client = new DefaultKubernetesClient()\n+\n+  val DEFAULT_NUMBER_EXECUTORS = 2\n+  val sparkExecutorName = s\"spark-executor-${Random.alphanumeric take 5 mkString(\"\")}\".toLowerCase()\n+\n+  // TODO: do these need mutex guarding?\n+  // key is executor id, value is pod name\n+  var executorToPod = mutable.Map.empty[String, String] // active executors\n+  var shutdownToPod = mutable.Map.empty[String, String] // pending shutdown\n+  var executorID = 0\n+\n+  val sparkImage = conf.get(\"spark.kubernetes.sparkImage\")\n+  val clientJarUri = conf.get(\"spark.executor.jar\")\n+  val ns = conf.get(\n+    \"spark.kubernetes.namespace\",\n+    KubernetesClusterScheduler.defaultNameSpace)\n+  val dynamicExecutors = Utils.isDynamicAllocationEnabled(conf)\n+\n+  // executor back-ends take their configuration this way\n+  if (dynamicExecutors) {\n+    conf.setExecutorEnv(\"spark.dynamicAllocation.enabled\", \"true\")\n+    conf.setExecutorEnv(\"spark.shuffle.service.enabled\", \"true\")\n+  }\n+\n+  override def start(): Unit = {\n+    super.start()\n+    createExecutorPods(getInitialTargetExecutorNumber(sc.getConf))\n+  }\n+\n+  override def stop(): Unit = {\n+    // Kill all executor pods indiscriminately\n+    killExecutorPods(executorToPod.toVector)\n+    killExecutorPods(shutdownToPod.toVector)\n+    super.stop()\n+  }\n+\n+  // Dynamic allocation interfaces\n+  override def doRequestTotalExecutors(requestedTotal: Int): Future[Boolean] = {\n+    logInfo(s\"Received doRequestTotalExecutors: $requestedTotal\")\n+    val n = executorToPod.size\n+    val delta = requestedTotal - n\n+    if (delta > 0) {\n+      logInfo(s\"Adding $delta new executors\")\n+      createExecutorPods(delta)\n+    } else if (delta < 0) {\n+      val d = -delta\n+      val idle = executorToPod.toVector.filter { case (id, _) => !scheduler.isExecutorBusy(id) }\n+      if (idle.length > 0) {\n+        logInfo(s\"Shutting down ${idle.length} idle executors\")\n+        shutdownExecutors(idle.take(d))\n+      }\n+      val r = math.max(0, d - idle.length)\n+      if (r > 0) {\n+        logInfo(s\"Shutting down $r non-idle executors\")\n+        shutdownExecutors(executorToPod.toVector.slice(n - r, n))\n+      }\n+    }\n+    // TODO: are there meaningful failure modes here?\n+    Future.successful(true)\n+  }\n+\n+  override def doKillExecutors(executorIds: Seq[String]): Future[Boolean] = {\n+    logInfo(s\"Received doKillExecutors\")\n+    killExecutorPods(executorIds.map { id => (id, executorToPod(id)) })\n+    Future.successful(true)\n+  }\n+\n+  private def createExecutorPods(n: Int) {\n+    for (i <- 1 to n) {\n+      executorID += 1\n+      executorToPod += ((executorID.toString, createExecutorPod(executorID)))\n+    }\n+  }\n+\n+  def shutdownExecutors(idPodPairs: Seq[(String, String)]) {\n+    val active = getExecutorIds.toSet\n+\n+    // Check for any finished shutting down and kill the pods\n+    val shutdown = shutdownToPod.toVector.filter { case (e, _) => !active.contains(e) }\n+    killExecutorPods(shutdown)\n+\n+    // Now request shutdown for the new ones.\n+    // Move them from executor list to list pending shutdown\n+    for ((id, pod) <- idPodPairs) {\n+      try {\n+        // TODO: 'ask' returns a future - can it be used to check eventual success?\n+        Option(driverEndpoint).foreach(_.ask[Boolean](RemoveExecutor(id, ExecutorKilled)))\n+        executorToPod -= id\n+        shutdownToPod += ((id, pod))\n+      } catch {\n+        case e: Exception => logError(s\"Error shutting down executor $id\", e)\n+      }\n+    }\n+  }\n+\n+  private def killExecutorPods(idPodPairs: Seq[(String, String)]) {\n+    for ((id, pod) <- idPodPairs) {\n+      try {\n+        client.pods().inNamespace(ns).withName(pod).delete()\n+        executorToPod -= id\n+        shutdownToPod -= id\n+      } catch {\n+        case e: Exception => logError(s\"Error killing executor pod $pod\", e)\n+      }\n+    }\n+  }\n+\n+  def getInitialTargetExecutorNumber(conf: SparkConf,\n+                                     numExecutors: Int =\n+                                     DEFAULT_NUMBER_EXECUTORS): Int = {\n+    if (dynamicExecutors) {\n+      val minNumExecutors = conf.get(DYN_ALLOCATION_MIN_EXECUTORS)\n+      val initialNumExecutors =\n+        Utils.getDynamicAllocationInitialExecutors(conf)\n+      val maxNumExecutors = conf.get(DYN_ALLOCATION_MAX_EXECUTORS)\n+      require(\n+        initialNumExecutors >= minNumExecutors && initialNumExecutors <= maxNumExecutors,\n+        s\"initial executor number $initialNumExecutors must between min executor number \" +\n+          s\"$minNumExecutors and max executor number $maxNumExecutors\")\n+\n+      initialNumExecutors\n+    } else {\n+      conf.get(EXECUTOR_INSTANCES).getOrElse(numExecutors)\n+    }\n+  }\n+\n+  def createExecutorPod(executorNum: Int): String = {\n+    // create a single k8s executor pod.\n+    val labelMap = Map(\"type\" -> \"spark-executor\")\n+    val podName = s\"$sparkExecutorName-$executorNum\"\n+\n+    val submitArgs = mutable.ArrayBuffer.empty[String]\n+\n+    if (conf.getBoolean(\"spark.dynamicAllocation.enabled\", false)) {\n+      submitArgs ++= Vector(\n+        \"dynamic-executors\")\n+    }\n+\n+    submitArgs ++= Vector(\"org.apache.spark.executor.CoarseGrainedExecutorBackend\",\n+      \"--driver-url\", s\"$driverURL\",\n+      \"--executor-id\", s\"$executorNum\",\n+      \"--hostname\", \"localhost\",\n+      \"--app-id\", \"1\", // TODO: change app-id per application and pass from driver.\n+      \"--cores\", \"1\")\n+\n+    var pod = new PodBuilder()\n+      .withNewMetadata()\n+      .withLabels(labelMap.asJava)\n+      .withName(podName)\n+      .endMetadata()\n+      .withNewSpec()\n+      .withRestartPolicy(\"OnFailure\")\n+\n+      .addNewContainer().withName(\"spark-executor\").withImage(sparkImage)\n+      .withImagePullPolicy(\"IfNotPresent\")\n+      .withCommand(\"/opt/executor.sh\")",
    "line": 197
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "`spark-class` wasn't built to handle starting executors so it won't pick up memory flags for them. We have to execute the Java binary directly. `spark-class` I believe was built for running long-lived daemons like the history server and standalone components.\r\n\r\nAs for the driver - we can run `spark-submit` for now but when we move to supporting uploading files from the submitter's local disk we're going to need an in-between process in the pod to upload the local resources to it.",
    "commit": "8584913d51c1f68c8f9d3cdf142947cd35cdfcc2",
    "createdAt": "2016-11-30T22:33:32Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.kubernetes\n+\n+import collection.JavaConverters._\n+import io.fabric8.kubernetes.api.model.PodBuilder\n+import io.fabric8.kubernetes.api.model.extensions.JobBuilder\n+import io.fabric8.kubernetes.client.{ConfigBuilder, DefaultKubernetesClient}\n+import org.apache.spark.internal.config._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster._\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages._\n+import org.apache.spark.{SparkConf, SparkContext, SparkException}\n+import org.apache.spark.rpc.RpcEndpointAddress\n+import org.apache.spark.scheduler.TaskSchedulerImpl\n+import org.apache.spark.util.Utils\n+\n+import scala.collection.mutable\n+import scala.util.Random\n+import scala.concurrent.Future\n+\n+private[spark] class KubernetesClusterSchedulerBackend(\n+                                                  scheduler: TaskSchedulerImpl,\n+                                                  sc: SparkContext)\n+  extends CoarseGrainedSchedulerBackend(scheduler, sc.env.rpcEnv) {\n+\n+  val client = new DefaultKubernetesClient()\n+\n+  val DEFAULT_NUMBER_EXECUTORS = 2\n+  val sparkExecutorName = s\"spark-executor-${Random.alphanumeric take 5 mkString(\"\")}\".toLowerCase()\n+\n+  // TODO: do these need mutex guarding?\n+  // key is executor id, value is pod name\n+  var executorToPod = mutable.Map.empty[String, String] // active executors\n+  var shutdownToPod = mutable.Map.empty[String, String] // pending shutdown\n+  var executorID = 0\n+\n+  val sparkImage = conf.get(\"spark.kubernetes.sparkImage\")\n+  val clientJarUri = conf.get(\"spark.executor.jar\")\n+  val ns = conf.get(\n+    \"spark.kubernetes.namespace\",\n+    KubernetesClusterScheduler.defaultNameSpace)\n+  val dynamicExecutors = Utils.isDynamicAllocationEnabled(conf)\n+\n+  // executor back-ends take their configuration this way\n+  if (dynamicExecutors) {\n+    conf.setExecutorEnv(\"spark.dynamicAllocation.enabled\", \"true\")\n+    conf.setExecutorEnv(\"spark.shuffle.service.enabled\", \"true\")\n+  }\n+\n+  override def start(): Unit = {\n+    super.start()\n+    createExecutorPods(getInitialTargetExecutorNumber(sc.getConf))\n+  }\n+\n+  override def stop(): Unit = {\n+    // Kill all executor pods indiscriminately\n+    killExecutorPods(executorToPod.toVector)\n+    killExecutorPods(shutdownToPod.toVector)\n+    super.stop()\n+  }\n+\n+  // Dynamic allocation interfaces\n+  override def doRequestTotalExecutors(requestedTotal: Int): Future[Boolean] = {\n+    logInfo(s\"Received doRequestTotalExecutors: $requestedTotal\")\n+    val n = executorToPod.size\n+    val delta = requestedTotal - n\n+    if (delta > 0) {\n+      logInfo(s\"Adding $delta new executors\")\n+      createExecutorPods(delta)\n+    } else if (delta < 0) {\n+      val d = -delta\n+      val idle = executorToPod.toVector.filter { case (id, _) => !scheduler.isExecutorBusy(id) }\n+      if (idle.length > 0) {\n+        logInfo(s\"Shutting down ${idle.length} idle executors\")\n+        shutdownExecutors(idle.take(d))\n+      }\n+      val r = math.max(0, d - idle.length)\n+      if (r > 0) {\n+        logInfo(s\"Shutting down $r non-idle executors\")\n+        shutdownExecutors(executorToPod.toVector.slice(n - r, n))\n+      }\n+    }\n+    // TODO: are there meaningful failure modes here?\n+    Future.successful(true)\n+  }\n+\n+  override def doKillExecutors(executorIds: Seq[String]): Future[Boolean] = {\n+    logInfo(s\"Received doKillExecutors\")\n+    killExecutorPods(executorIds.map { id => (id, executorToPod(id)) })\n+    Future.successful(true)\n+  }\n+\n+  private def createExecutorPods(n: Int) {\n+    for (i <- 1 to n) {\n+      executorID += 1\n+      executorToPod += ((executorID.toString, createExecutorPod(executorID)))\n+    }\n+  }\n+\n+  def shutdownExecutors(idPodPairs: Seq[(String, String)]) {\n+    val active = getExecutorIds.toSet\n+\n+    // Check for any finished shutting down and kill the pods\n+    val shutdown = shutdownToPod.toVector.filter { case (e, _) => !active.contains(e) }\n+    killExecutorPods(shutdown)\n+\n+    // Now request shutdown for the new ones.\n+    // Move them from executor list to list pending shutdown\n+    for ((id, pod) <- idPodPairs) {\n+      try {\n+        // TODO: 'ask' returns a future - can it be used to check eventual success?\n+        Option(driverEndpoint).foreach(_.ask[Boolean](RemoveExecutor(id, ExecutorKilled)))\n+        executorToPod -= id\n+        shutdownToPod += ((id, pod))\n+      } catch {\n+        case e: Exception => logError(s\"Error shutting down executor $id\", e)\n+      }\n+    }\n+  }\n+\n+  private def killExecutorPods(idPodPairs: Seq[(String, String)]) {\n+    for ((id, pod) <- idPodPairs) {\n+      try {\n+        client.pods().inNamespace(ns).withName(pod).delete()\n+        executorToPod -= id\n+        shutdownToPod -= id\n+      } catch {\n+        case e: Exception => logError(s\"Error killing executor pod $pod\", e)\n+      }\n+    }\n+  }\n+\n+  def getInitialTargetExecutorNumber(conf: SparkConf,\n+                                     numExecutors: Int =\n+                                     DEFAULT_NUMBER_EXECUTORS): Int = {\n+    if (dynamicExecutors) {\n+      val minNumExecutors = conf.get(DYN_ALLOCATION_MIN_EXECUTORS)\n+      val initialNumExecutors =\n+        Utils.getDynamicAllocationInitialExecutors(conf)\n+      val maxNumExecutors = conf.get(DYN_ALLOCATION_MAX_EXECUTORS)\n+      require(\n+        initialNumExecutors >= minNumExecutors && initialNumExecutors <= maxNumExecutors,\n+        s\"initial executor number $initialNumExecutors must between min executor number \" +\n+          s\"$minNumExecutors and max executor number $maxNumExecutors\")\n+\n+      initialNumExecutors\n+    } else {\n+      conf.get(EXECUTOR_INSTANCES).getOrElse(numExecutors)\n+    }\n+  }\n+\n+  def createExecutorPod(executorNum: Int): String = {\n+    // create a single k8s executor pod.\n+    val labelMap = Map(\"type\" -> \"spark-executor\")\n+    val podName = s\"$sparkExecutorName-$executorNum\"\n+\n+    val submitArgs = mutable.ArrayBuffer.empty[String]\n+\n+    if (conf.getBoolean(\"spark.dynamicAllocation.enabled\", false)) {\n+      submitArgs ++= Vector(\n+        \"dynamic-executors\")\n+    }\n+\n+    submitArgs ++= Vector(\"org.apache.spark.executor.CoarseGrainedExecutorBackend\",\n+      \"--driver-url\", s\"$driverURL\",\n+      \"--executor-id\", s\"$executorNum\",\n+      \"--hostname\", \"localhost\",\n+      \"--app-id\", \"1\", // TODO: change app-id per application and pass from driver.\n+      \"--cores\", \"1\")\n+\n+    var pod = new PodBuilder()\n+      .withNewMetadata()\n+      .withLabels(labelMap.asJava)\n+      .withName(podName)\n+      .endMetadata()\n+      .withNewSpec()\n+      .withRestartPolicy(\"OnFailure\")\n+\n+      .addNewContainer().withName(\"spark-executor\").withImage(sparkImage)\n+      .withImagePullPolicy(\"IfNotPresent\")\n+      .withCommand(\"/opt/executor.sh\")",
    "line": 197
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "Actually I think I was mistaken here - `spark-class` eventually calls to [this class](https://github.com/apache/spark/blob/master/launcher/src/main/java/org/apache/spark/launcher/SparkClassCommandBuilder.java#L65) which then forks the actual process running the passed-in class. We can probably then indeed take advantage of `SPARK_JAVA_OPTS` and `SPARK_EXECUTOR_MEMORY`, etc. Arguments like executor-id which are passed as command line arguments and not JVM options should probably still be set as environment variables on the container though and the Dockerfile handles formatting the command line arguments sent to `spark-class`.",
    "commit": "8584913d51c1f68c8f9d3cdf142947cd35cdfcc2",
    "createdAt": "2016-12-01T01:16:54Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.kubernetes\n+\n+import collection.JavaConverters._\n+import io.fabric8.kubernetes.api.model.PodBuilder\n+import io.fabric8.kubernetes.api.model.extensions.JobBuilder\n+import io.fabric8.kubernetes.client.{ConfigBuilder, DefaultKubernetesClient}\n+import org.apache.spark.internal.config._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster._\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages._\n+import org.apache.spark.{SparkConf, SparkContext, SparkException}\n+import org.apache.spark.rpc.RpcEndpointAddress\n+import org.apache.spark.scheduler.TaskSchedulerImpl\n+import org.apache.spark.util.Utils\n+\n+import scala.collection.mutable\n+import scala.util.Random\n+import scala.concurrent.Future\n+\n+private[spark] class KubernetesClusterSchedulerBackend(\n+                                                  scheduler: TaskSchedulerImpl,\n+                                                  sc: SparkContext)\n+  extends CoarseGrainedSchedulerBackend(scheduler, sc.env.rpcEnv) {\n+\n+  val client = new DefaultKubernetesClient()\n+\n+  val DEFAULT_NUMBER_EXECUTORS = 2\n+  val sparkExecutorName = s\"spark-executor-${Random.alphanumeric take 5 mkString(\"\")}\".toLowerCase()\n+\n+  // TODO: do these need mutex guarding?\n+  // key is executor id, value is pod name\n+  var executorToPod = mutable.Map.empty[String, String] // active executors\n+  var shutdownToPod = mutable.Map.empty[String, String] // pending shutdown\n+  var executorID = 0\n+\n+  val sparkImage = conf.get(\"spark.kubernetes.sparkImage\")\n+  val clientJarUri = conf.get(\"spark.executor.jar\")\n+  val ns = conf.get(\n+    \"spark.kubernetes.namespace\",\n+    KubernetesClusterScheduler.defaultNameSpace)\n+  val dynamicExecutors = Utils.isDynamicAllocationEnabled(conf)\n+\n+  // executor back-ends take their configuration this way\n+  if (dynamicExecutors) {\n+    conf.setExecutorEnv(\"spark.dynamicAllocation.enabled\", \"true\")\n+    conf.setExecutorEnv(\"spark.shuffle.service.enabled\", \"true\")\n+  }\n+\n+  override def start(): Unit = {\n+    super.start()\n+    createExecutorPods(getInitialTargetExecutorNumber(sc.getConf))\n+  }\n+\n+  override def stop(): Unit = {\n+    // Kill all executor pods indiscriminately\n+    killExecutorPods(executorToPod.toVector)\n+    killExecutorPods(shutdownToPod.toVector)\n+    super.stop()\n+  }\n+\n+  // Dynamic allocation interfaces\n+  override def doRequestTotalExecutors(requestedTotal: Int): Future[Boolean] = {\n+    logInfo(s\"Received doRequestTotalExecutors: $requestedTotal\")\n+    val n = executorToPod.size\n+    val delta = requestedTotal - n\n+    if (delta > 0) {\n+      logInfo(s\"Adding $delta new executors\")\n+      createExecutorPods(delta)\n+    } else if (delta < 0) {\n+      val d = -delta\n+      val idle = executorToPod.toVector.filter { case (id, _) => !scheduler.isExecutorBusy(id) }\n+      if (idle.length > 0) {\n+        logInfo(s\"Shutting down ${idle.length} idle executors\")\n+        shutdownExecutors(idle.take(d))\n+      }\n+      val r = math.max(0, d - idle.length)\n+      if (r > 0) {\n+        logInfo(s\"Shutting down $r non-idle executors\")\n+        shutdownExecutors(executorToPod.toVector.slice(n - r, n))\n+      }\n+    }\n+    // TODO: are there meaningful failure modes here?\n+    Future.successful(true)\n+  }\n+\n+  override def doKillExecutors(executorIds: Seq[String]): Future[Boolean] = {\n+    logInfo(s\"Received doKillExecutors\")\n+    killExecutorPods(executorIds.map { id => (id, executorToPod(id)) })\n+    Future.successful(true)\n+  }\n+\n+  private def createExecutorPods(n: Int) {\n+    for (i <- 1 to n) {\n+      executorID += 1\n+      executorToPod += ((executorID.toString, createExecutorPod(executorID)))\n+    }\n+  }\n+\n+  def shutdownExecutors(idPodPairs: Seq[(String, String)]) {\n+    val active = getExecutorIds.toSet\n+\n+    // Check for any finished shutting down and kill the pods\n+    val shutdown = shutdownToPod.toVector.filter { case (e, _) => !active.contains(e) }\n+    killExecutorPods(shutdown)\n+\n+    // Now request shutdown for the new ones.\n+    // Move them from executor list to list pending shutdown\n+    for ((id, pod) <- idPodPairs) {\n+      try {\n+        // TODO: 'ask' returns a future - can it be used to check eventual success?\n+        Option(driverEndpoint).foreach(_.ask[Boolean](RemoveExecutor(id, ExecutorKilled)))\n+        executorToPod -= id\n+        shutdownToPod += ((id, pod))\n+      } catch {\n+        case e: Exception => logError(s\"Error shutting down executor $id\", e)\n+      }\n+    }\n+  }\n+\n+  private def killExecutorPods(idPodPairs: Seq[(String, String)]) {\n+    for ((id, pod) <- idPodPairs) {\n+      try {\n+        client.pods().inNamespace(ns).withName(pod).delete()\n+        executorToPod -= id\n+        shutdownToPod -= id\n+      } catch {\n+        case e: Exception => logError(s\"Error killing executor pod $pod\", e)\n+      }\n+    }\n+  }\n+\n+  def getInitialTargetExecutorNumber(conf: SparkConf,\n+                                     numExecutors: Int =\n+                                     DEFAULT_NUMBER_EXECUTORS): Int = {\n+    if (dynamicExecutors) {\n+      val minNumExecutors = conf.get(DYN_ALLOCATION_MIN_EXECUTORS)\n+      val initialNumExecutors =\n+        Utils.getDynamicAllocationInitialExecutors(conf)\n+      val maxNumExecutors = conf.get(DYN_ALLOCATION_MAX_EXECUTORS)\n+      require(\n+        initialNumExecutors >= minNumExecutors && initialNumExecutors <= maxNumExecutors,\n+        s\"initial executor number $initialNumExecutors must between min executor number \" +\n+          s\"$minNumExecutors and max executor number $maxNumExecutors\")\n+\n+      initialNumExecutors\n+    } else {\n+      conf.get(EXECUTOR_INSTANCES).getOrElse(numExecutors)\n+    }\n+  }\n+\n+  def createExecutorPod(executorNum: Int): String = {\n+    // create a single k8s executor pod.\n+    val labelMap = Map(\"type\" -> \"spark-executor\")\n+    val podName = s\"$sparkExecutorName-$executorNum\"\n+\n+    val submitArgs = mutable.ArrayBuffer.empty[String]\n+\n+    if (conf.getBoolean(\"spark.dynamicAllocation.enabled\", false)) {\n+      submitArgs ++= Vector(\n+        \"dynamic-executors\")\n+    }\n+\n+    submitArgs ++= Vector(\"org.apache.spark.executor.CoarseGrainedExecutorBackend\",\n+      \"--driver-url\", s\"$driverURL\",\n+      \"--executor-id\", s\"$executorNum\",\n+      \"--hostname\", \"localhost\",\n+      \"--app-id\", \"1\", // TODO: change app-id per application and pass from driver.\n+      \"--cores\", \"1\")\n+\n+    var pod = new PodBuilder()\n+      .withNewMetadata()\n+      .withLabels(labelMap.asJava)\n+      .withName(podName)\n+      .endMetadata()\n+      .withNewSpec()\n+      .withRestartPolicy(\"OnFailure\")\n+\n+      .addNewContainer().withName(\"spark-executor\").withImage(sparkImage)\n+      .withImagePullPolicy(\"IfNotPresent\")\n+      .withCommand(\"/opt/executor.sh\")",
    "line": 197
  }],
  "prId": 16061
}, {
  "comments": [{
    "author": {
      "login": "mccheah"
    },
    "body": "Should we use the HOSTNAME environment variable here?",
    "commit": "8584913d51c1f68c8f9d3cdf142947cd35cdfcc2",
    "createdAt": "2016-11-30T20:26:59Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.kubernetes\n+\n+import collection.JavaConverters._\n+import io.fabric8.kubernetes.api.model.PodBuilder\n+import io.fabric8.kubernetes.api.model.extensions.JobBuilder\n+import io.fabric8.kubernetes.client.{ConfigBuilder, DefaultKubernetesClient}\n+import org.apache.spark.internal.config._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster._\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages._\n+import org.apache.spark.{SparkConf, SparkContext, SparkException}\n+import org.apache.spark.rpc.RpcEndpointAddress\n+import org.apache.spark.scheduler.TaskSchedulerImpl\n+import org.apache.spark.util.Utils\n+\n+import scala.collection.mutable\n+import scala.util.Random\n+import scala.concurrent.Future\n+\n+private[spark] class KubernetesClusterSchedulerBackend(\n+                                                  scheduler: TaskSchedulerImpl,\n+                                                  sc: SparkContext)\n+  extends CoarseGrainedSchedulerBackend(scheduler, sc.env.rpcEnv) {\n+\n+  val client = new DefaultKubernetesClient()\n+\n+  val DEFAULT_NUMBER_EXECUTORS = 2\n+  val sparkExecutorName = s\"spark-executor-${Random.alphanumeric take 5 mkString(\"\")}\".toLowerCase()\n+\n+  // TODO: do these need mutex guarding?\n+  // key is executor id, value is pod name\n+  var executorToPod = mutable.Map.empty[String, String] // active executors\n+  var shutdownToPod = mutable.Map.empty[String, String] // pending shutdown\n+  var executorID = 0\n+\n+  val sparkImage = conf.get(\"spark.kubernetes.sparkImage\")\n+  val clientJarUri = conf.get(\"spark.executor.jar\")\n+  val ns = conf.get(\n+    \"spark.kubernetes.namespace\",\n+    KubernetesClusterScheduler.defaultNameSpace)\n+  val dynamicExecutors = Utils.isDynamicAllocationEnabled(conf)\n+\n+  // executor back-ends take their configuration this way\n+  if (dynamicExecutors) {\n+    conf.setExecutorEnv(\"spark.dynamicAllocation.enabled\", \"true\")\n+    conf.setExecutorEnv(\"spark.shuffle.service.enabled\", \"true\")\n+  }\n+\n+  override def start(): Unit = {\n+    super.start()\n+    createExecutorPods(getInitialTargetExecutorNumber(sc.getConf))\n+  }\n+\n+  override def stop(): Unit = {\n+    // Kill all executor pods indiscriminately\n+    killExecutorPods(executorToPod.toVector)\n+    killExecutorPods(shutdownToPod.toVector)\n+    super.stop()\n+  }\n+\n+  // Dynamic allocation interfaces\n+  override def doRequestTotalExecutors(requestedTotal: Int): Future[Boolean] = {\n+    logInfo(s\"Received doRequestTotalExecutors: $requestedTotal\")\n+    val n = executorToPod.size\n+    val delta = requestedTotal - n\n+    if (delta > 0) {\n+      logInfo(s\"Adding $delta new executors\")\n+      createExecutorPods(delta)\n+    } else if (delta < 0) {\n+      val d = -delta\n+      val idle = executorToPod.toVector.filter { case (id, _) => !scheduler.isExecutorBusy(id) }\n+      if (idle.length > 0) {\n+        logInfo(s\"Shutting down ${idle.length} idle executors\")\n+        shutdownExecutors(idle.take(d))\n+      }\n+      val r = math.max(0, d - idle.length)\n+      if (r > 0) {\n+        logInfo(s\"Shutting down $r non-idle executors\")\n+        shutdownExecutors(executorToPod.toVector.slice(n - r, n))\n+      }\n+    }\n+    // TODO: are there meaningful failure modes here?\n+    Future.successful(true)\n+  }\n+\n+  override def doKillExecutors(executorIds: Seq[String]): Future[Boolean] = {\n+    logInfo(s\"Received doKillExecutors\")\n+    killExecutorPods(executorIds.map { id => (id, executorToPod(id)) })\n+    Future.successful(true)\n+  }\n+\n+  private def createExecutorPods(n: Int) {\n+    for (i <- 1 to n) {\n+      executorID += 1\n+      executorToPod += ((executorID.toString, createExecutorPod(executorID)))\n+    }\n+  }\n+\n+  def shutdownExecutors(idPodPairs: Seq[(String, String)]) {\n+    val active = getExecutorIds.toSet\n+\n+    // Check for any finished shutting down and kill the pods\n+    val shutdown = shutdownToPod.toVector.filter { case (e, _) => !active.contains(e) }\n+    killExecutorPods(shutdown)\n+\n+    // Now request shutdown for the new ones.\n+    // Move them from executor list to list pending shutdown\n+    for ((id, pod) <- idPodPairs) {\n+      try {\n+        // TODO: 'ask' returns a future - can it be used to check eventual success?\n+        Option(driverEndpoint).foreach(_.ask[Boolean](RemoveExecutor(id, ExecutorKilled)))\n+        executorToPod -= id\n+        shutdownToPod += ((id, pod))\n+      } catch {\n+        case e: Exception => logError(s\"Error shutting down executor $id\", e)\n+      }\n+    }\n+  }\n+\n+  private def killExecutorPods(idPodPairs: Seq[(String, String)]) {\n+    for ((id, pod) <- idPodPairs) {\n+      try {\n+        client.pods().inNamespace(ns).withName(pod).delete()\n+        executorToPod -= id\n+        shutdownToPod -= id\n+      } catch {\n+        case e: Exception => logError(s\"Error killing executor pod $pod\", e)\n+      }\n+    }\n+  }\n+\n+  def getInitialTargetExecutorNumber(conf: SparkConf,\n+                                     numExecutors: Int =\n+                                     DEFAULT_NUMBER_EXECUTORS): Int = {\n+    if (dynamicExecutors) {\n+      val minNumExecutors = conf.get(DYN_ALLOCATION_MIN_EXECUTORS)\n+      val initialNumExecutors =\n+        Utils.getDynamicAllocationInitialExecutors(conf)\n+      val maxNumExecutors = conf.get(DYN_ALLOCATION_MAX_EXECUTORS)\n+      require(\n+        initialNumExecutors >= minNumExecutors && initialNumExecutors <= maxNumExecutors,\n+        s\"initial executor number $initialNumExecutors must between min executor number \" +\n+          s\"$minNumExecutors and max executor number $maxNumExecutors\")\n+\n+      initialNumExecutors\n+    } else {\n+      conf.get(EXECUTOR_INSTANCES).getOrElse(numExecutors)\n+    }\n+  }\n+\n+  def createExecutorPod(executorNum: Int): String = {\n+    // create a single k8s executor pod.\n+    val labelMap = Map(\"type\" -> \"spark-executor\")\n+    val podName = s\"$sparkExecutorName-$executorNum\"\n+\n+    val submitArgs = mutable.ArrayBuffer.empty[String]\n+\n+    if (conf.getBoolean(\"spark.dynamicAllocation.enabled\", false)) {\n+      submitArgs ++= Vector(\n+        \"dynamic-executors\")\n+    }\n+\n+    submitArgs ++= Vector(\"org.apache.spark.executor.CoarseGrainedExecutorBackend\",\n+      \"--driver-url\", s\"$driverURL\",\n+      \"--executor-id\", s\"$executorNum\",\n+      \"--hostname\", \"localhost\",",
    "line": 183
  }, {
    "author": {
      "login": "erikerlandson"
    },
    "body": "I'm not sure how that plays out in a pod context.   \"localhost\" has been working, but its worth testing alternatives.",
    "commit": "8584913d51c1f68c8f9d3cdf142947cd35cdfcc2",
    "createdAt": "2016-12-02T16:54:40Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.kubernetes\n+\n+import collection.JavaConverters._\n+import io.fabric8.kubernetes.api.model.PodBuilder\n+import io.fabric8.kubernetes.api.model.extensions.JobBuilder\n+import io.fabric8.kubernetes.client.{ConfigBuilder, DefaultKubernetesClient}\n+import org.apache.spark.internal.config._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster._\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages._\n+import org.apache.spark.{SparkConf, SparkContext, SparkException}\n+import org.apache.spark.rpc.RpcEndpointAddress\n+import org.apache.spark.scheduler.TaskSchedulerImpl\n+import org.apache.spark.util.Utils\n+\n+import scala.collection.mutable\n+import scala.util.Random\n+import scala.concurrent.Future\n+\n+private[spark] class KubernetesClusterSchedulerBackend(\n+                                                  scheduler: TaskSchedulerImpl,\n+                                                  sc: SparkContext)\n+  extends CoarseGrainedSchedulerBackend(scheduler, sc.env.rpcEnv) {\n+\n+  val client = new DefaultKubernetesClient()\n+\n+  val DEFAULT_NUMBER_EXECUTORS = 2\n+  val sparkExecutorName = s\"spark-executor-${Random.alphanumeric take 5 mkString(\"\")}\".toLowerCase()\n+\n+  // TODO: do these need mutex guarding?\n+  // key is executor id, value is pod name\n+  var executorToPod = mutable.Map.empty[String, String] // active executors\n+  var shutdownToPod = mutable.Map.empty[String, String] // pending shutdown\n+  var executorID = 0\n+\n+  val sparkImage = conf.get(\"spark.kubernetes.sparkImage\")\n+  val clientJarUri = conf.get(\"spark.executor.jar\")\n+  val ns = conf.get(\n+    \"spark.kubernetes.namespace\",\n+    KubernetesClusterScheduler.defaultNameSpace)\n+  val dynamicExecutors = Utils.isDynamicAllocationEnabled(conf)\n+\n+  // executor back-ends take their configuration this way\n+  if (dynamicExecutors) {\n+    conf.setExecutorEnv(\"spark.dynamicAllocation.enabled\", \"true\")\n+    conf.setExecutorEnv(\"spark.shuffle.service.enabled\", \"true\")\n+  }\n+\n+  override def start(): Unit = {\n+    super.start()\n+    createExecutorPods(getInitialTargetExecutorNumber(sc.getConf))\n+  }\n+\n+  override def stop(): Unit = {\n+    // Kill all executor pods indiscriminately\n+    killExecutorPods(executorToPod.toVector)\n+    killExecutorPods(shutdownToPod.toVector)\n+    super.stop()\n+  }\n+\n+  // Dynamic allocation interfaces\n+  override def doRequestTotalExecutors(requestedTotal: Int): Future[Boolean] = {\n+    logInfo(s\"Received doRequestTotalExecutors: $requestedTotal\")\n+    val n = executorToPod.size\n+    val delta = requestedTotal - n\n+    if (delta > 0) {\n+      logInfo(s\"Adding $delta new executors\")\n+      createExecutorPods(delta)\n+    } else if (delta < 0) {\n+      val d = -delta\n+      val idle = executorToPod.toVector.filter { case (id, _) => !scheduler.isExecutorBusy(id) }\n+      if (idle.length > 0) {\n+        logInfo(s\"Shutting down ${idle.length} idle executors\")\n+        shutdownExecutors(idle.take(d))\n+      }\n+      val r = math.max(0, d - idle.length)\n+      if (r > 0) {\n+        logInfo(s\"Shutting down $r non-idle executors\")\n+        shutdownExecutors(executorToPod.toVector.slice(n - r, n))\n+      }\n+    }\n+    // TODO: are there meaningful failure modes here?\n+    Future.successful(true)\n+  }\n+\n+  override def doKillExecutors(executorIds: Seq[String]): Future[Boolean] = {\n+    logInfo(s\"Received doKillExecutors\")\n+    killExecutorPods(executorIds.map { id => (id, executorToPod(id)) })\n+    Future.successful(true)\n+  }\n+\n+  private def createExecutorPods(n: Int) {\n+    for (i <- 1 to n) {\n+      executorID += 1\n+      executorToPod += ((executorID.toString, createExecutorPod(executorID)))\n+    }\n+  }\n+\n+  def shutdownExecutors(idPodPairs: Seq[(String, String)]) {\n+    val active = getExecutorIds.toSet\n+\n+    // Check for any finished shutting down and kill the pods\n+    val shutdown = shutdownToPod.toVector.filter { case (e, _) => !active.contains(e) }\n+    killExecutorPods(shutdown)\n+\n+    // Now request shutdown for the new ones.\n+    // Move them from executor list to list pending shutdown\n+    for ((id, pod) <- idPodPairs) {\n+      try {\n+        // TODO: 'ask' returns a future - can it be used to check eventual success?\n+        Option(driverEndpoint).foreach(_.ask[Boolean](RemoveExecutor(id, ExecutorKilled)))\n+        executorToPod -= id\n+        shutdownToPod += ((id, pod))\n+      } catch {\n+        case e: Exception => logError(s\"Error shutting down executor $id\", e)\n+      }\n+    }\n+  }\n+\n+  private def killExecutorPods(idPodPairs: Seq[(String, String)]) {\n+    for ((id, pod) <- idPodPairs) {\n+      try {\n+        client.pods().inNamespace(ns).withName(pod).delete()\n+        executorToPod -= id\n+        shutdownToPod -= id\n+      } catch {\n+        case e: Exception => logError(s\"Error killing executor pod $pod\", e)\n+      }\n+    }\n+  }\n+\n+  def getInitialTargetExecutorNumber(conf: SparkConf,\n+                                     numExecutors: Int =\n+                                     DEFAULT_NUMBER_EXECUTORS): Int = {\n+    if (dynamicExecutors) {\n+      val minNumExecutors = conf.get(DYN_ALLOCATION_MIN_EXECUTORS)\n+      val initialNumExecutors =\n+        Utils.getDynamicAllocationInitialExecutors(conf)\n+      val maxNumExecutors = conf.get(DYN_ALLOCATION_MAX_EXECUTORS)\n+      require(\n+        initialNumExecutors >= minNumExecutors && initialNumExecutors <= maxNumExecutors,\n+        s\"initial executor number $initialNumExecutors must between min executor number \" +\n+          s\"$minNumExecutors and max executor number $maxNumExecutors\")\n+\n+      initialNumExecutors\n+    } else {\n+      conf.get(EXECUTOR_INSTANCES).getOrElse(numExecutors)\n+    }\n+  }\n+\n+  def createExecutorPod(executorNum: Int): String = {\n+    // create a single k8s executor pod.\n+    val labelMap = Map(\"type\" -> \"spark-executor\")\n+    val podName = s\"$sparkExecutorName-$executorNum\"\n+\n+    val submitArgs = mutable.ArrayBuffer.empty[String]\n+\n+    if (conf.getBoolean(\"spark.dynamicAllocation.enabled\", false)) {\n+      submitArgs ++= Vector(\n+        \"dynamic-executors\")\n+    }\n+\n+    submitArgs ++= Vector(\"org.apache.spark.executor.CoarseGrainedExecutorBackend\",\n+      \"--driver-url\", s\"$driverURL\",\n+      \"--executor-id\", s\"$executorNum\",\n+      \"--hostname\", \"localhost\",",
    "line": 183
  }],
  "prId": 16061
}, {
  "comments": [{
    "author": {
      "login": "mccheah"
    },
    "body": "Is this used anywhere in this class?",
    "commit": "8584913d51c1f68c8f9d3cdf142947cd35cdfcc2",
    "createdAt": "2016-11-30T20:43:48Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.kubernetes\n+\n+import collection.JavaConverters._\n+import io.fabric8.kubernetes.api.model.PodBuilder\n+import io.fabric8.kubernetes.api.model.extensions.JobBuilder\n+import io.fabric8.kubernetes.client.{ConfigBuilder, DefaultKubernetesClient}\n+import org.apache.spark.internal.config._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster._\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages._\n+import org.apache.spark.{SparkConf, SparkContext, SparkException}\n+import org.apache.spark.rpc.RpcEndpointAddress\n+import org.apache.spark.scheduler.TaskSchedulerImpl\n+import org.apache.spark.util.Utils\n+\n+import scala.collection.mutable\n+import scala.util.Random\n+import scala.concurrent.Future\n+\n+private[spark] class KubernetesClusterSchedulerBackend(\n+                                                  scheduler: TaskSchedulerImpl,\n+                                                  sc: SparkContext)\n+  extends CoarseGrainedSchedulerBackend(scheduler, sc.env.rpcEnv) {\n+\n+  val client = new DefaultKubernetesClient()\n+\n+  val DEFAULT_NUMBER_EXECUTORS = 2\n+  val sparkExecutorName = s\"spark-executor-${Random.alphanumeric take 5 mkString(\"\")}\".toLowerCase()\n+\n+  // TODO: do these need mutex guarding?\n+  // key is executor id, value is pod name\n+  var executorToPod = mutable.Map.empty[String, String] // active executors\n+  var shutdownToPod = mutable.Map.empty[String, String] // pending shutdown\n+  var executorID = 0\n+\n+  val sparkImage = conf.get(\"spark.kubernetes.sparkImage\")\n+  val clientJarUri = conf.get(\"spark.executor.jar\")",
    "line": 54
  }],
  "prId": 16061
}, {
  "comments": [{
    "author": {
      "login": "mccheah"
    },
    "body": "We can fill this in with the `applicationId()` method defined in `SchedulerBackend`.",
    "commit": "8584913d51c1f68c8f9d3cdf142947cd35cdfcc2",
    "createdAt": "2016-11-30T20:45:11Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.kubernetes\n+\n+import collection.JavaConverters._\n+import io.fabric8.kubernetes.api.model.PodBuilder\n+import io.fabric8.kubernetes.api.model.extensions.JobBuilder\n+import io.fabric8.kubernetes.client.{ConfigBuilder, DefaultKubernetesClient}\n+import org.apache.spark.internal.config._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster._\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages._\n+import org.apache.spark.{SparkConf, SparkContext, SparkException}\n+import org.apache.spark.rpc.RpcEndpointAddress\n+import org.apache.spark.scheduler.TaskSchedulerImpl\n+import org.apache.spark.util.Utils\n+\n+import scala.collection.mutable\n+import scala.util.Random\n+import scala.concurrent.Future\n+\n+private[spark] class KubernetesClusterSchedulerBackend(\n+                                                  scheduler: TaskSchedulerImpl,\n+                                                  sc: SparkContext)\n+  extends CoarseGrainedSchedulerBackend(scheduler, sc.env.rpcEnv) {\n+\n+  val client = new DefaultKubernetesClient()\n+\n+  val DEFAULT_NUMBER_EXECUTORS = 2\n+  val sparkExecutorName = s\"spark-executor-${Random.alphanumeric take 5 mkString(\"\")}\".toLowerCase()\n+\n+  // TODO: do these need mutex guarding?\n+  // key is executor id, value is pod name\n+  var executorToPod = mutable.Map.empty[String, String] // active executors\n+  var shutdownToPod = mutable.Map.empty[String, String] // pending shutdown\n+  var executorID = 0\n+\n+  val sparkImage = conf.get(\"spark.kubernetes.sparkImage\")\n+  val clientJarUri = conf.get(\"spark.executor.jar\")\n+  val ns = conf.get(\n+    \"spark.kubernetes.namespace\",\n+    KubernetesClusterScheduler.defaultNameSpace)\n+  val dynamicExecutors = Utils.isDynamicAllocationEnabled(conf)\n+\n+  // executor back-ends take their configuration this way\n+  if (dynamicExecutors) {\n+    conf.setExecutorEnv(\"spark.dynamicAllocation.enabled\", \"true\")\n+    conf.setExecutorEnv(\"spark.shuffle.service.enabled\", \"true\")\n+  }\n+\n+  override def start(): Unit = {\n+    super.start()\n+    createExecutorPods(getInitialTargetExecutorNumber(sc.getConf))\n+  }\n+\n+  override def stop(): Unit = {\n+    // Kill all executor pods indiscriminately\n+    killExecutorPods(executorToPod.toVector)\n+    killExecutorPods(shutdownToPod.toVector)\n+    super.stop()\n+  }\n+\n+  // Dynamic allocation interfaces\n+  override def doRequestTotalExecutors(requestedTotal: Int): Future[Boolean] = {\n+    logInfo(s\"Received doRequestTotalExecutors: $requestedTotal\")\n+    val n = executorToPod.size\n+    val delta = requestedTotal - n\n+    if (delta > 0) {\n+      logInfo(s\"Adding $delta new executors\")\n+      createExecutorPods(delta)\n+    } else if (delta < 0) {\n+      val d = -delta\n+      val idle = executorToPod.toVector.filter { case (id, _) => !scheduler.isExecutorBusy(id) }\n+      if (idle.length > 0) {\n+        logInfo(s\"Shutting down ${idle.length} idle executors\")\n+        shutdownExecutors(idle.take(d))\n+      }\n+      val r = math.max(0, d - idle.length)\n+      if (r > 0) {\n+        logInfo(s\"Shutting down $r non-idle executors\")\n+        shutdownExecutors(executorToPod.toVector.slice(n - r, n))\n+      }\n+    }\n+    // TODO: are there meaningful failure modes here?\n+    Future.successful(true)\n+  }\n+\n+  override def doKillExecutors(executorIds: Seq[String]): Future[Boolean] = {\n+    logInfo(s\"Received doKillExecutors\")\n+    killExecutorPods(executorIds.map { id => (id, executorToPod(id)) })\n+    Future.successful(true)\n+  }\n+\n+  private def createExecutorPods(n: Int) {\n+    for (i <- 1 to n) {\n+      executorID += 1\n+      executorToPod += ((executorID.toString, createExecutorPod(executorID)))\n+    }\n+  }\n+\n+  def shutdownExecutors(idPodPairs: Seq[(String, String)]) {\n+    val active = getExecutorIds.toSet\n+\n+    // Check for any finished shutting down and kill the pods\n+    val shutdown = shutdownToPod.toVector.filter { case (e, _) => !active.contains(e) }\n+    killExecutorPods(shutdown)\n+\n+    // Now request shutdown for the new ones.\n+    // Move them from executor list to list pending shutdown\n+    for ((id, pod) <- idPodPairs) {\n+      try {\n+        // TODO: 'ask' returns a future - can it be used to check eventual success?\n+        Option(driverEndpoint).foreach(_.ask[Boolean](RemoveExecutor(id, ExecutorKilled)))\n+        executorToPod -= id\n+        shutdownToPod += ((id, pod))\n+      } catch {\n+        case e: Exception => logError(s\"Error shutting down executor $id\", e)\n+      }\n+    }\n+  }\n+\n+  private def killExecutorPods(idPodPairs: Seq[(String, String)]) {\n+    for ((id, pod) <- idPodPairs) {\n+      try {\n+        client.pods().inNamespace(ns).withName(pod).delete()\n+        executorToPod -= id\n+        shutdownToPod -= id\n+      } catch {\n+        case e: Exception => logError(s\"Error killing executor pod $pod\", e)\n+      }\n+    }\n+  }\n+\n+  def getInitialTargetExecutorNumber(conf: SparkConf,\n+                                     numExecutors: Int =\n+                                     DEFAULT_NUMBER_EXECUTORS): Int = {\n+    if (dynamicExecutors) {\n+      val minNumExecutors = conf.get(DYN_ALLOCATION_MIN_EXECUTORS)\n+      val initialNumExecutors =\n+        Utils.getDynamicAllocationInitialExecutors(conf)\n+      val maxNumExecutors = conf.get(DYN_ALLOCATION_MAX_EXECUTORS)\n+      require(\n+        initialNumExecutors >= minNumExecutors && initialNumExecutors <= maxNumExecutors,\n+        s\"initial executor number $initialNumExecutors must between min executor number \" +\n+          s\"$minNumExecutors and max executor number $maxNumExecutors\")\n+\n+      initialNumExecutors\n+    } else {\n+      conf.get(EXECUTOR_INSTANCES).getOrElse(numExecutors)\n+    }\n+  }\n+\n+  def createExecutorPod(executorNum: Int): String = {\n+    // create a single k8s executor pod.\n+    val labelMap = Map(\"type\" -> \"spark-executor\")\n+    val podName = s\"$sparkExecutorName-$executorNum\"\n+\n+    val submitArgs = mutable.ArrayBuffer.empty[String]\n+\n+    if (conf.getBoolean(\"spark.dynamicAllocation.enabled\", false)) {\n+      submitArgs ++= Vector(\n+        \"dynamic-executors\")\n+    }\n+\n+    submitArgs ++= Vector(\"org.apache.spark.executor.CoarseGrainedExecutorBackend\",\n+      \"--driver-url\", s\"$driverURL\",\n+      \"--executor-id\", s\"$executorNum\",\n+      \"--hostname\", \"localhost\",\n+      \"--app-id\", \"1\", // TODO: change app-id per application and pass from driver.",
    "line": 184
  }],
  "prId": 16061
}]