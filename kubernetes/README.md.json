[{
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "Do we need to prepare an official image for this?",
    "commit": "8584913d51c1f68c8f9d3cdf142947cd35cdfcc2",
    "createdAt": "2016-11-30T04:53:06Z",
    "diffHunk": "@@ -0,0 +1,21 @@\n+# Pre-requisites\n+* maven, JDK and all other pre-requisites for building Spark.\n+\n+# Steps to compile\n+\n+* Clone the fork of spark: https://github.com/foxish/spark/ and switch to the k8s-support branch.\n+* Build the project\n+    * ./build/mvn -Pkubernetes -Phadoop-2.4 -Dhadoop.version=2.4.0 -DskipTests package\n+* Ensure that you are pointing to a k8s cluster (kubectl config current-context), which you want to use with spark.\n+* Launch a spark-submit job:\n+   * `./bin/spark-submit --deploy-mode cluster --class org.apache.spark.examples.SparkPi --master k8s://default --conf spark.executor.instances=5 --conf spark.kubernetes.sparkImage=manyangled/kube-spark:dynamic  http://storage.googleapis.com/foxish-spark-distro/original-spark-examples_2.11-2.1.0-SNAPSHOT.jar 10000`",
    "line": 11
  }, {
    "author": {
      "login": "erikerlandson"
    },
    "body": "There will need to be some official Apache Spark repository for images, which I presume will be up to Apache Spark to create.   The exact nature of the images to be produced is still being discussed\r\n\r\nI did create a \"semi-official\" org up on docker hub called \"k8s4spark\":\r\nhttps://hub.docker.com/u/k8s4spark/dashboard/\r\n\r\nI haven't actually pushed any images to it, but we could start using it as an interim repo if people think that is useful",
    "commit": "8584913d51c1f68c8f9d3cdf142947cd35cdfcc2",
    "createdAt": "2016-11-30T17:48:13Z",
    "diffHunk": "@@ -0,0 +1,21 @@\n+# Pre-requisites\n+* maven, JDK and all other pre-requisites for building Spark.\n+\n+# Steps to compile\n+\n+* Clone the fork of spark: https://github.com/foxish/spark/ and switch to the k8s-support branch.\n+* Build the project\n+    * ./build/mvn -Pkubernetes -Phadoop-2.4 -Dhadoop.version=2.4.0 -DskipTests package\n+* Ensure that you are pointing to a k8s cluster (kubectl config current-context), which you want to use with spark.\n+* Launch a spark-submit job:\n+   * `./bin/spark-submit --deploy-mode cluster --class org.apache.spark.examples.SparkPi --master k8s://default --conf spark.executor.instances=5 --conf spark.kubernetes.sparkImage=manyangled/kube-spark:dynamic  http://storage.googleapis.com/foxish-spark-distro/original-spark-examples_2.11-2.1.0-SNAPSHOT.jar 10000`",
    "line": 11
  }],
  "prId": 16061
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "I think this is not correct now?",
    "commit": "8584913d51c1f68c8f9d3cdf142947cd35cdfcc2",
    "createdAt": "2016-11-30T04:53:46Z",
    "diffHunk": "@@ -0,0 +1,21 @@\n+# Pre-requisites\n+* maven, JDK and all other pre-requisites for building Spark.\n+\n+# Steps to compile\n+\n+* Clone the fork of spark: https://github.com/foxish/spark/ and switch to the k8s-support branch.",
    "line": 6
  }],
  "prId": 16061
}, {
  "comments": [{
    "author": {
      "login": "jaceklaskowski"
    },
    "body": "I think `hadoop-2.4` is gone in master.",
    "commit": "8584913d51c1f68c8f9d3cdf142947cd35cdfcc2",
    "createdAt": "2017-03-03T17:46:45Z",
    "diffHunk": "@@ -0,0 +1,21 @@\n+# Pre-requisites\n+* maven, JDK and all other pre-requisites for building Spark.\n+\n+# Steps to compile\n+\n+* Clone the fork of spark: https://github.com/foxish/spark/ and switch to the k8s-support branch.\n+* Build the project\n+    * ./build/mvn -Pkubernetes -Phadoop-2.4 -Dhadoop.version=2.4.0 -DskipTests package",
    "line": 8
  }],
  "prId": 16061
}]