[{
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "slightly cleaner: `.foreach { case (k, v) => map.put(k, toJavaObject(v)) }`\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T00:23:11Z",
    "diffHunk": "@@ -0,0 +1,720 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.io.IOException\n+import java.net.{URI, URL}\n+import java.text.DateFormat\n+import java.util.concurrent.atomic.AtomicLong\n+import java.util.{ArrayList => JArrayList, Collection => JCollection, Date, HashMap => JHashMap, Map => JMap}\n+import java.{lang, util}\n+\n+import scala.collection.JavaConversions._\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.service.Service\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse.TimelinePutError\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+import org.json4s.JsonAST.JObject\n+import org.json4s._\n+import org.json4s.jackson.JsonMethods._\n+\n+import org.apache.spark.{SparkContext, Logging}\n+import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+import org.apache.spark.scheduler.{SparkListenerStageCompleted, SparkListenerStageSubmitted, SparkListenerExecutorAdded, SparkListenerExecutorRemoved, SparkListenerJobEnd, SparkListenerJobStart, SparkListenerApplicationEnd, SparkListenerApplicationStart, SparkListenerEvent}\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+/**\n+ * Utility methods for timeline classes.\n+ */\n+private[spark] object YarnTimelineUtils extends Logging {\n+\n+  /**\n+   * What attempt ID to use as the attempt ID field (not the entity ID) when\n+   * there is no attempt info\n+   */\n+  val SINGLE_ATTEMPT = \"1\"\n+\n+\n+  /**\n+   * Exception text when there is no event info data to unmarshall\n+   */\n+  val E_NO_EVENTINFO = \"No 'eventinfo' entry\"\n+\n+  /**\n+   * Exception text when there is event info entry in the timeline event, but it is empty\n+   */\n+\n+  val E_EMPTY_EVENTINFO = \"Empty 'eventinfo' entry\"\n+\n+  /**\n+   * counter incremented on every spark event to timeline event creation,\n+   * so guaranteeing uniqueness of event IDs across a single application attempt\n+   * (which is implicitly, one per JVM)\n+   */\n+  val uid = new AtomicLong(System.currentTimeMillis())\n+\n+  /**\n+   * Converts a Java object to its equivalent json4s representation.\n+   */\n+  def toJValue(obj: Object): JValue = {\n+    obj match {\n+      case str: String => JString(str)\n+      case dbl: java.lang.Double => JDouble(dbl)\n+      case dec: java.math.BigDecimal => JDecimal(dec)\n+      case int: java.lang.Integer => JInt(BigInt(int))\n+      case long: java.lang.Long => JInt(BigInt(long))\n+      case bool: java.lang.Boolean => JBool(bool)\n+      case map: JMap[_, _] =>\n+        val jmap = map.asInstanceOf[JMap[String, Object]]\n+        JObject(jmap.entrySet().map { e => (e.getKey() -> toJValue(e.getValue())) }.toList)\n+      case array: JCollection[_] =>\n+        JArray(array.asInstanceOf[JCollection[Object]].map(o => toJValue(o)).toList)\n+      case null => JNothing\n+    }\n+  }\n+\n+  /**\n+   * Converts a JValue into its Java equivalent.\n+   */\n+  def toJavaObject(v: JValue): Object = {\n+    v match {\n+      case JNothing => null\n+      case JNull => null\n+      case JString(s) => s\n+      case JDouble(num) => java.lang.Double.valueOf(num)\n+      case JDecimal(num) => num.bigDecimal\n+      case JInt(num) => java.lang.Long.valueOf(num.longValue)\n+      case JBool(value) => java.lang.Boolean.valueOf(value)\n+      case obj: JObject => toJavaMap(obj)\n+      case JArray(vals) => {\n+        val list = new JArrayList[Object]()\n+        vals.foreach(x => list.add(toJavaObject(x)))\n+        list\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Converts a json4s list of fields into a Java Map suitable for serialization by Jackson,\n+   * which is used by the ATS client library.\n+   */\n+  def toJavaMap(sourceObj: JObject): JHashMap[String, Object] = {\n+    val map = new JHashMap[String, Object]()\n+    sourceObj.obj.foreach(f => map.put(f._1, toJavaObject(f._2)))"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Why `IOException`?\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T00:23:46Z",
    "diffHunk": "@@ -0,0 +1,720 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.io.IOException\n+import java.net.{URI, URL}\n+import java.text.DateFormat\n+import java.util.concurrent.atomic.AtomicLong\n+import java.util.{ArrayList => JArrayList, Collection => JCollection, Date, HashMap => JHashMap, Map => JMap}\n+import java.{lang, util}\n+\n+import scala.collection.JavaConversions._\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.service.Service\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse.TimelinePutError\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+import org.json4s.JsonAST.JObject\n+import org.json4s._\n+import org.json4s.jackson.JsonMethods._\n+\n+import org.apache.spark.{SparkContext, Logging}\n+import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+import org.apache.spark.scheduler.{SparkListenerStageCompleted, SparkListenerStageSubmitted, SparkListenerExecutorAdded, SparkListenerExecutorRemoved, SparkListenerJobEnd, SparkListenerJobStart, SparkListenerApplicationEnd, SparkListenerApplicationStart, SparkListenerEvent}\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+/**\n+ * Utility methods for timeline classes.\n+ */\n+private[spark] object YarnTimelineUtils extends Logging {\n+\n+  /**\n+   * What attempt ID to use as the attempt ID field (not the entity ID) when\n+   * there is no attempt info\n+   */\n+  val SINGLE_ATTEMPT = \"1\"\n+\n+\n+  /**\n+   * Exception text when there is no event info data to unmarshall\n+   */\n+  val E_NO_EVENTINFO = \"No 'eventinfo' entry\"\n+\n+  /**\n+   * Exception text when there is event info entry in the timeline event, but it is empty\n+   */\n+\n+  val E_EMPTY_EVENTINFO = \"Empty 'eventinfo' entry\"\n+\n+  /**\n+   * counter incremented on every spark event to timeline event creation,\n+   * so guaranteeing uniqueness of event IDs across a single application attempt\n+   * (which is implicitly, one per JVM)\n+   */\n+  val uid = new AtomicLong(System.currentTimeMillis())\n+\n+  /**\n+   * Converts a Java object to its equivalent json4s representation.\n+   */\n+  def toJValue(obj: Object): JValue = {\n+    obj match {\n+      case str: String => JString(str)\n+      case dbl: java.lang.Double => JDouble(dbl)\n+      case dec: java.math.BigDecimal => JDecimal(dec)\n+      case int: java.lang.Integer => JInt(BigInt(int))\n+      case long: java.lang.Long => JInt(BigInt(long))\n+      case bool: java.lang.Boolean => JBool(bool)\n+      case map: JMap[_, _] =>\n+        val jmap = map.asInstanceOf[JMap[String, Object]]\n+        JObject(jmap.entrySet().map { e => (e.getKey() -> toJValue(e.getValue())) }.toList)\n+      case array: JCollection[_] =>\n+        JArray(array.asInstanceOf[JCollection[Object]].map(o => toJValue(o)).toList)\n+      case null => JNothing\n+    }\n+  }\n+\n+  /**\n+   * Converts a JValue into its Java equivalent.\n+   */\n+  def toJavaObject(v: JValue): Object = {\n+    v match {\n+      case JNothing => null\n+      case JNull => null\n+      case JString(s) => s\n+      case JDouble(num) => java.lang.Double.valueOf(num)\n+      case JDecimal(num) => num.bigDecimal\n+      case JInt(num) => java.lang.Long.valueOf(num.longValue)\n+      case JBool(value) => java.lang.Boolean.valueOf(value)\n+      case obj: JObject => toJavaMap(obj)\n+      case JArray(vals) => {\n+        val list = new JArrayList[Object]()\n+        vals.foreach(x => list.add(toJavaObject(x)))\n+        list\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Converts a json4s list of fields into a Java Map suitable for serialization by Jackson,\n+   * which is used by the ATS client library.\n+   */\n+  def toJavaMap(sourceObj: JObject): JHashMap[String, Object] = {\n+    val map = new JHashMap[String, Object]()\n+    sourceObj.obj.foreach(f => map.put(f._1, toJavaObject(f._2)))\n+    map\n+  }\n+\n+  /**\n+   * Convert a timeline event to a spark one. Includes\n+   * some basic checks for validity of event payload.\n+   * @param event timeline event\n+   * @return an unmarshalled event\n+   */\n+  def toSparkEvent(event: TimelineEvent): SparkListenerEvent = {\n+    val info = event.getEventInfo()\n+    if (info == null) {\n+      throw new IOException(E_NO_EVENTINFO)"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "I think I considered ser/deser issues to be part of the general IO process. Would you suggest something else?\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T17:13:34Z",
    "diffHunk": "@@ -0,0 +1,720 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.io.IOException\n+import java.net.{URI, URL}\n+import java.text.DateFormat\n+import java.util.concurrent.atomic.AtomicLong\n+import java.util.{ArrayList => JArrayList, Collection => JCollection, Date, HashMap => JHashMap, Map => JMap}\n+import java.{lang, util}\n+\n+import scala.collection.JavaConversions._\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.service.Service\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse.TimelinePutError\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+import org.json4s.JsonAST.JObject\n+import org.json4s._\n+import org.json4s.jackson.JsonMethods._\n+\n+import org.apache.spark.{SparkContext, Logging}\n+import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+import org.apache.spark.scheduler.{SparkListenerStageCompleted, SparkListenerStageSubmitted, SparkListenerExecutorAdded, SparkListenerExecutorRemoved, SparkListenerJobEnd, SparkListenerJobStart, SparkListenerApplicationEnd, SparkListenerApplicationStart, SparkListenerEvent}\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+/**\n+ * Utility methods for timeline classes.\n+ */\n+private[spark] object YarnTimelineUtils extends Logging {\n+\n+  /**\n+   * What attempt ID to use as the attempt ID field (not the entity ID) when\n+   * there is no attempt info\n+   */\n+  val SINGLE_ATTEMPT = \"1\"\n+\n+\n+  /**\n+   * Exception text when there is no event info data to unmarshall\n+   */\n+  val E_NO_EVENTINFO = \"No 'eventinfo' entry\"\n+\n+  /**\n+   * Exception text when there is event info entry in the timeline event, but it is empty\n+   */\n+\n+  val E_EMPTY_EVENTINFO = \"Empty 'eventinfo' entry\"\n+\n+  /**\n+   * counter incremented on every spark event to timeline event creation,\n+   * so guaranteeing uniqueness of event IDs across a single application attempt\n+   * (which is implicitly, one per JVM)\n+   */\n+  val uid = new AtomicLong(System.currentTimeMillis())\n+\n+  /**\n+   * Converts a Java object to its equivalent json4s representation.\n+   */\n+  def toJValue(obj: Object): JValue = {\n+    obj match {\n+      case str: String => JString(str)\n+      case dbl: java.lang.Double => JDouble(dbl)\n+      case dec: java.math.BigDecimal => JDecimal(dec)\n+      case int: java.lang.Integer => JInt(BigInt(int))\n+      case long: java.lang.Long => JInt(BigInt(long))\n+      case bool: java.lang.Boolean => JBool(bool)\n+      case map: JMap[_, _] =>\n+        val jmap = map.asInstanceOf[JMap[String, Object]]\n+        JObject(jmap.entrySet().map { e => (e.getKey() -> toJValue(e.getValue())) }.toList)\n+      case array: JCollection[_] =>\n+        JArray(array.asInstanceOf[JCollection[Object]].map(o => toJValue(o)).toList)\n+      case null => JNothing\n+    }\n+  }\n+\n+  /**\n+   * Converts a JValue into its Java equivalent.\n+   */\n+  def toJavaObject(v: JValue): Object = {\n+    v match {\n+      case JNothing => null\n+      case JNull => null\n+      case JString(s) => s\n+      case JDouble(num) => java.lang.Double.valueOf(num)\n+      case JDecimal(num) => num.bigDecimal\n+      case JInt(num) => java.lang.Long.valueOf(num.longValue)\n+      case JBool(value) => java.lang.Boolean.valueOf(value)\n+      case obj: JObject => toJavaMap(obj)\n+      case JArray(vals) => {\n+        val list = new JArrayList[Object]()\n+        vals.foreach(x => list.add(toJavaObject(x)))\n+        list\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Converts a json4s list of fields into a Java Map suitable for serialization by Jackson,\n+   * which is used by the ATS client library.\n+   */\n+  def toJavaMap(sourceObj: JObject): JHashMap[String, Object] = {\n+    val map = new JHashMap[String, Object]()\n+    sourceObj.obj.foreach(f => map.put(f._1, toJavaObject(f._2)))\n+    map\n+  }\n+\n+  /**\n+   * Convert a timeline event to a spark one. Includes\n+   * some basic checks for validity of event payload.\n+   * @param event timeline event\n+   * @return an unmarshalled event\n+   */\n+  def toSparkEvent(event: TimelineEvent): SparkListenerEvent = {\n+    val info = event.getEventInfo()\n+    if (info == null) {\n+      throw new IOException(E_NO_EVENTINFO)"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "It's just that this doesn't seem like a deser issue; you seem to be asserting that the data retrieved from the ATS matches your expectations. If it were caused by bad serialized data, wouldn't an `IOException` or some deserializer-specific exception be thrown somewhere up the call stack?\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T20:02:10Z",
    "diffHunk": "@@ -0,0 +1,720 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.io.IOException\n+import java.net.{URI, URL}\n+import java.text.DateFormat\n+import java.util.concurrent.atomic.AtomicLong\n+import java.util.{ArrayList => JArrayList, Collection => JCollection, Date, HashMap => JHashMap, Map => JMap}\n+import java.{lang, util}\n+\n+import scala.collection.JavaConversions._\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.service.Service\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse.TimelinePutError\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+import org.json4s.JsonAST.JObject\n+import org.json4s._\n+import org.json4s.jackson.JsonMethods._\n+\n+import org.apache.spark.{SparkContext, Logging}\n+import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+import org.apache.spark.scheduler.{SparkListenerStageCompleted, SparkListenerStageSubmitted, SparkListenerExecutorAdded, SparkListenerExecutorRemoved, SparkListenerJobEnd, SparkListenerJobStart, SparkListenerApplicationEnd, SparkListenerApplicationStart, SparkListenerEvent}\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+/**\n+ * Utility methods for timeline classes.\n+ */\n+private[spark] object YarnTimelineUtils extends Logging {\n+\n+  /**\n+   * What attempt ID to use as the attempt ID field (not the entity ID) when\n+   * there is no attempt info\n+   */\n+  val SINGLE_ATTEMPT = \"1\"\n+\n+\n+  /**\n+   * Exception text when there is no event info data to unmarshall\n+   */\n+  val E_NO_EVENTINFO = \"No 'eventinfo' entry\"\n+\n+  /**\n+   * Exception text when there is event info entry in the timeline event, but it is empty\n+   */\n+\n+  val E_EMPTY_EVENTINFO = \"Empty 'eventinfo' entry\"\n+\n+  /**\n+   * counter incremented on every spark event to timeline event creation,\n+   * so guaranteeing uniqueness of event IDs across a single application attempt\n+   * (which is implicitly, one per JVM)\n+   */\n+  val uid = new AtomicLong(System.currentTimeMillis())\n+\n+  /**\n+   * Converts a Java object to its equivalent json4s representation.\n+   */\n+  def toJValue(obj: Object): JValue = {\n+    obj match {\n+      case str: String => JString(str)\n+      case dbl: java.lang.Double => JDouble(dbl)\n+      case dec: java.math.BigDecimal => JDecimal(dec)\n+      case int: java.lang.Integer => JInt(BigInt(int))\n+      case long: java.lang.Long => JInt(BigInt(long))\n+      case bool: java.lang.Boolean => JBool(bool)\n+      case map: JMap[_, _] =>\n+        val jmap = map.asInstanceOf[JMap[String, Object]]\n+        JObject(jmap.entrySet().map { e => (e.getKey() -> toJValue(e.getValue())) }.toList)\n+      case array: JCollection[_] =>\n+        JArray(array.asInstanceOf[JCollection[Object]].map(o => toJValue(o)).toList)\n+      case null => JNothing\n+    }\n+  }\n+\n+  /**\n+   * Converts a JValue into its Java equivalent.\n+   */\n+  def toJavaObject(v: JValue): Object = {\n+    v match {\n+      case JNothing => null\n+      case JNull => null\n+      case JString(s) => s\n+      case JDouble(num) => java.lang.Double.valueOf(num)\n+      case JDecimal(num) => num.bigDecimal\n+      case JInt(num) => java.lang.Long.valueOf(num.longValue)\n+      case JBool(value) => java.lang.Boolean.valueOf(value)\n+      case obj: JObject => toJavaMap(obj)\n+      case JArray(vals) => {\n+        val list = new JArrayList[Object]()\n+        vals.foreach(x => list.add(toJavaObject(x)))\n+        list\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Converts a json4s list of fields into a Java Map suitable for serialization by Jackson,\n+   * which is used by the ATS client library.\n+   */\n+  def toJavaMap(sourceObj: JObject): JHashMap[String, Object] = {\n+    val map = new JHashMap[String, Object]()\n+    sourceObj.obj.foreach(f => map.put(f._1, toJavaObject(f._2)))\n+    map\n+  }\n+\n+  /**\n+   * Convert a timeline event to a spark one. Includes\n+   * some basic checks for validity of event payload.\n+   * @param event timeline event\n+   * @return an unmarshalled event\n+   */\n+  def toSparkEvent(event: TimelineEvent): SparkListenerEvent = {\n+    val info = event.getEventInfo()\n+    if (info == null) {\n+      throw new IOException(E_NO_EVENTINFO)"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: funky indentation.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T00:24:15Z",
    "diffHunk": "@@ -0,0 +1,720 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.io.IOException\n+import java.net.{URI, URL}\n+import java.text.DateFormat\n+import java.util.concurrent.atomic.AtomicLong\n+import java.util.{ArrayList => JArrayList, Collection => JCollection, Date, HashMap => JHashMap, Map => JMap}\n+import java.{lang, util}\n+\n+import scala.collection.JavaConversions._\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.service.Service\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse.TimelinePutError\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+import org.json4s.JsonAST.JObject\n+import org.json4s._\n+import org.json4s.jackson.JsonMethods._\n+\n+import org.apache.spark.{SparkContext, Logging}\n+import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+import org.apache.spark.scheduler.{SparkListenerStageCompleted, SparkListenerStageSubmitted, SparkListenerExecutorAdded, SparkListenerExecutorRemoved, SparkListenerJobEnd, SparkListenerJobStart, SparkListenerApplicationEnd, SparkListenerApplicationStart, SparkListenerEvent}\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+/**\n+ * Utility methods for timeline classes.\n+ */\n+private[spark] object YarnTimelineUtils extends Logging {\n+\n+  /**\n+   * What attempt ID to use as the attempt ID field (not the entity ID) when\n+   * there is no attempt info\n+   */\n+  val SINGLE_ATTEMPT = \"1\"\n+\n+\n+  /**\n+   * Exception text when there is no event info data to unmarshall\n+   */\n+  val E_NO_EVENTINFO = \"No 'eventinfo' entry\"\n+\n+  /**\n+   * Exception text when there is event info entry in the timeline event, but it is empty\n+   */\n+\n+  val E_EMPTY_EVENTINFO = \"Empty 'eventinfo' entry\"\n+\n+  /**\n+   * counter incremented on every spark event to timeline event creation,\n+   * so guaranteeing uniqueness of event IDs across a single application attempt\n+   * (which is implicitly, one per JVM)\n+   */\n+  val uid = new AtomicLong(System.currentTimeMillis())\n+\n+  /**\n+   * Converts a Java object to its equivalent json4s representation.\n+   */\n+  def toJValue(obj: Object): JValue = {\n+    obj match {\n+      case str: String => JString(str)\n+      case dbl: java.lang.Double => JDouble(dbl)\n+      case dec: java.math.BigDecimal => JDecimal(dec)\n+      case int: java.lang.Integer => JInt(BigInt(int))\n+      case long: java.lang.Long => JInt(BigInt(long))\n+      case bool: java.lang.Boolean => JBool(bool)\n+      case map: JMap[_, _] =>\n+        val jmap = map.asInstanceOf[JMap[String, Object]]\n+        JObject(jmap.entrySet().map { e => (e.getKey() -> toJValue(e.getValue())) }.toList)\n+      case array: JCollection[_] =>\n+        JArray(array.asInstanceOf[JCollection[Object]].map(o => toJValue(o)).toList)\n+      case null => JNothing\n+    }\n+  }\n+\n+  /**\n+   * Converts a JValue into its Java equivalent.\n+   */\n+  def toJavaObject(v: JValue): Object = {\n+    v match {\n+      case JNothing => null\n+      case JNull => null\n+      case JString(s) => s\n+      case JDouble(num) => java.lang.Double.valueOf(num)\n+      case JDecimal(num) => num.bigDecimal\n+      case JInt(num) => java.lang.Long.valueOf(num.longValue)\n+      case JBool(value) => java.lang.Boolean.valueOf(value)\n+      case obj: JObject => toJavaMap(obj)\n+      case JArray(vals) => {\n+        val list = new JArrayList[Object]()\n+        vals.foreach(x => list.add(toJavaObject(x)))\n+        list\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Converts a json4s list of fields into a Java Map suitable for serialization by Jackson,\n+   * which is used by the ATS client library.\n+   */\n+  def toJavaMap(sourceObj: JObject): JHashMap[String, Object] = {\n+    val map = new JHashMap[String, Object]()\n+    sourceObj.obj.foreach(f => map.put(f._1, toJavaObject(f._2)))\n+    map\n+  }\n+\n+  /**\n+   * Convert a timeline event to a spark one. Includes\n+   * some basic checks for validity of event payload.\n+   * @param event timeline event\n+   * @return an unmarshalled event\n+   */\n+  def toSparkEvent(event: TimelineEvent): SparkListenerEvent = {\n+    val info = event.getEventInfo()\n+    if (info == null) {\n+      throw new IOException(E_NO_EVENTINFO)\n+    }\n+    if (info.size() == 0) {\n+      throw new IOException(E_EMPTY_EVENTINFO)\n+    }\n+    val payload = toJValue(info)\n+    def jsonToString: String = {\n+      val json = compact(render(payload))\n+      val limit = 256\n+      if (json.length < limit) {\n+        json\n+      } else {\n+          json.substring(0, limit) + \" ... }\""
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "```\n`val eventDetails = try { ... `\n```\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T00:26:18Z",
    "diffHunk": "@@ -0,0 +1,720 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.io.IOException\n+import java.net.{URI, URL}\n+import java.text.DateFormat\n+import java.util.concurrent.atomic.AtomicLong\n+import java.util.{ArrayList => JArrayList, Collection => JCollection, Date, HashMap => JHashMap, Map => JMap}\n+import java.{lang, util}\n+\n+import scala.collection.JavaConversions._\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.service.Service\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse.TimelinePutError\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+import org.json4s.JsonAST.JObject\n+import org.json4s._\n+import org.json4s.jackson.JsonMethods._\n+\n+import org.apache.spark.{SparkContext, Logging}\n+import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+import org.apache.spark.scheduler.{SparkListenerStageCompleted, SparkListenerStageSubmitted, SparkListenerExecutorAdded, SparkListenerExecutorRemoved, SparkListenerJobEnd, SparkListenerJobStart, SparkListenerApplicationEnd, SparkListenerApplicationStart, SparkListenerEvent}\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+/**\n+ * Utility methods for timeline classes.\n+ */\n+private[spark] object YarnTimelineUtils extends Logging {\n+\n+  /**\n+   * What attempt ID to use as the attempt ID field (not the entity ID) when\n+   * there is no attempt info\n+   */\n+  val SINGLE_ATTEMPT = \"1\"\n+\n+\n+  /**\n+   * Exception text when there is no event info data to unmarshall\n+   */\n+  val E_NO_EVENTINFO = \"No 'eventinfo' entry\"\n+\n+  /**\n+   * Exception text when there is event info entry in the timeline event, but it is empty\n+   */\n+\n+  val E_EMPTY_EVENTINFO = \"Empty 'eventinfo' entry\"\n+\n+  /**\n+   * counter incremented on every spark event to timeline event creation,\n+   * so guaranteeing uniqueness of event IDs across a single application attempt\n+   * (which is implicitly, one per JVM)\n+   */\n+  val uid = new AtomicLong(System.currentTimeMillis())\n+\n+  /**\n+   * Converts a Java object to its equivalent json4s representation.\n+   */\n+  def toJValue(obj: Object): JValue = {\n+    obj match {\n+      case str: String => JString(str)\n+      case dbl: java.lang.Double => JDouble(dbl)\n+      case dec: java.math.BigDecimal => JDecimal(dec)\n+      case int: java.lang.Integer => JInt(BigInt(int))\n+      case long: java.lang.Long => JInt(BigInt(long))\n+      case bool: java.lang.Boolean => JBool(bool)\n+      case map: JMap[_, _] =>\n+        val jmap = map.asInstanceOf[JMap[String, Object]]\n+        JObject(jmap.entrySet().map { e => (e.getKey() -> toJValue(e.getValue())) }.toList)\n+      case array: JCollection[_] =>\n+        JArray(array.asInstanceOf[JCollection[Object]].map(o => toJValue(o)).toList)\n+      case null => JNothing\n+    }\n+  }\n+\n+  /**\n+   * Converts a JValue into its Java equivalent.\n+   */\n+  def toJavaObject(v: JValue): Object = {\n+    v match {\n+      case JNothing => null\n+      case JNull => null\n+      case JString(s) => s\n+      case JDouble(num) => java.lang.Double.valueOf(num)\n+      case JDecimal(num) => num.bigDecimal\n+      case JInt(num) => java.lang.Long.valueOf(num.longValue)\n+      case JBool(value) => java.lang.Boolean.valueOf(value)\n+      case obj: JObject => toJavaMap(obj)\n+      case JArray(vals) => {\n+        val list = new JArrayList[Object]()\n+        vals.foreach(x => list.add(toJavaObject(x)))\n+        list\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Converts a json4s list of fields into a Java Map suitable for serialization by Jackson,\n+   * which is used by the ATS client library.\n+   */\n+  def toJavaMap(sourceObj: JObject): JHashMap[String, Object] = {\n+    val map = new JHashMap[String, Object]()\n+    sourceObj.obj.foreach(f => map.put(f._1, toJavaObject(f._2)))\n+    map\n+  }\n+\n+  /**\n+   * Convert a timeline event to a spark one. Includes\n+   * some basic checks for validity of event payload.\n+   * @param event timeline event\n+   * @return an unmarshalled event\n+   */\n+  def toSparkEvent(event: TimelineEvent): SparkListenerEvent = {\n+    val info = event.getEventInfo()\n+    if (info == null) {\n+      throw new IOException(E_NO_EVENTINFO)\n+    }\n+    if (info.size() == 0) {\n+      throw new IOException(E_EMPTY_EVENTINFO)\n+    }\n+    val payload = toJValue(info)\n+    def jsonToString: String = {\n+      val json = compact(render(payload))\n+      val limit = 256\n+      if (json.length < limit) {\n+        json\n+      } else {\n+          json.substring(0, limit) + \" ... }\"\n+        }\n+    }\n+    logDebug(s\"payload is ${jsonToString}\")\n+    val eventField = payload \\ \"Event\"\n+    if (eventField == JNothing) {\n+      throw new IOException(s\"No 'Event' entry in $jsonToString\")\n+    }\n+\n+    // now the real unmarshalling\n+    try {\n+      JsonProtocol.sparkEventFromJson(payload)\n+    } catch {\n+      // failure in the marshalling; include payload in the message\n+      case ex: MappingException => {\n+        logError(s\"$ex while rendering $jsonToString\", ex)\n+        throw ex\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Convert a spark event to a timeline event\n+   * @param event handled spark event\n+   * @return a timeline event\n+   */\n+  def toTimelineEvent(event: HandleSparkEvent): TimelineEvent = {\n+    val tlEvent = new TimelineEvent()\n+    tlEvent.setEventType(Utils.getFormattedClassName(event.sparkEvent)\n+        + \"-\" + YarnTimelineUtils.uid.incrementAndGet.toString)\n+    tlEvent.setTimestamp(event.time)\n+    val kvMap = new JHashMap[String, Object]()\n+    val json = JsonProtocol.sparkEventToJson(event.sparkEvent)\n+    val jObject = json.asInstanceOf[JObject]\n+    // the timeline event wants a map of java objects for Jackson to serialize\n+    val hashMap = toJavaMap(jObject)\n+    tlEvent.setEventInfo(hashMap)\n+    tlEvent\n+  }\n+\n+  /**\n+   * Describe the event for logging\n+   * @param event timeline event\n+   * @return a description\n+   */\n+  def describeEvent(event: TimelineEvent): String = {\n+    var sparkEventDetails = \"\""
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "replace the whole thing with:\n\n```\nevents.map(decribeEvent).mkString(\"\\n\")\n```\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T00:27:47Z",
    "diffHunk": "@@ -0,0 +1,720 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.io.IOException\n+import java.net.{URI, URL}\n+import java.text.DateFormat\n+import java.util.concurrent.atomic.AtomicLong\n+import java.util.{ArrayList => JArrayList, Collection => JCollection, Date, HashMap => JHashMap, Map => JMap}\n+import java.{lang, util}\n+\n+import scala.collection.JavaConversions._\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.service.Service\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse.TimelinePutError\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+import org.json4s.JsonAST.JObject\n+import org.json4s._\n+import org.json4s.jackson.JsonMethods._\n+\n+import org.apache.spark.{SparkContext, Logging}\n+import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+import org.apache.spark.scheduler.{SparkListenerStageCompleted, SparkListenerStageSubmitted, SparkListenerExecutorAdded, SparkListenerExecutorRemoved, SparkListenerJobEnd, SparkListenerJobStart, SparkListenerApplicationEnd, SparkListenerApplicationStart, SparkListenerEvent}\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+/**\n+ * Utility methods for timeline classes.\n+ */\n+private[spark] object YarnTimelineUtils extends Logging {\n+\n+  /**\n+   * What attempt ID to use as the attempt ID field (not the entity ID) when\n+   * there is no attempt info\n+   */\n+  val SINGLE_ATTEMPT = \"1\"\n+\n+\n+  /**\n+   * Exception text when there is no event info data to unmarshall\n+   */\n+  val E_NO_EVENTINFO = \"No 'eventinfo' entry\"\n+\n+  /**\n+   * Exception text when there is event info entry in the timeline event, but it is empty\n+   */\n+\n+  val E_EMPTY_EVENTINFO = \"Empty 'eventinfo' entry\"\n+\n+  /**\n+   * counter incremented on every spark event to timeline event creation,\n+   * so guaranteeing uniqueness of event IDs across a single application attempt\n+   * (which is implicitly, one per JVM)\n+   */\n+  val uid = new AtomicLong(System.currentTimeMillis())\n+\n+  /**\n+   * Converts a Java object to its equivalent json4s representation.\n+   */\n+  def toJValue(obj: Object): JValue = {\n+    obj match {\n+      case str: String => JString(str)\n+      case dbl: java.lang.Double => JDouble(dbl)\n+      case dec: java.math.BigDecimal => JDecimal(dec)\n+      case int: java.lang.Integer => JInt(BigInt(int))\n+      case long: java.lang.Long => JInt(BigInt(long))\n+      case bool: java.lang.Boolean => JBool(bool)\n+      case map: JMap[_, _] =>\n+        val jmap = map.asInstanceOf[JMap[String, Object]]\n+        JObject(jmap.entrySet().map { e => (e.getKey() -> toJValue(e.getValue())) }.toList)\n+      case array: JCollection[_] =>\n+        JArray(array.asInstanceOf[JCollection[Object]].map(o => toJValue(o)).toList)\n+      case null => JNothing\n+    }\n+  }\n+\n+  /**\n+   * Converts a JValue into its Java equivalent.\n+   */\n+  def toJavaObject(v: JValue): Object = {\n+    v match {\n+      case JNothing => null\n+      case JNull => null\n+      case JString(s) => s\n+      case JDouble(num) => java.lang.Double.valueOf(num)\n+      case JDecimal(num) => num.bigDecimal\n+      case JInt(num) => java.lang.Long.valueOf(num.longValue)\n+      case JBool(value) => java.lang.Boolean.valueOf(value)\n+      case obj: JObject => toJavaMap(obj)\n+      case JArray(vals) => {\n+        val list = new JArrayList[Object]()\n+        vals.foreach(x => list.add(toJavaObject(x)))\n+        list\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Converts a json4s list of fields into a Java Map suitable for serialization by Jackson,\n+   * which is used by the ATS client library.\n+   */\n+  def toJavaMap(sourceObj: JObject): JHashMap[String, Object] = {\n+    val map = new JHashMap[String, Object]()\n+    sourceObj.obj.foreach(f => map.put(f._1, toJavaObject(f._2)))\n+    map\n+  }\n+\n+  /**\n+   * Convert a timeline event to a spark one. Includes\n+   * some basic checks for validity of event payload.\n+   * @param event timeline event\n+   * @return an unmarshalled event\n+   */\n+  def toSparkEvent(event: TimelineEvent): SparkListenerEvent = {\n+    val info = event.getEventInfo()\n+    if (info == null) {\n+      throw new IOException(E_NO_EVENTINFO)\n+    }\n+    if (info.size() == 0) {\n+      throw new IOException(E_EMPTY_EVENTINFO)\n+    }\n+    val payload = toJValue(info)\n+    def jsonToString: String = {\n+      val json = compact(render(payload))\n+      val limit = 256\n+      if (json.length < limit) {\n+        json\n+      } else {\n+          json.substring(0, limit) + \" ... }\"\n+        }\n+    }\n+    logDebug(s\"payload is ${jsonToString}\")\n+    val eventField = payload \\ \"Event\"\n+    if (eventField == JNothing) {\n+      throw new IOException(s\"No 'Event' entry in $jsonToString\")\n+    }\n+\n+    // now the real unmarshalling\n+    try {\n+      JsonProtocol.sparkEventFromJson(payload)\n+    } catch {\n+      // failure in the marshalling; include payload in the message\n+      case ex: MappingException => {\n+        logError(s\"$ex while rendering $jsonToString\", ex)\n+        throw ex\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Convert a spark event to a timeline event\n+   * @param event handled spark event\n+   * @return a timeline event\n+   */\n+  def toTimelineEvent(event: HandleSparkEvent): TimelineEvent = {\n+    val tlEvent = new TimelineEvent()\n+    tlEvent.setEventType(Utils.getFormattedClassName(event.sparkEvent)\n+        + \"-\" + YarnTimelineUtils.uid.incrementAndGet.toString)\n+    tlEvent.setTimestamp(event.time)\n+    val kvMap = new JHashMap[String, Object]()\n+    val json = JsonProtocol.sparkEventToJson(event.sparkEvent)\n+    val jObject = json.asInstanceOf[JObject]\n+    // the timeline event wants a map of java objects for Jackson to serialize\n+    val hashMap = toJavaMap(jObject)\n+    tlEvent.setEventInfo(hashMap)\n+    tlEvent\n+  }\n+\n+  /**\n+   * Describe the event for logging\n+   * @param event timeline event\n+   * @return a description\n+   */\n+  def describeEvent(event: TimelineEvent): String = {\n+    var sparkEventDetails = \"\"\n+    try {\n+      sparkEventDetails = toSparkEvent(event).toString\n+    } catch {\n+      case _: Exception =>\n+        sparkEventDetails = \"(cannot convert event details to spark exception)\"\n+    }\n+    s\"${event.getEventType()} @ ${new Date(event.getTimestamp())}\" +\n+        s\"\\n    ${sparkEventDetails}\"\n+  }\n+\n+  /**\n+   * Create details of a timeline entity, by describing every event inside it\n+   * @param entity entity containing a possibly empty or null list of events\n+   * @return a list of event details, with a newline between each one\n+   */\n+  def eventDetails(entity: TimelineEntity): String = {\n+    val events: util.List[TimelineEvent] = entity.getEvents\n+    if (events != null) {"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "I was getting some IDE warnings about the automatic conversions being deprecated. If they're ok in the source then I'll use them there & in a couple of other places.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-10-21T19:14:19Z",
    "diffHunk": "@@ -0,0 +1,720 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.io.IOException\n+import java.net.{URI, URL}\n+import java.text.DateFormat\n+import java.util.concurrent.atomic.AtomicLong\n+import java.util.{ArrayList => JArrayList, Collection => JCollection, Date, HashMap => JHashMap, Map => JMap}\n+import java.{lang, util}\n+\n+import scala.collection.JavaConversions._\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.service.Service\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse.TimelinePutError\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+import org.json4s.JsonAST.JObject\n+import org.json4s._\n+import org.json4s.jackson.JsonMethods._\n+\n+import org.apache.spark.{SparkContext, Logging}\n+import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+import org.apache.spark.scheduler.{SparkListenerStageCompleted, SparkListenerStageSubmitted, SparkListenerExecutorAdded, SparkListenerExecutorRemoved, SparkListenerJobEnd, SparkListenerJobStart, SparkListenerApplicationEnd, SparkListenerApplicationStart, SparkListenerEvent}\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+/**\n+ * Utility methods for timeline classes.\n+ */\n+private[spark] object YarnTimelineUtils extends Logging {\n+\n+  /**\n+   * What attempt ID to use as the attempt ID field (not the entity ID) when\n+   * there is no attempt info\n+   */\n+  val SINGLE_ATTEMPT = \"1\"\n+\n+\n+  /**\n+   * Exception text when there is no event info data to unmarshall\n+   */\n+  val E_NO_EVENTINFO = \"No 'eventinfo' entry\"\n+\n+  /**\n+   * Exception text when there is event info entry in the timeline event, but it is empty\n+   */\n+\n+  val E_EMPTY_EVENTINFO = \"Empty 'eventinfo' entry\"\n+\n+  /**\n+   * counter incremented on every spark event to timeline event creation,\n+   * so guaranteeing uniqueness of event IDs across a single application attempt\n+   * (which is implicitly, one per JVM)\n+   */\n+  val uid = new AtomicLong(System.currentTimeMillis())\n+\n+  /**\n+   * Converts a Java object to its equivalent json4s representation.\n+   */\n+  def toJValue(obj: Object): JValue = {\n+    obj match {\n+      case str: String => JString(str)\n+      case dbl: java.lang.Double => JDouble(dbl)\n+      case dec: java.math.BigDecimal => JDecimal(dec)\n+      case int: java.lang.Integer => JInt(BigInt(int))\n+      case long: java.lang.Long => JInt(BigInt(long))\n+      case bool: java.lang.Boolean => JBool(bool)\n+      case map: JMap[_, _] =>\n+        val jmap = map.asInstanceOf[JMap[String, Object]]\n+        JObject(jmap.entrySet().map { e => (e.getKey() -> toJValue(e.getValue())) }.toList)\n+      case array: JCollection[_] =>\n+        JArray(array.asInstanceOf[JCollection[Object]].map(o => toJValue(o)).toList)\n+      case null => JNothing\n+    }\n+  }\n+\n+  /**\n+   * Converts a JValue into its Java equivalent.\n+   */\n+  def toJavaObject(v: JValue): Object = {\n+    v match {\n+      case JNothing => null\n+      case JNull => null\n+      case JString(s) => s\n+      case JDouble(num) => java.lang.Double.valueOf(num)\n+      case JDecimal(num) => num.bigDecimal\n+      case JInt(num) => java.lang.Long.valueOf(num.longValue)\n+      case JBool(value) => java.lang.Boolean.valueOf(value)\n+      case obj: JObject => toJavaMap(obj)\n+      case JArray(vals) => {\n+        val list = new JArrayList[Object]()\n+        vals.foreach(x => list.add(toJavaObject(x)))\n+        list\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Converts a json4s list of fields into a Java Map suitable for serialization by Jackson,\n+   * which is used by the ATS client library.\n+   */\n+  def toJavaMap(sourceObj: JObject): JHashMap[String, Object] = {\n+    val map = new JHashMap[String, Object]()\n+    sourceObj.obj.foreach(f => map.put(f._1, toJavaObject(f._2)))\n+    map\n+  }\n+\n+  /**\n+   * Convert a timeline event to a spark one. Includes\n+   * some basic checks for validity of event payload.\n+   * @param event timeline event\n+   * @return an unmarshalled event\n+   */\n+  def toSparkEvent(event: TimelineEvent): SparkListenerEvent = {\n+    val info = event.getEventInfo()\n+    if (info == null) {\n+      throw new IOException(E_NO_EVENTINFO)\n+    }\n+    if (info.size() == 0) {\n+      throw new IOException(E_EMPTY_EVENTINFO)\n+    }\n+    val payload = toJValue(info)\n+    def jsonToString: String = {\n+      val json = compact(render(payload))\n+      val limit = 256\n+      if (json.length < limit) {\n+        json\n+      } else {\n+          json.substring(0, limit) + \" ... }\"\n+        }\n+    }\n+    logDebug(s\"payload is ${jsonToString}\")\n+    val eventField = payload \\ \"Event\"\n+    if (eventField == JNothing) {\n+      throw new IOException(s\"No 'Event' entry in $jsonToString\")\n+    }\n+\n+    // now the real unmarshalling\n+    try {\n+      JsonProtocol.sparkEventFromJson(payload)\n+    } catch {\n+      // failure in the marshalling; include payload in the message\n+      case ex: MappingException => {\n+        logError(s\"$ex while rendering $jsonToString\", ex)\n+        throw ex\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Convert a spark event to a timeline event\n+   * @param event handled spark event\n+   * @return a timeline event\n+   */\n+  def toTimelineEvent(event: HandleSparkEvent): TimelineEvent = {\n+    val tlEvent = new TimelineEvent()\n+    tlEvent.setEventType(Utils.getFormattedClassName(event.sparkEvent)\n+        + \"-\" + YarnTimelineUtils.uid.incrementAndGet.toString)\n+    tlEvent.setTimestamp(event.time)\n+    val kvMap = new JHashMap[String, Object]()\n+    val json = JsonProtocol.sparkEventToJson(event.sparkEvent)\n+    val jObject = json.asInstanceOf[JObject]\n+    // the timeline event wants a map of java objects for Jackson to serialize\n+    val hashMap = toJavaMap(jObject)\n+    tlEvent.setEventInfo(hashMap)\n+    tlEvent\n+  }\n+\n+  /**\n+   * Describe the event for logging\n+   * @param event timeline event\n+   * @return a description\n+   */\n+  def describeEvent(event: TimelineEvent): String = {\n+    var sparkEventDetails = \"\"\n+    try {\n+      sparkEventDetails = toSparkEvent(event).toString\n+    } catch {\n+      case _: Exception =>\n+        sparkEventDetails = \"(cannot convert event details to spark exception)\"\n+    }\n+    s\"${event.getEventType()} @ ${new Date(event.getTimestamp())}\" +\n+        s\"\\n    ${sparkEventDetails}\"\n+  }\n+\n+  /**\n+   * Create details of a timeline entity, by describing every event inside it\n+   * @param entity entity containing a possibly empty or null list of events\n+   * @return a list of event details, with a newline between each one\n+   */\n+  def eventDetails(entity: TimelineEntity): String = {\n+    val events: util.List[TimelineEvent] = entity.getEvents\n+    if (events != null) {"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "scalastyle rejects the code unless I have the .asScala logic\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-10-22T18:16:49Z",
    "diffHunk": "@@ -0,0 +1,720 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.io.IOException\n+import java.net.{URI, URL}\n+import java.text.DateFormat\n+import java.util.concurrent.atomic.AtomicLong\n+import java.util.{ArrayList => JArrayList, Collection => JCollection, Date, HashMap => JHashMap, Map => JMap}\n+import java.{lang, util}\n+\n+import scala.collection.JavaConversions._\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.service.Service\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse.TimelinePutError\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+import org.json4s.JsonAST.JObject\n+import org.json4s._\n+import org.json4s.jackson.JsonMethods._\n+\n+import org.apache.spark.{SparkContext, Logging}\n+import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+import org.apache.spark.scheduler.{SparkListenerStageCompleted, SparkListenerStageSubmitted, SparkListenerExecutorAdded, SparkListenerExecutorRemoved, SparkListenerJobEnd, SparkListenerJobStart, SparkListenerApplicationEnd, SparkListenerApplicationStart, SparkListenerEvent}\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+/**\n+ * Utility methods for timeline classes.\n+ */\n+private[spark] object YarnTimelineUtils extends Logging {\n+\n+  /**\n+   * What attempt ID to use as the attempt ID field (not the entity ID) when\n+   * there is no attempt info\n+   */\n+  val SINGLE_ATTEMPT = \"1\"\n+\n+\n+  /**\n+   * Exception text when there is no event info data to unmarshall\n+   */\n+  val E_NO_EVENTINFO = \"No 'eventinfo' entry\"\n+\n+  /**\n+   * Exception text when there is event info entry in the timeline event, but it is empty\n+   */\n+\n+  val E_EMPTY_EVENTINFO = \"Empty 'eventinfo' entry\"\n+\n+  /**\n+   * counter incremented on every spark event to timeline event creation,\n+   * so guaranteeing uniqueness of event IDs across a single application attempt\n+   * (which is implicitly, one per JVM)\n+   */\n+  val uid = new AtomicLong(System.currentTimeMillis())\n+\n+  /**\n+   * Converts a Java object to its equivalent json4s representation.\n+   */\n+  def toJValue(obj: Object): JValue = {\n+    obj match {\n+      case str: String => JString(str)\n+      case dbl: java.lang.Double => JDouble(dbl)\n+      case dec: java.math.BigDecimal => JDecimal(dec)\n+      case int: java.lang.Integer => JInt(BigInt(int))\n+      case long: java.lang.Long => JInt(BigInt(long))\n+      case bool: java.lang.Boolean => JBool(bool)\n+      case map: JMap[_, _] =>\n+        val jmap = map.asInstanceOf[JMap[String, Object]]\n+        JObject(jmap.entrySet().map { e => (e.getKey() -> toJValue(e.getValue())) }.toList)\n+      case array: JCollection[_] =>\n+        JArray(array.asInstanceOf[JCollection[Object]].map(o => toJValue(o)).toList)\n+      case null => JNothing\n+    }\n+  }\n+\n+  /**\n+   * Converts a JValue into its Java equivalent.\n+   */\n+  def toJavaObject(v: JValue): Object = {\n+    v match {\n+      case JNothing => null\n+      case JNull => null\n+      case JString(s) => s\n+      case JDouble(num) => java.lang.Double.valueOf(num)\n+      case JDecimal(num) => num.bigDecimal\n+      case JInt(num) => java.lang.Long.valueOf(num.longValue)\n+      case JBool(value) => java.lang.Boolean.valueOf(value)\n+      case obj: JObject => toJavaMap(obj)\n+      case JArray(vals) => {\n+        val list = new JArrayList[Object]()\n+        vals.foreach(x => list.add(toJavaObject(x)))\n+        list\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Converts a json4s list of fields into a Java Map suitable for serialization by Jackson,\n+   * which is used by the ATS client library.\n+   */\n+  def toJavaMap(sourceObj: JObject): JHashMap[String, Object] = {\n+    val map = new JHashMap[String, Object]()\n+    sourceObj.obj.foreach(f => map.put(f._1, toJavaObject(f._2)))\n+    map\n+  }\n+\n+  /**\n+   * Convert a timeline event to a spark one. Includes\n+   * some basic checks for validity of event payload.\n+   * @param event timeline event\n+   * @return an unmarshalled event\n+   */\n+  def toSparkEvent(event: TimelineEvent): SparkListenerEvent = {\n+    val info = event.getEventInfo()\n+    if (info == null) {\n+      throw new IOException(E_NO_EVENTINFO)\n+    }\n+    if (info.size() == 0) {\n+      throw new IOException(E_EMPTY_EVENTINFO)\n+    }\n+    val payload = toJValue(info)\n+    def jsonToString: String = {\n+      val json = compact(render(payload))\n+      val limit = 256\n+      if (json.length < limit) {\n+        json\n+      } else {\n+          json.substring(0, limit) + \" ... }\"\n+        }\n+    }\n+    logDebug(s\"payload is ${jsonToString}\")\n+    val eventField = payload \\ \"Event\"\n+    if (eventField == JNothing) {\n+      throw new IOException(s\"No 'Event' entry in $jsonToString\")\n+    }\n+\n+    // now the real unmarshalling\n+    try {\n+      JsonProtocol.sparkEventFromJson(payload)\n+    } catch {\n+      // failure in the marshalling; include payload in the message\n+      case ex: MappingException => {\n+        logError(s\"$ex while rendering $jsonToString\", ex)\n+        throw ex\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Convert a spark event to a timeline event\n+   * @param event handled spark event\n+   * @return a timeline event\n+   */\n+  def toTimelineEvent(event: HandleSparkEvent): TimelineEvent = {\n+    val tlEvent = new TimelineEvent()\n+    tlEvent.setEventType(Utils.getFormattedClassName(event.sparkEvent)\n+        + \"-\" + YarnTimelineUtils.uid.incrementAndGet.toString)\n+    tlEvent.setTimestamp(event.time)\n+    val kvMap = new JHashMap[String, Object]()\n+    val json = JsonProtocol.sparkEventToJson(event.sparkEvent)\n+    val jObject = json.asInstanceOf[JObject]\n+    // the timeline event wants a map of java objects for Jackson to serialize\n+    val hashMap = toJavaMap(jObject)\n+    tlEvent.setEventInfo(hashMap)\n+    tlEvent\n+  }\n+\n+  /**\n+   * Describe the event for logging\n+   * @param event timeline event\n+   * @return a description\n+   */\n+  def describeEvent(event: TimelineEvent): String = {\n+    var sparkEventDetails = \"\"\n+    try {\n+      sparkEventDetails = toSparkEvent(event).toString\n+    } catch {\n+      case _: Exception =>\n+        sparkEventDetails = \"(cannot convert event details to spark exception)\"\n+    }\n+    s\"${event.getEventType()} @ ${new Date(event.getTimestamp())}\" +\n+        s\"\\n    ${sparkEventDetails}\"\n+  }\n+\n+  /**\n+   * Create details of a timeline entity, by describing every event inside it\n+   * @param entity entity containing a possibly empty or null list of events\n+   * @return a list of event details, with a newline between each one\n+   */\n+  def eventDetails(entity: TimelineEntity): String = {\n+    val events: util.List[TimelineEvent] = entity.getEvents\n+    if (events != null) {"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`map` + `mkString` should also work here and is easier to read.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T00:29:23Z",
    "diffHunk": "@@ -0,0 +1,720 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.io.IOException\n+import java.net.{URI, URL}\n+import java.text.DateFormat\n+import java.util.concurrent.atomic.AtomicLong\n+import java.util.{ArrayList => JArrayList, Collection => JCollection, Date, HashMap => JHashMap, Map => JMap}\n+import java.{lang, util}\n+\n+import scala.collection.JavaConversions._\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.service.Service\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse.TimelinePutError\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+import org.json4s.JsonAST.JObject\n+import org.json4s._\n+import org.json4s.jackson.JsonMethods._\n+\n+import org.apache.spark.{SparkContext, Logging}\n+import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+import org.apache.spark.scheduler.{SparkListenerStageCompleted, SparkListenerStageSubmitted, SparkListenerExecutorAdded, SparkListenerExecutorRemoved, SparkListenerJobEnd, SparkListenerJobStart, SparkListenerApplicationEnd, SparkListenerApplicationStart, SparkListenerEvent}\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+/**\n+ * Utility methods for timeline classes.\n+ */\n+private[spark] object YarnTimelineUtils extends Logging {\n+\n+  /**\n+   * What attempt ID to use as the attempt ID field (not the entity ID) when\n+   * there is no attempt info\n+   */\n+  val SINGLE_ATTEMPT = \"1\"\n+\n+\n+  /**\n+   * Exception text when there is no event info data to unmarshall\n+   */\n+  val E_NO_EVENTINFO = \"No 'eventinfo' entry\"\n+\n+  /**\n+   * Exception text when there is event info entry in the timeline event, but it is empty\n+   */\n+\n+  val E_EMPTY_EVENTINFO = \"Empty 'eventinfo' entry\"\n+\n+  /**\n+   * counter incremented on every spark event to timeline event creation,\n+   * so guaranteeing uniqueness of event IDs across a single application attempt\n+   * (which is implicitly, one per JVM)\n+   */\n+  val uid = new AtomicLong(System.currentTimeMillis())\n+\n+  /**\n+   * Converts a Java object to its equivalent json4s representation.\n+   */\n+  def toJValue(obj: Object): JValue = {\n+    obj match {\n+      case str: String => JString(str)\n+      case dbl: java.lang.Double => JDouble(dbl)\n+      case dec: java.math.BigDecimal => JDecimal(dec)\n+      case int: java.lang.Integer => JInt(BigInt(int))\n+      case long: java.lang.Long => JInt(BigInt(long))\n+      case bool: java.lang.Boolean => JBool(bool)\n+      case map: JMap[_, _] =>\n+        val jmap = map.asInstanceOf[JMap[String, Object]]\n+        JObject(jmap.entrySet().map { e => (e.getKey() -> toJValue(e.getValue())) }.toList)\n+      case array: JCollection[_] =>\n+        JArray(array.asInstanceOf[JCollection[Object]].map(o => toJValue(o)).toList)\n+      case null => JNothing\n+    }\n+  }\n+\n+  /**\n+   * Converts a JValue into its Java equivalent.\n+   */\n+  def toJavaObject(v: JValue): Object = {\n+    v match {\n+      case JNothing => null\n+      case JNull => null\n+      case JString(s) => s\n+      case JDouble(num) => java.lang.Double.valueOf(num)\n+      case JDecimal(num) => num.bigDecimal\n+      case JInt(num) => java.lang.Long.valueOf(num.longValue)\n+      case JBool(value) => java.lang.Boolean.valueOf(value)\n+      case obj: JObject => toJavaMap(obj)\n+      case JArray(vals) => {\n+        val list = new JArrayList[Object]()\n+        vals.foreach(x => list.add(toJavaObject(x)))\n+        list\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Converts a json4s list of fields into a Java Map suitable for serialization by Jackson,\n+   * which is used by the ATS client library.\n+   */\n+  def toJavaMap(sourceObj: JObject): JHashMap[String, Object] = {\n+    val map = new JHashMap[String, Object]()\n+    sourceObj.obj.foreach(f => map.put(f._1, toJavaObject(f._2)))\n+    map\n+  }\n+\n+  /**\n+   * Convert a timeline event to a spark one. Includes\n+   * some basic checks for validity of event payload.\n+   * @param event timeline event\n+   * @return an unmarshalled event\n+   */\n+  def toSparkEvent(event: TimelineEvent): SparkListenerEvent = {\n+    val info = event.getEventInfo()\n+    if (info == null) {\n+      throw new IOException(E_NO_EVENTINFO)\n+    }\n+    if (info.size() == 0) {\n+      throw new IOException(E_EMPTY_EVENTINFO)\n+    }\n+    val payload = toJValue(info)\n+    def jsonToString: String = {\n+      val json = compact(render(payload))\n+      val limit = 256\n+      if (json.length < limit) {\n+        json\n+      } else {\n+          json.substring(0, limit) + \" ... }\"\n+        }\n+    }\n+    logDebug(s\"payload is ${jsonToString}\")\n+    val eventField = payload \\ \"Event\"\n+    if (eventField == JNothing) {\n+      throw new IOException(s\"No 'Event' entry in $jsonToString\")\n+    }\n+\n+    // now the real unmarshalling\n+    try {\n+      JsonProtocol.sparkEventFromJson(payload)\n+    } catch {\n+      // failure in the marshalling; include payload in the message\n+      case ex: MappingException => {\n+        logError(s\"$ex while rendering $jsonToString\", ex)\n+        throw ex\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Convert a spark event to a timeline event\n+   * @param event handled spark event\n+   * @return a timeline event\n+   */\n+  def toTimelineEvent(event: HandleSparkEvent): TimelineEvent = {\n+    val tlEvent = new TimelineEvent()\n+    tlEvent.setEventType(Utils.getFormattedClassName(event.sparkEvent)\n+        + \"-\" + YarnTimelineUtils.uid.incrementAndGet.toString)\n+    tlEvent.setTimestamp(event.time)\n+    val kvMap = new JHashMap[String, Object]()\n+    val json = JsonProtocol.sparkEventToJson(event.sparkEvent)\n+    val jObject = json.asInstanceOf[JObject]\n+    // the timeline event wants a map of java objects for Jackson to serialize\n+    val hashMap = toJavaMap(jObject)\n+    tlEvent.setEventInfo(hashMap)\n+    tlEvent\n+  }\n+\n+  /**\n+   * Describe the event for logging\n+   * @param event timeline event\n+   * @return a description\n+   */\n+  def describeEvent(event: TimelineEvent): String = {\n+    var sparkEventDetails = \"\"\n+    try {\n+      sparkEventDetails = toSparkEvent(event).toString\n+    } catch {\n+      case _: Exception =>\n+        sparkEventDetails = \"(cannot convert event details to spark exception)\"\n+    }\n+    s\"${event.getEventType()} @ ${new Date(event.getTimestamp())}\" +\n+        s\"\\n    ${sparkEventDetails}\"\n+  }\n+\n+  /**\n+   * Create details of a timeline entity, by describing every event inside it\n+   * @param entity entity containing a possibly empty or null list of events\n+   * @return a list of event details, with a newline between each one\n+   */\n+  def eventDetails(entity: TimelineEntity): String = {\n+    val events: util.List[TimelineEvent] = entity.getEvents\n+    if (events != null) {\n+      events.foldLeft(\"\") {\n+        (s, evt) => (s + \"\\n\" + describeEvent(evt))\n+      }\n+    } else {\n+      \"\"\n+    }\n+  }\n+\n+  /**\n+   * Describe a timeline entity\n+   * @param entity entity\n+   * @return a string description.\n+   */\n+  def describeEntity(entity: TimelineEntity): String = {\n+    val events: util.List[TimelineEvent] = entity.getEvents\n+    val eventSummary = if (events != null) {\n+      s\"contains ${events.size()} event(s)\"\n+    } else {\n+      \"contains no events\"\n+    }\n+\n+    val header = s\"${entity.getEntityType }/${entity.getEntityId } @${entity.getDomainId }}\"\n+    val otherInfo = entity.getOtherInfo().foldLeft(\"\\n\") ("
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "- indentation is off\n- `s\"no $field time\"`\n\nThe final string reads a little weirdly, too.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T00:30:57Z",
    "diffHunk": "@@ -0,0 +1,720 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.io.IOException\n+import java.net.{URI, URL}\n+import java.text.DateFormat\n+import java.util.concurrent.atomic.AtomicLong\n+import java.util.{ArrayList => JArrayList, Collection => JCollection, Date, HashMap => JHashMap, Map => JMap}\n+import java.{lang, util}\n+\n+import scala.collection.JavaConversions._\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.service.Service\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse.TimelinePutError\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+import org.json4s.JsonAST.JObject\n+import org.json4s._\n+import org.json4s.jackson.JsonMethods._\n+\n+import org.apache.spark.{SparkContext, Logging}\n+import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+import org.apache.spark.scheduler.{SparkListenerStageCompleted, SparkListenerStageSubmitted, SparkListenerExecutorAdded, SparkListenerExecutorRemoved, SparkListenerJobEnd, SparkListenerJobStart, SparkListenerApplicationEnd, SparkListenerApplicationStart, SparkListenerEvent}\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+/**\n+ * Utility methods for timeline classes.\n+ */\n+private[spark] object YarnTimelineUtils extends Logging {\n+\n+  /**\n+   * What attempt ID to use as the attempt ID field (not the entity ID) when\n+   * there is no attempt info\n+   */\n+  val SINGLE_ATTEMPT = \"1\"\n+\n+\n+  /**\n+   * Exception text when there is no event info data to unmarshall\n+   */\n+  val E_NO_EVENTINFO = \"No 'eventinfo' entry\"\n+\n+  /**\n+   * Exception text when there is event info entry in the timeline event, but it is empty\n+   */\n+\n+  val E_EMPTY_EVENTINFO = \"Empty 'eventinfo' entry\"\n+\n+  /**\n+   * counter incremented on every spark event to timeline event creation,\n+   * so guaranteeing uniqueness of event IDs across a single application attempt\n+   * (which is implicitly, one per JVM)\n+   */\n+  val uid = new AtomicLong(System.currentTimeMillis())\n+\n+  /**\n+   * Converts a Java object to its equivalent json4s representation.\n+   */\n+  def toJValue(obj: Object): JValue = {\n+    obj match {\n+      case str: String => JString(str)\n+      case dbl: java.lang.Double => JDouble(dbl)\n+      case dec: java.math.BigDecimal => JDecimal(dec)\n+      case int: java.lang.Integer => JInt(BigInt(int))\n+      case long: java.lang.Long => JInt(BigInt(long))\n+      case bool: java.lang.Boolean => JBool(bool)\n+      case map: JMap[_, _] =>\n+        val jmap = map.asInstanceOf[JMap[String, Object]]\n+        JObject(jmap.entrySet().map { e => (e.getKey() -> toJValue(e.getValue())) }.toList)\n+      case array: JCollection[_] =>\n+        JArray(array.asInstanceOf[JCollection[Object]].map(o => toJValue(o)).toList)\n+      case null => JNothing\n+    }\n+  }\n+\n+  /**\n+   * Converts a JValue into its Java equivalent.\n+   */\n+  def toJavaObject(v: JValue): Object = {\n+    v match {\n+      case JNothing => null\n+      case JNull => null\n+      case JString(s) => s\n+      case JDouble(num) => java.lang.Double.valueOf(num)\n+      case JDecimal(num) => num.bigDecimal\n+      case JInt(num) => java.lang.Long.valueOf(num.longValue)\n+      case JBool(value) => java.lang.Boolean.valueOf(value)\n+      case obj: JObject => toJavaMap(obj)\n+      case JArray(vals) => {\n+        val list = new JArrayList[Object]()\n+        vals.foreach(x => list.add(toJavaObject(x)))\n+        list\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Converts a json4s list of fields into a Java Map suitable for serialization by Jackson,\n+   * which is used by the ATS client library.\n+   */\n+  def toJavaMap(sourceObj: JObject): JHashMap[String, Object] = {\n+    val map = new JHashMap[String, Object]()\n+    sourceObj.obj.foreach(f => map.put(f._1, toJavaObject(f._2)))\n+    map\n+  }\n+\n+  /**\n+   * Convert a timeline event to a spark one. Includes\n+   * some basic checks for validity of event payload.\n+   * @param event timeline event\n+   * @return an unmarshalled event\n+   */\n+  def toSparkEvent(event: TimelineEvent): SparkListenerEvent = {\n+    val info = event.getEventInfo()\n+    if (info == null) {\n+      throw new IOException(E_NO_EVENTINFO)\n+    }\n+    if (info.size() == 0) {\n+      throw new IOException(E_EMPTY_EVENTINFO)\n+    }\n+    val payload = toJValue(info)\n+    def jsonToString: String = {\n+      val json = compact(render(payload))\n+      val limit = 256\n+      if (json.length < limit) {\n+        json\n+      } else {\n+          json.substring(0, limit) + \" ... }\"\n+        }\n+    }\n+    logDebug(s\"payload is ${jsonToString}\")\n+    val eventField = payload \\ \"Event\"\n+    if (eventField == JNothing) {\n+      throw new IOException(s\"No 'Event' entry in $jsonToString\")\n+    }\n+\n+    // now the real unmarshalling\n+    try {\n+      JsonProtocol.sparkEventFromJson(payload)\n+    } catch {\n+      // failure in the marshalling; include payload in the message\n+      case ex: MappingException => {\n+        logError(s\"$ex while rendering $jsonToString\", ex)\n+        throw ex\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Convert a spark event to a timeline event\n+   * @param event handled spark event\n+   * @return a timeline event\n+   */\n+  def toTimelineEvent(event: HandleSparkEvent): TimelineEvent = {\n+    val tlEvent = new TimelineEvent()\n+    tlEvent.setEventType(Utils.getFormattedClassName(event.sparkEvent)\n+        + \"-\" + YarnTimelineUtils.uid.incrementAndGet.toString)\n+    tlEvent.setTimestamp(event.time)\n+    val kvMap = new JHashMap[String, Object]()\n+    val json = JsonProtocol.sparkEventToJson(event.sparkEvent)\n+    val jObject = json.asInstanceOf[JObject]\n+    // the timeline event wants a map of java objects for Jackson to serialize\n+    val hashMap = toJavaMap(jObject)\n+    tlEvent.setEventInfo(hashMap)\n+    tlEvent\n+  }\n+\n+  /**\n+   * Describe the event for logging\n+   * @param event timeline event\n+   * @return a description\n+   */\n+  def describeEvent(event: TimelineEvent): String = {\n+    var sparkEventDetails = \"\"\n+    try {\n+      sparkEventDetails = toSparkEvent(event).toString\n+    } catch {\n+      case _: Exception =>\n+        sparkEventDetails = \"(cannot convert event details to spark exception)\"\n+    }\n+    s\"${event.getEventType()} @ ${new Date(event.getTimestamp())}\" +\n+        s\"\\n    ${sparkEventDetails}\"\n+  }\n+\n+  /**\n+   * Create details of a timeline entity, by describing every event inside it\n+   * @param entity entity containing a possibly empty or null list of events\n+   * @return a list of event details, with a newline between each one\n+   */\n+  def eventDetails(entity: TimelineEntity): String = {\n+    val events: util.List[TimelineEvent] = entity.getEvents\n+    if (events != null) {\n+      events.foldLeft(\"\") {\n+        (s, evt) => (s + \"\\n\" + describeEvent(evt))\n+      }\n+    } else {\n+      \"\"\n+    }\n+  }\n+\n+  /**\n+   * Describe a timeline entity\n+   * @param entity entity\n+   * @return a string description.\n+   */\n+  def describeEntity(entity: TimelineEntity): String = {\n+    val events: util.List[TimelineEvent] = entity.getEvents\n+    val eventSummary = if (events != null) {\n+      s\"contains ${events.size()} event(s)\"\n+    } else {\n+      \"contains no events\"\n+    }\n+\n+    val header = s\"${entity.getEntityType }/${entity.getEntityId } @${entity.getDomainId }}\"\n+    val otherInfo = entity.getOtherInfo().foldLeft(\"\\n\") (\n+      (acc, kv) => acc + s\" ${kv._1} = ${kv._2}; \"\n+    )\n+    s\"Timeline Entity \" + header +\n+        \" \" + otherInfo +\n+        \" \" + timeFieldToString(entity.getStartTime(), \"start\") +\n+        \" \" + eventSummary\n+  }\n+\n+  /**\n+   * Convert a `java.lang.Long` reference to a string value, or, if the reference is null,\n+   * to text declaring that the named field is empty\n+   * @param time time reference\n+   * @param field field name for error message\n+   * @return a string to describe the field\n+   */\n+  def timeFieldToString(time: lang.Long, field: String): String = {\n+    if (time != null) {\n+      new Date(time).toString\n+    } else {\n+       (\"no \" + field + \" time\")"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "it gets used in the example `timeFieldToString(entity.getStartTime(), \"start\")` so isn't so bad in use\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T17:21:05Z",
    "diffHunk": "@@ -0,0 +1,720 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.io.IOException\n+import java.net.{URI, URL}\n+import java.text.DateFormat\n+import java.util.concurrent.atomic.AtomicLong\n+import java.util.{ArrayList => JArrayList, Collection => JCollection, Date, HashMap => JHashMap, Map => JMap}\n+import java.{lang, util}\n+\n+import scala.collection.JavaConversions._\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.service.Service\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse.TimelinePutError\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+import org.json4s.JsonAST.JObject\n+import org.json4s._\n+import org.json4s.jackson.JsonMethods._\n+\n+import org.apache.spark.{SparkContext, Logging}\n+import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+import org.apache.spark.scheduler.{SparkListenerStageCompleted, SparkListenerStageSubmitted, SparkListenerExecutorAdded, SparkListenerExecutorRemoved, SparkListenerJobEnd, SparkListenerJobStart, SparkListenerApplicationEnd, SparkListenerApplicationStart, SparkListenerEvent}\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+/**\n+ * Utility methods for timeline classes.\n+ */\n+private[spark] object YarnTimelineUtils extends Logging {\n+\n+  /**\n+   * What attempt ID to use as the attempt ID field (not the entity ID) when\n+   * there is no attempt info\n+   */\n+  val SINGLE_ATTEMPT = \"1\"\n+\n+\n+  /**\n+   * Exception text when there is no event info data to unmarshall\n+   */\n+  val E_NO_EVENTINFO = \"No 'eventinfo' entry\"\n+\n+  /**\n+   * Exception text when there is event info entry in the timeline event, but it is empty\n+   */\n+\n+  val E_EMPTY_EVENTINFO = \"Empty 'eventinfo' entry\"\n+\n+  /**\n+   * counter incremented on every spark event to timeline event creation,\n+   * so guaranteeing uniqueness of event IDs across a single application attempt\n+   * (which is implicitly, one per JVM)\n+   */\n+  val uid = new AtomicLong(System.currentTimeMillis())\n+\n+  /**\n+   * Converts a Java object to its equivalent json4s representation.\n+   */\n+  def toJValue(obj: Object): JValue = {\n+    obj match {\n+      case str: String => JString(str)\n+      case dbl: java.lang.Double => JDouble(dbl)\n+      case dec: java.math.BigDecimal => JDecimal(dec)\n+      case int: java.lang.Integer => JInt(BigInt(int))\n+      case long: java.lang.Long => JInt(BigInt(long))\n+      case bool: java.lang.Boolean => JBool(bool)\n+      case map: JMap[_, _] =>\n+        val jmap = map.asInstanceOf[JMap[String, Object]]\n+        JObject(jmap.entrySet().map { e => (e.getKey() -> toJValue(e.getValue())) }.toList)\n+      case array: JCollection[_] =>\n+        JArray(array.asInstanceOf[JCollection[Object]].map(o => toJValue(o)).toList)\n+      case null => JNothing\n+    }\n+  }\n+\n+  /**\n+   * Converts a JValue into its Java equivalent.\n+   */\n+  def toJavaObject(v: JValue): Object = {\n+    v match {\n+      case JNothing => null\n+      case JNull => null\n+      case JString(s) => s\n+      case JDouble(num) => java.lang.Double.valueOf(num)\n+      case JDecimal(num) => num.bigDecimal\n+      case JInt(num) => java.lang.Long.valueOf(num.longValue)\n+      case JBool(value) => java.lang.Boolean.valueOf(value)\n+      case obj: JObject => toJavaMap(obj)\n+      case JArray(vals) => {\n+        val list = new JArrayList[Object]()\n+        vals.foreach(x => list.add(toJavaObject(x)))\n+        list\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Converts a json4s list of fields into a Java Map suitable for serialization by Jackson,\n+   * which is used by the ATS client library.\n+   */\n+  def toJavaMap(sourceObj: JObject): JHashMap[String, Object] = {\n+    val map = new JHashMap[String, Object]()\n+    sourceObj.obj.foreach(f => map.put(f._1, toJavaObject(f._2)))\n+    map\n+  }\n+\n+  /**\n+   * Convert a timeline event to a spark one. Includes\n+   * some basic checks for validity of event payload.\n+   * @param event timeline event\n+   * @return an unmarshalled event\n+   */\n+  def toSparkEvent(event: TimelineEvent): SparkListenerEvent = {\n+    val info = event.getEventInfo()\n+    if (info == null) {\n+      throw new IOException(E_NO_EVENTINFO)\n+    }\n+    if (info.size() == 0) {\n+      throw new IOException(E_EMPTY_EVENTINFO)\n+    }\n+    val payload = toJValue(info)\n+    def jsonToString: String = {\n+      val json = compact(render(payload))\n+      val limit = 256\n+      if (json.length < limit) {\n+        json\n+      } else {\n+          json.substring(0, limit) + \" ... }\"\n+        }\n+    }\n+    logDebug(s\"payload is ${jsonToString}\")\n+    val eventField = payload \\ \"Event\"\n+    if (eventField == JNothing) {\n+      throw new IOException(s\"No 'Event' entry in $jsonToString\")\n+    }\n+\n+    // now the real unmarshalling\n+    try {\n+      JsonProtocol.sparkEventFromJson(payload)\n+    } catch {\n+      // failure in the marshalling; include payload in the message\n+      case ex: MappingException => {\n+        logError(s\"$ex while rendering $jsonToString\", ex)\n+        throw ex\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Convert a spark event to a timeline event\n+   * @param event handled spark event\n+   * @return a timeline event\n+   */\n+  def toTimelineEvent(event: HandleSparkEvent): TimelineEvent = {\n+    val tlEvent = new TimelineEvent()\n+    tlEvent.setEventType(Utils.getFormattedClassName(event.sparkEvent)\n+        + \"-\" + YarnTimelineUtils.uid.incrementAndGet.toString)\n+    tlEvent.setTimestamp(event.time)\n+    val kvMap = new JHashMap[String, Object]()\n+    val json = JsonProtocol.sparkEventToJson(event.sparkEvent)\n+    val jObject = json.asInstanceOf[JObject]\n+    // the timeline event wants a map of java objects for Jackson to serialize\n+    val hashMap = toJavaMap(jObject)\n+    tlEvent.setEventInfo(hashMap)\n+    tlEvent\n+  }\n+\n+  /**\n+   * Describe the event for logging\n+   * @param event timeline event\n+   * @return a description\n+   */\n+  def describeEvent(event: TimelineEvent): String = {\n+    var sparkEventDetails = \"\"\n+    try {\n+      sparkEventDetails = toSparkEvent(event).toString\n+    } catch {\n+      case _: Exception =>\n+        sparkEventDetails = \"(cannot convert event details to spark exception)\"\n+    }\n+    s\"${event.getEventType()} @ ${new Date(event.getTimestamp())}\" +\n+        s\"\\n    ${sparkEventDetails}\"\n+  }\n+\n+  /**\n+   * Create details of a timeline entity, by describing every event inside it\n+   * @param entity entity containing a possibly empty or null list of events\n+   * @return a list of event details, with a newline between each one\n+   */\n+  def eventDetails(entity: TimelineEntity): String = {\n+    val events: util.List[TimelineEvent] = entity.getEvents\n+    if (events != null) {\n+      events.foldLeft(\"\") {\n+        (s, evt) => (s + \"\\n\" + describeEvent(evt))\n+      }\n+    } else {\n+      \"\"\n+    }\n+  }\n+\n+  /**\n+   * Describe a timeline entity\n+   * @param entity entity\n+   * @return a string description.\n+   */\n+  def describeEntity(entity: TimelineEntity): String = {\n+    val events: util.List[TimelineEvent] = entity.getEvents\n+    val eventSummary = if (events != null) {\n+      s\"contains ${events.size()} event(s)\"\n+    } else {\n+      \"contains no events\"\n+    }\n+\n+    val header = s\"${entity.getEntityType }/${entity.getEntityId } @${entity.getDomainId }}\"\n+    val otherInfo = entity.getOtherInfo().foldLeft(\"\\n\") (\n+      (acc, kv) => acc + s\" ${kv._1} = ${kv._2}; \"\n+    )\n+    s\"Timeline Entity \" + header +\n+        \" \" + otherInfo +\n+        \" \" + timeFieldToString(entity.getStartTime(), \"start\") +\n+        \" \" + eventSummary\n+  }\n+\n+  /**\n+   * Convert a `java.lang.Long` reference to a string value, or, if the reference is null,\n+   * to text declaring that the named field is empty\n+   * @param time time reference\n+   * @param field field name for error message\n+   * @return a string to describe the field\n+   */\n+  def timeFieldToString(time: lang.Long, field: String): String = {\n+    if (time != null) {\n+      new Date(time).toString\n+    } else {\n+       (\"no \" + field + \" time\")"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Are all these methods really useful? They're really noisy and the code in them is kinda hard to read. Using more common methods like `mkString` would help, but I'm starting to question whether they're really helping at all.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T00:32:21Z",
    "diffHunk": "@@ -0,0 +1,720 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.io.IOException\n+import java.net.{URI, URL}\n+import java.text.DateFormat\n+import java.util.concurrent.atomic.AtomicLong\n+import java.util.{ArrayList => JArrayList, Collection => JCollection, Date, HashMap => JHashMap, Map => JMap}\n+import java.{lang, util}\n+\n+import scala.collection.JavaConversions._\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.service.Service\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse.TimelinePutError\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+import org.json4s.JsonAST.JObject\n+import org.json4s._\n+import org.json4s.jackson.JsonMethods._\n+\n+import org.apache.spark.{SparkContext, Logging}\n+import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+import org.apache.spark.scheduler.{SparkListenerStageCompleted, SparkListenerStageSubmitted, SparkListenerExecutorAdded, SparkListenerExecutorRemoved, SparkListenerJobEnd, SparkListenerJobStart, SparkListenerApplicationEnd, SparkListenerApplicationStart, SparkListenerEvent}\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+/**\n+ * Utility methods for timeline classes.\n+ */\n+private[spark] object YarnTimelineUtils extends Logging {\n+\n+  /**\n+   * What attempt ID to use as the attempt ID field (not the entity ID) when\n+   * there is no attempt info\n+   */\n+  val SINGLE_ATTEMPT = \"1\"\n+\n+\n+  /**\n+   * Exception text when there is no event info data to unmarshall\n+   */\n+  val E_NO_EVENTINFO = \"No 'eventinfo' entry\"\n+\n+  /**\n+   * Exception text when there is event info entry in the timeline event, but it is empty\n+   */\n+\n+  val E_EMPTY_EVENTINFO = \"Empty 'eventinfo' entry\"\n+\n+  /**\n+   * counter incremented on every spark event to timeline event creation,\n+   * so guaranteeing uniqueness of event IDs across a single application attempt\n+   * (which is implicitly, one per JVM)\n+   */\n+  val uid = new AtomicLong(System.currentTimeMillis())\n+\n+  /**\n+   * Converts a Java object to its equivalent json4s representation.\n+   */\n+  def toJValue(obj: Object): JValue = {\n+    obj match {\n+      case str: String => JString(str)\n+      case dbl: java.lang.Double => JDouble(dbl)\n+      case dec: java.math.BigDecimal => JDecimal(dec)\n+      case int: java.lang.Integer => JInt(BigInt(int))\n+      case long: java.lang.Long => JInt(BigInt(long))\n+      case bool: java.lang.Boolean => JBool(bool)\n+      case map: JMap[_, _] =>\n+        val jmap = map.asInstanceOf[JMap[String, Object]]\n+        JObject(jmap.entrySet().map { e => (e.getKey() -> toJValue(e.getValue())) }.toList)\n+      case array: JCollection[_] =>\n+        JArray(array.asInstanceOf[JCollection[Object]].map(o => toJValue(o)).toList)\n+      case null => JNothing\n+    }\n+  }\n+\n+  /**\n+   * Converts a JValue into its Java equivalent.\n+   */\n+  def toJavaObject(v: JValue): Object = {\n+    v match {\n+      case JNothing => null\n+      case JNull => null\n+      case JString(s) => s\n+      case JDouble(num) => java.lang.Double.valueOf(num)\n+      case JDecimal(num) => num.bigDecimal\n+      case JInt(num) => java.lang.Long.valueOf(num.longValue)\n+      case JBool(value) => java.lang.Boolean.valueOf(value)\n+      case obj: JObject => toJavaMap(obj)\n+      case JArray(vals) => {\n+        val list = new JArrayList[Object]()\n+        vals.foreach(x => list.add(toJavaObject(x)))\n+        list\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Converts a json4s list of fields into a Java Map suitable for serialization by Jackson,\n+   * which is used by the ATS client library.\n+   */\n+  def toJavaMap(sourceObj: JObject): JHashMap[String, Object] = {\n+    val map = new JHashMap[String, Object]()\n+    sourceObj.obj.foreach(f => map.put(f._1, toJavaObject(f._2)))\n+    map\n+  }\n+\n+  /**\n+   * Convert a timeline event to a spark one. Includes\n+   * some basic checks for validity of event payload.\n+   * @param event timeline event\n+   * @return an unmarshalled event\n+   */\n+  def toSparkEvent(event: TimelineEvent): SparkListenerEvent = {\n+    val info = event.getEventInfo()\n+    if (info == null) {\n+      throw new IOException(E_NO_EVENTINFO)\n+    }\n+    if (info.size() == 0) {\n+      throw new IOException(E_EMPTY_EVENTINFO)\n+    }\n+    val payload = toJValue(info)\n+    def jsonToString: String = {\n+      val json = compact(render(payload))\n+      val limit = 256\n+      if (json.length < limit) {\n+        json\n+      } else {\n+          json.substring(0, limit) + \" ... }\"\n+        }\n+    }\n+    logDebug(s\"payload is ${jsonToString}\")\n+    val eventField = payload \\ \"Event\"\n+    if (eventField == JNothing) {\n+      throw new IOException(s\"No 'Event' entry in $jsonToString\")\n+    }\n+\n+    // now the real unmarshalling\n+    try {\n+      JsonProtocol.sparkEventFromJson(payload)\n+    } catch {\n+      // failure in the marshalling; include payload in the message\n+      case ex: MappingException => {\n+        logError(s\"$ex while rendering $jsonToString\", ex)\n+        throw ex\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Convert a spark event to a timeline event\n+   * @param event handled spark event\n+   * @return a timeline event\n+   */\n+  def toTimelineEvent(event: HandleSparkEvent): TimelineEvent = {\n+    val tlEvent = new TimelineEvent()\n+    tlEvent.setEventType(Utils.getFormattedClassName(event.sparkEvent)\n+        + \"-\" + YarnTimelineUtils.uid.incrementAndGet.toString)\n+    tlEvent.setTimestamp(event.time)\n+    val kvMap = new JHashMap[String, Object]()\n+    val json = JsonProtocol.sparkEventToJson(event.sparkEvent)\n+    val jObject = json.asInstanceOf[JObject]\n+    // the timeline event wants a map of java objects for Jackson to serialize\n+    val hashMap = toJavaMap(jObject)\n+    tlEvent.setEventInfo(hashMap)\n+    tlEvent\n+  }\n+\n+  /**\n+   * Describe the event for logging\n+   * @param event timeline event\n+   * @return a description\n+   */\n+  def describeEvent(event: TimelineEvent): String = {\n+    var sparkEventDetails = \"\"\n+    try {\n+      sparkEventDetails = toSparkEvent(event).toString\n+    } catch {\n+      case _: Exception =>\n+        sparkEventDetails = \"(cannot convert event details to spark exception)\"\n+    }\n+    s\"${event.getEventType()} @ ${new Date(event.getTimestamp())}\" +\n+        s\"\\n    ${sparkEventDetails}\"\n+  }\n+\n+  /**\n+   * Create details of a timeline entity, by describing every event inside it\n+   * @param entity entity containing a possibly empty or null list of events\n+   * @return a list of event details, with a newline between each one\n+   */\n+  def eventDetails(entity: TimelineEntity): String = {\n+    val events: util.List[TimelineEvent] = entity.getEvents\n+    if (events != null) {\n+      events.foldLeft(\"\") {\n+        (s, evt) => (s + \"\\n\" + describeEvent(evt))\n+      }\n+    } else {\n+      \"\"\n+    }\n+  }\n+\n+  /**\n+   * Describe a timeline entity\n+   * @param entity entity\n+   * @return a string description.\n+   */\n+  def describeEntity(entity: TimelineEntity): String = {\n+    val events: util.List[TimelineEvent] = entity.getEvents\n+    val eventSummary = if (events != null) {\n+      s\"contains ${events.size()} event(s)\"\n+    } else {\n+      \"contains no events\"\n+    }\n+\n+    val header = s\"${entity.getEntityType }/${entity.getEntityId } @${entity.getDomainId }}\"\n+    val otherInfo = entity.getOtherInfo().foldLeft(\"\\n\") (\n+      (acc, kv) => acc + s\" ${kv._1} = ${kv._2}; \"\n+    )\n+    s\"Timeline Entity \" + header +\n+        \" \" + otherInfo +\n+        \" \" + timeFieldToString(entity.getStartTime(), \"start\") +\n+        \" \" + eventSummary\n+  }\n+\n+  /**\n+   * Convert a `java.lang.Long` reference to a string value, or, if the reference is null,\n+   * to text declaring that the named field is empty\n+   * @param time time reference\n+   * @param field field name for error message\n+   * @return a string to describe the field\n+   */\n+  def timeFieldToString(time: lang.Long, field: String): String = {\n+    if (time != null) {\n+      new Date(time).toString\n+    } else {\n+       (\"no \" + field + \" time\")\n+     }\n+  }\n+\n+  /**\n+   * A verbose description of the entity which contains event details and info about\n+   * primary/secondary keys\n+   * @param entity timeline entity\n+   * @return a verbose description of the field\n+   */\n+  def describeEntityVerbose(entity: TimelineEntity): String = {"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "They are for debugging; they don't normally surface and their aim is to go into some/more detail on the entities, especially when trying to understand what has come back from the far end. \n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T17:22:05Z",
    "diffHunk": "@@ -0,0 +1,720 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.io.IOException\n+import java.net.{URI, URL}\n+import java.text.DateFormat\n+import java.util.concurrent.atomic.AtomicLong\n+import java.util.{ArrayList => JArrayList, Collection => JCollection, Date, HashMap => JHashMap, Map => JMap}\n+import java.{lang, util}\n+\n+import scala.collection.JavaConversions._\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.service.Service\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse.TimelinePutError\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+import org.json4s.JsonAST.JObject\n+import org.json4s._\n+import org.json4s.jackson.JsonMethods._\n+\n+import org.apache.spark.{SparkContext, Logging}\n+import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+import org.apache.spark.scheduler.{SparkListenerStageCompleted, SparkListenerStageSubmitted, SparkListenerExecutorAdded, SparkListenerExecutorRemoved, SparkListenerJobEnd, SparkListenerJobStart, SparkListenerApplicationEnd, SparkListenerApplicationStart, SparkListenerEvent}\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+/**\n+ * Utility methods for timeline classes.\n+ */\n+private[spark] object YarnTimelineUtils extends Logging {\n+\n+  /**\n+   * What attempt ID to use as the attempt ID field (not the entity ID) when\n+   * there is no attempt info\n+   */\n+  val SINGLE_ATTEMPT = \"1\"\n+\n+\n+  /**\n+   * Exception text when there is no event info data to unmarshall\n+   */\n+  val E_NO_EVENTINFO = \"No 'eventinfo' entry\"\n+\n+  /**\n+   * Exception text when there is event info entry in the timeline event, but it is empty\n+   */\n+\n+  val E_EMPTY_EVENTINFO = \"Empty 'eventinfo' entry\"\n+\n+  /**\n+   * counter incremented on every spark event to timeline event creation,\n+   * so guaranteeing uniqueness of event IDs across a single application attempt\n+   * (which is implicitly, one per JVM)\n+   */\n+  val uid = new AtomicLong(System.currentTimeMillis())\n+\n+  /**\n+   * Converts a Java object to its equivalent json4s representation.\n+   */\n+  def toJValue(obj: Object): JValue = {\n+    obj match {\n+      case str: String => JString(str)\n+      case dbl: java.lang.Double => JDouble(dbl)\n+      case dec: java.math.BigDecimal => JDecimal(dec)\n+      case int: java.lang.Integer => JInt(BigInt(int))\n+      case long: java.lang.Long => JInt(BigInt(long))\n+      case bool: java.lang.Boolean => JBool(bool)\n+      case map: JMap[_, _] =>\n+        val jmap = map.asInstanceOf[JMap[String, Object]]\n+        JObject(jmap.entrySet().map { e => (e.getKey() -> toJValue(e.getValue())) }.toList)\n+      case array: JCollection[_] =>\n+        JArray(array.asInstanceOf[JCollection[Object]].map(o => toJValue(o)).toList)\n+      case null => JNothing\n+    }\n+  }\n+\n+  /**\n+   * Converts a JValue into its Java equivalent.\n+   */\n+  def toJavaObject(v: JValue): Object = {\n+    v match {\n+      case JNothing => null\n+      case JNull => null\n+      case JString(s) => s\n+      case JDouble(num) => java.lang.Double.valueOf(num)\n+      case JDecimal(num) => num.bigDecimal\n+      case JInt(num) => java.lang.Long.valueOf(num.longValue)\n+      case JBool(value) => java.lang.Boolean.valueOf(value)\n+      case obj: JObject => toJavaMap(obj)\n+      case JArray(vals) => {\n+        val list = new JArrayList[Object]()\n+        vals.foreach(x => list.add(toJavaObject(x)))\n+        list\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Converts a json4s list of fields into a Java Map suitable for serialization by Jackson,\n+   * which is used by the ATS client library.\n+   */\n+  def toJavaMap(sourceObj: JObject): JHashMap[String, Object] = {\n+    val map = new JHashMap[String, Object]()\n+    sourceObj.obj.foreach(f => map.put(f._1, toJavaObject(f._2)))\n+    map\n+  }\n+\n+  /**\n+   * Convert a timeline event to a spark one. Includes\n+   * some basic checks for validity of event payload.\n+   * @param event timeline event\n+   * @return an unmarshalled event\n+   */\n+  def toSparkEvent(event: TimelineEvent): SparkListenerEvent = {\n+    val info = event.getEventInfo()\n+    if (info == null) {\n+      throw new IOException(E_NO_EVENTINFO)\n+    }\n+    if (info.size() == 0) {\n+      throw new IOException(E_EMPTY_EVENTINFO)\n+    }\n+    val payload = toJValue(info)\n+    def jsonToString: String = {\n+      val json = compact(render(payload))\n+      val limit = 256\n+      if (json.length < limit) {\n+        json\n+      } else {\n+          json.substring(0, limit) + \" ... }\"\n+        }\n+    }\n+    logDebug(s\"payload is ${jsonToString}\")\n+    val eventField = payload \\ \"Event\"\n+    if (eventField == JNothing) {\n+      throw new IOException(s\"No 'Event' entry in $jsonToString\")\n+    }\n+\n+    // now the real unmarshalling\n+    try {\n+      JsonProtocol.sparkEventFromJson(payload)\n+    } catch {\n+      // failure in the marshalling; include payload in the message\n+      case ex: MappingException => {\n+        logError(s\"$ex while rendering $jsonToString\", ex)\n+        throw ex\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Convert a spark event to a timeline event\n+   * @param event handled spark event\n+   * @return a timeline event\n+   */\n+  def toTimelineEvent(event: HandleSparkEvent): TimelineEvent = {\n+    val tlEvent = new TimelineEvent()\n+    tlEvent.setEventType(Utils.getFormattedClassName(event.sparkEvent)\n+        + \"-\" + YarnTimelineUtils.uid.incrementAndGet.toString)\n+    tlEvent.setTimestamp(event.time)\n+    val kvMap = new JHashMap[String, Object]()\n+    val json = JsonProtocol.sparkEventToJson(event.sparkEvent)\n+    val jObject = json.asInstanceOf[JObject]\n+    // the timeline event wants a map of java objects for Jackson to serialize\n+    val hashMap = toJavaMap(jObject)\n+    tlEvent.setEventInfo(hashMap)\n+    tlEvent\n+  }\n+\n+  /**\n+   * Describe the event for logging\n+   * @param event timeline event\n+   * @return a description\n+   */\n+  def describeEvent(event: TimelineEvent): String = {\n+    var sparkEventDetails = \"\"\n+    try {\n+      sparkEventDetails = toSparkEvent(event).toString\n+    } catch {\n+      case _: Exception =>\n+        sparkEventDetails = \"(cannot convert event details to spark exception)\"\n+    }\n+    s\"${event.getEventType()} @ ${new Date(event.getTimestamp())}\" +\n+        s\"\\n    ${sparkEventDetails}\"\n+  }\n+\n+  /**\n+   * Create details of a timeline entity, by describing every event inside it\n+   * @param entity entity containing a possibly empty or null list of events\n+   * @return a list of event details, with a newline between each one\n+   */\n+  def eventDetails(entity: TimelineEntity): String = {\n+    val events: util.List[TimelineEvent] = entity.getEvents\n+    if (events != null) {\n+      events.foldLeft(\"\") {\n+        (s, evt) => (s + \"\\n\" + describeEvent(evt))\n+      }\n+    } else {\n+      \"\"\n+    }\n+  }\n+\n+  /**\n+   * Describe a timeline entity\n+   * @param entity entity\n+   * @return a string description.\n+   */\n+  def describeEntity(entity: TimelineEntity): String = {\n+    val events: util.List[TimelineEvent] = entity.getEvents\n+    val eventSummary = if (events != null) {\n+      s\"contains ${events.size()} event(s)\"\n+    } else {\n+      \"contains no events\"\n+    }\n+\n+    val header = s\"${entity.getEntityType }/${entity.getEntityId } @${entity.getDomainId }}\"\n+    val otherInfo = entity.getOtherInfo().foldLeft(\"\\n\") (\n+      (acc, kv) => acc + s\" ${kv._1} = ${kv._2}; \"\n+    )\n+    s\"Timeline Entity \" + header +\n+        \" \" + otherInfo +\n+        \" \" + timeFieldToString(entity.getStartTime(), \"start\") +\n+        \" \" + eventSummary\n+  }\n+\n+  /**\n+   * Convert a `java.lang.Long` reference to a string value, or, if the reference is null,\n+   * to text declaring that the named field is empty\n+   * @param time time reference\n+   * @param field field name for error message\n+   * @return a string to describe the field\n+   */\n+  def timeFieldToString(time: lang.Long, field: String): String = {\n+    if (time != null) {\n+      new Date(time).toString\n+    } else {\n+       (\"no \" + field + \" time\")\n+     }\n+  }\n+\n+  /**\n+   * A verbose description of the entity which contains event details and info about\n+   * primary/secondary keys\n+   * @param entity timeline entity\n+   * @return a verbose description of the field\n+   */\n+  def describeEntityVerbose(entity: TimelineEntity): String = {"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`responseErrs != null`\n\nI recommend running the `scalastyle` target in sbt on your machine (`mvn install` should also run it). Jenkins does not test build / test code since it's in a separate profile, so it's not complaining about all these style violations.\n\n(Another way would be to add a dummy commit that makes the profile enabled by default and forces the hadoop version to 2.6, just to force jenkins to run your code.)\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T00:35:39Z",
    "diffHunk": "@@ -0,0 +1,720 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.io.IOException\n+import java.net.{URI, URL}\n+import java.text.DateFormat\n+import java.util.concurrent.atomic.AtomicLong\n+import java.util.{ArrayList => JArrayList, Collection => JCollection, Date, HashMap => JHashMap, Map => JMap}\n+import java.{lang, util}\n+\n+import scala.collection.JavaConversions._\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.service.Service\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse.TimelinePutError\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+import org.json4s.JsonAST.JObject\n+import org.json4s._\n+import org.json4s.jackson.JsonMethods._\n+\n+import org.apache.spark.{SparkContext, Logging}\n+import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+import org.apache.spark.scheduler.{SparkListenerStageCompleted, SparkListenerStageSubmitted, SparkListenerExecutorAdded, SparkListenerExecutorRemoved, SparkListenerJobEnd, SparkListenerJobStart, SparkListenerApplicationEnd, SparkListenerApplicationStart, SparkListenerEvent}\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+/**\n+ * Utility methods for timeline classes.\n+ */\n+private[spark] object YarnTimelineUtils extends Logging {\n+\n+  /**\n+   * What attempt ID to use as the attempt ID field (not the entity ID) when\n+   * there is no attempt info\n+   */\n+  val SINGLE_ATTEMPT = \"1\"\n+\n+\n+  /**\n+   * Exception text when there is no event info data to unmarshall\n+   */\n+  val E_NO_EVENTINFO = \"No 'eventinfo' entry\"\n+\n+  /**\n+   * Exception text when there is event info entry in the timeline event, but it is empty\n+   */\n+\n+  val E_EMPTY_EVENTINFO = \"Empty 'eventinfo' entry\"\n+\n+  /**\n+   * counter incremented on every spark event to timeline event creation,\n+   * so guaranteeing uniqueness of event IDs across a single application attempt\n+   * (which is implicitly, one per JVM)\n+   */\n+  val uid = new AtomicLong(System.currentTimeMillis())\n+\n+  /**\n+   * Converts a Java object to its equivalent json4s representation.\n+   */\n+  def toJValue(obj: Object): JValue = {\n+    obj match {\n+      case str: String => JString(str)\n+      case dbl: java.lang.Double => JDouble(dbl)\n+      case dec: java.math.BigDecimal => JDecimal(dec)\n+      case int: java.lang.Integer => JInt(BigInt(int))\n+      case long: java.lang.Long => JInt(BigInt(long))\n+      case bool: java.lang.Boolean => JBool(bool)\n+      case map: JMap[_, _] =>\n+        val jmap = map.asInstanceOf[JMap[String, Object]]\n+        JObject(jmap.entrySet().map { e => (e.getKey() -> toJValue(e.getValue())) }.toList)\n+      case array: JCollection[_] =>\n+        JArray(array.asInstanceOf[JCollection[Object]].map(o => toJValue(o)).toList)\n+      case null => JNothing\n+    }\n+  }\n+\n+  /**\n+   * Converts a JValue into its Java equivalent.\n+   */\n+  def toJavaObject(v: JValue): Object = {\n+    v match {\n+      case JNothing => null\n+      case JNull => null\n+      case JString(s) => s\n+      case JDouble(num) => java.lang.Double.valueOf(num)\n+      case JDecimal(num) => num.bigDecimal\n+      case JInt(num) => java.lang.Long.valueOf(num.longValue)\n+      case JBool(value) => java.lang.Boolean.valueOf(value)\n+      case obj: JObject => toJavaMap(obj)\n+      case JArray(vals) => {\n+        val list = new JArrayList[Object]()\n+        vals.foreach(x => list.add(toJavaObject(x)))\n+        list\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Converts a json4s list of fields into a Java Map suitable for serialization by Jackson,\n+   * which is used by the ATS client library.\n+   */\n+  def toJavaMap(sourceObj: JObject): JHashMap[String, Object] = {\n+    val map = new JHashMap[String, Object]()\n+    sourceObj.obj.foreach(f => map.put(f._1, toJavaObject(f._2)))\n+    map\n+  }\n+\n+  /**\n+   * Convert a timeline event to a spark one. Includes\n+   * some basic checks for validity of event payload.\n+   * @param event timeline event\n+   * @return an unmarshalled event\n+   */\n+  def toSparkEvent(event: TimelineEvent): SparkListenerEvent = {\n+    val info = event.getEventInfo()\n+    if (info == null) {\n+      throw new IOException(E_NO_EVENTINFO)\n+    }\n+    if (info.size() == 0) {\n+      throw new IOException(E_EMPTY_EVENTINFO)\n+    }\n+    val payload = toJValue(info)\n+    def jsonToString: String = {\n+      val json = compact(render(payload))\n+      val limit = 256\n+      if (json.length < limit) {\n+        json\n+      } else {\n+          json.substring(0, limit) + \" ... }\"\n+        }\n+    }\n+    logDebug(s\"payload is ${jsonToString}\")\n+    val eventField = payload \\ \"Event\"\n+    if (eventField == JNothing) {\n+      throw new IOException(s\"No 'Event' entry in $jsonToString\")\n+    }\n+\n+    // now the real unmarshalling\n+    try {\n+      JsonProtocol.sparkEventFromJson(payload)\n+    } catch {\n+      // failure in the marshalling; include payload in the message\n+      case ex: MappingException => {\n+        logError(s\"$ex while rendering $jsonToString\", ex)\n+        throw ex\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Convert a spark event to a timeline event\n+   * @param event handled spark event\n+   * @return a timeline event\n+   */\n+  def toTimelineEvent(event: HandleSparkEvent): TimelineEvent = {\n+    val tlEvent = new TimelineEvent()\n+    tlEvent.setEventType(Utils.getFormattedClassName(event.sparkEvent)\n+        + \"-\" + YarnTimelineUtils.uid.incrementAndGet.toString)\n+    tlEvent.setTimestamp(event.time)\n+    val kvMap = new JHashMap[String, Object]()\n+    val json = JsonProtocol.sparkEventToJson(event.sparkEvent)\n+    val jObject = json.asInstanceOf[JObject]\n+    // the timeline event wants a map of java objects for Jackson to serialize\n+    val hashMap = toJavaMap(jObject)\n+    tlEvent.setEventInfo(hashMap)\n+    tlEvent\n+  }\n+\n+  /**\n+   * Describe the event for logging\n+   * @param event timeline event\n+   * @return a description\n+   */\n+  def describeEvent(event: TimelineEvent): String = {\n+    var sparkEventDetails = \"\"\n+    try {\n+      sparkEventDetails = toSparkEvent(event).toString\n+    } catch {\n+      case _: Exception =>\n+        sparkEventDetails = \"(cannot convert event details to spark exception)\"\n+    }\n+    s\"${event.getEventType()} @ ${new Date(event.getTimestamp())}\" +\n+        s\"\\n    ${sparkEventDetails}\"\n+  }\n+\n+  /**\n+   * Create details of a timeline entity, by describing every event inside it\n+   * @param entity entity containing a possibly empty or null list of events\n+   * @return a list of event details, with a newline between each one\n+   */\n+  def eventDetails(entity: TimelineEntity): String = {\n+    val events: util.List[TimelineEvent] = entity.getEvents\n+    if (events != null) {\n+      events.foldLeft(\"\") {\n+        (s, evt) => (s + \"\\n\" + describeEvent(evt))\n+      }\n+    } else {\n+      \"\"\n+    }\n+  }\n+\n+  /**\n+   * Describe a timeline entity\n+   * @param entity entity\n+   * @return a string description.\n+   */\n+  def describeEntity(entity: TimelineEntity): String = {\n+    val events: util.List[TimelineEvent] = entity.getEvents\n+    val eventSummary = if (events != null) {\n+      s\"contains ${events.size()} event(s)\"\n+    } else {\n+      \"contains no events\"\n+    }\n+\n+    val header = s\"${entity.getEntityType }/${entity.getEntityId } @${entity.getDomainId }}\"\n+    val otherInfo = entity.getOtherInfo().foldLeft(\"\\n\") (\n+      (acc, kv) => acc + s\" ${kv._1} = ${kv._2}; \"\n+    )\n+    s\"Timeline Entity \" + header +\n+        \" \" + otherInfo +\n+        \" \" + timeFieldToString(entity.getStartTime(), \"start\") +\n+        \" \" + eventSummary\n+  }\n+\n+  /**\n+   * Convert a `java.lang.Long` reference to a string value, or, if the reference is null,\n+   * to text declaring that the named field is empty\n+   * @param time time reference\n+   * @param field field name for error message\n+   * @return a string to describe the field\n+   */\n+  def timeFieldToString(time: lang.Long, field: String): String = {\n+    if (time != null) {\n+      new Date(time).toString\n+    } else {\n+       (\"no \" + field + \" time\")\n+     }\n+  }\n+\n+  /**\n+   * A verbose description of the entity which contains event details and info about\n+   * primary/secondary keys\n+   * @param entity timeline entity\n+   * @return a verbose description of the field\n+   */\n+  def describeEntityVerbose(entity: TimelineEntity): String = {\n+    val header = describeEntity(entity)\n+    val primaryFilters = entity.getPrimaryFilters.toMap\n+    var filterElements = \"\"\n+    for ((k, v) <- primaryFilters) {\n+      filterElements = filterElements +\n+        \" filter \" + k + \": [ \" + v.foldLeft(\"\")((s, o) => (s + o.toString + \" \")) +\"]\\n\"\n+    }\n+\n+    val events = eventDetails(entity)\n+    header + \"\\n\" + filterElements + events\n+  }\n+\n+  /**\n+   * Stop any optional service\n+   * @param svc service\n+   */\n+  def stopOptionalService(svc: Option[Service]): Unit = {\n+    svc match {\n+      case Some(client) => client.stop()\n+      case None =>\n+    }\n+  }\n+\n+  /**\n+   * Split a comma separated String, filter out any empty items, and return a Set of strings\n+   */\n+  def stringToSet(list: String): Set[String] = {\n+    list.split(',').map(_.trim).filter(!_.isEmpty).toSet\n+  }\n+\n+  /**\n+   * Try to get the event time off an event. Not all events have the required information\n+   * @param event event to process\n+   * @return the event time\n+   */\n+  def eventTime(event: SparkListenerEvent): Option[Long] = {\n+    event match {\n+      case evt: SparkListenerApplicationStart =>\n+        Some(evt.time)\n+      case evt: SparkListenerApplicationEnd =>\n+        Some(evt.time)\n+      case evt: SparkListenerJobStart =>\n+        Some(evt.time)\n+      case evt: SparkListenerJobEnd =>\n+        Some(evt.time)\n+      case evt: SparkListenerExecutorAdded =>\n+        Some(evt.time)\n+      case evt: SparkListenerExecutorRemoved =>\n+        Some(evt.time)\n+      case evt: SparkListenerStageSubmitted =>\n+        evt.stageInfo.submissionTime\n+      case evt: SparkListenerStageCompleted =>\n+        evt.stageInfo.completionTime\n+      case _ => None\n+    }\n+  }\n+\n+\n+  /**\n+   * Create and start a timeline client, using the configuration context to\n+   * set up the binding\n+   * @param sparkContext spark context\n+   * @return the started instance\n+   */\n+  def createTimelineClient(sparkContext: SparkContext): TimelineClient = {\n+    val client = TimelineClient.createTimelineClient\n+    client.init(sparkContext.hadoopConfiguration)\n+    client.start\n+    client\n+  }\n+\n+  /**\n+   * The path for the V1 ATS REST API\n+   */\n+  val TIMELINE_REST_PATH = s\"/ws/v1/timeline/\"\n+\n+  /**\n+   * Build the URI to the base of the timeline web application\n+   * from the Hadoop context.\n+   * <p>\n+   * Raises an exception if the address cannot be determined.\n+   * <p>\n+   * Does not perform any checks as to whether or note the timeline\n+   * service is enabled\n+   * @param conf configuration\n+   * @return the URI to the timeline service.\n+   */\n+  def getTimelineEndpoint(conf: Configuration): URI = {\n+    val isHttps = YarnConfiguration.useHttps(conf)\n+    val address = if (isHttps) {\n+      conf.get(YarnConfiguration.TIMELINE_SERVICE_WEBAPP_HTTPS_ADDRESS,\n+                YarnConfiguration.DEFAULT_TIMELINE_SERVICE_WEBAPP_HTTPS_ADDRESS)\n+    } else {\n+      conf.get(YarnConfiguration.TIMELINE_SERVICE_WEBAPP_ADDRESS,\n+                YarnConfiguration.DEFAULT_TIMELINE_SERVICE_WEBAPP_ADDRESS)\n+    }\n+    val protocol = if (isHttps) \"https://\" else \"http://\"\n+    require(address != null, s\"No timeline service defined\")\n+    URI.create(s\"$protocol${address}$TIMELINE_REST_PATH\")\n+  }\n+\n+  /**\n+   * Create a URI to the history service. This uses the entity type of\n+   * <code>YarnHistoryService.ENTITY_TYPE</code> to\n+   * @param conf\n+   * @return\n+   */\n+  def timelineWebappUri(conf: Configuration): URI = {\n+    timelineWebappUri(conf, YarnHistoryService.SPARK_EVENT_ENTITY_TYPE)\n+  }\n+\n+  /**\n+   * Get the URI of a path under the timeline web UI\n+   * @param conf configuration\n+   * @param subpath path under the root web UI\n+   * @return a URI\n+   */\n+  def timelineWebappUri(conf: Configuration, subpath: String): URI = {\n+    val base = getTimelineEndpoint(conf)\n+    new URL(base.toURL, subpath).toURI\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled(conf: Configuration): Boolean = {\n+    conf.getBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED,\n+                    YarnConfiguration.DEFAULT_TIMELINE_SERVICE_ENABLED)\n+  }\n+\n+  /**\n+   * Get the URI to an application under the timeline\n+   * (this requires the applicationID to have been used to\n+   * publish entities there)\n+   * @param timelineUri timeline URI\n+   * @param appId App ID (really, the entityId used to publish)\n+   * @return the path\n+   */\n+  def applicationURI(timelineUri: URI, appId: String): URI = {\n+    require(appId != null && !appId.isEmpty, \"No application ID\")\n+    require(!appId.contains(\"/\"), s\"Illegal character '/' in $appId\")\n+    timelineUri.resolve(s\"${timelineUri.getPath()}/$appId\")\n+  }\n+\n+  /**\n+   * Map an error code to a string. For known codes, it returns\n+   * a description; for others it just returns the error code.\n+   * \n+   * @param code error code\n+   * @return a string description for error messages\n+   */\n+  def timelineErrorCodeToString(code: Int): String ={\n+    code match {\n+      case 1 => \"No start time\"\n+      case 2 => \"IO Exception\"\n+      case 3 => \"System Filter Conflict\"\n+      case 4 => \"Access Denied\"\n+      case 5 => \"No Domain\"\n+      case 6 => \"Forbidden Relation\"\n+      case other: Int => s\"Error code $other\"\n+    }\n+  }\n+\n+  /**\n+   * Convert a timeline error response to a meaningful string\n+   * @param error error\n+   * @return text for diagnostics\n+   */\n+  def describeError(error: TimelinePutError): String = {\n+    s\"Entity ID=${error.getEntityId()}; Entity type=${error.getEntityType}\" +\n+    s\" Error code ${error.getErrorCode}\" +\n+    s\": ${timelineErrorCodeToString(error.getErrorCode)}\"\n+  }\n+\n+  /**\n+   * Describe a put response by enumerating and describing all errors\n+   * (if present. A null errors element is handles robustly)\n+   * @param response response to describe\n+   * @return text for diagnostics\n+   */\n+  def describePutResponse(response: TimelinePutResponse) : String = {\n+    val responseErrs = response.getErrors\n+    if (responseErrs!=null) {"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "I'd thought it was running; will use sbt\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T17:22:59Z",
    "diffHunk": "@@ -0,0 +1,720 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.io.IOException\n+import java.net.{URI, URL}\n+import java.text.DateFormat\n+import java.util.concurrent.atomic.AtomicLong\n+import java.util.{ArrayList => JArrayList, Collection => JCollection, Date, HashMap => JHashMap, Map => JMap}\n+import java.{lang, util}\n+\n+import scala.collection.JavaConversions._\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.service.Service\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse.TimelinePutError\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+import org.json4s.JsonAST.JObject\n+import org.json4s._\n+import org.json4s.jackson.JsonMethods._\n+\n+import org.apache.spark.{SparkContext, Logging}\n+import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+import org.apache.spark.scheduler.{SparkListenerStageCompleted, SparkListenerStageSubmitted, SparkListenerExecutorAdded, SparkListenerExecutorRemoved, SparkListenerJobEnd, SparkListenerJobStart, SparkListenerApplicationEnd, SparkListenerApplicationStart, SparkListenerEvent}\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+/**\n+ * Utility methods for timeline classes.\n+ */\n+private[spark] object YarnTimelineUtils extends Logging {\n+\n+  /**\n+   * What attempt ID to use as the attempt ID field (not the entity ID) when\n+   * there is no attempt info\n+   */\n+  val SINGLE_ATTEMPT = \"1\"\n+\n+\n+  /**\n+   * Exception text when there is no event info data to unmarshall\n+   */\n+  val E_NO_EVENTINFO = \"No 'eventinfo' entry\"\n+\n+  /**\n+   * Exception text when there is event info entry in the timeline event, but it is empty\n+   */\n+\n+  val E_EMPTY_EVENTINFO = \"Empty 'eventinfo' entry\"\n+\n+  /**\n+   * counter incremented on every spark event to timeline event creation,\n+   * so guaranteeing uniqueness of event IDs across a single application attempt\n+   * (which is implicitly, one per JVM)\n+   */\n+  val uid = new AtomicLong(System.currentTimeMillis())\n+\n+  /**\n+   * Converts a Java object to its equivalent json4s representation.\n+   */\n+  def toJValue(obj: Object): JValue = {\n+    obj match {\n+      case str: String => JString(str)\n+      case dbl: java.lang.Double => JDouble(dbl)\n+      case dec: java.math.BigDecimal => JDecimal(dec)\n+      case int: java.lang.Integer => JInt(BigInt(int))\n+      case long: java.lang.Long => JInt(BigInt(long))\n+      case bool: java.lang.Boolean => JBool(bool)\n+      case map: JMap[_, _] =>\n+        val jmap = map.asInstanceOf[JMap[String, Object]]\n+        JObject(jmap.entrySet().map { e => (e.getKey() -> toJValue(e.getValue())) }.toList)\n+      case array: JCollection[_] =>\n+        JArray(array.asInstanceOf[JCollection[Object]].map(o => toJValue(o)).toList)\n+      case null => JNothing\n+    }\n+  }\n+\n+  /**\n+   * Converts a JValue into its Java equivalent.\n+   */\n+  def toJavaObject(v: JValue): Object = {\n+    v match {\n+      case JNothing => null\n+      case JNull => null\n+      case JString(s) => s\n+      case JDouble(num) => java.lang.Double.valueOf(num)\n+      case JDecimal(num) => num.bigDecimal\n+      case JInt(num) => java.lang.Long.valueOf(num.longValue)\n+      case JBool(value) => java.lang.Boolean.valueOf(value)\n+      case obj: JObject => toJavaMap(obj)\n+      case JArray(vals) => {\n+        val list = new JArrayList[Object]()\n+        vals.foreach(x => list.add(toJavaObject(x)))\n+        list\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Converts a json4s list of fields into a Java Map suitable for serialization by Jackson,\n+   * which is used by the ATS client library.\n+   */\n+  def toJavaMap(sourceObj: JObject): JHashMap[String, Object] = {\n+    val map = new JHashMap[String, Object]()\n+    sourceObj.obj.foreach(f => map.put(f._1, toJavaObject(f._2)))\n+    map\n+  }\n+\n+  /**\n+   * Convert a timeline event to a spark one. Includes\n+   * some basic checks for validity of event payload.\n+   * @param event timeline event\n+   * @return an unmarshalled event\n+   */\n+  def toSparkEvent(event: TimelineEvent): SparkListenerEvent = {\n+    val info = event.getEventInfo()\n+    if (info == null) {\n+      throw new IOException(E_NO_EVENTINFO)\n+    }\n+    if (info.size() == 0) {\n+      throw new IOException(E_EMPTY_EVENTINFO)\n+    }\n+    val payload = toJValue(info)\n+    def jsonToString: String = {\n+      val json = compact(render(payload))\n+      val limit = 256\n+      if (json.length < limit) {\n+        json\n+      } else {\n+          json.substring(0, limit) + \" ... }\"\n+        }\n+    }\n+    logDebug(s\"payload is ${jsonToString}\")\n+    val eventField = payload \\ \"Event\"\n+    if (eventField == JNothing) {\n+      throw new IOException(s\"No 'Event' entry in $jsonToString\")\n+    }\n+\n+    // now the real unmarshalling\n+    try {\n+      JsonProtocol.sparkEventFromJson(payload)\n+    } catch {\n+      // failure in the marshalling; include payload in the message\n+      case ex: MappingException => {\n+        logError(s\"$ex while rendering $jsonToString\", ex)\n+        throw ex\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Convert a spark event to a timeline event\n+   * @param event handled spark event\n+   * @return a timeline event\n+   */\n+  def toTimelineEvent(event: HandleSparkEvent): TimelineEvent = {\n+    val tlEvent = new TimelineEvent()\n+    tlEvent.setEventType(Utils.getFormattedClassName(event.sparkEvent)\n+        + \"-\" + YarnTimelineUtils.uid.incrementAndGet.toString)\n+    tlEvent.setTimestamp(event.time)\n+    val kvMap = new JHashMap[String, Object]()\n+    val json = JsonProtocol.sparkEventToJson(event.sparkEvent)\n+    val jObject = json.asInstanceOf[JObject]\n+    // the timeline event wants a map of java objects for Jackson to serialize\n+    val hashMap = toJavaMap(jObject)\n+    tlEvent.setEventInfo(hashMap)\n+    tlEvent\n+  }\n+\n+  /**\n+   * Describe the event for logging\n+   * @param event timeline event\n+   * @return a description\n+   */\n+  def describeEvent(event: TimelineEvent): String = {\n+    var sparkEventDetails = \"\"\n+    try {\n+      sparkEventDetails = toSparkEvent(event).toString\n+    } catch {\n+      case _: Exception =>\n+        sparkEventDetails = \"(cannot convert event details to spark exception)\"\n+    }\n+    s\"${event.getEventType()} @ ${new Date(event.getTimestamp())}\" +\n+        s\"\\n    ${sparkEventDetails}\"\n+  }\n+\n+  /**\n+   * Create details of a timeline entity, by describing every event inside it\n+   * @param entity entity containing a possibly empty or null list of events\n+   * @return a list of event details, with a newline between each one\n+   */\n+  def eventDetails(entity: TimelineEntity): String = {\n+    val events: util.List[TimelineEvent] = entity.getEvents\n+    if (events != null) {\n+      events.foldLeft(\"\") {\n+        (s, evt) => (s + \"\\n\" + describeEvent(evt))\n+      }\n+    } else {\n+      \"\"\n+    }\n+  }\n+\n+  /**\n+   * Describe a timeline entity\n+   * @param entity entity\n+   * @return a string description.\n+   */\n+  def describeEntity(entity: TimelineEntity): String = {\n+    val events: util.List[TimelineEvent] = entity.getEvents\n+    val eventSummary = if (events != null) {\n+      s\"contains ${events.size()} event(s)\"\n+    } else {\n+      \"contains no events\"\n+    }\n+\n+    val header = s\"${entity.getEntityType }/${entity.getEntityId } @${entity.getDomainId }}\"\n+    val otherInfo = entity.getOtherInfo().foldLeft(\"\\n\") (\n+      (acc, kv) => acc + s\" ${kv._1} = ${kv._2}; \"\n+    )\n+    s\"Timeline Entity \" + header +\n+        \" \" + otherInfo +\n+        \" \" + timeFieldToString(entity.getStartTime(), \"start\") +\n+        \" \" + eventSummary\n+  }\n+\n+  /**\n+   * Convert a `java.lang.Long` reference to a string value, or, if the reference is null,\n+   * to text declaring that the named field is empty\n+   * @param time time reference\n+   * @param field field name for error message\n+   * @return a string to describe the field\n+   */\n+  def timeFieldToString(time: lang.Long, field: String): String = {\n+    if (time != null) {\n+      new Date(time).toString\n+    } else {\n+       (\"no \" + field + \" time\")\n+     }\n+  }\n+\n+  /**\n+   * A verbose description of the entity which contains event details and info about\n+   * primary/secondary keys\n+   * @param entity timeline entity\n+   * @return a verbose description of the field\n+   */\n+  def describeEntityVerbose(entity: TimelineEntity): String = {\n+    val header = describeEntity(entity)\n+    val primaryFilters = entity.getPrimaryFilters.toMap\n+    var filterElements = \"\"\n+    for ((k, v) <- primaryFilters) {\n+      filterElements = filterElements +\n+        \" filter \" + k + \": [ \" + v.foldLeft(\"\")((s, o) => (s + o.toString + \" \")) +\"]\\n\"\n+    }\n+\n+    val events = eventDetails(entity)\n+    header + \"\\n\" + filterElements + events\n+  }\n+\n+  /**\n+   * Stop any optional service\n+   * @param svc service\n+   */\n+  def stopOptionalService(svc: Option[Service]): Unit = {\n+    svc match {\n+      case Some(client) => client.stop()\n+      case None =>\n+    }\n+  }\n+\n+  /**\n+   * Split a comma separated String, filter out any empty items, and return a Set of strings\n+   */\n+  def stringToSet(list: String): Set[String] = {\n+    list.split(',').map(_.trim).filter(!_.isEmpty).toSet\n+  }\n+\n+  /**\n+   * Try to get the event time off an event. Not all events have the required information\n+   * @param event event to process\n+   * @return the event time\n+   */\n+  def eventTime(event: SparkListenerEvent): Option[Long] = {\n+    event match {\n+      case evt: SparkListenerApplicationStart =>\n+        Some(evt.time)\n+      case evt: SparkListenerApplicationEnd =>\n+        Some(evt.time)\n+      case evt: SparkListenerJobStart =>\n+        Some(evt.time)\n+      case evt: SparkListenerJobEnd =>\n+        Some(evt.time)\n+      case evt: SparkListenerExecutorAdded =>\n+        Some(evt.time)\n+      case evt: SparkListenerExecutorRemoved =>\n+        Some(evt.time)\n+      case evt: SparkListenerStageSubmitted =>\n+        evt.stageInfo.submissionTime\n+      case evt: SparkListenerStageCompleted =>\n+        evt.stageInfo.completionTime\n+      case _ => None\n+    }\n+  }\n+\n+\n+  /**\n+   * Create and start a timeline client, using the configuration context to\n+   * set up the binding\n+   * @param sparkContext spark context\n+   * @return the started instance\n+   */\n+  def createTimelineClient(sparkContext: SparkContext): TimelineClient = {\n+    val client = TimelineClient.createTimelineClient\n+    client.init(sparkContext.hadoopConfiguration)\n+    client.start\n+    client\n+  }\n+\n+  /**\n+   * The path for the V1 ATS REST API\n+   */\n+  val TIMELINE_REST_PATH = s\"/ws/v1/timeline/\"\n+\n+  /**\n+   * Build the URI to the base of the timeline web application\n+   * from the Hadoop context.\n+   * <p>\n+   * Raises an exception if the address cannot be determined.\n+   * <p>\n+   * Does not perform any checks as to whether or note the timeline\n+   * service is enabled\n+   * @param conf configuration\n+   * @return the URI to the timeline service.\n+   */\n+  def getTimelineEndpoint(conf: Configuration): URI = {\n+    val isHttps = YarnConfiguration.useHttps(conf)\n+    val address = if (isHttps) {\n+      conf.get(YarnConfiguration.TIMELINE_SERVICE_WEBAPP_HTTPS_ADDRESS,\n+                YarnConfiguration.DEFAULT_TIMELINE_SERVICE_WEBAPP_HTTPS_ADDRESS)\n+    } else {\n+      conf.get(YarnConfiguration.TIMELINE_SERVICE_WEBAPP_ADDRESS,\n+                YarnConfiguration.DEFAULT_TIMELINE_SERVICE_WEBAPP_ADDRESS)\n+    }\n+    val protocol = if (isHttps) \"https://\" else \"http://\"\n+    require(address != null, s\"No timeline service defined\")\n+    URI.create(s\"$protocol${address}$TIMELINE_REST_PATH\")\n+  }\n+\n+  /**\n+   * Create a URI to the history service. This uses the entity type of\n+   * <code>YarnHistoryService.ENTITY_TYPE</code> to\n+   * @param conf\n+   * @return\n+   */\n+  def timelineWebappUri(conf: Configuration): URI = {\n+    timelineWebappUri(conf, YarnHistoryService.SPARK_EVENT_ENTITY_TYPE)\n+  }\n+\n+  /**\n+   * Get the URI of a path under the timeline web UI\n+   * @param conf configuration\n+   * @param subpath path under the root web UI\n+   * @return a URI\n+   */\n+  def timelineWebappUri(conf: Configuration, subpath: String): URI = {\n+    val base = getTimelineEndpoint(conf)\n+    new URL(base.toURL, subpath).toURI\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled(conf: Configuration): Boolean = {\n+    conf.getBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED,\n+                    YarnConfiguration.DEFAULT_TIMELINE_SERVICE_ENABLED)\n+  }\n+\n+  /**\n+   * Get the URI to an application under the timeline\n+   * (this requires the applicationID to have been used to\n+   * publish entities there)\n+   * @param timelineUri timeline URI\n+   * @param appId App ID (really, the entityId used to publish)\n+   * @return the path\n+   */\n+  def applicationURI(timelineUri: URI, appId: String): URI = {\n+    require(appId != null && !appId.isEmpty, \"No application ID\")\n+    require(!appId.contains(\"/\"), s\"Illegal character '/' in $appId\")\n+    timelineUri.resolve(s\"${timelineUri.getPath()}/$appId\")\n+  }\n+\n+  /**\n+   * Map an error code to a string. For known codes, it returns\n+   * a description; for others it just returns the error code.\n+   * \n+   * @param code error code\n+   * @return a string description for error messages\n+   */\n+  def timelineErrorCodeToString(code: Int): String ={\n+    code match {\n+      case 1 => \"No start time\"\n+      case 2 => \"IO Exception\"\n+      case 3 => \"System Filter Conflict\"\n+      case 4 => \"Access Denied\"\n+      case 5 => \"No Domain\"\n+      case 6 => \"Forbidden Relation\"\n+      case other: Int => s\"Error code $other\"\n+    }\n+  }\n+\n+  /**\n+   * Convert a timeline error response to a meaningful string\n+   * @param error error\n+   * @return text for diagnostics\n+   */\n+  def describeError(error: TimelinePutError): String = {\n+    s\"Entity ID=${error.getEntityId()}; Entity type=${error.getEntityType}\" +\n+    s\" Error code ${error.getErrorCode}\" +\n+    s\": ${timelineErrorCodeToString(error.getErrorCode)}\"\n+  }\n+\n+  /**\n+   * Describe a put response by enumerating and describing all errors\n+   * (if present. A null errors element is handles robustly)\n+   * @param response response to describe\n+   * @return text for diagnostics\n+   */\n+  def describePutResponse(response: TimelinePutResponse) : String = {\n+    val responseErrs = response.getErrors\n+    if (responseErrs!=null) {"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "You don't really use half of these arguments.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T00:39:12Z",
    "diffHunk": "@@ -0,0 +1,720 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.io.IOException\n+import java.net.{URI, URL}\n+import java.text.DateFormat\n+import java.util.concurrent.atomic.AtomicLong\n+import java.util.{ArrayList => JArrayList, Collection => JCollection, Date, HashMap => JHashMap, Map => JMap}\n+import java.{lang, util}\n+\n+import scala.collection.JavaConversions._\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.service.Service\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse.TimelinePutError\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+import org.json4s.JsonAST.JObject\n+import org.json4s._\n+import org.json4s.jackson.JsonMethods._\n+\n+import org.apache.spark.{SparkContext, Logging}\n+import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+import org.apache.spark.scheduler.{SparkListenerStageCompleted, SparkListenerStageSubmitted, SparkListenerExecutorAdded, SparkListenerExecutorRemoved, SparkListenerJobEnd, SparkListenerJobStart, SparkListenerApplicationEnd, SparkListenerApplicationStart, SparkListenerEvent}\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+/**\n+ * Utility methods for timeline classes.\n+ */\n+private[spark] object YarnTimelineUtils extends Logging {\n+\n+  /**\n+   * What attempt ID to use as the attempt ID field (not the entity ID) when\n+   * there is no attempt info\n+   */\n+  val SINGLE_ATTEMPT = \"1\"\n+\n+\n+  /**\n+   * Exception text when there is no event info data to unmarshall\n+   */\n+  val E_NO_EVENTINFO = \"No 'eventinfo' entry\"\n+\n+  /**\n+   * Exception text when there is event info entry in the timeline event, but it is empty\n+   */\n+\n+  val E_EMPTY_EVENTINFO = \"Empty 'eventinfo' entry\"\n+\n+  /**\n+   * counter incremented on every spark event to timeline event creation,\n+   * so guaranteeing uniqueness of event IDs across a single application attempt\n+   * (which is implicitly, one per JVM)\n+   */\n+  val uid = new AtomicLong(System.currentTimeMillis())\n+\n+  /**\n+   * Converts a Java object to its equivalent json4s representation.\n+   */\n+  def toJValue(obj: Object): JValue = {\n+    obj match {\n+      case str: String => JString(str)\n+      case dbl: java.lang.Double => JDouble(dbl)\n+      case dec: java.math.BigDecimal => JDecimal(dec)\n+      case int: java.lang.Integer => JInt(BigInt(int))\n+      case long: java.lang.Long => JInt(BigInt(long))\n+      case bool: java.lang.Boolean => JBool(bool)\n+      case map: JMap[_, _] =>\n+        val jmap = map.asInstanceOf[JMap[String, Object]]\n+        JObject(jmap.entrySet().map { e => (e.getKey() -> toJValue(e.getValue())) }.toList)\n+      case array: JCollection[_] =>\n+        JArray(array.asInstanceOf[JCollection[Object]].map(o => toJValue(o)).toList)\n+      case null => JNothing\n+    }\n+  }\n+\n+  /**\n+   * Converts a JValue into its Java equivalent.\n+   */\n+  def toJavaObject(v: JValue): Object = {\n+    v match {\n+      case JNothing => null\n+      case JNull => null\n+      case JString(s) => s\n+      case JDouble(num) => java.lang.Double.valueOf(num)\n+      case JDecimal(num) => num.bigDecimal\n+      case JInt(num) => java.lang.Long.valueOf(num.longValue)\n+      case JBool(value) => java.lang.Boolean.valueOf(value)\n+      case obj: JObject => toJavaMap(obj)\n+      case JArray(vals) => {\n+        val list = new JArrayList[Object]()\n+        vals.foreach(x => list.add(toJavaObject(x)))\n+        list\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Converts a json4s list of fields into a Java Map suitable for serialization by Jackson,\n+   * which is used by the ATS client library.\n+   */\n+  def toJavaMap(sourceObj: JObject): JHashMap[String, Object] = {\n+    val map = new JHashMap[String, Object]()\n+    sourceObj.obj.foreach(f => map.put(f._1, toJavaObject(f._2)))\n+    map\n+  }\n+\n+  /**\n+   * Convert a timeline event to a spark one. Includes\n+   * some basic checks for validity of event payload.\n+   * @param event timeline event\n+   * @return an unmarshalled event\n+   */\n+  def toSparkEvent(event: TimelineEvent): SparkListenerEvent = {\n+    val info = event.getEventInfo()\n+    if (info == null) {\n+      throw new IOException(E_NO_EVENTINFO)\n+    }\n+    if (info.size() == 0) {\n+      throw new IOException(E_EMPTY_EVENTINFO)\n+    }\n+    val payload = toJValue(info)\n+    def jsonToString: String = {\n+      val json = compact(render(payload))\n+      val limit = 256\n+      if (json.length < limit) {\n+        json\n+      } else {\n+          json.substring(0, limit) + \" ... }\"\n+        }\n+    }\n+    logDebug(s\"payload is ${jsonToString}\")\n+    val eventField = payload \\ \"Event\"\n+    if (eventField == JNothing) {\n+      throw new IOException(s\"No 'Event' entry in $jsonToString\")\n+    }\n+\n+    // now the real unmarshalling\n+    try {\n+      JsonProtocol.sparkEventFromJson(payload)\n+    } catch {\n+      // failure in the marshalling; include payload in the message\n+      case ex: MappingException => {\n+        logError(s\"$ex while rendering $jsonToString\", ex)\n+        throw ex\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Convert a spark event to a timeline event\n+   * @param event handled spark event\n+   * @return a timeline event\n+   */\n+  def toTimelineEvent(event: HandleSparkEvent): TimelineEvent = {\n+    val tlEvent = new TimelineEvent()\n+    tlEvent.setEventType(Utils.getFormattedClassName(event.sparkEvent)\n+        + \"-\" + YarnTimelineUtils.uid.incrementAndGet.toString)\n+    tlEvent.setTimestamp(event.time)\n+    val kvMap = new JHashMap[String, Object]()\n+    val json = JsonProtocol.sparkEventToJson(event.sparkEvent)\n+    val jObject = json.asInstanceOf[JObject]\n+    // the timeline event wants a map of java objects for Jackson to serialize\n+    val hashMap = toJavaMap(jObject)\n+    tlEvent.setEventInfo(hashMap)\n+    tlEvent\n+  }\n+\n+  /**\n+   * Describe the event for logging\n+   * @param event timeline event\n+   * @return a description\n+   */\n+  def describeEvent(event: TimelineEvent): String = {\n+    var sparkEventDetails = \"\"\n+    try {\n+      sparkEventDetails = toSparkEvent(event).toString\n+    } catch {\n+      case _: Exception =>\n+        sparkEventDetails = \"(cannot convert event details to spark exception)\"\n+    }\n+    s\"${event.getEventType()} @ ${new Date(event.getTimestamp())}\" +\n+        s\"\\n    ${sparkEventDetails}\"\n+  }\n+\n+  /**\n+   * Create details of a timeline entity, by describing every event inside it\n+   * @param entity entity containing a possibly empty or null list of events\n+   * @return a list of event details, with a newline between each one\n+   */\n+  def eventDetails(entity: TimelineEntity): String = {\n+    val events: util.List[TimelineEvent] = entity.getEvents\n+    if (events != null) {\n+      events.foldLeft(\"\") {\n+        (s, evt) => (s + \"\\n\" + describeEvent(evt))\n+      }\n+    } else {\n+      \"\"\n+    }\n+  }\n+\n+  /**\n+   * Describe a timeline entity\n+   * @param entity entity\n+   * @return a string description.\n+   */\n+  def describeEntity(entity: TimelineEntity): String = {\n+    val events: util.List[TimelineEvent] = entity.getEvents\n+    val eventSummary = if (events != null) {\n+      s\"contains ${events.size()} event(s)\"\n+    } else {\n+      \"contains no events\"\n+    }\n+\n+    val header = s\"${entity.getEntityType }/${entity.getEntityId } @${entity.getDomainId }}\"\n+    val otherInfo = entity.getOtherInfo().foldLeft(\"\\n\") (\n+      (acc, kv) => acc + s\" ${kv._1} = ${kv._2}; \"\n+    )\n+    s\"Timeline Entity \" + header +\n+        \" \" + otherInfo +\n+        \" \" + timeFieldToString(entity.getStartTime(), \"start\") +\n+        \" \" + eventSummary\n+  }\n+\n+  /**\n+   * Convert a `java.lang.Long` reference to a string value, or, if the reference is null,\n+   * to text declaring that the named field is empty\n+   * @param time time reference\n+   * @param field field name for error message\n+   * @return a string to describe the field\n+   */\n+  def timeFieldToString(time: lang.Long, field: String): String = {\n+    if (time != null) {\n+      new Date(time).toString\n+    } else {\n+       (\"no \" + field + \" time\")\n+     }\n+  }\n+\n+  /**\n+   * A verbose description of the entity which contains event details and info about\n+   * primary/secondary keys\n+   * @param entity timeline entity\n+   * @return a verbose description of the field\n+   */\n+  def describeEntityVerbose(entity: TimelineEntity): String = {\n+    val header = describeEntity(entity)\n+    val primaryFilters = entity.getPrimaryFilters.toMap\n+    var filterElements = \"\"\n+    for ((k, v) <- primaryFilters) {\n+      filterElements = filterElements +\n+        \" filter \" + k + \": [ \" + v.foldLeft(\"\")((s, o) => (s + o.toString + \" \")) +\"]\\n\"\n+    }\n+\n+    val events = eventDetails(entity)\n+    header + \"\\n\" + filterElements + events\n+  }\n+\n+  /**\n+   * Stop any optional service\n+   * @param svc service\n+   */\n+  def stopOptionalService(svc: Option[Service]): Unit = {\n+    svc match {\n+      case Some(client) => client.stop()\n+      case None =>\n+    }\n+  }\n+\n+  /**\n+   * Split a comma separated String, filter out any empty items, and return a Set of strings\n+   */\n+  def stringToSet(list: String): Set[String] = {\n+    list.split(',').map(_.trim).filter(!_.isEmpty).toSet\n+  }\n+\n+  /**\n+   * Try to get the event time off an event. Not all events have the required information\n+   * @param event event to process\n+   * @return the event time\n+   */\n+  def eventTime(event: SparkListenerEvent): Option[Long] = {\n+    event match {\n+      case evt: SparkListenerApplicationStart =>\n+        Some(evt.time)\n+      case evt: SparkListenerApplicationEnd =>\n+        Some(evt.time)\n+      case evt: SparkListenerJobStart =>\n+        Some(evt.time)\n+      case evt: SparkListenerJobEnd =>\n+        Some(evt.time)\n+      case evt: SparkListenerExecutorAdded =>\n+        Some(evt.time)\n+      case evt: SparkListenerExecutorRemoved =>\n+        Some(evt.time)\n+      case evt: SparkListenerStageSubmitted =>\n+        evt.stageInfo.submissionTime\n+      case evt: SparkListenerStageCompleted =>\n+        evt.stageInfo.completionTime\n+      case _ => None\n+    }\n+  }\n+\n+\n+  /**\n+   * Create and start a timeline client, using the configuration context to\n+   * set up the binding\n+   * @param sparkContext spark context\n+   * @return the started instance\n+   */\n+  def createTimelineClient(sparkContext: SparkContext): TimelineClient = {\n+    val client = TimelineClient.createTimelineClient\n+    client.init(sparkContext.hadoopConfiguration)\n+    client.start\n+    client\n+  }\n+\n+  /**\n+   * The path for the V1 ATS REST API\n+   */\n+  val TIMELINE_REST_PATH = s\"/ws/v1/timeline/\"\n+\n+  /**\n+   * Build the URI to the base of the timeline web application\n+   * from the Hadoop context.\n+   * <p>\n+   * Raises an exception if the address cannot be determined.\n+   * <p>\n+   * Does not perform any checks as to whether or note the timeline\n+   * service is enabled\n+   * @param conf configuration\n+   * @return the URI to the timeline service.\n+   */\n+  def getTimelineEndpoint(conf: Configuration): URI = {\n+    val isHttps = YarnConfiguration.useHttps(conf)\n+    val address = if (isHttps) {\n+      conf.get(YarnConfiguration.TIMELINE_SERVICE_WEBAPP_HTTPS_ADDRESS,\n+                YarnConfiguration.DEFAULT_TIMELINE_SERVICE_WEBAPP_HTTPS_ADDRESS)\n+    } else {\n+      conf.get(YarnConfiguration.TIMELINE_SERVICE_WEBAPP_ADDRESS,\n+                YarnConfiguration.DEFAULT_TIMELINE_SERVICE_WEBAPP_ADDRESS)\n+    }\n+    val protocol = if (isHttps) \"https://\" else \"http://\"\n+    require(address != null, s\"No timeline service defined\")\n+    URI.create(s\"$protocol${address}$TIMELINE_REST_PATH\")\n+  }\n+\n+  /**\n+   * Create a URI to the history service. This uses the entity type of\n+   * <code>YarnHistoryService.ENTITY_TYPE</code> to\n+   * @param conf\n+   * @return\n+   */\n+  def timelineWebappUri(conf: Configuration): URI = {\n+    timelineWebappUri(conf, YarnHistoryService.SPARK_EVENT_ENTITY_TYPE)\n+  }\n+\n+  /**\n+   * Get the URI of a path under the timeline web UI\n+   * @param conf configuration\n+   * @param subpath path under the root web UI\n+   * @return a URI\n+   */\n+  def timelineWebappUri(conf: Configuration, subpath: String): URI = {\n+    val base = getTimelineEndpoint(conf)\n+    new URL(base.toURL, subpath).toURI\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled(conf: Configuration): Boolean = {\n+    conf.getBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED,\n+                    YarnConfiguration.DEFAULT_TIMELINE_SERVICE_ENABLED)\n+  }\n+\n+  /**\n+   * Get the URI to an application under the timeline\n+   * (this requires the applicationID to have been used to\n+   * publish entities there)\n+   * @param timelineUri timeline URI\n+   * @param appId App ID (really, the entityId used to publish)\n+   * @return the path\n+   */\n+  def applicationURI(timelineUri: URI, appId: String): URI = {\n+    require(appId != null && !appId.isEmpty, \"No application ID\")\n+    require(!appId.contains(\"/\"), s\"Illegal character '/' in $appId\")\n+    timelineUri.resolve(s\"${timelineUri.getPath()}/$appId\")\n+  }\n+\n+  /**\n+   * Map an error code to a string. For known codes, it returns\n+   * a description; for others it just returns the error code.\n+   * \n+   * @param code error code\n+   * @return a string description for error messages\n+   */\n+  def timelineErrorCodeToString(code: Int): String ={\n+    code match {\n+      case 1 => \"No start time\"\n+      case 2 => \"IO Exception\"\n+      case 3 => \"System Filter Conflict\"\n+      case 4 => \"Access Denied\"\n+      case 5 => \"No Domain\"\n+      case 6 => \"Forbidden Relation\"\n+      case other: Int => s\"Error code $other\"\n+    }\n+  }\n+\n+  /**\n+   * Convert a timeline error response to a meaningful string\n+   * @param error error\n+   * @return text for diagnostics\n+   */\n+  def describeError(error: TimelinePutError): String = {\n+    s\"Entity ID=${error.getEntityId()}; Entity type=${error.getEntityType}\" +\n+    s\" Error code ${error.getErrorCode}\" +\n+    s\": ${timelineErrorCodeToString(error.getErrorCode)}\"\n+  }\n+\n+  /**\n+   * Describe a put response by enumerating and describing all errors\n+   * (if present. A null errors element is handles robustly)\n+   * @param response response to describe\n+   * @return text for diagnostics\n+   */\n+  def describePutResponse(response: TimelinePutResponse) : String = {\n+    val responseErrs = response.getErrors\n+    if (responseErrs!=null) {\n+      val errors: List[String] = List(s\"TimelinePutResponse with ${responseErrs.size()} errors\")\n+      for (err <- responseErrs) {\n+        errors +: describeError(err)\n+      }\n+      errors.foldLeft(\"\")((buff, elt) => buff + \"\\n\" + elt)\n+    } else {\n+      s\"TimelinePutResponse with null error list\"\n+    }\n+  }\n+\n+  /**\n+   * This is used to highlight an undefined field\n+   */\n+  val UNDEFINED_FIELD = \"Undefined\"\n+\n+  /**\n+   * Lookup a field in the `otherInfo` section of a [[TimelineEntity]]\n+   * @param en entity\n+   * @param name field name\n+   * @return the value or the string [[UNDEFINED_FIELD]] if not\n+   * @throws Exception if the field is not found\n+   */\n+  def field(en: TimelineEntity, name: String) : Object = {\n+    fieldOption(en, name) match {\n+      case Some(v) => v\n+      case None => UNDEFINED_FIELD\n+    }\n+  }\n+\n+  /**\n+   * Lookup a field in the `otherInfo` section of a [[TimelineEntity]]\n+   * @param en entity\n+   * @param name field name\n+   * @return the value\n+   * @throws Exception if the field is not found\n+   */\n+  def fieldOption(en: TimelineEntity, name: String) : Option[Object] = {\n+    var value = en.getOtherInfo().get(name)\n+    Option.apply(value)\n+  }\n+\n+  /**\n+   * Lookup a field in the `otherInfo` section of a [[TimelineEntity]]\n+   * @param en entity\n+   * @param name field name\n+   * @return the value converted to a string\n+   * @throws Exception if the field is not found\n+   */\n+  def stringFieldOption(en: TimelineEntity, name: String): Option[String] = {\n+    var value = en.getOtherInfo().get(name)\n+    if (value != null ) {\n+      Some(value.toString)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  /**\n+   * Lookup a numeric field in the `otherInfo` section of a [[TimelineEntity]],\n+   * fall back to `defval` if the field is absent or cannot be parsed\n+   * @param en entity\n+   * @param name field name\n+   * @param defval default value; default is 0L\n+   * @return the value\n+   */\n+  def numberField(en: TimelineEntity, name: String, defval:Long = 0L) : Number = {\n+    try {\n+      fieldOption(en, name) match {\n+        case Some(n: Number) => n\n+        case _ => defval\n+      }\n+    } catch {\n+      case NonFatal(e) => defval\n+    }\n+  }\n+\n+  /**\n+   * Take a sequence of timeline events and return an ordered list of spark events.\n+   * Important: this reverses the input in the process\n+   * @param events event sequence\n+   * @return spark event sequence\n+   */\n+  def asSparkEvents(events: Seq[TimelineEvent]): Seq[SparkListenerEvent] = {\n+    events.reverse.map { event =>\n+      toSparkEvent(event)\n+    }\n+  }\n+\n+  /**\n+   * Build date for display in status messages\n+   * @param timestamp time in milliseconds post-Epoch\n+   * @param unset string to use if timestamp == 0\n+   * @return a string for messages\n+   */\n+  def humanDateCurrentTZ(timestamp: Long, unset: String) : String = {\n+    if (timestamp == 0) {\n+      unset\n+    } else {\n+      val dateFormatter = DateFormat.getDateTimeInstance(DateFormat.DEFAULT,\n+                                                          DateFormat.LONG)\n+      dateFormatter.format(timestamp)\n+    }\n+  }\n+\n+  /**\n+   * Short formatted time\n+   * @param timestamp time in milliseconds post-Epoch\n+   * @param unset string to use if timestamp == 0\n+   * @return a string for messages\n+   */\n+  def timeShort(timestamp: Long, unset: String) : String = {\n+    if (timestamp == 0) {\n+      unset\n+    } else {\n+      val dateFormatter = DateFormat.getTimeInstance(DateFormat.SHORT)\n+      dateFormatter.format(timestamp)\n+    }\n+  }\n+\n+  /**\n+   * Generate the timeline entity ID from the application and attempt ID.\n+   * This is required to be unique across all entities in the timeline server\n+   * @param yarnAppId yarn application ID as passed in during creation"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "This argument is not used.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T00:40:00Z",
    "diffHunk": "@@ -0,0 +1,720 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.io.IOException\n+import java.net.{URI, URL}\n+import java.text.DateFormat\n+import java.util.concurrent.atomic.AtomicLong\n+import java.util.{ArrayList => JArrayList, Collection => JCollection, Date, HashMap => JHashMap, Map => JMap}\n+import java.{lang, util}\n+\n+import scala.collection.JavaConversions._\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.service.Service\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse.TimelinePutError\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+import org.json4s.JsonAST.JObject\n+import org.json4s._\n+import org.json4s.jackson.JsonMethods._\n+\n+import org.apache.spark.{SparkContext, Logging}\n+import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+import org.apache.spark.scheduler.{SparkListenerStageCompleted, SparkListenerStageSubmitted, SparkListenerExecutorAdded, SparkListenerExecutorRemoved, SparkListenerJobEnd, SparkListenerJobStart, SparkListenerApplicationEnd, SparkListenerApplicationStart, SparkListenerEvent}\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+/**\n+ * Utility methods for timeline classes.\n+ */\n+private[spark] object YarnTimelineUtils extends Logging {\n+\n+  /**\n+   * What attempt ID to use as the attempt ID field (not the entity ID) when\n+   * there is no attempt info\n+   */\n+  val SINGLE_ATTEMPT = \"1\"\n+\n+\n+  /**\n+   * Exception text when there is no event info data to unmarshall\n+   */\n+  val E_NO_EVENTINFO = \"No 'eventinfo' entry\"\n+\n+  /**\n+   * Exception text when there is event info entry in the timeline event, but it is empty\n+   */\n+\n+  val E_EMPTY_EVENTINFO = \"Empty 'eventinfo' entry\"\n+\n+  /**\n+   * counter incremented on every spark event to timeline event creation,\n+   * so guaranteeing uniqueness of event IDs across a single application attempt\n+   * (which is implicitly, one per JVM)\n+   */\n+  val uid = new AtomicLong(System.currentTimeMillis())\n+\n+  /**\n+   * Converts a Java object to its equivalent json4s representation.\n+   */\n+  def toJValue(obj: Object): JValue = {\n+    obj match {\n+      case str: String => JString(str)\n+      case dbl: java.lang.Double => JDouble(dbl)\n+      case dec: java.math.BigDecimal => JDecimal(dec)\n+      case int: java.lang.Integer => JInt(BigInt(int))\n+      case long: java.lang.Long => JInt(BigInt(long))\n+      case bool: java.lang.Boolean => JBool(bool)\n+      case map: JMap[_, _] =>\n+        val jmap = map.asInstanceOf[JMap[String, Object]]\n+        JObject(jmap.entrySet().map { e => (e.getKey() -> toJValue(e.getValue())) }.toList)\n+      case array: JCollection[_] =>\n+        JArray(array.asInstanceOf[JCollection[Object]].map(o => toJValue(o)).toList)\n+      case null => JNothing\n+    }\n+  }\n+\n+  /**\n+   * Converts a JValue into its Java equivalent.\n+   */\n+  def toJavaObject(v: JValue): Object = {\n+    v match {\n+      case JNothing => null\n+      case JNull => null\n+      case JString(s) => s\n+      case JDouble(num) => java.lang.Double.valueOf(num)\n+      case JDecimal(num) => num.bigDecimal\n+      case JInt(num) => java.lang.Long.valueOf(num.longValue)\n+      case JBool(value) => java.lang.Boolean.valueOf(value)\n+      case obj: JObject => toJavaMap(obj)\n+      case JArray(vals) => {\n+        val list = new JArrayList[Object]()\n+        vals.foreach(x => list.add(toJavaObject(x)))\n+        list\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Converts a json4s list of fields into a Java Map suitable for serialization by Jackson,\n+   * which is used by the ATS client library.\n+   */\n+  def toJavaMap(sourceObj: JObject): JHashMap[String, Object] = {\n+    val map = new JHashMap[String, Object]()\n+    sourceObj.obj.foreach(f => map.put(f._1, toJavaObject(f._2)))\n+    map\n+  }\n+\n+  /**\n+   * Convert a timeline event to a spark one. Includes\n+   * some basic checks for validity of event payload.\n+   * @param event timeline event\n+   * @return an unmarshalled event\n+   */\n+  def toSparkEvent(event: TimelineEvent): SparkListenerEvent = {\n+    val info = event.getEventInfo()\n+    if (info == null) {\n+      throw new IOException(E_NO_EVENTINFO)\n+    }\n+    if (info.size() == 0) {\n+      throw new IOException(E_EMPTY_EVENTINFO)\n+    }\n+    val payload = toJValue(info)\n+    def jsonToString: String = {\n+      val json = compact(render(payload))\n+      val limit = 256\n+      if (json.length < limit) {\n+        json\n+      } else {\n+          json.substring(0, limit) + \" ... }\"\n+        }\n+    }\n+    logDebug(s\"payload is ${jsonToString}\")\n+    val eventField = payload \\ \"Event\"\n+    if (eventField == JNothing) {\n+      throw new IOException(s\"No 'Event' entry in $jsonToString\")\n+    }\n+\n+    // now the real unmarshalling\n+    try {\n+      JsonProtocol.sparkEventFromJson(payload)\n+    } catch {\n+      // failure in the marshalling; include payload in the message\n+      case ex: MappingException => {\n+        logError(s\"$ex while rendering $jsonToString\", ex)\n+        throw ex\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Convert a spark event to a timeline event\n+   * @param event handled spark event\n+   * @return a timeline event\n+   */\n+  def toTimelineEvent(event: HandleSparkEvent): TimelineEvent = {\n+    val tlEvent = new TimelineEvent()\n+    tlEvent.setEventType(Utils.getFormattedClassName(event.sparkEvent)\n+        + \"-\" + YarnTimelineUtils.uid.incrementAndGet.toString)\n+    tlEvent.setTimestamp(event.time)\n+    val kvMap = new JHashMap[String, Object]()\n+    val json = JsonProtocol.sparkEventToJson(event.sparkEvent)\n+    val jObject = json.asInstanceOf[JObject]\n+    // the timeline event wants a map of java objects for Jackson to serialize\n+    val hashMap = toJavaMap(jObject)\n+    tlEvent.setEventInfo(hashMap)\n+    tlEvent\n+  }\n+\n+  /**\n+   * Describe the event for logging\n+   * @param event timeline event\n+   * @return a description\n+   */\n+  def describeEvent(event: TimelineEvent): String = {\n+    var sparkEventDetails = \"\"\n+    try {\n+      sparkEventDetails = toSparkEvent(event).toString\n+    } catch {\n+      case _: Exception =>\n+        sparkEventDetails = \"(cannot convert event details to spark exception)\"\n+    }\n+    s\"${event.getEventType()} @ ${new Date(event.getTimestamp())}\" +\n+        s\"\\n    ${sparkEventDetails}\"\n+  }\n+\n+  /**\n+   * Create details of a timeline entity, by describing every event inside it\n+   * @param entity entity containing a possibly empty or null list of events\n+   * @return a list of event details, with a newline between each one\n+   */\n+  def eventDetails(entity: TimelineEntity): String = {\n+    val events: util.List[TimelineEvent] = entity.getEvents\n+    if (events != null) {\n+      events.foldLeft(\"\") {\n+        (s, evt) => (s + \"\\n\" + describeEvent(evt))\n+      }\n+    } else {\n+      \"\"\n+    }\n+  }\n+\n+  /**\n+   * Describe a timeline entity\n+   * @param entity entity\n+   * @return a string description.\n+   */\n+  def describeEntity(entity: TimelineEntity): String = {\n+    val events: util.List[TimelineEvent] = entity.getEvents\n+    val eventSummary = if (events != null) {\n+      s\"contains ${events.size()} event(s)\"\n+    } else {\n+      \"contains no events\"\n+    }\n+\n+    val header = s\"${entity.getEntityType }/${entity.getEntityId } @${entity.getDomainId }}\"\n+    val otherInfo = entity.getOtherInfo().foldLeft(\"\\n\") (\n+      (acc, kv) => acc + s\" ${kv._1} = ${kv._2}; \"\n+    )\n+    s\"Timeline Entity \" + header +\n+        \" \" + otherInfo +\n+        \" \" + timeFieldToString(entity.getStartTime(), \"start\") +\n+        \" \" + eventSummary\n+  }\n+\n+  /**\n+   * Convert a `java.lang.Long` reference to a string value, or, if the reference is null,\n+   * to text declaring that the named field is empty\n+   * @param time time reference\n+   * @param field field name for error message\n+   * @return a string to describe the field\n+   */\n+  def timeFieldToString(time: lang.Long, field: String): String = {\n+    if (time != null) {\n+      new Date(time).toString\n+    } else {\n+       (\"no \" + field + \" time\")\n+     }\n+  }\n+\n+  /**\n+   * A verbose description of the entity which contains event details and info about\n+   * primary/secondary keys\n+   * @param entity timeline entity\n+   * @return a verbose description of the field\n+   */\n+  def describeEntityVerbose(entity: TimelineEntity): String = {\n+    val header = describeEntity(entity)\n+    val primaryFilters = entity.getPrimaryFilters.toMap\n+    var filterElements = \"\"\n+    for ((k, v) <- primaryFilters) {\n+      filterElements = filterElements +\n+        \" filter \" + k + \": [ \" + v.foldLeft(\"\")((s, o) => (s + o.toString + \" \")) +\"]\\n\"\n+    }\n+\n+    val events = eventDetails(entity)\n+    header + \"\\n\" + filterElements + events\n+  }\n+\n+  /**\n+   * Stop any optional service\n+   * @param svc service\n+   */\n+  def stopOptionalService(svc: Option[Service]): Unit = {\n+    svc match {\n+      case Some(client) => client.stop()\n+      case None =>\n+    }\n+  }\n+\n+  /**\n+   * Split a comma separated String, filter out any empty items, and return a Set of strings\n+   */\n+  def stringToSet(list: String): Set[String] = {\n+    list.split(',').map(_.trim).filter(!_.isEmpty).toSet\n+  }\n+\n+  /**\n+   * Try to get the event time off an event. Not all events have the required information\n+   * @param event event to process\n+   * @return the event time\n+   */\n+  def eventTime(event: SparkListenerEvent): Option[Long] = {\n+    event match {\n+      case evt: SparkListenerApplicationStart =>\n+        Some(evt.time)\n+      case evt: SparkListenerApplicationEnd =>\n+        Some(evt.time)\n+      case evt: SparkListenerJobStart =>\n+        Some(evt.time)\n+      case evt: SparkListenerJobEnd =>\n+        Some(evt.time)\n+      case evt: SparkListenerExecutorAdded =>\n+        Some(evt.time)\n+      case evt: SparkListenerExecutorRemoved =>\n+        Some(evt.time)\n+      case evt: SparkListenerStageSubmitted =>\n+        evt.stageInfo.submissionTime\n+      case evt: SparkListenerStageCompleted =>\n+        evt.stageInfo.completionTime\n+      case _ => None\n+    }\n+  }\n+\n+\n+  /**\n+   * Create and start a timeline client, using the configuration context to\n+   * set up the binding\n+   * @param sparkContext spark context\n+   * @return the started instance\n+   */\n+  def createTimelineClient(sparkContext: SparkContext): TimelineClient = {\n+    val client = TimelineClient.createTimelineClient\n+    client.init(sparkContext.hadoopConfiguration)\n+    client.start\n+    client\n+  }\n+\n+  /**\n+   * The path for the V1 ATS REST API\n+   */\n+  val TIMELINE_REST_PATH = s\"/ws/v1/timeline/\"\n+\n+  /**\n+   * Build the URI to the base of the timeline web application\n+   * from the Hadoop context.\n+   * <p>\n+   * Raises an exception if the address cannot be determined.\n+   * <p>\n+   * Does not perform any checks as to whether or note the timeline\n+   * service is enabled\n+   * @param conf configuration\n+   * @return the URI to the timeline service.\n+   */\n+  def getTimelineEndpoint(conf: Configuration): URI = {\n+    val isHttps = YarnConfiguration.useHttps(conf)\n+    val address = if (isHttps) {\n+      conf.get(YarnConfiguration.TIMELINE_SERVICE_WEBAPP_HTTPS_ADDRESS,\n+                YarnConfiguration.DEFAULT_TIMELINE_SERVICE_WEBAPP_HTTPS_ADDRESS)\n+    } else {\n+      conf.get(YarnConfiguration.TIMELINE_SERVICE_WEBAPP_ADDRESS,\n+                YarnConfiguration.DEFAULT_TIMELINE_SERVICE_WEBAPP_ADDRESS)\n+    }\n+    val protocol = if (isHttps) \"https://\" else \"http://\"\n+    require(address != null, s\"No timeline service defined\")\n+    URI.create(s\"$protocol${address}$TIMELINE_REST_PATH\")\n+  }\n+\n+  /**\n+   * Create a URI to the history service. This uses the entity type of\n+   * <code>YarnHistoryService.ENTITY_TYPE</code> to\n+   * @param conf\n+   * @return\n+   */\n+  def timelineWebappUri(conf: Configuration): URI = {\n+    timelineWebappUri(conf, YarnHistoryService.SPARK_EVENT_ENTITY_TYPE)\n+  }\n+\n+  /**\n+   * Get the URI of a path under the timeline web UI\n+   * @param conf configuration\n+   * @param subpath path under the root web UI\n+   * @return a URI\n+   */\n+  def timelineWebappUri(conf: Configuration, subpath: String): URI = {\n+    val base = getTimelineEndpoint(conf)\n+    new URL(base.toURL, subpath).toURI\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled(conf: Configuration): Boolean = {\n+    conf.getBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED,\n+                    YarnConfiguration.DEFAULT_TIMELINE_SERVICE_ENABLED)\n+  }\n+\n+  /**\n+   * Get the URI to an application under the timeline\n+   * (this requires the applicationID to have been used to\n+   * publish entities there)\n+   * @param timelineUri timeline URI\n+   * @param appId App ID (really, the entityId used to publish)\n+   * @return the path\n+   */\n+  def applicationURI(timelineUri: URI, appId: String): URI = {\n+    require(appId != null && !appId.isEmpty, \"No application ID\")\n+    require(!appId.contains(\"/\"), s\"Illegal character '/' in $appId\")\n+    timelineUri.resolve(s\"${timelineUri.getPath()}/$appId\")\n+  }\n+\n+  /**\n+   * Map an error code to a string. For known codes, it returns\n+   * a description; for others it just returns the error code.\n+   * \n+   * @param code error code\n+   * @return a string description for error messages\n+   */\n+  def timelineErrorCodeToString(code: Int): String ={\n+    code match {\n+      case 1 => \"No start time\"\n+      case 2 => \"IO Exception\"\n+      case 3 => \"System Filter Conflict\"\n+      case 4 => \"Access Denied\"\n+      case 5 => \"No Domain\"\n+      case 6 => \"Forbidden Relation\"\n+      case other: Int => s\"Error code $other\"\n+    }\n+  }\n+\n+  /**\n+   * Convert a timeline error response to a meaningful string\n+   * @param error error\n+   * @return text for diagnostics\n+   */\n+  def describeError(error: TimelinePutError): String = {\n+    s\"Entity ID=${error.getEntityId()}; Entity type=${error.getEntityType}\" +\n+    s\" Error code ${error.getErrorCode}\" +\n+    s\": ${timelineErrorCodeToString(error.getErrorCode)}\"\n+  }\n+\n+  /**\n+   * Describe a put response by enumerating and describing all errors\n+   * (if present. A null errors element is handles robustly)\n+   * @param response response to describe\n+   * @return text for diagnostics\n+   */\n+  def describePutResponse(response: TimelinePutResponse) : String = {\n+    val responseErrs = response.getErrors\n+    if (responseErrs!=null) {\n+      val errors: List[String] = List(s\"TimelinePutResponse with ${responseErrs.size()} errors\")\n+      for (err <- responseErrs) {\n+        errors +: describeError(err)\n+      }\n+      errors.foldLeft(\"\")((buff, elt) => buff + \"\\n\" + elt)\n+    } else {\n+      s\"TimelinePutResponse with null error list\"\n+    }\n+  }\n+\n+  /**\n+   * This is used to highlight an undefined field\n+   */\n+  val UNDEFINED_FIELD = \"Undefined\"\n+\n+  /**\n+   * Lookup a field in the `otherInfo` section of a [[TimelineEntity]]\n+   * @param en entity\n+   * @param name field name\n+   * @return the value or the string [[UNDEFINED_FIELD]] if not\n+   * @throws Exception if the field is not found\n+   */\n+  def field(en: TimelineEntity, name: String) : Object = {\n+    fieldOption(en, name) match {\n+      case Some(v) => v\n+      case None => UNDEFINED_FIELD\n+    }\n+  }\n+\n+  /**\n+   * Lookup a field in the `otherInfo` section of a [[TimelineEntity]]\n+   * @param en entity\n+   * @param name field name\n+   * @return the value\n+   * @throws Exception if the field is not found\n+   */\n+  def fieldOption(en: TimelineEntity, name: String) : Option[Object] = {\n+    var value = en.getOtherInfo().get(name)\n+    Option.apply(value)\n+  }\n+\n+  /**\n+   * Lookup a field in the `otherInfo` section of a [[TimelineEntity]]\n+   * @param en entity\n+   * @param name field name\n+   * @return the value converted to a string\n+   * @throws Exception if the field is not found\n+   */\n+  def stringFieldOption(en: TimelineEntity, name: String): Option[String] = {\n+    var value = en.getOtherInfo().get(name)\n+    if (value != null ) {\n+      Some(value.toString)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  /**\n+   * Lookup a numeric field in the `otherInfo` section of a [[TimelineEntity]],\n+   * fall back to `defval` if the field is absent or cannot be parsed\n+   * @param en entity\n+   * @param name field name\n+   * @param defval default value; default is 0L\n+   * @return the value\n+   */\n+  def numberField(en: TimelineEntity, name: String, defval:Long = 0L) : Number = {\n+    try {\n+      fieldOption(en, name) match {\n+        case Some(n: Number) => n\n+        case _ => defval\n+      }\n+    } catch {\n+      case NonFatal(e) => defval\n+    }\n+  }\n+\n+  /**\n+   * Take a sequence of timeline events and return an ordered list of spark events.\n+   * Important: this reverses the input in the process\n+   * @param events event sequence\n+   * @return spark event sequence\n+   */\n+  def asSparkEvents(events: Seq[TimelineEvent]): Seq[SparkListenerEvent] = {\n+    events.reverse.map { event =>\n+      toSparkEvent(event)\n+    }\n+  }\n+\n+  /**\n+   * Build date for display in status messages\n+   * @param timestamp time in milliseconds post-Epoch\n+   * @param unset string to use if timestamp == 0\n+   * @return a string for messages\n+   */\n+  def humanDateCurrentTZ(timestamp: Long, unset: String) : String = {\n+    if (timestamp == 0) {\n+      unset\n+    } else {\n+      val dateFormatter = DateFormat.getDateTimeInstance(DateFormat.DEFAULT,\n+                                                          DateFormat.LONG)\n+      dateFormatter.format(timestamp)\n+    }\n+  }\n+\n+  /**\n+   * Short formatted time\n+   * @param timestamp time in milliseconds post-Epoch\n+   * @param unset string to use if timestamp == 0\n+   * @return a string for messages\n+   */\n+  def timeShort(timestamp: Long, unset: String) : String = {\n+    if (timestamp == 0) {\n+      unset\n+    } else {\n+      val dateFormatter = DateFormat.getTimeInstance(DateFormat.SHORT)\n+      dateFormatter.format(timestamp)\n+    }\n+  }\n+\n+  /**\n+   * Generate the timeline entity ID from the application and attempt ID.\n+   * This is required to be unique across all entities in the timeline server\n+   * @param yarnAppId yarn application ID as passed in during creation\n+   * @param yarnAttemptId YARN attempt ID as passed in during creation\n+   * @param sparkAppId application ID in the application start event\n+   * @param attemptId attempt ID in the application start event.\n+   */\n+  def buildEntityId(yarnAppId: ApplicationId,\n+      yarnAttemptId: Option[ApplicationAttemptId],\n+      sparkAppId: Option[String],\n+      attemptId: Option[String]): String = {\n+    yarnAttemptId match {\n+      case Some(aid) => aid.toString\n+      case None => yarnAppId.toString\n+    }\n+  }\n+\n+  /**\n+   * Generate the application ID for use in entity fields from the application and attempt ID.\n+   * @param yarnAppId yarn application ID as passed in during creation\n+   * @param sparkAppId application ID as submitted in the application start event"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "This is all related to to the move to app-attempt logic; the choice of which ID to use is implemented entirely inside this code, rather than in tests or the rest of the application; it let me switch between appId, yarnApp id and the one which came in with the spark attempt.\n\nNow that things are fixed and working with yarn-app-id -> entity ID, I'm happy to pull all this out and go direct. Shall I?\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T17:25:57Z",
    "diffHunk": "@@ -0,0 +1,720 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.io.IOException\n+import java.net.{URI, URL}\n+import java.text.DateFormat\n+import java.util.concurrent.atomic.AtomicLong\n+import java.util.{ArrayList => JArrayList, Collection => JCollection, Date, HashMap => JHashMap, Map => JMap}\n+import java.{lang, util}\n+\n+import scala.collection.JavaConversions._\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.service.Service\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse.TimelinePutError\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+import org.json4s.JsonAST.JObject\n+import org.json4s._\n+import org.json4s.jackson.JsonMethods._\n+\n+import org.apache.spark.{SparkContext, Logging}\n+import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+import org.apache.spark.scheduler.{SparkListenerStageCompleted, SparkListenerStageSubmitted, SparkListenerExecutorAdded, SparkListenerExecutorRemoved, SparkListenerJobEnd, SparkListenerJobStart, SparkListenerApplicationEnd, SparkListenerApplicationStart, SparkListenerEvent}\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+/**\n+ * Utility methods for timeline classes.\n+ */\n+private[spark] object YarnTimelineUtils extends Logging {\n+\n+  /**\n+   * What attempt ID to use as the attempt ID field (not the entity ID) when\n+   * there is no attempt info\n+   */\n+  val SINGLE_ATTEMPT = \"1\"\n+\n+\n+  /**\n+   * Exception text when there is no event info data to unmarshall\n+   */\n+  val E_NO_EVENTINFO = \"No 'eventinfo' entry\"\n+\n+  /**\n+   * Exception text when there is event info entry in the timeline event, but it is empty\n+   */\n+\n+  val E_EMPTY_EVENTINFO = \"Empty 'eventinfo' entry\"\n+\n+  /**\n+   * counter incremented on every spark event to timeline event creation,\n+   * so guaranteeing uniqueness of event IDs across a single application attempt\n+   * (which is implicitly, one per JVM)\n+   */\n+  val uid = new AtomicLong(System.currentTimeMillis())\n+\n+  /**\n+   * Converts a Java object to its equivalent json4s representation.\n+   */\n+  def toJValue(obj: Object): JValue = {\n+    obj match {\n+      case str: String => JString(str)\n+      case dbl: java.lang.Double => JDouble(dbl)\n+      case dec: java.math.BigDecimal => JDecimal(dec)\n+      case int: java.lang.Integer => JInt(BigInt(int))\n+      case long: java.lang.Long => JInt(BigInt(long))\n+      case bool: java.lang.Boolean => JBool(bool)\n+      case map: JMap[_, _] =>\n+        val jmap = map.asInstanceOf[JMap[String, Object]]\n+        JObject(jmap.entrySet().map { e => (e.getKey() -> toJValue(e.getValue())) }.toList)\n+      case array: JCollection[_] =>\n+        JArray(array.asInstanceOf[JCollection[Object]].map(o => toJValue(o)).toList)\n+      case null => JNothing\n+    }\n+  }\n+\n+  /**\n+   * Converts a JValue into its Java equivalent.\n+   */\n+  def toJavaObject(v: JValue): Object = {\n+    v match {\n+      case JNothing => null\n+      case JNull => null\n+      case JString(s) => s\n+      case JDouble(num) => java.lang.Double.valueOf(num)\n+      case JDecimal(num) => num.bigDecimal\n+      case JInt(num) => java.lang.Long.valueOf(num.longValue)\n+      case JBool(value) => java.lang.Boolean.valueOf(value)\n+      case obj: JObject => toJavaMap(obj)\n+      case JArray(vals) => {\n+        val list = new JArrayList[Object]()\n+        vals.foreach(x => list.add(toJavaObject(x)))\n+        list\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Converts a json4s list of fields into a Java Map suitable for serialization by Jackson,\n+   * which is used by the ATS client library.\n+   */\n+  def toJavaMap(sourceObj: JObject): JHashMap[String, Object] = {\n+    val map = new JHashMap[String, Object]()\n+    sourceObj.obj.foreach(f => map.put(f._1, toJavaObject(f._2)))\n+    map\n+  }\n+\n+  /**\n+   * Convert a timeline event to a spark one. Includes\n+   * some basic checks for validity of event payload.\n+   * @param event timeline event\n+   * @return an unmarshalled event\n+   */\n+  def toSparkEvent(event: TimelineEvent): SparkListenerEvent = {\n+    val info = event.getEventInfo()\n+    if (info == null) {\n+      throw new IOException(E_NO_EVENTINFO)\n+    }\n+    if (info.size() == 0) {\n+      throw new IOException(E_EMPTY_EVENTINFO)\n+    }\n+    val payload = toJValue(info)\n+    def jsonToString: String = {\n+      val json = compact(render(payload))\n+      val limit = 256\n+      if (json.length < limit) {\n+        json\n+      } else {\n+          json.substring(0, limit) + \" ... }\"\n+        }\n+    }\n+    logDebug(s\"payload is ${jsonToString}\")\n+    val eventField = payload \\ \"Event\"\n+    if (eventField == JNothing) {\n+      throw new IOException(s\"No 'Event' entry in $jsonToString\")\n+    }\n+\n+    // now the real unmarshalling\n+    try {\n+      JsonProtocol.sparkEventFromJson(payload)\n+    } catch {\n+      // failure in the marshalling; include payload in the message\n+      case ex: MappingException => {\n+        logError(s\"$ex while rendering $jsonToString\", ex)\n+        throw ex\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Convert a spark event to a timeline event\n+   * @param event handled spark event\n+   * @return a timeline event\n+   */\n+  def toTimelineEvent(event: HandleSparkEvent): TimelineEvent = {\n+    val tlEvent = new TimelineEvent()\n+    tlEvent.setEventType(Utils.getFormattedClassName(event.sparkEvent)\n+        + \"-\" + YarnTimelineUtils.uid.incrementAndGet.toString)\n+    tlEvent.setTimestamp(event.time)\n+    val kvMap = new JHashMap[String, Object]()\n+    val json = JsonProtocol.sparkEventToJson(event.sparkEvent)\n+    val jObject = json.asInstanceOf[JObject]\n+    // the timeline event wants a map of java objects for Jackson to serialize\n+    val hashMap = toJavaMap(jObject)\n+    tlEvent.setEventInfo(hashMap)\n+    tlEvent\n+  }\n+\n+  /**\n+   * Describe the event for logging\n+   * @param event timeline event\n+   * @return a description\n+   */\n+  def describeEvent(event: TimelineEvent): String = {\n+    var sparkEventDetails = \"\"\n+    try {\n+      sparkEventDetails = toSparkEvent(event).toString\n+    } catch {\n+      case _: Exception =>\n+        sparkEventDetails = \"(cannot convert event details to spark exception)\"\n+    }\n+    s\"${event.getEventType()} @ ${new Date(event.getTimestamp())}\" +\n+        s\"\\n    ${sparkEventDetails}\"\n+  }\n+\n+  /**\n+   * Create details of a timeline entity, by describing every event inside it\n+   * @param entity entity containing a possibly empty or null list of events\n+   * @return a list of event details, with a newline between each one\n+   */\n+  def eventDetails(entity: TimelineEntity): String = {\n+    val events: util.List[TimelineEvent] = entity.getEvents\n+    if (events != null) {\n+      events.foldLeft(\"\") {\n+        (s, evt) => (s + \"\\n\" + describeEvent(evt))\n+      }\n+    } else {\n+      \"\"\n+    }\n+  }\n+\n+  /**\n+   * Describe a timeline entity\n+   * @param entity entity\n+   * @return a string description.\n+   */\n+  def describeEntity(entity: TimelineEntity): String = {\n+    val events: util.List[TimelineEvent] = entity.getEvents\n+    val eventSummary = if (events != null) {\n+      s\"contains ${events.size()} event(s)\"\n+    } else {\n+      \"contains no events\"\n+    }\n+\n+    val header = s\"${entity.getEntityType }/${entity.getEntityId } @${entity.getDomainId }}\"\n+    val otherInfo = entity.getOtherInfo().foldLeft(\"\\n\") (\n+      (acc, kv) => acc + s\" ${kv._1} = ${kv._2}; \"\n+    )\n+    s\"Timeline Entity \" + header +\n+        \" \" + otherInfo +\n+        \" \" + timeFieldToString(entity.getStartTime(), \"start\") +\n+        \" \" + eventSummary\n+  }\n+\n+  /**\n+   * Convert a `java.lang.Long` reference to a string value, or, if the reference is null,\n+   * to text declaring that the named field is empty\n+   * @param time time reference\n+   * @param field field name for error message\n+   * @return a string to describe the field\n+   */\n+  def timeFieldToString(time: lang.Long, field: String): String = {\n+    if (time != null) {\n+      new Date(time).toString\n+    } else {\n+       (\"no \" + field + \" time\")\n+     }\n+  }\n+\n+  /**\n+   * A verbose description of the entity which contains event details and info about\n+   * primary/secondary keys\n+   * @param entity timeline entity\n+   * @return a verbose description of the field\n+   */\n+  def describeEntityVerbose(entity: TimelineEntity): String = {\n+    val header = describeEntity(entity)\n+    val primaryFilters = entity.getPrimaryFilters.toMap\n+    var filterElements = \"\"\n+    for ((k, v) <- primaryFilters) {\n+      filterElements = filterElements +\n+        \" filter \" + k + \": [ \" + v.foldLeft(\"\")((s, o) => (s + o.toString + \" \")) +\"]\\n\"\n+    }\n+\n+    val events = eventDetails(entity)\n+    header + \"\\n\" + filterElements + events\n+  }\n+\n+  /**\n+   * Stop any optional service\n+   * @param svc service\n+   */\n+  def stopOptionalService(svc: Option[Service]): Unit = {\n+    svc match {\n+      case Some(client) => client.stop()\n+      case None =>\n+    }\n+  }\n+\n+  /**\n+   * Split a comma separated String, filter out any empty items, and return a Set of strings\n+   */\n+  def stringToSet(list: String): Set[String] = {\n+    list.split(',').map(_.trim).filter(!_.isEmpty).toSet\n+  }\n+\n+  /**\n+   * Try to get the event time off an event. Not all events have the required information\n+   * @param event event to process\n+   * @return the event time\n+   */\n+  def eventTime(event: SparkListenerEvent): Option[Long] = {\n+    event match {\n+      case evt: SparkListenerApplicationStart =>\n+        Some(evt.time)\n+      case evt: SparkListenerApplicationEnd =>\n+        Some(evt.time)\n+      case evt: SparkListenerJobStart =>\n+        Some(evt.time)\n+      case evt: SparkListenerJobEnd =>\n+        Some(evt.time)\n+      case evt: SparkListenerExecutorAdded =>\n+        Some(evt.time)\n+      case evt: SparkListenerExecutorRemoved =>\n+        Some(evt.time)\n+      case evt: SparkListenerStageSubmitted =>\n+        evt.stageInfo.submissionTime\n+      case evt: SparkListenerStageCompleted =>\n+        evt.stageInfo.completionTime\n+      case _ => None\n+    }\n+  }\n+\n+\n+  /**\n+   * Create and start a timeline client, using the configuration context to\n+   * set up the binding\n+   * @param sparkContext spark context\n+   * @return the started instance\n+   */\n+  def createTimelineClient(sparkContext: SparkContext): TimelineClient = {\n+    val client = TimelineClient.createTimelineClient\n+    client.init(sparkContext.hadoopConfiguration)\n+    client.start\n+    client\n+  }\n+\n+  /**\n+   * The path for the V1 ATS REST API\n+   */\n+  val TIMELINE_REST_PATH = s\"/ws/v1/timeline/\"\n+\n+  /**\n+   * Build the URI to the base of the timeline web application\n+   * from the Hadoop context.\n+   * <p>\n+   * Raises an exception if the address cannot be determined.\n+   * <p>\n+   * Does not perform any checks as to whether or note the timeline\n+   * service is enabled\n+   * @param conf configuration\n+   * @return the URI to the timeline service.\n+   */\n+  def getTimelineEndpoint(conf: Configuration): URI = {\n+    val isHttps = YarnConfiguration.useHttps(conf)\n+    val address = if (isHttps) {\n+      conf.get(YarnConfiguration.TIMELINE_SERVICE_WEBAPP_HTTPS_ADDRESS,\n+                YarnConfiguration.DEFAULT_TIMELINE_SERVICE_WEBAPP_HTTPS_ADDRESS)\n+    } else {\n+      conf.get(YarnConfiguration.TIMELINE_SERVICE_WEBAPP_ADDRESS,\n+                YarnConfiguration.DEFAULT_TIMELINE_SERVICE_WEBAPP_ADDRESS)\n+    }\n+    val protocol = if (isHttps) \"https://\" else \"http://\"\n+    require(address != null, s\"No timeline service defined\")\n+    URI.create(s\"$protocol${address}$TIMELINE_REST_PATH\")\n+  }\n+\n+  /**\n+   * Create a URI to the history service. This uses the entity type of\n+   * <code>YarnHistoryService.ENTITY_TYPE</code> to\n+   * @param conf\n+   * @return\n+   */\n+  def timelineWebappUri(conf: Configuration): URI = {\n+    timelineWebappUri(conf, YarnHistoryService.SPARK_EVENT_ENTITY_TYPE)\n+  }\n+\n+  /**\n+   * Get the URI of a path under the timeline web UI\n+   * @param conf configuration\n+   * @param subpath path under the root web UI\n+   * @return a URI\n+   */\n+  def timelineWebappUri(conf: Configuration, subpath: String): URI = {\n+    val base = getTimelineEndpoint(conf)\n+    new URL(base.toURL, subpath).toURI\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled(conf: Configuration): Boolean = {\n+    conf.getBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED,\n+                    YarnConfiguration.DEFAULT_TIMELINE_SERVICE_ENABLED)\n+  }\n+\n+  /**\n+   * Get the URI to an application under the timeline\n+   * (this requires the applicationID to have been used to\n+   * publish entities there)\n+   * @param timelineUri timeline URI\n+   * @param appId App ID (really, the entityId used to publish)\n+   * @return the path\n+   */\n+  def applicationURI(timelineUri: URI, appId: String): URI = {\n+    require(appId != null && !appId.isEmpty, \"No application ID\")\n+    require(!appId.contains(\"/\"), s\"Illegal character '/' in $appId\")\n+    timelineUri.resolve(s\"${timelineUri.getPath()}/$appId\")\n+  }\n+\n+  /**\n+   * Map an error code to a string. For known codes, it returns\n+   * a description; for others it just returns the error code.\n+   * \n+   * @param code error code\n+   * @return a string description for error messages\n+   */\n+  def timelineErrorCodeToString(code: Int): String ={\n+    code match {\n+      case 1 => \"No start time\"\n+      case 2 => \"IO Exception\"\n+      case 3 => \"System Filter Conflict\"\n+      case 4 => \"Access Denied\"\n+      case 5 => \"No Domain\"\n+      case 6 => \"Forbidden Relation\"\n+      case other: Int => s\"Error code $other\"\n+    }\n+  }\n+\n+  /**\n+   * Convert a timeline error response to a meaningful string\n+   * @param error error\n+   * @return text for diagnostics\n+   */\n+  def describeError(error: TimelinePutError): String = {\n+    s\"Entity ID=${error.getEntityId()}; Entity type=${error.getEntityType}\" +\n+    s\" Error code ${error.getErrorCode}\" +\n+    s\": ${timelineErrorCodeToString(error.getErrorCode)}\"\n+  }\n+\n+  /**\n+   * Describe a put response by enumerating and describing all errors\n+   * (if present. A null errors element is handles robustly)\n+   * @param response response to describe\n+   * @return text for diagnostics\n+   */\n+  def describePutResponse(response: TimelinePutResponse) : String = {\n+    val responseErrs = response.getErrors\n+    if (responseErrs!=null) {\n+      val errors: List[String] = List(s\"TimelinePutResponse with ${responseErrs.size()} errors\")\n+      for (err <- responseErrs) {\n+        errors +: describeError(err)\n+      }\n+      errors.foldLeft(\"\")((buff, elt) => buff + \"\\n\" + elt)\n+    } else {\n+      s\"TimelinePutResponse with null error list\"\n+    }\n+  }\n+\n+  /**\n+   * This is used to highlight an undefined field\n+   */\n+  val UNDEFINED_FIELD = \"Undefined\"\n+\n+  /**\n+   * Lookup a field in the `otherInfo` section of a [[TimelineEntity]]\n+   * @param en entity\n+   * @param name field name\n+   * @return the value or the string [[UNDEFINED_FIELD]] if not\n+   * @throws Exception if the field is not found\n+   */\n+  def field(en: TimelineEntity, name: String) : Object = {\n+    fieldOption(en, name) match {\n+      case Some(v) => v\n+      case None => UNDEFINED_FIELD\n+    }\n+  }\n+\n+  /**\n+   * Lookup a field in the `otherInfo` section of a [[TimelineEntity]]\n+   * @param en entity\n+   * @param name field name\n+   * @return the value\n+   * @throws Exception if the field is not found\n+   */\n+  def fieldOption(en: TimelineEntity, name: String) : Option[Object] = {\n+    var value = en.getOtherInfo().get(name)\n+    Option.apply(value)\n+  }\n+\n+  /**\n+   * Lookup a field in the `otherInfo` section of a [[TimelineEntity]]\n+   * @param en entity\n+   * @param name field name\n+   * @return the value converted to a string\n+   * @throws Exception if the field is not found\n+   */\n+  def stringFieldOption(en: TimelineEntity, name: String): Option[String] = {\n+    var value = en.getOtherInfo().get(name)\n+    if (value != null ) {\n+      Some(value.toString)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  /**\n+   * Lookup a numeric field in the `otherInfo` section of a [[TimelineEntity]],\n+   * fall back to `defval` if the field is absent or cannot be parsed\n+   * @param en entity\n+   * @param name field name\n+   * @param defval default value; default is 0L\n+   * @return the value\n+   */\n+  def numberField(en: TimelineEntity, name: String, defval:Long = 0L) : Number = {\n+    try {\n+      fieldOption(en, name) match {\n+        case Some(n: Number) => n\n+        case _ => defval\n+      }\n+    } catch {\n+      case NonFatal(e) => defval\n+    }\n+  }\n+\n+  /**\n+   * Take a sequence of timeline events and return an ordered list of spark events.\n+   * Important: this reverses the input in the process\n+   * @param events event sequence\n+   * @return spark event sequence\n+   */\n+  def asSparkEvents(events: Seq[TimelineEvent]): Seq[SparkListenerEvent] = {\n+    events.reverse.map { event =>\n+      toSparkEvent(event)\n+    }\n+  }\n+\n+  /**\n+   * Build date for display in status messages\n+   * @param timestamp time in milliseconds post-Epoch\n+   * @param unset string to use if timestamp == 0\n+   * @return a string for messages\n+   */\n+  def humanDateCurrentTZ(timestamp: Long, unset: String) : String = {\n+    if (timestamp == 0) {\n+      unset\n+    } else {\n+      val dateFormatter = DateFormat.getDateTimeInstance(DateFormat.DEFAULT,\n+                                                          DateFormat.LONG)\n+      dateFormatter.format(timestamp)\n+    }\n+  }\n+\n+  /**\n+   * Short formatted time\n+   * @param timestamp time in milliseconds post-Epoch\n+   * @param unset string to use if timestamp == 0\n+   * @return a string for messages\n+   */\n+  def timeShort(timestamp: Long, unset: String) : String = {\n+    if (timestamp == 0) {\n+      unset\n+    } else {\n+      val dateFormatter = DateFormat.getTimeInstance(DateFormat.SHORT)\n+      dateFormatter.format(timestamp)\n+    }\n+  }\n+\n+  /**\n+   * Generate the timeline entity ID from the application and attempt ID.\n+   * This is required to be unique across all entities in the timeline server\n+   * @param yarnAppId yarn application ID as passed in during creation\n+   * @param yarnAttemptId YARN attempt ID as passed in during creation\n+   * @param sparkAppId application ID in the application start event\n+   * @param attemptId attempt ID in the application start event.\n+   */\n+  def buildEntityId(yarnAppId: ApplicationId,\n+      yarnAttemptId: Option[ApplicationAttemptId],\n+      sparkAppId: Option[String],\n+      attemptId: Option[String]): String = {\n+    yarnAttemptId match {\n+      case Some(aid) => aid.toString\n+      case None => yarnAppId.toString\n+    }\n+  }\n+\n+  /**\n+   * Generate the application ID for use in entity fields from the application and attempt ID.\n+   * @param yarnAppId yarn application ID as passed in during creation\n+   * @param sparkAppId application ID as submitted in the application start event"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "I don't have an opinion on whether to keep the methods or not, but just would rather not require unused parameters to be passed to them.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T20:18:06Z",
    "diffHunk": "@@ -0,0 +1,720 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.io.IOException\n+import java.net.{URI, URL}\n+import java.text.DateFormat\n+import java.util.concurrent.atomic.AtomicLong\n+import java.util.{ArrayList => JArrayList, Collection => JCollection, Date, HashMap => JHashMap, Map => JMap}\n+import java.{lang, util}\n+\n+import scala.collection.JavaConversions._\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.service.Service\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse.TimelinePutError\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+import org.json4s.JsonAST.JObject\n+import org.json4s._\n+import org.json4s.jackson.JsonMethods._\n+\n+import org.apache.spark.{SparkContext, Logging}\n+import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+import org.apache.spark.scheduler.{SparkListenerStageCompleted, SparkListenerStageSubmitted, SparkListenerExecutorAdded, SparkListenerExecutorRemoved, SparkListenerJobEnd, SparkListenerJobStart, SparkListenerApplicationEnd, SparkListenerApplicationStart, SparkListenerEvent}\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+/**\n+ * Utility methods for timeline classes.\n+ */\n+private[spark] object YarnTimelineUtils extends Logging {\n+\n+  /**\n+   * What attempt ID to use as the attempt ID field (not the entity ID) when\n+   * there is no attempt info\n+   */\n+  val SINGLE_ATTEMPT = \"1\"\n+\n+\n+  /**\n+   * Exception text when there is no event info data to unmarshall\n+   */\n+  val E_NO_EVENTINFO = \"No 'eventinfo' entry\"\n+\n+  /**\n+   * Exception text when there is event info entry in the timeline event, but it is empty\n+   */\n+\n+  val E_EMPTY_EVENTINFO = \"Empty 'eventinfo' entry\"\n+\n+  /**\n+   * counter incremented on every spark event to timeline event creation,\n+   * so guaranteeing uniqueness of event IDs across a single application attempt\n+   * (which is implicitly, one per JVM)\n+   */\n+  val uid = new AtomicLong(System.currentTimeMillis())\n+\n+  /**\n+   * Converts a Java object to its equivalent json4s representation.\n+   */\n+  def toJValue(obj: Object): JValue = {\n+    obj match {\n+      case str: String => JString(str)\n+      case dbl: java.lang.Double => JDouble(dbl)\n+      case dec: java.math.BigDecimal => JDecimal(dec)\n+      case int: java.lang.Integer => JInt(BigInt(int))\n+      case long: java.lang.Long => JInt(BigInt(long))\n+      case bool: java.lang.Boolean => JBool(bool)\n+      case map: JMap[_, _] =>\n+        val jmap = map.asInstanceOf[JMap[String, Object]]\n+        JObject(jmap.entrySet().map { e => (e.getKey() -> toJValue(e.getValue())) }.toList)\n+      case array: JCollection[_] =>\n+        JArray(array.asInstanceOf[JCollection[Object]].map(o => toJValue(o)).toList)\n+      case null => JNothing\n+    }\n+  }\n+\n+  /**\n+   * Converts a JValue into its Java equivalent.\n+   */\n+  def toJavaObject(v: JValue): Object = {\n+    v match {\n+      case JNothing => null\n+      case JNull => null\n+      case JString(s) => s\n+      case JDouble(num) => java.lang.Double.valueOf(num)\n+      case JDecimal(num) => num.bigDecimal\n+      case JInt(num) => java.lang.Long.valueOf(num.longValue)\n+      case JBool(value) => java.lang.Boolean.valueOf(value)\n+      case obj: JObject => toJavaMap(obj)\n+      case JArray(vals) => {\n+        val list = new JArrayList[Object]()\n+        vals.foreach(x => list.add(toJavaObject(x)))\n+        list\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Converts a json4s list of fields into a Java Map suitable for serialization by Jackson,\n+   * which is used by the ATS client library.\n+   */\n+  def toJavaMap(sourceObj: JObject): JHashMap[String, Object] = {\n+    val map = new JHashMap[String, Object]()\n+    sourceObj.obj.foreach(f => map.put(f._1, toJavaObject(f._2)))\n+    map\n+  }\n+\n+  /**\n+   * Convert a timeline event to a spark one. Includes\n+   * some basic checks for validity of event payload.\n+   * @param event timeline event\n+   * @return an unmarshalled event\n+   */\n+  def toSparkEvent(event: TimelineEvent): SparkListenerEvent = {\n+    val info = event.getEventInfo()\n+    if (info == null) {\n+      throw new IOException(E_NO_EVENTINFO)\n+    }\n+    if (info.size() == 0) {\n+      throw new IOException(E_EMPTY_EVENTINFO)\n+    }\n+    val payload = toJValue(info)\n+    def jsonToString: String = {\n+      val json = compact(render(payload))\n+      val limit = 256\n+      if (json.length < limit) {\n+        json\n+      } else {\n+          json.substring(0, limit) + \" ... }\"\n+        }\n+    }\n+    logDebug(s\"payload is ${jsonToString}\")\n+    val eventField = payload \\ \"Event\"\n+    if (eventField == JNothing) {\n+      throw new IOException(s\"No 'Event' entry in $jsonToString\")\n+    }\n+\n+    // now the real unmarshalling\n+    try {\n+      JsonProtocol.sparkEventFromJson(payload)\n+    } catch {\n+      // failure in the marshalling; include payload in the message\n+      case ex: MappingException => {\n+        logError(s\"$ex while rendering $jsonToString\", ex)\n+        throw ex\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Convert a spark event to a timeline event\n+   * @param event handled spark event\n+   * @return a timeline event\n+   */\n+  def toTimelineEvent(event: HandleSparkEvent): TimelineEvent = {\n+    val tlEvent = new TimelineEvent()\n+    tlEvent.setEventType(Utils.getFormattedClassName(event.sparkEvent)\n+        + \"-\" + YarnTimelineUtils.uid.incrementAndGet.toString)\n+    tlEvent.setTimestamp(event.time)\n+    val kvMap = new JHashMap[String, Object]()\n+    val json = JsonProtocol.sparkEventToJson(event.sparkEvent)\n+    val jObject = json.asInstanceOf[JObject]\n+    // the timeline event wants a map of java objects for Jackson to serialize\n+    val hashMap = toJavaMap(jObject)\n+    tlEvent.setEventInfo(hashMap)\n+    tlEvent\n+  }\n+\n+  /**\n+   * Describe the event for logging\n+   * @param event timeline event\n+   * @return a description\n+   */\n+  def describeEvent(event: TimelineEvent): String = {\n+    var sparkEventDetails = \"\"\n+    try {\n+      sparkEventDetails = toSparkEvent(event).toString\n+    } catch {\n+      case _: Exception =>\n+        sparkEventDetails = \"(cannot convert event details to spark exception)\"\n+    }\n+    s\"${event.getEventType()} @ ${new Date(event.getTimestamp())}\" +\n+        s\"\\n    ${sparkEventDetails}\"\n+  }\n+\n+  /**\n+   * Create details of a timeline entity, by describing every event inside it\n+   * @param entity entity containing a possibly empty or null list of events\n+   * @return a list of event details, with a newline between each one\n+   */\n+  def eventDetails(entity: TimelineEntity): String = {\n+    val events: util.List[TimelineEvent] = entity.getEvents\n+    if (events != null) {\n+      events.foldLeft(\"\") {\n+        (s, evt) => (s + \"\\n\" + describeEvent(evt))\n+      }\n+    } else {\n+      \"\"\n+    }\n+  }\n+\n+  /**\n+   * Describe a timeline entity\n+   * @param entity entity\n+   * @return a string description.\n+   */\n+  def describeEntity(entity: TimelineEntity): String = {\n+    val events: util.List[TimelineEvent] = entity.getEvents\n+    val eventSummary = if (events != null) {\n+      s\"contains ${events.size()} event(s)\"\n+    } else {\n+      \"contains no events\"\n+    }\n+\n+    val header = s\"${entity.getEntityType }/${entity.getEntityId } @${entity.getDomainId }}\"\n+    val otherInfo = entity.getOtherInfo().foldLeft(\"\\n\") (\n+      (acc, kv) => acc + s\" ${kv._1} = ${kv._2}; \"\n+    )\n+    s\"Timeline Entity \" + header +\n+        \" \" + otherInfo +\n+        \" \" + timeFieldToString(entity.getStartTime(), \"start\") +\n+        \" \" + eventSummary\n+  }\n+\n+  /**\n+   * Convert a `java.lang.Long` reference to a string value, or, if the reference is null,\n+   * to text declaring that the named field is empty\n+   * @param time time reference\n+   * @param field field name for error message\n+   * @return a string to describe the field\n+   */\n+  def timeFieldToString(time: lang.Long, field: String): String = {\n+    if (time != null) {\n+      new Date(time).toString\n+    } else {\n+       (\"no \" + field + \" time\")\n+     }\n+  }\n+\n+  /**\n+   * A verbose description of the entity which contains event details and info about\n+   * primary/secondary keys\n+   * @param entity timeline entity\n+   * @return a verbose description of the field\n+   */\n+  def describeEntityVerbose(entity: TimelineEntity): String = {\n+    val header = describeEntity(entity)\n+    val primaryFilters = entity.getPrimaryFilters.toMap\n+    var filterElements = \"\"\n+    for ((k, v) <- primaryFilters) {\n+      filterElements = filterElements +\n+        \" filter \" + k + \": [ \" + v.foldLeft(\"\")((s, o) => (s + o.toString + \" \")) +\"]\\n\"\n+    }\n+\n+    val events = eventDetails(entity)\n+    header + \"\\n\" + filterElements + events\n+  }\n+\n+  /**\n+   * Stop any optional service\n+   * @param svc service\n+   */\n+  def stopOptionalService(svc: Option[Service]): Unit = {\n+    svc match {\n+      case Some(client) => client.stop()\n+      case None =>\n+    }\n+  }\n+\n+  /**\n+   * Split a comma separated String, filter out any empty items, and return a Set of strings\n+   */\n+  def stringToSet(list: String): Set[String] = {\n+    list.split(',').map(_.trim).filter(!_.isEmpty).toSet\n+  }\n+\n+  /**\n+   * Try to get the event time off an event. Not all events have the required information\n+   * @param event event to process\n+   * @return the event time\n+   */\n+  def eventTime(event: SparkListenerEvent): Option[Long] = {\n+    event match {\n+      case evt: SparkListenerApplicationStart =>\n+        Some(evt.time)\n+      case evt: SparkListenerApplicationEnd =>\n+        Some(evt.time)\n+      case evt: SparkListenerJobStart =>\n+        Some(evt.time)\n+      case evt: SparkListenerJobEnd =>\n+        Some(evt.time)\n+      case evt: SparkListenerExecutorAdded =>\n+        Some(evt.time)\n+      case evt: SparkListenerExecutorRemoved =>\n+        Some(evt.time)\n+      case evt: SparkListenerStageSubmitted =>\n+        evt.stageInfo.submissionTime\n+      case evt: SparkListenerStageCompleted =>\n+        evt.stageInfo.completionTime\n+      case _ => None\n+    }\n+  }\n+\n+\n+  /**\n+   * Create and start a timeline client, using the configuration context to\n+   * set up the binding\n+   * @param sparkContext spark context\n+   * @return the started instance\n+   */\n+  def createTimelineClient(sparkContext: SparkContext): TimelineClient = {\n+    val client = TimelineClient.createTimelineClient\n+    client.init(sparkContext.hadoopConfiguration)\n+    client.start\n+    client\n+  }\n+\n+  /**\n+   * The path for the V1 ATS REST API\n+   */\n+  val TIMELINE_REST_PATH = s\"/ws/v1/timeline/\"\n+\n+  /**\n+   * Build the URI to the base of the timeline web application\n+   * from the Hadoop context.\n+   * <p>\n+   * Raises an exception if the address cannot be determined.\n+   * <p>\n+   * Does not perform any checks as to whether or note the timeline\n+   * service is enabled\n+   * @param conf configuration\n+   * @return the URI to the timeline service.\n+   */\n+  def getTimelineEndpoint(conf: Configuration): URI = {\n+    val isHttps = YarnConfiguration.useHttps(conf)\n+    val address = if (isHttps) {\n+      conf.get(YarnConfiguration.TIMELINE_SERVICE_WEBAPP_HTTPS_ADDRESS,\n+                YarnConfiguration.DEFAULT_TIMELINE_SERVICE_WEBAPP_HTTPS_ADDRESS)\n+    } else {\n+      conf.get(YarnConfiguration.TIMELINE_SERVICE_WEBAPP_ADDRESS,\n+                YarnConfiguration.DEFAULT_TIMELINE_SERVICE_WEBAPP_ADDRESS)\n+    }\n+    val protocol = if (isHttps) \"https://\" else \"http://\"\n+    require(address != null, s\"No timeline service defined\")\n+    URI.create(s\"$protocol${address}$TIMELINE_REST_PATH\")\n+  }\n+\n+  /**\n+   * Create a URI to the history service. This uses the entity type of\n+   * <code>YarnHistoryService.ENTITY_TYPE</code> to\n+   * @param conf\n+   * @return\n+   */\n+  def timelineWebappUri(conf: Configuration): URI = {\n+    timelineWebappUri(conf, YarnHistoryService.SPARK_EVENT_ENTITY_TYPE)\n+  }\n+\n+  /**\n+   * Get the URI of a path under the timeline web UI\n+   * @param conf configuration\n+   * @param subpath path under the root web UI\n+   * @return a URI\n+   */\n+  def timelineWebappUri(conf: Configuration, subpath: String): URI = {\n+    val base = getTimelineEndpoint(conf)\n+    new URL(base.toURL, subpath).toURI\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled(conf: Configuration): Boolean = {\n+    conf.getBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED,\n+                    YarnConfiguration.DEFAULT_TIMELINE_SERVICE_ENABLED)\n+  }\n+\n+  /**\n+   * Get the URI to an application under the timeline\n+   * (this requires the applicationID to have been used to\n+   * publish entities there)\n+   * @param timelineUri timeline URI\n+   * @param appId App ID (really, the entityId used to publish)\n+   * @return the path\n+   */\n+  def applicationURI(timelineUri: URI, appId: String): URI = {\n+    require(appId != null && !appId.isEmpty, \"No application ID\")\n+    require(!appId.contains(\"/\"), s\"Illegal character '/' in $appId\")\n+    timelineUri.resolve(s\"${timelineUri.getPath()}/$appId\")\n+  }\n+\n+  /**\n+   * Map an error code to a string. For known codes, it returns\n+   * a description; for others it just returns the error code.\n+   * \n+   * @param code error code\n+   * @return a string description for error messages\n+   */\n+  def timelineErrorCodeToString(code: Int): String ={\n+    code match {\n+      case 1 => \"No start time\"\n+      case 2 => \"IO Exception\"\n+      case 3 => \"System Filter Conflict\"\n+      case 4 => \"Access Denied\"\n+      case 5 => \"No Domain\"\n+      case 6 => \"Forbidden Relation\"\n+      case other: Int => s\"Error code $other\"\n+    }\n+  }\n+\n+  /**\n+   * Convert a timeline error response to a meaningful string\n+   * @param error error\n+   * @return text for diagnostics\n+   */\n+  def describeError(error: TimelinePutError): String = {\n+    s\"Entity ID=${error.getEntityId()}; Entity type=${error.getEntityType}\" +\n+    s\" Error code ${error.getErrorCode}\" +\n+    s\": ${timelineErrorCodeToString(error.getErrorCode)}\"\n+  }\n+\n+  /**\n+   * Describe a put response by enumerating and describing all errors\n+   * (if present. A null errors element is handles robustly)\n+   * @param response response to describe\n+   * @return text for diagnostics\n+   */\n+  def describePutResponse(response: TimelinePutResponse) : String = {\n+    val responseErrs = response.getErrors\n+    if (responseErrs!=null) {\n+      val errors: List[String] = List(s\"TimelinePutResponse with ${responseErrs.size()} errors\")\n+      for (err <- responseErrs) {\n+        errors +: describeError(err)\n+      }\n+      errors.foldLeft(\"\")((buff, elt) => buff + \"\\n\" + elt)\n+    } else {\n+      s\"TimelinePutResponse with null error list\"\n+    }\n+  }\n+\n+  /**\n+   * This is used to highlight an undefined field\n+   */\n+  val UNDEFINED_FIELD = \"Undefined\"\n+\n+  /**\n+   * Lookup a field in the `otherInfo` section of a [[TimelineEntity]]\n+   * @param en entity\n+   * @param name field name\n+   * @return the value or the string [[UNDEFINED_FIELD]] if not\n+   * @throws Exception if the field is not found\n+   */\n+  def field(en: TimelineEntity, name: String) : Object = {\n+    fieldOption(en, name) match {\n+      case Some(v) => v\n+      case None => UNDEFINED_FIELD\n+    }\n+  }\n+\n+  /**\n+   * Lookup a field in the `otherInfo` section of a [[TimelineEntity]]\n+   * @param en entity\n+   * @param name field name\n+   * @return the value\n+   * @throws Exception if the field is not found\n+   */\n+  def fieldOption(en: TimelineEntity, name: String) : Option[Object] = {\n+    var value = en.getOtherInfo().get(name)\n+    Option.apply(value)\n+  }\n+\n+  /**\n+   * Lookup a field in the `otherInfo` section of a [[TimelineEntity]]\n+   * @param en entity\n+   * @param name field name\n+   * @return the value converted to a string\n+   * @throws Exception if the field is not found\n+   */\n+  def stringFieldOption(en: TimelineEntity, name: String): Option[String] = {\n+    var value = en.getOtherInfo().get(name)\n+    if (value != null ) {\n+      Some(value.toString)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  /**\n+   * Lookup a numeric field in the `otherInfo` section of a [[TimelineEntity]],\n+   * fall back to `defval` if the field is absent or cannot be parsed\n+   * @param en entity\n+   * @param name field name\n+   * @param defval default value; default is 0L\n+   * @return the value\n+   */\n+  def numberField(en: TimelineEntity, name: String, defval:Long = 0L) : Number = {\n+    try {\n+      fieldOption(en, name) match {\n+        case Some(n: Number) => n\n+        case _ => defval\n+      }\n+    } catch {\n+      case NonFatal(e) => defval\n+    }\n+  }\n+\n+  /**\n+   * Take a sequence of timeline events and return an ordered list of spark events.\n+   * Important: this reverses the input in the process\n+   * @param events event sequence\n+   * @return spark event sequence\n+   */\n+  def asSparkEvents(events: Seq[TimelineEvent]): Seq[SparkListenerEvent] = {\n+    events.reverse.map { event =>\n+      toSparkEvent(event)\n+    }\n+  }\n+\n+  /**\n+   * Build date for display in status messages\n+   * @param timestamp time in milliseconds post-Epoch\n+   * @param unset string to use if timestamp == 0\n+   * @return a string for messages\n+   */\n+  def humanDateCurrentTZ(timestamp: Long, unset: String) : String = {\n+    if (timestamp == 0) {\n+      unset\n+    } else {\n+      val dateFormatter = DateFormat.getDateTimeInstance(DateFormat.DEFAULT,\n+                                                          DateFormat.LONG)\n+      dateFormatter.format(timestamp)\n+    }\n+  }\n+\n+  /**\n+   * Short formatted time\n+   * @param timestamp time in milliseconds post-Epoch\n+   * @param unset string to use if timestamp == 0\n+   * @return a string for messages\n+   */\n+  def timeShort(timestamp: Long, unset: String) : String = {\n+    if (timestamp == 0) {\n+      unset\n+    } else {\n+      val dateFormatter = DateFormat.getTimeInstance(DateFormat.SHORT)\n+      dateFormatter.format(timestamp)\n+    }\n+  }\n+\n+  /**\n+   * Generate the timeline entity ID from the application and attempt ID.\n+   * This is required to be unique across all entities in the timeline server\n+   * @param yarnAppId yarn application ID as passed in during creation\n+   * @param yarnAttemptId YARN attempt ID as passed in during creation\n+   * @param sparkAppId application ID in the application start event\n+   * @param attemptId attempt ID in the application start event.\n+   */\n+  def buildEntityId(yarnAppId: ApplicationId,\n+      yarnAttemptId: Option[ApplicationAttemptId],\n+      sparkAppId: Option[String],\n+      attemptId: Option[String]): String = {\n+    yarnAttemptId match {\n+      case Some(aid) => aid.toString\n+      case None => yarnAppId.toString\n+    }\n+  }\n+\n+  /**\n+   * Generate the application ID for use in entity fields from the application and attempt ID.\n+   * @param yarnAppId yarn application ID as passed in during creation\n+   * @param sparkAppId application ID as submitted in the application start event"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "This argument is not used.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T00:40:24Z",
    "diffHunk": "@@ -0,0 +1,720 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.io.IOException\n+import java.net.{URI, URL}\n+import java.text.DateFormat\n+import java.util.concurrent.atomic.AtomicLong\n+import java.util.{ArrayList => JArrayList, Collection => JCollection, Date, HashMap => JHashMap, Map => JMap}\n+import java.{lang, util}\n+\n+import scala.collection.JavaConversions._\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.service.Service\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse.TimelinePutError\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+import org.json4s.JsonAST.JObject\n+import org.json4s._\n+import org.json4s.jackson.JsonMethods._\n+\n+import org.apache.spark.{SparkContext, Logging}\n+import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+import org.apache.spark.scheduler.{SparkListenerStageCompleted, SparkListenerStageSubmitted, SparkListenerExecutorAdded, SparkListenerExecutorRemoved, SparkListenerJobEnd, SparkListenerJobStart, SparkListenerApplicationEnd, SparkListenerApplicationStart, SparkListenerEvent}\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+/**\n+ * Utility methods for timeline classes.\n+ */\n+private[spark] object YarnTimelineUtils extends Logging {\n+\n+  /**\n+   * What attempt ID to use as the attempt ID field (not the entity ID) when\n+   * there is no attempt info\n+   */\n+  val SINGLE_ATTEMPT = \"1\"\n+\n+\n+  /**\n+   * Exception text when there is no event info data to unmarshall\n+   */\n+  val E_NO_EVENTINFO = \"No 'eventinfo' entry\"\n+\n+  /**\n+   * Exception text when there is event info entry in the timeline event, but it is empty\n+   */\n+\n+  val E_EMPTY_EVENTINFO = \"Empty 'eventinfo' entry\"\n+\n+  /**\n+   * counter incremented on every spark event to timeline event creation,\n+   * so guaranteeing uniqueness of event IDs across a single application attempt\n+   * (which is implicitly, one per JVM)\n+   */\n+  val uid = new AtomicLong(System.currentTimeMillis())\n+\n+  /**\n+   * Converts a Java object to its equivalent json4s representation.\n+   */\n+  def toJValue(obj: Object): JValue = {\n+    obj match {\n+      case str: String => JString(str)\n+      case dbl: java.lang.Double => JDouble(dbl)\n+      case dec: java.math.BigDecimal => JDecimal(dec)\n+      case int: java.lang.Integer => JInt(BigInt(int))\n+      case long: java.lang.Long => JInt(BigInt(long))\n+      case bool: java.lang.Boolean => JBool(bool)\n+      case map: JMap[_, _] =>\n+        val jmap = map.asInstanceOf[JMap[String, Object]]\n+        JObject(jmap.entrySet().map { e => (e.getKey() -> toJValue(e.getValue())) }.toList)\n+      case array: JCollection[_] =>\n+        JArray(array.asInstanceOf[JCollection[Object]].map(o => toJValue(o)).toList)\n+      case null => JNothing\n+    }\n+  }\n+\n+  /**\n+   * Converts a JValue into its Java equivalent.\n+   */\n+  def toJavaObject(v: JValue): Object = {\n+    v match {\n+      case JNothing => null\n+      case JNull => null\n+      case JString(s) => s\n+      case JDouble(num) => java.lang.Double.valueOf(num)\n+      case JDecimal(num) => num.bigDecimal\n+      case JInt(num) => java.lang.Long.valueOf(num.longValue)\n+      case JBool(value) => java.lang.Boolean.valueOf(value)\n+      case obj: JObject => toJavaMap(obj)\n+      case JArray(vals) => {\n+        val list = new JArrayList[Object]()\n+        vals.foreach(x => list.add(toJavaObject(x)))\n+        list\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Converts a json4s list of fields into a Java Map suitable for serialization by Jackson,\n+   * which is used by the ATS client library.\n+   */\n+  def toJavaMap(sourceObj: JObject): JHashMap[String, Object] = {\n+    val map = new JHashMap[String, Object]()\n+    sourceObj.obj.foreach(f => map.put(f._1, toJavaObject(f._2)))\n+    map\n+  }\n+\n+  /**\n+   * Convert a timeline event to a spark one. Includes\n+   * some basic checks for validity of event payload.\n+   * @param event timeline event\n+   * @return an unmarshalled event\n+   */\n+  def toSparkEvent(event: TimelineEvent): SparkListenerEvent = {\n+    val info = event.getEventInfo()\n+    if (info == null) {\n+      throw new IOException(E_NO_EVENTINFO)\n+    }\n+    if (info.size() == 0) {\n+      throw new IOException(E_EMPTY_EVENTINFO)\n+    }\n+    val payload = toJValue(info)\n+    def jsonToString: String = {\n+      val json = compact(render(payload))\n+      val limit = 256\n+      if (json.length < limit) {\n+        json\n+      } else {\n+          json.substring(0, limit) + \" ... }\"\n+        }\n+    }\n+    logDebug(s\"payload is ${jsonToString}\")\n+    val eventField = payload \\ \"Event\"\n+    if (eventField == JNothing) {\n+      throw new IOException(s\"No 'Event' entry in $jsonToString\")\n+    }\n+\n+    // now the real unmarshalling\n+    try {\n+      JsonProtocol.sparkEventFromJson(payload)\n+    } catch {\n+      // failure in the marshalling; include payload in the message\n+      case ex: MappingException => {\n+        logError(s\"$ex while rendering $jsonToString\", ex)\n+        throw ex\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Convert a spark event to a timeline event\n+   * @param event handled spark event\n+   * @return a timeline event\n+   */\n+  def toTimelineEvent(event: HandleSparkEvent): TimelineEvent = {\n+    val tlEvent = new TimelineEvent()\n+    tlEvent.setEventType(Utils.getFormattedClassName(event.sparkEvent)\n+        + \"-\" + YarnTimelineUtils.uid.incrementAndGet.toString)\n+    tlEvent.setTimestamp(event.time)\n+    val kvMap = new JHashMap[String, Object]()\n+    val json = JsonProtocol.sparkEventToJson(event.sparkEvent)\n+    val jObject = json.asInstanceOf[JObject]\n+    // the timeline event wants a map of java objects for Jackson to serialize\n+    val hashMap = toJavaMap(jObject)\n+    tlEvent.setEventInfo(hashMap)\n+    tlEvent\n+  }\n+\n+  /**\n+   * Describe the event for logging\n+   * @param event timeline event\n+   * @return a description\n+   */\n+  def describeEvent(event: TimelineEvent): String = {\n+    var sparkEventDetails = \"\"\n+    try {\n+      sparkEventDetails = toSparkEvent(event).toString\n+    } catch {\n+      case _: Exception =>\n+        sparkEventDetails = \"(cannot convert event details to spark exception)\"\n+    }\n+    s\"${event.getEventType()} @ ${new Date(event.getTimestamp())}\" +\n+        s\"\\n    ${sparkEventDetails}\"\n+  }\n+\n+  /**\n+   * Create details of a timeline entity, by describing every event inside it\n+   * @param entity entity containing a possibly empty or null list of events\n+   * @return a list of event details, with a newline between each one\n+   */\n+  def eventDetails(entity: TimelineEntity): String = {\n+    val events: util.List[TimelineEvent] = entity.getEvents\n+    if (events != null) {\n+      events.foldLeft(\"\") {\n+        (s, evt) => (s + \"\\n\" + describeEvent(evt))\n+      }\n+    } else {\n+      \"\"\n+    }\n+  }\n+\n+  /**\n+   * Describe a timeline entity\n+   * @param entity entity\n+   * @return a string description.\n+   */\n+  def describeEntity(entity: TimelineEntity): String = {\n+    val events: util.List[TimelineEvent] = entity.getEvents\n+    val eventSummary = if (events != null) {\n+      s\"contains ${events.size()} event(s)\"\n+    } else {\n+      \"contains no events\"\n+    }\n+\n+    val header = s\"${entity.getEntityType }/${entity.getEntityId } @${entity.getDomainId }}\"\n+    val otherInfo = entity.getOtherInfo().foldLeft(\"\\n\") (\n+      (acc, kv) => acc + s\" ${kv._1} = ${kv._2}; \"\n+    )\n+    s\"Timeline Entity \" + header +\n+        \" \" + otherInfo +\n+        \" \" + timeFieldToString(entity.getStartTime(), \"start\") +\n+        \" \" + eventSummary\n+  }\n+\n+  /**\n+   * Convert a `java.lang.Long` reference to a string value, or, if the reference is null,\n+   * to text declaring that the named field is empty\n+   * @param time time reference\n+   * @param field field name for error message\n+   * @return a string to describe the field\n+   */\n+  def timeFieldToString(time: lang.Long, field: String): String = {\n+    if (time != null) {\n+      new Date(time).toString\n+    } else {\n+       (\"no \" + field + \" time\")\n+     }\n+  }\n+\n+  /**\n+   * A verbose description of the entity which contains event details and info about\n+   * primary/secondary keys\n+   * @param entity timeline entity\n+   * @return a verbose description of the field\n+   */\n+  def describeEntityVerbose(entity: TimelineEntity): String = {\n+    val header = describeEntity(entity)\n+    val primaryFilters = entity.getPrimaryFilters.toMap\n+    var filterElements = \"\"\n+    for ((k, v) <- primaryFilters) {\n+      filterElements = filterElements +\n+        \" filter \" + k + \": [ \" + v.foldLeft(\"\")((s, o) => (s + o.toString + \" \")) +\"]\\n\"\n+    }\n+\n+    val events = eventDetails(entity)\n+    header + \"\\n\" + filterElements + events\n+  }\n+\n+  /**\n+   * Stop any optional service\n+   * @param svc service\n+   */\n+  def stopOptionalService(svc: Option[Service]): Unit = {\n+    svc match {\n+      case Some(client) => client.stop()\n+      case None =>\n+    }\n+  }\n+\n+  /**\n+   * Split a comma separated String, filter out any empty items, and return a Set of strings\n+   */\n+  def stringToSet(list: String): Set[String] = {\n+    list.split(',').map(_.trim).filter(!_.isEmpty).toSet\n+  }\n+\n+  /**\n+   * Try to get the event time off an event. Not all events have the required information\n+   * @param event event to process\n+   * @return the event time\n+   */\n+  def eventTime(event: SparkListenerEvent): Option[Long] = {\n+    event match {\n+      case evt: SparkListenerApplicationStart =>\n+        Some(evt.time)\n+      case evt: SparkListenerApplicationEnd =>\n+        Some(evt.time)\n+      case evt: SparkListenerJobStart =>\n+        Some(evt.time)\n+      case evt: SparkListenerJobEnd =>\n+        Some(evt.time)\n+      case evt: SparkListenerExecutorAdded =>\n+        Some(evt.time)\n+      case evt: SparkListenerExecutorRemoved =>\n+        Some(evt.time)\n+      case evt: SparkListenerStageSubmitted =>\n+        evt.stageInfo.submissionTime\n+      case evt: SparkListenerStageCompleted =>\n+        evt.stageInfo.completionTime\n+      case _ => None\n+    }\n+  }\n+\n+\n+  /**\n+   * Create and start a timeline client, using the configuration context to\n+   * set up the binding\n+   * @param sparkContext spark context\n+   * @return the started instance\n+   */\n+  def createTimelineClient(sparkContext: SparkContext): TimelineClient = {\n+    val client = TimelineClient.createTimelineClient\n+    client.init(sparkContext.hadoopConfiguration)\n+    client.start\n+    client\n+  }\n+\n+  /**\n+   * The path for the V1 ATS REST API\n+   */\n+  val TIMELINE_REST_PATH = s\"/ws/v1/timeline/\"\n+\n+  /**\n+   * Build the URI to the base of the timeline web application\n+   * from the Hadoop context.\n+   * <p>\n+   * Raises an exception if the address cannot be determined.\n+   * <p>\n+   * Does not perform any checks as to whether or note the timeline\n+   * service is enabled\n+   * @param conf configuration\n+   * @return the URI to the timeline service.\n+   */\n+  def getTimelineEndpoint(conf: Configuration): URI = {\n+    val isHttps = YarnConfiguration.useHttps(conf)\n+    val address = if (isHttps) {\n+      conf.get(YarnConfiguration.TIMELINE_SERVICE_WEBAPP_HTTPS_ADDRESS,\n+                YarnConfiguration.DEFAULT_TIMELINE_SERVICE_WEBAPP_HTTPS_ADDRESS)\n+    } else {\n+      conf.get(YarnConfiguration.TIMELINE_SERVICE_WEBAPP_ADDRESS,\n+                YarnConfiguration.DEFAULT_TIMELINE_SERVICE_WEBAPP_ADDRESS)\n+    }\n+    val protocol = if (isHttps) \"https://\" else \"http://\"\n+    require(address != null, s\"No timeline service defined\")\n+    URI.create(s\"$protocol${address}$TIMELINE_REST_PATH\")\n+  }\n+\n+  /**\n+   * Create a URI to the history service. This uses the entity type of\n+   * <code>YarnHistoryService.ENTITY_TYPE</code> to\n+   * @param conf\n+   * @return\n+   */\n+  def timelineWebappUri(conf: Configuration): URI = {\n+    timelineWebappUri(conf, YarnHistoryService.SPARK_EVENT_ENTITY_TYPE)\n+  }\n+\n+  /**\n+   * Get the URI of a path under the timeline web UI\n+   * @param conf configuration\n+   * @param subpath path under the root web UI\n+   * @return a URI\n+   */\n+  def timelineWebappUri(conf: Configuration, subpath: String): URI = {\n+    val base = getTimelineEndpoint(conf)\n+    new URL(base.toURL, subpath).toURI\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled(conf: Configuration): Boolean = {\n+    conf.getBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED,\n+                    YarnConfiguration.DEFAULT_TIMELINE_SERVICE_ENABLED)\n+  }\n+\n+  /**\n+   * Get the URI to an application under the timeline\n+   * (this requires the applicationID to have been used to\n+   * publish entities there)\n+   * @param timelineUri timeline URI\n+   * @param appId App ID (really, the entityId used to publish)\n+   * @return the path\n+   */\n+  def applicationURI(timelineUri: URI, appId: String): URI = {\n+    require(appId != null && !appId.isEmpty, \"No application ID\")\n+    require(!appId.contains(\"/\"), s\"Illegal character '/' in $appId\")\n+    timelineUri.resolve(s\"${timelineUri.getPath()}/$appId\")\n+  }\n+\n+  /**\n+   * Map an error code to a string. For known codes, it returns\n+   * a description; for others it just returns the error code.\n+   * \n+   * @param code error code\n+   * @return a string description for error messages\n+   */\n+  def timelineErrorCodeToString(code: Int): String ={\n+    code match {\n+      case 1 => \"No start time\"\n+      case 2 => \"IO Exception\"\n+      case 3 => \"System Filter Conflict\"\n+      case 4 => \"Access Denied\"\n+      case 5 => \"No Domain\"\n+      case 6 => \"Forbidden Relation\"\n+      case other: Int => s\"Error code $other\"\n+    }\n+  }\n+\n+  /**\n+   * Convert a timeline error response to a meaningful string\n+   * @param error error\n+   * @return text for diagnostics\n+   */\n+  def describeError(error: TimelinePutError): String = {\n+    s\"Entity ID=${error.getEntityId()}; Entity type=${error.getEntityType}\" +\n+    s\" Error code ${error.getErrorCode}\" +\n+    s\": ${timelineErrorCodeToString(error.getErrorCode)}\"\n+  }\n+\n+  /**\n+   * Describe a put response by enumerating and describing all errors\n+   * (if present. A null errors element is handles robustly)\n+   * @param response response to describe\n+   * @return text for diagnostics\n+   */\n+  def describePutResponse(response: TimelinePutResponse) : String = {\n+    val responseErrs = response.getErrors\n+    if (responseErrs!=null) {\n+      val errors: List[String] = List(s\"TimelinePutResponse with ${responseErrs.size()} errors\")\n+      for (err <- responseErrs) {\n+        errors +: describeError(err)\n+      }\n+      errors.foldLeft(\"\")((buff, elt) => buff + \"\\n\" + elt)\n+    } else {\n+      s\"TimelinePutResponse with null error list\"\n+    }\n+  }\n+\n+  /**\n+   * This is used to highlight an undefined field\n+   */\n+  val UNDEFINED_FIELD = \"Undefined\"\n+\n+  /**\n+   * Lookup a field in the `otherInfo` section of a [[TimelineEntity]]\n+   * @param en entity\n+   * @param name field name\n+   * @return the value or the string [[UNDEFINED_FIELD]] if not\n+   * @throws Exception if the field is not found\n+   */\n+  def field(en: TimelineEntity, name: String) : Object = {\n+    fieldOption(en, name) match {\n+      case Some(v) => v\n+      case None => UNDEFINED_FIELD\n+    }\n+  }\n+\n+  /**\n+   * Lookup a field in the `otherInfo` section of a [[TimelineEntity]]\n+   * @param en entity\n+   * @param name field name\n+   * @return the value\n+   * @throws Exception if the field is not found\n+   */\n+  def fieldOption(en: TimelineEntity, name: String) : Option[Object] = {\n+    var value = en.getOtherInfo().get(name)\n+    Option.apply(value)\n+  }\n+\n+  /**\n+   * Lookup a field in the `otherInfo` section of a [[TimelineEntity]]\n+   * @param en entity\n+   * @param name field name\n+   * @return the value converted to a string\n+   * @throws Exception if the field is not found\n+   */\n+  def stringFieldOption(en: TimelineEntity, name: String): Option[String] = {\n+    var value = en.getOtherInfo().get(name)\n+    if (value != null ) {\n+      Some(value.toString)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  /**\n+   * Lookup a numeric field in the `otherInfo` section of a [[TimelineEntity]],\n+   * fall back to `defval` if the field is absent or cannot be parsed\n+   * @param en entity\n+   * @param name field name\n+   * @param defval default value; default is 0L\n+   * @return the value\n+   */\n+  def numberField(en: TimelineEntity, name: String, defval:Long = 0L) : Number = {\n+    try {\n+      fieldOption(en, name) match {\n+        case Some(n: Number) => n\n+        case _ => defval\n+      }\n+    } catch {\n+      case NonFatal(e) => defval\n+    }\n+  }\n+\n+  /**\n+   * Take a sequence of timeline events and return an ordered list of spark events.\n+   * Important: this reverses the input in the process\n+   * @param events event sequence\n+   * @return spark event sequence\n+   */\n+  def asSparkEvents(events: Seq[TimelineEvent]): Seq[SparkListenerEvent] = {\n+    events.reverse.map { event =>\n+      toSparkEvent(event)\n+    }\n+  }\n+\n+  /**\n+   * Build date for display in status messages\n+   * @param timestamp time in milliseconds post-Epoch\n+   * @param unset string to use if timestamp == 0\n+   * @return a string for messages\n+   */\n+  def humanDateCurrentTZ(timestamp: Long, unset: String) : String = {\n+    if (timestamp == 0) {\n+      unset\n+    } else {\n+      val dateFormatter = DateFormat.getDateTimeInstance(DateFormat.DEFAULT,\n+                                                          DateFormat.LONG)\n+      dateFormatter.format(timestamp)\n+    }\n+  }\n+\n+  /**\n+   * Short formatted time\n+   * @param timestamp time in milliseconds post-Epoch\n+   * @param unset string to use if timestamp == 0\n+   * @return a string for messages\n+   */\n+  def timeShort(timestamp: Long, unset: String) : String = {\n+    if (timestamp == 0) {\n+      unset\n+    } else {\n+      val dateFormatter = DateFormat.getTimeInstance(DateFormat.SHORT)\n+      dateFormatter.format(timestamp)\n+    }\n+  }\n+\n+  /**\n+   * Generate the timeline entity ID from the application and attempt ID.\n+   * This is required to be unique across all entities in the timeline server\n+   * @param yarnAppId yarn application ID as passed in during creation\n+   * @param yarnAttemptId YARN attempt ID as passed in during creation\n+   * @param sparkAppId application ID in the application start event\n+   * @param attemptId attempt ID in the application start event.\n+   */\n+  def buildEntityId(yarnAppId: ApplicationId,\n+      yarnAttemptId: Option[ApplicationAttemptId],\n+      sparkAppId: Option[String],\n+      attemptId: Option[String]): String = {\n+    yarnAttemptId match {\n+      case Some(aid) => aid.toString\n+      case None => yarnAppId.toString\n+    }\n+  }\n+\n+  /**\n+   * Generate the application ID for use in entity fields from the application and attempt ID.\n+   * @param yarnAppId yarn application ID as passed in during creation\n+   * @param sparkAppId application ID as submitted in the application start event\n+   */\n+  def buildApplicationIdField(yarnAppId: ApplicationId, sparkAppId: Option[String]): String = {\n+    yarnAppId.toString\n+  }\n+\n+  /**\n+   * Generate an attempt ID for use in the timeline entity \"other/app_id\" field\n+   * from the application and attempt ID.\n+   *\n+   * This is not guaranteed to be unique across all entities. It is\n+   * only required to be unique across all attempts of an application.\n+   *\n+   * If the application doesn't have an attempt ID, then it is\n+   * an application instance which, implicitly, is single-attempt.\n+   * The value [[SINGLE_ATTEMPT]] is returned\n+   * @param sparkAppId application ID as submitted in the application start event"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "its from the time I was trying to work out what was the appropriate ids to use for entities, apps & attempts; took a while to get right, and isolating in some methods (with tests) kept things under control. Given that the spark attempt Id can't be used for entity keys, I'll cut it, and, where appropriate, the same elsewhere, except for the overall `createTimelineEntity()` method which takes everything and makes the id->entity decisions.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-10-21T20:03:46Z",
    "diffHunk": "@@ -0,0 +1,720 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.io.IOException\n+import java.net.{URI, URL}\n+import java.text.DateFormat\n+import java.util.concurrent.atomic.AtomicLong\n+import java.util.{ArrayList => JArrayList, Collection => JCollection, Date, HashMap => JHashMap, Map => JMap}\n+import java.{lang, util}\n+\n+import scala.collection.JavaConversions._\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.service.Service\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse.TimelinePutError\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+import org.json4s.JsonAST.JObject\n+import org.json4s._\n+import org.json4s.jackson.JsonMethods._\n+\n+import org.apache.spark.{SparkContext, Logging}\n+import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+import org.apache.spark.scheduler.{SparkListenerStageCompleted, SparkListenerStageSubmitted, SparkListenerExecutorAdded, SparkListenerExecutorRemoved, SparkListenerJobEnd, SparkListenerJobStart, SparkListenerApplicationEnd, SparkListenerApplicationStart, SparkListenerEvent}\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+/**\n+ * Utility methods for timeline classes.\n+ */\n+private[spark] object YarnTimelineUtils extends Logging {\n+\n+  /**\n+   * What attempt ID to use as the attempt ID field (not the entity ID) when\n+   * there is no attempt info\n+   */\n+  val SINGLE_ATTEMPT = \"1\"\n+\n+\n+  /**\n+   * Exception text when there is no event info data to unmarshall\n+   */\n+  val E_NO_EVENTINFO = \"No 'eventinfo' entry\"\n+\n+  /**\n+   * Exception text when there is event info entry in the timeline event, but it is empty\n+   */\n+\n+  val E_EMPTY_EVENTINFO = \"Empty 'eventinfo' entry\"\n+\n+  /**\n+   * counter incremented on every spark event to timeline event creation,\n+   * so guaranteeing uniqueness of event IDs across a single application attempt\n+   * (which is implicitly, one per JVM)\n+   */\n+  val uid = new AtomicLong(System.currentTimeMillis())\n+\n+  /**\n+   * Converts a Java object to its equivalent json4s representation.\n+   */\n+  def toJValue(obj: Object): JValue = {\n+    obj match {\n+      case str: String => JString(str)\n+      case dbl: java.lang.Double => JDouble(dbl)\n+      case dec: java.math.BigDecimal => JDecimal(dec)\n+      case int: java.lang.Integer => JInt(BigInt(int))\n+      case long: java.lang.Long => JInt(BigInt(long))\n+      case bool: java.lang.Boolean => JBool(bool)\n+      case map: JMap[_, _] =>\n+        val jmap = map.asInstanceOf[JMap[String, Object]]\n+        JObject(jmap.entrySet().map { e => (e.getKey() -> toJValue(e.getValue())) }.toList)\n+      case array: JCollection[_] =>\n+        JArray(array.asInstanceOf[JCollection[Object]].map(o => toJValue(o)).toList)\n+      case null => JNothing\n+    }\n+  }\n+\n+  /**\n+   * Converts a JValue into its Java equivalent.\n+   */\n+  def toJavaObject(v: JValue): Object = {\n+    v match {\n+      case JNothing => null\n+      case JNull => null\n+      case JString(s) => s\n+      case JDouble(num) => java.lang.Double.valueOf(num)\n+      case JDecimal(num) => num.bigDecimal\n+      case JInt(num) => java.lang.Long.valueOf(num.longValue)\n+      case JBool(value) => java.lang.Boolean.valueOf(value)\n+      case obj: JObject => toJavaMap(obj)\n+      case JArray(vals) => {\n+        val list = new JArrayList[Object]()\n+        vals.foreach(x => list.add(toJavaObject(x)))\n+        list\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Converts a json4s list of fields into a Java Map suitable for serialization by Jackson,\n+   * which is used by the ATS client library.\n+   */\n+  def toJavaMap(sourceObj: JObject): JHashMap[String, Object] = {\n+    val map = new JHashMap[String, Object]()\n+    sourceObj.obj.foreach(f => map.put(f._1, toJavaObject(f._2)))\n+    map\n+  }\n+\n+  /**\n+   * Convert a timeline event to a spark one. Includes\n+   * some basic checks for validity of event payload.\n+   * @param event timeline event\n+   * @return an unmarshalled event\n+   */\n+  def toSparkEvent(event: TimelineEvent): SparkListenerEvent = {\n+    val info = event.getEventInfo()\n+    if (info == null) {\n+      throw new IOException(E_NO_EVENTINFO)\n+    }\n+    if (info.size() == 0) {\n+      throw new IOException(E_EMPTY_EVENTINFO)\n+    }\n+    val payload = toJValue(info)\n+    def jsonToString: String = {\n+      val json = compact(render(payload))\n+      val limit = 256\n+      if (json.length < limit) {\n+        json\n+      } else {\n+          json.substring(0, limit) + \" ... }\"\n+        }\n+    }\n+    logDebug(s\"payload is ${jsonToString}\")\n+    val eventField = payload \\ \"Event\"\n+    if (eventField == JNothing) {\n+      throw new IOException(s\"No 'Event' entry in $jsonToString\")\n+    }\n+\n+    // now the real unmarshalling\n+    try {\n+      JsonProtocol.sparkEventFromJson(payload)\n+    } catch {\n+      // failure in the marshalling; include payload in the message\n+      case ex: MappingException => {\n+        logError(s\"$ex while rendering $jsonToString\", ex)\n+        throw ex\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Convert a spark event to a timeline event\n+   * @param event handled spark event\n+   * @return a timeline event\n+   */\n+  def toTimelineEvent(event: HandleSparkEvent): TimelineEvent = {\n+    val tlEvent = new TimelineEvent()\n+    tlEvent.setEventType(Utils.getFormattedClassName(event.sparkEvent)\n+        + \"-\" + YarnTimelineUtils.uid.incrementAndGet.toString)\n+    tlEvent.setTimestamp(event.time)\n+    val kvMap = new JHashMap[String, Object]()\n+    val json = JsonProtocol.sparkEventToJson(event.sparkEvent)\n+    val jObject = json.asInstanceOf[JObject]\n+    // the timeline event wants a map of java objects for Jackson to serialize\n+    val hashMap = toJavaMap(jObject)\n+    tlEvent.setEventInfo(hashMap)\n+    tlEvent\n+  }\n+\n+  /**\n+   * Describe the event for logging\n+   * @param event timeline event\n+   * @return a description\n+   */\n+  def describeEvent(event: TimelineEvent): String = {\n+    var sparkEventDetails = \"\"\n+    try {\n+      sparkEventDetails = toSparkEvent(event).toString\n+    } catch {\n+      case _: Exception =>\n+        sparkEventDetails = \"(cannot convert event details to spark exception)\"\n+    }\n+    s\"${event.getEventType()} @ ${new Date(event.getTimestamp())}\" +\n+        s\"\\n    ${sparkEventDetails}\"\n+  }\n+\n+  /**\n+   * Create details of a timeline entity, by describing every event inside it\n+   * @param entity entity containing a possibly empty or null list of events\n+   * @return a list of event details, with a newline between each one\n+   */\n+  def eventDetails(entity: TimelineEntity): String = {\n+    val events: util.List[TimelineEvent] = entity.getEvents\n+    if (events != null) {\n+      events.foldLeft(\"\") {\n+        (s, evt) => (s + \"\\n\" + describeEvent(evt))\n+      }\n+    } else {\n+      \"\"\n+    }\n+  }\n+\n+  /**\n+   * Describe a timeline entity\n+   * @param entity entity\n+   * @return a string description.\n+   */\n+  def describeEntity(entity: TimelineEntity): String = {\n+    val events: util.List[TimelineEvent] = entity.getEvents\n+    val eventSummary = if (events != null) {\n+      s\"contains ${events.size()} event(s)\"\n+    } else {\n+      \"contains no events\"\n+    }\n+\n+    val header = s\"${entity.getEntityType }/${entity.getEntityId } @${entity.getDomainId }}\"\n+    val otherInfo = entity.getOtherInfo().foldLeft(\"\\n\") (\n+      (acc, kv) => acc + s\" ${kv._1} = ${kv._2}; \"\n+    )\n+    s\"Timeline Entity \" + header +\n+        \" \" + otherInfo +\n+        \" \" + timeFieldToString(entity.getStartTime(), \"start\") +\n+        \" \" + eventSummary\n+  }\n+\n+  /**\n+   * Convert a `java.lang.Long` reference to a string value, or, if the reference is null,\n+   * to text declaring that the named field is empty\n+   * @param time time reference\n+   * @param field field name for error message\n+   * @return a string to describe the field\n+   */\n+  def timeFieldToString(time: lang.Long, field: String): String = {\n+    if (time != null) {\n+      new Date(time).toString\n+    } else {\n+       (\"no \" + field + \" time\")\n+     }\n+  }\n+\n+  /**\n+   * A verbose description of the entity which contains event details and info about\n+   * primary/secondary keys\n+   * @param entity timeline entity\n+   * @return a verbose description of the field\n+   */\n+  def describeEntityVerbose(entity: TimelineEntity): String = {\n+    val header = describeEntity(entity)\n+    val primaryFilters = entity.getPrimaryFilters.toMap\n+    var filterElements = \"\"\n+    for ((k, v) <- primaryFilters) {\n+      filterElements = filterElements +\n+        \" filter \" + k + \": [ \" + v.foldLeft(\"\")((s, o) => (s + o.toString + \" \")) +\"]\\n\"\n+    }\n+\n+    val events = eventDetails(entity)\n+    header + \"\\n\" + filterElements + events\n+  }\n+\n+  /**\n+   * Stop any optional service\n+   * @param svc service\n+   */\n+  def stopOptionalService(svc: Option[Service]): Unit = {\n+    svc match {\n+      case Some(client) => client.stop()\n+      case None =>\n+    }\n+  }\n+\n+  /**\n+   * Split a comma separated String, filter out any empty items, and return a Set of strings\n+   */\n+  def stringToSet(list: String): Set[String] = {\n+    list.split(',').map(_.trim).filter(!_.isEmpty).toSet\n+  }\n+\n+  /**\n+   * Try to get the event time off an event. Not all events have the required information\n+   * @param event event to process\n+   * @return the event time\n+   */\n+  def eventTime(event: SparkListenerEvent): Option[Long] = {\n+    event match {\n+      case evt: SparkListenerApplicationStart =>\n+        Some(evt.time)\n+      case evt: SparkListenerApplicationEnd =>\n+        Some(evt.time)\n+      case evt: SparkListenerJobStart =>\n+        Some(evt.time)\n+      case evt: SparkListenerJobEnd =>\n+        Some(evt.time)\n+      case evt: SparkListenerExecutorAdded =>\n+        Some(evt.time)\n+      case evt: SparkListenerExecutorRemoved =>\n+        Some(evt.time)\n+      case evt: SparkListenerStageSubmitted =>\n+        evt.stageInfo.submissionTime\n+      case evt: SparkListenerStageCompleted =>\n+        evt.stageInfo.completionTime\n+      case _ => None\n+    }\n+  }\n+\n+\n+  /**\n+   * Create and start a timeline client, using the configuration context to\n+   * set up the binding\n+   * @param sparkContext spark context\n+   * @return the started instance\n+   */\n+  def createTimelineClient(sparkContext: SparkContext): TimelineClient = {\n+    val client = TimelineClient.createTimelineClient\n+    client.init(sparkContext.hadoopConfiguration)\n+    client.start\n+    client\n+  }\n+\n+  /**\n+   * The path for the V1 ATS REST API\n+   */\n+  val TIMELINE_REST_PATH = s\"/ws/v1/timeline/\"\n+\n+  /**\n+   * Build the URI to the base of the timeline web application\n+   * from the Hadoop context.\n+   * <p>\n+   * Raises an exception if the address cannot be determined.\n+   * <p>\n+   * Does not perform any checks as to whether or note the timeline\n+   * service is enabled\n+   * @param conf configuration\n+   * @return the URI to the timeline service.\n+   */\n+  def getTimelineEndpoint(conf: Configuration): URI = {\n+    val isHttps = YarnConfiguration.useHttps(conf)\n+    val address = if (isHttps) {\n+      conf.get(YarnConfiguration.TIMELINE_SERVICE_WEBAPP_HTTPS_ADDRESS,\n+                YarnConfiguration.DEFAULT_TIMELINE_SERVICE_WEBAPP_HTTPS_ADDRESS)\n+    } else {\n+      conf.get(YarnConfiguration.TIMELINE_SERVICE_WEBAPP_ADDRESS,\n+                YarnConfiguration.DEFAULT_TIMELINE_SERVICE_WEBAPP_ADDRESS)\n+    }\n+    val protocol = if (isHttps) \"https://\" else \"http://\"\n+    require(address != null, s\"No timeline service defined\")\n+    URI.create(s\"$protocol${address}$TIMELINE_REST_PATH\")\n+  }\n+\n+  /**\n+   * Create a URI to the history service. This uses the entity type of\n+   * <code>YarnHistoryService.ENTITY_TYPE</code> to\n+   * @param conf\n+   * @return\n+   */\n+  def timelineWebappUri(conf: Configuration): URI = {\n+    timelineWebappUri(conf, YarnHistoryService.SPARK_EVENT_ENTITY_TYPE)\n+  }\n+\n+  /**\n+   * Get the URI of a path under the timeline web UI\n+   * @param conf configuration\n+   * @param subpath path under the root web UI\n+   * @return a URI\n+   */\n+  def timelineWebappUri(conf: Configuration, subpath: String): URI = {\n+    val base = getTimelineEndpoint(conf)\n+    new URL(base.toURL, subpath).toURI\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled(conf: Configuration): Boolean = {\n+    conf.getBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED,\n+                    YarnConfiguration.DEFAULT_TIMELINE_SERVICE_ENABLED)\n+  }\n+\n+  /**\n+   * Get the URI to an application under the timeline\n+   * (this requires the applicationID to have been used to\n+   * publish entities there)\n+   * @param timelineUri timeline URI\n+   * @param appId App ID (really, the entityId used to publish)\n+   * @return the path\n+   */\n+  def applicationURI(timelineUri: URI, appId: String): URI = {\n+    require(appId != null && !appId.isEmpty, \"No application ID\")\n+    require(!appId.contains(\"/\"), s\"Illegal character '/' in $appId\")\n+    timelineUri.resolve(s\"${timelineUri.getPath()}/$appId\")\n+  }\n+\n+  /**\n+   * Map an error code to a string. For known codes, it returns\n+   * a description; for others it just returns the error code.\n+   * \n+   * @param code error code\n+   * @return a string description for error messages\n+   */\n+  def timelineErrorCodeToString(code: Int): String ={\n+    code match {\n+      case 1 => \"No start time\"\n+      case 2 => \"IO Exception\"\n+      case 3 => \"System Filter Conflict\"\n+      case 4 => \"Access Denied\"\n+      case 5 => \"No Domain\"\n+      case 6 => \"Forbidden Relation\"\n+      case other: Int => s\"Error code $other\"\n+    }\n+  }\n+\n+  /**\n+   * Convert a timeline error response to a meaningful string\n+   * @param error error\n+   * @return text for diagnostics\n+   */\n+  def describeError(error: TimelinePutError): String = {\n+    s\"Entity ID=${error.getEntityId()}; Entity type=${error.getEntityType}\" +\n+    s\" Error code ${error.getErrorCode}\" +\n+    s\": ${timelineErrorCodeToString(error.getErrorCode)}\"\n+  }\n+\n+  /**\n+   * Describe a put response by enumerating and describing all errors\n+   * (if present. A null errors element is handles robustly)\n+   * @param response response to describe\n+   * @return text for diagnostics\n+   */\n+  def describePutResponse(response: TimelinePutResponse) : String = {\n+    val responseErrs = response.getErrors\n+    if (responseErrs!=null) {\n+      val errors: List[String] = List(s\"TimelinePutResponse with ${responseErrs.size()} errors\")\n+      for (err <- responseErrs) {\n+        errors +: describeError(err)\n+      }\n+      errors.foldLeft(\"\")((buff, elt) => buff + \"\\n\" + elt)\n+    } else {\n+      s\"TimelinePutResponse with null error list\"\n+    }\n+  }\n+\n+  /**\n+   * This is used to highlight an undefined field\n+   */\n+  val UNDEFINED_FIELD = \"Undefined\"\n+\n+  /**\n+   * Lookup a field in the `otherInfo` section of a [[TimelineEntity]]\n+   * @param en entity\n+   * @param name field name\n+   * @return the value or the string [[UNDEFINED_FIELD]] if not\n+   * @throws Exception if the field is not found\n+   */\n+  def field(en: TimelineEntity, name: String) : Object = {\n+    fieldOption(en, name) match {\n+      case Some(v) => v\n+      case None => UNDEFINED_FIELD\n+    }\n+  }\n+\n+  /**\n+   * Lookup a field in the `otherInfo` section of a [[TimelineEntity]]\n+   * @param en entity\n+   * @param name field name\n+   * @return the value\n+   * @throws Exception if the field is not found\n+   */\n+  def fieldOption(en: TimelineEntity, name: String) : Option[Object] = {\n+    var value = en.getOtherInfo().get(name)\n+    Option.apply(value)\n+  }\n+\n+  /**\n+   * Lookup a field in the `otherInfo` section of a [[TimelineEntity]]\n+   * @param en entity\n+   * @param name field name\n+   * @return the value converted to a string\n+   * @throws Exception if the field is not found\n+   */\n+  def stringFieldOption(en: TimelineEntity, name: String): Option[String] = {\n+    var value = en.getOtherInfo().get(name)\n+    if (value != null ) {\n+      Some(value.toString)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  /**\n+   * Lookup a numeric field in the `otherInfo` section of a [[TimelineEntity]],\n+   * fall back to `defval` if the field is absent or cannot be parsed\n+   * @param en entity\n+   * @param name field name\n+   * @param defval default value; default is 0L\n+   * @return the value\n+   */\n+  def numberField(en: TimelineEntity, name: String, defval:Long = 0L) : Number = {\n+    try {\n+      fieldOption(en, name) match {\n+        case Some(n: Number) => n\n+        case _ => defval\n+      }\n+    } catch {\n+      case NonFatal(e) => defval\n+    }\n+  }\n+\n+  /**\n+   * Take a sequence of timeline events and return an ordered list of spark events.\n+   * Important: this reverses the input in the process\n+   * @param events event sequence\n+   * @return spark event sequence\n+   */\n+  def asSparkEvents(events: Seq[TimelineEvent]): Seq[SparkListenerEvent] = {\n+    events.reverse.map { event =>\n+      toSparkEvent(event)\n+    }\n+  }\n+\n+  /**\n+   * Build date for display in status messages\n+   * @param timestamp time in milliseconds post-Epoch\n+   * @param unset string to use if timestamp == 0\n+   * @return a string for messages\n+   */\n+  def humanDateCurrentTZ(timestamp: Long, unset: String) : String = {\n+    if (timestamp == 0) {\n+      unset\n+    } else {\n+      val dateFormatter = DateFormat.getDateTimeInstance(DateFormat.DEFAULT,\n+                                                          DateFormat.LONG)\n+      dateFormatter.format(timestamp)\n+    }\n+  }\n+\n+  /**\n+   * Short formatted time\n+   * @param timestamp time in milliseconds post-Epoch\n+   * @param unset string to use if timestamp == 0\n+   * @return a string for messages\n+   */\n+  def timeShort(timestamp: Long, unset: String) : String = {\n+    if (timestamp == 0) {\n+      unset\n+    } else {\n+      val dateFormatter = DateFormat.getTimeInstance(DateFormat.SHORT)\n+      dateFormatter.format(timestamp)\n+    }\n+  }\n+\n+  /**\n+   * Generate the timeline entity ID from the application and attempt ID.\n+   * This is required to be unique across all entities in the timeline server\n+   * @param yarnAppId yarn application ID as passed in during creation\n+   * @param yarnAttemptId YARN attempt ID as passed in during creation\n+   * @param sparkAppId application ID in the application start event\n+   * @param attemptId attempt ID in the application start event.\n+   */\n+  def buildEntityId(yarnAppId: ApplicationId,\n+      yarnAttemptId: Option[ApplicationAttemptId],\n+      sparkAppId: Option[String],\n+      attemptId: Option[String]): String = {\n+    yarnAttemptId match {\n+      case Some(aid) => aid.toString\n+      case None => yarnAppId.toString\n+    }\n+  }\n+\n+  /**\n+   * Generate the application ID for use in entity fields from the application and attempt ID.\n+   * @param yarnAppId yarn application ID as passed in during creation\n+   * @param sparkAppId application ID as submitted in the application start event\n+   */\n+  def buildApplicationIdField(yarnAppId: ApplicationId, sparkAppId: Option[String]): String = {\n+    yarnAppId.toString\n+  }\n+\n+  /**\n+   * Generate an attempt ID for use in the timeline entity \"other/app_id\" field\n+   * from the application and attempt ID.\n+   *\n+   * This is not guaranteed to be unique across all entities. It is\n+   * only required to be unique across all attempts of an application.\n+   *\n+   * If the application doesn't have an attempt ID, then it is\n+   * an application instance which, implicitly, is single-attempt.\n+   * The value [[SINGLE_ATTEMPT]] is returned\n+   * @param sparkAppId application ID as submitted in the application start event"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: `foreach { v => ... }`\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T00:40:42Z",
    "diffHunk": "@@ -0,0 +1,720 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.io.IOException\n+import java.net.{URI, URL}\n+import java.text.DateFormat\n+import java.util.concurrent.atomic.AtomicLong\n+import java.util.{ArrayList => JArrayList, Collection => JCollection, Date, HashMap => JHashMap, Map => JMap}\n+import java.{lang, util}\n+\n+import scala.collection.JavaConversions._\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.service.Service\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse.TimelinePutError\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+import org.json4s.JsonAST.JObject\n+import org.json4s._\n+import org.json4s.jackson.JsonMethods._\n+\n+import org.apache.spark.{SparkContext, Logging}\n+import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+import org.apache.spark.scheduler.{SparkListenerStageCompleted, SparkListenerStageSubmitted, SparkListenerExecutorAdded, SparkListenerExecutorRemoved, SparkListenerJobEnd, SparkListenerJobStart, SparkListenerApplicationEnd, SparkListenerApplicationStart, SparkListenerEvent}\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+/**\n+ * Utility methods for timeline classes.\n+ */\n+private[spark] object YarnTimelineUtils extends Logging {\n+\n+  /**\n+   * What attempt ID to use as the attempt ID field (not the entity ID) when\n+   * there is no attempt info\n+   */\n+  val SINGLE_ATTEMPT = \"1\"\n+\n+\n+  /**\n+   * Exception text when there is no event info data to unmarshall\n+   */\n+  val E_NO_EVENTINFO = \"No 'eventinfo' entry\"\n+\n+  /**\n+   * Exception text when there is event info entry in the timeline event, but it is empty\n+   */\n+\n+  val E_EMPTY_EVENTINFO = \"Empty 'eventinfo' entry\"\n+\n+  /**\n+   * counter incremented on every spark event to timeline event creation,\n+   * so guaranteeing uniqueness of event IDs across a single application attempt\n+   * (which is implicitly, one per JVM)\n+   */\n+  val uid = new AtomicLong(System.currentTimeMillis())\n+\n+  /**\n+   * Converts a Java object to its equivalent json4s representation.\n+   */\n+  def toJValue(obj: Object): JValue = {\n+    obj match {\n+      case str: String => JString(str)\n+      case dbl: java.lang.Double => JDouble(dbl)\n+      case dec: java.math.BigDecimal => JDecimal(dec)\n+      case int: java.lang.Integer => JInt(BigInt(int))\n+      case long: java.lang.Long => JInt(BigInt(long))\n+      case bool: java.lang.Boolean => JBool(bool)\n+      case map: JMap[_, _] =>\n+        val jmap = map.asInstanceOf[JMap[String, Object]]\n+        JObject(jmap.entrySet().map { e => (e.getKey() -> toJValue(e.getValue())) }.toList)\n+      case array: JCollection[_] =>\n+        JArray(array.asInstanceOf[JCollection[Object]].map(o => toJValue(o)).toList)\n+      case null => JNothing\n+    }\n+  }\n+\n+  /**\n+   * Converts a JValue into its Java equivalent.\n+   */\n+  def toJavaObject(v: JValue): Object = {\n+    v match {\n+      case JNothing => null\n+      case JNull => null\n+      case JString(s) => s\n+      case JDouble(num) => java.lang.Double.valueOf(num)\n+      case JDecimal(num) => num.bigDecimal\n+      case JInt(num) => java.lang.Long.valueOf(num.longValue)\n+      case JBool(value) => java.lang.Boolean.valueOf(value)\n+      case obj: JObject => toJavaMap(obj)\n+      case JArray(vals) => {\n+        val list = new JArrayList[Object]()\n+        vals.foreach(x => list.add(toJavaObject(x)))\n+        list\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Converts a json4s list of fields into a Java Map suitable for serialization by Jackson,\n+   * which is used by the ATS client library.\n+   */\n+  def toJavaMap(sourceObj: JObject): JHashMap[String, Object] = {\n+    val map = new JHashMap[String, Object]()\n+    sourceObj.obj.foreach(f => map.put(f._1, toJavaObject(f._2)))\n+    map\n+  }\n+\n+  /**\n+   * Convert a timeline event to a spark one. Includes\n+   * some basic checks for validity of event payload.\n+   * @param event timeline event\n+   * @return an unmarshalled event\n+   */\n+  def toSparkEvent(event: TimelineEvent): SparkListenerEvent = {\n+    val info = event.getEventInfo()\n+    if (info == null) {\n+      throw new IOException(E_NO_EVENTINFO)\n+    }\n+    if (info.size() == 0) {\n+      throw new IOException(E_EMPTY_EVENTINFO)\n+    }\n+    val payload = toJValue(info)\n+    def jsonToString: String = {\n+      val json = compact(render(payload))\n+      val limit = 256\n+      if (json.length < limit) {\n+        json\n+      } else {\n+          json.substring(0, limit) + \" ... }\"\n+        }\n+    }\n+    logDebug(s\"payload is ${jsonToString}\")\n+    val eventField = payload \\ \"Event\"\n+    if (eventField == JNothing) {\n+      throw new IOException(s\"No 'Event' entry in $jsonToString\")\n+    }\n+\n+    // now the real unmarshalling\n+    try {\n+      JsonProtocol.sparkEventFromJson(payload)\n+    } catch {\n+      // failure in the marshalling; include payload in the message\n+      case ex: MappingException => {\n+        logError(s\"$ex while rendering $jsonToString\", ex)\n+        throw ex\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Convert a spark event to a timeline event\n+   * @param event handled spark event\n+   * @return a timeline event\n+   */\n+  def toTimelineEvent(event: HandleSparkEvent): TimelineEvent = {\n+    val tlEvent = new TimelineEvent()\n+    tlEvent.setEventType(Utils.getFormattedClassName(event.sparkEvent)\n+        + \"-\" + YarnTimelineUtils.uid.incrementAndGet.toString)\n+    tlEvent.setTimestamp(event.time)\n+    val kvMap = new JHashMap[String, Object]()\n+    val json = JsonProtocol.sparkEventToJson(event.sparkEvent)\n+    val jObject = json.asInstanceOf[JObject]\n+    // the timeline event wants a map of java objects for Jackson to serialize\n+    val hashMap = toJavaMap(jObject)\n+    tlEvent.setEventInfo(hashMap)\n+    tlEvent\n+  }\n+\n+  /**\n+   * Describe the event for logging\n+   * @param event timeline event\n+   * @return a description\n+   */\n+  def describeEvent(event: TimelineEvent): String = {\n+    var sparkEventDetails = \"\"\n+    try {\n+      sparkEventDetails = toSparkEvent(event).toString\n+    } catch {\n+      case _: Exception =>\n+        sparkEventDetails = \"(cannot convert event details to spark exception)\"\n+    }\n+    s\"${event.getEventType()} @ ${new Date(event.getTimestamp())}\" +\n+        s\"\\n    ${sparkEventDetails}\"\n+  }\n+\n+  /**\n+   * Create details of a timeline entity, by describing every event inside it\n+   * @param entity entity containing a possibly empty or null list of events\n+   * @return a list of event details, with a newline between each one\n+   */\n+  def eventDetails(entity: TimelineEntity): String = {\n+    val events: util.List[TimelineEvent] = entity.getEvents\n+    if (events != null) {\n+      events.foldLeft(\"\") {\n+        (s, evt) => (s + \"\\n\" + describeEvent(evt))\n+      }\n+    } else {\n+      \"\"\n+    }\n+  }\n+\n+  /**\n+   * Describe a timeline entity\n+   * @param entity entity\n+   * @return a string description.\n+   */\n+  def describeEntity(entity: TimelineEntity): String = {\n+    val events: util.List[TimelineEvent] = entity.getEvents\n+    val eventSummary = if (events != null) {\n+      s\"contains ${events.size()} event(s)\"\n+    } else {\n+      \"contains no events\"\n+    }\n+\n+    val header = s\"${entity.getEntityType }/${entity.getEntityId } @${entity.getDomainId }}\"\n+    val otherInfo = entity.getOtherInfo().foldLeft(\"\\n\") (\n+      (acc, kv) => acc + s\" ${kv._1} = ${kv._2}; \"\n+    )\n+    s\"Timeline Entity \" + header +\n+        \" \" + otherInfo +\n+        \" \" + timeFieldToString(entity.getStartTime(), \"start\") +\n+        \" \" + eventSummary\n+  }\n+\n+  /**\n+   * Convert a `java.lang.Long` reference to a string value, or, if the reference is null,\n+   * to text declaring that the named field is empty\n+   * @param time time reference\n+   * @param field field name for error message\n+   * @return a string to describe the field\n+   */\n+  def timeFieldToString(time: lang.Long, field: String): String = {\n+    if (time != null) {\n+      new Date(time).toString\n+    } else {\n+       (\"no \" + field + \" time\")\n+     }\n+  }\n+\n+  /**\n+   * A verbose description of the entity which contains event details and info about\n+   * primary/secondary keys\n+   * @param entity timeline entity\n+   * @return a verbose description of the field\n+   */\n+  def describeEntityVerbose(entity: TimelineEntity): String = {\n+    val header = describeEntity(entity)\n+    val primaryFilters = entity.getPrimaryFilters.toMap\n+    var filterElements = \"\"\n+    for ((k, v) <- primaryFilters) {\n+      filterElements = filterElements +\n+        \" filter \" + k + \": [ \" + v.foldLeft(\"\")((s, o) => (s + o.toString + \" \")) +\"]\\n\"\n+    }\n+\n+    val events = eventDetails(entity)\n+    header + \"\\n\" + filterElements + events\n+  }\n+\n+  /**\n+   * Stop any optional service\n+   * @param svc service\n+   */\n+  def stopOptionalService(svc: Option[Service]): Unit = {\n+    svc match {\n+      case Some(client) => client.stop()\n+      case None =>\n+    }\n+  }\n+\n+  /**\n+   * Split a comma separated String, filter out any empty items, and return a Set of strings\n+   */\n+  def stringToSet(list: String): Set[String] = {\n+    list.split(',').map(_.trim).filter(!_.isEmpty).toSet\n+  }\n+\n+  /**\n+   * Try to get the event time off an event. Not all events have the required information\n+   * @param event event to process\n+   * @return the event time\n+   */\n+  def eventTime(event: SparkListenerEvent): Option[Long] = {\n+    event match {\n+      case evt: SparkListenerApplicationStart =>\n+        Some(evt.time)\n+      case evt: SparkListenerApplicationEnd =>\n+        Some(evt.time)\n+      case evt: SparkListenerJobStart =>\n+        Some(evt.time)\n+      case evt: SparkListenerJobEnd =>\n+        Some(evt.time)\n+      case evt: SparkListenerExecutorAdded =>\n+        Some(evt.time)\n+      case evt: SparkListenerExecutorRemoved =>\n+        Some(evt.time)\n+      case evt: SparkListenerStageSubmitted =>\n+        evt.stageInfo.submissionTime\n+      case evt: SparkListenerStageCompleted =>\n+        evt.stageInfo.completionTime\n+      case _ => None\n+    }\n+  }\n+\n+\n+  /**\n+   * Create and start a timeline client, using the configuration context to\n+   * set up the binding\n+   * @param sparkContext spark context\n+   * @return the started instance\n+   */\n+  def createTimelineClient(sparkContext: SparkContext): TimelineClient = {\n+    val client = TimelineClient.createTimelineClient\n+    client.init(sparkContext.hadoopConfiguration)\n+    client.start\n+    client\n+  }\n+\n+  /**\n+   * The path for the V1 ATS REST API\n+   */\n+  val TIMELINE_REST_PATH = s\"/ws/v1/timeline/\"\n+\n+  /**\n+   * Build the URI to the base of the timeline web application\n+   * from the Hadoop context.\n+   * <p>\n+   * Raises an exception if the address cannot be determined.\n+   * <p>\n+   * Does not perform any checks as to whether or note the timeline\n+   * service is enabled\n+   * @param conf configuration\n+   * @return the URI to the timeline service.\n+   */\n+  def getTimelineEndpoint(conf: Configuration): URI = {\n+    val isHttps = YarnConfiguration.useHttps(conf)\n+    val address = if (isHttps) {\n+      conf.get(YarnConfiguration.TIMELINE_SERVICE_WEBAPP_HTTPS_ADDRESS,\n+                YarnConfiguration.DEFAULT_TIMELINE_SERVICE_WEBAPP_HTTPS_ADDRESS)\n+    } else {\n+      conf.get(YarnConfiguration.TIMELINE_SERVICE_WEBAPP_ADDRESS,\n+                YarnConfiguration.DEFAULT_TIMELINE_SERVICE_WEBAPP_ADDRESS)\n+    }\n+    val protocol = if (isHttps) \"https://\" else \"http://\"\n+    require(address != null, s\"No timeline service defined\")\n+    URI.create(s\"$protocol${address}$TIMELINE_REST_PATH\")\n+  }\n+\n+  /**\n+   * Create a URI to the history service. This uses the entity type of\n+   * <code>YarnHistoryService.ENTITY_TYPE</code> to\n+   * @param conf\n+   * @return\n+   */\n+  def timelineWebappUri(conf: Configuration): URI = {\n+    timelineWebappUri(conf, YarnHistoryService.SPARK_EVENT_ENTITY_TYPE)\n+  }\n+\n+  /**\n+   * Get the URI of a path under the timeline web UI\n+   * @param conf configuration\n+   * @param subpath path under the root web UI\n+   * @return a URI\n+   */\n+  def timelineWebappUri(conf: Configuration, subpath: String): URI = {\n+    val base = getTimelineEndpoint(conf)\n+    new URL(base.toURL, subpath).toURI\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled(conf: Configuration): Boolean = {\n+    conf.getBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED,\n+                    YarnConfiguration.DEFAULT_TIMELINE_SERVICE_ENABLED)\n+  }\n+\n+  /**\n+   * Get the URI to an application under the timeline\n+   * (this requires the applicationID to have been used to\n+   * publish entities there)\n+   * @param timelineUri timeline URI\n+   * @param appId App ID (really, the entityId used to publish)\n+   * @return the path\n+   */\n+  def applicationURI(timelineUri: URI, appId: String): URI = {\n+    require(appId != null && !appId.isEmpty, \"No application ID\")\n+    require(!appId.contains(\"/\"), s\"Illegal character '/' in $appId\")\n+    timelineUri.resolve(s\"${timelineUri.getPath()}/$appId\")\n+  }\n+\n+  /**\n+   * Map an error code to a string. For known codes, it returns\n+   * a description; for others it just returns the error code.\n+   * \n+   * @param code error code\n+   * @return a string description for error messages\n+   */\n+  def timelineErrorCodeToString(code: Int): String ={\n+    code match {\n+      case 1 => \"No start time\"\n+      case 2 => \"IO Exception\"\n+      case 3 => \"System Filter Conflict\"\n+      case 4 => \"Access Denied\"\n+      case 5 => \"No Domain\"\n+      case 6 => \"Forbidden Relation\"\n+      case other: Int => s\"Error code $other\"\n+    }\n+  }\n+\n+  /**\n+   * Convert a timeline error response to a meaningful string\n+   * @param error error\n+   * @return text for diagnostics\n+   */\n+  def describeError(error: TimelinePutError): String = {\n+    s\"Entity ID=${error.getEntityId()}; Entity type=${error.getEntityType}\" +\n+    s\" Error code ${error.getErrorCode}\" +\n+    s\": ${timelineErrorCodeToString(error.getErrorCode)}\"\n+  }\n+\n+  /**\n+   * Describe a put response by enumerating and describing all errors\n+   * (if present. A null errors element is handles robustly)\n+   * @param response response to describe\n+   * @return text for diagnostics\n+   */\n+  def describePutResponse(response: TimelinePutResponse) : String = {\n+    val responseErrs = response.getErrors\n+    if (responseErrs!=null) {\n+      val errors: List[String] = List(s\"TimelinePutResponse with ${responseErrs.size()} errors\")\n+      for (err <- responseErrs) {\n+        errors +: describeError(err)\n+      }\n+      errors.foldLeft(\"\")((buff, elt) => buff + \"\\n\" + elt)\n+    } else {\n+      s\"TimelinePutResponse with null error list\"\n+    }\n+  }\n+\n+  /**\n+   * This is used to highlight an undefined field\n+   */\n+  val UNDEFINED_FIELD = \"Undefined\"\n+\n+  /**\n+   * Lookup a field in the `otherInfo` section of a [[TimelineEntity]]\n+   * @param en entity\n+   * @param name field name\n+   * @return the value or the string [[UNDEFINED_FIELD]] if not\n+   * @throws Exception if the field is not found\n+   */\n+  def field(en: TimelineEntity, name: String) : Object = {\n+    fieldOption(en, name) match {\n+      case Some(v) => v\n+      case None => UNDEFINED_FIELD\n+    }\n+  }\n+\n+  /**\n+   * Lookup a field in the `otherInfo` section of a [[TimelineEntity]]\n+   * @param en entity\n+   * @param name field name\n+   * @return the value\n+   * @throws Exception if the field is not found\n+   */\n+  def fieldOption(en: TimelineEntity, name: String) : Option[Object] = {\n+    var value = en.getOtherInfo().get(name)\n+    Option.apply(value)\n+  }\n+\n+  /**\n+   * Lookup a field in the `otherInfo` section of a [[TimelineEntity]]\n+   * @param en entity\n+   * @param name field name\n+   * @return the value converted to a string\n+   * @throws Exception if the field is not found\n+   */\n+  def stringFieldOption(en: TimelineEntity, name: String): Option[String] = {\n+    var value = en.getOtherInfo().get(name)\n+    if (value != null ) {\n+      Some(value.toString)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  /**\n+   * Lookup a numeric field in the `otherInfo` section of a [[TimelineEntity]],\n+   * fall back to `defval` if the field is absent or cannot be parsed\n+   * @param en entity\n+   * @param name field name\n+   * @param defval default value; default is 0L\n+   * @return the value\n+   */\n+  def numberField(en: TimelineEntity, name: String, defval:Long = 0L) : Number = {\n+    try {\n+      fieldOption(en, name) match {\n+        case Some(n: Number) => n\n+        case _ => defval\n+      }\n+    } catch {\n+      case NonFatal(e) => defval\n+    }\n+  }\n+\n+  /**\n+   * Take a sequence of timeline events and return an ordered list of spark events.\n+   * Important: this reverses the input in the process\n+   * @param events event sequence\n+   * @return spark event sequence\n+   */\n+  def asSparkEvents(events: Seq[TimelineEvent]): Seq[SparkListenerEvent] = {\n+    events.reverse.map { event =>\n+      toSparkEvent(event)\n+    }\n+  }\n+\n+  /**\n+   * Build date for display in status messages\n+   * @param timestamp time in milliseconds post-Epoch\n+   * @param unset string to use if timestamp == 0\n+   * @return a string for messages\n+   */\n+  def humanDateCurrentTZ(timestamp: Long, unset: String) : String = {\n+    if (timestamp == 0) {\n+      unset\n+    } else {\n+      val dateFormatter = DateFormat.getDateTimeInstance(DateFormat.DEFAULT,\n+                                                          DateFormat.LONG)\n+      dateFormatter.format(timestamp)\n+    }\n+  }\n+\n+  /**\n+   * Short formatted time\n+   * @param timestamp time in milliseconds post-Epoch\n+   * @param unset string to use if timestamp == 0\n+   * @return a string for messages\n+   */\n+  def timeShort(timestamp: Long, unset: String) : String = {\n+    if (timestamp == 0) {\n+      unset\n+    } else {\n+      val dateFormatter = DateFormat.getTimeInstance(DateFormat.SHORT)\n+      dateFormatter.format(timestamp)\n+    }\n+  }\n+\n+  /**\n+   * Generate the timeline entity ID from the application and attempt ID.\n+   * This is required to be unique across all entities in the timeline server\n+   * @param yarnAppId yarn application ID as passed in during creation\n+   * @param yarnAttemptId YARN attempt ID as passed in during creation\n+   * @param sparkAppId application ID in the application start event\n+   * @param attemptId attempt ID in the application start event.\n+   */\n+  def buildEntityId(yarnAppId: ApplicationId,\n+      yarnAttemptId: Option[ApplicationAttemptId],\n+      sparkAppId: Option[String],\n+      attemptId: Option[String]): String = {\n+    yarnAttemptId match {\n+      case Some(aid) => aid.toString\n+      case None => yarnAppId.toString\n+    }\n+  }\n+\n+  /**\n+   * Generate the application ID for use in entity fields from the application and attempt ID.\n+   * @param yarnAppId yarn application ID as passed in during creation\n+   * @param sparkAppId application ID as submitted in the application start event\n+   */\n+  def buildApplicationIdField(yarnAppId: ApplicationId, sparkAppId: Option[String]): String = {\n+    yarnAppId.toString\n+  }\n+\n+  /**\n+   * Generate an attempt ID for use in the timeline entity \"other/app_id\" field\n+   * from the application and attempt ID.\n+   *\n+   * This is not guaranteed to be unique across all entities. It is\n+   * only required to be unique across all attempts of an application.\n+   *\n+   * If the application doesn't have an attempt ID, then it is\n+   * an application instance which, implicitly, is single-attempt.\n+   * The value [[SINGLE_ATTEMPT]] is returned\n+   * @param sparkAppId application ID as submitted in the application start event\n+   * @param sparkAttemptId attempt ID\n+   * @return the attempt ID.\n+   */\n+  def buildApplicationAttemptIdField(sparkAppId: Option[String],\n+      sparkAttemptId: Option[String]): String = {\n+    sparkAttemptId.getOrElse(SINGLE_ATTEMPT)\n+  }\n+\n+  /**\n+   * Add a filter and field if the value is set\n+   * @param entity entity to update\n+   * @param name filter/field name\n+   * @param value optional value\n+   */\n+  private def addFilterAndField(entity: TimelineEntity,\n+      name: String, value: Option[String]): Unit = {\n+    value.foreach(v => addFilterAndField(entity, name, v))"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: indented too far.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T00:41:13Z",
    "diffHunk": "@@ -0,0 +1,720 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.io.IOException\n+import java.net.{URI, URL}\n+import java.text.DateFormat\n+import java.util.concurrent.atomic.AtomicLong\n+import java.util.{ArrayList => JArrayList, Collection => JCollection, Date, HashMap => JHashMap, Map => JMap}\n+import java.{lang, util}\n+\n+import scala.collection.JavaConversions._\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.service.Service\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse.TimelinePutError\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+import org.json4s.JsonAST.JObject\n+import org.json4s._\n+import org.json4s.jackson.JsonMethods._\n+\n+import org.apache.spark.{SparkContext, Logging}\n+import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+import org.apache.spark.scheduler.{SparkListenerStageCompleted, SparkListenerStageSubmitted, SparkListenerExecutorAdded, SparkListenerExecutorRemoved, SparkListenerJobEnd, SparkListenerJobStart, SparkListenerApplicationEnd, SparkListenerApplicationStart, SparkListenerEvent}\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+\n+/**\n+ * Utility methods for timeline classes.\n+ */\n+private[spark] object YarnTimelineUtils extends Logging {\n+\n+  /**\n+   * What attempt ID to use as the attempt ID field (not the entity ID) when\n+   * there is no attempt info\n+   */\n+  val SINGLE_ATTEMPT = \"1\"\n+\n+\n+  /**\n+   * Exception text when there is no event info data to unmarshall\n+   */\n+  val E_NO_EVENTINFO = \"No 'eventinfo' entry\"\n+\n+  /**\n+   * Exception text when there is event info entry in the timeline event, but it is empty\n+   */\n+\n+  val E_EMPTY_EVENTINFO = \"Empty 'eventinfo' entry\"\n+\n+  /**\n+   * counter incremented on every spark event to timeline event creation,\n+   * so guaranteeing uniqueness of event IDs across a single application attempt\n+   * (which is implicitly, one per JVM)\n+   */\n+  val uid = new AtomicLong(System.currentTimeMillis())\n+\n+  /**\n+   * Converts a Java object to its equivalent json4s representation.\n+   */\n+  def toJValue(obj: Object): JValue = {\n+    obj match {\n+      case str: String => JString(str)\n+      case dbl: java.lang.Double => JDouble(dbl)\n+      case dec: java.math.BigDecimal => JDecimal(dec)\n+      case int: java.lang.Integer => JInt(BigInt(int))\n+      case long: java.lang.Long => JInt(BigInt(long))\n+      case bool: java.lang.Boolean => JBool(bool)\n+      case map: JMap[_, _] =>\n+        val jmap = map.asInstanceOf[JMap[String, Object]]\n+        JObject(jmap.entrySet().map { e => (e.getKey() -> toJValue(e.getValue())) }.toList)\n+      case array: JCollection[_] =>\n+        JArray(array.asInstanceOf[JCollection[Object]].map(o => toJValue(o)).toList)\n+      case null => JNothing\n+    }\n+  }\n+\n+  /**\n+   * Converts a JValue into its Java equivalent.\n+   */\n+  def toJavaObject(v: JValue): Object = {\n+    v match {\n+      case JNothing => null\n+      case JNull => null\n+      case JString(s) => s\n+      case JDouble(num) => java.lang.Double.valueOf(num)\n+      case JDecimal(num) => num.bigDecimal\n+      case JInt(num) => java.lang.Long.valueOf(num.longValue)\n+      case JBool(value) => java.lang.Boolean.valueOf(value)\n+      case obj: JObject => toJavaMap(obj)\n+      case JArray(vals) => {\n+        val list = new JArrayList[Object]()\n+        vals.foreach(x => list.add(toJavaObject(x)))\n+        list\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Converts a json4s list of fields into a Java Map suitable for serialization by Jackson,\n+   * which is used by the ATS client library.\n+   */\n+  def toJavaMap(sourceObj: JObject): JHashMap[String, Object] = {\n+    val map = new JHashMap[String, Object]()\n+    sourceObj.obj.foreach(f => map.put(f._1, toJavaObject(f._2)))\n+    map\n+  }\n+\n+  /**\n+   * Convert a timeline event to a spark one. Includes\n+   * some basic checks for validity of event payload.\n+   * @param event timeline event\n+   * @return an unmarshalled event\n+   */\n+  def toSparkEvent(event: TimelineEvent): SparkListenerEvent = {\n+    val info = event.getEventInfo()\n+    if (info == null) {\n+      throw new IOException(E_NO_EVENTINFO)\n+    }\n+    if (info.size() == 0) {\n+      throw new IOException(E_EMPTY_EVENTINFO)\n+    }\n+    val payload = toJValue(info)\n+    def jsonToString: String = {\n+      val json = compact(render(payload))\n+      val limit = 256\n+      if (json.length < limit) {\n+        json\n+      } else {\n+          json.substring(0, limit) + \" ... }\"\n+        }\n+    }\n+    logDebug(s\"payload is ${jsonToString}\")\n+    val eventField = payload \\ \"Event\"\n+    if (eventField == JNothing) {\n+      throw new IOException(s\"No 'Event' entry in $jsonToString\")\n+    }\n+\n+    // now the real unmarshalling\n+    try {\n+      JsonProtocol.sparkEventFromJson(payload)\n+    } catch {\n+      // failure in the marshalling; include payload in the message\n+      case ex: MappingException => {\n+        logError(s\"$ex while rendering $jsonToString\", ex)\n+        throw ex\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Convert a spark event to a timeline event\n+   * @param event handled spark event\n+   * @return a timeline event\n+   */\n+  def toTimelineEvent(event: HandleSparkEvent): TimelineEvent = {\n+    val tlEvent = new TimelineEvent()\n+    tlEvent.setEventType(Utils.getFormattedClassName(event.sparkEvent)\n+        + \"-\" + YarnTimelineUtils.uid.incrementAndGet.toString)\n+    tlEvent.setTimestamp(event.time)\n+    val kvMap = new JHashMap[String, Object]()\n+    val json = JsonProtocol.sparkEventToJson(event.sparkEvent)\n+    val jObject = json.asInstanceOf[JObject]\n+    // the timeline event wants a map of java objects for Jackson to serialize\n+    val hashMap = toJavaMap(jObject)\n+    tlEvent.setEventInfo(hashMap)\n+    tlEvent\n+  }\n+\n+  /**\n+   * Describe the event for logging\n+   * @param event timeline event\n+   * @return a description\n+   */\n+  def describeEvent(event: TimelineEvent): String = {\n+    var sparkEventDetails = \"\"\n+    try {\n+      sparkEventDetails = toSparkEvent(event).toString\n+    } catch {\n+      case _: Exception =>\n+        sparkEventDetails = \"(cannot convert event details to spark exception)\"\n+    }\n+    s\"${event.getEventType()} @ ${new Date(event.getTimestamp())}\" +\n+        s\"\\n    ${sparkEventDetails}\"\n+  }\n+\n+  /**\n+   * Create details of a timeline entity, by describing every event inside it\n+   * @param entity entity containing a possibly empty or null list of events\n+   * @return a list of event details, with a newline between each one\n+   */\n+  def eventDetails(entity: TimelineEntity): String = {\n+    val events: util.List[TimelineEvent] = entity.getEvents\n+    if (events != null) {\n+      events.foldLeft(\"\") {\n+        (s, evt) => (s + \"\\n\" + describeEvent(evt))\n+      }\n+    } else {\n+      \"\"\n+    }\n+  }\n+\n+  /**\n+   * Describe a timeline entity\n+   * @param entity entity\n+   * @return a string description.\n+   */\n+  def describeEntity(entity: TimelineEntity): String = {\n+    val events: util.List[TimelineEvent] = entity.getEvents\n+    val eventSummary = if (events != null) {\n+      s\"contains ${events.size()} event(s)\"\n+    } else {\n+      \"contains no events\"\n+    }\n+\n+    val header = s\"${entity.getEntityType }/${entity.getEntityId } @${entity.getDomainId }}\"\n+    val otherInfo = entity.getOtherInfo().foldLeft(\"\\n\") (\n+      (acc, kv) => acc + s\" ${kv._1} = ${kv._2}; \"\n+    )\n+    s\"Timeline Entity \" + header +\n+        \" \" + otherInfo +\n+        \" \" + timeFieldToString(entity.getStartTime(), \"start\") +\n+        \" \" + eventSummary\n+  }\n+\n+  /**\n+   * Convert a `java.lang.Long` reference to a string value, or, if the reference is null,\n+   * to text declaring that the named field is empty\n+   * @param time time reference\n+   * @param field field name for error message\n+   * @return a string to describe the field\n+   */\n+  def timeFieldToString(time: lang.Long, field: String): String = {\n+    if (time != null) {\n+      new Date(time).toString\n+    } else {\n+       (\"no \" + field + \" time\")\n+     }\n+  }\n+\n+  /**\n+   * A verbose description of the entity which contains event details and info about\n+   * primary/secondary keys\n+   * @param entity timeline entity\n+   * @return a verbose description of the field\n+   */\n+  def describeEntityVerbose(entity: TimelineEntity): String = {\n+    val header = describeEntity(entity)\n+    val primaryFilters = entity.getPrimaryFilters.toMap\n+    var filterElements = \"\"\n+    for ((k, v) <- primaryFilters) {\n+      filterElements = filterElements +\n+        \" filter \" + k + \": [ \" + v.foldLeft(\"\")((s, o) => (s + o.toString + \" \")) +\"]\\n\"\n+    }\n+\n+    val events = eventDetails(entity)\n+    header + \"\\n\" + filterElements + events\n+  }\n+\n+  /**\n+   * Stop any optional service\n+   * @param svc service\n+   */\n+  def stopOptionalService(svc: Option[Service]): Unit = {\n+    svc match {\n+      case Some(client) => client.stop()\n+      case None =>\n+    }\n+  }\n+\n+  /**\n+   * Split a comma separated String, filter out any empty items, and return a Set of strings\n+   */\n+  def stringToSet(list: String): Set[String] = {\n+    list.split(',').map(_.trim).filter(!_.isEmpty).toSet\n+  }\n+\n+  /**\n+   * Try to get the event time off an event. Not all events have the required information\n+   * @param event event to process\n+   * @return the event time\n+   */\n+  def eventTime(event: SparkListenerEvent): Option[Long] = {\n+    event match {\n+      case evt: SparkListenerApplicationStart =>\n+        Some(evt.time)\n+      case evt: SparkListenerApplicationEnd =>\n+        Some(evt.time)\n+      case evt: SparkListenerJobStart =>\n+        Some(evt.time)\n+      case evt: SparkListenerJobEnd =>\n+        Some(evt.time)\n+      case evt: SparkListenerExecutorAdded =>\n+        Some(evt.time)\n+      case evt: SparkListenerExecutorRemoved =>\n+        Some(evt.time)\n+      case evt: SparkListenerStageSubmitted =>\n+        evt.stageInfo.submissionTime\n+      case evt: SparkListenerStageCompleted =>\n+        evt.stageInfo.completionTime\n+      case _ => None\n+    }\n+  }\n+\n+\n+  /**\n+   * Create and start a timeline client, using the configuration context to\n+   * set up the binding\n+   * @param sparkContext spark context\n+   * @return the started instance\n+   */\n+  def createTimelineClient(sparkContext: SparkContext): TimelineClient = {\n+    val client = TimelineClient.createTimelineClient\n+    client.init(sparkContext.hadoopConfiguration)\n+    client.start\n+    client\n+  }\n+\n+  /**\n+   * The path for the V1 ATS REST API\n+   */\n+  val TIMELINE_REST_PATH = s\"/ws/v1/timeline/\"\n+\n+  /**\n+   * Build the URI to the base of the timeline web application\n+   * from the Hadoop context.\n+   * <p>\n+   * Raises an exception if the address cannot be determined.\n+   * <p>\n+   * Does not perform any checks as to whether or note the timeline\n+   * service is enabled\n+   * @param conf configuration\n+   * @return the URI to the timeline service.\n+   */\n+  def getTimelineEndpoint(conf: Configuration): URI = {\n+    val isHttps = YarnConfiguration.useHttps(conf)\n+    val address = if (isHttps) {\n+      conf.get(YarnConfiguration.TIMELINE_SERVICE_WEBAPP_HTTPS_ADDRESS,\n+                YarnConfiguration.DEFAULT_TIMELINE_SERVICE_WEBAPP_HTTPS_ADDRESS)\n+    } else {\n+      conf.get(YarnConfiguration.TIMELINE_SERVICE_WEBAPP_ADDRESS,\n+                YarnConfiguration.DEFAULT_TIMELINE_SERVICE_WEBAPP_ADDRESS)\n+    }\n+    val protocol = if (isHttps) \"https://\" else \"http://\"\n+    require(address != null, s\"No timeline service defined\")\n+    URI.create(s\"$protocol${address}$TIMELINE_REST_PATH\")\n+  }\n+\n+  /**\n+   * Create a URI to the history service. This uses the entity type of\n+   * <code>YarnHistoryService.ENTITY_TYPE</code> to\n+   * @param conf\n+   * @return\n+   */\n+  def timelineWebappUri(conf: Configuration): URI = {\n+    timelineWebappUri(conf, YarnHistoryService.SPARK_EVENT_ENTITY_TYPE)\n+  }\n+\n+  /**\n+   * Get the URI of a path under the timeline web UI\n+   * @param conf configuration\n+   * @param subpath path under the root web UI\n+   * @return a URI\n+   */\n+  def timelineWebappUri(conf: Configuration, subpath: String): URI = {\n+    val base = getTimelineEndpoint(conf)\n+    new URL(base.toURL, subpath).toURI\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled(conf: Configuration): Boolean = {\n+    conf.getBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED,\n+                    YarnConfiguration.DEFAULT_TIMELINE_SERVICE_ENABLED)\n+  }\n+\n+  /**\n+   * Get the URI to an application under the timeline\n+   * (this requires the applicationID to have been used to\n+   * publish entities there)\n+   * @param timelineUri timeline URI\n+   * @param appId App ID (really, the entityId used to publish)\n+   * @return the path\n+   */\n+  def applicationURI(timelineUri: URI, appId: String): URI = {\n+    require(appId != null && !appId.isEmpty, \"No application ID\")\n+    require(!appId.contains(\"/\"), s\"Illegal character '/' in $appId\")\n+    timelineUri.resolve(s\"${timelineUri.getPath()}/$appId\")\n+  }\n+\n+  /**\n+   * Map an error code to a string. For known codes, it returns\n+   * a description; for others it just returns the error code.\n+   * \n+   * @param code error code\n+   * @return a string description for error messages\n+   */\n+  def timelineErrorCodeToString(code: Int): String ={\n+    code match {\n+      case 1 => \"No start time\"\n+      case 2 => \"IO Exception\"\n+      case 3 => \"System Filter Conflict\"\n+      case 4 => \"Access Denied\"\n+      case 5 => \"No Domain\"\n+      case 6 => \"Forbidden Relation\"\n+      case other: Int => s\"Error code $other\"\n+    }\n+  }\n+\n+  /**\n+   * Convert a timeline error response to a meaningful string\n+   * @param error error\n+   * @return text for diagnostics\n+   */\n+  def describeError(error: TimelinePutError): String = {\n+    s\"Entity ID=${error.getEntityId()}; Entity type=${error.getEntityType}\" +\n+    s\" Error code ${error.getErrorCode}\" +\n+    s\": ${timelineErrorCodeToString(error.getErrorCode)}\"\n+  }\n+\n+  /**\n+   * Describe a put response by enumerating and describing all errors\n+   * (if present. A null errors element is handles robustly)\n+   * @param response response to describe\n+   * @return text for diagnostics\n+   */\n+  def describePutResponse(response: TimelinePutResponse) : String = {\n+    val responseErrs = response.getErrors\n+    if (responseErrs!=null) {\n+      val errors: List[String] = List(s\"TimelinePutResponse with ${responseErrs.size()} errors\")\n+      for (err <- responseErrs) {\n+        errors +: describeError(err)\n+      }\n+      errors.foldLeft(\"\")((buff, elt) => buff + \"\\n\" + elt)\n+    } else {\n+      s\"TimelinePutResponse with null error list\"\n+    }\n+  }\n+\n+  /**\n+   * This is used to highlight an undefined field\n+   */\n+  val UNDEFINED_FIELD = \"Undefined\"\n+\n+  /**\n+   * Lookup a field in the `otherInfo` section of a [[TimelineEntity]]\n+   * @param en entity\n+   * @param name field name\n+   * @return the value or the string [[UNDEFINED_FIELD]] if not\n+   * @throws Exception if the field is not found\n+   */\n+  def field(en: TimelineEntity, name: String) : Object = {\n+    fieldOption(en, name) match {\n+      case Some(v) => v\n+      case None => UNDEFINED_FIELD\n+    }\n+  }\n+\n+  /**\n+   * Lookup a field in the `otherInfo` section of a [[TimelineEntity]]\n+   * @param en entity\n+   * @param name field name\n+   * @return the value\n+   * @throws Exception if the field is not found\n+   */\n+  def fieldOption(en: TimelineEntity, name: String) : Option[Object] = {\n+    var value = en.getOtherInfo().get(name)\n+    Option.apply(value)\n+  }\n+\n+  /**\n+   * Lookup a field in the `otherInfo` section of a [[TimelineEntity]]\n+   * @param en entity\n+   * @param name field name\n+   * @return the value converted to a string\n+   * @throws Exception if the field is not found\n+   */\n+  def stringFieldOption(en: TimelineEntity, name: String): Option[String] = {\n+    var value = en.getOtherInfo().get(name)\n+    if (value != null ) {\n+      Some(value.toString)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  /**\n+   * Lookup a numeric field in the `otherInfo` section of a [[TimelineEntity]],\n+   * fall back to `defval` if the field is absent or cannot be parsed\n+   * @param en entity\n+   * @param name field name\n+   * @param defval default value; default is 0L\n+   * @return the value\n+   */\n+  def numberField(en: TimelineEntity, name: String, defval:Long = 0L) : Number = {\n+    try {\n+      fieldOption(en, name) match {\n+        case Some(n: Number) => n\n+        case _ => defval\n+      }\n+    } catch {\n+      case NonFatal(e) => defval\n+    }\n+  }\n+\n+  /**\n+   * Take a sequence of timeline events and return an ordered list of spark events.\n+   * Important: this reverses the input in the process\n+   * @param events event sequence\n+   * @return spark event sequence\n+   */\n+  def asSparkEvents(events: Seq[TimelineEvent]): Seq[SparkListenerEvent] = {\n+    events.reverse.map { event =>\n+      toSparkEvent(event)\n+    }\n+  }\n+\n+  /**\n+   * Build date for display in status messages\n+   * @param timestamp time in milliseconds post-Epoch\n+   * @param unset string to use if timestamp == 0\n+   * @return a string for messages\n+   */\n+  def humanDateCurrentTZ(timestamp: Long, unset: String) : String = {\n+    if (timestamp == 0) {\n+      unset\n+    } else {\n+      val dateFormatter = DateFormat.getDateTimeInstance(DateFormat.DEFAULT,\n+                                                          DateFormat.LONG)\n+      dateFormatter.format(timestamp)\n+    }\n+  }\n+\n+  /**\n+   * Short formatted time\n+   * @param timestamp time in milliseconds post-Epoch\n+   * @param unset string to use if timestamp == 0\n+   * @return a string for messages\n+   */\n+  def timeShort(timestamp: Long, unset: String) : String = {\n+    if (timestamp == 0) {\n+      unset\n+    } else {\n+      val dateFormatter = DateFormat.getTimeInstance(DateFormat.SHORT)\n+      dateFormatter.format(timestamp)\n+    }\n+  }\n+\n+  /**\n+   * Generate the timeline entity ID from the application and attempt ID.\n+   * This is required to be unique across all entities in the timeline server\n+   * @param yarnAppId yarn application ID as passed in during creation\n+   * @param yarnAttemptId YARN attempt ID as passed in during creation\n+   * @param sparkAppId application ID in the application start event\n+   * @param attemptId attempt ID in the application start event.\n+   */\n+  def buildEntityId(yarnAppId: ApplicationId,\n+      yarnAttemptId: Option[ApplicationAttemptId],\n+      sparkAppId: Option[String],\n+      attemptId: Option[String]): String = {\n+    yarnAttemptId match {\n+      case Some(aid) => aid.toString\n+      case None => yarnAppId.toString\n+    }\n+  }\n+\n+  /**\n+   * Generate the application ID for use in entity fields from the application and attempt ID.\n+   * @param yarnAppId yarn application ID as passed in during creation\n+   * @param sparkAppId application ID as submitted in the application start event\n+   */\n+  def buildApplicationIdField(yarnAppId: ApplicationId, sparkAppId: Option[String]): String = {\n+    yarnAppId.toString\n+  }\n+\n+  /**\n+   * Generate an attempt ID for use in the timeline entity \"other/app_id\" field\n+   * from the application and attempt ID.\n+   *\n+   * This is not guaranteed to be unique across all entities. It is\n+   * only required to be unique across all attempts of an application.\n+   *\n+   * If the application doesn't have an attempt ID, then it is\n+   * an application instance which, implicitly, is single-attempt.\n+   * The value [[SINGLE_ATTEMPT]] is returned\n+   * @param sparkAppId application ID as submitted in the application start event\n+   * @param sparkAttemptId attempt ID\n+   * @return the attempt ID.\n+   */\n+  def buildApplicationAttemptIdField(sparkAppId: Option[String],\n+      sparkAttemptId: Option[String]): String = {\n+    sparkAttemptId.getOrElse(SINGLE_ATTEMPT)\n+  }\n+\n+  /**\n+   * Add a filter and field if the value is set\n+   * @param entity entity to update\n+   * @param name filter/field name\n+   * @param value optional value\n+   */\n+  private def addFilterAndField(entity: TimelineEntity,\n+      name: String, value: Option[String]): Unit = {\n+    value.foreach(v => addFilterAndField(entity, name, v))\n+  }\n+\n+  /**\n+   * Add a filter and field\n+   * @param entity entity to update\n+   * @param name filter/field name\n+   * @param value value\n+   */\n+  private def addFilterAndField(entity: TimelineEntity, name: String, value: String): Unit = {\n+      entity.addPrimaryFilter(name, value)"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "steveloughran"
    },
    "body": "I've got this wrong; `errors` must be a mutable list & appended to. Will fix\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-10-21T19:17:57Z",
    "diffHunk": "@@ -0,0 +1,765 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.io.IOException\n+import java.net.{InetSocketAddress, NoRouteToHostException, URI, URL}\n+import java.text.DateFormat\n+import java.util.concurrent.atomic.AtomicLong\n+import java.util.{ArrayList => JArrayList, Collection => JCollection, Date, HashMap => JHashMap, Map => JMap}\n+import java.{lang, util}\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse.TimelinePutError\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+import org.json4s.JsonAST.JObject\n+import org.json4s._\n+import org.json4s.jackson.JsonMethods._\n+\n+import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+import org.apache.spark.scheduler.{SparkListenerApplicationEnd, SparkListenerApplicationStart, SparkListenerEvent, SparkListenerExecutorAdded, SparkListenerExecutorRemoved, SparkListenerJobEnd, SparkListenerJobStart, SparkListenerStageCompleted, SparkListenerStageSubmitted}\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Utility methods for timeline classes.\n+ */\n+private[spark] object YarnTimelineUtils extends Logging {\n+\n+  /**\n+   * What attempt ID to use as the attempt ID field (not the entity ID) when\n+   * there is no attempt info.\n+   */\n+  val SINGLE_ATTEMPT = \"1\"\n+\n+  /**\n+   * Exception text when there is no event info data to unmarshall.\n+   */\n+  val E_NO_EVENTINFO = \"No 'eventinfo' entry\"\n+\n+  /**\n+   * Exception text when there is event info entry in the timeline event, but it is empty.\n+   */\n+\n+  val E_EMPTY_EVENTINFO = \"Empty 'eventinfo' entry\"\n+\n+  /**\n+   * counter incremented on every spark event to timeline event creation,\n+   * so guaranteeing uniqueness of event IDs across a single application attempt\n+   * (which is implicitly, one per JVM).\n+   */\n+  val uid = new AtomicLong(System.currentTimeMillis())\n+\n+  /**\n+   * Converts a Java object to its equivalent json4s representation.\n+   */\n+  def toJValue(obj: Object): JValue = {\n+    obj match {\n+      case str: String => JString(str)\n+      case dbl: java.lang.Double => JDouble(dbl)\n+      case dec: java.math.BigDecimal => JDecimal(dec)\n+      case int: java.lang.Integer => JInt(BigInt(int))\n+      case long: java.lang.Long => JInt(BigInt(long))\n+      case bool: java.lang.Boolean => JBool(bool)\n+      case map: JMap[_, _] =>\n+        val jmap = map.asInstanceOf[JMap[String, Object]]\n+        JObject(jmap.entrySet().asScala.map { e => e.getKey -> toJValue(e.getValue) }.toList)\n+      case array: JCollection[_] =>\n+        JArray(array.asInstanceOf[JCollection[Object]].asScala.map(o => toJValue(o)).toList)\n+      case null => JNothing\n+    }\n+  }\n+\n+  /**\n+   * Converts a JValue into its Java equivalent.\n+   */\n+  def toJavaObject(v: JValue): Object = {\n+    v match {\n+      case JNothing => null\n+      case JNull => null\n+      case JString(s) => s\n+      case JDouble(num) => java.lang.Double.valueOf(num)\n+      case JDecimal(num) => num.bigDecimal\n+      case JInt(num) => java.lang.Long.valueOf(num.longValue())\n+      case JBool(value) => java.lang.Boolean.valueOf(value)\n+      case obj: JObject => toJavaMap(obj)\n+      case JArray(vals) =>\n+        val list = new JArrayList[Object]()\n+        vals.foreach(x => list.add(toJavaObject(x)))\n+        list\n+    }\n+  }\n+\n+  /**\n+   * Converts a json4s list of fields into a Java Map suitable for serialization by Jackson,\n+   * which is used by the ATS client library.\n+   */\n+  def toJavaMap(sourceObj: JObject): JHashMap[String, Object] = {\n+    val map = new JHashMap[String, Object]()\n+    sourceObj.obj.foreach { case (k, v) => map.put(k, toJavaObject(v)) }\n+    map\n+  }\n+\n+  /**\n+   * Convert a timeline event to a spark one. Includes some basic checks for validity of\n+   * the event payload.\n+   * @param event timeline event\n+   * @return an unmarshalled event\n+   */\n+  def toSparkEvent(event: TimelineEvent): SparkListenerEvent = {\n+    val info = event.getEventInfo\n+    if (info == null) {\n+      throw new IOException(E_NO_EVENTINFO)\n+    }\n+    if (info.size() == 0) {\n+      throw new IOException(E_EMPTY_EVENTINFO)\n+    }\n+    val payload = toJValue(info)\n+    def jsonToString: String = {\n+      val json = compact(render(payload))\n+      val limit = 256\n+      if (json.length < limit) {\n+        json\n+      } else {\n+         json.substring(0, limit) + \" ... }\"\n+      }\n+    }\n+    logDebug(s\"toSparkEvent payload is $jsonToString\")\n+    val eventField = payload \\ \"Event\"\n+    if (eventField == JNothing) {\n+      throw new IOException(s\"No 'Event' entry in $jsonToString\")\n+    }\n+\n+    // now the real unmarshalling\n+    try {\n+      JsonProtocol.sparkEventFromJson(payload)\n+    } catch {\n+      // failure in the marshalling; include payload in the message\n+      case ex: MappingException =>\n+        logDebug(s\"$ex while rendering $jsonToString\", ex)\n+        throw ex\n+    }\n+  }\n+\n+  /**\n+   * Convert a spark event to a timeline event\n+   * @param event handled spark event\n+   * @return a timeline event\n+   */\n+  def toTimelineEvent(event: SparkListenerEvent, timestamp: Long): TimelineEvent = {\n+    val tlEvent = new TimelineEvent()\n+    tlEvent.setEventType(Utils.getFormattedClassName(event)\n+        + \"-\" + YarnTimelineUtils.uid.incrementAndGet.toString)\n+    tlEvent.setTimestamp(timestamp)\n+    val kvMap = new JHashMap[String, Object]()\n+    val json = JsonProtocol.sparkEventToJson(event)\n+    val jObject = json.asInstanceOf[JObject]\n+    // the timeline event wants a map of java objects for Jackson to serialize\n+    val hashMap = toJavaMap(jObject)\n+    tlEvent.setEventInfo(hashMap)\n+    tlEvent\n+  }\n+\n+  /**\n+   * Describe the event for logging.\n+   *\n+   * @param event timeline event\n+   * @return a description\n+   */\n+  def describeEvent(event: TimelineEvent): String = {\n+    val sparkEventDetails = try { {\n+      toSparkEvent(event).toString\n+    }\n+    } catch {\n+      case _: MappingException =>\n+       \"(cannot convert event details to spark exception)\"\n+    }\n+    s\"${event.getEventType()} @ ${new Date(event.getTimestamp())}\" +\n+      s\"\\n    $sparkEventDetails\"\n+  }\n+\n+  /**\n+   * Create details of a timeline entity, by describing every event inside it.\n+   *\n+   * @param entity entity containing a possibly empty or null list of events\n+   * @return a list of event details, with a newline between each one\n+   */\n+  def eventDetails(entity: TimelineEntity): String = {\n+    val events: util.List[TimelineEvent] = entity.getEvents\n+    if (events != null) {\n+      events.asScala.map(describeEvent).mkString(\"\\n\")\n+    } else {\n+      \"\"\n+    }\n+  }\n+\n+  /**\n+   * Describe a timeline entity.\n+   * @param entity entity\n+   * @return a string description.\n+   */\n+  def describeEntity(entity: TimelineEntity): String = {\n+    val events: util.List[TimelineEvent] = entity.getEvents\n+    val eventSummary = if (events != null) {\n+      s\"contains ${events.size()} event(s)\"\n+    } else {\n+      \"contains no events\"\n+    }\n+\n+    val domain = if (entity.getDomainId != null) s\" Domain ${entity.getDomainId}\" else \"\"\n+    val header = s\"${entity.getEntityType}/${entity.getEntityId} $domain\"\n+    try {\n+      events.asScala.map(describeEvent).mkString(\"\\n\")\n+      val otherInfo = entity.getOtherInfo.asScala.map {\n+        case (k, v) => s\" $k = $v;\"\n+      }.mkString(\"\\n\")\n+      s\"Timeline Entity \" + header +\n+          \" \" + otherInfo + \"\\n\" +\n+          \" started: \" + timeFieldToString(entity.getStartTime, \"start\") + \"\\n\" +\n+          \" \" + eventSummary\n+    } catch {\n+      case e: MappingException =>\n+        // failure to marshall/unmarshall; downgrade\n+        s\"Timeline Entity $header\"\n+    }\n+  }\n+\n+  /**\n+   * Convert a `java.lang.Long` reference to a string value, or, if the reference is null,\n+   * to text declaring that the named field is empty.\n+   *\n+   * @param time time reference\n+   * @param field field name for error message\n+   * @return a string to describe the field\n+   */\n+  def timeFieldToString(time: lang.Long, field: String): String = {\n+    if (time != null) {\n+      new Date(time).toString\n+    } else {\n+      s\"no $field time\"\n+    }\n+  }\n+\n+  /**\n+   * A verbose description of the entity which contains event details and info about\n+   * primary/secondary keys.\n+   *\n+   * @param entity timeline entity\n+   * @return a verbose description of the field\n+   */\n+  def describeEntityVerbose(entity: TimelineEntity): String = {\n+    val header = describeEntity(entity)\n+    val primaryFilters = entity.getPrimaryFilters.asScala.toMap\n+    var filterElements = \"\"\n+    for ((k, v) <- primaryFilters) {\n+      filterElements = filterElements +\n+        \" filter \" + k + \": [ \" + v.asScala.foldLeft(\"\")((s, o) => s + o.toString + \" \") + \"]\\n\"\n+    }\n+    val events = eventDetails(entity)\n+    header + \"\\n\" + filterElements + events\n+  }\n+\n+  /**\n+   * Split a comma separated String, filter out any empty items, and return a `Set` of strings.\n+   */\n+  def stringToSet(list: String): Set[String] = {\n+    list.split(',').map(_.trim).filter(!_.isEmpty).toSet\n+  }\n+\n+  /**\n+   * Try to get the event time off an event. Not all events have the required information.\n+   *\n+   * @param event event to process\n+   * @return the event time\n+   */\n+  def eventTime(event: SparkListenerEvent): Option[Long] = {\n+    event match {\n+      case evt: SparkListenerApplicationStart =>\n+        Some(evt.time)\n+      case evt: SparkListenerApplicationEnd =>\n+        Some(evt.time)\n+      case evt: SparkListenerJobStart =>\n+        Some(evt.time)\n+      case evt: SparkListenerJobEnd =>\n+        Some(evt.time)\n+      case evt: SparkListenerExecutorAdded =>\n+        Some(evt.time)\n+      case evt: SparkListenerExecutorRemoved =>\n+        Some(evt.time)\n+      case evt: SparkListenerStageSubmitted =>\n+        evt.stageInfo.submissionTime\n+      case evt: SparkListenerStageCompleted =>\n+        evt.stageInfo.completionTime\n+      case _ => None\n+    }\n+  }\n+\n+  /**\n+   * Create and start a timeline client, using the configuration context to\n+   * set up the binding.\n+   *\n+   * @param sparkContext spark context\n+   * @return the started instance\n+   */\n+  def createTimelineClient(sparkContext: SparkContext): TimelineClient = {\n+    val client = TimelineClient.createTimelineClient\n+    client.init(sparkContext.hadoopConfiguration)\n+    client.start()\n+    client\n+  }\n+\n+  /**\n+   * The path for the V1 ATS REST API.\n+   */\n+  val TIMELINE_REST_PATH = s\"/ws/v1/timeline/\"\n+\n+  /**\n+   * Build the URI to the base of the timeline web application\n+   * from the Hadoop context.\n+   *\n+   * Raises an exception if the address cannot be determined or is considered invalid from\n+   * a networking perspective.\n+   *\n+   * Does not perform any checks as to whether or not the timeline service is enabled\n+   * @param conf configuration\n+   * @return the URI to the timeline service.\n+   */\n+  def getTimelineEndpoint(conf: Configuration): URI = {\n+    val isHttps = YarnConfiguration.useHttps(conf)\n+    val address = if (isHttps) {\n+      conf.get(YarnConfiguration.TIMELINE_SERVICE_WEBAPP_HTTPS_ADDRESS,\n+                YarnConfiguration.DEFAULT_TIMELINE_SERVICE_WEBAPP_HTTPS_ADDRESS)\n+    } else {\n+      conf.get(YarnConfiguration.TIMELINE_SERVICE_WEBAPP_ADDRESS,\n+                YarnConfiguration.DEFAULT_TIMELINE_SERVICE_WEBAPP_ADDRESS)\n+    }\n+    val protocol = if (isHttps) \"https://\" else \"http://\"\n+    require(address != null, s\"No timeline service defined\")\n+    validateEndpoint(URI.create(s\"$protocol$address$TIMELINE_REST_PATH\"))\n+  }\n+\n+  /**\n+   * Create a URI to the history service. This uses the entity type of\n+   * [[YarnHistoryService#ENTITY_TYPE]] for spark application histories.\n+   * @param conf hadoop configuration to examine\n+   * @return\n+   */\n+  def timelineWebappUri(conf: Configuration): URI = {\n+    timelineWebappUri(conf, YarnHistoryService.SPARK_EVENT_ENTITY_TYPE)\n+  }\n+\n+  /**\n+   * Get the URI of a path under the timeline web UI.\n+   *\n+   * @param conf configuration\n+   * @param subpath path under the root web UI\n+   * @return a URI\n+   */\n+  def timelineWebappUri(conf: Configuration, subpath: String): URI = {\n+    val base = getTimelineEndpoint(conf)\n+    new URL(base.toURL, subpath).toURI\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled.\n+   *\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED` is set.\n+   */\n+  def timelineServiceEnabled(conf: Configuration): Boolean = {\n+    conf.getBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED,\n+                    YarnConfiguration.DEFAULT_TIMELINE_SERVICE_ENABLED)\n+  }\n+\n+  /**\n+   * Get the URI to an application under the timeline\n+   * (this requires the applicationID to have been used to\n+   * publish entities there)\n+   * @param timelineUri timeline URI\n+   * @param appId App ID (really, the entityId used to publish)\n+   * @return the path\n+   */\n+  def applicationURI(timelineUri: URI, appId: String): URI = {\n+    require(appId != null && !appId.isEmpty, \"No application ID\")\n+    require(!appId.contains(\"/\"), s\"Illegal character '/' in $appId\")\n+    timelineUri.resolve(s\"${timelineUri.getPath()}/$appId\")\n+  }\n+\n+  /**\n+   * Map an error code to a string. For known codes, it returns\n+   * a description; for others it just returns the error code.\n+   *\n+   * @param code error code\n+   * @return a string description for error messages\n+   */\n+  def timelineErrorCodeToString(code: Int): String = {\n+    code match {\n+      case 0 => \"0: no error\"\n+      case 1 => \"No start time\"\n+      case 2 => \"IO Exception\"\n+      case 3 => \"System Filter Conflict\"\n+      case 4 => \"Access Denied\"\n+      case 5 => \"No Domain\"\n+      case 6 => \"Forbidden Relation\"\n+      case other: Int => s\"Error code $other\"\n+    }\n+  }\n+\n+  /**\n+   * Convert a timeline error response to a slightly more meaningful string.\n+   * @param error error\n+   * @return text for diagnostics\n+   */\n+  def describeError(error: TimelinePutError): String = {\n+    s\"Entity ID=${error.getEntityId()}; Entity type=${error.getEntityType}\" +\n+    s\" Error code ${error.getErrorCode}\" +\n+    s\": ${timelineErrorCodeToString(error.getErrorCode)}\"\n+  }\n+\n+  /**\n+   * Describe a put response by enumerating and describing all errors.\n+   * (if present. A null `errors` element is handled robustly).\n+   *\n+   * @param response response to describe\n+   * @return text for diagnostics\n+   */\n+  def describePutResponse(response: TimelinePutResponse) : String = {\n+    val responseErrs = response.getErrors\n+    if (responseErrs != null) {\n+      var errors = List(s\"TimelinePutResponse with ${responseErrs.size()} errors\")\n+      for (err <- responseErrs.asScala) {\n+        errors :+ describeError(err)"
  }],
  "prId": 8744
}]